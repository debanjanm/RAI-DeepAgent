{"title": "A path-dependent PDE solver based on signature kernels", "sections": [{"section_id": 0, "text": "#### Abstract\n\nWe develop a provably convergent kernel-based solver for path-dependent PDEs (PPDEs). Our numerical scheme leverages signature kernels, a recently introduced class of kernels on path-space. Specifically, we solve an optimal recovery problem by approximating the solution of a PPDE with an element of minimal norm in the signature reproducing kernel Hilbert space constrained to satisfy the PPDE at a finite collection of collocation paths. In the linear case, we show that the optimisation has a unique closed-form solution expressed in terms of signature kernel evaluations at the collocation paths. We prove consistency of the proposed scheme, guaranteeing convergence to the PPDE solution as the number of collocation points increases. Finally, several numerical examples are presented, in particular in the context of option pricing under rough volatility. Our numerical scheme constitutes a valid alternative to the ubiquitous Monte Carlo methods.", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1. IntroduCTION\n\n1.1. Path-dependent PDEs. The Feynman-Kac formula asserts that solutions to backward Kolmogorov equations have a probabilistic representation as $u_{0}(t, x)=\\mathbb{E}\\left[\\phi\\left(Y_{T}\\right) \\mid Y_{t}=x\\right]$, for $0 \\leq t \\leq$ $T, x \\in \\mathbb{R}^{d}$, where $Y$ is an underlying Markov process. If $Y$ models the evolution of a financial asset price, then this expectation corresponds to the price of a European-type derivative written on this asset. In its seminal paper, Dupire [21] introduces a first generation of path-dependent PDEs (PPDEs) which solution is represented as $u_{1}(t, \\gamma)=\\mathbb{E}\\left[\\phi\\left(Y_{[0, T]}\\right) \\mid Y_{[0, t]}=\\gamma\\right]$ where $\\phi$ acts on the whole past of $Y$ and $\\gamma$ is a path. This expectation represents the price of a path-dependent derivative. Rough volatility models, and more generally stochastic Volterra processes, are not Markov processes hence neither formulation applies. The recent framework of [54] tackles precisely this class of processes, for which the path-dependence is inherent to the dynamics. We illustrate this with an example to highlight the differences with the aforementioned cases. Let us consider a one factor rough volatility model\n\n$$\nX_{t}=X_{0}+\\int_{0}^{t} \\sqrt{\\psi_{s}\\left(\\widehat{W}_{s}\\right)} \\mathrm{d} B_{s}-\\frac{1}{2} \\int_{0}^{t} \\psi_{s}\\left(\\widehat{W}_{s}\\right) \\mathrm{d} s, \\quad \\widehat{W}_{t}:=\\int_{0}^{t} K(t, r) \\mathrm{d} W_{r}\n$$\n\nwhere $X$ represents the log-price, $B$ and $W$ are correlated Brownian motions and generate their natural filtration $\\left(\\mathcal{F}_{t}\\right)_{t \\in[0, T]}$, the kernel $K$ is square integrable and typically taken to be $K(t, r)=$ $(t-r)^{H-\\frac{1}{2}}$ with $H \\in(0,1 / 2)$. In this situation, $\\widehat{W}$ is a Gaussian Volterra process, which is neither a semimartingale nor a Markov process, hence why classical tools are not suitable. For $s \\geq t$, we make the orthogonal decomposition $\\widehat{W}_{s}=\\Theta_{s}^{s}+I_{s}^{t}$ where $\\Theta_{s}^{t}=\\int_{0}^{t} K(s, r) \\mathrm{d} W_{r}$ is $\\mathcal{F}_{t}$-measurable while $I_{s}^{t}=\\int_{t}^{s} K(s, r) \\mathrm{d} W_{r}$ is independent from $\\mathcal{F}_{t}$. As a consequence one recovers the Markovian\n\n[^0]\n[^0]:    E-mail address: pannier@lpsm.paris, c.salvi@imperial.ac.uk.\n    2020 Mathematics Subject Classification. 35R15, 60L10, 65N35, 91G60.\n    Key words and phrases. Path signature, kernel methods, path-dependent PDEs, rough volatility.\n\nstructure as follows:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\phi\\left(X_{T}\\right) \\mid \\mathcal{F}_{t}\\right] & =\\mathbb{E}\\left[\\phi\\left(X_{t}+\\int_{t}^{T} \\sqrt{\\psi_{s}\\left(\\widehat{W}_{s}\\right)} \\mathrm{d} B_{s}-\\frac{1}{2} \\int_{t}^{T} \\psi_{s}\\left(\\widehat{W}_{s}\\right) \\mathrm{d} s\\right) \\mid \\mathcal{F}_{t}\\right] \\\\\n& =\\mathbb{E}\\left[\\phi\\left(X_{t}+\\int_{t}^{T} \\sqrt{\\psi_{s}\\left(\\Theta_{s}^{t}+I_{s}^{t}\\right)} \\mathrm{d} B_{s}-\\frac{1}{2} \\int_{t}^{T} \\psi_{s}\\left(\\Theta_{s}^{t}+I_{s}^{t}\\right) \\mathrm{d} s\\right) \\mid X_{t}, \\Theta_{[t, T]}^{t}\\right] \\\\\n& =\\mathbb{E}\\left[\\phi\\left(X_{T}\\right) \\mid X_{t}, \\Theta_{[t, T]}^{t}\\right]\n\\end{aligned}\n$$\n\nThis exhibits the path-dependence of the option price and entails there is a measurable map $u_{2}$ such that $u_{2}(t, x, \\gamma)=\\mathbb{E}\\left[\\phi\\left(X_{T}\\right) \\mid X_{t}=x, \\Theta^{t}=\\gamma_{[t, T]}\\right]$. Building on Viens and Zhang's functional It\u00f4 formula, further works $[10,55]$ study the associated path-dependent PDE of second generation of the form\n\n$$\n\\left\\{\\begin{array}{l}\n\\mathcal{L} u(t, x, \\gamma)=0 \\\\\nu(T, x, \\gamma)=\\phi(x)\n\\end{array}\\right.\n$$\n\nfor all $t \\in[0, T], x \\in \\mathbb{R}, \\gamma \\in \\mathcal{C}([0, T] ; \\mathbb{R})$, and where the linear operator $\\mathcal{L}$ is a combination of derivatives of all inputs. The pathwise derivatives are defined in Section 2.2 while important well-posed examples can be found in Section 2.3. This paper aims at solving this type of PPDEs.\n\nApart from the realm of rough volatility, stochastic Volterra processes have appeared in the modeling of electricity prices and turbulence $[3,7,19,39]$, in principal-agent problems [1], climate modeling [22, 52] and in asymptotic studies motivated by physcial phenomena [26, 27, 35]. More broadly, fractional Brownian motions and related processes are present in a variety of situations where intertemporal correlation is observed, with the seminal paper [37].\n\nWhile the PPDE representation finds other applications (e.g. weak error rates in [10]), this article is concerned with their numerical computation. The solution to a PPDE is a function of a path (and other finite dimensional inputs). Contrary to the wide literature on numerical schemes for highdimensional PDEs, we are here interested in purely infinite dimensional inputs that we will treat as such instead of discretising them. Despite the central place of such PPDEs in mathematical finance, numerical methods pertaining to them are scarse, see Section 1.6 for an overview of the literature. Our motivation is then twofold: design a numerical solver for such PDEs and overcome the technical challenges of learning a function on paths.\n1.2. Signature kernels. Kernel methods are a well-established class of algorithms that are the key components of several machine learning models such as support vector machines (SVMs). The central idea of kernel methods is to lift the (possibly unstructured) input data to a (possibly infinite dimensional) Hilbert space by means of a nonlinear feature map defined in terms of a kernel. The advantage of doing so lies in the fact that a non-linear optimisation task in the original space becomes linear once lifted to the feature space. Thus, the solution of the original optimisation problem can often be expressed entirely in terms of evaluations of the kernel on training points by means of celebrated representer theorems. The kernel can often be evaluated efficiently with no reference to the feature map, a property known as kernel trick. The selection of an effective kernel will usually be a task-dependent problem, and this challenge is particularly difficult when the data is sequential. Signature kernels $[33,46]$ are a class of universal kernels on sequential data which have received attention in recent years thanks to their efficiency in handling path-dependent problems $[34,49,17,47,16,30,29,38]$.\n1.3. Kernels vs neural networks for PDEs. In recent years, PDE solvers based on neural networks and kernel methods have been introduced as alternatives to classical finite difference and finite elements schemes. The most famous examples of neural PDE solvers are the Deep Galerkin model (DGM) [51] and Physics informed neural networks (PINNs) [43]. These techniques essentially parameterise the solution of the PDE as a neural network which is then trained using the PDE and its boundary conditions as loss function. More recently, DGM and PINNs have been extended to path-dependent\n\nPDEs by [45, 50, 31]. PDE solvers based on kernel methods have been investigated in various works $[18,12,13]$, but the development of kernel methods for path-dependent PDEs has remained open. PDE solvers based on kernel methods hold potential for considerable advantages over neural PDE solvers, both in terms of theoretical analysis and numerical implementation. In effect, the theoretical analysis of neural PDE solvers is often limited to density or universal approximation results aimed at showing the existence of a network of a requisite size achieving a certain error rate, without guaranteeing whether this network is computable in practice via stochastic gradient descent. On the contrary, as we will demonstrate in this work, kernel-based PDE solvers are provably convergent and amenable to rigorous numerical analysis based on a rich arsenal of functional analytic results from the kernel literature.\n1.4. Monte Carlo methods for pricing under rough volatility. The notion of rough volatility has recently disrupted stochastic modelling in quantitative finance [6], leaving no one indifferent in the field. In this setting, the instantaneous volatility of assets is modelled as a fractional Brownian motion with small Hurst parameter as in (1.1) or as a stochastic Volterra process. This feature has been shown empirically to be consistent with historical market trajectories and to capture important stylised facts of equity options in a parsimonous way. However, this new paradigm comes with an increased computational cost compared to classical techniques. With the notable exception of the rough Heston model, the absence of Markovianity of volatility prevents any pricing tools other than Monte Carlo simulations. However, the new promises-for estimation and calibration-of these rough volatility models have encouraged deep and fast innovations in numerical methods for pricing, in particular the now standard Hybrid scheme [8]. The complexity of a Monte Carlo scheme scales roughly as $\\mathcal{O}\\left(h^{-2} N d^{2}\\right)$, where $N$ is the number of sample paths, $h$ is the discretisation step and $d$ is the dimension of the process. This complexity can be further reduced to $\\mathcal{O}\\left((h \\log (h))^{-1} N d^{2}\\right)$ with hybrid schemes [8]. In this paper, we propose a simulation-free alternative to Monte Carlo pricing by constructing a kernel-based solver for the corresponding PPDE. The complexity of the resulting solver scales as $\\mathcal{O}\\left(h^{-2} \\hat{N} d^{2}\\right)$ where this time $\\hat{N}$ is the number of collocation paths. This cost can be further reduced to $\\mathcal{O}\\left(h^{-1} \\hat{N} d^{2}\\right)$ if the operations are carried out on a GPU, see [46] for details. As our numerical experiments demonstrate, a transparent comparison of complexities between our approach and Monte Carlo for pricing under rough volatility can only be achieved by obtaining error rates for the convergence of the approximations, which we intend to explore as future work.\n1.5. Contributions. In this paper we design a novel numerical solver for PPDEs based on signature kernel and accompanied by convergence guarantees. Our numerical scheme does not use any classical probabilistic representation or any other pre-existing solver; it constitutes a valid alternative to the ubiquitous Monte Carlo methods, in particular in the context of option pricing under rough volatility. The hurdles in the theoretical analysis are many-fold and their resolution contributes to the development of the theory of signature kernels. Challenges include the translation of the PPDE framework into one amenable for the use of signature kernels, the proof of existence and uniqueness of a global minimum in the optimisation problem as well as an analytic form of the solution by means of a representer theorem, and a consistency result based on the adaptation of convergence results from [12] to this infinite-dimensional setting via a compactness argument for functions on path spaces (Theorem 4.4 and Lemma 4.6). Finally, several numerical examples are presented in Section 5 and the code is open-sourced at https://github.com/crispitagorico/sigppde.\n1.6. Related literature. Several deep learning and signature-based approaches have been proposed in recent years. In [45], the authors use a probabilistic approach based on the martingale representation theorem to design an objective function for solving time-discretised PPDEs via a parametric model combining RNNs and signatures. The martingale representation theorem is also central in [23], where the authors make use of continuous-time RNNs developed in [40]. In both these papers, the signature has to be truncated for the implementation. The authors of [50] extend the deep Galerkin framework\n\nproposed by [51] using an LSTM architecture at each time step. The only paper applicable to Volterra processes [31] develops a backward discretisation algorithm using neural networks, drawing inspiration from the deep learning methodology introduced in [28]. We note that none of the aforementioned papers present theoretical results on the proposed numerical schemes and they all rely on the training of neural networks to approximate the solution. Aloof from the learning paradigm, convergence guarantees of monotone schemes can be found in [44] for the viscosity solutions to first generation PPDEs. The present paper is, to our knowledge, the first to propose a numerical scheme for second generation PPDEs with convergence guarantees.\n1.7. Outline. Section 2 describes the rigorous framework of PPDEs and how we intent to approximate them. This is followed in Section 3 by an introduction to signature kernels and the definitions we will need in the remainder of the paper. The main theoretical analysis and results are presented in Section 4 while the numerical experiments can be found in Section 5. We give an outlook on future research in Section 6. Finally, Appendix A gathers some technical proofs.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 2. Path-dependent PDEs \n\nAs the example from the introduction suggests, in situations where the underlying process is fractional, such as fractional Brownian motion, rough volatility models or stochastic Volterra processes, conditional expectations are functions of paths. In this section, we will present several instances where these functions are solutions to well-posed path-dependent PDEs. Beforehand, we must set the mathematical scene in a rigorous way. We introduce the notations and definitions needed to formalise equations such as (1.2). This framework enables to present central examples in subsection 2.3, then in subsection 2.4 we adapt this setup to the signature realm and finally outline the numerical method is subsection 2.5 .\n2.1. Path spaces. Let us set $d, e \\in \\mathbb{N}_{0}:=\\mathbb{N} \\cup\\{0\\}$, a finite time horizon $T>0$ and an interval $\\mathbb{T}:=$ $[0, T]$. Throughout this paper we deal with continuous paths $\\gamma: \\mathbb{T} \\rightarrow \\mathbb{R}^{e}$ for which we need to introduce a number of spaces and norms. When the interval $\\mathbb{T}$ and state space $\\mathbb{R}^{e}$ are clear from the context we will often omit them. The space of continuous functions from $\\mathbb{T}$ to $\\mathbb{R}^{e}$, denoted $\\mathcal{C}\\left(\\mathbb{T}, \\mathbb{R}^{e}\\right)$, is equipped with the supremum norm $\\|x\\|_{0, \\mathbb{T}}:=\\sup _{t \\in \\mathbb{T}}\\left|\\gamma_{t}\\right|$, unless stated otherwise. The notation $|\\cdot|$ refers to the Euclidean norm in one or multiple dimensions. The space of c\u00e0dl\u00e0g paths from $\\mathbb{T}$ to $\\mathbb{R}^{e}$ is denoted $D\\left(\\mathbb{T}, \\mathbb{R}^{e}\\right)$ while for all $t \\in \\mathbb{T}$ we will make use of the space of c\u00e0dl\u00e0g paths which are continuous on $[t, T]$, that is\n\n$$\n\\mathcal{C}_{t}:=\\left\\{\\gamma \\in D\\left(\\mathbb{T}, \\mathbb{R}^{e}\\right):\\left.\\gamma\\right|_{[t, T]} \\in \\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right)\\right\\}\n$$\n\nThis space allows to consider paths of the form $\\gamma+\\varepsilon \\eta \\mathbb{1}_{[t, T]}$ where $\\gamma, \\eta \\in \\mathcal{C}\\left(\\mathbb{T}, \\mathbb{R}^{e}\\right)=\\mathcal{C}_{0}$ and $\\varepsilon>0$ which in turn permits the definition of pathwise derivatives on this space. In parallel, for $p \\geq 1$, we also leverage the $p$-variation seminorm which is defined for $0 \\leq t<t^{\\prime} \\leq T$ and a path $\\gamma:\\left[t, t^{\\prime}\\right] \\rightarrow \\mathbb{R}^{e}$ as\n\n$$\n|\\gamma|_{p-\\operatorname{var}:\\left[t, t^{\\prime}\\right]}:=\\left(\\sup _{\\left(t_{i}\\right)_{i} \\in \\mathcal{D}} \\sum_{i}\\left|\\gamma_{t_{i+1}}-\\gamma_{t_{i}}\\right|^{p}\\right)^{1 / p}\n$$\n\nwhere $\\mathcal{D}$ is the set of all partitions of the form $t=t_{0}<t_{1}<\\cdots<t_{n}=t^{\\prime}$ for some $n \\in \\mathbb{N}$. The associated norm is $\\|\\gamma\\|_{p-\\operatorname{var}:\\left[t, t^{\\prime}\\right]}=\\left|\\gamma_{t}\\right|+\\left|\\gamma\\right|_{p-\\operatorname{var}:\\left[t, t^{\\prime}\\right]}$. This gives rise to the spaces $C^{p-\\operatorname{var}}\\left(\\left[t, t^{\\prime}\\right], \\mathbb{R}^{e}\\right)$ of continuous paths with one strictly monotone coordinate ${ }^{1}$ and finite $p$-variation norm. For each such\n\n[^0]\n[^0]:    ${ }^{1}$ This is a technical assumption needed to ensure injectivity of the signature and consequently the point separating property assumption needed to establish universality of the signature kernel by Stone-Weierstrass arguments. More generally one could work with equivalence classes obtained by quotienting $\\mathcal{C}$ by the so-called tree-like equivalence relation $x \\sim_{\\tau} y \\Longleftrightarrow S(x)=S(y)$; with the assumption that the paths have one strictly monotone coordinate, each equivalence class collapses to a singleton, which is the path itself. The monotone coordinate is usually taken to be time.\n\npath $\\gamma$ we denote by $\\gamma^{0}$ the strictly monotone coordinate and by $\\bar{\\gamma}$ the $\\mathbb{R}^{e-1}$-valued path without the strictly monotone coordinate.\n\nThe case $p=1$ is called the bounded variation space and also denoted $\\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], \\mathbb{R}^{e}\\right)$. Of particular interest to us are the spaces $C_{t}^{p-v a r}:=C^{p-v a r}\\left([t, T], \\mathbb{R}^{e}\\right)$ for some $t \\in \\mathbb{T}$. While the path space $\\mathcal{C}_{t}$ in the original theory of [54] allows for one jump, this would lead to severe complications for the application of signature kernels. We show in the next subsection how to adapt our framework to avoid the introduction of jumps. Furthermore, we want to equip the space $\\mathrm{BV}_{0}:=C_{0}^{1-v a r}$ with a topology that renders the signature map continuous (see Section 3 for the details). We could choose any $1 \\leq p$-variation topology for this task, however the necessity to characterise compact sets of $\\mathrm{BV}_{0}$ later on forces us to consider $p>1$. This task involves the introduction of H\u00f6lder spaces $C^{\\alpha}\\left(\\left[t, t^{\\prime}\\right], \\mathbb{R}^{e}\\right)$, with $\\alpha \\in(0,1)$ endowed with the $\\alpha$-H\u00f6lder norm\n\n$$\n\\|\\gamma\\|_{\\alpha ;\\left[t, t^{\\prime}\\right]}:=\\left|\\gamma_{t}\\right|+\\left|\\gamma\\right|_{\\alpha ;\\left[t, t^{\\prime}\\right]}:=\\left|\\gamma_{t}\\right|+\\sup _{0 \\leq s<t \\leq T} \\frac{\\left|\\gamma_{t}-\\gamma_{s}\\right|}{|t-s|^{\\alpha}}\n$$\n\nIf the interval $\\left[t, t^{\\prime}\\right]$ is clear from the context, for instance if $\\gamma \\in \\mathrm{BV}\\left([t, T], \\mathbb{R}^{e}\\right)$, we omit to write it as a subscript. Having in mind to characterise compact subsets of $C^{p-v a r}$, we follow [20] and intersect it with a H\u00f6lder space: let $C^{p-v a r, \\alpha}\\left(\\left[t, t^{\\prime}\\right], \\mathbb{R}^{e}\\right):=C^{p-v a r}\\left(\\left[t, t^{\\prime}\\right], \\mathbb{R}^{e}\\right) \\cap C^{\\alpha}\\left(\\left[t, t^{\\prime}\\right], \\mathbb{R}^{e}\\right)$ be the space of $\\alpha$-H\u00f6lder continuous paths with finite $p$-variation. The corresponding norm is $\\|\\gamma\\|_{p-v a r, \\alpha ;\\left[t, t^{\\prime}\\right]}:=\\left|\\gamma_{t}\\right|+\\left|\\gamma\\right|_{\\alpha ;\\left[t, t^{\\prime}\\right]}+$ $\\left|\\gamma\\right|_{p-v a r ;\\left[t, t^{\\prime}\\right]}$. It is then proven in [20, Theorema A.8] that the embedding $C^{p-v a r, \\alpha} \\hookrightarrow C^{p^{\\prime}-v a r, \\alpha^{\\prime}}$ is compact for all $1 \\leq p<p^{\\prime} \\leq \\infty$ and $0 \\leq \\alpha^{\\prime}<\\alpha<1$ such that $\\alpha p<1$ and $\\alpha^{\\prime} p^{\\prime}<1$. In particular, bounded subsets of $C^{1-v a r, \\alpha}$ are compact with respect to the $p$-variation topology for all $\\alpha \\in(0,1)$ and $p>1$, see also [25, Proposition 5.30].\n\nThe solutions to the path-dependent PDEs of interest are functionals of time, space and paths, hence we define suitable space and distance, borrowed from [54]:\n\n$$\n\\widetilde{\\Omega}=\\left\\{(t, x, \\gamma) \\in \\mathbb{T} \\times \\mathbb{R}^{d} \\times \\mathcal{C}_{t}\\right\\}, \\quad \\widetilde{\\boldsymbol{d}}((t, x, \\gamma),(\\bar{t}, \\bar{x}, \\bar{\\gamma})):=|t-\\bar{t}|+|x-\\bar{x}|+\\|\\gamma-\\bar{\\gamma}\\|_{0 ; \\mathbb{T}}\n$$\n\nNote that $\\gamma$ lives in $\\mathcal{C}_{t}$ which depends directly on $t$. We will denote by $\\Omega$ the state space with $\\mathrm{BV}_{0}$ instead of $\\mathcal{C}_{t}$ and such that paths are constant on $[0, t]$ (except for the strictly monotone coordinate), that is\n\n$$\n\\Omega:=\\left\\{(t, x, \\gamma) \\in \\mathbb{T} \\times \\mathbb{R}^{d} \\times \\mathrm{BV}_{0}: \\dot{\\gamma}_{s}=\\dot{\\gamma}_{0} \\text { for all } s \\leq t\\right\\}\n$$\n\nThe analogous distance is then\n\n$$\n\\boldsymbol{d}((t, x, \\gamma),(\\bar{t}, \\bar{x}, \\bar{\\gamma})):=|t-\\bar{t}|+|x-\\bar{x}|+\\|\\gamma-\\bar{\\gamma}\\|_{p-v a r ; \\mathbb{T}}\n$$\n\nNotice that if $B$ is a compact subset of $\\left(\\mathrm{BV}_{0},\\|\\cdot\\|_{p-v a r}\\right)$ then for any reals $a<b$, the set $\\mathbb{T} \\times[a, b]^{d} \\times B$ is a compact subset of $\\Omega$ with respect to $\\boldsymbol{d}$.\n2.2. Functions on continuous paths. This subsection presents the framework of [54]. Let $\\mathcal{C}(\\widetilde{\\Omega}):=$ $\\mathcal{C}(\\widetilde{\\Omega}, \\mathbb{R})$ denote the set of real-valued functions $u: \\widetilde{\\Omega} \\rightarrow \\mathbb{R}$ continuous under $\\widetilde{\\boldsymbol{d}}$. For $u \\in \\mathcal{C}(\\widetilde{\\Omega})$, define the right time derivative\n\n$$\n\\partial_{t} u(t, x, \\gamma):=\\lim _{\\varepsilon \\downarrow 0} \\frac{u(t+\\varepsilon, x, \\gamma)-u(t, x, \\gamma)}{\\varepsilon}\n$$\n\nfor all $t \\in[0, T)$, provided the limit exists. By convention we set $\\partial_{t} u(T, x, \\gamma)=0$. The derivative $\\partial_{x} u(t, x, \\gamma)$ is defined in the same way for all $(t, x, \\gamma) \\in \\widetilde{\\Omega}$. We also introduce the directional derivative $\\widetilde{\\partial}_{\\gamma}^{u} u(t, x, \\gamma)$ with respect to $\\gamma \\in \\mathcal{C}_{0}$, in the direction of $\\eta \\in \\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right)$ :\n\n$$\nu(t, x, \\gamma+\\varepsilon \\eta \\mathbb{1}_{[t, T]})-u(t, x, \\gamma)=\\widetilde{\\partial}_{\\gamma}^{u} u(t, x, \\gamma)+o\\left(\\left\\|\\eta \\mathbb{1}_{[t, T]}\\right\\|_{\\mathbb{T}}\\right)\n$$\n\nNotice that the path is only perturbed on the interval $[t, T]$, hence the necessity of a path space allowing for jumps. The operator $\\eta \\mapsto \\widetilde{\\partial}_{\\gamma}^{u} u$ is linear on $\\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right)$ and a Fr\u00e9chet derivative; moreover if it exists it is equal to the Gateaux derivative. We observe crucially that both the Fr\u00e9chet and Gateaux derivative are operators acting on functions from $\\widetilde{\\Omega}$ to $\\mathbb{R}$; in particular the time dependence is essential.\n\nWe define higher derivatives as linear operators on $\\left(\\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right)\\right)^{n}$, in particular the second derivative is a bilinear operator on $\\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right) \\times \\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right)$ and we denote it $\\widetilde{\\partial}_{i}^{\\eta \\bar{\\eta}} u(t, x, \\omega)$ for $\\eta, \\bar{\\eta} \\in \\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right)$.\n\nLet $n, n_{1}, n_{2}, n_{3} \\in \\mathbb{N}_{0}$. We denote the higher derivatives $\\partial_{t}^{n_{1}}, \\partial_{x}^{n_{2}}$ and $\\widetilde{\\partial}_{i}^{q}$, for $\\boldsymbol{\\eta}=\\left(\\eta_{1}, \\cdots, \\eta_{n_{3}}\\right) \\in$ $\\left(\\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right)\\right)^{n_{3}}$; the order of the directional derivative is implied by the dimension of $\\boldsymbol{\\eta}$. We say that a function $u \\in \\mathcal{C}(\\widetilde{\\Omega})$ belongs to $\\mathcal{C}^{\\mathcal{N}}(\\widetilde{\\Omega})$ if the linear operator $\\partial_{t}^{n_{1}} \\partial_{x}^{n_{2}} \\widetilde{\\partial}_{i}^{q}$ exists and is continuous with respect to $\\widetilde{\\boldsymbol{d}}$ for all $\\left(n_{1}, n_{2}, n_{3}\\right) \\in \\mathcal{N} \\subset \\mathbb{N}_{0}^{3}$. The examples of interest arise from Kolmogorov equations, hence only the first order time derivative appears and is never combined with the spatial and pathwise derivatives, while the latter are present up to order two. The corresponding subset is $\\mathcal{N}=\\left\\{\\left(n_{1}, n_{2}, n_{3}\\right) \\in \\mathbb{N}_{0}^{3}: n_{1} \\leq 1, n_{2}=n_{3}=0\\right.$, or $\\left.n_{1}=0, n_{2}+n_{3} \\leq 2\\right\\}$.\n\nWe intend to deal with fractional processes with singular kernels of the form $K(s, t)=(s-t)^{H-\\frac{1}{2}} \\mathbb{1}_{s>t}$ with $H \\in\\left(0, \\frac{1}{2}\\right)$. Taken as a function of $s \\in[t, T]$, these kernels turn out to be the direction of the pathwise derivative but lie outside of $\\mathcal{C}(\\mathbb{T}, \\mathbb{R})$, hence an approximation procedure is required. For $0 \\leq$ $t<s \\leq T$, we define the path $K^{t}(s):=K(s, t)$ and its truncated version $K^{\\delta, t}(s):=K(s \\wedge(t+\\delta), t)$, for $\\delta>0$. The directional derivative for such a singular path is then defined, if it exists, as\n\n$$\n\\widetilde{\\partial}_{\\gamma}^{\\mathcal{K}^{t}} u(t, x, \\gamma):=\\lim _{\\delta \\downarrow 0} \\widetilde{\\partial}_{\\gamma}^{\\mathcal{K}^{\\delta, t}} u(t, x, \\gamma)\n$$\n\nand similarly for the higher derivatives. The generalisation of the space $\\mathcal{C}^{\\mathcal{N}}$ to this type of derivative is more involved and requires regularity estimates tailored to the speed of explosion of the kernel. For precise definitions, we will point towards the appropriate references in the examples below. We are now ready to state the equations we wish to solve.\n2.3. The target PDEs. This paper's focus is on approximating functionals $u$ which only depend on the path over $[t, T]$, that is there exist $\\tilde{u}$ such that $u(t, x, \\gamma)=\\tilde{u}\\left(t,\\left.x, \\gamma\\right|_{[t, T]}\\right)$. This is the case of all the examples presented in this section. These functionals are solutions to linear PPDEs taking the general form for all $(t, x, \\gamma) \\in \\widetilde{\\Omega}$\n\n$$\n\\left\\{\\begin{array}{l}\n\\widetilde{\\mathcal{L}}_{t} u(t, x, \\gamma)=g(t, x, \\gamma) \\\\\nu(T, x, \\gamma)=\\phi(T, x, \\gamma)\n\\end{array}\\right.\n$$\n\nwhere $f, \\phi: \\widetilde{\\Omega} \\rightarrow \\mathbb{R}$ are measurable functions and the linear operator $\\widetilde{\\mathcal{L}}_{t}$ is a combination of derivatives of all arguments. More precisely, there exist $\\mathcal{N} \\subset \\mathbb{N}_{0}^{3}, a_{n} \\in \\mathbb{R}$ for all $n \\in \\mathcal{N}$ and $\\boldsymbol{\\eta} \\in\\left(\\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right)\\right)^{n_{3}}$ such that\n\n$$\n\\widetilde{\\mathcal{L}}_{t}:=\\sum_{n=\\left(n_{1}, n_{2}, n_{3}\\right) \\in \\mathcal{N}} a_{n} \\partial_{t}^{n_{1}} \\partial_{x}^{n_{2}} \\widetilde{\\partial}_{x}^{\\boldsymbol{\\eta}}\n$$\n\nwhere the sum is taken over multi-indices $n \\in \\mathcal{N}$. We emphasise on the $t$ dependence which is important because the directional derivative is a linear operator on $\\mathcal{C}\\left([t, T], \\mathbb{R}^{e}\\right)$. We need this notation instead of the more standard multi-index one in order to emphasise on the order of each derivative (in our applications $n_{1} \\leq 1, n_{2}+n_{3} \\leq 2$ ) and also on the direction $\\boldsymbol{\\eta}$.\n\nMore precisely, this paper will focus on three examples of parabolic second order PDEs which arise from the Feynman-Kac formula. The solution to these PPDEs thus have a probabilistic representation, a crucial advantage to assess the accuracy of our method. However we emphasise that the numerical scheme does not use this a priori knowledge as in supervised learning tasks. Much like deep Galerkin or PINN algorithms [51, 43], our method can be deployed to solve PPDEs for which no numerical method is known.\n\nOur examples take as underlying a Gaussian Volterra process $\\widetilde{W}$ defined as $\\widetilde{W}_{t}:=\\int_{0}^{t} K(t, r) \\mathrm{d} W_{r}$ for some kernel $K \\in L^{2}\\left(\\mathbb{T}^{2}, \\mathbb{R}\\right)$. The key object, already introduced in the introduction, is the double index process, for $0 \\leq t \\leq s, \\Theta_{s}^{t}=\\mathbb{E}_{s}\\left[\\widetilde{W}_{t}\\right]=\\int_{0}^{t} K(s, r) \\mathrm{d} W_{r}$. In alignment with the prolific rough volatility literature, we will mostly focus on the specification $K(t, r)=(t-r)^{H-1 / 2} \\mathbb{1}_{r \\in[0, t)}$ for $H \\in(0,1)$ but\n\nother choices are possible, see for instance [10, Assummption 2.6]. The current setup (introduced in [54]), as well as our numerical method, allow for path-dependent payoffs. Since $\\widehat{W}$ is a fractional process, the path-dependence is inherent and transfers to the PDE even for state-dependent terminal conditions (payoffs). For clarity of exposition and because well-posedness results are scarcer in the path-dependent case, we will however stick to state-dependent payoffs in our numerical experiments.\n\nIn each of the three subsequent subsections, we present (i) the model (ii) the value function $u$ and (iii) the PPDE, in that order. We point to the suitable references for well-posedness and for more details. We emphasise that all three examples are particular cases of [10, Proposition 2.14]; this result ensures, under general assumptions, existence and uniqueness of a classical solution to the PPDE associated to a Volterra process.\n2.3.1. Fractional Brownian motion. In this first toy example we consider test functions of $\\left(\\widehat{W}_{s}\\right)_{s \\in[t, T]}$ which corresponds to the state space $\\widehat{\\Omega}$ with $d=0$ and $e=1$. Let us define the function\n\n$$\n\\begin{aligned}\nu(t, \\gamma) & :=\\mathbb{E}\\left[\\phi\\left(\\widehat{W}_{T}\\right)+\\int_{t}^{T} f_{s}\\left(\\widehat{W}_{s}\\right) \\mathrm{d} s \\mid \\Theta^{t}=\\gamma\\right] \\\\\n& =\\mathbb{E}\\left[\\phi\\left(\\gamma_{T}+\\int_{t}^{T} K(T, s) \\mathrm{d} W_{s}\\right)+\\int_{t}^{T} f_{s}\\left(\\gamma_{s}+\\int_{t}^{s} K(t, r) \\mathrm{d} W_{r}\\right) \\mathrm{d} s\\right]\n\\end{aligned}\n$$\n\nIn particular, we observe that $u\\left(t, \\Theta^{t}\\right)=\\mathbb{E}\\left[\\phi\\left(\\widehat{W}_{T}\\right)+\\int_{t}^{T} f_{s}\\left(\\widehat{W}_{s}\\right) \\mathrm{d} s \\mid \\mathcal{F}_{t}\\right]$. Note that the path-dependence of the conditional expectation is contained in the path $\\Theta^{t}$ because the payoff only acts on $\\widehat{W}_{s}$ for $s \\in[t, T]$, whereas a payoff depending on $\\widehat{W}_{s}$ for all $s \\in[0, T]$ would require to consider the concatenated path $\\widehat{W}_{s} \\mathbb{1}_{s<t}+\\Theta_{s}^{t} \\mathbb{1}_{s \\geq t}$.\n\nBased on [54, Theorem 4.1], under some standard regularity assumptions on $\\phi, \\psi$ and $K, u$ is a solution to the path-dependent heat equation for all $(t, \\gamma) \\in \\widehat{\\Omega}$\n\n$$\n\\left\\{\\begin{array}{l}\n\\partial_{t} u(t, \\gamma)+\\frac{1}{2} \\widetilde{\\partial}_{\\gamma}^{K^{t} K^{t}} u(t, \\gamma)+\\psi(t, \\gamma)=0 \\\\\nu(T, \\gamma)=\\phi\\left(\\gamma_{T}\\right)\n\\end{array}\\right.\n$$\n\nIt can also be proved [10] that this solution is unique in the space where the functional It\u00f4 formula [54, Theorem 3.17] is applicable.\n2.3.2. VIX options under rough Bergomi model. Options on VIX and realised variance form an important class of derivatives, which can be seen as options with path-dependent payoffs. We define $\\operatorname{VIX}_{T}^{2}=\\frac{1}{\\Delta} \\int_{T}^{T+\\Delta} \\mathbb{E}_{T}\\left[\\psi_{s}\\left(\\widehat{W}_{s}\\right) \\mid \\mathrm{d} s\\right.$ where $\\Delta$ is a positive constant corresponding to 30 days, and $\\widehat{W}$ is a Gaussian Volterra process in $\\mathbb{R}^{e}$ for any $e \\in \\mathbb{N}$. It is shown in [42] that an option on the VIX (future) has the representation\n\n$$\n\\mathbb{E}\\left[\\phi\\left(\\operatorname{VIX}_{T}^{2}\\right) \\mid \\mathcal{F}_{t}\\right]=\\mathbb{E}\\left[\\phi \\circ \\mathfrak{F}\\left(\\left\\{\\Theta_{s}^{t}+J_{s}^{t, T}: s \\in[T, T+\\Delta]\\right\\}\\right) \\mid \\Theta^{t}\\right]\n$$\n\nwhere $J_{s}^{t, T}=\\Theta_{s}^{T}-\\Theta_{s}^{t}$ is independent of $\\mathcal{F}_{t}$ and $\\mathfrak{F}(\\gamma):=\\frac{1}{\\Delta} \\int_{T}^{T+\\Delta} \\mathbb{E}\\left[\\psi_{s}\\left(\\gamma_{s}+I_{s}^{T}\\right)\\right] \\mathrm{d} s$ where $I_{s}^{T} \\sim$ $\\mathcal{N}\\left(\\mathbf{0}_{d}, \\int_{0}^{s} K(T, r) K(T, r)^{\\top} \\mathrm{d} r\\right)$. Under natural assumptions on $\\phi, \\psi$ and $K$, the map $u(t, \\gamma):=\\mathbb{E}[\\phi \\circ$ $\\mathfrak{F}\\left(\\left\\{\\gamma_{s}+J_{s}^{t, T}: s \\in[T, T+\\Delta]\\right\\}\\right)]$ is the unique solution to the PPDE for all $(t, \\gamma) \\in \\widehat{\\Omega}$\n\n$$\n\\left\\{\\begin{array}{l}\n\\partial_{t} u(t, \\gamma)+\\frac{1}{2} \\widetilde{\\partial}_{\\gamma}^{K^{t} K^{t}} u(t, \\gamma)=0 \\\\\nu(T, \\gamma)=\\phi \\circ \\mathfrak{F}(\\gamma)\n\\end{array}\\right.\n$$\n\n2.3.3. Options under rough Bergomi model. In this last example, which was already presented in the introduction, the underlying is the (log-)asset price in a rough Bergomi-type model. The value function\n\nin the rough Bergomi model is indexed on the state space $\\widetilde{\\Omega}$ with $d=1$. As we have already seen,\n\n$$\nu(t, x, \\gamma):=\\mathbb{E}\\left[\\phi\\left(X_{T}\\right) \\mid X_{t}=x, \\Theta^{t}=\\gamma\\right]\n$$\n\nwhich entails $u\\left(t, X_{t}, \\Theta^{t}\\right)=\\mathbb{E}\\left[\\phi\\left(X_{T}\\right) \\mid \\mathcal{F}_{t}\\right]$. Based on [10, Theorem 2.25], natural assumptions on $\\phi, \\psi$ and $K$ yield the well-posedness of the PPDE for all $(t, x, \\gamma) \\in \\widetilde{\\Omega}$\n\n$$\n\\left\\{\\begin{array}{l}\n\\partial_{t} u+\\frac{1}{2} \\psi_{t}\\left(\\gamma_{t}\\right)\\left(\\partial_{x}^{2} u-\\partial_{x} u\\right)+\\frac{1}{2} \\widetilde{\\partial}_{\\gamma}^{K^{\\dagger} K^{\\dagger}} u+\\rho \\sqrt{\\psi_{t}\\left(\\gamma_{t}\\right)} \\widetilde{\\partial}_{\\gamma}^{K^{\\dagger}}\\left(\\partial_{x} u\\right)=0 \\\\\nu(T, x, \\gamma)=\\phi(x)\n\\end{array}\\right.\n$$\n\nNotice the boundary condition only applies to $x$ and is not path-dependent.\nRemark 2.1. In the rough Bergomi model, the paths $\\Theta$ have a financial interpretation as they are linked to the forward variance curves $\\xi$, a quantity observed on the markets. We refer to [42, Remark $4.7(5)]$ for more details.\n2.3.4. An important remark on the singularity of the kernel. We emphasise that the PPDEs presented in these examples fit in (2.3) albeit with a non-continuous direction $K^{t}$. The pathwise derivative is thus defined as the limit of the regularised derivative with the approximation $K^{t, \\delta}$ as in (2.2). One can show thanks to the probabilistic representations that the solution to the regularised PPDE converges to the solution of the singular one as $\\delta$ goes to zero. Our numerical scheme approximates the PPDE (2.3) with a regularised kernel, for a fixed $\\delta>0$.\n2.4. Functions on bounded variation paths. Making the framework of subsection 2.2 amenable to the application of signature kernels requires some carefulness. Firstly, the latter are defined on bounded variation paths, a subset of the continuous paths considered in the previous subsection. In this subsection, functions are thus defined over $\\Omega$ instead of $\\widetilde{\\Omega}$.\n\nFurther, we make the observation that all three examples presented above are functionals $(t, x, \\gamma) \\mapsto$ $u(t, x, \\gamma)$ that, for all $t \\in \\mathbb{T}$, only act on $\\left(\\gamma_{x}\\right)_{s \\in[t, T]}$, the path after time $t$. This crucial property is the reason why we want to consider paths that are constant on $[0, t]$, so that no unnecessary information is included. We denote by $\\mathcal{C}(\\Omega)$ the set of continuous functions $u: \\Omega \\rightarrow \\mathbb{R}$ with respect to $\\boldsymbol{d}$; for those there exists a family $\\left(u_{t, x}\\right)_{t \\in \\mathbb{T}, x \\in \\mathbb{R}^{\\delta}}$ of continuous maps from $\\mathrm{BV}_{t}$ to $\\mathbb{R}$ such that $u(t, x, \\gamma)=$ $u_{t, x}\\left(\\left.\\gamma^{0}\\right|_{[t, T]}, \\gamma\\right|_{[t, T]}$ ). Note that this distance looks into the path on $[0, t]$, indeed without this information one would lose the positivity for two paths distinct on $[0, t]$, while considering only continuity of $u_{t, x}$ leads to issues in defining compact sets of $\\Omega$. To be consistent with the derivative of the signature kernel introduced in [47], we choose a weaker form of pathwise derivative than the Fr\u00e9chet one: the Gateaux derivative (it will appear later that signatures are continuous with respect to the $\\mathcal{C}_{0}$ norm and therefore both derivatives agree). This is where the restriction to $[t, T]$ becomes useful and allows us to avoid jumps. Recall that the Gateaux derivative perturbs the path on the whole interval where it is defined\n\n$$\n\\partial_{\\gamma}^{n} u(t, x, \\gamma):=\\lim _{\\varepsilon \\downarrow 0} \\frac{u(t, x, \\gamma+\\varepsilon \\eta)-u(t, x, \\gamma)}{\\varepsilon}\n$$\n\nWe instead define the pathwise derivative of $u \\in \\mathcal{C}(\\Omega)$ in the direction $\\eta \\in \\mathrm{BV}_{0}$ such that only the path $\\hat{\\gamma}$ is perturbed and solely on $[0, t]$ :\n\n$$\n\\widetilde{\\partial}_{\\gamma}^{n} u(t, x, \\gamma):=\\partial_{\\gamma}^{\\left(\\left.0, \\hat{\\eta}\\right|_{[t, T]}\\right)} u_{t, x}\\left(\\left.\\gamma^{0}\\right|_{[t, T]}, \\hat{\\gamma}\\right|_{[t, T]}\\right)=\\lim _{\\varepsilon \\rightarrow 0} \\frac{u_{t, x}\\left(\\left.\\gamma^{0}\\right|_{[t, T]},(\\hat{\\gamma}+\\varepsilon \\eta)\\right|_{[t, T]}\\right)-u_{t, x}\\left(\\left.\\gamma^{0}\\right|_{[t, T]}, \\hat{\\gamma}\\right|_{[t, T]}\\right)}{\\varepsilon}\n$$\n\nNote that this amounts to three different notions of pathwise derivatives $\\widetilde{\\partial}, \\widetilde{\\partial}, \\partial$. Similarly to the former setup in subsection 2.2 , for $n, n_{1}, n_{2}, n_{3} \\in \\mathbb{N}_{0}$, we can define the higher derivatives $\\partial_{t}^{n_{1}}, \\partial_{x}^{n_{2}}$ and $\\widetilde{\\partial}_{\\gamma}^{n}$, for $\\boldsymbol{\\eta}=\\left(\\eta_{1}, \\cdots, \\eta_{n_{3}}\\right) \\in\\left(\\mathrm{BV}_{0}\\right)^{n_{3}}$. We denote $\\mathcal{C}^{\\mathcal{N}}(\\Omega)$ the space of functions admitting continuous such\n\npathwise derivatives of order $\\mathcal{N} \\subset \\mathbb{N}_{0}^{3}$. We can now define PPDEs on $\\Omega$ similar to (2.3)\n\n$$\n\\left\\{\\begin{array}{l}\n\\mathcal{L}_{t} u(t, x, \\gamma)=g(t, x, \\gamma) \\\\\nu(T, x, \\gamma)=\\phi(T, x, \\gamma)\n\\end{array}\\right.\n$$\n\nwhere $f, \\phi: \\Omega \\rightarrow \\mathbb{R}$ are measurable functions and the linear operator $\\mathcal{L}_{t}$ is a combination of derivatives of all arguments, analogue of $\\widetilde{\\mathcal{L}}_{t}$ for functions independent of $\\gamma_{[0, t]}$, albeit with a pathwise derivative of Gateaux type instead of Fr\u00e9chet. More precisely, there exist $\\mathcal{N} \\subset \\mathbb{N}_{0}^{3}, a_{n} \\in \\mathbb{R}$ for all $n \\in \\mathcal{N}$ and $\\boldsymbol{\\eta} \\in\\left(\\mathrm{BV}_{0}\\right)^{n_{3}}$ such that\n\n$$\n\\mathcal{L}_{t}:=\\sum_{n=\\left(n_{1}, n_{2}, n_{3}\\right) \\in \\mathcal{N}} a_{n} \\partial_{t}^{n_{1}} \\partial_{x}^{n_{2}} \\widetilde{\\partial}_{\\gamma}^{n}\n$$\n\nwhere the sum is taken over multi-indices $n \\in \\mathcal{N}$.\n2.5. A kernel method for PPDEs. We saw that we cannot solve a PPDE of the type $\\widetilde{\\mathcal{L}}_{t} u=f$ on $\\widetilde{\\Omega}$ directly with signature kernel methods, hence why we defined an analogous weaker PPDE $\\mathcal{L}_{t} u=f$ on $\\Omega \\subset \\widetilde{\\Omega}$ which solution can be suitably represented with signature kernels. Functions admitting Fr\u00e9chet derivatives also admit Gateaux derivatives and they are equal, therefore solutions to the former PPDE are also solutions to the latter. This will allow us in Theorem 4.4 to bridge the gap between the equation we solve and the equation we target. Now that we have this picture in mind, we can present the gist of our method, inspired by [12] and extended to the path-dependent case.\n\nWe consider a kernel $\\kappa$, indexed on a compact subset $\\mathcal{X}$ of $\\Omega$, as product of an RBF kernel and a signature kernel indexed on $\\mathbb{R}^{d+1}$ and $\\mathrm{BV}_{0}$ respectively. We call $\\mathcal{H}$ its associated RKHS. We pick $m+n$ collocation points $\\omega^{1}, \\ldots, \\omega^{m+n} \\in \\mathcal{X}$ of the form $\\omega^{i}=\\left(t^{i}, x^{i}, \\gamma^{i}\\right)$ and order them in such a way that $0 \\leq t^{i}<T$ for $i=1, \\ldots, m$ and $t^{i}=T$ for $i=m+1, \\ldots, m+n$. The former belong to the interior of the domain while the latter correspond to the boundary (terminal) condition. The main idea consists in approximating the solution of the PPDE (2.12) with a minimiser of the following optimal recovery problem\n\n$$\n\\min _{u \\in \\mathcal{H}} \\frac{1}{2}\\|u\\|_{\\mathcal{H}}^{2} \\quad \\text { such that } \\quad \\begin{cases}\\mathcal{L}_{t} u\\left(\\omega^{i}\\right)=g\\left(\\omega^{i}\\right) & i=1, \\ldots, m \\\\ u\\left(\\omega^{i}\\right)=\\phi\\left(\\omega^{i}\\right) & i=m+1, \\ldots, m+n\\end{cases}\n$$\n\nIn other words, we search for the function with minimal RKHS norm that satisfies the PDE constraints at all the collocation points. This problem formulation raises a number of questions, of both theoretical and practical importance:\n(1) Does the domain of $\\mathcal{L}_{t}$ contain $\\mathcal{H}$ ?\n(2) Does the minimisation problem (2.14) have a unique solution $u_{m, n}$ ?\n(3) How does one solve this optimisation over an infinite-dimensional space ?\n(4) Does the PPDE (2.3) have a unique classical solution $u^{\\star}$ ?\n(5) Does $u_{m, n}$ converge to $u^{\\star}$ as $m, n \\rightarrow \\infty$ ?\n\nAfter designing our kernel and RKHS in Section 3, we answer positively to all these questions under appropriate conditions. In order to successively execute limit arguments, we exploit that $\\mathcal{X}$ is a compact set of $\\Omega$, which we define explicitly under the topology induced by $\\boldsymbol{d}$. Exploiting the robust signatures introduced in [14] may allow to lift this assumption; we will investigate this idea in the future. The interested reader may find a summary of the answers as follows.\n(1) Yes, Proposition 4.1 proves this assertion.\n(2) Yes, as this is a type of optimal recovery problem. See Section 4.1.\n(3) The optimal recovery problem reduces to a finite dimensional quadratic optimisation problem with linear constraints. The latter can be solved via gradient descent, aloof from sophisticated learning algorithms. See Section 4.1.\n\n(4) As pointed out in Section 2.3, existence and uniqueness of this PPDE holds under some conditions, which are thoroughly checked in the given references for the examples of interest.\n(5) Theorem 4.4 ensures convergence towards the true solution under two different sets of assumptions, both requiring that $u^{*}$ is the unique solution of the $\\operatorname{PPDE}(2.3)$ and that the collocation points form a countable dense set of $\\mathcal{H}$.\nFinally, let us summarise the several levels of approximation. We numerically solve the optimisation problem (2.14), which is a discretisation of the PPDE (2.12) over $\\Omega$. Under the right set of assumptions, the sequence of solutions to the optimisation problem converges the solution to the PPDE (2.3) over $\\widetilde{\\Omega}$. Finally, the latter is an approximation of the PPDE with a singular direction $\\boldsymbol{\\eta}$. While our setup is designed to match PPDEs of second generation (arising from Volterra processes), it could be adapted to the first generation of Dupire (corresponding to path-dependent payoffs).", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 3. SignATURE KERNELS \n\nOur numerical solver is based on so-called signature kernels, a special class of kernels indexed on $\\mathrm{BV}_{0}$. In this section we enumerate the main properties of such signature kernels and highlight important aspects related to their numerical evaluation. We begin by recalling definition and properties of classical kernels.\n3.1. Classical kernels. A scalar-valued kernel on a set $\\Omega$ is a symmetric function of the form $\\kappa$ : $\\Omega \\times \\Omega \\rightarrow \\mathbb{R}$. Such kernel is said to be positive semidefinite if for any $n \\in \\mathbb{N}$ and points $\\omega^{1}, \\ldots, \\omega^{n} \\in \\Omega$, the Gram matrix $\\mathcal{K}:=\\left(\\kappa\\left(\\omega^{i}, \\omega^{j}\\right)\\right)_{i, j}$ is positive semidefinite, i.e. if for any $\\alpha_{1}, \\ldots, \\alpha_{n} \\in \\mathbb{R}$\n\n$$\n\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} \\kappa\\left(\\omega^{i}, \\omega^{j}\\right) \\geq 0\n$$\n\nRecall that a Hilbert space $\\mathcal{H}$ of functions defined on a set $\\Omega$ is a reproducing kernel Hilbert space (RKHS) over $\\Omega$ if, for each $\\omega \\in \\Omega$, the point evaluation functional at $\\omega, f \\mapsto f(\\omega)$, is a continuous linear functional, i.e. there exists a constant $C_{\\omega} \\geq 0$ such that\n\n$$\n|f(\\omega)| \\leq C_{\\omega}\\|f\\|_{\\mathcal{H}}, \\quad \\text { for all } f \\in \\mathcal{H}\n$$\n\nThe classical Moore-Aronszajn Theorem [2] ensures that if $\\kappa$ is a positive semidefinite kernel, then there exists a unique RKHS $\\mathcal{H}$ such that $\\kappa$ has the following reproducing property\n\n$$\n\\langle\\kappa(\\omega, \\cdot), f\\rangle_{\\mathcal{H}}=f(\\omega), \\quad \\text { for all } f \\in \\mathcal{H} \\text { and } \\omega \\in \\Omega\n$$\n\nWhen dealing with questions related to function approximation, one is often interested in understanding how well elements of an RKHS can approximate a function in a given class. One important such class is the family of continuous functions, leading to the notion of cc-universality [53]. If the set $\\Omega$ is equipped with an arbitrary topology and $\\kappa: \\Omega \\times \\Omega \\rightarrow \\mathbb{R}$ is a continuous, symmetric, positive semidefinite kernel on $\\Omega$ with RKHS $\\mathcal{H}$, then we say that $\\kappa$ is cc-universal on $\\Omega$ if, for every compact subset $\\mathcal{K} \\subset \\Omega, \\mathcal{H}$ is dense in $\\mathcal{C}(\\mathcal{K})$ in the topology of uniform convergence.\n\nKernels on Euclidean spaces such as linear, polynomial, Gaussian and Mat\u00e9rn kernels have been extensively studied in prior literature. Arguably, the most popular choice (which is also the one we make in our experiments) of cc-universal kernel on $\\mathbb{R}_{+} \\times \\mathbb{R}^{d}$ is the radial basis function (RBF) kernels $g_{\\sigma}: \\mathbb{R}_{+} \\times \\mathbb{R}^{d} \\times \\mathbb{R}_{+} \\times \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ defined for $(s, x),(t, y) \\in \\mathbb{R}_{+} \\times \\mathbb{R}^{d}$ as\n\n$$\ng_{\\sigma}((s, x),(t, y))=\\exp \\left(-\\frac{\\|(s, x)-(t, y)\\|^{2}}{2 \\sigma^{2}}\\right), \\quad \\sigma \\in \\mathbb{R}\n$$\n\nTo handle solutions of PPDEs we need to introduce kernels indexed on $\\mathrm{BV}_{0}$. Signature kernels $[33,46]$ provide a natural class of cc-universal of kernels on $\\mathrm{BV}_{0}$ as we shall discuss next.\n\n3.2. Signature kernels. The definition of signature kernel requires an initial algebraic setup. Denote by $\\otimes$ the standard tensor product of vector spaces. Let $V$ be a Hilbert space with inner product $\\langle\\cdot, \\cdot\\rangle_{V}$. In this section, we adopt a more general notation, although we will specifically consider $V$ as the vector space $\\mathbb{R}^{e}$ in the sequel of the paper. For any $n \\in \\mathbb{N}$, the canonical Hilbert-Schmidt inner product $\\langle\\cdot, \\cdot\\rangle_{V^{\\otimes n}}$ on $V^{\\otimes n}$ is defined as\n\n$$\n\\langle a, b\\rangle_{V^{\\otimes n}}=\\prod_{i=1}^{n}\\left\\langle a_{i}, b_{i}\\right\\rangle_{V}\n$$\n\nwhere $a=\\left(a_{1}, \\ldots, a_{n}\\right)$ and $b=\\left(b_{1}, \\ldots, b_{n}\\right)$ are tensors in $V^{\\otimes n}$. Let $T((V))=\\prod_{n=1}^{\\infty} V^{\\otimes n}$ the vector space of formal tensor series and $T(V) \\subset T((V))$ the space of formal tensor polynomials defined as\n\n$$\nT(V)=\\{a \\in T((V)) \\mid a \\text { has a finite number of non-zero entries }\\}\n$$\n\nThere are many ways of extending by linearity the inner product in (3.3) to an inner product on $T(V)$. The simplest way to achieve this is by defining the inner product as follows\n\n$$\n\\langle a, b\\rangle_{T((V))}=\\sum_{n=0}^{\\infty}\\left\\langle a_{n}, b_{n}\\right\\rangle_{V^{\\otimes n}}\n$$\n\nfor any $a=\\left(a_{0}, a_{1}, \\ldots\\right), b=\\left(b_{0}, b_{1} \\ldots\\right)$ in $T(V)$.\nRemark 3.1. A more general class of inner products was introduced in [11] by means of a weight function $\\phi: \\mathbb{N}_{0} \\rightarrow \\mathbb{R}_{+}$in the following way\n\n$$\n\\langle a, b\\rangle_{T((V))}=\\sum_{n=0}^{\\infty} \\phi(n)\\left\\langle a_{n}, b_{n}\\right\\rangle_{V^{\\otimes n}}\n$$\n\nWe denote by $\\overline{T(V)}$ the Hilbert space obtained by completing $T(V)$ with respect to $\\langle\\cdot, \\cdot\\rangle_{T((V))}$. Supposing that $V$ is $d$-dimensional, for some $d>1$, it will be convenient to introduce the set of words $\\mathbb{W}$ in the non-commuting letters $\\{1, \\ldots, d\\}$, i.e. ordered $m$-tuples $w=\\left(i_{1}, i_{2}, \\cdots, i_{m}\\right) \\in\\{1, \\ldots, d\\}^{m}$ with $|w|:=m \\in \\mathbb{N}$. The principal ingredient needed to define the signature kernel is a classical transform in stochastic analysis known as the signature. Next we recall its definition. Let $\\mathrm{BV}(\\mathbb{T}, V)$ be defined as in Section 2.1 where the norm in the definition of $p$-variation is the norm in $V$. For any closed interval $\\left[t, t^{\\prime}\\right] \\subset \\mathbb{T}$ and any path $\\gamma \\in \\mathrm{BV}(\\mathbb{T}, V)$, the signature of $\\gamma$ over $\\left[t, t^{\\prime}\\right]$ is the following infinite collection\n\n$$\nS(\\gamma)_{t, t^{\\prime}}=\\left(S(\\gamma)_{t, t^{\\prime}}^{(w)}\\right)_{w \\in \\mathbb{W}} \\in \\overline{T(V)} \\cong l^{2}(\\mathbb{W})\n$$\n\nof Riemann-Stieltjes iterated integrals\n\n$$\nS(\\gamma)_{t, t^{\\prime}}^{(w)}=\\int_{t<t_{1}<\\ldots<t_{m}<t^{\\prime}} d \\gamma_{t_{1}}^{(i_{1})} \\ldots d \\gamma_{t_{m}}^{(i_{m})}, \\quad \\text { for any } w=\\left(i_{1}, \\ldots, i_{m}\\right) \\in \\mathbb{W}\n$$\n\nWhen it is clear from the context, we will suppress the dependence on the interval and instead denote the signature of $\\gamma$ by $S(\\gamma)$. We refer the interested reader to [32, 24] for an account and examples on the use of signature methods in machine learning.\n\nThe signature of a BV path $\\gamma$ can be equivalently defined as the solution of a $\\overline{T(V)}$-valued differential equations controlled by $\\gamma$, as stated in the following lemma.\n\nLemma 3.2. Let $\\gamma \\in \\mathrm{BV}(\\mathbb{T}, V)$. Then, the linear controlled differential equation\n\n$$\nd Y_{s}=Y_{s} \\cdot d \\gamma_{s} \\text { on }\\left[t, t^{\\prime}\\right], \\text { started at } A \\in \\overline{T(V)}\n$$\n\nadmits $Y_{s}=A \\cdot S(\\gamma)_{[t, s]}$ as unique solution, where the product $\\cdot$ on $\\overline{T(V)}$ is defined for any $v=\\left(v_{0}, v_{1}, \\ldots\\right)$ and $w=\\left(w_{0}, w_{1}, \\ldots\\right)$ in $\\overline{T(V)}$ as the element $v \\cdot w=\\left(z_{0}, z_{1}, \\ldots\\right)$ in $\\overline{T(V)}$ such that\n\n$$\nz_{k}=\\sum_{j=0}^{k} v_{j} \\otimes w_{k-j} \\in V^{\\otimes k}, \\quad \\forall k \\in \\mathbb{N} \\cup\\{0\\}\n$$\n\nand where $d \\gamma_{t}$ is identified with $\\left(0, d \\gamma_{t}, 0, \\ldots\\right) \\in \\overline{T(V)}$. In particular, the signature $S(\\gamma)_{t, t^{\\prime}}$ is the unique solution to (3.7) with initial condition $A=\\mathbf{1}:=(1,0,0, \\ldots) \\in \\overline{T(V)}$.\n\nProof. For any $a \\in V$, the map $f_{a}: \\overline{T(V)} \\rightarrow L(V, \\overline{T(V)})$ given by $f_{a}(Y)=Y \\cdot a$ is well defined and linear. Thus, it can easily been seen to satisfy the conditions of the classical versions of Picard-Lindel\u00f6f theorem for Young CDEs (e.g. [36, Thm. 1.28]). Therefore, (3.7) admits a unique solution in $\\overline{T(V)}$. The fact that any solution $Y_{s}=\\left(Y_{s}^{0}, Y_{s}^{1}, Y_{s}^{2}, \\ldots\\right)$ to (3.7) in $\\overline{T(V)}$ must satisfy $Y_{s}^{k}=\\left(A \\cdot S(\\gamma)_{t, s}\\right)^{k}$ for any $k$ in $\\mathbb{N}$ and any $s$ in $\\left[t, t^{\\prime}\\right]$ can be shown by a simple induction on $k$.\n\nRemark 3.3. By construction, the iterated integrals in (3.6) admit the following recursive structure\n\n$$\nS(\\gamma)_{t, t^{\\prime}}^{(w)}=\\int_{t}^{t^{\\prime}} S(\\gamma)_{t, t^{\\prime}}^{(w_{-})} d \\gamma_{u}^{\\left(i_{m}\\right)}, \\quad \\text { where } w_{-}=\\left(i_{1}, \\ldots, i_{m-1}\\right)\n$$\n\nThus the coordinates of the signature, when taken in isolation, do not satisfy a differential equation.\nWe can now define the signature kernel as the inner product of two signatures.\nDefinition 3.4. Let $0 \\leq s<s^{\\prime} \\leq T, 0 \\leq t<t^{\\prime} \\leq T$. Define the signature kernel $\\kappa_{\\text {sig }}: \\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], V\\right) \\times$ $\\mathrm{BV}\\left(\\left[s, s^{\\prime}\\right], V\\right) \\rightarrow \\mathbb{R}$ as the following symmetric function\n\n$$\n\\kappa_{\\text {sig }}(\\gamma, \\tau)=\\left\\langle S(\\gamma)_{t, t^{\\prime}}, S(\\tau)_{s, s^{\\prime}}\\right\\rangle_{\\overline{T(V)}}=\\sum_{k=0}^{\\infty} \\sum_{w \\in \\mathbb{W}:|w|=k} S(\\gamma)_{t, t^{\\prime}}^{(w)} S(\\tau)_{s, s^{\\prime}}^{(w)}\n$$\n\nRemark 3.5. Note that the two paths $\\gamma, \\tau$ need not be defined on the same time interval. As for the norms, we will omit the subscript $t, t^{\\prime}$ when it is clear from the context.\n\nIt was proved in [46] that the signature kernel can be realised as the solution of a Goursat PDE as stated in the next lemma; this effectively provides a kernel trick for the signature kernel, i.e. a way of computing it without reference to the signature map.\n\nLemma 3.6. [46, Theorem 2.5] Let $\\left[s, s^{\\prime}\\right],\\left[t, t^{\\prime}\\right] \\in[0, T]$ and $\\gamma \\in \\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], V\\right), \\tau \\in \\mathrm{BV}\\left(\\left[s, s^{\\prime}\\right], V\\right)$ be two paths. Then, the signature kernel of $\\gamma, \\tau$ is realised as the solution of the following two-parameter integral equation\n\n$$\n\\kappa_{\\text {sig }}(\\gamma, \\tau)=\\kappa_{\\text {sig }}(\\gamma, \\tau)_{t^{\\prime}, s^{\\prime}} \\quad \\text { where } \\quad \\kappa_{\\text {sig }}(\\gamma, \\tau)_{r, q}=1+\\int_{s}^{q} \\int_{t}^{r} \\kappa_{\\text {sig }}(\\gamma, \\tau)_{u, v}\\left\\langle d \\gamma_{u}, d \\tau_{v}\\right\\rangle_{V}\n$$\n\nand with boundary conditions $\\kappa_{\\text {sig }}(\\gamma, \\tau)_{t, q}=\\kappa_{\\text {sig }}(\\gamma, \\tau)_{r, s}=1$ for all $q \\in\\left[s, s^{\\prime}\\right], r \\in\\left[t, t^{\\prime}\\right]$.\n\nSeveral finite difference schemes are available for numerically evaluating solutions to equation (3.8), see [46, Section 3.1] for details.\n3.3. Signature kernels from static kernels. In view of data science applications, instead of taking inner products of signatures directly, it might be beneficial to first lift paths in $V$ to paths with values in the RKHS $\\mathcal{H}_{g}$ of a static kernel $g: V \\times V \\rightarrow \\mathbb{R}$. More precisely, for any $\\gamma \\in \\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], V\\right)$, with $0 \\leq t<t^{\\prime} \\leq T$, denote by $\\gamma^{g}:\\left[t, t^{\\prime}\\right] \\rightarrow \\mathcal{H}_{g}$ the path defined for any $r \\in\\left[t, t^{\\prime}\\right]$ as $\\gamma_{r}^{g}=g\\left(\\gamma_{r}, \\cdot\\right)$. For the signature of a lifted path $\\gamma^{g}$ to be well-defined we will require that the latter is at least continuous and of bounded variation, i.e. $\\gamma^{g} \\in \\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], \\mathcal{H}_{g}\\right)$. The following result gives a sufficient condition to ensure this and immediately follows from the definitions of bounded variation and Lipschitz continuity. In particular, in the experimental section, we will make use of the radial basis function kernel as $g$, which is $C^{\\infty}$ and Lipschitz continuous.\n\nLemma 3.7. Let $g: V \\times V \\rightarrow \\mathbb{R}$ be a kernel such that for any $v \\in V$ the canonical feature map $g(v, \\cdot)$ is Lipschitz-continuous and denote by $\\mathcal{H}_{g}$ its RKHS. Then, for any path $\\gamma \\in \\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], V\\right)$, the lifted path $\\gamma^{g}:\\left[t, t^{\\prime}\\right] \\rightarrow \\mathcal{H}_{g}$ defined for any $r \\in\\left[t, t^{\\prime}\\right]$ as $\\gamma_{r}^{g}=g\\left(\\gamma_{r}, \\cdot\\right)$ is continuous and of bounded variation.\n\nOnce paths in $\\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], V\\right)$ are lifted to paths in $\\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], \\mathcal{H}_{g}\\right)$ one can compute their signatures and take their inner products in the same way as before. More precisely, given any path $\\gamma \\in \\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], V\\right)$, the signature $S\\left(\\gamma^{g}\\right)$ of the lifted path $\\gamma^{g} \\in \\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], \\mathcal{H}_{g}\\right)$ is a well-defined element of the closure of $T\\left(\\mathcal{H}_{g}\\right)$. This yields the following definition of lifted signature kernel.\n\nDefinition 3.8. Let $g: V \\times V \\rightarrow \\mathbb{R}$ be a kernel such that for any $v \\in V$ the canonical feature map $g(v, \\cdot)$ is Lipschitz-continuous and denote by $\\mathcal{H}_{g}$ its RKHS. Then, for any $\\left[s, s^{\\prime}\\right],\\left[t, t^{\\prime}\\right] \\subset[0, T]$, the $g$-lifted signature kernel $\\kappa_{\\text {sig }}^{g}: \\mathrm{BV}\\left(\\left[t, t^{\\prime}\\right], V\\right) \\times \\mathrm{BV}\\left(\\left[s, s^{\\prime}\\right], V\\right) \\rightarrow \\mathbb{R}$ is defined as follows\n\n$$\n\\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)=\\left\\langle S\\left(\\gamma^{g}\\right)_{t, t^{\\prime}}, S\\left(\\tau^{g}\\right)_{s, s^{\\prime}}\\right\\rangle\n$$\n\nwhere the inner product is taken in the closure of $T\\left(\\mathcal{H}_{g}\\right)$.\nIn the sequel, the intervals $\\left[t, t^{\\prime}\\right]$ and $\\left[s, s^{\\prime}\\right]$ will be understood implicitly from the domain of definition of $\\gamma$ and $\\tau$. Lemma 3.6 directly yields a kernel trick for the $g$-lifted signature kernel $\\kappa_{\\text {sig }}^{g}$ :\n\n$$\n\\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)=\\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)_{t^{\\prime}, s^{\\prime}} \\quad \\text { where } \\quad \\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)_{r, q}=1+\\int_{s}^{q} \\int_{t}^{r} \\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)_{u, v}\\left\\langle d \\gamma_{u}^{g}, d \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}\n$$\n\nwith the boundary conditions $\\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{t, q}=\\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{r, s}=1$ for all $q \\in\\left[s, s^{\\prime}\\right], r \\in\\left[t, t^{\\prime}\\right]$.\nRemark 3.9. The integration with respect to $\\mathcal{H}_{g}$ paths is understood in the sense of\n\n$$\n\\mathrm{d} \\gamma_{u}^{g}=\\mathrm{d} g\\left(\\gamma_{u}, \\cdot\\right)=\\sum_{i=1}^{d} \\partial_{x^{i}} g\\left(\\gamma_{u}, \\cdot\\right) \\mathrm{d} \\gamma_{u}^{i}\n$$\n\n3.4. Derivatives of static kernels. To approximate solutions to PPDEs, it will be necessary to differentiate a kernel with respect to its input variables. The RBF kernel $g_{\\sigma}$, defined in (3.2), is clearly $C^{\\infty}$ and its $n^{t h}$ order derivative can be obtained using elementary calculus. For example, the first two derivatives admit the following expressions\n\n$$\n\\begin{aligned}\n\\frac{\\partial g_{\\sigma}(x, y)}{\\partial x} & =\\frac{y-x}{\\sigma^{2}} \\exp \\left(-\\frac{(x-y)^{2}}{2 \\sigma^{2}}\\right) \\\\\n\\frac{\\partial^{2} g_{\\sigma}(x, y)}{\\partial x^{2}} & =\\frac{(x-y)^{2}-\\sigma^{2}}{\\sigma^{4}} \\exp \\left(-\\frac{(x-y)^{2}}{2 \\sigma^{2}}\\right)\n\\end{aligned}\n$$\n\nBy the results in [57], for all $x \\in V$, the derivatives of $g(x, \\cdot)$ are elements of $\\mathcal{H}_{g}$. Furthermore, [57, Theorem 1] states that, for any $h \\in \\mathcal{H}_{g}$ and any multi-index $I$,\n\n$$\n\\left\\langle\\partial_{x^{I}}^{[I]} g(x, \\cdot), h\\right\\rangle_{\\mathcal{H}_{g}}=\\partial_{x^{I}}^{[I]} h(x)=\\partial_{x^{I}}^{[I]}\\left\\langle g(x, \\cdot), h\\right\\rangle_{\\mathcal{H}_{g}}\n$$\n\nwhere $x^{I}:=\\left\\{x^{i}, i \\in I\\right\\}$. For $n, m \\in \\mathbb{N}$ let $I, J$ be multi-indices on $\\{1, \\cdots, n\\}$ and $\\{1, \\cdots, m\\}$ respectively. In particular, we have\n\n$$\n\\left\\langle\\partial_{x^{i}}^{n} g(x, \\cdot), \\partial_{y^{i}}^{m} g(y, \\cdot)\\right\\rangle_{\\mathcal{H}_{g}}=\\partial_{x^{i}}^{n} \\partial_{y^{i}}^{m}\\left\\langle g(x, \\cdot), g(y, \\cdot)\\right\\rangle_{\\mathcal{H}_{g}}=\\partial_{x^{i}}^{n} \\partial_{y^{i}}^{m} g(x, y)\n$$\n\nWe make the following standing assumption on the static kernel.\nAssumption 3.10. $g: V \\times V \\rightarrow \\mathbb{R}$ is four times differentiable and, for all $0 \\leq n, m \\leq 4$, we have\n\n$$\n\\overline{g_{n m}}:=\\sup _{x, y \\in V}\\left(\\sum_{|I|=n,|J|=m}\\left|\\partial_{x^{I}}^{n} \\partial_{y^{J}}^{m} g(x, y)\\right|^{2}\\right)^{\\frac{1}{2}}<\\infty\n$$\n\nThis assumption is satisfied for the RBF kernel (3.2).\n3.5. Derivatives of signature kernels. In this section we will only consider paths on intervals of the form $[t, T],[s, T]$, for $s, t \\in \\mathbb{T}$ and fix $V=\\mathbb{R}^{e}$; this way the path spaces boil down to those introduced in Section 2.1. For two paths $\\gamma \\in \\mathrm{BV}_{t}$ and $\\tau \\in \\mathrm{BV}_{s}$, the directional derivative of the signature kernel with\n\nrespect to its first variable and along the direction of a path $\\eta \\in \\mathrm{BV}_{t}$ is the Gateaux derivative (2.10) which perturbs the whole path:\n\n$$\n\\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}(\\gamma, \\tau):=\\lim _{\\varepsilon \\rightarrow 0} \\frac{\\kappa_{\\text {sig }(\\gamma}+\\varepsilon \\eta, \\tau)-\\kappa_{\\text {sig }}(\\gamma, \\tau)}{\\varepsilon}\n$$\n\nIt was shown in [47] that the directional derivative in equation (3.14) is such that\n\n$$\n\\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}(\\gamma, \\tau)=\\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}(\\gamma, \\tau)_{T, T}\n$$\n\nwhere the map $\\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}(\\gamma, \\tau):[t, T] \\times[s, T] \\rightarrow \\mathbb{R}$ solves a two-parameter integral equation similar to (3.8)\n\n$$\n\\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}=\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}}\\left(\\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\gamma_{u}, \\mathrm{~d} \\tau_{v}\\right\\rangle+\\kappa_{\\text {sig }}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\eta_{u}, \\mathrm{~d} \\tau_{v}\\right\\rangle\\right)\n$$\n\nLeveraging (3.9), we can extend this PDE formulation to the lifted kernel and to its second derivative, as we show in Proposition 3.11. For all $\\gamma \\in \\mathrm{BV}_{t}$, define the map $G: \\mathrm{BV}_{t} \\rightarrow \\mathrm{BV}\\left([t, T], \\mathcal{H}_{g}\\right)$ as\n\n$$\nG(\\gamma)_{u}=\\gamma_{u}^{g}=g\\left(\\gamma_{u}, \\cdot\\right), \\quad \\text { for all } u \\in[t, T]\n$$\n\nThe proofs of all the results of this section are postponed to Appendix A. 2 to ease the flow of the paper.\n\nProposition 3.11. Let Assumption 3.10 holds and $\\gamma \\in \\mathrm{BV}_{t}, \\tau \\in \\mathrm{BV}_{s}$ for $s, t \\in \\mathbb{T}$. Then the following hold for all $s^{\\prime} \\in[s, T]$ and $t^{\\prime} \\in[t, T]$.\n(1) For any $\\eta \\in \\mathrm{BV}_{t}$, the directional derivative of the lifted kernel satisfies the following PDE\n\n$$\n\\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}=\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}}\\left(\\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\gamma_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}+\\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\partial_{\\gamma}^{\\eta} G(\\gamma)_{u}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}\\right)\n$$\n\n(2) For any $\\eta, \\bar{\\eta} \\in \\mathrm{BV}_{t}$, the second derivative of the lifted kernel satisfies the following PDE\n\n$$\n\\begin{aligned}\n\\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}= & \\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\gamma_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}+\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\partial_{\\gamma}^{\\bar{\\eta}} G(\\gamma)_{u}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\bar{\\eta}} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\partial_{\\gamma}^{\\eta} G(\\gamma)_{u}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}+\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} G(\\gamma)_{u}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}\n\\end{aligned}\n$$\n\nFrom the PDEs of Proposition 3.11 arise uniform estimates in $s, t \\in \\mathbb{T}$ for the signature kernel and its derivatives with respect to the 1 -variation norms of $\\gamma, \\tau, \\eta, \\bar{\\eta}$. The precise statements and bounds are gathered in Lemma A. 2 in the Appendix. In Section 4 we will restrict the state space to a compact subspace of $\\mathrm{BV}_{0}$ with bounded 1-variation norm which yield uniform bounds.\n\nRemark 3.12. Expliciting the partial derivatives in (3.16) yields the equivalent formulation:\n\n$$\n\\begin{aligned}\n\\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}= & \\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\eta} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, k=1}^{c} \\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, j, k=1}^{c} \\partial_{x^{i} j y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\eta_{u}^{j} \\mathrm{~d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa_{\\text {sig }}^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, k=1}^{c} \\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\eta_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}\n\\end{aligned}\n$$\n\nEquation (3.16) is more conducive for numerical computations while Equation (3.18) will be used for the theoretical analysis. One can perform the same expansion for the second derivative, as can be seen in the proof (Equation (A.16)).\n\nRemark 3.13. By iterating the same arguments one could prove that the pathwise derivative of the kernel at any order satisfies a linear PDE of the same kind. We do not pursue this here, but note in particular that $\\kappa(\\cdot, \\tau)$ is infinitely many times differentiable for all $\\tau \\in \\mathrm{BV}\\left(\\mathbb{T}, \\mathbb{R}^{e}\\right)$.\n\nSimilarly to the kernel case, given two paths $\\gamma, \\eta \\in \\mathrm{BV}_{t}$, we can define the directional derivative of the signature on lifted paths\n\n$$\n\\partial_{\\gamma}^{\\eta} S \\circ G(\\gamma)_{t, t^{\\prime}}=\\partial_{\\gamma}^{\\eta} S\\left(\\gamma^{g}\\right)_{t, t^{\\prime}}:=\\lim _{\\varepsilon \\rightarrow 0} \\frac{S\\left((\\gamma+\\varepsilon \\eta)^{g}\\right)_{t, t^{\\prime}}-S\\left(\\gamma^{g}\\right)_{t, t^{\\prime}}}{\\varepsilon}\n$$\n\nas well as the second derivative with respect to another direction $\\bar{\\eta} \\in \\mathrm{BV}(\\mathbb{T}, V)$, denoted $\\partial_{\\gamma}^{\\eta \\bar{\\eta}} S \\circ G$. The following result is a corollary of Lemma A.1, which also provides estimates of the signature and its derivatives with respect to the 1 -variation norms of $\\gamma, \\eta, \\bar{\\eta}$.\n\nProposition 3.14. For all $\\gamma, \\eta, \\bar{\\eta} \\in \\mathrm{BV}\\left(\\mathbb{T}, \\mathbb{R}^{e}\\right)$, $e \\in \\mathbb{N}$, the signature and its first two directional derivatives $\\partial_{\\gamma}^{\\eta} S \\circ G$ and $\\partial_{\\eta}^{\\eta \\bar{\\eta}} S \\circ G$ exist and are continuous with respect to the $\\mathcal{C}_{0}$-norm. Moreover, the derivatives satisfy the CDEs (A.9) and (A.12).\n\nThis result is analogous to [25, Theorem 4.4] albeit in $\\mathcal{H}_{g}$-valued paths.\n3.6. Computations of kernel derivatives. In practice, signature kernels and their derivatives can be computed with a single call to a PDE solver. To see this, for any $\\gamma, \\tau, \\eta, \\bar{\\eta} \\in \\mathrm{BV}_{0}$ and any $s, t \\in \\mathbb{T}$ set\n\n$$\n\\mathbb{A}_{s, t}(\\gamma, \\tau)=\\left\\langle\\partial_{t} \\gamma_{s}^{g}, \\partial_{t} \\tau_{t}^{g}\\right\\rangle_{\\mathcal{H}_{g}}, \\quad \\mathbb{A}_{s, t}^{\\eta}(\\gamma, \\tau)=\\left\\langle\\partial_{t} \\partial_{\\gamma}^{\\eta} G(\\gamma)_{s}, \\partial_{t} \\tau_{t}^{g}\\right\\rangle_{\\mathcal{H}_{g}}, \\quad \\mathbb{A}_{s, t}^{\\eta \\bar{\\eta}}(\\gamma, \\tau)=\\left\\langle\\partial_{t} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} G(\\gamma)_{s}, \\partial_{t} \\tau_{t}^{g}\\right\\rangle_{\\mathcal{H}_{g}}\n$$\n\nThen, the augmented variable\n\n$$\n\\mathbf{K}_{s, t}^{\\eta \\bar{\\eta}}(\\gamma, \\tau)=\\left(\\begin{array}{c}\n\\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)_{s, t} \\\\\n\\partial_{\\gamma}^{\\eta} \\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)_{s, t} \\\\\n\\partial_{\\gamma}^{\\eta} \\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)_{s, t} \\\\\n\\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)_{s, t}\n\\end{array}\\right) \\in \\mathbb{R}^{4}\n$$\n\nsatisfies the following linear system of hyperbolic PDEs\n\n$$\n\\frac{\\partial^{2}}{\\partial_{s} \\partial_{t}} \\mathbf{K}_{s, t}^{\\eta \\bar{\\eta}}(\\gamma, \\tau)=\\left(\\begin{array}{cccc}\n\\mathbb{A}_{s, t}(\\gamma, \\tau) & 0 & 0 & 0 \\\\\n\\mathbb{A}_{s, t}^{\\eta}(\\gamma, \\tau) & \\mathbb{A}_{s, t}(\\gamma, \\tau) & 0 & 0 \\\\\n\\mathbb{A}_{s, t}^{\\eta}(\\gamma, \\tau) & 0 & \\mathbb{A}_{s, t}(\\gamma, \\tau) & 0 \\\\\n\\mathbb{A}_{s, t}^{\\eta \\bar{\\eta}}(\\gamma, \\tau) & \\mathbb{A}_{s, t}^{\\bar{\\eta}}(\\gamma, \\tau) & \\mathbb{A}_{s, t}^{\\eta}(\\gamma, \\tau) & \\mathbb{A}_{s, t}(\\gamma, \\tau)\n\\end{array}\\right) \\mathbf{K}_{s, t}^{\\eta \\bar{\\eta}}(\\gamma, \\tau)\n$$\n\nwith boundary conditions\n\n$$\n\\mathbf{K}_{0, t}^{\\eta \\bar{\\eta}}(\\gamma, \\tau)=\\mathbf{K}_{s, 0}^{\\eta \\bar{\\eta}}(\\gamma, \\tau)=(1,0,0,0)^{\\top}\n$$\n\nRemark 3.15. In the case where $g=i d$ the system of PDEs (3.20) simplifies as $\\mathbb{A}_{s, t}^{\\eta}(\\gamma, \\tau)=\\mathbb{A}_{s, t}(\\eta, \\tau)$ and $\\mathbb{A}_{s, t}^{\\eta \\bar{\\eta}}=0$. For more general static kernels $g$, the derivative $\\mathbb{A}$ might not be available in close form. In that case we can approximate them by finite difference as follows\n\n$$\n\\mathbb{A}_{s, t}(\\gamma, \\tau) \\approx \\frac{1}{\\Delta s \\Delta t}\\left(g\\left(\\gamma_{s}, \\tau_{t}\\right)-g\\left(\\gamma_{s}, \\tau_{t-\\Delta t}\\right)-g\\left(\\gamma_{s-\\Delta s}, \\tau_{t}\\right)+g\\left(\\gamma_{s-\\Delta s}, \\tau_{t-\\Delta t}\\right)\\right)\n$$\n\nand similarly for $\\mathbb{A}^{\\eta}$ and $\\mathbb{A}^{\\eta, \\bar{\\eta}}$.\n3.7. Universal product kernels. The solutions to our PPDEs are defined on the product space $\\Omega=$ $\\mathbb{T} \\times \\mathbb{R}^{d} \\times \\mathrm{BV}_{0}$. Therefore, we will need to consider product kernels indexed on such product spaces. The following result provides a simple way of constructing kernels that are universal on a product space as products of kernel on the individual factor spaces.\n\nLemma 3.16. [9, Lemma 5.2] Let $\\kappa_{1}, \\kappa_{2}$ be two cc-universal kernels indexed on $\\Omega_{1}, \\Omega_{2}$ respectively. Then the product kernel defined for any $\\omega_{1}, \\omega_{1}^{\\prime} \\in \\Omega_{1}$ and $\\omega_{2}, \\omega_{2}^{\\prime} \\in \\Omega_{2}$ as\n\n$$\n\\kappa\\left(\\left(\\omega_{1}, \\omega_{2}\\right),\\left(\\omega_{1}^{\\prime}, \\omega_{2}^{\\prime}\\right)\\right):=\\kappa_{1}\\left(\\omega_{1}, \\omega_{1}^{\\prime}\\right) \\kappa_{2}\\left(\\omega_{2}, \\omega_{2}^{\\prime}\\right)\n$$\n\nis cc-universal on $\\Omega_{1} \\times \\Omega_{2}$.\nSince the domains $\\Omega$ of our PPDEs are of the form $\\Omega=\\mathbb{T} \\times \\mathbb{R}^{d} \\times \\mathrm{BV}_{0} \\subset \\mathbb{R}^{d+1} \\times \\mathrm{BV}_{0}$, it would be tempting to consider product kernels of the form $\\widetilde{\\kappa}^{k, g}: \\Omega \\times \\Omega \\rightarrow \\mathbb{R}$ defined for any $(s, x, \\gamma),(t, y, \\tau) \\in \\Omega$ as\n\n$$\n\\widetilde{\\kappa}^{k, g}((s, x, \\gamma),(t, y, \\tau)):=k((s, x),(t, y)) \\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)\n$$\n\nwhere $k: \\mathbb{R}^{d+1} \\times \\mathbb{R}^{d+1} \\rightarrow \\mathbb{R}$ is some cc-universal kernel on $\\mathbb{R}^{d+1}$ such as the RBF kernel in (3.2), and $\\kappa_{\\text {sig }}^{g}$ is a $g$-lifted signature kernel.\n\nHowever, signature kernels are only universal when restricted to classes of paths where the signature is an injective map. One such space is $\\mathrm{BV}_{0}^{0} \\subset \\mathrm{BV}_{0}$, the subset of continuous bounded variation paths that are time-augmented and started at the origin 0 , endowed with the 1 -variation topology. Denote by $\\mathcal{H}_{\\kappa_{\\text {sig }}^{g}}$ the RKHS associated to the signature kernel $\\kappa_{\\text {sig }}^{g}$. The next result states that, when restricted to a compact subset $B \\subset \\mathrm{BV}_{0}^{0}$, elements of $\\left.\\mathcal{H}_{\\kappa}\\right|_{B}$ are dense in $\\mathcal{C}(B)$ with the topology of uniform convergence. In other words, $\\kappa_{\\text {sig }}^{g}$ is cc-universal on $\\mathrm{BV}_{0}^{0}$.\n\nLemma 3.17. Let $B \\subset \\mathrm{BV}_{0}^{0}$ be compact in 1-variation. Then, $\\left.\\mathcal{H}_{\\kappa}\\right|_{B}$ is a dense subset of $\\mathcal{C}(B)$ with the topology of uniform convergence.\n\nThe proof of this statement is analogous to [11, Proposition 3.3] with the difference that the pointseparation assumption in the Stone-Weirestrass theorem is justified by noting that $S(\\gamma)=S(\\tau) \\Longleftrightarrow$ $\\gamma=\\tau$ when $\\gamma, \\tau \\in \\mathrm{BV}_{0}^{0}$.\n\nIt is important to note that the above result might no longer hold if one considers compact subsets of $\\mathrm{BV}_{0}$ that are not time-augmented and that do not share a common origin. This is due the fact that the signature map is no longer injective on $\\mathrm{BV}_{0}$, thus the point-separation assumption used in the proof of Lemma 3.17 invoking the Stone-Weirestrass Theorem no longer holds. However, in this paper we are interested in paths that might not have a common starting point. To remedy this issue, we modify the signature kernel $\\kappa_{\\text {sig }}^{g}$ and, for any $s, t \\in \\mathbb{T}$, define the kernel $\\kappa_{\\text {sig }}^{g, \\ell}: \\mathrm{BV}_{0} \\times \\mathrm{BV}_{0} \\rightarrow \\mathbb{R}$ as\n\n$$\n\\kappa_{\\mathrm{sig}}^{g, \\ell}(\\gamma, \\tau)=\\ell\\left(\\gamma_{0}, \\tau_{0}\\right) \\kappa_{\\mathrm{sig}}^{g}\\left(\\gamma-\\gamma_{0}, \\tau-\\tau_{0}\\right)\n$$\n\nwhere $\\ell: \\mathbb{R}^{e} \\times \\mathbb{R}^{e} \\rightarrow \\mathbb{R}$ is yet another kernel that keeps track of the starting points of the two input paths. The signature kernel $\\kappa_{\\text {sig }}^{g}$ is cc-universal on $\\mathrm{BV}_{0}$ by Lemma 3.17 hence, if $\\ell$ is cc-universal, Lemma 3.16 yields that $\\kappa_{\\text {sig }}^{g, \\ell}$ is cc-universal on $\\mathrm{BV}_{0}$. Hence, we define the product kernel $\\kappa: \\Omega \\times \\Omega \\rightarrow \\mathbb{R}$ for any $(s, x, \\gamma),(t, y, \\tau) \\in \\Omega$ as\n\n$$\n\\kappa((s, x, \\gamma),(t, y, \\tau))=\\kappa^{k, g, \\ell}((s, x, \\gamma),(t, y, \\tau))=k((s, x),(t, y)) \\kappa_{\\mathrm{sig}}^{g, \\ell}(\\gamma, \\tau)\n$$\n\nwhere $k: \\mathbb{R}^{d+1} \\times \\mathbb{R}^{d+1} \\rightarrow \\mathbb{R}$ is a cc-universal kernel on $\\mathbb{R}^{d+1}$. Note that for a path $\\bar{\\gamma}$ which is constant on $[0, s]$ we have $\\bar{\\gamma}-\\bar{\\gamma}_{0}=\\bar{\\gamma}-\\bar{\\gamma}_{s}$ such that $\\left.\\left(\\bar{\\gamma}-\\gamma_{0}\\right)\\right|_{[s, T]}$ is a path starting at zero. Moreover, because the signature is invariant to translation, there exists a measurable map $\\tilde{\\kappa}_{s}^{g}: \\mathrm{BV}_{s} \\rightarrow \\mathbb{R}$ such that for any $\\gamma, \\tau \\in \\mathrm{BV}_{0}^{0}$ with $\\bar{\\gamma}$ constant on $[0, s]$ one has\n\n$$\n\\kappa_{\\mathrm{sig}}^{g}(\\gamma, \\tau)=\\left\\langle S\\left(\\gamma^{g}\\right)_{0, T}, S\\left(\\tau^{g}\\right)_{0, T}\\right\\rangle=\\left\\langle S\\left(\\gamma^{g}\\right)_{0, s} \\cdot S\\left(\\gamma^{g}\\right)_{s, T}, S\\left(\\tau^{g}\\right)_{0, T}\\right\\rangle=\\tilde{\\kappa}_{s}^{g}\\left(\\left.\\gamma\\right|_{[s, T]}, \\tau\\right)\n$$\n\nThis entails $(s, x, \\gamma) \\mapsto \\kappa((s, x, \\gamma),(t, y, \\tau)) \\in \\mathcal{C}(\\Omega)$, and it is clear what the Gateaux derivative (2.11) means in this context. For future reference and technical reasons, we make the following assumptions on these kernels, satisfied for instance by RBF kernels.\n\nAssumption 3.18. The feature maps $\\varphi: \\mathbb{R}^{e} \\rightarrow \\mathbb{R}^{e}$ and $\\chi: \\mathbb{R}^{d+1} \\rightarrow \\mathbb{R}^{d+1}$, associated to the kernels $\\ell$ and $k$ respectively, are bounded, Lipschitz continuous, twice differentiable and with bounded, Lipschitz continuous derivatives, with bounds and Lipschitz constants $C_{\\varphi}$ and $C_{\\chi}$ respectively.\n\nFor the convergence of the numerical scheme to take place, we have to work on a compact subspace of $\\left(\\mathrm{BV}_{0},\\|\\cdot\\|_{p-v a r}\\right)$, for $p>1$, on which the 1 -variation norm is bounded. This results in all the estimates of Lemmas A. 1 and A. 2 to be bounded and enables the use of Arzel\u00e0-Ascoli theorem. As\n\nwe recalled in the introduction, for all $R>0$ and $\\alpha>0$, the set $B(R):=\\left\\{\\gamma \\in \\mathrm{BV}_{0}:\\|\\gamma\\|_{1-v a r, \\alpha} \\leq R\\right\\}$ is compact of $\\left(\\mathrm{BV}_{0},\\|\\cdot\\|_{p-v a r}\\right)$ for all $p>1$. Compact sets of $\\Omega$ with respect to $\\boldsymbol{d}$ are thus typically of the form $\\mathcal{X}=\\mathbb{T} \\times \\bigcap_{i=1}^{d}\\left[a_{i}, b_{i}\\right] \\times B(R)$, for $a_{i}, b_{i} \\in \\mathbb{R}, i=1, \\cdots, d$ and $R>0$.\n\nWe denote by $\\mathcal{H}$ the RKHS of the kernel $\\kappa: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ restricted to act on $\\mathcal{X}$. We define the feature map $\\Phi: \\mathcal{X} \\rightarrow \\mathcal{H}$ as $\\Phi(\\omega)=\\kappa(\\omega, \\cdot)$. Since the two kernels $h$ and $\\kappa_{\\text {sig }}^{q, \\ell}$ are continuously twice differentiable, as we have seen in the previous section, so is their product. This implies that for each $\\omega \\in \\mathcal{X}$, the map $\\kappa(\\omega, \\cdot)$ belongs to $C^{\\infty, \\infty, 2}(\\mathcal{X})$.\n\nMoreover, as a consequence of Lemmas 3.16 and 3.17, the product kernel $\\kappa$ is cc-univeral on $\\mathcal{X} \\times \\mathcal{X}$. In other words, $\\mathcal{H}$ is dense in $\\mathcal{C}(\\mathcal{X})$ with respect to the topology of uniform convergence.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 4. Main THEORETICAL RESULTS \n\nThis section aims at answering the questions posed in Section 2.5 and restricts our domain of study to pathwise derivatives of order two or lower. This framework is relevant for the backward Kolmogorov equations we presented in Section 2.3 and can rely on the equations derived in Proposition 3.11. More precisely we fix $\\bar{N} \\in \\mathbb{N}$ and consider the operator $\\mathcal{L}_{t}$ in (2.13) with\n\n$$\n\\mathcal{N}:=\\left\\{n=\\left(n_{1}, n_{2}, n_{3}\\right) \\in \\mathbb{N}_{0}^{3}: n_{1} \\leq \\bar{N}, n_{2} \\leq \\bar{N}, n_{3} \\leq 2\\right\\}\n$$\n\n4.1. Optimal recovery and well-posedness. This section aims at answering the questions (1)(3) of Section 2.5 regarding the well-posedness of the optimisation problem (2.14). Our first result verifies that elements of $\\mathcal{H}$ are indeed twice differentiable, and therefore belong to the domain of the PPDE operator $\\mathcal{L}_{t}$. This answers Question (1) in 2.5. The proof builds on the estimates derived in Lemma A.2. The technical hurdle consists in interchanging the infinite series and the derivative operator. As advertised earlier, this operation requires the restriction of $\\mathcal{H}$ to a compact subset of $\\Omega$ called $\\mathcal{X}$. Even though $\\mathcal{X}$ is closed, the time derivative of $h \\in \\mathcal{H}$ is well-defined since it only perturbs to the right and we set $\\partial_{t} u(T, x, \\gamma)=0$. Moreover the spatial and pathwise derivatives are also welldefined over $\\bigcap_{i=1}^{d}\\left[a_{i}, b_{i}\\right] \\times B(R)$ since the latter is included in an open set of $\\Omega$ and the kernel $\\kappa$ is defined over $\\Omega \\times \\Omega$.\n\nProposition 4.1. Let Assumptions 3.10 and 3.18 hold for $g$ and $k, \\ell$ respectively. For all $h \\in \\mathcal{H}$ such that $h=\\sum_{i=1}^{\\infty} \\alpha_{i} \\kappa\\left(\\omega^{i}, \\cdot\\right)$, with $\\left(\\alpha_{i}\\right)_{i \\in \\mathbb{N}}$ in $\\mathbb{R}$ and $\\left(\\omega^{i}\\right)_{i \\in \\mathbb{N}}$ in $\\mathcal{X}$, for all $\\partial \\in\\left\\{\\partial_{t}^{n_{1}} \\partial_{x}^{n_{2}} \\widetilde{\\partial}_{\\gamma}^{\\eta}:\\left(n_{1}, n_{2}, n_{3}\\right) \\in\\right.$ $\\mathcal{N}, \\boldsymbol{\\eta} \\in \\mathrm{BV}_{0}^{\\otimes n_{3}}$ ) and $\\omega=(t, x, \\gamma) \\in \\mathcal{X}$, we have\n\n$$\n\\partial h(\\omega)=\\sum_{i=1}^{\\infty} \\alpha_{i} \\partial \\kappa\\left(\\omega^{i}, \\omega\\right)\n$$\n\nIn particular the domain of $\\mathcal{L}_{t}$ is included in $\\mathcal{H}$ for all $t \\in[0, T)$.\nProof. Let $h \\in \\mathcal{H}$ such that $h=\\sum_{i=1}^{\\infty} \\alpha_{i} \\kappa\\left(\\omega^{i}, \\cdot\\right)$, with $\\left(\\alpha_{i}\\right)_{i \\in \\mathbb{N}}$ in $\\mathbb{R}$ and $\\left(\\omega^{i}\\right)_{i \\in \\mathbb{N}}$ in $\\mathcal{X}$. We prove the claim only for the pathwise derivative $\\partial \\in\\left\\{\\widetilde{\\partial}_{\\gamma}^{\\eta}, \\widetilde{\\partial}_{\\gamma}^{\\eta \\bar{\\eta}}\\right\\}$, with $\\eta, \\bar{\\eta} \\in \\mathrm{BV}_{0}$, and note that the other cases can be proved in an identical fashion, as the RBF kernel $k$ is infinitely many times differentiable and its derivatives are uniformly bounded. An important observation is that for all $(s, x, \\gamma),(t, y, \\tau) \\in \\mathcal{X}$ we have\n\n$$\n\\widetilde{\\partial}_{\\gamma}^{\\eta} \\kappa((s, x, \\gamma),(t, y, \\tau))=k((s, x),(t, y)) \\partial_{\\gamma}^{\\eta[t, \\tau]} \\kappa_{\\text {sig }}^{q, \\ell}\\left(\\gamma|_{[t, T]}, \\tau\\right)\n$$\n\nThis highlights the link between the two types of pathwise derivatives $\\widetilde{\\partial}_{\\gamma}^{\\eta}$ and $\\partial_{\\gamma}^{\\eta}$.\n$\\left(\\boldsymbol{\\partial}_{2}^{\\eta}\\right)$ For all $n \\in \\mathbb{N}$, let $h_{n}:=\\sum_{i=1}^{n} \\alpha_{i} \\kappa\\left(\\omega^{i}, \\cdot\\right)$ hence $h_{n}$ tends to $f$ pointwise and in RKHS norm as $n \\rightarrow \\infty$. For all $\\omega \\in \\mathcal{X}$ and $\\eta \\in \\mathrm{BV}\\left(\\mathbb{T}, \\mathbb{R}^{e}\\right)$, we have and we define\n\n$$\n\\partial_{z}^{n} h_{n}(\\omega)=\\sum_{i=1}^{n} \\alpha_{i} \\partial_{z}^{n} \\kappa\\left(\\omega^{i}, \\omega\\right), \\quad h^{\\prime}(\\omega):=\\sum_{i=1}^{\\infty} \\alpha_{i} \\partial_{z}^{n} \\kappa\\left(\\omega^{i}, \\omega\\right)\n$$\n\nand we will prove that $\\partial_{\\gamma}^{n} h_{n}$ converges to $h^{\\prime}$ uniformly on $\\mathcal{X}$. Let $n \\in \\mathbb{N}$, the Cauchy-Schwarz inequality allows us to disentangle the two limits:\n\n$$\n\\begin{aligned}\n\\left|\\partial_{\\gamma}^{n} h_{n}(\\omega)-h^{\\prime}(\\omega)\\right|=\\left|\\sum_{i=n+1}^{\\infty} \\alpha_{i} \\partial_{\\gamma}^{n} \\kappa\\left(\\omega^{i}, \\omega\\right)\\right| & =\\left|\\sum_{i=n+1}^{\\infty} \\alpha_{i} \\lim _{\\varepsilon \\rightarrow 0} \\frac{\\kappa\\left(\\omega^{i},(t, x, \\gamma+\\varepsilon \\eta))-\\kappa\\left(\\omega^{i},(t, x, \\gamma)\\right)\\right)}{\\varepsilon}\\right| \\\\\n& =\\lim _{N \\rightarrow \\infty} \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left|\\sum_{i=n+1}^{N} \\alpha_{i}\\left\\langle\\Phi\\left(\\omega^{i}\\right), \\Phi(t, x, \\gamma+\\varepsilon \\eta)-\\Phi(t, x, \\gamma)\\right\\rangle\\right| \\\\\n& =\\lim _{N \\rightarrow \\infty} \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left|\\left\\langle\\sum_{i=n+1}^{N} \\alpha_{i} \\Phi\\left(\\omega^{i}\\right), \\Phi(t, x, \\gamma+\\varepsilon \\eta)-\\Phi(t, x, \\gamma)\\right\\rangle\\right| \\\\\n& \\leq \\lim _{N \\rightarrow \\infty}\\left\\|\\sum_{i=n+1}^{N} \\alpha_{i} \\Phi\\left(\\omega^{i}\\right)\\right\\|_{\\mathcal{H}} \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\|\\Phi(t, x, \\gamma+\\varepsilon \\eta)-\\Phi(t, x, \\gamma)\\|_{\\mathcal{H}}\n\\end{aligned}\n$$\n\nWe observe that\n\n$$\n\\begin{aligned}\n\\left\\|\\sum_{i=n+1}^{\\infty} \\alpha_{i} \\Phi\\left(\\omega^{i}\\right)\\right\\|_{\\mathcal{H}}^{2}=\\left\\langle\\sum_{i=n+1}^{\\infty} \\alpha_{i} \\Phi\\left(\\omega^{i}\\right), \\sum_{j=n+1}^{\\infty} \\alpha_{j} \\Phi\\left(\\omega^{j}\\right)\\right\\rangle_{\\mathcal{H}} & =\\sum_{i=n+1}^{\\infty} \\alpha_{i} \\sum_{j=n+1}^{\\infty} \\alpha_{j}\\left\\langle\\Phi\\left(\\omega^{i}\\right), \\Phi\\left(\\omega^{j}\\right)\\right\\rangle_{\\mathcal{H}} \\\\\n& =\\sum_{i, j=n+1}^{\\infty} \\alpha_{i} \\alpha_{j} \\kappa\\left(\\omega^{i}, \\omega^{j}\\right) \\\\\n& =\\left\\|h-h_{n}\\right\\|_{\\mathcal{H}}^{2}\n\\end{aligned}\n$$\n\nwhich tends to zero. Furthermore,\n\n$$\n\\begin{aligned}\n\\lim _{\\varepsilon \\rightarrow 0} & \\frac{1}{\\varepsilon^{2}}\\|\\Phi(t, x, \\gamma+\\varepsilon \\eta)-\\Phi(t, x, \\gamma)\\|_{\\mathcal{H}}^{2} \\\\\n= & k((t, x),(t, x)) \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon^{2}}\\left(\\kappa_{\\mathrm{sig}}^{g, \\ell}(\\gamma+\\varepsilon \\eta, \\gamma+\\varepsilon \\eta)-2 \\kappa_{\\mathrm{sig}}^{g, \\ell}(\\gamma+\\varepsilon \\eta, \\gamma)+\\kappa_{\\mathrm{sig}}^{g, \\ell}(\\gamma, \\gamma)\\right) \\\\\n= & k((t, x),(t, x))\\left(\\ell\\left(\\gamma_{t}, \\gamma_{t}\\right) \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon^{2}}\\left\\|S\\left((\\gamma+\\varepsilon \\eta)^{g}\\right)-S\\left(\\gamma^{g}\\right)\\right\\|_{T\\left(\\mathcal{H}_{g}\\right)}^{2}\\right. \\\\\n& \\left.+\\kappa_{\\mathrm{sig}}^{g}\\left(\\gamma-\\gamma_{t}, \\gamma-\\gamma_{t}\\right) \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon^{2}}\\left|\\varphi\\left(\\gamma_{t}+\\varepsilon \\eta_{t}\\right)-\\varphi\\left(\\gamma_{t}\\right)\\right|^{2}\\right) \\\\\n= & k((t, x),(t, x))\\left(\\ell\\left(\\gamma_{t}, \\gamma_{t}\\right)\\left\\|\\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)\\right\\|_{\\widetilde{T}\\left(\\mathcal{H}_{g}\\right)}^{2}+\\kappa_{\\mathrm{sig}}^{g}\\left(\\gamma-\\gamma_{t}, \\gamma-\\gamma_{t}\\right)\\left|\\partial_{\\gamma_{t}}^{n} \\varphi\\left(\\gamma_{t}\\right)\\right|^{2}\\right)\n\\end{aligned}\n$$\n\nwhich is uniformly bounded over all $\\omega \\in \\mathcal{X}$ by Lemma A. 1 and Assumption 3.18 for $\\ell$. We used dominated convergence to pass to the last line which follows from (A.2). This entails the convergence of $\\partial_{\\gamma}^{n} h_{n}$ towards $h^{\\prime}$ is uniform in $\\mathcal{X}$; therefore $h$ is differentiable and $\\partial_{\\gamma}^{n} h=h^{\\prime}=\\sum_{i=1}^{\\infty} \\alpha_{i} \\partial_{\\gamma}^{n} \\kappa\\left(\\omega^{i}, \\cdot\\right)$.\n$\\left(\\partial_{\\gamma}^{n \\bar{\\eta}}\\right)$ For all $\\omega \\in \\mathcal{X}$ and $\\eta, \\bar{\\eta} \\in \\mathrm{BV}\\left(\\mathbb{T}, \\mathbb{R}^{c}\\right)$, we have and we set\n\n$$\n\\partial_{\\gamma}^{n \\bar{\\eta}} h_{n}(\\omega)=\\sum_{i=1}^{n} \\alpha_{i} \\partial_{\\gamma}^{n \\bar{\\eta}} \\kappa\\left(\\omega^{i}, \\omega\\right), \\quad h^{\\prime \\prime}(\\omega):=\\sum_{i=1}^{\\infty} \\alpha_{i} \\partial_{\\gamma}^{n \\bar{\\eta}} \\kappa\\left(\\omega^{i}, \\omega\\right)\n$$\n\nand we will prove that $\\lim _{n \\rightarrow \\infty} \\partial_{\\gamma}^{n \\bar{\\eta}} h_{n}=h^{\\prime \\prime}$. First note that\n\n$$\n\\begin{aligned}\n\\partial_{\\gamma}^{n \\bar{\\eta}} \\kappa\\left(\\omega^{i}, \\omega\\right) & =\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left(\\partial_{\\gamma}^{n} \\kappa\\left(\\omega^{i},(t, x, \\gamma+\\varepsilon \\eta)\\right)-\\partial_{\\gamma}^{n} \\kappa\\left(\\omega^{i},(t, x, \\gamma)\\right)\\right) \\\\\n& =\\lim _{\\varepsilon \\rightarrow 0} \\lim _{\\delta \\rightarrow 0} \\frac{1}{\\varepsilon \\delta}\\left(\\kappa\\left(\\omega^{i},(t, x, \\gamma+\\varepsilon \\eta+\\delta \\bar{\\eta})\\right)-\\kappa\\left(\\omega^{i},(t, x, \\gamma+\\varepsilon \\eta)\\right)-\\kappa\\left(\\omega^{i},(t, x, \\gamma+\\delta \\bar{\\eta})\\right)+\\kappa\\left(\\omega^{i},(t, x, \\gamma)\\right)\\right) \\\\\n& =\\lim _{\\varepsilon \\rightarrow 0} \\lim _{\\delta \\rightarrow 0} \\frac{1}{\\varepsilon \\delta}\\left\\langle\\Phi\\left(\\omega^{i}\\right), \\Phi(t, x, \\gamma+\\varepsilon \\eta+\\delta \\bar{\\eta})\\right)-\\Phi(t, x, \\gamma+\\varepsilon \\eta))-\\Phi(t, x, \\gamma+\\delta \\bar{\\eta}))+\\Phi(t, x, \\gamma)\\rangle\n\\end{aligned}\n$$\n\nApplying the same approach as for the first derivative, we obtain by Cauchy-Schwarz inequality\n\n$$\n\\begin{aligned}\n& \\left|\\partial_{\\gamma}^{\\eta \\bar{\\eta}} h_{n}(\\omega)-h^{\\prime \\prime}(\\omega)\\right|=\\left|\\sum_{i=n+1}^{\\infty} \\alpha_{i} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa\\left(\\omega^{i}, \\omega\\right)\\right| \\\\\n& =\\lim _{\\mathcal{H} \\rightarrow \\infty} \\lim _{\\varepsilon \\rightarrow 0} \\lim _{\\delta \\rightarrow 0} \\frac{1}{\\varepsilon \\delta}\\left|\\left\\langle\\sum_{i=n+1}^{\\mathcal{N}} \\alpha_{i} \\Phi\\left(\\omega^{i}\\right), \\Phi(t, x, \\gamma+\\varepsilon \\eta+\\delta \\bar{\\eta})-\\Phi(t, x, \\gamma+\\varepsilon \\eta)-\\Phi(t, x, \\gamma+\\delta \\bar{\\eta})+\\Phi(t, x, \\gamma)\\right\\rangle\\right| \\\\\n& \\leq\\left\\|h-h_{n}\\right\\|_{\\mathcal{H}} \\lim _{\\varepsilon \\rightarrow 0} \\lim _{\\delta \\rightarrow 0} \\frac{1}{\\varepsilon \\delta}\\|\\Phi(t, x, \\gamma+\\varepsilon \\eta+\\delta \\bar{\\eta})-\\Phi(t, x, \\gamma+\\varepsilon \\eta)-\\Phi(t, x, \\gamma+\\delta \\bar{\\eta})+\\Phi(t, x, \\gamma)\\|_{\\mathcal{H}} \\\\\n& =\\left\\|h-h_{n}\\right\\|_{\\mathcal{H}} k((t, x),(t, x))(\\left|\\varphi\\left(\\gamma_{t}\\right)\\right|\\left\\|\\partial_{\\gamma}^{\\eta \\bar{\\eta}} S\\left(\\gamma^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}+\\left|\\partial_{\\gamma_{t}}^{\\eta} \\varphi\\left(\\gamma_{t}\\right)\\right|\\left\\|\\partial_{\\gamma}^{\\bar{\\eta}} S\\left(\\gamma^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& +\\left|\\partial_{\\gamma_{t}}^{\\bar{\\eta}} \\varphi\\left(\\gamma_{t}\\right)\\right|\\left\\|\\partial_{\\gamma}^{\\eta} S\\left(\\gamma^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}+\\left|\\partial_{\\gamma_{t}}^{\\eta_{t}} \\varphi\\left(\\gamma_{t}\\right)\\right|\\left\\|S\\left(\\gamma^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\text {, }\n\\end{aligned}\n$$\n\nwhere we used dominated convergence as for the first derivative. Lemma A. 1 and Assumption 3.18 ensure that $\\partial_{\\gamma}^{\\eta \\bar{\\eta}} h_{n}$ converges towards $h^{\\prime \\prime}$ uniformly in $\\mathcal{X}$ and thus $h$ is twice differentiable with $\\partial_{\\gamma}^{\\eta \\bar{\\eta}} h=$ $h^{\\prime \\prime}=\\sum_{i=1}^{\\infty} \\alpha_{i} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa\\left(\\omega^{i}, \\cdot\\right)$.\n\nNow that we have checked that the constraints of the optimal recovery problem (2.14) make sense, we can solve this optimisation problem explicitly due a variant of the representer theorem involving higher order derivatives [41], which gives an answer to Question (2) of our list 2.5. For this purpose, let us define $\\mathcal{X}^{\\circ}:=\\{\\omega=(t, x, \\gamma) \\in \\mathcal{X}: t \\in[0, T)\\}$ and $\\partial \\mathcal{X}:=\\{\\omega=(t, x, \\gamma) \\in \\mathcal{X}: t=T\\}$ such that $\\mathcal{X}=$ $\\mathcal{X}^{\\circ} \\cup \\partial \\mathcal{X}$. Notably, let ssumptions 3.10 and 3.18 hold for $g$ and $k, \\ell$ respectively. Then, for all $n, m \\in \\mathbb{N}$ and collocation points $\\left(\\omega^{i}\\right)_{i=1}^{m} \\in \\mathcal{X}^{\\circ},\\left(\\omega^{i}\\right)_{i=m+1}^{m+n} \\in \\partial \\mathcal{X}$, there exist weights $\\alpha_{1}, \\cdots, \\alpha_{m+n} \\in \\mathbb{R}$ such that the optimal recovery problem\n\n$$\n\\min _{u \\in \\mathcal{H}} \\frac{1}{2}\\|u\\|_{\\mathcal{H}}^{2} \\quad \\text { s.t. } \\quad \\begin{cases}\\mathcal{L}_{t} u\\left(\\omega^{i}\\right)=g\\left(\\omega^{i}\\right) & i=1, \\ldots, m \\\\ u\\left(\\omega^{i}\\right)=\\phi\\left(\\omega^{i}\\right) & i=m+1, \\ldots, m+n\\end{cases}\n$$\n\nassociated to the RKHS $\\mathcal{H}$ is well-posed and has a unique minimiser $u_{m, n} \\in \\mathcal{H}$ of the form\n\n$$\nu_{m, n}=\\sum_{i=1}^{m+n} \\alpha_{i} \\kappa\\left(\\omega^{i}, \\cdot\\right)\n$$\n\nthat is obtained by solving the finite dimensional quadratic optimisation problem with linear constraints\n\n$$\n\\min _{\\boldsymbol{\\alpha} \\in \\mathbb{R}^{m+n}} \\frac{1}{2} \\boldsymbol{\\alpha}^{\\top} \\mathcal{K} \\boldsymbol{\\alpha} \\text { subject to } \\quad \\widetilde{\\mathcal{K}} \\boldsymbol{\\alpha}=\\boldsymbol{b}\n$$\n\nwhere $\\boldsymbol{\\alpha}=\\left(\\alpha_{1}, \\ldots, \\alpha_{m+n}\\right) \\in \\mathbb{R}^{m+n}, \\mathcal{K} \\in \\mathbb{R}^{(m+n) \\times(m+n)}$ is the matrix with entries $\\mathcal{K}_{i, j}=\\kappa\\left(\\omega^{i}, \\omega^{j}\\right)$, and where $\\widetilde{\\mathcal{K}} \\in \\mathbb{R}^{(m+n) \\times(m+n)}$ and $\\boldsymbol{b} \\in \\mathbb{R}^{m+n}$ are defined as follows\n\n$$\n\\widetilde{\\mathcal{K}}:=\\left(\\begin{array}{ccc}\n\\mathcal{L}_{t} \\kappa\\left(\\omega^{1}, \\omega^{1}\\right) & \\ldots & \\mathcal{L}_{t} \\kappa\\left(\\omega^{1}, \\omega^{m+n}\\right) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathcal{L}_{t} \\kappa\\left(\\omega^{m}, \\omega^{1}\\right) & \\ldots & \\mathcal{L}_{t} \\kappa\\left(\\omega^{m}, \\omega^{m+n}\\right) \\\\\n\\kappa\\left(\\omega^{m+1}, \\omega^{1}\\right) & \\ldots & \\kappa\\left(\\omega^{m+1}, \\omega^{m+n}\\right) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\kappa\\left(\\omega^{m+n}, \\omega^{1}\\right) & \\ldots & \\kappa\\left(\\omega^{m+n}, \\omega^{m+n}\\right)\n\\end{array}\\right) \\quad \\text { and } \\quad \\boldsymbol{b}:=\\left(\\begin{array}{c}\ng\\left(\\omega^{1}\\right) \\\\\n\\vdots \\\\\ng\\left(\\omega^{m}\\right) \\\\\n\\phi\\left(\\omega^{m+1}\\right) \\\\\n\\vdots \\\\\n\\phi\\left(\\omega^{m+n}\\right)\n\\end{array}\\right)\n$$\n\nThis lifts the main computational obstacle and opens a clear path to solving the optimisation problem, thereby resolving Question (3) of 2.5 .\nRemark 4.2. The matrix $\\widetilde{\\mathcal{K}}$ is well-defined by the differentiability of the product kernel.\nRemark 4.3. Note that $\\left(\\alpha_{i}\\right)_{i=1}^{m+n}$ crucially depend on the collocation points and on $m$ and $n$; in particular they have no reason to remain stable for different values of $m, n$. This makes comparison\n\namong the terms of the sequence $\\left(u_{m, n}\\right)_{m, n}$ much trickier. As another avenue for future research, we note that orthogonalising the collocation points in a way that the $\\left(\\alpha_{i}\\right)_{i}$ remain constant for all $m, n$ could lead to better control of the norm.\n4.2. Consistency. For any subset $\\mathcal{N} \\in \\mathbb{N}_{0}^{3}$, let us define the $\\mathcal{C}^{\\mathcal{N}}(\\mathcal{X})$ norm as\n\n$$\n\\|h\\|_{\\mathcal{N}}:=\\sum_{\\left(n_{1}, n_{2}, n_{3}\\right) \\in \\mathcal{N}} \\sup \\left\\{\\left|\\partial_{t}^{n_{1}} \\partial_{x}^{n_{2}} \\partial_{\\gamma}^{n} h(t, x, \\gamma)\\right|:(t, x, \\gamma) \\in \\mathcal{X}, \\boldsymbol{\\eta} \\in \\mathrm{BV}_{0}^{\\otimes n_{3}},\\|\\boldsymbol{\\eta}\\|_{1-\\operatorname{var} ; \\bar{\\gamma}} \\leq 1\\right\\}\n$$\n\nDue to the fact that our estimates in Lemma A. 1 do not hold for all $n \\in \\mathbb{N}$, we restrict to the specification needed in our examples\n\n$$\n\\mathcal{N}=\\left\\{\\left(n_{1}, n_{2}, n_{3}\\right) \\in \\mathbb{N}_{0}^{3}: n_{1} \\leq 1, n_{2}=n_{3}=0, \\text { or } n_{1}=0, n_{2}+n_{3} \\leq 2\\right\\}\n$$\n\nWe associate to it the range of derivatives $\\mathcal{D}:=\\left\\{\\mathrm{id}, \\partial_{t}, \\partial_{x}, \\partial_{x}^{2}, \\partial_{\\gamma}^{n} \\partial_{x}, \\partial_{\\gamma}^{n}, \\partial_{\\gamma}^{n \\bar{\\eta}}\\right\\}$ such that the norm also reads\n\n$$\n\\|h\\|_{\\mathcal{N}}:=\\sum_{\\partial \\in \\mathcal{D}} \\sup \\left\\{|\\partial h(\\omega)|: \\omega \\in \\mathcal{X}, \\eta, \\bar{\\eta} \\in \\mathrm{BV}_{0},\\|\\eta\\|_{1} \\vee\\|\\bar{\\eta}\\|_{1-\\operatorname{var}} \\leq 1\\right\\}\n$$\n\nOur main result, inspired by [12, Theorem 1.2], follows. Its proof is given at the end of the section.\nTheorem 4.4. Let Assumptions 3.10 and 3.18 hold for $g$ and $k, \\ell$ respectively. Let $\\mathcal{N}$ be as in (4.5) with $\\eta, \\bar{\\eta} \\in \\mathrm{BV}_{0}$ and assume either one of the following conditions holds:\n(i) The family $\\left(u_{m, n}\\right)_{m, n \\in \\mathbb{N}}$ is relatively compact in $\\mathcal{C}^{\\mathcal{N}}(\\mathcal{X})$ and there exists a unique solution $u^{\\star}$ to the $\\operatorname{PPDE}(2.3)$ in $\\mathcal{C}^{\\mathcal{N}}(\\mathcal{X})$;\n(ii) There exists a unique solution $u^{\\star}$ to the $\\operatorname{PPDE}(2.3)$ in $\\mathcal{H}$.\n\nIf, moreover, as $m$ and $n$ tend to infinity, the collocation points $\\left(\\omega^{i}\\right)_{i=1}^{m}$ and $\\left(\\omega^{i}\\right)_{i=m+1}^{m+n}$ form a dense family of $\\mathcal{X}^{\\circ}$ and $\\partial \\mathcal{X}$ respectively, then $\\left(u_{m, n}\\right)_{m, n \\in \\mathbb{N}}$ converges towards $u^{\\star}$ in the $\\mathcal{C}^{\\mathcal{N}}$-topology as $m, n$ tend to $+\\infty$.\n\nThe assumptions of the theorem all deserve separate discussion, studied in reverse order.\nOn the collocation points. It is natural to ask the collocation points to be dense in $\\mathcal{X}$, however this leaves a lot of flexibility for choosing them in an optimal way, especially in the space of paths. This issue is directly related to the rate of convergence of the method which lies beyond the scope of this paper. In practice though, one is only interested in covering the support of the evaluation points. During the course of our experiments, we found that sampling the collocation points randomly (as Brownian motion trajectories) led to similar accuracy as sampling them from the same distribution as the evaluation points.\n\nOn the well-posedness of (2.3) Existence and uniqueness of (2.3) is discussed in several examples of interest in Section 2.3, and in the more general Volterra case in [10]. Uniqueness allows us to show that all the converging subsequences of $\\left(u_{m, n}\\right)_{m, n}$ have the same limit, thus implying the sequence does converge. Unfortunately, known uniqueness results hold on a subset of $\\mathcal{C}(\\widetilde{\\Omega})$ whereas we require uniqueness in (a subset of) $\\mathcal{C}(\\mathcal{X})$ where $\\mathcal{X}$ is strictly included in $\\widetilde{\\Omega}$.\n\nOn the assumption $u^{\\star} \\in \\mathcal{H}$.\n\n- Proving convergence of $\\left(u_{m, n}\\right)_{m, n}$ at the very least requires to find a norm in which this family is uniformly bounded. The reason for assuming $u^{\\star} \\in \\mathcal{H}$ is that it implies both the RKHS and $\\mathcal{C}^{\\mathcal{N}}$ norms of $\\left(u_{m, n}\\right)_{m, n}$ are uniformly bounded, which in turn yields the relative compactness of this family in $\\mathcal{C}^{\\mathcal{N}}$, by Lemma 4.6. It is still slightly different from condition (i) because uniqueness holds in a different space.\n- If $u^{\\star} \\notin \\mathcal{H}$ then the RKHS norm cannot be expected to be bounded and we lose our most promising tool to derive estimates for other norms. Note that we do not need equicontinuity if we look for compactness in $\\mathbb{R}$, but we still require $u_{m, n}$ and their derivatives to be bounded uniformly in $\\mathbb{N}^{2}$.\n\n- For $u^{*}$ to be in $\\mathcal{H}$ would require at minima to be in $\\mathcal{C}^{\\infty}$. In this direction, Theorem 3.10 in [21] shows how to expand a path (semimartingale) functional with respect to terms of the signature. It is conceivable that such a representation also holds in the fractional setting for sufficiently smooth functionals, which could then be identified to an element of the signature RKHS. When the underlying process is a semimartingale, smoothness of the payoff function (and the coefficients) essentially ensures smoothness of the conditional expectation (i.e. the solution to the Kolmogorov equation). However when the direction of the pathwise derivative is singular (only square integrable) one can only prove twice differentiabiliy of the solution (see [10, Proposition 2.23]). This remains a glass ceiling until one unveils how to exploit the regularisation properties of the expectation.\n\nRemark 4.5. Several other works in the literature assume that $u^{*} \\in \\mathcal{H}$. [12] make in addition the classical assumption that $\\mathcal{H}$ is embedded in a Sobolev space, a condition we drop because we are able to prove sufficient regularity only with the help of the bounded RKHS norm. In finite dimensions, this condition allows to prove convergence rates, see [56, Proposition 11.30]. See also [4] who provide error bounds under additional assumptions.\n\nThe proof strategy for Theorem 4.4 using condition (ii) consists in showing that $\\left(u_{m, n}\\right)$ is relatively compact and then extracting converging subsequences. Hence before presenting it we need to identify compact subsets of $\\mathcal{H}$ with respect to the $\\mathcal{C}^{\\mathcal{N}}$-topology. The RKHS norm is linked to the regularity of the function, therefore a family of $\\mathcal{H}$ bounded in RKHS norm is equicontinuous. If this family is also bounded in $\\mathcal{C}^{\\mathcal{N}}$ norm we can conclude by Arzel\u00e0-Ascoli theorem that it is relatively compact.\n\nLemma 4.6. Let Assumptions 3.10 and 3.18 hold for $g$ and $k, \\ell$ respectively. Let $\\mathcal{Y} \\subset \\mathcal{H}$ be bounded under both the RKHS and $\\mathcal{C}^{\\mathcal{N}}$ norms, then $\\mathcal{Y}$ is relatively compact with respect to the $\\mathcal{C}^{\\mathcal{N}}$ topology.\n\nProof. We will prove sequential compactness thanks to Arzel\u00e0-Ascoli theorem. Consider a sequence of functions $\\left(h_{n}\\right)_{n \\in \\mathbb{N}} \\subset \\mathcal{Y}$, bounded under the $\\mathcal{C}^{\\mathcal{N}}$-topology and the RKHS norm by a constant $C_{h}$. We consider $\\omega=(t, x, \\gamma), \\bar{\\omega}=(t, x, \\bar{\\gamma}) \\in \\mathcal{X}$, and we have by the reproducing property, Cauchy-Schwarz inequality and (A.2) that\n\n$$\n\\begin{aligned}\n\\left|h_{n}(\\omega)-h_{n}(\\bar{\\omega})\\right| & =\\left|\\left\\langle h_{n}, \\Phi(\\omega)-\\Phi(\\bar{\\omega})\\right\\rangle_{\\mathcal{H}}\\right| \\\\\n& \\leq\\left\\|h_{n}\\right\\|_{\\mathcal{H}}\\|\\Phi(\\omega)-\\Phi(\\bar{\\omega})\\|_{\\mathcal{H}} \\\\\n& \\leq C_{h} \\sqrt{k((t, x),(t, x))}\\left(\\left|\\varphi\\left(\\gamma_{t}\\right)\\right|\\left\\|S\\left(\\gamma^{g}\\right)-S\\left(\\bar{\\gamma}^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}+\\left|\\varphi\\left(\\gamma_{t}\\right)-\\varphi\\left(\\bar{\\gamma}_{t}\\right)\\right|\\left\\|S\\left(\\bar{\\gamma}^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}\\right) \\\\\n& \\leq C_{h} C_{\\varphi} \\sqrt{k_{\\infty}}\\|\\gamma-\\bar{\\gamma}\\|_{0 ;[t, T]}\\left(\\overline{D S_{0}}\\left(\\left|\\gamma\\right|_{1-\\text { var } ;[t, T]},|\\bar{\\gamma}|_{1-\\text { var } ;[t, T]}\\right)+\\overline{S_{0}}\\left(\\left|\\bar{\\gamma}\\right|_{1-\\text { var } ;[t, T]}\\right)\\right)\n\\end{aligned}\n$$\n\nwhere $k_{\\infty}:=\\sup _{(t, x, \\gamma) \\in \\mathcal{X}} k((t, x),(t, x))<+\\infty$ and $C_{\\varphi}<\\infty$ by Assumption 3.18. Since $\\overline{S_{0}}$ and $\\overline{D S_{0}}$ are continuous and $\\left|\\gamma\\right|_{1-\\text { var } ;[t, T]},|\\bar{\\gamma}|_{1-\\text { var } ;[t, T]}$ are bounded by $R$, there exists a constant $C>0$, independent of $\\omega$, such that $\\left|h_{n}(\\omega)-h_{n}(\\bar{\\omega})\\right| \\leq C\\|\\gamma-\\bar{\\gamma}\\|_{0 ;[t, T]} \\leq C\\|\\gamma-\\bar{\\gamma}\\|_{p-\\text { var } ;[t, T]}$. Therefore, $\\left\\{h_{n}\\right\\}_{n \\in \\mathbb{N}}$ is equicontinuous on the compact $\\mathcal{X}$ as it is equipped with the $p$-variation norm.\n\nFollowing the approach of (4.1) and leveraging on (A.4) and (A.6), similar bounds hold for $h_{n}$ 's derivatives with $\\eta, \\bar{\\eta} \\in \\mathrm{BV}(\\mathbb{T}, V)$. Similarly to the computations in the proof of Proposition 4.1, Assumption 3.18 and Lemma A. 1 yield\n\n$$\n\\begin{aligned}\n& \\left|\\partial_{\\gamma}^{n} h_{n}(\\omega)-\\partial_{\\gamma}^{n} h_{n}(\\bar{\\omega})\\right| \\\\\n& \\leq C_{h} k_{\\infty}\\left(\\left|\\varphi\\left(\\gamma_{t}\\right)\\right|\\left\\|\\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)-\\partial_{\\gamma}^{n} S\\left(\\bar{\\gamma}^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}+\\left|\\partial_{\\gamma_{t}}^{n} \\varphi\\left(\\gamma_{t}\\right)\\right|\\left\\|S\\left(\\gamma^{g}\\right)-S\\left(\\bar{\\gamma}^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& \\quad+\\left|\\varphi\\left(\\gamma_{t}\\right)-\\varphi\\left(\\bar{\\gamma}_{t}\\right)\\right|\\left\\|\\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}+\\left|\\partial_{\\gamma_{t}}^{n} \\varphi\\left(\\gamma_{t}\\right)-\\partial_{\\gamma_{t}}^{n} \\varphi\\left(\\bar{\\gamma}_{t}\\right)\\right|\\left\\|S\\left(\\gamma^{g}\\right)\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& \\leq C_{h} C_{\\varphi} k_{\\infty}\\|\\gamma-\\bar{\\gamma}\\|_{0 ;[t, T]}\\left(\\overline{D S_{1}}\\left(\\left|\\gamma\\right|_{1},|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1}\\right)+\\overline{D S_{0}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1}\\right)+\\overline{S_{1}}\\left(\\left|\\gamma\\right|_{1},\\|\\eta\\|_{1}\\right)+\\overline{S_{0}}\\left(\\left|\\gamma\\right|_{1}\\right)\\right.\n\\end{aligned}\n$$\n\nwhere we abbreviated the norms to $|\\gamma|_{1}=|\\gamma|_{1-\\text { var },[t, T]}$. Finally, in the same manner one obtains\n\n$$\n\\begin{aligned}\n& \\left|\\partial_{\\gamma}^{\\eta \\bar{\\eta}} h_{n}(\\omega)-\\partial_{\\gamma}^{\\eta \\bar{\\eta}} h_{n}(\\bar{\\omega})\\right| \\\\\n& \\leq C_{h} C_{\\varphi} k_{\\infty}\\|\\gamma-\\bar{\\gamma}\\|_{0 ;[t, T]}\\left(\\overline{D S_{2}}\\left(|\\gamma|_{1},|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)+\\overline{D S_{1}}\\left(|\\gamma|_{1},|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1}\\right)\\right. \\\\\n& +\\overline{D S_{1}}\\left(|\\gamma|_{1},|\\bar{\\gamma}|_{1},\\|\\bar{\\eta}\\|_{1}\\right)+\\overline{D S_{0}}\\left(|\\gamma|_{1},|\\bar{\\gamma}|_{1}\\right)+\\overline{S_{2}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)+\\overline{S_{2}}\\left(|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right) \\\\\n& +\\overline{S_{1}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1}\\right)+\\overline{S_{1}}\\left(|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1}\\right)+\\overline{S_{1}}\\left(|\\gamma|_{1},\\|\\bar{\\eta}\\|_{1}\\right)+\\overline{S_{1}}\\left(|\\bar{\\gamma}|_{1},\\|\\bar{\\eta}\\|_{1}\\right)+\\overline{S_{0}}\\left(|\\gamma|_{1}\\right)+\\overline{S_{0}}\\left(|\\bar{\\gamma}|_{1}\\right)\\right)\n\\end{aligned}\n$$\n\nBy the continuity of $\\overline{S_{0}}, \\overline{S_{1}}, \\overline{S_{2}}, \\overline{D S_{0}}, \\overline{D S_{1}}$ and $\\overline{D S_{2}}$, we can also conclude that the families $\\left(\\partial_{\\gamma}^{\\eta} h_{n}\\right)_{n \\in \\mathbb{N}}$ and $\\left(\\partial_{\\gamma}^{\\eta \\bar{\\eta}} h_{n}\\right)_{n \\in \\mathbb{N}}$ are equicontinuous on the compact $\\mathcal{X}$ with respect to the $p$-variation. The case where $\\ell\\left(\\gamma_{t}, \\bar{\\gamma}_{t}\\right) \\neq 1$ would yield the same result by exploiting the boundedness and Lipschitz continuity of $\\varphi$ and its derivatives, and the estimates from Lemma A.1.\n\nWe only give details of the equicontinuity with respect to paths since the counterpart on $\\mathbb{T} \\times \\mathbb{R}^{d}$ can be proved in a similar but easier fashion, using that the feature map associated to $k$ and its two derivatives are bounded and Lipschitz continuous. Furthermore, equicontinuity of the derivatives with respect to time and space is also straightforward with the same arguments.\n\nCoupled with the uniform $\\mathcal{C}^{\\mathcal{N}}$ bounds, Arzel\u00e0-Ascoli's theorem implies that $\\left(\\partial h_{n}\\right)_{n \\in \\mathbb{N}}$ is relatively compact in $\\mathcal{C}(\\mathcal{X})$ for all $\\partial \\in \\mathcal{D}$. In particular, there exists a subsequence $\\left(n_{i}\\right)_{i \\in \\mathbb{N}}$ such that $h_{n_{i}}$ converges as $i$ goes to $+\\infty$; then $\\left(\\partial_{t} h_{n_{i}}\\right)_{i \\in \\mathbb{N}}$ is a subset of a relatively compact space hence it is one itself and we can find a subsubsequence $n_{i_{j}}$ such that $\\partial_{t} h_{n_{i_{j}}}$ converges. We can go on for each further derivative until we have found a subsequence $\\left(n_{l}\\right)$ for which $\\partial h_{n_{l}}$ converges in $\\mathcal{C}(\\mathcal{X})$ converges for all $\\partial \\in \\mathcal{D}$. This is precisely convergence in $\\mathcal{C}^{\\mathcal{N}}$ with respect to the norm defined in (4.6).\n\nProof of Theorem 4.4. Assume that condition (ii) holds. Since $u^{\\star}$ solves the PPDE (2.3) with the Fr\u00e9chet derivative $\\widetilde{\\partial}^{\\eta}$ and that the Gateaux derivative $\\widetilde{\\partial}^{\\eta}$ is a weaker type, $u^{\\star}$ also solves the weaker PPDE (2.12). In particular, $u^{\\star}$ satisfies the constraints of the optimal recovery problem (4.3) at every point in $\\mathcal{X}$, and since $u_{m, n}$ is the minimiser, we must have $\\left\\|u_{m, n}\\right\\|_{\\mathcal{H}} \\leq\\left\\|u^{\\star}\\right\\|_{\\mathcal{H}}<\\infty$. Moreover, for all $\\partial \\in \\mathcal{D}$ and $\\eta, \\bar{\\eta} \\in \\mathrm{BV}_{0}$ such that $\\max \\left(\\|\\eta\\|_{1-\\text { var },[0, T]},\\|\\bar{\\eta}\\|_{1-\\text { var },[0, T]}\\right) \\leq 1$, Cauchy-Schwarz inequality and the same calculations as in the proof of Proposition 4.1 yield\n\n$$\n\\sup _{\\omega \\in \\mathcal{X}}\\left|\\partial u_{m, n}(\\omega)\\right|=\\sup _{\\omega \\in \\mathcal{X}}\\left|\\partial\\left\\langle u_{m, n}, \\kappa(\\omega, \\cdot)\\right\\rangle_{\\mathcal{H}}\\right| \\leq \\sup _{\\omega \\in \\mathcal{X}}\\left\\|\\partial \\kappa(\\omega, \\cdot)\\right\\|_{\\mathcal{H}}\\left\\|u_{m, n}\\right\\|_{\\mathcal{H}}\n$$\n\nWe take evaluation points over a compact and $\\partial \\kappa(\\omega, \\cdot)$ is continuous hence $\\sup _{\\omega \\in \\mathcal{X}}\\|\\partial \\kappa(\\omega, \\cdot)\\|_{\\mathcal{H}}^{2}<\\infty$. Thus there exists $C>0$ such that for all such $\\eta, \\bar{\\eta}$,\n\n$$\n\\sup _{\\omega \\in \\mathcal{X}}\\left|\\partial u_{m, n}(\\omega)\\right| \\leq C\\left\\|u^{\\star}\\right\\|_{\\mathcal{H}}, \\quad \\text { for all } \\partial \\in \\mathcal{D}\n$$\n\nThis proves that $\\left(u_{m, n}\\right)_{m, n \\in \\mathbb{N}}$ is bounded under both the RKHS and the $\\mathcal{C}^{\\mathcal{N}}$ norms and therefore this family form a relatively compact space by Lemma 4.6. This means that for each sequence $\\left(u_{m_{k}, n_{k}}\\right)_{k \\in \\mathbb{N}}$ included in $\\left(u_{m, n}\\right)_{m, n \\in \\mathbb{N}}$, where $n_{k}, m_{k}$ diverge to infinity as $k$ goes to infinity, there exists a subsequence, also denoted $\\left(u_{m_{k}, n_{k}}\\right)_{k \\in \\mathbb{N}}$ for conciseness, which converges in $\\mathcal{C}^{\\mathcal{N}}$ to a limit $u_{\\infty}$, as $k$ goes to infinity. Since $\\left(u_{m_{k}, n_{k}}\\right)_{k \\in \\mathbb{N}}$ is bounded under the RKHS norm, we have $u_{\\infty} \\in \\mathcal{H}$.\n\nUnder condition (i), the same conclusion holds except that $u_{\\infty}$ is an element of the closure of $\\mathcal{H}$ with respect to the $\\mathcal{C}^{\\mathcal{N}}$ norm. The universality property of the kernel derived in Lemma 3.17 extends to $\\mathcal{C}^{\\mathcal{N}}$ by Proposition 4.1 and entails that $\\overline{\\mathcal{H}}=\\mathcal{C}^{\\mathcal{N}}(\\mathcal{X})$. Indeed, for all $h \\in \\mathcal{C}^{\\mathcal{N}}(\\mathcal{X})$ there is a sequence $\\left(h_{k}\\right)_{k \\in \\mathbb{N}}$ in $\\mathcal{H}$ that converges to $h$ in $\\mathcal{C}_{0}$. Moreover, for all $\\partial \\in \\mathcal{D}$ and $\\omega \\in \\mathcal{X}, \\partial h_{k}(\\omega)=\\left\\langle h_{k}, \\partial \\kappa(\\omega, \\cdot)\\right\\rangle_{\\mathcal{H}}$ hence $\\partial h_{k}$ also converges in $\\mathcal{C}_{0}$ towards $\\partial h$.\n\nWe now need to show that $u_{\\infty}$ solves the PPDE (2.3). Let us define $v:=\\mathcal{L}_{t} u_{\\infty}$ and $v_{k}:=\\mathcal{L}_{t} u_{m_{k}, n_{k}}$ for all $k \\in \\mathbb{N}$. For any $\\omega=(t, x, \\gamma) \\in \\mathcal{X}^{\\circ}$, by the triangle inequality and because $v_{k}\\left(\\omega^{i}\\right)=g\\left(\\omega^{i}\\right)$ for\n\nany $1 \\leq i \\leq m_{k}$, we have\n\n$$\n|v(\\omega)-g(\\omega)| \\leq \\min _{1 \\leq i \\leq m_{k}}\\left\\{\\left|v(\\omega)-v\\left(\\omega^{i}\\right)\\right|+\\left|v\\left(\\omega^{i}\\right)-v_{k}\\left(\\omega^{i}\\right)\\right|+\\left|g\\left(\\omega^{i}\\right)-g(\\omega)\\right|\\right\\}\n$$\n\nRecall that $v$ and $g$ are uniformly continuous over the compact set $\\mathcal{X}$ and, as $k$ goes to infinity, $\\left(\\omega^{i}\\right)_{1 \\leq i \\leq m_{k}}$ form a dense family of $\\mathcal{X}^{\\circ}$. Therefore, for all $\\varepsilon>0$ there exists $M \\in \\mathbb{N}$ such that, if $m_{k} \\geq M$, we have\n\n$$\n\\min _{1 \\leq i \\leq m_{k}}\\left\\{\\left|v(\\omega)-v\\left(\\omega^{i}\\right)\\right|+\\left|g\\left(\\omega^{i}\\right)-g(\\omega)\\right|\\right\\} \\leq \\varepsilon\n$$\n\nIn addition, $v_{k}$ converges uniformly to $v$ because $u_{m_{k}, n_{k}}$ converges to $u_{\\infty}$ in $\\mathcal{C}^{\\mathcal{N}}$. Since $\\varepsilon>0$ was arbitrary, Equation (4.7) thus entails that $v(\\omega)=g(\\omega)$. Following a similar argument it can be shown that $u_{\\infty}(\\omega)=\\phi(\\omega)$ for all $\\omega \\in \\partial \\mathcal{X}$.\n\nIn conclusion, $u_{\\infty}$ is a solution of the PPDE (2.3). Under condition (i) (respectively (ii)), it belongs to $\\mathcal{C}^{\\mathcal{N}}$ (respectively $\\mathcal{H}$ ) and is the unique classical solution in this space, implying that $u^{\\star}=u_{\\infty}$. Since every convergent subsequence $\\left(u_{m, n}\\right)_{m, n \\in \\mathbb{N}}$ converges to the same limit $u^{\\star}$, the whole sequence also converges to $u^{\\star}$ in the $\\mathcal{C}^{\\mathcal{N}}$ topology.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 5. NUMERICAL EXPERIMENTS \n\nIn this section, we present numerical experiments benchmarking the signature kernel PPDE solver presented in the previous section against either analytic solutions (if they are available) or classical Monte Carlo solvers. We consider two examples mentioned in subsection 2.3, namely the pathdependent heat equation (2.7)) and the rough Bergomi PPDE (2.9). For both experiments, we consider two types of errors between the predicted prices and true prices, namely the mean squared error (MSE), and the mean absolute error (MAE). The optimal kernel hyperparameters are determined by cross-validation. We will conclude the section with a discussion to compare our kernel approach with recent neural networks techniques for solving PPDEs. Our code is available at https://github.com/crispitagorico/sigppde.\n5.1. Fractional Brownian motion. In this first example, we consider a one dimensional fractional Brownian motion $\\left(\\widehat{W}_{t}=\\int_{0}^{t} K(t, r) \\mathrm{d} W_{r}\\right)_{t \\in \\mathbb{T}}$ with Hurst exponent $H \\in\\left(0, \\frac{1}{2}\\right)$. We recall that $\\Theta_{s}^{t}=$ $\\mathbb{E}\\left[\\widehat{W}_{s} \\mid \\mathcal{F}_{t}\\right]$ for $s \\geq t$. By [54, Theorem 4.1], under appropriate regularity assumptions on the functions $\\phi, f$, the conditional expectation\n\n$$\n\\mathbb{E}\\left[\\phi\\left(\\widehat{W}_{T}\\right)+\\int_{t}^{T} f\\left(s, \\widehat{W}_{s}\\right) \\mathrm{d} s \\mid \\mathcal{F}_{t}\\right]=: u\\left(t, \\Theta^{t}\\right)\n$$\n\nis realised as the solution of the path-dependent heat equation\n\n$$\n\\partial_{t} u(t, \\gamma)+\\left\\langle\\partial_{\\gamma}^{2} u(t, \\gamma),\\left(K^{t}, K^{t}\\right)\\right\\rangle+f(t, \\gamma)=0, \\quad u(T, \\gamma)=\\phi\\left(\\gamma_{T}\\right)\n$$\n\nIn our experiments we choose $f \\equiv 0$ and consider the following three instances of function $\\phi$, for which analytic expressions of the conditional expectation are available:\n(1) $f(x)=x$,\n$\\mathbb{E}\\left[f\\left(\\widehat{W}_{T}\\right) \\mid \\mathcal{F}_{t}\\right]=\\Theta_{T}^{t}$;\n(2) $f(x)=\\mathrm{e}^{\\nu x}$,\n$\\mathbb{E}\\left[f\\left(\\widehat{W}_{T}\\right) \\mid \\mathcal{F}_{t}\\right]=\\exp \\left(\\nu \\Theta_{T}^{t}+\\frac{\\nu^{2}(T-t)^{2 H}}{2}\\right)$;\n(3) $f(x)=(x-K)_{+}$,\n$\\mathbb{E}\\left[f\\left(\\widehat{W}_{T}\\right) \\mid \\mathcal{F}_{t}\\right]=\\frac{(T-t)^{H}}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{\\left(K-\\Theta_{T}^{t}\\right)^{2}}{2(T-t)^{2 H}}\\right)-\\left(K-\\Theta_{T}^{t}\\right) \\vartheta\\left(\\frac{\\Theta_{T}^{t}-K}{(T-t)^{H}}\\right)$,\nwhere $\\nu, K \\in \\mathbb{R}$ and $\\vartheta$ is the standard normal cumulative distribution function.\nBecause analytic prices are available, we limit ourselves to asses the performance of our kernel algorithm to recover these true prices. Collocation points for the kernel method were chosen uniformly on $[0,1]$ for the time variable and sampled from the process $\\Theta^{t}$ for the path variable.\n\nAs it can be observed from Figure 1, our kernel approach is capable to recovering with a high level of accuracy the prices for all considered payoff profiles.\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1. Analytic prices against Signature Kernel prices with 400 collocation points.\n5.2. Rough Bergomi. In this section, we make use of the following formulation of the rough Bergomi model introduced by [5] and already written down in the introduction\n\n$$\n\\begin{gathered}\nX_{t}:=\\int_{0}^{t} \\sqrt{\\psi_{r}\\left(\\widehat{W}_{r}\\right)} \\mathrm{d} B_{r}-\\frac{1}{2} \\int_{0}^{t} \\psi_{r}\\left(\\widehat{W}_{r}\\right) \\mathrm{d} r, \\quad B_{r}:=\\rho W_{r}^{1}+\\sqrt{1-\\rho^{2}} W_{r}^{2} \\\\\n\\psi_{t}(x):=\\xi \\exp \\left\\{\\eta x-\\frac{\\eta^{2}}{2} t^{2 H}\\right\\}, \\quad \\widehat{W}_{t}^{H}:=\\sqrt{2 H} \\int_{0}^{t}(t-u)^{H-\\frac{1}{2}} \\mathrm{~d} W_{r}^{1}\n\\end{gathered}\n$$\n\nfor independent Brownian motions $\\left(W^{1}, W^{2}\\right)$. The hybrid scheme of [8] is used for efficient, $\\mathcal{O}(\\ell \\log \\ell)$, simulation of the Volterra process $\\widehat{W}$ where $\\ell$ is the length of the simulated paths. For the experiments we take the same parameters as in the repository ${ }^{2}$, namely $T=1, \\xi=0.055, \\eta=1.9, \\rho=-0.9$. We set the ground truth to be the Monte Carlo prices obtained with 100000 sample paths. We recall that the value function $u$ defined in subsection 2.3.3 (and dubbed $u_{2}$ in the introduction) solves the PPDE\n\n$$\n\\left\\{\\begin{array}{c}\n\\partial_{t} u+\\frac{1}{2} \\psi_{t}\\left(\\gamma_{t}\\right)\\left(\\partial_{x}^{2}-\\partial_{x}\\right) u+\\frac{1}{2} \\widetilde{\\partial}_{\\gamma}^{K^{t} K^{t}} u+\\rho \\sqrt{\\psi_{t}\\left(\\gamma_{t}\\right)} \\partial_{\\gamma}^{K^{t}}\\left(\\partial_{x} u\\right)=0 \\\\\nu(T, x, \\gamma)=\\phi(x)\n\\end{array}\\right.\n$$\n\nTo evaluate the conditional expectation in the Monte Carlo benchmark we remark that, setting\n\n$$\nI_{s}^{t}:=\\int_{t}^{s} K(s, r) \\mathrm{d} W_{r}, \\quad \\text { where } K(s, r)=\\sqrt{2 H}(s-r)^{H-\\frac{1}{2}}\n$$\n\nif $s>t$ and 0 otherwise, the value function $u$ has the following probabilistic representation\n\n$$\nu(t, x, \\gamma)=\\mathbb{E}\\left[f\\left(x+\\int_{t}^{T} \\sqrt{\\psi_{s}\\left(\\gamma_{s}+I_{s}^{t}\\right)} \\mathrm{d} B_{s}-\\frac{1}{2} \\int_{t}^{T} \\psi_{s}\\left(\\gamma_{s}+I_{s}^{t}\\right) \\mathrm{d} s\\right)\\right]\n$$\n\nsince $\\Theta^{t}$ is an $\\mathcal{F}_{t}$ measurable process and $I^{t}$ is independent from $\\mathcal{F}_{t}$. Collocation points for the kernel method were sampled uniformly at random on $[0,1]$ for the time variable, uniformly at random on $\\left[x_{\\min }, x_{\\max }\\right]$ for the price variables, where $x_{\\min }, x_{\\max }$ were chosen using ground truth prices, and sampled from the process $\\Theta^{t}$ for the path variable.\n\nWe consider two values of $H=0.1$ and $H=0.3$ and two strike values $K=0.1$ and $K=1.0$. We take the same number of sample paths and collocation points for Monte Carlo and the signature kernel solver respectively, and consider three different increasing values, namely 20,100 and 500. As it can be observed from the tables, the numerical performance in terms of MSE and MAE of the two algorithms are comparable across the different settings.\n\nThe time complexity of a Monte Carlo scheme scales roughly as $\\mathcal{O}\\left(h^{-2} N d^{2}\\right)$, where $N$ is the number of sample paths, $h$ is the discretisation step and $d$ is the dimension of the process. This complexity can\n\n[^0]\n[^0]:    ${ }^{2}$ https://github.com/ryanmccrickerd/rough_bergomi/tree/master.\n\n$$\nH=0.1-\\text { Strike }=0.1\n$$\n\n![table_0](table_0)\n\n$$\nH=0.1-\\text { Strike }=1.0\n$$\n\n![table_1](table_1)\n\n$$\nH=0.3-\\text { Strike }=0.1\n$$\n\n![table_2](table_2)\n\n$$\nH=0.3-\\text { Strike }=1.0\n$$\n\n![table_3](table_3)\n\nfurther reduced to $\\mathcal{O}\\left((h \\log (h))^{-1} N d^{2}\\right)$ using schemes such as the one proposed by [8]. The complexity for evaluating our solver (once it has been trained offline) scales as $\\mathcal{O}\\left(h^{-2} \\tilde{N} d^{2}\\right)$ where $\\tilde{N}$ is the number of collocation points. This cost can be further reduced to $\\mathcal{O}\\left(h^{-1} \\tilde{N} d^{2}\\right)$ if the operations are carried out on a GPU, see [46] for additional details. It is clear that a transparent comparison of complexities between our approach and Monte Carlo for pricing under rough volatility can only be achieved by obtaining error rates for the convergence of the approximations governing the values of $N$ and $\\tilde{N}$ respectively in the above complexities, which we intend to explore as future work.\n\nNevertheless, the PDE approach provides by design advantages over Monte Carlo methods. Indeedn, finite-dimension greeks are prone to instability, let alone pathwise ones, while all derivatives of the PPDE solution appear from (the linear combination of) standard PDEs.\n5.3. Comparison with neural PDE solvers. As antiticipated, we will conclude this section with some remarks aimed at comparing our kernel approach to the numerous neural network techniques to solve PPDE which have been proposed in the literature in recent years. Neural PDE solvers such as DGM [51] and PINNs [43] essentially parameterise the solution of the PDE as a neural network which is then trained using the PDE and its boundary conditions as loss function. One drawback of\n\nthese approaches in the path-dependent setting $[45,50,31]$ is that they require a time-discretisation of the solution. The time grids over which solutions and derivatives are evaluated during training and testing need to agree, usually followed by an arbitrary interpolation. On the contrary, our kernel approach operates directly on continuous paths, and it is therefore mesh-free. Furthermore, derivatives can be accessed directly by differentiating kernels, with no need to prematurely discretise in time. We note that concomitant to this paper is [23], which uses a neural rough differential equation (Neural RDE) model $[40,48]$ to parameterise the solution of a PPDE and is therefore also mesh-free. Another drawback of neural PDE solvers is that their theoretical analysis is often limited to density or universal approximation results; showing the existence of a network of a requisite size achieving a certain error rate, without guarantees whether this network is computable in practice. In fact the optimisation, done with variants of stochastic gradient descent, is never convex so there is no guarantee to attain a global minimum. Our kernel approach boils down to a convex finite dimensional optimisation, guaranteed to attain the unique global minimum, and that is consistent with the original problem when the number of collocation points is sent to infinity.", "tables": {"table_0": "| Model | Monte Carlo |  |  | SigPPDE |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Complexity | 20 paths | 100 paths | 500 paths | 20 c. pts | 100 c. pts | 500 c. pts |\n| MSE | 0.2070 | 0.0079 | 0.0080 | 0.2084 | 0.0016 | 0.0012 |\n| MAE | 1.3193 | 0.2683 | 0.2300 | 0.6375 | 0.0714 | 0.0672 |", "table_1": "| Model | Monte Carlo |  |  | SigPPDE |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Complexity | 20 paths | 100 paths | 500 paths | 20 c. pts | 100 c. pts | 500 c. pts |\n| MSE | 0.0324 | 0.0013 | 0.0001 | 0.1576 | 0.0092 | 0.0008 |\n| MAE | 0.0679 | 0.0557 | 0.0131 | 0.7998 | 0.2671 | 0.0938 |", "table_2": "| Model | Monte Carlo |  |  | SigPPDE |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Complexity | 20 paths | 100 paths | 500 paths | 20 c. pts | 100 c. pts | 500 c. pts |\n| MSE | 0.0198 | 0.0013 | 0.0005 | 0.1198 | 0.0026 | 0.0002 |\n| MAE | 0.3330 | 0.0718 | 0.0624 | 0.6344 | 0.0973 | 0.0284 |", "table_3": "| Model | Monte Carlo |  |  | SigPPDE |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Complexity | 20 paths | 100 paths | 500 paths | 20 c. pts | 100 c. pts | 500 c. pts |\n| MSE | 0.0113 | 0.0015 | 0.0006 | 0.4201 | 0.0046 | 0.0014 |\n| MAE | 0.2341 | 0.0945 | 0.0679 | 1.1832 | 0.1050 | 0.0905 |"}, "images": {"img-0.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 6, "text": "# 6. OUTLOOK \n\nThe numerical solver presented in this paper competes with Monte Carlo methods in terms of the balance between complexity and accuracy. Our next objective will be to derive error estimates with respect to the number of collocation points in order to carry out a fair comparison. This step is essential for choosing the collocation points in an optimal way and better assess the efficiency of the algorithm. For this purpose we may follow the approach of [4] or decide to regularise our optimisation problem as in [15]. Further extensions include PPDEs arising from general Volterra processes, non-linear PPDEs, and path-dependent payoffs as the VIX example presented in Subsection 2.3.2. Finally, the inverse problem is particularly relevant for financial applications. We expect that the calibration of model parameters can be considerably sped up if the kernel is enhanced with this additional data.", "tables": {}, "images": {}}, {"section_id": 7, "text": "## Appendix A. Proofs of signature and signature kernel estimates\n\nA.1. Differentiability and continuity estimates for the signature. For the sake of uncluttering notations, in this section we fix $t \\in \\mathbb{T}$ and we will write $|\\gamma|_{1}$ instead of $|\\gamma|_{1-\\text { var. }[t, T]}$ and $\\|\\gamma\\|_{0}$ instead of $\\|\\gamma\\|_{0 ;[t, T]}$ for every $\\gamma \\in \\mathrm{BV}_{t}$. We present estimates for the signature and its derivatives, in passing obtaining CDEs satisfied by the latter, and showing continuity with respect to the $\\mathcal{C}_{0}$-norm as claimed in Proposition 3.14. The following lemma is inspiredfrom [25], Theorems 3.15 and 4.4.\n\nLemma A.1. Let Assumption 3.10 hold for $g$. For all $\\gamma, \\bar{\\gamma}, \\eta, \\bar{\\eta} \\in \\mathrm{BV}_{t}$, the signature is two times differentiable. Moreover, there exist continuous functions $\\overline{S_{k}}: \\mathbb{R}_{+}^{k} \\rightarrow \\mathbb{R}_{+}$and $\\overline{D S_{k}}: \\mathbb{R}_{+}^{k} \\rightarrow \\mathbb{R}_{+}$, for $k=1,2,3$, such that the following estimates hold\n\n$$\n\\sup _{t^{\\prime} \\in[t, T]}\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq \\overline{S_{0}}\\left(|\\gamma|_{1}\\right)\n$$\n\n$$\n\\sup _{t^{\\prime} \\in[t, T]}\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}-S\\left(\\bar{\\gamma}^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0} \\overline{D S_{0}}\\left(|\\gamma|_{1},|\\bar{\\gamma}|_{1}\\right)\n$$\n\n$$\n\\sup _{t^{\\prime} \\in[t, T]}\\left\\|\\partial_{\\gamma}^{0} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq \\overline{S_{1}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1}\\right)\n$$\n\n$$\n\\sup _{t^{\\prime} \\in[t, T]}\\left\\|\\partial_{\\gamma}^{0} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}-\\partial_{\\bar{\\gamma}}^{0} S\\left(\\bar{\\gamma}^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0} \\overline{D S_{1}}\\left(|\\gamma|_{1},|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1}\\right)\n$$\n\n$$\n\\sup _{t^{\\prime} \\in[t, T]}\\left\\|\\partial_{\\gamma}^{0 \\bar{\\eta}} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq \\overline{S_{2}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)\n$$\n\n$$\n\\sup _{t^{\\prime} \\in[t, T]}\\left\\|\\partial_{\\gamma}^{0 \\bar{\\eta}} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}-\\partial_{\\bar{\\gamma}}^{0 \\bar{\\eta}} S\\left(\\bar{\\gamma}^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0} \\overline{D S_{2}}\\left(|\\gamma|_{1},|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)\n$$\n\nIn particular, $\\partial_{\\gamma}^{0} S \\circ G$ and $\\partial_{\\gamma}^{0 \\bar{\\eta}} S \\circ G$ are Fr\u00e9chet derivatives.\n\nProof. Let $0 \\leq t \\leq t^{\\prime} \\leq T$ and $\\gamma, \\bar{\\gamma}, \\eta, \\bar{\\eta} \\in \\mathrm{BV}_{t}$.\n\n1) As in Lemma 3.2, the signature on lifted paths satisfies the CDE\n\n$$\nS\\left(\\gamma^{g}\\right)_{t^{\\prime}}=\\mathbf{1}+\\int_{t}^{t^{\\prime}} S(\\gamma)_{u} \\cdot \\mathrm{~d} \\gamma_{u}^{g}=\\mathbf{1}+\\int_{t}^{t^{\\prime}} S(\\gamma)_{u} \\cdot \\nabla g\\left(\\gamma_{u}, \\cdot\\right) \\mathrm{d} \\gamma_{u}\n$$\n\nhence we have the bound\n\n$$\n\\begin{aligned}\n\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} & \\leq \\mathbf{1}+\\int_{t}^{t^{\\prime}}\\left\\|S(\\gamma)_{u}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\cdot\\left\\|\\nabla g\\left(\\gamma_{u}, \\cdot\\right) \\mathrm{d} \\gamma_{u}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& \\leq \\mathbf{1}+\\int_{t}^{t^{\\prime}}\\left\\|S(\\gamma)_{u}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}\\left\\|\\nabla g\\left(\\gamma_{u}, \\cdot\\right)\\right\\|_{\\mathcal{H}_{g}}\\left|\\mathrm{~d} \\gamma_{u}\\right|\n\\end{aligned}\n$$\n\nwhere $\\left\\|\\nabla g\\left(\\gamma_{u}, \\cdot\\right)\\right\\|_{\\mathcal{H}_{g}}=\\sum_{i, j=1}^{e} \\partial_{x^{i} y^{j}}^{2} g\\left(\\gamma_{u}, \\gamma_{u}\\right) \\leq \\overline{g_{11}}$ and $\\left|\\mathrm{d} \\gamma_{u}\\right|=\\left|\\dot{\\gamma}_{u}\\right| \\mathrm{d} u$. By Gr\u00f6nwall's lemma 25, Lemma 3.2] this yields\n\n$$\n\\sup _{t^{\\prime} \\in[t, T]}\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq \\mathrm{e}^{\\overline{g_{11}}\\left|\\gamma_{1}\\right|}=: \\overline{S_{0}}(\\gamma)\n$$\n\n2) Furthermore, 25, Proposition 2.9] gives the 1-variation bound\n\n$$\n\\left|\\left\\|S\\left(\\gamma^{g}\\right) \\cdot\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}\\right|_{1} \\leq \\overline{S_{0}}(\\gamma) \\overline{g_{11}}|\\gamma|_{1}\n$$\n\nFor any $\\bar{\\gamma} \\in \\mathrm{BV}_{t}$, imitating the proof of 25 , Theorem 3.15] yields\n\n$$\n\\begin{aligned}\n\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}-S\\left(\\bar{\\gamma}^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} & \\leq \\int_{t}^{t^{\\prime}}\\left\\|S\\left(\\gamma^{g}\\right)_{u}-S\\left(\\bar{\\gamma}^{g}\\right)_{u}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\mid \\mathrm{d} \\gamma_{u}^{g} \\mid+\\left\\|\\int_{t}^{t^{\\prime}} S\\left(\\bar{\\gamma}^{g}\\right) \\mathrm{d}\\left(\\gamma^{g}-\\bar{\\gamma}^{g}\\right)_{u}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& \\leq \\int_{t}^{t^{\\prime}}\\left\\|S\\left(\\gamma^{g}\\right)_{u}-S\\left(\\bar{\\gamma}^{g}\\right)_{u}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\overline{g_{11}} \\mid \\mathrm{d} \\gamma_{u} \\mid+\\left\\|\\left\\|\\gamma^{g}-\\bar{\\gamma}^{g}\\right\\|_{\\mathcal{H}_{g}}\\right\\|_{0}\\left(1+\\left|\\left\\|S\\left(\\bar{\\gamma}^{g}\\right) \\cdot\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}\\right|_{1}\\right)\n\\end{aligned}\n$$\n\nThus, applying the inequality $\\left\\|\\left\\|\\gamma^{g}-\\bar{\\gamma}^{g}\\right\\|_{\\mathcal{H}_{g}}\\right\\|_{0} \\leq \\overline{g_{11}}\\|\\gamma-\\bar{\\gamma}\\|_{0}$ and Gr\u00f6nwall's lemma we obtain\n\n$$\n\\sup _{t^{\\prime} \\in[t, T]}\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}-S\\left(\\bar{\\gamma}^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0} \\overline{g_{11}}\\left(1+\\overline{S_{0}}(\\bar{\\gamma}) \\overline{g_{11}}|\\bar{\\gamma}|_{1}\\right) \\mathrm{e}^{\\overline{g_{11}}\\left|\\gamma_{1}\\right|}=:\\|\\gamma-\\bar{\\gamma}\\|_{0} \\overline{D S_{0}}\\left(|\\gamma|_{1},|\\bar{\\gamma}|_{1}\\right)\n$$\n\nwhere $\\overline{D S_{0}}: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}$ is a continuous function.\n3) We continue with the following observation. For all $u \\in[t, T]$ we have\n\n$$\n\\begin{aligned}\n\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon} \\mathrm{~d}(\\gamma+\\varepsilon \\eta)_{u}^{g}-\\gamma_{u}^{g} & =\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon} \\sum_{i=1}^{e}\\left\\{\\partial_{i} g\\left(\\gamma_{u}+\\varepsilon \\eta_{u}, \\cdot\\right) \\mathrm{d}\\left(\\gamma_{u}^{i}+\\varepsilon \\eta_{u}^{i}\\right)-\\partial_{i} g\\left(\\gamma_{u}, \\cdot\\right) \\mathrm{d} \\gamma_{u}^{i}\\right\\} \\\\\n& =\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon} \\sum_{i=1}^{e}\\left\\{\\partial_{i} g\\left(\\gamma_{u}+\\varepsilon \\eta_{u}, \\cdot\\right)-\\partial_{i} g\\left(\\gamma_{u}, \\cdot\\right)\\right\\} \\mathrm{d} \\gamma_{u}^{i}+\\lim _{\\varepsilon \\rightarrow 0} \\sum_{i=1}^{e} \\partial_{i} g\\left(\\gamma_{u}+\\varepsilon \\eta_{u}, \\cdot\\right) \\mathrm{d} \\eta_{u}^{i} \\\\\n& =\\sum_{i=1}^{e}\\left\\{\\partial_{\\gamma}^{u} \\partial_{x^{i}} g\\left(\\gamma_{u}, \\cdot\\right) \\mathrm{d} \\gamma_{u}^{i}+\\partial_{x^{i}} g\\left(\\gamma_{u}, \\cdot\\right) \\mathrm{d} \\eta_{u}^{i}\\right\\} \\\\\n& =\\mathrm{d}\\left\\{\\sum_{i=1}^{e} \\partial_{x^{i}} g\\left(\\gamma_{u}, \\cdot\\right) \\eta_{u}^{i}\\right\\} \\\\\n& =\\mathrm{d}\\left\\{\\partial_{\\gamma}^{u} G(\\gamma)_{u}\\right\\}\n\\end{aligned}\n$$\n\nwhere we recall that $G(\\gamma)_{t}=g\\left(\\gamma_{t}, \\cdot\\right)$. Further, we note that, for all $\\varepsilon \\in(0,1)$,\n\n$$\n\\frac{1}{\\varepsilon} \\sup _{t^{\\prime} \\in[t, T]}\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}-S\\left((\\gamma+\\varepsilon \\eta)^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq\\|\\eta\\|_{0} \\overline{D S_{0}}\\left(|\\gamma|_{1},|\\gamma|_{1}+|\\eta|_{1}\\right)\n$$\n\nHence, by dominated convergence and (A.8), we get\n\n$$\n\\begin{aligned}\n\\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)_{t^{\\prime}} & =\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left(S\\left((\\gamma+\\varepsilon \\eta)^{g}\\right)_{t^{\\prime}}-S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right) \\\\\n& =\\int_{t}^{t^{\\prime}} \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left(S\\left((\\gamma+\\varepsilon \\eta)^{g}\\right)_{u}-S\\left(\\gamma^{g}\\right)_{u}\\right) \\cdot \\mathrm{d} \\gamma_{u}^{g}+\\int_{t}^{t^{\\prime}} \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon} S\\left(\\gamma^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left((\\gamma+\\varepsilon \\eta)^{g}\\right)_{u}-\\gamma_{u}^{g}\\right) \\\\\n& =\\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)_{u} \\cdot \\mathrm{~d} \\gamma_{u}^{g}+\\int_{t}^{t^{\\prime}} S\\left(\\gamma^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left\\{\\partial_{\\gamma}^{n} G(\\gamma)_{u}\\right\\}\n\\end{aligned}\n$$\n\nNote that $\\left\\|\\partial_{x^{i j}}^{2} g\\left(\\gamma_{u},\\cdot\\right)\\right\\|_{\\mathcal{H}_{g}}=\\partial_{x^{i j} y^{i j}}^{4} g\\left(\\gamma_{u}, \\gamma_{u}\\right)$, and hence\n\n$$\n\\begin{aligned}\n\\int_{t}^{t^{\\prime}}\\left\\|\\mathrm{d}\\left\\{\\partial_{\\gamma}^{n} G(\\gamma)_{u}\\right\\}\\right\\|_{\\mathcal{H}_{g}} & \\leq \\int_{t}^{t^{\\prime}} \\sum_{i, j=1}^{e}\\left\\|\\partial_{x^{i j}} g\\left(\\gamma_{u},\\cdot\\right)\\right\\|_{\\mathcal{H}_{g}}\\left|\\eta_{n}\\right|\\left|\\mathrm{d} \\gamma_{u}\\right|+\\int_{t}^{t^{\\prime}} \\sum_{i=1}^{e}\\left\\|\\partial_{x^{i}} g\\left(\\gamma_{u},\\cdot\\right)\\right\\|_{\\mathcal{H}_{g}}\\left|\\mathrm{~d} \\eta_{u}\\right| \\\\\n& \\leq \\overline{g_{22}}\\|\\eta\\|_{0}|\\gamma|_{1}+\\overline{g_{11}}|\\eta|_{1} \\leq \\overline{G_{1}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1}\\right)\n\\end{aligned}\n$$\n\nwhere $\\overline{G_{1}}: \\mathbb{R}_{+}^{2} \\rightarrow \\mathbb{R}_{+}$is a continuous function. Applying Gr\u00f6nwall's lemma as in the previous steps yields the estimate\n\n$$\n\\left\\|\\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq \\overline{S_{0}}\\left(|\\gamma|_{1}\\right) \\overline{G_{1}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1}\\right) \\mathrm{e}^{\\overline{g_{11}}|\\gamma|_{1}}=: \\overline{S_{1}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1}\\right)\n$$\n\n4) Noting that, for all $t \\leq s^{\\prime} \\leq t^{\\prime} \\leq T$,\n\n$$\n\\left|\\left\\|\\int_{t} S\\left(\\gamma^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left\\{\\partial_{\\gamma}^{n} G(\\gamma)_{u}\\right\\}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}\\right|_{1} \\leq \\overline{S_{0}}(\\gamma)\\left|\\int_{t}\\left\\|\\mathrm{~d}\\left\\{\\partial_{\\gamma}^{n} G(\\gamma)_{u}\\right\\}\\right\\|_{\\mathcal{H}_{g}}\\right|_{1} \\leq \\overline{S_{0}}(\\gamma) \\overline{G_{1}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1}\\right)\n$$\n\nProposition 2.9 of [25] thus gives\n\n$$\n\\left|\\left\\|\\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}\\right|_{1} \\leq \\overline{S_{1}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1}\\right) \\overline{g_{11}}|\\gamma|_{1}+\\overline{S_{0}}(\\gamma) \\overline{G_{1}}\\left(|\\gamma|_{1},\\|\\eta\\|_{1}\\right)\n$$\n\nThen, from the integration by parts formula we observe that\n\n$$\n\\int_{t}^{t^{\\prime}}\\left|\\eta_{u}\\right| \\mathrm{d}\\left(\\left|\\gamma_{u}-\\bar{\\gamma}_{u}\\right|\\right)=\\int_{t}^{t^{\\prime}}\\left|\\gamma_{u}-\\bar{\\gamma}_{u}\\right| \\mathrm{d}\\left|\\eta_{u}\\right|+\\left|\\eta_{t}\\right|\\left|\\gamma_{t}-\\bar{\\gamma}_{t}\\right| \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0}|\\eta|_{1}+\\|\\eta\\|_{0}\\|\\gamma-\\bar{\\gamma}\\|_{0}\n$$\n\nThis allows us to study the following regularity, leveraging the uniform bounds of $g$ 's derivatives:\n\n$$\n\\begin{aligned}\n& \\int_{t}^{t^{\\prime}}\\left\\|\\mathrm{d}\\left\\{\\partial_{\\gamma}^{n}\\left(G(\\gamma)_{u}-G(\\bar{\\gamma})_{u}\\right)\\right\\}\\right\\|_{\\mathcal{H}_{g}} \\leq \\int_{t}^{t^{\\prime}} \\sum_{i, j=1}^{e}\\left\\|\\partial_{x^{i j}}\\left(g\\left(\\gamma_{u}, \\cdot\\right)-g\\left(\\bar{\\gamma}_{u}, \\cdot\\right)\\right)\\right\\|_{\\mathcal{H}_{g}}\\left|\\eta_{u}\\right|\\left|\\mathrm{d} \\gamma_{u}\\right| \\\\\n& +\\int_{t}^{t^{\\prime}} \\sum_{i, j=1}^{e}\\left\\|\\partial_{x^{i j}} g\\left(\\bar{\\gamma}_{u}, \\cdot\\right)\\right\\|_{\\mathcal{H}_{g}}\\left|\\eta_{u}\\right|\\left|\\mathrm{d}\\left(\\gamma_{u}-\\bar{\\gamma}_{u}\\right)\\right|+\\int_{t}^{t^{\\prime}} \\sum_{i=1}^{e}\\left\\|\\partial_{x^{i}}\\left(g\\left(\\gamma_{u}, \\cdot\\right)-g\\left(\\bar{\\gamma}_{u}, \\cdot\\right)\\right)\\right\\|_{\\mathcal{H}_{g}}\\left|\\mathrm{~d} \\eta_{u}\\right| \\\\\n& \\leq \\overline{g_{33}}\\|\\gamma-\\bar{\\gamma}\\|_{0}\\|\\eta\\|_{0}|\\gamma|_{1}+\\overline{g_{22}} \\int_{t}^{t^{\\prime}}\\left|\\eta_{u}\\right| \\mathrm{d}\\left(\\left|\\gamma_{u}-\\bar{\\gamma}_{u}\\right|\\right)+\\overline{g_{22}}\\|\\gamma-\\bar{\\gamma}\\|_{0}|\\eta|_{1} \\\\\n& \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0} \\overline{D G_{1}}\\left(|\\gamma|_{1},|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1}\\right)\n\\end{aligned}\n$$\n\nwhere $\\overline{D G_{1}}: \\mathbb{R}_{+}^{3} \\rightarrow \\mathbb{R}_{+}$is a continuous function. We deduce that the difference of the source terms in (A.9) has the following bound\n\n$$\n\\begin{aligned}\n& \\left\\|\\int_{t}^{t^{\\prime}} S\\left(\\gamma^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left\\{\\partial_{\\gamma}^{n} G(\\gamma)_{u}\\right\\}-\\int_{t}^{t^{\\prime}} S\\left(\\bar{\\gamma}^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left\\{\\partial_{\\gamma}^{n} G(\\bar{\\gamma})_{u}\\right\\}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& \\leq \\sup _{t^{\\prime} \\in[t, T]}\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}-S\\left(\\bar{\\gamma}^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\overline{G_{1}}(\\gamma, \\eta)+\\sup _{t^{\\prime} \\in[t, T]}\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\int_{t}^{t^{\\prime}}\\left\\|\\mathrm{d}\\left\\{\\partial_{\\gamma}^{n}\\left(G(\\gamma)_{u}-G(\\bar{\\gamma})_{u}\\right)\\right\\}\\right\\|_{\\mathcal{H}_{g}} \\\\\n& \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0}\\left(\\overline{D S_{0}}\\left(\\|\\gamma\\|_{1},|\\bar{\\gamma}|_{1}\\right) \\overline{G_{1}}\\left(\\|\\gamma\\|_{1},\\|\\eta\\|_{1}\\right)+\\overline{S_{0}}(\\gamma) \\overline{D G_{1}}\\left(\\|\\gamma\\|_{1},|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1}\\right)\\right)\n\\end{aligned}\n$$\n\nIn virtue of (A.9) and (A.11), the same arguments as in the proof of [25, Theorem 3.15] entails there exists a continuous function $\\overline{D S_{1}}: \\mathbb{R}_{+}^{3} \\rightarrow \\mathbb{R}_{+}$such that\n\n$$\n\\begin{aligned}\n\\sup _{t^{\\prime} \\in[t, T]} & \\left\\|\\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}-\\partial_{\\gamma}^{n} S\\left(\\bar{\\gamma}^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& \\leq \\mathrm{e}^{2 \\overline{g_{11}}\\left(\\|\\gamma\\|_{1} \\vee\\|\\bar{\\gamma}\\|_{1}\\right)}\\left\\|\\int_{t}^{t^{\\prime}} S\\left(\\gamma^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left\\{\\partial_{\\gamma}^{n} G(\\gamma)_{u}\\right\\}-\\int_{t}^{t^{\\prime}} S\\left(\\bar{\\gamma}^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left\\{\\partial_{\\gamma}^{n} G(\\bar{\\gamma})_{u}\\right\\}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0} \\overline{D S_{1}}\\left(\\|\\gamma\\|_{1},|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1}\\right)\n\\end{aligned}\n$$\n\n5) The latest estimate allows to use dominated convergence one more time and compute the second derivative in the direction of $\\eta, \\bar{\\eta}$ :\n\n$$\n\\partial_{\\gamma}^{\\eta \\bar{\\eta}} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}=\\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} S\\left(\\gamma^{g}\\right)_{u} \\mathrm{~d} \\gamma_{u}^{g}+\\operatorname{Source}(\\gamma, \\eta, \\bar{\\eta})\n$$\n\nwhere\n$\\operatorname{Source}(\\gamma, \\eta, \\bar{\\eta}):=\\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left\\{\\partial_{\\gamma}^{n} G(\\gamma)_{u}\\right\\}+\\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{n} S\\left(\\gamma^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left\\{\\partial_{\\gamma}^{n} G(\\gamma)_{u}\\right\\}+\\int_{t}^{t^{\\prime}} S\\left(\\gamma^{g}\\right)_{u} \\cdot \\mathrm{~d}\\left\\{\\partial_{\\gamma}^{\\eta \\bar{\\eta}} G(\\gamma)_{u}\\right\\}$.\nUsing the same arguments as in (A.8) one deduces that\n\n$$\n\\begin{aligned}\n\\int_{t}^{t^{\\prime}}\\left\\|\\mathrm{d}\\left\\{\\partial_{\\gamma}^{\\eta \\bar{\\eta}} G(\\gamma)_{u}\\right\\}\\right\\|_{\\mathcal{H}_{g}} \\leq & \\int_{t}^{t^{\\prime}} \\sum_{i, j, k=1}^{c}\\left\\|\\partial_{x^{i j k}}^{3} g\\left(\\gamma_{u}, \\cdot\\right)\\right\\|_{\\mathcal{H}_{g}}\\left|\\bar{\\eta}_{u}\\right|\\left|\\eta_{u}\\right|\\left|\\mathrm{d} \\gamma_{u}\\right| \\\\\n& +\\int_{t}^{t^{\\prime}} \\sum_{i, j=1}^{c}\\left\\|\\partial_{x^{i j}}^{2} g\\left(\\gamma_{u}, \\cdot\\right)\\right\\|_{\\mathcal{H}_{g}}\\left(\\left|\\eta_{u}\\right|\\left|\\mathrm{d} \\bar{\\eta}_{u}\\right|+\\left|\\bar{\\eta}_{u}\\right|\\left|\\mathrm{d} \\eta_{u}\\right|\\right) \\\\\n\\leq & \\overline{g_{31}}\\|\\eta\\|_{0}\\|\\bar{\\eta}\\|_{0}|\\gamma|_{1}+\\overline{g_{22}}\\left(\\|\\eta\\|_{0}|\\bar{\\eta}|_{1}+\\|\\bar{\\eta}\\|_{0}|\\eta|_{1}\\right) \\leq \\overline{G_{2}}\\left(\\|\\gamma\\|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)\n\\end{aligned}\n$$\n\nwhere $\\overline{G_{2}}: \\mathbb{R}_{+}^{3} \\rightarrow \\mathbb{R}_{+}$is a continuous function. Gr\u00f6nwall's inequality once again yields the bound\n\n$$\n\\begin{aligned}\n& \\sup _{t^{\\prime} \\in[t, T]}\\left\\|\\partial_{\\gamma}^{\\eta \\bar{\\eta}} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& \\leq\\left(\\overline{S_{1}}\\left(\\|\\gamma\\|_{1},\\|\\eta\\|_{1}\\right) \\overline{G_{1}}\\left(\\|\\gamma\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)+\\overline{S_{1}}\\left(\\|\\gamma\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right) \\overline{G_{1}}\\left(\\|\\gamma\\|_{1},\\|\\eta\\|_{1}\\right)+\\overline{S_{0}}\\left(\\|\\gamma\\|_{1}\\right) \\overline{G_{2}}\\left(\\|\\gamma\\|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)\\right) \\mathrm{e}^{\\overline{g_{11}}\\|\\gamma\\|_{1}} \\\\\n& =: \\overline{S_{2}}\\left(\\|\\gamma\\|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)\n\\end{aligned}\n$$\n\n6) The same steps as for the first derivative yield the existence of a continuous function $\\overline{D G_{2}}: \\mathbb{R}^{4} \\rightarrow \\mathbb{R}$ such that\n\n$$\n\\int_{t}^{t^{\\prime}}\\left\\|\\mathrm{d}\\left\\{\\partial_{\\gamma}^{\\eta \\bar{\\eta}}\\left(G(\\gamma)_{u}-G(\\bar{\\gamma})_{u}\\right)\\right\\}\\right\\|_{\\mathcal{H}_{g}} \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0} \\overline{D G_{2}}\\left(\\|\\gamma\\|_{1},|\\bar{\\gamma}|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)\n$$\n\nHence the difference of the source term for two paths $\\gamma, \\bar{\\gamma}$ can be bounded as follows\n\n$$\n\\begin{aligned}\n& \\left\\|\\operatorname{Source}(\\gamma, \\eta, \\bar{\\eta})-\\operatorname{Source}(\\bar{\\gamma}, \\eta, \\bar{\\eta})\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\\\\n& \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0}\\left(\\overline{S_{1}}\\left(\\left|\\gamma\\right|_{1},\\|\\eta\\|_{1}\\right) \\overline{D G_{1}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1},\\|\\eta\\|_{1}\\right)+\\overline{G_{1}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1},\\|\\eta\\|_{1}\\right) \\overline{D S_{1}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1},\\|\\eta\\|_{1}\\right) \\\\\n& \\quad+\\overline{S_{1}}\\left(\\left|\\gamma\\right|_{1},\\|\\bar{\\eta}\\|_{1}\\right) \\overline{D G_{1}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1},\\|\\bar{\\eta}\\|_{1}\\right)+\\overline{G_{1}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1},\\|\\bar{\\eta}\\|_{1}\\right) \\overline{D S_{1}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1},\\|\\bar{\\eta}\\|_{1}\\right) \\\\\n& \\quad+\\overline{S_{0}}\\left(\\left|\\gamma\\right|_{1}\\right) \\overline{G_{2}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)+\\overline{D S_{0}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1}\\right) \\overline{G_{2}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)\\right) .\n\\end{aligned}\n$$\n\nFinally, by [25, Theorem 3.15] again there is a continuous function $\\overline{D S_{2}}: \\mathbb{R}^{4} \\rightarrow \\mathbb{R}$ such that\n\n$$\n\\sup _{t^{\\prime} \\in[t, T]}\\left\\|\\partial_{\\gamma}^{\\eta \\bar{\\eta}} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}-\\partial_{\\gamma}^{\\eta \\bar{\\eta}} S\\left(\\bar{\\gamma}^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq\\|\\gamma-\\bar{\\gamma}\\|_{0} \\overline{D S_{2}}\\left(\\left|\\gamma\\right|_{1},\\left|\\bar{\\gamma}\\right|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right)\n$$\n\nthus concluding the proof.\nA.2. Differentiability and continuity estimates for the signature kernel. We start by stating estimates for the signature kernel and its derivatives, which follow from Lemma A.1. The PDE formulations in Proposition 3.11 are immediate corollaries. In terms of notations, for two paths $\\gamma \\in \\mathrm{BV}_{t}$ and $\\tau \\in \\mathrm{BV}_{s}$, we will denote $\\left|\\gamma\\right|_{1}=\\left|\\gamma\\right|_{1-\\text { var } ;[t, T]}$ and $\\left|\\tau\\right|_{1}=\\left|\\tau\\right|_{1-\\text { var } ;[s, T]}$.\n\nLemma A.2. Let Assumption and 3.10 hold for $g$. Let $s, t \\in \\mathbb{T}$, the paths $\\gamma, \\eta, \\bar{\\eta} \\in \\mathrm{BV}_{t}$ and $\\tau \\in \\mathrm{BV}_{s}$. Then the following estimates hold:\n\n$$\n\\begin{aligned}\n& \\sup _{s^{\\prime} \\in[s, T], t^{\\prime} \\in[t, T]}\\left|\\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}\\right| \\leq \\overline{S_{0}}\\left(\\left|\\gamma\\right|_{1}\\right) \\overline{S_{0}}\\left(\\left|\\tau\\right|_{1}\\right) ; \\\\\n& \\sup _{s^{\\prime} \\in[s, T], t^{\\prime} \\in[t, T]}\\left|\\partial_{\\gamma}^{\\eta} \\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}\\right| \\leq \\overline{S_{1}}\\left(\\left|\\gamma\\right|_{1},\\|\\eta\\|_{1}\\right) \\overline{S_{0}}\\left(\\left|\\tau\\right|_{1}\\right) ; \\\\\n& \\sup _{s^{\\prime} \\in[s, T], t^{\\prime} \\in[t, T]}\\left|\\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}\\right| \\leq \\overline{S_{2}}\\left(\\left|\\gamma\\right|_{1},\\|\\eta\\|_{1},\\|\\bar{\\eta}\\|_{1}\\right) \\overline{S_{0}}\\left(\\left|\\tau\\right|_{1}\\right) .\n\\end{aligned}\n$$\n\nProof. Let $0 \\leq s \\leq s^{\\prime} \\leq T, 0 \\leq t \\leq t^{\\prime} \\leq T$ and $\\gamma, \\eta, \\bar{\\eta} \\in \\mathrm{BV}_{t}, \\tau \\in \\mathrm{BV}_{s}$. By the definition of the signature kernel, Cauchy-Schwarz inequality and (A.1) we have\n\n$$\n\\left|\\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}\\right|=\\left|\\left\\langle S\\left(\\gamma^{g}\\right)_{t^{\\prime}}, S\\left(\\tau^{g}\\right)_{s^{\\prime}}\\right\\rangle\\right| \\leq\\left\\|S\\left(\\gamma^{g}\\right)_{t^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}}\\left\\|S\\left(\\tau^{g}\\right)_{s^{\\prime}}\\right\\|_{\\overline{T\\left(\\mathcal{H}_{g}\\right)}} \\leq \\overline{S_{0}}\\left(\\left|\\gamma\\right|_{1}\\right) \\overline{S_{0}}\\left(\\left|\\tau\\right|_{1}\\right)\n$$\n\nThen for any $\\varepsilon>0$, Cauchy-Schwarz inequality and (A.2) yield\n\n$$\n|1| \\varepsilon\\left|\\kappa^{g}(\\gamma+\\varepsilon \\eta, \\tau)_{s^{\\prime}, t^{\\prime}}-\\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}\\right| \\leq\\|\\eta\\|_{0} \\overline{D S_{0}}\\left(\\left|\\gamma\\right|_{1},\\left|\\gamma\\right|_{1}+\\left|\\eta\\right|_{1}\\right) \\overline{S_{0}}\\left(\\left|\\tau\\right|_{1}\\right)\n$$\n\nThis dominated convergence argument combined with Cauchy-Schwarz inequality and (A.3) entail\n\n$$\n\\left|\\partial_{\\gamma}^{\\eta} \\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}\\right|=\\left|\\left\\langle\\partial_{\\eta}^{\\gamma} S\\left(\\gamma^{g}\\right)_{t^{\\prime}}, S\\left(\\tau^{g}\\right)_{s^{\\prime}}\\right\\rangle\\right| \\leq \\overline{S_{1}}\\left(\\left|\\gamma\\right|_{1},\\|\\eta\\|_{1}\\right) \\overline{S_{0}}\\left(\\left|\\tau\\right|_{1}\\right)\n$$\n\nThe same ideas coupled with the bounds (A.5) and (A.6) lead to the last estimate.\nProof of Proposition 3.11. Let $0 \\leq s \\leq s^{\\prime} \\leq T$ and $0 \\leq t \\leq t^{\\prime} \\leq T$ and $\\gamma, \\eta, \\bar{\\eta} \\in \\mathrm{BV}_{t}, \\tau \\in \\mathrm{BV}_{s}$.\n\n1) In virtue of (A.2), dominated convergence allows to push the limit inside the integrals:\n\n$$\n\\begin{aligned}\n\\partial_{\\gamma}^{\\eta} \\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}= & \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left(\\kappa^{g}(\\gamma+\\varepsilon \\eta, \\tau)_{s^{\\prime}, t^{\\prime}}-\\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}\\right) \\\\\n= & \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon} \\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}}\\left(\\kappa^{g}(\\gamma+\\varepsilon \\eta, \\tau)_{u, v}\\left\\langle\\mathrm{~d}(\\gamma+\\varepsilon \\eta)_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}-\\kappa^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\gamma_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}\\right) \\\\\n= & \\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left(\\kappa^{g}(\\gamma+\\varepsilon \\eta, \\tau)_{u, v}-\\kappa^{g}(\\gamma, \\tau)_{u, v}\\right)\\left\\langle\\mathrm{d}(\\gamma+\\varepsilon \\eta)_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}} \\\\\n& +\\kappa^{g}(\\gamma, \\tau)_{u, v} \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left\\langle\\mathrm{~d}(\\gamma+\\varepsilon \\eta)_{u}^{g}-\\mathrm{d} \\gamma_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}} \\\\\n= & \\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}}\\left(\\partial_{\\gamma}^{\\eta} \\kappa^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\gamma_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}+\\kappa^{g}(\\gamma, \\tau)_{u, v} \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left\\langle\\mathrm{~d}(\\gamma+\\varepsilon \\eta)_{u}^{g}-\\mathrm{d} \\gamma_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}\\right) .\n\\end{aligned}\n$$\n\nFor all $u \\in[t, T]$ and $v \\in[s, T]$ we have similarly to (A.8)\n\n$$\n\\begin{aligned}\n& \\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left\\langle\\mathrm{~d}(\\gamma+\\varepsilon \\eta)_{u}^{g}-\\gamma_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}} \\\\\n& =\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon} \\sum_{i, k=1}^{e}\\left\\langle\\partial_{i} g\\left(\\gamma_{u}+\\varepsilon \\eta_{u}, \\cdot\\right) \\mathrm{d}\\left(\\gamma_{u}^{i}+\\varepsilon \\eta_{u}^{i}\\right)-\\partial_{i} g\\left(\\gamma_{u}, \\cdot\\right) \\mathrm{d} \\gamma_{u}^{i}, \\partial_{k} g\\left(\\tau_{v}, \\cdot\\right) \\mathrm{d} \\tau_{v}^{k}\\right\\rangle \\\\\n& =\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon} \\sum_{i, k=1}^{e}\\left\\langle\\partial_{i} g\\left(\\gamma_{u}+\\varepsilon \\eta_{u}, \\cdot\\right)-\\partial_{i} g\\left(\\gamma_{u}, \\cdot\\right), \\partial_{k} g\\left(\\tau_{v}, \\cdot\\right)\\right\\rangle \\mathrm{d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}+\\lim _{\\varepsilon \\rightarrow 0} \\sum_{i, k=1}^{e}\\left\\langle\\partial_{i} g\\left(\\gamma_{u}+\\varepsilon \\eta_{u}, \\cdot\\right), \\partial_{k} g\\left(\\tau_{v}, \\cdot\\right)\\right\\rangle \\mathrm{d} \\eta_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& =\\sum_{i, k=1}^{e} \\lim _{\\varepsilon \\rightarrow 0}\\left\\{\\frac{1}{\\varepsilon} \\partial_{x^{i}} \\partial_{y^{k}}\\left(g\\left(\\gamma_{u}+\\varepsilon \\eta_{u}, \\tau_{v}\\right)-g\\left(\\gamma_{u}, \\tau_{v}\\right)\\right) \\mathrm{d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}+\\partial_{x^{i}} \\partial_{y^{k}} g\\left(\\gamma_{u}+\\varepsilon \\eta_{u}, \\tau_{v}\\right) \\mathrm{d} \\eta_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}\\right\\} \\\\\n& =\\sum_{i, k=1}^{e}\\left\\{\\partial_{\\gamma}^{a} \\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\gamma_{u}^{i}+\\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\eta_{u}^{i}\\right\\} \\mathrm{d} \\tau_{v}^{k}\n\\end{aligned}\n$$\n\n$$\n=\\sum_{i, k=1}^{e}\\left\\{\\sum_{j=1}^{e} \\partial_{x^{i j} y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\eta_{u}^{j} \\mathrm{~d} \\gamma_{u}^{i}+\\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\eta_{u}^{i}\\right\\} \\mathrm{d} \\tau_{v}^{k}\n$$\n\nwhere we used the reproducing property of derivatives of $g$ (3.13) and we can swap partial derivatives because $g(\\cdot, x) \\in \\mathcal{C}^{2}\\left(\\mathbb{R}^{e}, \\mathbb{R}\\right)$ for all $x \\in \\mathbb{R}^{e}$. Unrolling in the other direction one can also obtain\n\n$$\n\\begin{aligned}\n\\sum_{i, k=1}^{e}\\left\\{\\sum_{j=1}^{e} \\partial_{x^{i j} y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\eta_{u}^{j} \\mathrm{~d} \\gamma_{u}^{i}+\\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\eta_{u}^{i}\\right\\} \\mathrm{d} \\tau_{v}^{k} & =\\left\\langle\\mathrm{d}\\left\\{\\sum_{i=1}^{e} \\partial_{x^{i}} g\\left(\\gamma_{u}, \\cdot\\right) \\eta_{u}^{i}\\right\\}, \\mathrm{d} \\tau_{v}^{g}\\right\\rangle \\\\\n& =\\left\\langle\\mathrm{d}\\left\\{\\partial_{\\gamma}^{a} G(\\gamma)_{u}\\right\\}, \\mathrm{d} \\tau_{v}^{g}\\right\\rangle\n\\end{aligned}\n$$\n\nThe expressions (A.15) and (A.14) yield (3.16) and (3.18) respectively.\n2) We take the terms of (3.18) one by one to compute\n\n$$\n\\begin{aligned}\n& \\partial_{\\gamma}^{a q} \\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}:=\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left(\\partial_{\\gamma}^{a} \\kappa^{g}(\\gamma+\\varepsilon \\bar{\\eta}, \\tau)_{s^{\\prime}, t^{\\prime}}-\\partial_{\\gamma}^{a} \\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}\\right) \\\\\n& =\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left\\{\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{a} \\kappa^{g}(\\gamma+\\varepsilon \\bar{\\eta}, \\tau)_{u, v} \\sum_{i, k=1}^{e} \\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}+\\varepsilon \\bar{\\eta}_{u}, \\tau_{v}\\right) \\mathrm{d}\\left(\\gamma_{u}^{i}+\\varepsilon \\bar{\\eta}_{u}^{i}\\right) \\mathrm{d} \\tau_{v}^{k}\\right. \\\\\n& \\left.-\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{a} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, k=1}^{e} \\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}\\right\\} \\\\\n& +\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left\\{\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa^{g}(\\gamma+\\varepsilon \\bar{\\eta}, \\tau)_{u, v} \\sum_{i, j, k=1}^{e} \\partial_{x^{i j} y^{k}}^{3} g\\left(\\gamma_{u}+\\varepsilon \\bar{\\eta}_{u}, \\tau_{v}\\right) \\eta_{u}^{j} \\mathrm{~d}\\left(\\gamma_{u}^{i}+\\varepsilon \\bar{\\eta}_{u}^{i}\\right) \\mathrm{d} \\tau_{v}^{k}\\right. \\\\\n& \\left.-\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, j, k=1}^{e} \\partial_{x^{i j} y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\eta_{u}^{j} \\mathrm{~d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}\\right\\} \\\\\n& +\\lim _{\\varepsilon \\rightarrow 0} \\frac{1}{\\varepsilon}\\left\\{\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa^{g}(\\gamma+\\varepsilon \\bar{\\eta}, \\tau)_{u, v} \\sum_{i, k=1}^{e} \\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}+\\varepsilon \\bar{\\eta}_{u}, \\tau_{v}\\right) \\mathrm{d} \\eta_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}\\right. \\\\\n& \\left.-\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, k=1}^{e} \\partial_{x^{i} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\eta_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}\\right\\} .\n\\end{aligned}\n$$\n\nThanks to Lemma A.1, we can use dominated convergence to swap limits and integrals. After rearranging terms we obtain\n\n$$\n\\begin{aligned}\n\\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa^{g}(\\gamma, \\tau)_{s^{\\prime}, t^{\\prime}}= & \\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, k=1}^{c} \\partial_{x^{\\prime} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\eta} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, k, l=1}^{c} \\partial_{x^{\\prime} i y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\bar{\\eta}_{u}^{i} \\mathrm{~d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\eta} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, k=1}^{c} \\partial_{x^{\\prime} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\bar{\\eta}_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\bar{\\eta}} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, j, k=1}^{c} \\partial_{x^{\\prime} j y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\eta_{u}^{j} \\mathrm{~d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, j, k, l=1}^{c} \\partial_{x^{\\prime} j l y^{k}}^{4} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\bar{\\eta}_{u}^{i} \\eta_{u}^{j} \\mathrm{~d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, j, k=1}^{c} \\partial_{x^{\\prime} j y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\eta_{u}^{j} \\mathrm{~d} \\bar{\\eta}_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\bar{\\eta}} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, k=1}^{c} \\partial_{x^{\\prime} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\eta_{u}^{i} \\mathrm{~d} \\tau_{v}^{k} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa^{g}(\\gamma, \\tau)_{u, v} \\sum_{i, k, l=1}^{c} \\partial_{x^{\\prime} i y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\bar{\\eta}_{u}^{i} \\mathrm{~d} \\eta_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}\n\\end{aligned}\n$$\n\nwhere we also used the property (3.13). We can regroup the terms according to the order of $\\kappa^{g}$, using $\\sum_{i, k=1}^{c} \\partial_{x^{\\prime} y^{k}}^{2} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\mathrm{d} \\gamma_{u}^{i} \\mathrm{~d} \\tau_{v}^{k}=\\left\\langle\\mathrm{d} \\gamma_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}$, Equation (A.15) and similarly for the next order:\n(A.17)\n\n$$\n\\left\\langle\\mathrm{d} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} G(\\gamma)_{u}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}=\\sum_{i, k, l=1}^{c}\\left\\{\\sum_{j=1}^{c} \\partial_{x^{\\prime} i^{\\prime} y^{k}}^{4} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\bar{\\eta}_{u}^{i} \\eta_{u}^{j} \\mathrm{~d} \\gamma_{u}^{i}+\\partial_{x^{\\prime} j y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\eta_{u}^{j} \\mathrm{~d} \\bar{\\eta}_{u}^{i}+\\partial_{x^{\\prime} i y^{k}}^{3} g\\left(\\gamma_{u}, \\tau_{v}\\right) \\bar{\\eta}_{u}^{i} \\mathrm{~d} \\eta_{u}^{i}\\right\\} \\mathrm{d} \\tau_{v}^{k}\n$$\n\nEventually this yields the more compact formulation\n\n$$\n\\begin{aligned}\n\\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa^{g}(\\gamma, \\tau)_{s, t}= & \\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} \\kappa^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\gamma_{u}^{g}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}+\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\eta} \\kappa^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\partial_{\\gamma}^{\\bar{\\eta}} G(\\gamma)_{u}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}} \\\\\n& +\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\partial_{\\gamma}^{\\bar{\\eta}} \\kappa^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\partial_{\\gamma}^{\\eta} G(\\gamma)_{u}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}+\\int_{s}^{s^{\\prime}} \\int_{t}^{t^{\\prime}} \\kappa^{g}(\\gamma, \\tau)_{u, v}\\left\\langle\\mathrm{~d} \\partial_{\\gamma}^{\\eta \\bar{\\eta}} G(\\gamma)_{u}, \\mathrm{~d} \\tau_{v}^{g}\\right\\rangle_{\\mathcal{H}_{g}}\n\\end{aligned}\n$$\n\nas claimed.", "tables": {}, "images": {}}, {"section_id": 8, "text": "# REFERENCES \n\n[1] E. Abi Jaber and S. Villeneuve. Gaussian agency problems with memory and linear contracts. Available at SSRN 4226543, 2022.\n[2] N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, 68(3):337-404, 1950 .\n[3] O. E. Barndorff-Nielsen, M. S. Pakkanen, and J. Schmiegel. Assessing relative volatility/intermittency/energy dissipation. Electron. J. Statist., 8(2):1996-2021, 2014.\n[4] P. Batlle, Y. Chen, B. Hosseini, H. Owhadi, and A. M. Stuart. Error analysis of kernel/GP methods for nonlinear and parametric PDEs. arXiv preprint arXiv:2305.04962, 2023.\n[5] C. Bayer, P. Friz, and J. Gatheral. Pricing under rough volatility. Quantitative Finance, 16(6):887-904, 2016.\n[6] C. Bayer, P. K. Friz, M. Fukasawa, J. Gatheral, A. Jacquier, and M. Rosenbaum. Rough volatility. SIAM, 2023.\n\n[7] M. Bennedsen. A rough multi-factor model of electricity spot prices. In Commodities, pages 149-178. Chapman and Hall/CRC, 2022.\n[8] M. Bennedsen, A. Lunde, and M. S. Pakkanen. Hybrid scheme for Brownian semistationary processes. Finance and Stochastics, 21:931-965, 2017.\n[9] G. Blanchard, G. Lee, and C. Scott. Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems, 24, 2011.\n[10] O. Bonesini, A. Jacquier, and A. Pannier. Rough volatility, path-dependent PDEs and weak rates of convergence. arXiv preprint arXiv:2304.03042, 2023.\n[11] T. Cass, T. Lyons, and X. Xu. Weighted signature kernels. The Annals of Applied Probability, 34(1A):585-626, 2024.\n[12] Y. Chen, B. Hosseini, H. Owhadi, and A. M. Stuart. Solving and learning nonlinear PDEs with Gaussian processes. arXiv preprint arXiv:2103.12959, 2021.\n[13] Y. Chen, H. Owhadi, and F. Sch\u00e4fer. Sparse Cholesky factorization for solving nonlinear PDEs via Gaussian processes. arXiv preprint arXiv:2304.01294, 2023.\n[14] I. Chevyrev and H. Oberhauser. Signature moments to characterize laws of stochastic processes. arXiv preprint arXiv:1810.10971, 2018.\n[15] A. Christmann and I. Steinwart. Consistency and robustness of kernel-based regression in convex risk minimization. Bernoulli, 13(3):799-819, 2007.\n[16] N. M. Cirone, M. Lemercier, and C. Salvi. Neural signature kernels as infinite-width-depth-limits of controlled ResNets. International Conference on Machine Learning, 2023.\n[17] T. Cochrane, P. Foster, V. Chhabra, M. Lemercier, T. Lyons, and C. Salvi. Sk-tree: a systematic malware detection algorithm on streaming trees via the signature kernel. In 2021 IEEE international conference on cyber security and resilience (CSR), pages 35-40. IEEE, 2021.\n[18] J. Cockayne, C. Oates, T. Sullivan, and M. Girolami. Probabilistic numerical methods for partial differential equations and Bayesian inverse problems. arXiv preprint arXiv:1605.07811, 2016.\n[19] J. M. Corcuera, E. Hedevang, M. S. Pakkanen, and M. Podolskij. Asymptotic theory for Brownian semi-stationary processes with application to turbulence. Stochastic processes and their applications, 123(7):2552-2574, 2013.\n[20] C. Cuchiero, P. Schmocker, and J. Teichmann. Global universal approximation of functional input maps on weighted spaces. arXiv preprint arXiv:2306.03303, 2023.\n[21] B. Dupire and V. Tissot-Daguette. Functional expansions. arXiv preprint arXiv:2212.13628, 2022.\n[22] K. Eichinger, C. Kuehn, and A. Neamtu. Sample paths estimates for stochastic fast-slow systems driven by fractional Brownian motion. Journal of Statistical Physics, 179(5):1222-1266, 2020.\n[23] B. Fang, H. Ni, and Y. Wu. A neural RDE-based model for solving path-dependent PDEs. arXiv preprint arXiv:2306.01123, 2023.\n[24] A. Fermanian, T. Lyons, J. Morrill, and C. Salvi. New directions in the applications of rough path theory. IEEE BITS the Information Theory Magazine, 2023.\n[25] P. K. Friz and N. B. Victoir. Multidimensional stochastic processes as rough paths: theory and applications, volume 120. Cambridge University Press, 2010.\n[26] J. Gehringer and X.-M. Li. Functional limit theorems for the fractional Ornstein-Uhlenbeck process. Journal of Theoretical Probability, 35(1):426-456, 2022.\n[27] M. Hairer. Ergodicity of stochastic differential equations driven by fractional Brownian motion. Ann. Probab., $33(2): 703-758,2005$.\n[28] J. Han, A. Jentzen, and W. E. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510, 2018.\n[29] B. Horvath, M. Lemercier, C. Liu, T. Lyons, and C. Salvi. Optimal stopping via distribution regression: a higher rank signature approach. arXiv preprint arXiv:2304.01479, 2023.\n[30] Z. Issa, B. Horvath, M. Lemercier, and C. Salvi. Non-adversarial training of neural SDEs with signature kernel scores. Advances in Neural Information Processing Systems, 2023.\n[31] A. Jacquier and M. Oumgari. Deep curve-dependent PDEs for affine rough volatility. SIAM Journal on Financial Mathematics, 14(2):353-382, 2023.\n[32] P. Kidger, P. Bonnier, I. Perez Arribas, C. Salvi, and T. Lyons. Deep signature transforms. Advances in Neural Information Processing Systems, 32, 2019.\n[33] F. J. Kir\u00e1ly and H. Oberhauser. Kernels for sequentially ordered data. Journal of Machine Learning Research, 20, 2019.\n[34] M. Lemercier, C. Salvi, T. Damoulas, E. Bonilla, and T. Lyons. Distribution regression for sequential data. In International Conference on Artificial Intelligence and Statistics, pages 3754-3762. PMLR, 2021.\n[35] X.-M. Li and J. Sieber. Slow-fast systems with fractional environment and dynamics. The Annals of Applied Probability, 32(5):3964-4003, 2022.\n[36] T. J. Lyons, M. Caruana, and T. L\u00e9vy. Differential equations driven by rough paths. Springer, 2007.\n\n[37] B. B. Mandelbrot and J. W. Van Ness. Fractional Brownian motions, fractional noises and applications. SIAM review, 10(4):422-437, 1968.\n[38] G. Manten, C. Casolo, E. Ferrucci, S. W. Mogensen, C. Salvi, and N. Kilbertus. Signature kernel conditional independence tests in causal discovery for stochastic processes. arXiv preprint arXiv:2402.18477, 2024.\n[39] Y. Mishura, S. Ottaviano, and T. Vargiolu. Gaussian Volterra processes as models of electricity markets. arXiv preprint arXiv:2311.09384, 2023.\n[40] J. Morrill, C. Salvi, P. Kidger, and J. Foster. Neural rough differential equations for long time series. In International Conference on Machine Learning, pages 7829-7838. PMLR, 2021.\n[41] H. Owhadi and C. Scovel. Operator-Adapted Wavelets, Fast Solvers, and Numerical Homogenization: From a Game Theoretic Approach to Numerical Approximation and Algorithm Design, volume 35. Cambridge University Press, 2019.\n[42] A. Pannier. Path-dependent PDEs for volatility derivatives. arXiv preprint arXiv:2311.08289, 2023.\n[43] M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686-707, 2019.\n[44] Z. Ren and X. Tan. On the convergence of monotone schemes for path-dependent PDEs. Stochastic Processes and their Applications, 127(6):1738-1762, 2017.\n[45] M. Sabate-Vidales, D. \u0160i\u0161ka, and L. Szpruch. Solving path dependent PDEs with LSTM networks and path signatures. arXiv preprint arXiv:2011.10630, 2020.\n[46] C. Salvi, T. Cass, J. Foster, T. Lyons, and W. Y. The signature kernel is the solution of a Goursat PDE. SIAM Journal on Mathematics of Data Science, 3(3):873-899, 2021.\n[47] C. Salvi, M. Lemercier, T. Cass, E. V. Bonilla, T. Damoulas, and T. J. Lyons. SigGPDE: Scaling sparse Gaussian processes on sequential data. In International Conference on Machine Learning, pages 6233-6242. PMLR, 2021.\n[48] C. Salvi, M. Lemercier, and A. Gerasimovics. Neural stochastic PDEs: Resolution-invariant learning of continuous spatiotemporal dynamics. Advances in Neural Information Processing Systems, 35:1333-1344, 2022.\n[49] C. Salvi, M. Lemercier, C. Liu, B. Horvath, T. Damoulas, and T. Lyons. Higher order kernel mean embeddings to capture filtrations of stochastic processes. Advances in Neural Information Processing Systems, 34:16635-16647, 2021.\n[50] Y. F. Saporito and Z. Zhang. Path-dependent deep Galerkin method: A neural network approach to solve pathdependent partial differential equations. SIAM Journal on Financial Mathematics, 12(3):912-940, 2021.\n[51] J. Sirignano and K. Spiliopoulos. DGM: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339-1364, 2018.\n[52] D. Sonechkin. Climate dynamics as a nonlinear Brownian motion. International Journal of Bifurcation and Chaos, $8(04): 799-803,1998$.\n[53] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch\u00f6lkopf, and G. R. Lanckriet. Hilbert space embeddings and metrics on probability measures. The Journal of Machine Learning Research, 11:1517-1561, 2010.\n[54] F. Viens and J. Zhang. A martingale approach for fractional Brownian motions and related path dependent PDEs. The Annals of Applied Probability, 29(6):3489-3540, 2019.\n[55] H. Wang, J. Yong, and J. Zhang. Path dependent Feynman-Kac formula for forward backward stochastic Volterra integral equations. Annales de l'Institut Henri Poincar\u00e9: Probabilit\u00e9s et Statistiques, 58(2):603-638, 2022.\n[56] H. Wendland. Scattered data approximation, volume 17. Cambridge university press, 2004.\n[57] D.-X. Zhou. Derivative reproducing properties for kernel methods in learning theory. Journal of computational and Applied Mathematics, 220(1-2):456-463, 2008.", "tables": {}, "images": {}}], "id": "2403.11738v3", "authors": ["Alexandre Pannier", "Cristopher Salvi"], "categories": ["math.NA", "cs.NA", "q-fin.CP"], "abstract": "We develop a provably convergent kernel-based solver for path-dependent PDEs\n(PPDEs). Our numerical scheme leverages signature kernels, a recently\nintroduced class of kernels on path-space. Specifically, we solve an optimal\nrecovery problem by approximating the solution of a PPDE with an element of\nminimal norm in the signature reproducing kernel Hilbert space (RKHS)\nconstrained to satisfy the PPDE at a finite collection of collocation paths. In\nthe linear case, we show that the optimisation has a unique closed-form\nsolution expressed in terms of signature kernel evaluations at the collocation\npaths. We prove consistency of the proposed scheme, guaranteeing convergence to\nthe PPDE solution as the number of collocation points increases. Finally,\nseveral numerical examples are presented, in particular in the context of\noption pricing under rough volatility. Our numerical scheme constitutes a valid\nalternative to the ubiquitous Monte Carlo methods.", "updated": "2024-10-26T13:52:05Z", "published": "2024-03-18T12:47:19Z"}