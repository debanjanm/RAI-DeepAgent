{
  "title": "Sharp Testable Implications of Encouragement Designs",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nThis paper studies a potential outcome model with a continuous or discrete outcome, a discrete multi-valued treatment, and a discrete multi-valued instrument. We derive sharp, closedform testable implications for a class of restrictions on potential treatments where each value of the instrument encourages towards at most one unique treatment choice; such restrictions serve as the key identifying assumption in several prominent recent empirical papers. Borrowing the terminology used in randomized experiments, we call such a setting an encouragement design. The testable implications are inequalities in terms of the conditional distributions of choices and the outcome given the instrument. Through a novel constructive argument, we show these inequalities are sharp in the sense that any distribution of the observed data that satisfies these inequalities is compatible with this class of restrictions on potential treatments. Based on these inequalities, we propose tests of the restrictions. In an empirical application, we show some of these restrictions are violated and pinpoint the substitution pattern that leads to the violation.\n\n\nKEYWORDS: Multi-valued treatment, instrumental variable, encouragement design, random utility model, moment inequalities\n\nJEL classification codes: C14, C31, C35, C36\n\n[^0]\n[^0]:    *We thank Tim Armstrong, Stephane Bonhomme, Deniz Dutz, D\u00e9sir\u00e9 K\u00e9dagni, Toru Kitagawa, Simon Lee, Lilnua Lei, Matt Masten, Ismael Mourifi\u00e9, Jonathan Roth, Azeem Shaikh, Alex Torgovitsky, Atom Vayalinkal, Yuanyuan Wan, as well as seminar participants at the University of Chicago, 2024 Southern Economic Association Annual Meeting, the Chinese University of Hong Kong, Shenzhen, and the University of Washington for helpful comments. We thank Kirill Ponomarev for helping us draw a connection with the random set theory. We thank Ismael Mourifi\u00e9 and Yuanyuan Wan for sharing the code for Mourifi\u00e9 and Wan (2017). Any and all errors are our own.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "# 1 Introduction \n\nThe analysis of potential outcome models with a discrete multi-valued treatment and a discrete multi-valued instrument has gained considerable interest recently in economics. In the setting where both the treatment and the instrument are binary, the well-known monotonicity assumption of Imbens and Angrist (1994) serves a key role in the identification of causal effects. Beyond this setting, the most natural extension to the monotonicity assumption for modeling choice behavior is arguably to assume that each value of the instrument (e.g., subsidy or voucher towards a program) increases the appeal of at most one unique treatment choice (e.g., the corresponding program). In fact, such an assumption on choice behavior serves as the key identifying assumption in several prominent recent empirical papers (see, for instance, Kirkeboen et al., 2016; Kline and Walters, 2016). Borrowing the terminology used in randomized experiments, we call such a setting an encouragement design (Powers and Swinton, 1984; Holland, 1988; Duflo et al., 2007). We derive sharp, closed-form testable implications, in the form of inequalities on the conditional distributions of choices and the outcome given the instrument, that characterize when the distribution of the observed data is consistent with an encouragement design. These inequalities are sharp in the sense that they exhaust all the information in the model. Because the testable implications are in closed form, if in fact the implications are violated, then we can pinpoint which substitution patterns lead to the violation. In an empirical application to Behaghel et al. (2013, 2014), we apply a test based on our sharp testable implications and demonstrate that the data is not compatible with assuming that the instrument does not affect the appeal of the control group, which is often thought of as a harmless normalization. Moreover, our method identifies which substitution pattern leads to the violation.\n\nTo motivate the assumptions on potential treatments that we consider, suppose there are three preschool programs (the treatment or choice), and each person receives a voucher (the instrument) towards one of them. Because we assume that the voucher towards a program only increases the appeal of the corresponding program, receiving such a voucher should not change the comparison among the remaining programs. Therefore, for each person, there exists a \"default\" choice (which possibly differs across people), which is the choice they would have made if the instrument didn't exist; when the instrument equals $j$, then the person chooses either treatment $j$ or the default choice. These restrictions on potential treatments immediately lead to a set of inequalities on conditional distributions given the instrument which are easy to interpret. To the best of our knowledge, these inequalities are new to the literature beyond the setting where both the treatment and the instrument are binary. We then show through a novel constructive argument that they are sharp; that is, for each distribution of the observed data that satisfies these inequalities, we construct a distribution of the potential outcomes and potential treatments that generates the observed distribution while satisfying the restrictions discussed above. We note that these restrictions immediately imply the following substitution patterns: when the instrument changes from $j$ to $k, k$ becomes more appealing\n\nbut $j$ becomes less appealing, and hence one may stay at their original choice if it is the default choice, switch to $k$, or fall back to the default choice, which may be neither $j$ nor $k$. The sharp testable inequalities, however, involve more complex restrictions on substitution patterns across multiple values of the instrument beyond simple pairwise comparison.\n\nTo accommodate a larger class of empirical examples, we further allow researchers to impose that the values of certain choices are not affected by the instrument at all, and that there exists a \"base state\" of the instrument which does not affect the values of any choice. Examples of such settings include Kline and Walters (2016) and Kirkeboen et al. (2016). ${ }^{1}$ In Kline and Walters (2016), the treatment takes three values: no preschool, other preschools, Head Start; and the instrument is a binary indicator of whether the household receives an offer for Head Start. In this case, not receiving the Head Start offer is a \"base state\" which does not change the appeal of any choice, and the values of no preschool and other preschools are never shifted by the instrument. As a result, we recover the key identifying restriction in Kline and Walters (2016), that receiving an offer to Head Start may shift the household to participate in Head Start, but not lead them to switch between no preschool and other preschools. Although the restriction that the values of some choices are not affected by the instrument is reasonable in Kline and Walters (2016), it is otherwise often motivated as a harmless \"normalization\" that the value of the control status is not affected by the instrument. As we demonstrate in this paper, however, this \"normalization\" is not innocuous, and potentially refutable in the data. The key insight is that when there is a base state of the instrument, the default choice further coincides with the choice made under this base state. Furthermore, the substitution patterns become more restrictive: when the instrument changes from the base state to $j$, one might only switch to $j$ if their choice changes. Therefore, the testable inequalities simplify, although the construction to show sharpness requires further modification. Surprisingly, only the pairwise substitution patterns between the base state and other values of the instrument appear in the sharp testable implications.\n\nFor the setting with a binary treatment, a binary instrument, and a binary outcome, Balke and Pearl (1997a,b) provide the first set of inequalities that sharply characterize when the distribution of the observed data is consistent with instrument exogeneity and monotonicity. Their results are obtained through a linear programming formulation. Kitagawa (2015) generalizes these inequalities when the outcome is allowed to be continuous and importantly, shows they are sharp constructively. He further proposes a corresponding test. Mourifi\u00e9 and Wan (2017) leverage the intersection bounds framework of Chernozhukov et al. (2013) to construct an alternative test based on the same testable implications. When the treatment and the instrument are binary, the model in this paper is equivalent to the model studied in Imbens and Angrist (1994). In that case, we recover the inequalities in Balke and Pearl (1997a,b) and Kitagawa (2015). However, both the inequalities and our construction to establish their sharpness beyond this special case are, to our knowledge, novel to the literature.\n\n[^0]\n[^0]:    ${ }^{1}$ Both examples are also studied in Lee and Salani\u00e9 (2024), who focus instead on the identification of treatment effect parameters conditional on \"response groups\" defined as sets of possible values of potential treatments.\n\nAs shown by Vytlacil (2002), the model considered in the papers above is equivalent to the nonparametric selection model in Heckman and Vytlacil (2005), who also discuss testable implications in the binary setting with a possibly continuous instrument. K\u00e9dagni and Mourifi\u00e9 (2020) derive a set of inequalities for instrument exogeneity with a binary outcome, a possibly multi-valued treatment and a multi-valued instrument, and further show they are sharp when both the treatment and the instrument are binary. Kitagawa (2021) derives a set of sharp inequalities for instrument exogeneity with a continuous outcome, a binary treatment, and a binary instrument. Sun (2023) derives a set of inequalities with a possibly multi-valued treatment and instrument, under instrument exogeneity and the \"unordered monotonicity\" assumption of Heckman and Pinto (2018), but does not establish that these are necessarily sharp. As we explain in Remark 2.3 below, however, the \"unordered monotonicity\" assumption is not implied by, nor does it imply, our assumption. Kwon and Roth (2024) characterize testable implications in the setting with a multi-valued treatment and a binary instrument ${ }^{2}$.\n\nOur paper is also related to a vast literature that studies identification and inference for treatment effects using instrumental variables. See, for example, Bhattacharya et al. (2008, 2012), Machado et al. (2019), S\u0142oczy\u0144ski (2020), Mogstad et al. (2021), Goff (2024), as well as the comprehensive review article on instrumental variables by Mogstad and Torgovitsky (2024). Of particular relevance to our setting are the papers which study multi-valued treatments and instruments: see, for instance, Lee and Salani\u00e9 (2018), Kamat et al. (2023), Bai et al. (2024a), Bai et al. (2024b), and Bhuller and Sigstad (2024).\n\nThe remainder of the paper is organized as follows. In Section 2, we describe our setup and notation. In Section 3, we characterize the set of inequalities implied by our model and show that these are sharp. For simplicity, we first state versions of our results in a setting with only a treatment and an instrument. In Section 4.1, we extend the results to a setting with an additional, possibly continuous, outcome variable. We propose tests of the model based on these sharp implications in Section 5 and examine the performance of these tests through simulations in Section 6. We apply tests based on our sharp testable implications to Behaghel et al. $(2013,2014)$ in Section 7, where we demonstrate that the data is not compatible with assuming that the instrument does not affect the appeal of the control group. Moreover, our method identifies which choice patterns lead to the violation.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 2 Setup and Notation \n\nLet $J \\geq 2$ be an integer. Let $D \\in\\{0, \\ldots, J-1\\}$ denote a multi-valued treatment choice and $Z \\in \\mathcal{Z} \\subseteq\\{0, \\ldots, J-1\\}$ denote a multi-valued instrument. (In Section 4.1, we additionally consider\n\n[^0]\n[^0]:    ${ }^{2}$ However, we note that they frame their contribution in the context of developing tests for mediation analysis.\n\nan outcome variable $Y \\in \\mathcal{Y}$, but we ignore it for the time being.) In the models that we consider, each value of the instrument encourages towards at most one unique choice. As we explain further below, to accommodate a larger class of empirical examples, we do not require that the support of $Z$ be the same as that of $D$. Specifically, we consider two forms of $\\mathcal{Z}$ : (1) $\\mathcal{Z}=\\{0, \\ldots, J-1\\}$ and (2) $\\mathcal{Z}=\\left\\{0, J_{0}, \\ldots, J-1\\right\\}$, for some $1 \\leq J_{0} \\leq J-1$. The first form corresponds to a setting where every choice has a corresponding instrument value which encourages towards it. The second form corresponds to a setting where the first $J_{0}$ choices are not affected by the instrument, and in this case we interpret $Z=0$ as the \"base state\" of the instrument. To avoid notational ambiguity, we will always explicitly state the value of $J_{0}{ }^{3}$. Let $D_{z}$ for $z \\in \\mathcal{Z}$ denote the potential treatment choice when assigned to the instrument value $z$. As usual, the observed choice is related to potential treatment choices and the instrument through\n\n$$\nD=\\sum_{z \\in \\mathcal{Z}} D_{z} I\\{Z=z\\}\n$$\n\nLet $Q$ denote the distribution of $\\left(\\left(D_{z}: z \\in \\mathcal{Z}\\right), Z\\right)$ and $P$ denote the distribution of $(D, Z)$. Note that given the mapping $T$ such that $D=T\\left(\\left(D_{z}: z \\in \\mathcal{Z}\\right), Z\\right)$ as implied by (1), we obtain by construction that $P=Q T^{-1}$. Throughout, we will impose the assumption that the instrument is exogenous; formally:\n\nAssumption 2.1. $\\left(D_{z}: z \\in \\mathcal{Z}\\right) \\Perp Z$ under $Q$.\n\nWe will further rule out degenerate situations by requiring that the instrument takes on each value in its support with strictly positive probability:\n\nAssumption 2.2. $Q\\{Z=z\\}>0$ for $z \\in \\mathcal{Z}$.\n\nIn what follows, we will frequently use the following facts about the relationship between $P$ and $Q$. Suppose $P=Q T^{-1}$ for some $Q$ that satisfies Assumptions 2.1-2.2. Then, for $z \\in \\mathcal{Z}$,\n\n$$\nP\\{Z=z\\}=Q\\{Z=z\\}>0\n$$\n\nTherefore, the conditional choice probabilities can be defined and they satisfy\n\n$$\nP\\{D=j \\mid Z=z\\}=Q\\left\\{D_{z}=j \\mid Z=z\\right\\}=Q\\left\\{D_{z}=j\\right\\}\n$$\n\nwhere the first equality follows because $D=D_{z}$ when $Z=z$ by (1), and the second equality follows from Assumption 2.1.\n\nOur goal is to study the necessary and sufficient conditions for $P$ to be consistent with a class of restrictions on potential treatments that are commonly imposed when analyzing encouragement\n\n[^0]\n[^0]:    ${ }^{3}$ This is particularly important when $\\mathcal{Z}=\\{0,1, \\ldots, J-1\\}$, which is possible when either $J_{0}=0$ or $J_{0}=1$.\n\ndesigns. Loosely speaking, these restrictions dictate that each value of the instrument encourages towards at most one unique choice. As we explain below, these restrictions, or stronger versions of them, are used as the key identifying restrictions for the causal interpretation of regression estimands. To state the class of restrictions, fix $0 \\leq J_{0} \\leq J-1$, where $J_{0}$ is the number of choices that are not affected by the instrument. Recall from the beginning of this section that the support of the instrument is $\\mathcal{Z}=\\left\\{0, J_{0}, \\ldots, J-1\\right\\}$.\n\nAssumption 2.3. There exists a random variable $j^{*}$ such that $0 \\leq j^{*} \\leq J-1$ and\n\n$$\nQ\\left\\{D_{j} \\in\\left\\{j, j^{*}\\right\\}\\right\\}=1 \\text { for } 0 \\leq j \\leq J-1\n$$\n\nFurthermore, when $J_{0}>0, Q\\left\\{j^{*}=D_{0}\\right\\}=1$, so that (3) becomes\n\n$$\nQ\\left\\{D_{j} \\in\\left\\{j, D_{0}\\right\\}\\right\\}=1 \\text { for } J_{0} \\leq j \\leq J-1\n$$\n\nTo interpret Assumption 2.3, first consider the case $J_{0}=0$. Because $Z=j$ is an encouragement towards $D=j$, it should not affect the comparison among all other choices $\\{0, \\ldots, J-1\\} \\backslash\\{j\\}$. As a result, if we think of $j^{*}$ as a \"default\" choice (which is a random variable so could differ across people) that the person would have made if the instrument didn't exist, then $Z=j$ either pushes them to choose $j$ or stay at $j^{*}$; in particular, the person cannot choose any choice that is not $j$ or $j^{*}$. Furthermore, the substitution patterns must be as follows: with a change from $Z=j$ to $Z=k$, the person may stay with the original choice if it is the default choice, switch to $k$, or fall back to the default choice $j^{*}$, which may be neither $j$ nor $k$.\n\nNote that Assumption 2.3 rules out a large number of vectors of potential treatments. Consider for instance the setting where $J=3$ and $J_{0}=0$. The restriction in (3) implies\n\n$$\nQ\\left\\{D_{0}=1, D_{1}=2\\right\\}=0\n$$\n\nTo see why, suppose $D_{0}(\\omega)=1$ and $D_{1}(\\omega)=2$ for some individual $\\omega \\in \\Omega$, where $\\Omega$ denotes the underlying probability space. Then, $D_{0}(\\omega) \\in\\left\\{0, j^{*}(\\omega)\\right\\}$ implies $j^{*}(\\omega)=1$; at the same time, $D_{1}(\\omega) \\in\\left\\{1, j^{*}(\\omega)\\right\\}$ implies $j^{*}(\\omega)=2$, a contradiction. Following similar arguments, we can conclude that under $Q \\in \\mathbf{Q}_{1},\\left(D_{0}, D_{1}, D_{2}\\right)$ can at most take 10 values with positive probabilities, listed in Table 1, instead of $3^{3}=27$ values.\n\nIn some settings, one may further want to restrict the model so that the appeal of the first $J_{0}>0$ choices are not affected by the instrument. In this case, we interpret $Z=0$ as the \"base state\" as if the instrument didn't exist. As a result, $j^{*}=D_{0}$. As illustrated through Examples 2.1-2.3 below, this additional restriction may be reasonable in some examples but not others, and is in general not a harmless normalization. When $J=2, J_{0}=0$ and $J_{0}=1$ are equivalent, and in both cases, Assumption 2.3 is simply stating $Q\\left\\{\\left(D_{0}, D_{1}\\right)=(1,0)\\right\\}=0$, i.e., defiers are ruled out.\n\n![table_0](table_0)\n\nTable 1: The support of $\\left(D_{0}, D_{1}, D_{2}\\right)$ if $Q$ satisfies (3) $\\left(J_{0}=0\\right)$.\n\nWhen $J>2$, however, setting $J_{0}=1$ is no longer without loss of generality, because $J_{0}=1$ implies more restrictive substitution patterns than $J_{0}=0$. Indeed, consider $J=3$ as an example. If $J_{0}=1$, then $Q\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,1,1)\\right\\}=0$, because (4) requires that $D_{2} \\in\\left\\{D_{0}, 2\\right\\}$ with probability one. On the other hand, if $J_{0}=0$, then (3) allows for $Q\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,1,1)\\right\\}>0$. The reason is that when $J_{0}=0$, changing $Z=0$ to $Z=2$ not only increases the appeal of $D=2$, but decreases the appeal of $D=0$ as well (because $D=0$ is encouraged by $Z=0$ but not $Z=2$ ), making it possible for the person to fall back to the default choice, which in this case is $D=1$. When $J_{0}=1$, however, changing $Z=0$ to $Z=2$ only increases the appeal of $D=2$ but does not decrease the appeal of $D=0$, so one can only switch to $D=2$ instead of $D=1$. As we show below, the testable implications in Sections 3 and 4.1 are different when $J_{0}=0$ and $J_{0}>0$, enabling us to test directly for whether the instrument does not affect the appeal of some choices.\n\nLet $\\mathbf{Q}_{1}$ denote the set of all distributions of $\\left(\\left(D_{z}: z \\in \\mathcal{Z}\\right), Z\\right)$ that satisfy Assumptions 2.1-2.3. We now present a series of empirical examples in which Assumption 2.3 or some strengthened version is used as the key identifying assumption for the causal interpretation of regression estimands. The first example will be revisited in the empirical application of Section 7.\n\nExample 2.1. Behaghel et al. $(2013,2014)$ study a randomized controlled trial involving three job search counseling programs in France. In their setting, $Z=0$ denotes encouragement towards the usual public program without intensive counseling, $Z=1$ denotes encouragement towards the public program with intensive counseling, and $Z=2$ denotes encouragement towards the private program. $D=0,1,2$ denotes participation in the corresponding programs. Assumption 2.1 holds because $Z$ is randomly assigned and Assumption 2.2 holds as long as a nontrivial portion of people are assigned to each arm. To recover a causal interpretation of the IV estimand as a local average treatment effect (LATE), however, Behaghel et al. (2013) impose an assumption called \"extended monotonicity.\" As shown in Appendix C, this assumption is strictly stronger than setting $J_{0}=1$ in our model. However, because each value of $Z$ encourages towards one value of $D$, it is reasonable to expect we are in a situation where $J_{0}=0$ instead of $J_{0}=1$; in particular, there is no compelling reason\n\nto handle $Z=0$ asymmetrically with $Z \\in\\{1,2\\}$ purely because it is the encouragement towards a control group. The discussion following Assumption 2.3 demonstrated that $\\left\\{D_{0} \\neq 1, D_{2}=1\\right\\}$ is not allowed when $J_{0}=1$, but is allowed when $J_{0}=0$. In the empirical application in Section 7, we reject $J_{0}=1$, and thus also their extended monotonicity assumption. Moreover, we show that the substitution pattern $\\left\\{D_{0} \\neq 1, D_{2}=1\\right\\}$ is exactly the reason for rejection. At the same time, we cannot reject $J_{0}=0$. The findings therefore suggest that at least for some people, $Z=0$ strictly increases the appeal of the public program $D=0$.\n\nExample 2.2. Kline and Walters (2016) consider an RCT with a \"close substitute\" to study the effects of preschooling on educational outcomes and impose (4) with $J=3$ and $J_{0}=2$. In their setting, $D \\in\\{0,1,2\\}$, where $D=0$ denotes home care (no preschool), $D=2$ denotes participation in a preschool program called Head Start, and $D=1$ denotes participation in preschools other than Head Start, namely the close substitute. Here, $Z \\in\\{0,2\\}$, where $Z=2$ denotes that the household receives an offer to attend Head Start, and $Z=0$ denoted otherwise. Assumption 2.1 holds because $Z$ is randomly assigned and Assumption 2.2 holds as long as a nontrivial portion of households are assigned to each arm. In their equation (1), Kline and Walters (2016) impose the restriction that\n\n$$\nQ\\left\\{D_{2}=2 \\mid D_{0} \\neq D_{2}\\right\\}=1\n$$\n\nThe restriction in (5) states that if a household switches their choice upon receiving a Head Start offer, then they must be switching to Head Start. In other words, receiving an offer to Head Start does not change the comparison between no preschool and preschools other than Head Start. The restriction in (5) is equivalent to $Q\\left\\{D_{2} \\in\\left\\{D_{0}, 2\\right\\}\\right\\}=1$, which is exactly (4) with $J=3$ and $J_{0}=2$. The support of $\\left(D_{0}, D_{2}\\right)$ is summarized in Table 2. The restriction in (5) is plausible, although it may be violated because of, for instance, a salience effect, where receiving the offer to Head Start makes the household more aware of the merit of preschools in general and causes them to send the kid to other preschools. In that case, a change of $Z=0$ to $Z=2$ may also decrease the appeal of home care or increase the appeal of other preschools.\n\n![table_1](table_1)\n\nTable 2: The support of $\\left(D_{0}, D_{2}\\right)$ in Kline and Walters (2016).\n\nThe key insight behind the empirical results in Kline and Walters (2016) is that under the assumption in (5), the Wald estimand from the IV regression identifies a weighted combination of what they call \"sub-LATEs.\" Kline and Walters (2016) further establish conditions under which optimal\n\npolicy depends upon these \"sub-LATEs.\" Our results in Sections 3 and 4.1 will characterize when the distribution of the data is consistent with (5).\n\nExample 2.3. Kirkeboen et al. (2016) study the effects of fields of study on earnings and impose (4) with $J=3$ and $J_{0}=1$. In their setting, $D \\in\\{0,1,2\\}$ represent three fields of study, ordered by their (soft) admission cutoffs from the lowest to the highest. The instrument $Z \\in\\{0,1,2\\}$ is called \"expected offer\" in their paper. Roughly speaking, $Z=1$ when the student crosses the (soft) admission cutoff for field $1, Z=2$ when the student crosses the (soft) admission cutoff for field 2 , and $Z=0$ otherwise. The authors assume that $Z$ is exogenous in the sense that $Q$ satisfies Assumption 2.1, and also assume Assumption 2.2 holds. They further impose the following monotonicity conditions:\n\n$$\n\\begin{aligned}\n& Q\\left\\{D_{1}=1 \\mid D_{0}=1\\right\\}=1 \\\\\n& Q\\left\\{D_{2}=2 \\mid D_{0}=2\\right\\}=1\n\\end{aligned}\n$$\n\nThe conditions in (6)-(7) require that crossing the cutoff for field 1 or 2 weakly encourages them towards that field. They further impose the following \"irrelevance\" conditions:\n\n$$\n\\begin{aligned}\n& Q\\left\\{I\\left\\{D_{1}=2\\right\\}=I\\left\\{D_{0}=2\\right\\} \\mid D_{0} \\neq 1, D_{1} \\neq 1\\right\\}=1 \\\\\n& Q\\left\\{I\\left\\{D_{2}=1\\right\\}=I\\left\\{D_{0}=1\\right\\} \\mid D_{0} \\neq 2, D_{2} \\neq 2\\right\\}=1\n\\end{aligned}\n$$\n\nThe condition in (8) states that if crossing the cutoff for field 1 does not cause the student to switch to field 1 , then it does not cause them to switch to or away from field 2 . A similar interpretation applies to (9).\n\nThe restrictions in (6)-(9) are equivalent to (4) with $J=3$ and $J_{0}=1$. We now show they imply (4) and the other direction is straightforward. Suppose by contradiction that $D_{1} \\notin\\left\\{D_{0}, 1\\right\\}$. Under this assumption, if $D_{1}=2$, then (6) implies that $D_{0} \\neq 1$; (8) therefore implies that $D_{0}=2$, so $D_{1}=D_{0}$, a contradiction to $D_{1} \\notin\\left\\{D_{0}, 1\\right\\}$. If instead $D_{1}=0$, then (6) again implies that $D_{0} \\neq 1$; because $D_{1} \\notin\\left\\{D_{0}, 1\\right\\}$, we know $D_{0}=2$, which by (8) implies $D_{1}=2$, a contradiction to $D_{1}=0$. Therefore, (4) is satisfied for $j=1$. Similar arguments show it is satisfied for $j=2$ as well. The support of $\\left(D_{0}, D_{1}, D_{2}\\right)$ is summarized in Table 3. The restrictions in (6)-(9) may be violated if, for instance, the change from $Z=0$ to $Z=1$ not only increases the appeal of field 1 but also decreases the appeal of field 0 .\n\nKirkeboen et al. (2016) derive causal interpretations of the IV estimand under (6)-(9) plus the restriction that $D_{0}=0$, which they call the \"next-best\" condition. The full set of assumptions are then equivalent to one-sided noncompliance, meaning that $D_{j} \\in\\{j, 0\\}$ for each $j$. Under all these assumptions together with Assumptions 2.1-2.2, they show that an IV regression identifies the average treatment effects for the \"compliers\" of each instrument value relative to $Z=0$. Our results\n\n![table_2](table_2)\n\nTable 3: The support of $\\left(D_{0}, D_{1}, D_{2}\\right)$ in Kirkeboen et al. (2016).\nin Sections 3 and 4.1 will characterize when the distribution of the data is consistent with (6)-(9), and the results in Section 4.2 apply to the case when the \"next best\" condition is additionally imposed.\n\nRemark 2.1. Although we feel that Assumption 2.3 is intuitive, it can also further be motivated through the lens of a fairly general (non-separable) random utility model where each instrument increases the utility of at most one unique choice. In particular, in Appendix A we introduce such a model and establish its equivalence to Assumptions 2.1-2.3 in our setting. We note that the same model was shown by Kline and Walters (2016) to imply the restrictions in (5). We will in turn show that they are in fact equivalent.\n\nRemark 2.2. Assumption 2.3 is distinct from the restriction considered in Bai et al. (2024b), which in the current context states\n\n$$\nQ\\left\\{D_{j}=j \\mid D_{k}=j \\text { for some } k \\neq j\\right\\}=1\n$$\n\nThe restriction (10) can be shown to be equivalent to (3) when $J=3$, but is weaker than (3) when $J \\geq 4$. Indeed, $\\left(D_{0}, D_{1}, D_{2}, D_{3}\\right)=(1,1,2,2)$ is not ruled out by (10), but is ruled out by (3), because $D_{0}(\\omega)=1 \\neq 0$ implies $j^{*}(\\omega)=1$, whereas $D_{3}(\\omega)=2 \\neq 3$ implies $j^{*}(\\omega)=2$, a contradiction.\n\nRemark 2.3. In a setting with a multi-valued treatment and a multi-valued instrument, Heckman and Pinto (2018) propose a condition called \"unordered monotonicity,\" which requires that for each $0 \\leq j \\leq$ $J-1$ and $z, z^{\\prime} \\in \\mathcal{Z}$, either $Q\\left\\{I\\left\\{D_{z}=j\\right\\} \\leq I\\left\\{D_{z^{\\prime}}=j\\right\\}\\right\\}=1$ or $Q\\left\\{I\\left\\{D_{z^{\\prime}}=j\\right\\} \\leq I\\left\\{D_{z}=j\\right\\}\\right\\}=1$. Sun (2023) derives a set of inequalities that are implied by this assumption. We note that this assumption is not implied by, nor does it imply, Assumption 2.3.",
      "tables": {
        "table_0": "| $D_{0}$ | $D_{1}$ | $D_{2}$ |\n| :--: | :--: | :--: |\n| 0 | 0 | 0 |\n| 1 | 1 | 1 |\n| 2 | 2 | 2 |\n| 0 | 1 | 0 |\n| 0 | 0 | 2 |\n| 1 | 1 | 2 |\n| 0 | 1 | 1 |\n| 2 | 1 | 2 |\n| 0 | 2 | 2 |\n| 0 | 1 | 2 |",
        "table_1": "| $D_{0}$ | $D_{2}$ |\n| :--: | :--: |\n| 0 | 0 |\n| 1 | 1 |\n| 2 | 2 |\n| 0 | 2 |\n| 1 | 2 |",
        "table_2": "| $D_{0}$ | $D_{1}$ | $D_{2}$ |\n| :--: | :--: | :--: |\n| 0 | 0 | 0 |\n| 0 | 1 | 0 |\n| 0 | 0 | 2 |\n| 1 | 1 | 1 |\n| 1 | 1 | 2 |\n| 2 | 2 | 2 |\n| 2 | 1 | 2 |\n| 0 | 1 | 2 |"
      },
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 3 Main Results \n\nIn this section, we present our main results on sharp testable implications of Assumptions 2.12.3. In order to do so, in Section 3.1, we first derive inequalities in terms of the conditional choice\n\nprobabilities. Then, in Section 3.2, for each $P$ that satisfies these inequalities, we explicitly construct a distribution $Q \\in \\mathbf{Q}_{1}$ such that $P=Q T^{-1}$, thus showing the inequalities are sharp.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 3.1 Testable implications \n\nThe following theorem characterizes a set of necessary conditions in order for $P$ to be consistent with $\\mathbf{Q}_{1}$. Recall $\\mathcal{Z}=\\left\\{0, J_{0}, \\ldots, J-1\\right\\}$. We further define\n\n$$\n\\mathcal{Z}(j)= \\begin{cases}\\mathcal{Z}, & \\text { for } 0 \\leq j \\leq J_{0}-1 \\\\ \\mathcal{Z} \\backslash\\{j\\}, & \\text { for } J_{0} \\leq j \\leq J-1\\end{cases}\n$$\n\nHere, when $J_{0}=0$, it is understood that $\\mathcal{Z}(j)=\\mathcal{Z} \\backslash\\{j\\}$ for $0 \\leq j \\leq J-1$.\nTheorem 3.1. Suppose $P=Q T^{-1}$ for $Q \\in \\mathbf{Q}_{1}$. Then, for $z(j) \\in \\mathcal{Z}(j), 0 \\leq j \\leq J-1$,\n\n$$\n\\sum_{0 \\leq j \\leq J-1} P\\{D=j \\mid Z=z(j)\\} \\leq 1\n$$\n\nThe inequalities in (11) are direct consequences of the restriction in (3). To see why, first suppose $J_{0}=0$, so that $z(j) \\neq j$ for $0 \\leq j \\leq J-1$, and consider the events\n\n$$\n\\left\\{D_{z(0)}=0\\right\\}, \\ldots,\\left\\{D_{z(J-1)}=J-1\\right\\}\n$$\n\nFix $\\omega \\in \\Omega$. Because $z(j) \\neq j$ for all $j, D_{z(j)}(\\omega)=j$ and (3) imply that the default choice $j^{*}(\\omega)=j$. As a result, the events listed above are disjoint across $0 \\leq j \\leq J-1$, so their probabilities sum up to less than one, and (11) follows. When $J_{0}>0, D_{0}(\\omega)=j$ implies that $j^{*}(\\omega)=j$, and hence $\\left\\{D_{0}=j\\right\\}$ is disjoint from all other events as well. Therefore, (11) holds in addition when $z(j)=0$ for $0 \\leq j \\leq J_{0}-1$.\n\nThe inequalities in (11) restrict the substitution patterns jointly at different values of the instrument $z(0), \\ldots, z(J-1)$. In particular, it does not suffice to consider pairwise substitution patterns when the instrument changes from one value to another. Note that $z(0), \\ldots, z(J-1)$ do not have to be distinct. Therefore, for $j \\neq k$, by setting $z(j)=k$ and $z(\\ell)=j$ for all $\\ell \\neq j$, we have\n\n$$\nP\\{D=j \\mid Z=k\\}+\\sum_{\\ell \\neq j} P\\{D=\\ell \\mid Z=j\\} \\leq 1\n$$\n\nwhich implies\n\n$$\nP\\{D=j \\mid Z=k\\} \\leq P\\{D=j \\mid Z=j\\}\n$$\n\nThe inequality in (12) states that the conditional probability of choosing $j$ is maximized at $Z=j$, which aligns with the intuition that $Z=j$ \"encourages\" towards $D=j$.\n\nExample 3.1. When $J=2$ and $J_{0}=0$, Theorem 3.1 implies only one inequality:\n\n$$\nP\\{D=1 \\mid Z=0\\} \\leq P\\{D=1 \\mid Z=1\\}\n$$\n\nWhen $J=3$ and $J_{0}=0$, (12) leads to six inequalities:\n\n$$\n\\begin{aligned}\n& P\\{D=0 \\mid Z=1\\} \\leq P\\{D=0 \\mid Z=0\\} \\\\\n& P\\{D=0 \\mid Z=2\\} \\leq P\\{D=0 \\mid Z=0\\} \\\\\n& P\\{D=1 \\mid Z=2\\} \\leq P\\{D=1 \\mid Z=1\\} \\\\\n& P\\{D=1 \\mid Z=0\\} \\leq P\\{D=1 \\mid Z=1\\} \\\\\n& P\\{D=2 \\mid Z=0\\} \\leq P\\{D=2 \\mid Z=2\\} \\\\\n& P\\{D=2 \\mid Z=1\\} \\leq P\\{D=2 \\mid Z=2\\}\n\\end{aligned}\n$$\n\nIn addition, by considering the cases where $z(0), z(1), z(2)$ are all distinct in (11), we end up with two additional inequalities:\n\n$$\n\\begin{aligned}\n& P\\{D=1 \\mid Z=0\\}+P\\{D=2 \\mid Z=1\\}+P\\{D=0 \\mid Z=2\\} \\leq 1 \\\\\n& P\\{D=2 \\mid Z=0\\}+P\\{D=0 \\mid Z=1\\}+P\\{D=1 \\mid Z=2\\} \\leq 1\n\\end{aligned}\n$$\n\nThese two inequalities involve the substitution patterns across triplets of values of the instrument instead of simple pairwise comparison. In total, we obtain eight inequalities. For a general $J$, when $J_{0}=0$, we obtain $(J-1)^{J}$ inequalities.\n\nWhen $J_{0}>0$, we obtain the following simplification of the inequalities:\nCorollary 3.1. Suppose $P=Q T^{-1}$ for $Q \\in \\mathbf{Q}_{1}$ and $J_{0}>0$. Then, the inequalities described by (11) are equivalent to the statement that, for $0 \\leq j \\leq J-1$ and $k \\in \\mathcal{Z}$ such that $j \\neq k$,\n\n$$\nP\\{D=j \\mid Z=k\\} \\leq P\\{D=j \\mid Z=0\\}\n$$\n\nTo see why the inequalities in Corollary 3.1 follow from the ones in Theorem 3.1, first note that for $0 \\leq j \\leq J-1$ and $k \\in \\mathcal{Z}$ such that $j \\neq k$, by setting $z(j)=k \\in \\mathcal{Z}(j)$ and $z(\\ell)=0 \\in \\mathcal{Z}(\\ell)$ for $\\ell \\neq j$ in (11), we get\n\n$$\n\\sum_{\\ell \\neq j} P\\{D=\\ell \\mid Z=0\\}+P\\{D=j \\mid Z=k\\} \\leq 1\n$$\n\nwhich implies (13) immediately. On the other hand, suppose (13) holds for $0 \\leq j \\leq J-1$ and $k \\in \\mathcal{Z}$ such that $j \\neq k$. For $z(j) \\in \\mathcal{Z}(j)$ for $0 \\leq j \\leq J-1$, we have $z(j) \\neq j$ for $J_{0} \\leq j \\leq J-1$ and\n\n$z(j) \\in\\left\\{0, J_{0}, \\ldots, J-1\\right\\}$ for $0 \\leq j \\leq J_{0}-1$, and hence (13) implies\n\n$$\n\\sum_{0 \\leq j \\leq J-1} P\\{D=j \\mid Z=z(j)\\} \\leq \\sum_{0 \\leq j \\leq J-1} P\\{D=j \\mid Z=0\\} \\leq 1\n$$\n\nso (11) follows.\nThe inequality in (13) follows immediately from the restrictions in (4). To see that, suppose $D_{k}=j$ for $k \\neq j$. Because (4) implies $D_{k} \\in\\left\\{k, D_{0}\\right\\}$ and $k \\neq j$, it has to be the case that $D_{0}=j$. Therefore,\n\n$$\n\\left\\{D_{k}=j\\right\\} \\Longrightarrow\\left\\{D_{0}=j\\right\\}\n$$\n\nwhich immediately implies (13). An interesting feature of Corollary 3.1 is that only the substitution patterns between $Z=0$ and $Z=k$ appear in the testable implications, but the substitution patterns between $Z=k$ and $Z=\\ell$ for $k, \\ell \\neq 0$ and $k \\neq \\ell$ do not. Surprisingly, as we show in the next section, these inequalities exhaust all the information in the restrictions imposed by the model $\\mathbf{Q}_{1}$. That is, as long as $P$ satisfies (13), $P=Q T^{-1}$ for some $Q \\in \\mathbf{Q}_{1}$. As a result, when $J_{0}>0$, all information in the data about its consistency with the model is contained in the pairwise comparison between the choices when $Z=0$ versus $Z=k$. Before proceeding, we revisit the Examples 2.2-2.3 and apply Corollary 3.1.\n\nExample 3.2. Recall in Example 2.2 that $\\mathcal{Z}=\\{0,2\\}$ and $J_{0}=2$. In this case, two inequalities follow from (13):\n\n$$\n\\begin{aligned}\n& P\\{D=0 \\mid Z=2\\} \\leq P\\{D=0 \\mid Z=0\\} \\\\\n& P\\{D=1 \\mid Z=2\\} \\leq P\\{D=1 \\mid Z=0\\}\n\\end{aligned}\n$$\n\nThere is no additional inequality for $j=2$.\nExample 3.3. Recall in Example 2.3 that $\\mathcal{Z}=\\{0,1,2\\}$ and $J_{0}=1$. In this case, the following four inequalities follow from (13):\n\n$$\n\\begin{aligned}\n& P\\{D=0 \\mid Z=1\\} \\leq P\\{D=0 \\mid Z=0\\} \\\\\n& P\\{D=0 \\mid Z=2\\} \\leq P\\{D=0 \\mid Z=0\\} \\\\\n& P\\{D=1 \\mid Z=2\\} \\leq P\\{D=1 \\mid Z=0\\} \\\\\n& P\\{D=2 \\mid Z=1\\} \\leq P\\{D=2 \\mid Z=0\\}\n\\end{aligned}\n$$\n\nThese inequalities and their counterparts with an outcome will be used in Section 7 to test $J_{0}=1$ in an empirical application to the dataset for Behaghel et al. (2013, 2014).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 3.2 Sharpness of the Implications \n\nOur next theorem is the converse of Theorem 3.1-namely, for each $P$ that satisfies (11), there exists a distribution $Q \\in \\mathbf{Q}_{1}$ such that $P=Q T^{-1}$. In other words, the inequalities in Theorem 3.1 are sharp in the sense that they exhaust all the information in the behavioral restrictions imposed by the model $\\mathbf{Q}_{1}$. In the special case of $J=2$, a constructive proof was provided by Kitagawa (2015), but it does not extend to the case when $J>2$. In particular, the construction in Kitagawa (2015) relies crucially on the observation that if $D=1$ when $Z=0$, then $\\left(D_{0}, D_{1}\\right)=(1,1)$; this subgroup of individuals are referred to as the \"always takers,\" and the group that takes $D=0$ when $Z=1$ are called the \"never takers.\" The remaining probability mass is then assigned to the \"compliers,\" for whom $\\left(D_{0}, D_{1}\\right)=(0,1)$. When $J>2$, however, it is in general impossible to pin down the joint distribution of potential treatments using this argument, and hence the proof requires an entirely new strategy. Importantly, the proof that we present is still constructive, in that we construct $Q$ explicitly from the given distribution $P$.\n\nTheorem 3.2. Let $P$ be a probability distribution on $\\{0, \\ldots, J-1\\} \\times \\mathcal{Z}$ such that $P\\{Z=z\\}>0$ for every $z \\in \\mathcal{Z}$. Further suppose (11) holds for all $z(j) \\in \\mathcal{Z}(j), 0 \\leq j \\leq J-1$. Then, there exists a $Q \\in \\mathbf{Q}_{1}$ such that $P=Q T^{-1}$.\n\nTo illustrate why (11) is sufficient for determining whether $P$ is consistent with the model $\\mathbf{Q}_{1}$, we start by sketching the construction when $J=3$ and $J_{0}=0$. Let $Q^{*}$ denote a candidate distribution for which we wish to show $P=Q^{*} T^{-1}$ and $Q^{*} \\in \\mathbf{Q}_{1}$. We separately consider four classes of potential treatment vectors according to the value of the default choice $j^{*}$ and assign $Q^{*}$ separately for each class, such that $P=Q^{*} T^{-1}$.\n(a) $j^{*}=0$. Consider $P\\{D=0 \\mid Z=z\\}$. First note that if $P=Q^{*} T^{-1}$, then for $z \\in\\{0,1,2\\}$,\n\n$$\nP\\{D=0 \\mid Z=z\\}=Q^{*}\\left\\{D_{z}=0\\right\\} \\geq Q^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,0,0)\\right\\}\n$$\n\nRespecting this constraint, we set\n\n$$\nQ^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,0,0)\\right\\}=\\min _{z \\in \\mathcal{Z}} P\\{D=0 \\mid Z=z\\}\n$$\n\nThe inequalities in (12) imply the minimum on the right-hand side is attained at $z \\in\\{1,2\\}$, and without loss of generality suppose it is attained at $z=1$. If it is attained at $z=2$, then a symmetric construction applies. Next, note any candidate $Q^{*}$ has to satisfy\n\n$$\n\\begin{aligned}\nP\\{D=0 \\mid Z=2\\}=Q^{*}\\left\\{D_{2}=0\\right\\} & =Q^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,0,0)\\right\\}+Q^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,1,0)\\right\\} \\\\\n& =P\\{D=0 \\mid Z=1\\}+Q^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,1,0)\\right\\}\n\\end{aligned}\n$$\n\nand hence we have to define\n\n$$\nQ^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,1,0)\\right\\}=P\\{D=0 \\mid Z=2\\}-P\\{D=0 \\mid Z=1\\}\n$$\n\nThis step stops here. Note we have assigned no mass to $(0,0,2)$, so that\n\n$$\nQ^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,0,2)\\right\\}=0\n$$\n\nThe total mass we have assigned in this step is\n\n$$\nQ^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,0,0)\\right\\}+Q^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,1,0)\\right\\}=\\max _{z \\neq 0} P\\{D=0 \\mid Z=z\\}\n$$\n\nIn all the events we have considered so far, $j^{*}=0$, so that 0 is the default choice. It is chosen for at least two values of the instrument.\n(b) $j^{*}=1$. Similarly as in (a), carry out the construction for events corresponding to $P\\{D=1 \\mid Z=$ $z\\}$. The total mass assigned in this step is\n\n$$\n\\max _{z \\neq 1} P\\{D=1 \\mid Z=z\\}\n$$\n\n(c) $j^{*}=2$. Similarly as in (a), carry out the construction for events corresponding to $P\\{D=2 \\mid Z=$ $z\\}$. The total mass assigned in this step is\n\n$$\n\\max _{z \\neq 2} P\\{D=2 \\mid Z=z\\}\n$$\n\nAll of the events we have considered so far are disjoint. To see it, note the default choice is different in each class (a), (b) and (c), and it is chosen for at least two values of the instrument. For all other values of the instrument, the choice has to coincide with the instrument. Therefore, these events cannot intersect across classes. They are furthermore all disjoint from the final event:\n(d) Diagonal: Note the sum of the masses that we have assigned when considering $j=0,1,2$ is\n\n$$\n\\sum_{0 \\leq j \\leq 2} \\max _{z(j) \\neq j} P\\{D=j \\mid Z=z(j)\\} \\leq 1\n$$\n\nbecause of (11). We then assign all of the remaining mass to\n\n$$\nQ^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,1,2)\\right\\}=1-\\sum_{0 \\leq j \\leq 2} \\max _{z(j) \\neq j} P\\{D=j \\mid Z=z(j)\\} \\geq 0\n$$\n\n$Q^{*}$ is clearly a probability measure. We now show $P=Q^{*} T^{-1}$. It suffices to verify $Q^{*}\\left\\{D_{z}=\\right.$\n\n$j\\}=P\\{D=j \\mid Z=z\\}$ for $j \\in\\{0,1,2\\}$ and $z \\in\\{0,1,2\\}$. We start by verifying that $Q^{*}\\left\\{D_{z}=0\\right\\}=$ $P\\{D=0 \\mid Z=z\\}$ for all $z$. Note $D_{1}=0$ and $D_{2}=0$ is only allowed in the events in (a) above, so that\n\n$$\n\\begin{aligned}\nQ^{*}\\left\\{D_{1}=0\\right\\} & =Q^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,0,0)\\right\\}=P\\{D=0 \\mid Z=1\\} \\\\\nQ^{*}\\left\\{D_{2}=0\\right\\} & =Q^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,0,0)\\right\\}+Q^{*}\\left\\{\\left(D_{0}, D_{1}, D_{2}\\right)=(0,1,0)\\right\\} \\\\\n& =P\\{D=0 \\mid Z=1\\}+P\\{D=0 \\mid Z=2\\}-P\\{D=0 \\mid Z=1\\} \\\\\n& =P\\{D=0 \\mid Z=2\\}\n\\end{aligned}\n$$\n\nFollowing similar arguments, we can show that\n\n$$\nQ^{*}\\left\\{D_{z}=j\\right\\}=P\\{D=j \\mid Z=z\\}\n$$\n\nfor $0 \\leq z \\leq 2,0 \\leq j \\leq 2$, and $z \\neq j$. Given these equalities, for each $k \\in\\{0,1,2\\}$,\n\n$$\n\\begin{aligned}\nQ^{*}\\left\\{D_{k}=k\\right\\} & =1-\\sum_{0 \\leq j \\leq 2: j \\neq k} Q^{*}\\left\\{D_{k}=j\\right\\} \\\\\n& =1-\\sum_{0 \\leq j \\leq 2: j \\neq k} P\\{D=j \\mid Z=k\\} \\\\\n& =P\\{D=k \\mid Z=k\\}\n\\end{aligned}\n$$\n\nWe have therefore successfully shown that $Q^{*}\\left\\{D_{z}=j\\right\\}$ for $0 \\leq z \\leq 2$ and $0 \\leq j \\leq 2$, and hence $P=Q^{*} T^{-1}$.\n\nThe proof for general $J$ when $J_{0}=0$ follows similar arguments as in the previous sketch. Although we won't present the full proof in the main text, here we present some intuition on why the proof works in general. Note that although individuals cannot be classified into the three subgroups beyond the binary setting, each potential treatment vector permitted by (3) can still be characterized by the default choice $j^{*}$ together with the set\n\n$$\n\\left\\{z: D_{z}=z\\right\\}\n$$\n\nIn other words, any potential treatment vector permitted by (3) can be completely characterized by the default choice as well as the set of choices towards which the individual complies with the encouragement. For example, a person with $\\left(D_{0}, D_{1}, D_{2}, D_{3}, D_{4}\\right)=(0,0,2,0,4)$ can be thought of as a \" 0 -default, $\\{2,4\\}$-complier.\" Similarly, we can call someone with $\\left(D_{0}, D_{1}, D_{2}, D_{3}, D_{4}\\right)=$ $(0,0,0,0,0)$ a \" 0 -always taker.\" For each default value $0 \\leq j \\leq J-1$, we order $0 \\leq z \\leq J-1$ so that\n\n$$\nP\\left\\{D=j \\mid Z=z_{1}(j)\\right\\} \\leq \\cdots \\leq P\\left\\{D=j \\mid Z=z_{J}(j)\\right\\}\n$$\n\nFor this specific $j$, in step 1 , we first pin down the probability of \" $j$-always takers\" as\n\n$$\nP\\left\\{D=j \\mid Z=z_{1}(j)\\right\\}=\\min _{0 \\leq z \\leq J-1} P\\{D=j \\mid Z=z\\}\n$$\n\nThen, in step $\\ell$ for $2 \\leq \\ell \\leq J-1$, for $\\mathcal{J}_{\\ell}=\\left\\{z_{1}(j), \\ldots, z_{\\ell-1}(j)\\right\\}$, we define the probability of \" $j$-default, $\\mathcal{J}_{\\ell}$-compliers\" as\n\n$$\nP\\left\\{D=j \\mid Z=z_{\\ell}(j)\\right\\}-P\\left\\{D=j \\mid Z=z_{\\ell-1}(j)\\right\\}\n$$\n\nBecause we conclude at step $\\ell=J-1$, the total probability assigned for this specific $j$ is then\n\n$$\nP\\left\\{D=j \\mid Z=z_{J-1}(j)\\right\\}\n$$\n\nThe key reason why the construction guarantees $P=Q^{*} T^{-1}$ is as follows. If $z \\neq j$, then (12) implies $z=z_{\\ell}(j)$ for some $1 \\leq \\ell \\leq J-1$. As summarized in Table 4, $D_{z}=j$ happens only for \" $j$-always takers,\" \" $j$-default, $\\mathcal{J}_{2}$-compliers,\" through \" $j$-default, $\\mathcal{J}_{\\ell}$-compliers,\" whose probabilities sum up to\n\n$$\nP\\left\\{D=j \\mid Z=z_{\\ell}(j)\\right\\}=P\\{D=j \\mid Z=z\\}\n$$\n\nas can be seen from Table 4.\n\n![table_3](table_3)\n\nTable 4: Subgroups of individuals and their potential choices $D_{z_{\\ell}(j)}$.\n\nAfter carrying out this construction for each $j$, the remaining mass of $Q^{*}$ is assigned to the \"diagonal\" event that $\\left(D_{0}, \\ldots, D_{J-1}\\right)=(0, \\ldots, J-1)$, and the rest of the proof follows similarly as that for $J=3$. The proof when $J_{0}>0$ builds on the proof when $J_{0}=0$ and requires further modifications.\n\nRemark 3.1. In the special cases of $\\left(J, J_{0}\\right)=(3,1)$ and $\\left(J, J_{0}\\right)=(3,2)$, Lee and Salani\u00e9 (2024) present the testable implications in (13) (along with some redundant inequalities), but do not discuss\n\ntheir sharpness. The testable implications they present also do not include an outcome. In Section 4.1 below, we derive the sharp testable implications when an outcome is additionally considered.\n\nRemark 3.2. One may also consider obtaining the sharp inequalities using a random set approach (Beresteanu et al., 2012) based on Artstein's inequalities, or through a linear programming approach. These approaches provide implicit characterizations of the problem, by stating that $P$ is consistent with $\\mathbf{Q}_{1}$ as long as certain linear systems have nonnegative solutions. They need to be implemented case-by-case for each value of $J$ and preclude the consideration of a continuous outcome. Consider the case $J_{0}=0$ as an example. To introduce the random set approach, following Luo et al. (2024), let $\\mathcal{S}$ denote the support of $\\left(D_{0}, \\ldots, D_{J-1}\\right)$ allowed by (3). For $0 \\leq z \\leq J-1$, further define\n\n$$\nB_{z}(D)=\\left\\{\\left(D_{0}, \\ldots, D_{J-1}\\right) \\in\\{0, \\ldots, J-1\\}^{J}: D_{z}=D\\right\\}\n$$\n\nDefine\n\n$$\nG(D, Z)=\\sum_{z \\in \\mathcal{Z}} I\\{Z=z\\} B_{z}(D) \\cap \\mathcal{S}\n$$\n\nThe model predicts that $\\left(D_{0}, \\ldots, D_{J-1}\\right) \\in G(D, Z)$, which by Artstein's inequalities is equivalent to requiring for all $A \\subseteq\\{0, \\ldots, J-1\\}^{J}$, that\n\n$$\nQ\\left\\{\\left(D_{0}, \\ldots, D_{J-1}\\right) \\in A \\mid Z=z\\right\\} \\geq Q\\{G(D, Z) \\subseteq A \\mid Z=z\\}\n$$\n\nfor $0 \\leq z \\leq J-1$. One would then need to find the core-determining class of the sets $A$, and the problem then becomes characterizing when a linear system has a nonnegative solution. Another approach is to determine whether there exists a probability measure $Q$ that satisfies (2) and (3) (or (4) $J_{0}>0$ ) for $0 \\leq j \\leq J-1$ and $0 \\leq z \\leq J-1$. See, for example, Bai et al. (2024a) for a detailed description. Such an approach is again equivalent to determining whether a linear system has a nonnegative solution. Through solving what is called a facet enumeration problem, one can further convert these implicit characterizations into a closed-form characterization like ours, but such a step is computationally prohibitive unless $J$ is very small.",
      "tables": {
        "table_3": "| Subgroup | $D_{z}=j$ for | $D_{z_{\\ell}(j)}$ |\n| :--: | :--: | :--: |\n| $j$-always takers | all $z$ | $j$ |\n| $j$-default, $\\mathcal{J}_{2}$-compliers | $z \\neq z_{1}(j)$ | $j$ |\n| $j$-default, $\\mathcal{J}_{3}$-compliers | $z \\notin\\left\\{z_{1}(j), z_{2}(j)\\right\\}$ | $j$ |\n| $\\vdots$ | $\\vdots$ | $\\vdots$ |\n| $j$-default, $\\mathcal{J}_{\\ell}$-compliers | $z \\notin\\left\\{z_{1}(j), \\ldots, z_{\\ell-1}(j)\\right\\}$ | $j$ |\n| $j$-default, $\\mathcal{J}_{\\ell+1}$-compliers | $z \\notin\\left\\{z_{1}(j), \\ldots, z_{\\ell}(j)\\right\\}$ | $z_{\\ell}(j)$ |\n| $\\vdots$ | $\\vdots$ | $\\vdots$ |\n| $j$-default, $\\mathcal{J}_{J-1}$-compliers | $z \\notin\\left\\{z_{1}(j), \\ldots, z_{J-2}(j)\\right\\}$ | $z_{\\ell}(j)$ |"
      },
      "images": {}
    },
    {
      "section_id": 6,
      "text": "# 4 Extensions\n### 4.1 Results with an Outcome\n\nIn this section, we present the general results with an outcome variable. The discussion runs mostly in parallel with Section 3. Let $Y \\in \\mathbf{R}$ denote an observed outcome and $Y_{d}$ for $0 \\leq d \\leq J-1$ denote the potential outcome under treatment choice $d$. We allow $Y$ to be continuous or discrete and denote\n\nits support ${ }^{4}$ by $\\mathcal{Y}$. In addition to (1), the observed outcome and the potential outcomes are related through\n\n$$\nY=\\sum_{0 \\leq d \\leq J-1} Y_{d} I\\{D=d\\}\n$$\n\nWith some abuse of notation, we continue letting $T(\\cdot)$ denote the mapping defined by the equation above together with (1). Let $P$ denote the distribution of $(Y, D, Z)$ and $Q$ denote the distribution of $\\left(Y_{0}, \\ldots, Y_{J-1},\\left(D_{z}: z \\in \\mathcal{Z}\\right), Z\\right)$. We modify Assumption 2.1 to include the potential outcomes:\n\nAssumption 4.1. $\\left(Y_{0}, \\ldots, Y_{J-1},\\left(D_{z}: z \\in \\mathcal{Z}\\right), Z\\right) \\Perp Z$ under $Q$.\nLet $\\mathbf{Q}_{1}^{Y}$ denote the set of all distributions $Q$ for which Assumptions 4.1 and 2.2 as well as (3) hold. We first present the counterpart to Theorems 3.1 and 3.2.\n\nTheorem 4.1. Let $P$ be a probability distribution on $\\mathcal{Y} \\times\\{0, \\ldots, J-1\\} \\times \\mathcal{Z}$ such that $P\\{Z=z\\}>0$ for every $z \\in \\mathcal{Z}$. Then, $P=Q T^{-1}$ for $Q \\in \\mathbf{Q}_{1}^{Y}$ if and only if both of the following sets of conditions hold:\n(a) If for each $0 \\leq j \\leq J-1$, the Borel sets $\\left\\{B_{z}(j): z \\in \\mathcal{Z}(j)\\right\\}$ form a partition of $\\mathcal{Y}$, then\n\n$$\n\\sum_{0 \\leq j \\leq J-1} \\sum_{z \\in \\mathcal{Z}(j)} P\\left\\{Y \\in B_{z}(j), D=j \\mid Z=z\\right\\} \\leq 1\n$$\n\n(b) For each Borel set $B \\subseteq \\mathcal{Y}, J_{0} \\leq j \\leq J-1$, and $k \\neq j$,\n\n$$\nP\\{Y \\in B, D=j \\mid Z=k\\} \\leq P\\{Y \\in B, D=j \\mid Z=j\\}\n$$\n\nFor $z(j) \\in \\mathcal{Z}(j), 0 \\leq j \\leq J-1$, note that by taking $B_{z(j)}(j)=\\mathcal{Y}$ and $B_{z}(j)=\\emptyset$ for $0 \\leq j \\leq J-1$ and $z \\neq z(j)$ in (14), we recover (11).\n\nAs in Section 3.1, when $J_{0}>0$, we obtain the following simplification of the inequalities:\nCorollary 4.1. Suppose $J_{0}>0$. Then, $P \\in \\mathbf{Q}_{1}^{Y} T^{-1}$ if and only if Theorem 4.1(b) holds and for all Borel sets $B \\subseteq \\mathcal{Y}, 0 \\leq j \\leq J-1$ and $j \\neq k$,\n\n$$\nP\\{Y \\in B, D=j \\mid Z=k\\} \\leq P\\{Y \\in B, D=j \\mid Z=0\\}\n$$\n\nTo see why Corollary 4.1 holds, first note for $0 \\leq j \\leq J-1$ and $J_{0} \\leq k \\leq J-1$, by taking $B_{k}(j)=B, B_{0}(j)=\\mathcal{Y} \\backslash B$, and $B_{0}(\\ell)=\\mathcal{Y}$ and $B_{z}(\\ell)=\\emptyset$ for $\\ell \\neq j$ and $z \\neq 0$, we get\n\n$$\nP\\{Y \\in B, D=j \\mid Z=k\\}+P\\{Y \\notin B, D=j \\mid Z=0\\}+\\sum_{\\ell \\neq j} P\\{Y \\in \\mathcal{Y}, D=\\ell \\mid Z=0\\} \\leq 1\n$$\n\n[^0]\n[^0]:    ${ }^{4}$ Following pp.73-74 of Lifshits (1995), we define the (topological) support of $Y$ as the smallest closed set with probability one under $P$, i.e., $\\mathcal{Y}:=\\bigcap\\{F \\subseteq \\mathbf{R}: F$ closed,$P\\{Y \\in F\\}=1\\}$.\n\nfrom which we immediately obtain (16). On the other hand, suppose (16) holds for all $0 \\leq j \\leq J-1$, $J_{0} \\leq k \\leq J-1$, and $j \\neq k$, and for $0 \\leq j \\leq J-1$, Borel sets $\\left\\{B_{z}(j): z \\in \\mathcal{Z}(j)\\right\\}$ form a partition of $\\mathcal{Y}$. Then, (14) holds because\n\n$$\n\\sum_{0 \\leq j \\leq J-1} \\sum_{z \\in \\mathcal{Z}(j)} P\\left\\{Y \\in B_{z}(j), D=j \\mid Z=z\\right\\} \\leq \\sum_{0 \\leq j \\leq J-1} \\sum_{z \\in \\mathcal{Z}(j)} P\\left\\{Y \\in B_{z}(j), D=j \\mid Z=0\\right\\}=1\n$$\n\nRemark 4.1. When $J=2$, the only inequalities implied by Theorem 4.1 are\n\n$$\n\\begin{aligned}\n& P\\{Y \\in B, D=1 \\mid Z=0\\} \\leq P\\{Y \\in B, D=1 \\mid Z=1\\} \\\\\n& P\\{Y \\in B, D=0 \\mid Z=1\\} \\leq P\\{Y \\in B, D=0 \\mid Z=0\\}\n\\end{aligned}\n$$\n\nNote that these inequalities coincide with the simplification described in Corollary 4.1 when $J_{0}=1$. Furthermore, these inequalities are exactly those derived by Balke and Pearl (1997a,b) and Kitagawa (2015). We emphasize, however, that both the inequalities and the arguments to establish sharpness are novel beyond this setting.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 4.2 One-Sided Noncompliance \n\nIn this section, we extend our results to settings with one-sided noncompliance. Formally, let $\\mathcal{Z}=$ $\\{0, \\ldots, J-1\\}$ and $\\mathbf{Q}_{1,0}$ denote the collection of all distributions of $\\left(D_{0}, \\ldots, D_{J-1}, Z\\right)$ such that Assumptions 2.1-2.2 hold and\n\n$$\nQ\\left\\{D_{j} \\in\\{j, 0\\}\\right\\}=1 \\text { for all } j\n$$\n\nThe condition in (17) requires that when assigned $Z=j$, the subject either takes up $D=j$ or the control status $D=0$. Therefore, noncompliance can only be one-sided ( $j$ to 0 ) instead of the other way around. The only difference between (17) and (4) is that we additionally require $D_{0}=0$. Such a setting is prevalent in economics, especially if the instrument is the \"gate-keeper\" or eligibility for each program, so that one either takes up the program they are eligible for or falls back to the control status. As discussed in Example 2.3, assuming the \"next-best\" condition in Kirkeboen et al. (2016) together with (6)-(7) and (8)-(9) is equivalent to (17). See Angrist et al. (2009) for another example. The following theorem presents the sharp testable implications of (17) without and with an outcome. As in Section 4.1, let $\\mathbf{Q}_{1,0}^{Y}$ denote set of all distributions $Q$ for which Assumptions 4.1 and 2.2 as well as (17) hold.\n\nTheorem 4.2. (a) $P=Q T^{-1}$ for some $Q \\in \\mathbf{Q}_{1,0}$ if and only if $P\\{D=j \\mid Z=z\\}=0$ for $j \\notin\\{z, 0\\}$.\n(b) $P=Q T^{-1}$ for some $Q \\in \\mathbf{Q}_{1,0}^{Y}$ if and only if $P\\{D=j \\mid Z=z\\}=0$ for $j \\notin\\{z, 0\\}$, and for each Borel set $B \\subseteq \\mathcal{Y}$ and each $0 \\leq j \\leq J-1$,\n\n$$\nP\\{Y \\in B, D=0 \\mid Z=j\\} \\leq P\\{Y \\in B, D=0 \\mid Z=0\\}\n$$\n\nWe note that the results in Theorem 4.2 follow from Corollary 4.1 once we impose that $P\\{D=$ $j \\mid Z=z\\}=0$ for $j \\notin\\{z, 0\\}$. Indeed, with this additional restriction, (15) is vacuous because the left-hand side is always 0 . At the same time, both sides of (16) are 0 for $j \\neq 0$, so (16) is only meaningful when $j=0$, becoming (18).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "# 5 Inference \n\nIn this section, using the characterizations in Theorem 4.1 or Corollary 4.1, we construct tests to assess if the model is consistent with the distribution of the data. Formally, we test\n\n$$\nH_{0}: \\quad \\exists Q \\in \\mathbf{Q}_{1}^{Y} \\text { such that } P=Q T^{-1}\n$$\n\nin a way that is uniform in level across a large class of distributions. We now separately discuss tests for (19) according to whether $\\mathcal{Y}$ is discrete or continuous.\n\nIf $\\mathcal{Y}$ is discrete (or is discretized ex-ante), then Theorem 4.1 and Corollary 4.1 generate a finite number of inequalities. In particular, (14) becomes\n\n$$\n\\sum_{0 \\leq j \\leq J-1} \\sum_{y \\in \\mathcal{Y}} P\\{Y=y, D=j \\mid Z=z(j, y)\\} \\leq 1\n$$\n\nfor $z(j, y) \\in \\mathcal{Z}(j), 0 \\leq j \\leq J-1$. The inequalities in (15) become that for each $y \\in \\mathcal{Y}$,\n\n$$\nP\\{Y=y, D=j \\mid Z=k\\} \\leq P\\{Y=y, D=j \\mid Z=j\\}\n$$\n\nIn addition, the inequalities in (16) become that for each $y \\in \\mathcal{Y}$,\n\n$$\nP\\{Y=y, D=j \\mid Z=k\\} \\leq P\\{Y=y, D=j \\mid Z=0\\}\n$$\n\nThe inequalities in (20)-(22) can be tested using any off-the-shelf inference method for a finite number of moment inequalities. See, for instance, Canay and Shaikh (2017) for an overview. Here we sketch how we can convert the problem into testing the feasibility of a linear program, so that we can directly apply recent results in, for instance, Fang et al. (2023). Suppose $\\mathcal{Y}$ is discrete and let $p$ denote the vector of $(P\\{Y=y, D=j \\mid Z=z\\}: y \\in \\mathcal{Y}, 0 \\leq j \\leq J-1, z \\in \\mathcal{Z})$. We can represent the inequalities in Theorem 4.1 and Corollary 4.1 as\n\n$$\n\\Gamma p-\\gamma \\leq 0\n$$\n\nwhere $\\Gamma$ and $\\gamma$ have known entries which lie in $\\{-1,0,1\\}$. With a vector of slack variables $x$, (23) is\n\nequivalent to\n\n$$\n\\begin{aligned}\nA x & =\\beta(P) \\\\\nx & \\geq 0\n\\end{aligned}\n$$\n\nwhere $A$ is the identity matrix and $\\beta(P)=\\gamma-\\Gamma p$. This formulation maps into the notation of Fang et al. (2023), and their tests apply immediately.\n\nIf $\\mathcal{Y}$ is continuous and we do not wish to discretize the outcome, then we focus on testing the inequalities in Corollary 4.1, which apply when $J_{0}>0$; it seems difficult to test the general inequalities described in (14) without first discretizing the outcome. Here we discuss how to test (16) with a continuous outcome. We could develop a test based on the K-S statistic in Kitagawa (2015), but following Mourifi\u00e9 and Wan (2017), we discuss a method that transforms the infinite number of inequalities in (16) into a conditional moment inequality where the conditioning variable is $Y$ instead of $Z$. Indeed, note (16) holds if and only if\n\n$$\nE[I\\{Y \\in B\\} I\\{D=j, Z=k\\}] P\\{Z=0\\} \\leq E[I\\{Y \\in B\\} I\\{D=j, Z=0\\}] P\\{Z=k\\}\n$$\n\nwhich holds for all Borel sets $B \\subseteq \\mathcal{Y}$ if and only if\n\n$$\nE[I\\{D=j, Z=k\\} P\\{Z=0\\}-I\\{D=j, Z=0\\} P\\{Z=k\\} \\mid Y] \\leq 0\n$$\n\nwith probability one for $Y$. Similarly, (15) holds for all Borel sets $B \\subseteq \\mathcal{Y}$ if and only if\n\n$$\nE[I\\{D=j, Z=k\\} P\\{Z=j\\}-I\\{D=j, Z=j\\} P\\{Z=k\\} \\mid Y] \\leq 0\n$$\n\nwith probability one for $Y$. The conditional moment inequalities in (24)-(25) could then be tested using any off-the-shelf inference method for conditional moment inequalities. See, for instance, Andrews and Shi (2013), Chernozhukov et al. (2013), Armstrong and Chan (2016), and Chetverikov (2018).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 6 Simulations \n\nIn this section, we study the properties of the inference procedures described in Section 5. Our goal is to illustrate the size control and power properties of our tests for (19), and to compare them with other procedures that are based on an implicit characterization of the inequalities, as discussed in Remark 3.2. We present the results separately for $J_{0}>0$ and $J_{0}=0$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# 6.1 Simulations for $J_{0}>0$ \n\nIn this subsection, we study tests of the null hypothesis in (19) for $J=4$ and $J_{0}=1$, so that $\\mathcal{D}=\\mathcal{Z}=\\{0,1,2,3\\}$. Throughout this subsection, $Z$ is uniformly distributed on $\\mathcal{Z}$. Let $\\left(\\beta_{1}, \\beta_{2}, \\beta_{3}\\right)=(1.5,1,0.5)$ and $\\beta_{0}$ be specified below. Further let $\\epsilon=\\left(\\epsilon_{0}, \\epsilon_{1}, \\epsilon_{2}, \\epsilon_{3}\\right)^{\\prime} \\sim N\\left(\\mu, I_{4}\\right)$, where $\\mu=(0.5,1,1.5,2)^{\\prime}$ and $I_{4}$ is the $4 \\times 4$ identity matrix. Note that when $\\beta_{0}=0$, the distribution $P$ satisfies the null in (19) with $J_{0}=1$; furthermore, for each $z \\in \\mathcal{Z}, E\\left[\\max _{j \\in \\mathcal{D}}\\left(U_{j}(z)+\\epsilon_{j}\\right)\\right]=$ $E\\left[U_{z}(z)+\\epsilon_{z}\\right]=2.5$, so that the mean utility of the choice encouraged by $z, d=z$, stays constant across different values of the instrument $z$, but the mean utility of the alternative choice $d \\neq z$ varies across $z$. The potential outcomes are determined as $Y_{d}=I\\{d \\geq 1\\}+\\xi$, where $\\xi \\Perp(\\epsilon, Z)$ and $\\xi \\sim N(0,1)$. Next, to assess the power of the tests, we further consider $\\beta_{0} \\in\\{-0.5,-1,-1.5\\}$, so that it can be verified through direct calculation that (13) is violated and hence the null in (19) is violated.\n\nWe implement several tests of (19) with $J_{0}=1$ at the $5 \\%$ level. First, similarly as in Mourifi\u00e9 and Wan (2017), we test (24)-(25) using Chernozhukov et al. (2013) and the accompanying clrtest package in stata (Chernozhukov et al., 2015). Table 5 presents the rejection probabilities in percentages. We implement both the parametric and local options for the estimation of the conditional moments and follow the choices of tuning parameters in Mourifi\u00e9 and Wan (2017). Second, we binarize $Y$ to $I\\{Y \\geq 1\\}$ and test (21)-(22) using Fang et al. (2023) and the accompanying lpinfer package in R (Lau and Torgovitsky, 2021). The results are displayed in Table 6. Finally, with the discretized $Y$ and using Fang et al. (2023), we directly test the implicit linear programming formulation in Remark 3.2, i.e., whether there exists a probability measure $Q$ that satisfies (2) ${ }^{5}$ and (4). The results are displayed in Table 7. We perform each test at sample sizes of 500,1000 , and 2000. For each value of $\\beta_{0}$, and each sample size, we calculate the rejection probabilities across 5000 replications.\n\nWe begin by noting in Table 5 that the results for testing (24)-(25) using Chernozhukov et al. (2013) are very sensitive to the methods for estimating the conditional moments. With the local approach, the test fails to control size even at $n=2000$. The seemingly higher power is likely a result of its poor size control. The parametric approach controls size well and has nontrivial power against $\\beta_{0} \\in\\{-1,-1.5\\}$. The drastic difference in the size control of the two approaches may be because the conditional moments in (24)-(25) can be reasonably approximated by linear functions in the current data generating process, as can be seen from plotting the conditional moments. The over-rejection is also observed in the appendix to Mourifi\u00e9 and Wan (2017) when $J=2$, in which case our model is equivalent to the one in Imbens and Angrist (1994). Consequently, we test the same inequalities presented in Kitagawa (2015) as Mourifi\u00e9 and Wan (2017) do, and we further replicate this behavior in Appendix D. 1 for a range of designs with $J=2$. In any case, Chernozhukov et al. (2013) require choosing a number of tuning parameters. Next, in Tables 6 and 7, after discretizing $Y$, both testing\n\n[^0]\n[^0]:    ${ }^{5}$ With an outcome $Y$, (2) strengthens to $\\widetilde{P}\\{Y=y, D=j \\mid Z=z\\}=Q\\left\\{Y_{j}=y, D_{z}=j\\right\\}$, which is the restriction we consider whenever there is an outcome.\n\n![table_4](table_4)\n\nTable 5: Rejection probabilities in percentages for testing (24)-(25) at the $5 \\%$ level using Chernozhukov et al. (2013) with both parametric and local specifications. $J=4, J_{0}=1$.\n\n![table_5](table_5)\n\nTable 6: Rejection probabilities in percentages for testing (21)-(22) at the $5 \\%$ level using Fang et al. (2023) after binarizing $Y . J=4, J_{0}=1$.\n\n![table_6](table_6)\n\nTable 7: Rejection probabilities in percentages for testing the linear programming formulation at the $5 \\%$ level using Fang et al. (2023) after binarizing $Y . J=4, J_{0}=1$.\nthe moment inequalities in (21)-(22) and testing the linear program defined by (2) and (4) using Fang et al. (2023) control size well and have nontrivial power for $\\beta=-1.5$ at all sample sizes, for $\\beta=-1$ when $n=1000$ and $n=2000$, and also for $\\beta_{0}=-0.5$ when $n=2000$. Between the two methods, testing the closed-form inequalities in (21)-(22) is more powerful than testing the linear program defined by (2) and (4). Finally, comparing Table 5(a) and Table 6, although (21)-(22) based on discretizing $Y$ do not sharply characterize the model compared to the conditional moment inequalities in (24)-(25), the test based on the former discretization is still more powerful.",
      "tables": {
        "table_4": "|  | (a) parametric |  |  |  |\n| :--: | :--: | :--: | :--: | :--: |\n|  | $H_{0}$ | $H_{1}$ |  |  |\n| $n$ | $\\beta_{0}=0$ | $\\beta_{0}=-0.5$ | $\\beta_{0}=-1$ | $\\beta_{0}=-1.5$ |\n| 500 | 0.12 | 0.24 | 1.18 | 1.92 |\n| 1000 | 0.70 | 1.12 | 14.96 | 39.50 |\n| 2000 | 2.54 | 3.28 | 56.94 | 94.70 |\n|  | (b) local |  |  |  |\n|  | $H_{0}$ | $H_{1}$ |  |  |\n| $n$ | $\\beta_{0}=0$ | $\\beta_{0}=-0.5$ | $\\beta_{0}=-1$ | $\\beta_{0}=-1.5$ |\n| 500 | 60.60 | 80.82 | 90.80 | 94.64 |\n| 1000 | 44.28 | 66.26 | 83.84 | 89.76 |\n| 2000 | 27.92 | 49.76 | 73.04 | 86.42 |",
        "table_5": "|  | $H_{0}$ | $H_{1}$ |  |  |\n| :--: | :--: | :--: | :--: | :--: |\n| $n$ | $\\beta_{0}=0$ | $\\beta_{0}=-0.5$ | $\\beta_{0}=-1$ | $\\beta_{0}=-1.5$ |\n| 500 | 0.24 | 1.48 | 5.46 | 11.62 |\n| 1000 | 0.16 | 3.92 | 28.74 | 59.12 |\n| 2000 | 0.32 | 12.22 | 72.12 | 96.82 |",
        "table_6": "|  | $H_{0}$ | $H_{1}$ |  |  |\n| :--: | :--: | :--: | :--: | :--: |\n| $n$ | $\\beta_{0}=0$ | $\\beta_{0}=-0.5$ | $\\beta_{0}=-1$ | $\\beta_{0}=-1.5$ |\n| 500 | 0.10 | 1.28 | 4.72 | 9.66 |\n| 1000 | 0.18 | 3.36 | 25.46 | 55.48 |\n| 2000 | 0.18 | 9.16 | 65.88 | 93.26 |"
      },
      "images": {}
    },
    {
      "section_id": 11,
      "text": "# 6.2 Simulations for $J_{0}=0$ \n\nNext, we study tests of the hypothesis in (19) for $J_{0}=0$. The data is generated in the same way as in Section 6.1, except now that $\\beta_{0}=2$ under the null hypothesis. Throughout this subsection, we also binarize $Y$ as in Section 6.1. In this case, testing (20)-(21) using Fang et al. (2023) is computationally prohibitive when $J>4$, and testing the linear program defined by (2) and (3) using Fang et al. (2023) is also computationally prohibitive when $J>5$. As a result, we compare the two methods when $J=3$, and the design therefore uses only the first three entries of $\\beta$ and $\\epsilon$. Here, (20)-(21) generate 76 inequalities with 18 variables, while (2) and (3) define a linear system with 18 equalities with 80 latent variables. The results are presented in Tables 8 and 9. As in Section 6.1, both tests control size well, and the test based on the closed-form characterization in (20)-(21) is more powerful.\n\n![table_7](table_7)\n\nTable 8: Rejection probabilities in percentages for testing (20)-(21) at the $5 \\%$ level using Fang et al. (2023) after binarizing $Y . J=3, J_{0}=0$.\n\n![table_8](table_8)\n\nTable 9: Rejection probabilities in percentages for testing the linear programming formulation at the $5 \\%$ level using Fang et al. (2023) after binarizing $Y . J=3, J_{0}=0$.",
      "tables": {
        "table_7": "|  | $H_{0}$ | $H_{1}$ |  |  |\n| :--: | :--: | :--: | :--: | :--: |\n| $n$ | $\\beta_{0}=2$ | $\\beta_{0}=-0.5$ | $\\beta_{0}=-1$ | $\\beta_{0}=-1.5$ |\n| 500 | 0.04 | 0.90 | 11.22 | 42.02 |\n| 1000 | 0.00 | 1.28 | 26.42 | 83.78 |\n| 2000 | 0.00 | 1.16 | 51.58 | 99.64 |",
        "table_8": "|  | $H_{0}$ | $H_{1}$ |  |  |\n| :--: | :--: | :--: | :--: | :--: |\n| $n$ | $\\beta_{0}=2$ | $\\beta_{0}=-0.5$ | $\\beta_{0}=-1$ | $\\beta_{0}=-1.5$ |\n| 500 | 0.00 | 0.52 | 8.66 | 31.50 |\n| 1000 | 0.00 | 0.88 | 18.84 | 63.38 |\n| 2000 | 0.00 | 0.62 | 37.80 | 80.22 |"
      },
      "images": {}
    },
    {
      "section_id": 12,
      "text": "## 7 Empirical Application\n\nBecause the datasets in Kline and Walters (2016) and Kirkeboen et al. (2016) are confidential, we apply our methods to the dataset from Behaghel et al. (2013, 2014). They study a randomized controlled trial with three job search counseling programs in France. In their setting, $Z=0$ denotes assignment (of eligibility) to the usual public program without intensive counseling, which is thought of as the control group; $Z=1$ denotes assignment to the public program $Z$ with intensive counseling; and $Z=2$ denotes assignment to the private program with intensive counseling. $D=0,1,2$ denotes participation in the corresponding programs. Assumption 2.1 holds because $Z$ is randomly assigned.\n\nJob seekers did not necessarily comply with their assignment, i.e., they may not enter the program they were assigned to, and the noncompliance rate was as high as $60 \\%$ on average (Behaghel et al., 2014, footnote 9). As can be seen in Table 10 below, some job seekers assigned to the control group participated in the two treatment programs, and other job seekers assigned to either of the two treatment programs entered the control or the other treatment program. As a result, the assignment $Z=j$ becomes an encouragement towards $D=j$ which one may or may not take up.\n\n![table_9](table_9)\n\nTable 10: Estimated values of $P\\{D=d \\mid Z=z\\}$ for $d, z \\in\\{0,1,2\\}$ in Behaghel et al. (2013, 2014).\n\nWe consider three outcomes in their dataset: \"EMPLOI 6MOIS,\" which indicates exit from PES registers to employment; \"EMPLOI AR110 6MOIS,\" which indicates any employment; and \"SUCCES OPP 6MOIS,\" which indicates employment eligible for payment. All three outcomes are binary and were measured six months after the beginning of the program following their analysis. To recover a causal interpretation of the IV estimand, Behaghel et al. (2013) impose an assumption called \"extended monotonicity.\" As shown in Appendix C, this assumption is strictly stronger than setting $J_{0}=1$ in our model. As a result, we focus on testing (19) for $J_{0}=0$ and $J_{0}=1$. The test for $J_{0}=0$ indicates whether the assumption that each value of the instrument encourages towards an unique choice is consistent with the data. The test for $J_{0}=1$ further indicates whether it is truly without loss of generality to assume that assignment to the control group does not in fact encourage people to take up the control program. This may be a tempting assumption if the assignment to the control program is thought of as the \"base state\" of the instrument. Because all outcomes are discrete, we test (20)-(21) for $J_{0}=0$ and (21)-(22) for $J_{0}=1$ using Fang et al. (2023). To illustrate the gains from the refinement by including an outcome, we also include the result for testing (11) for $J_{0}=0$ and (13) for $J_{0}=1$ which do not make use of the outcome. Table 11 presents the $p$-values of the test in percentages for each null hypothesis and outcome combination.\n\n![table_10](table_10)\n\nTable 11: $p$-values in percentages for testing (11) and (13) without $Y$, (20)-(21) for $J_{0}=0$, and (21)-(22) for $J_{0}=1$, using Fang et al. (2023) on the dataset in Behaghel et al. (2014).\n\nWithout the outcome, the tests for $J_{0}=0$ and $J_{0}=1$ both fail to reject at the $10 \\%$ level. With\n\nany of the three outcomes, the test for $J_{0}=0$ fails to reject at the $10 \\%$ level, but the test for $J_{0}=1$ rejects at the $10 \\%$ level. For the outcome \"EMPLOI AR110 6MOIS,\" the test for $J_{0}=1$ further rejects at both the $5 \\%$ and $1 \\%$ levels. Therefore, including the outcome in the tests helps us reject the null hypothesis that $J_{0}=1$. To further investigate which moment inequalities among (21)-(22) are violated, for each outcome, we report the point estimates for the violated moments, i.e., moments that are estimated to violate (21)-(22). The results are presented in Table 12. Among all moments, the one that is consistently violated across outcomes is $P\\{Y=y, D=1 \\mid Z=2\\} \\leq P\\{Y=y, D=$ $1 \\mid Z=0\\}$. This moment is also violated without an outcome, but the violation is more pronounced with an outcome. Together with the results in Table 11, they indeed illustrate the gains from the refinement by including an outcome in the test. To understand the violated moments, note that in Assumption 2.3, $D_{2}=1$ and $D_{2} \\in\\left\\{D_{0}, 2\\right\\}$ implies $D_{0}=1$. The violation of this condition implies that one cannot think of the assignment to the control program as the \"base state.\" In other words, at least for some people, $Z=0$ strictly increases the appeal of the public program $D=0$. Our test clearly pinpoints the violation is through the substitution pattern that $D_{2}=1$ and $D_{0} \\neq 1$, i.e., some subjects choose program 1 when encouraged towards program 2 but do not choose program 1 when there is no encouragement. As discussed in Section 2, when $J_{0}=0$, changing $Z=0$ to $Z=2$ not only increases the appeal of $D=2$, but decreases the appeal of $D=0$ as well, making it possible for the person to switch to $D=1$. When $J_{0}=1$, however, changing $Z=0$ to $Z=2$ only increases the appeal of $D=2$ but does not decrease the appeal of $D=0$, so one can only switch to $D=2$ instead of $D=1$.\n\n![table_11](table_11)\n\nTable 12: Point estimates for violated moments for the dataset in Behaghel et al. (2013, 2014).",
      "tables": {
        "table_9": "| $Z$ | $D$ |  |  |\n| --: | --: | --: | --: |\n|  | 0 | 1 | 2 |\n| 0 | 0.9620 | 0.0036 | 0.0344 |\n| 1 | 0.6592 | 0.3170 | 0.0238 |\n| 2 | 0.5399 | 0.0044 | 0.4557 |",
        "table_10": "| $Y$ | $J_{0}=0$ | $J_{0}=1$ |\n| --: | --: | --: |\n| - | 100.00 | 21.58 |\n| - | 100.00 | 6.12 |\n| EMPLOI 6MOIS | 100.00 | 0.58 |\n| - | 100.00 | 8.28 |",
        "table_11": "| moment | point estimate $\\left(\\times 10^{-5}\\right)$ |\n| :--: | :--: |\n| Without $Y$ |  |\n| $P\\{D=1 \\mid Z=2\\}-P\\{D=1 \\mid Z=0\\}$ | 81.58 |\n| $Y=$ EMPLOI 6MOIS |  |\n| $P\\{Y=0, D=1 \\mid Z=2\\}-P\\{Y=0, D=1 \\mid Z=0\\}$ | 8.02 |\n| $P\\{Y=1, D=1 \\mid Z=2\\}-P\\{Y=1, D=1 \\mid Z=0\\}$ | 73.56 |\n| $P\\{Y=1, D=2 \\mid Z=1\\}-P\\{Y=1, D=2 \\mid Z=0\\}$ | 0.38 |\n| $Y=$ EMPLOI AR110 6MOIS |  |\n| $P\\{Y=1, D=1 \\mid Z=2\\}-P\\{Y=1, D=1 \\mid Z=0\\}$ | 130.86 |\n| $Y=$ SUCCES OPP 6MOIS |  |\n| $P\\{Y=0, D=1 \\mid Z=2\\}-P\\{Y=0, D=1 \\mid Z=0\\}$ | 17.78 |\n| $P\\{Y=1, D=1 \\mid Z=2\\}-P\\{Y=1, D=1 \\mid Z=0\\}$ | 63.80 |"
      },
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# 8 Conclusion \n\nIn this paper, we proposed sharp, closed-form testable implications of a potential outcome model that assumes each value of the instrument only encourages towards one choice. Because the testable implications are in closed form, we can immediately check if they are violated for a given dataset and more importantly, pinpoint where the violation occurs. In an empirical application to the dataset for Behaghel et al. $(2013,2014)$, we find that the data is not compatible with the assumption that the control program is not encouraged by its corresponding assignment, which may be a tempting assumption if the control program is thought of as the \"base state.\" The identity of the violated inequalities further indicates which choice patterns are incompatible with the data.\n\nOur assumption that each value of the instrument only encourages towards one choice is satisfied in many examples, sometimes with the additional restriction that some choices do not have an encouragement. That said, it would be interesting to study settings in which each value of the instrument possibly encourages towards multiple choices. We leave this question open for future work.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# A Equivalence with a Random Utility Model \n\nAlthough Assumption 2.3 is intuitive, we now provide additional motivation for it by showing that the model that imposes Assumptions 2.1-2.3 is equivalent to a fairly general random utility model, and such a model will shed more light on how the default choice naturally arises. The same model was shown by Kline and Walters (2016) to imply the restriction in (5). We will in turn show that they are equivalent. We emphasize, however, that the results in this appendix only serve as motivation for Assumption 2.3 and our results in the main text do not rely on the random utility model.\n\nFix $0 \\leq J_{0} \\leq J-1$. Recall $\\mathcal{Z}=\\left\\{0, J_{0}, \\ldots, J-1\\right\\}$. Let $\\Omega$ denote the underlying probability space. To define the random utility model, for $0 \\leq j \\leq J-1$ and $z \\in \\mathcal{Z}$, let $U_{j}(z, \\cdot): \\Omega \\rightarrow \\mathbf{R}$ be a random variable that denotes the random utility of choice $j$ when the instrument equals $z$. We assume that $\\left(U_{j}(z, \\cdot): 0 \\leq j \\leq J-1, z \\in \\mathcal{Z}\\right) \\Perp Z$. The random utility model maintains that the potential treatments $D_{z}$ for $z \\in \\mathcal{Z}$ are given by\n\n$$\nD_{z}(\\omega) \\in \\underset{0 \\leq j \\leq J-1}{\\operatorname{argmax}} U_{j}(z, \\omega)\n$$\n\nTo capture the idea that $Z=j$ only encourages towards $D=j$, we make the following restriction on the random utilities. For $0 \\leq j \\leq J-1$, let $U_{j}(\\cdot)$ be a random variable that denotes the baseline utility of choice $j$. We assume that for $0 \\leq j \\leq J-1, U_{j}(z, \\omega)=U_{j}(\\omega)$ for $z \\neq j$ and $U_{j}(j, \\omega) \\geq U_{j}(\\omega)$; if $J_{0}>0$ then additionally $U_{j}(j, \\omega)=U_{j}(\\omega)$ for $0 \\leq j \\leq J_{0}-1$. The restriction captures the idea that the utilities of first $J_{0}$ choices are not affected by the instrument, while for $J_{0} \\leq j \\leq J-1$, the utility is (weakly) increased when $z=j$ but otherwise stays the same. To rule out degenerate situations, we will further assume that the distribution of $\\left(U_{j}(\\omega): 0 \\leq j \\leq J-1\\right)$ is absolutely continuous with respect to the Lebesgue measure on $\\mathbf{R}^{J}$.\n\nLet $\\mathbf{Q}_{2}$ denote our random utility model; that is, for each $Q \\in \\mathbf{Q}_{2}$, (a) $D_{z}$ is determined by (26); (b) the distribution of $\\left(U_{j}(\\cdot): 0 \\leq j \\leq J-1\\right)$ is absolutely continuous with respect to the Lebesgue measure on $\\mathbf{R}^{J}$; (c) for $0 \\leq j \\leq J-1, U_{j}(z, \\omega)=U_{j}(\\omega)$ for $z \\neq j$ and $U_{j}(j, \\omega) \\geq U_{j}(\\omega)$, and if $J_{0}>0$ then additionally $U_{j}(j, \\omega)=U_{j}(\\omega)$ for $0 \\leq j \\leq J_{0}-1$; and (d) $\\left(U_{j}(z, \\cdot): 0 \\leq j \\leq J-1, z \\in \\mathcal{Z}\\right) \\Perp Z$ and $Q\\{Z=z\\}>0$ for all $z \\in \\mathcal{Z}$. The following result shows that $\\mathbf{Q}_{2}=\\mathbf{Q}_{1}$, so that each $Q$ that satisfies the encouragement design restriction on potential treatments can be generated by the random utility model under consideration. It extends Vytlacil (2002) to our setting, although a crucial distinction is that the instrument is discrete in our setting, and therefore the proof is elementary.\n\nLemma A.1. $\\mathbf{Q}_{2}=\\mathbf{Q}_{1}$.\n\nProof of Lemma A.1. $\\mathbf{Q}_{2} \\subseteq \\mathbf{Q}_{1}$. To show that $\\mathbf{Q}_{2} \\subseteq \\mathbf{Q}_{1}$, we need to show that every $Q$ in the random utility model satisfies Assumptions 2.1-2.3. Let $\\Omega$ denote the underlying probability space.\n\nFor each $\\omega \\in \\Omega$, let\n\n$$\nj^{*}(\\omega) \\in \\underset{0 \\leq j \\leq J-1}{\\operatorname{argmax}} U_{j}(\\omega)\n$$\n\nBecause the distribution of $\\left(U_{j}(\\cdot): 0 \\leq j \\leq J-1\\right)$ is absolutely continuous with respect to the Lebesgue measure on $\\mathbf{R}^{J}, j^{*}$ is unique with probability one. Therefore, $j^{*}$ is a random variable. Here, $j^{*}(\\omega)$ is the default choice when the instrument does not exist, and the value of $j^{*}(\\omega)$ could differ across $\\omega \\in \\Omega$. Furthermore, because ties happen with probability zero, we know that with probability one,\n\n$$\nU_{j^{*}(\\omega)}(\\omega)>\\max _{k \\neq j^{*}(\\omega)} U_{k}(\\omega)\n$$\n\nNext, we show by contradiction that with probability one, for $0 \\leq j \\leq J-1, D_{j}(\\omega) \\in\\left\\{j, j^{*}(\\omega)\\right\\}$. Indeed, suppose $D_{j}(\\omega)=k \\notin\\left\\{j, j^{*}(\\omega)\\right\\}$ for a set of $\\omega$ with strictly positive probability. Then,\n\n$$\nU_{k}(j, \\omega) \\geq U_{j^{*}(\\omega)}(j, \\omega)\n$$\n\nIf $j \\neq j^{*}(\\omega)$, then (29) imply that\n\n$$\nU_{k}(\\omega)=U_{k}(j, \\omega) \\geq U_{j^{*}(\\omega)}(j, \\omega)=U_{j^{*}(\\omega)}\\left(j^{*}(\\omega)\\right)\n$$\n\nwith strictly positive probability, a contradiction to (28). If $j=j^{*}(\\omega)$, then (29) implies\n\n$$\nU_{k}(\\omega)=U_{k}(j, \\omega) \\geq U_{j^{*}(\\omega)}\\left(j^{*}(\\omega), \\omega\\right) \\geq U_{j^{*}(\\omega)}(\\omega)\n$$\n\nanother contradiction to (28). Therefore, we have shown with probability one, $D_{j}(\\omega) \\in\\left\\{j, j^{*}(\\omega)\\right\\}$. In the special case where $J_{0}>0, U_{j}(0, \\omega)=U_{j}(\\omega)$ for $0 \\leq j \\leq J-1$, so\n\n$$\nD_{0}(\\omega) \\in \\underset{0 \\leq j \\leq J-1}{\\operatorname{argmax}} U_{j}(\\omega)\n$$\n\nand therefore (27) and the absolute continuity of the distribution of $\\left(U_{j}(\\cdot): 0 \\leq j \\leq J-1\\right)$ imply that $D_{0}=j^{*}$ with probability one.\n$\\mathbf{Q}_{1} \\subseteq \\mathbf{Q}_{2}$. To show $\\mathbf{Q}_{1} \\subseteq \\mathbf{Q}_{2}$, we need to show for every distribution $Q$ that satisfies Assumptions 2.1-2.3, it can be generated from a random utility model in $\\mathbf{Q}_{2}$. To do so, we will show that $Q$ can be generated by an additive random utility model $\\mathbf{Q}_{2}^{\\prime} \\subseteq \\mathbf{Q}_{2}$ which is seemingly much more restrictive. Doing so will imply $\\mathbf{Q}_{1} \\subseteq \\mathbf{Q}_{2}^{\\prime} \\subseteq \\mathbf{Q}_{2}$, and because we already know $\\mathbf{Q}_{2} \\subseteq \\mathbf{Q}_{1}$, the string of inclusion becomes equality; that is, $\\mathbf{Q}_{1}=\\mathbf{Q}_{2}^{\\prime}=\\mathbf{Q}_{2}$, and the proof is concluded.\n\nTo define the additive random utility model $\\mathbf{Q}_{2}^{\\prime}$, let $U_{0}, \\ldots, U_{J-1}: \\mathcal{Z} \\rightarrow \\mathbf{R}$ be deterministic functions and let $\\left(\\epsilon_{0}, \\ldots, \\epsilon_{J-1}\\right)$ be a random vector of unobservables, the distribution of which is absolutely continuous with respect to the Lebesgue measure on $\\mathbf{R}^{J}$. Further suppose $\\left(\\epsilon_{0}, \\ldots, \\epsilon_{J-1}\\right)$ and $Z$ are independent. The additive random utility model maintains that the potential treatments\n\n$D_{z}$ for $z \\in \\mathcal{Z}$ are given by\n\n$$\nD_{z} \\in \\underset{0 \\leq j \\leq J-1}{\\operatorname{argmax}}\\left(U_{j}(z)+\\epsilon_{j}\\right)\n$$\n\nBecause the distribution of $\\left(\\epsilon_{0}, \\ldots, \\epsilon_{J-1}\\right)$ is absolutely continuous with respect to the Lebesgue measure on $\\mathbf{R}^{J}$, ties happen with probability zero. Therefore, with probability one, $D_{z}$ is unique and\n\n$$\nI\\left\\{D_{z}=j\\right\\}=I\\left\\{U_{j}(z)+\\epsilon_{j}>U_{k}(z)+\\epsilon_{k} \\text { for all } k \\neq j\\right\\}\n$$\n\nWe further assume that\n\n$$\nU_{j}(z)=\\alpha_{j}+\\beta_{j} I\\{z=j\\}\n$$\n\nfor some $\\beta_{0}, \\ldots, \\beta_{J-1} \\geq 0$. In other words, each value of the instrument encourages towards or \"targets\" a unique choice. Because $\\alpha_{j}$ can be absorbed into $\\epsilon_{j}$ without loss of generality, we henceforth assume $\\alpha_{j}=0$, and accordingly\n\n$$\nU_{j}(z)=\\beta_{j} I\\{z=j\\}\n$$\n\nWhen $J_{0}>0$, to reflect the idea that first $J_{0}$ choices are not affected by the instrument, we simply set $\\beta_{j}=0$ for all $0 \\leq j \\leq J_{0}-1$. Again, we interpret $Z=0$ as the \"base state\" for the instrument that does not shift the utility of any choice. Fix $0 \\leq J_{0} \\leq J-1$. Let $\\mathbf{Q}_{2}^{\\prime}$ denote the set of all distributions of $\\left(\\left(D_{z}: z \\in \\mathcal{Z}\\right), Z\\right)$ which are consistent with our additive random utility model. That is, under each $Q \\in \\mathbf{Q}_{2}^{\\prime}$, (a) $D_{z}$ is determined by (30); (b) the distribution of $\\left(\\epsilon_{0}, \\ldots, \\epsilon_{J-1}\\right)$ is absolutely continuous with respect to the Lebesgue measure on $\\mathbf{R}^{J}$; (c) $U_{0}, \\ldots, U_{J-1}$ is given by (31), where $\\beta_{j}=0$ for $0 \\leq j \\leq J_{0}-1$ and $\\beta_{j} \\geq 0$ for $J_{0} \\leq j \\leq J-1$; and (d) $\\left(\\epsilon_{0}, \\ldots, \\epsilon_{J-1}\\right) \\Perp Z$ and $Q\\{Z=z\\}>0$ for all $z \\in \\mathcal{Z}$. It is clear that $\\mathbf{Q}_{2}^{\\prime} \\subseteq \\mathbf{Q}_{2}$ with $\\epsilon_{j}(\\omega)=U_{j}(\\omega)$.\n\nFirst suppose $J_{0}=0$. For $0 \\leq j \\leq J-1$, define $\\beta_{j}=1$. The restriction in (3) can equivalently be expressed as requiring the probabilities of some vectors of potential treatments are zero. In particular, define\n\n$$\n\\mathcal{S}=\\left\\{\\left(d_{0}, \\ldots, d_{J-1}\\right): d_{j} \\neq j, d_{k} \\neq k \\Longrightarrow d_{j}=d_{k}\\right\\}\n$$\n\nThe restriction in (3) can then be expressed as $Q\\left\\{\\left(D_{0}, \\ldots, D_{J-1}\\right)=\\left(d_{0}, \\ldots, d_{J-1}\\right)\\right\\}=0$ if $\\left(d_{0}, \\ldots, d_{J-1}\\right) \\notin$ $\\mathcal{S}$. Because the default choice $j^{*}$ only depends on the value of $\\left(D_{0}, \\ldots, D_{J-1}\\right)$, for $\\left(d_{0}, \\ldots, d_{J-1}\\right) \\in$ $\\mathcal{S} \\backslash\\{(0, \\ldots, J-1)\\}$, we define $j^{*}\\left(d_{0}, \\ldots, d_{J-1}\\right)$ as the associated default choice, which is the value of $d_{j}$ as long as $d_{j} \\neq j$. Further define\n\n$$\n\\Lambda\\left(d_{0}, \\ldots, d_{J-1}\\right)=\\left\\{0 \\leq j \\leq J-1: d_{j}=j \\neq j^{*}\\left(d_{0}, \\ldots, d_{J-1}\\right)\\right\\}\n$$\n\nLet $M$ be a constant to be chosen below. Define the corresponding region for $\\left(\\epsilon_{0}, \\epsilon_{1}, \\ldots, \\epsilon_{J-1}\\right)$ as\n\n$$\n\\begin{aligned}\nR\\left(d_{0}, \\ldots, d_{J-1}\\right)=\\left\\{\\left(\\epsilon_{0}, \\ldots, \\epsilon_{J-1}\\right):\\right. & \\beta_{j^{*}}+\\epsilon_{j^{*}}>\\epsilon_{j} \\text { for } j \\neq j^{*} \\\\\n& \\beta_{j}+\\epsilon_{j}>\\epsilon_{k} \\text { for } j \\in \\Lambda \\text { and } k \\neq j\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& \\beta_{j}+\\epsilon_{j}<\\epsilon_{j^{*}} \\text { for } j \\notin \\Lambda \\text { and } j \\neq j^{*} \\\\\n& \\left|\\epsilon_{j}\\right| \\leq M \\text { for all } j\\} \\subseteq \\mathbf{R}^{J}\n\\end{aligned}\n$$\n\nFinally, define\n\n$$\nR(0, \\ldots, J-1)=\\left\\{\\beta_{j}+\\epsilon_{j}>\\epsilon_{k} \\text { for } 0 \\leq j \\leq J-1 \\text { and } k \\neq j,\\left|\\epsilon_{j}\\right| \\leq M \\text { for all } j\\right\\}\n$$\n\nSuch regions are obviously disjoint across all possible $\\left(d_{0}, \\ldots, d_{J-1}\\right) \\in \\mathcal{S}$. By choosing $M$ large enough, all of these regions are nonempty but bounded. Let $\\left(\\epsilon_{0}, \\ldots, \\epsilon_{J-1}\\right)$ be uniformly distributed in each region, with density\n\n$$\n\\frac{Q\\left\\{\\left(D_{0}, \\ldots, D_{J-1}\\right)=\\left(d_{0}, \\ldots, d_{J-1}\\right)\\right\\}}{\\left|R\\left(d_{0}, \\ldots, d_{J-1}\\right)\\right|}\n$$\n\nwhere $\\left|R\\left(d_{0}, \\ldots, d_{J-1}\\right)\\right|$ is the Lebesgue measure of $R\\left(d_{0}, \\ldots, d_{J-1}\\right)$ in $\\mathbf{R}^{J}$. The result now follows. When $J_{0}>0$, the proof is similar, with the only difference being we set $\\beta_{j}=0$ for $0 \\leq j \\leq J_{0}-1$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "# B Proofs of Main Results\n## B. 1 Proof of Theorem 3.1\n\nProof of Theorem 3.1. First suppose $J_{0}=0$. To establish the inequalities, fix $z(0), \\ldots, z(J-$ 1) $\\in\\{0, \\ldots, J-1\\}$ such that $z(j) \\neq j$ for $0 \\leq j \\leq J-1$. By construction, $z(j) \\in \\mathcal{Z}(j)$ for $0 \\leq j \\leq J-1$. For a fixed $j$, if $D_{z(j)}(\\omega)=j$, then because $j \\neq z(j)$, we have that\n\n$$\nj^{*}(\\omega)=j\n$$\n\nAs a result, $D_{z(k)} \\in\\{j, z(k)\\}$ for $k \\neq j$. Because $k \\neq z(k)$ and $k \\neq j$, it cannot be the case that $D_{z(k)}=k$. Therefore,\n\n$$\n\\left\\{D_{z(0)}=0\\right\\}, \\ldots,\\left\\{D_{z(J-1)}=J-1\\right\\}\n$$\n\nare mutually exclusive events, so\n\n$$\n\\sum_{0 \\leq j \\leq J-1} Q\\left\\{D_{z(j)}=j\\right\\} \\leq 1\n$$\n\nThe desired conclusion now follows from Assumption 2.1. When $J_{0}>0,\\left\\{D_{0}=j\\right\\}$ for $0 \\leq j \\leq J_{0}-1$ is disjoint from all events above, and the result follows.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "# B. 2 Proof of Theorem 3.2\n## B.2.1 Proof when $J_{0}=0$\n\nWe construct $Q^{*}$ that satisfies Assumption 2.1 and (3). For each $0 \\leq j \\leq J-1$, we define $Q^{*}$ on a class of events that have $j^{*}=j$. Let $\\left\\{z_{1}(j), \\ldots, z_{J}(j)\\right\\}=\\{0, \\ldots, J-1\\}$ be such that\n\n$$\nP\\left\\{D=j \\mid Z=z_{1}(j)\\right\\} \\leq P\\left\\{D=j \\mid Z=z_{2}(j)\\right\\} \\leq \\cdots \\leq P\\left\\{D=j \\mid Z=z_{J}(j)\\right\\}\n$$\n\nNote that (12) implies $P\\{D=j \\mid Z=z\\}$ is maximized by $z=j$, so $z_{J}(j)=j$ (in the case of ties, simply define $z_{J}(j)=j$ ). Our construction for this fixed $j$ consists of the following steps:\n\nStep 1: define\n\n$$\nQ^{*}\\left\\{D_{z}=j \\text { for } 0 \\leq z \\leq J-1\\right\\}=P\\left\\{D=j \\mid Z=z_{1}(j)\\right\\}\n$$\n\nStep $\\ell$ for $2 \\leq \\ell \\leq J-1$ (note we stop at step $J-1$ instead of $J$ ): define\n\n$$\n\\begin{aligned}\n& Q^{*}\\left\\{D_{z_{1}(j)}=z_{1}(j), \\ldots, D_{z_{\\ell-1}(j)}=z_{\\ell-1}(j), D_{z}=j \\text { for } z \\notin\\left\\{z_{1}(j) \\ldots, z_{\\ell-1}(j)\\right\\}\\right\\} \\\\\n&=P\\left\\{D=j \\mid Z=z_{\\ell}(j)\\right\\}-P\\left\\{D=j \\mid Z=z_{\\ell-1}(j)\\right\\}\n\\end{aligned}\n$$\n\nAs mentioned above, in all of these events, $j^{*}=j$.\nAfter carrying out the construction for each $0 \\leq j \\leq J-1$, define the \"diagonal\" as\n\n$$\nQ^{*}\\left\\{D_{j}=j \\text { for } 0 \\leq j \\leq J-1\\right\\}=1-\\sum_{0 \\leq j \\leq J-1} P\\left\\{D=j \\mid Z=z_{J-1}(j)\\right\\}\n$$\n\nwhich is nonnegative because of (11) and the fact that $z_{J-1}(j) \\neq j$. (32)-(33) completely specify the probability of potential treatments where $j$ appears at least twice, and (34) closes the gap by specifying the probability that one complies to all values of the instrument. For all other vectors $\\left(d_{0}, \\ldots, d_{J-1}\\right)$, define $Q^{*}\\left\\{\\left(D_{0}, \\ldots, D_{J-1}\\right)=\\left(d_{0}, \\ldots, d_{J-1}\\right)\\right\\}=0$.\n\nFinally, for each $z$ and $\\left(d_{0}, \\ldots, d_{J-1}\\right)$, define\n\n$$\nQ^{*}\\left\\{D_{0}=d_{0}, \\ldots, D_{J-1}=d_{J-1}, Z=z\\right\\}=Q^{*}\\left\\{D_{0}=d_{0}, \\ldots, D_{J-1}=d_{J-1}\\right\\} P\\{Z=z\\}\n$$\n\nWe now verify that $Q^{*}$ satisfies Assumption 2.1 and (3). First note Assumption 2.1 holds by (35). All probabilities are nonnegative by construction. To verify that $Q^{*}$ is a probability measure, note for each $j$, the events in (32)-(33) are mutually exclusive from each other. In addition, they are mutually exclusive across $0 \\leq j \\leq J-1$ and are all exclusive from the event in (34) because for the events that appear in (32)-(33), the default choice is $j$, which will be selected for at least two values\n\nof the instrument, and the treatment equals the instrument for all other values of the instrument. Furthermore, for each $j$, the sum of (32)-(33) from $1 \\leq \\ell \\leq J-1$ is\n\n$$\nP\\left\\{D=j \\mid Z=z_{J-1}(j)\\right\\}\n$$\n\nTherefore, across $0 \\leq j \\leq J-1$, (32)-(34) sum up to\n\n$$\n\\sum_{0 \\leq j \\leq J-1} P\\left\\{D=j \\mid Z=z_{J-1}(j)\\right\\}+1-\\sum_{0 \\leq j \\leq J-1} P\\left\\{D=j \\mid Z=z_{J-1}(j)\\right\\}=1\n$$\n\nAs a result, $Q^{*}$ is a probability measure. By construction, $Q^{*}$ assigns zero probability to all events that are ruled out by (3), so (3) holds for $Q^{*}$.\n\nTo conclude the proof, we verify that $P=Q^{*} T^{-1}$, i.e., (2) holds. In order to do so, note if $z \\neq j$, then $z=z_{\\ell}(j)$ for some $1 \\leq \\ell \\leq J-1$, and\n\n$$\n\\begin{aligned}\n& Q^{*}\\left\\{D_{z_{\\ell}(j)}=j\\right\\} \\\\\n& =Q^{*}\\left\\{D_{k}=j \\text { for } 0 \\leq k \\leq J-1\\right\\}+\\ldots \\\\\n& \\quad+Q^{*}\\left\\{D_{z_{1}(j)}=z_{1}(j), \\ldots, D_{z_{\\ell-1}(j)}=z_{\\ell-1}(j), D_{z}=j \\text { for all } z \\notin\\left\\{z_{1}(j) \\ldots, z_{\\ell-1}(j)\\right\\}\\right\\} \\\\\n& =P\\left\\{D=j \\mid Z=z_{1}(j)\\right\\}+P\\left\\{D=j \\mid Z=z_{2}(j)\\right\\}-P\\left\\{D=j \\mid Z=z_{1}(j)\\right\\}+\\ldots \\\\\n& \\quad+P\\left\\{D=j \\mid Z=z_{\\ell}(j)\\right\\}-P\\left\\{D=j \\mid Z=z_{\\ell-1}(j)\\right\\} \\\\\n& =P\\left\\{D=j \\mid Z=z_{\\ell}(j)\\right\\}\n\\end{aligned}\n$$\n\nTherefore, we have verified $Q^{*}\\left\\{D_{z}=j\\right\\}=P\\{D=j \\mid Z=z\\}$ when $z \\neq j$. It therefore suffices to verify $Q^{*}\\left\\{D_{k}=k\\right\\}=P\\{D=k \\mid Z=k\\}$ for $0 \\leq k \\leq J-1$. To do so, note for $0 \\leq k \\leq J-1$,\n\n$$\n\\begin{aligned}\nQ^{*}\\left\\{D_{k} \\neq k\\right\\} & =\\sum_{0 \\leq j \\leq J-1: j \\neq k} Q^{*}\\left\\{D_{k}=j\\right\\} \\\\\n& =\\sum_{0 \\leq j \\leq J-1: j \\neq k} P\\{D=j \\mid Z=k\\}=1-P\\{D=k \\mid Z=k\\}\n\\end{aligned}\n$$\n\nBecause $Q^{*}$ is a probability measure, $Q^{*}\\left\\{D_{k}=k\\right\\}=1-Q^{*}\\left\\{D_{k} \\neq k\\right\\}=P\\{D=k \\mid Z=k\\}$, and the proof is completed.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# B.2.2 Proof when $J_{0}>0$ \n\nThe construction is similar to that in the proof of Theorem 3.2, with a few important changes:\n(a) For $J_{0} \\leq j \\leq J-1$, (13) implies when ordering $P\\{D=j \\mid Z=z\\}$ as in the proof of Theorem $3.2,\\left\\{z_{1}(j), \\ldots, z_{J-J_{0}-1}(j)\\right\\}=\\left\\{J_{0}, \\ldots, J-1\\right\\} \\backslash\\{j\\}$ and $z_{J-J_{0}}(j)=0$. We can therefore carry out the construction as in there, but because $z$ can only take $J-J_{0}+1$ values, the construction\n\nwill stop at step $J-J_{0}$ instead of $J-1$. In this part of the construction, because $z_{J-J_{0}}(j)=0$, the total mass assigned by $Q^{*}$ in this part is\n\n$$\nP\\left\\{D=j \\mid Z=z_{J-J_{0}}(j)\\right\\}=P\\{D=j \\mid Z=0\\}\n$$\n\nNote in this part, no mass is assigned to $\\left\\{D_{0}=k\\right\\}$ for any $0 \\leq k \\leq J_{0}-1$.\n(b) For $0 \\leq j \\leq J_{0}-1$, carry out the construction as above, and again stop at step $J-J_{0}$ instead of $J-1$. Note $z_{J-J_{0}}(j)=0$. The total mass assigned by $Q^{*}$ in this part is\n\n$$\nP\\left\\{D=j \\mid Z=z_{J-J_{0}}(j)\\right\\}=\\max _{z \\neq 0} P\\{D=j \\mid Z=z\\}\n$$\n\n(c) The diagonal now consists of a set of vectors of potential treatments instead of one. For $0 \\leq$ $j \\leq J_{0}-1$, we simply define\n\n$$\nQ^{*}\\left\\{\\left(D_{0}, D_{J_{0}}, \\ldots, D_{J-1}\\right)=\\left(j, J_{0} \\ldots, J-1\\right)\\right\\}=P\\{D=j \\mid Z=0\\}-\\max _{z \\neq 0} P\\{D=j \\mid Z=z\\}\n$$\n\nwhich is positive by (13).\n\nBy construction, $Q^{*}$ is obviously a probability measure and satisfies (4). Next, we verify $P=$ $Q T^{-1}$. First consider $0 \\leq j \\leq J_{0}-1$. Note (a) assigns no mass to $\\left\\{D_{0}=j\\right\\}$ while (b) and (c) assign\n\n$$\n\\begin{aligned}\nQ^{*}\\left\\{D_{0}=j\\right\\} & =\\max _{z \\neq 0} P\\{D=j \\mid Z=z\\}+P\\{D=j \\mid Z=0\\}-\\max _{z \\neq 0} P\\{D=j \\mid Z=z\\} \\\\\n& =P\\{D=j \\mid Z=0\\}\n\\end{aligned}\n$$\n\nOn the other hand, for $z \\neq 0,\\left\\{D_{z}=j\\right\\}$ is assigned mass only in (b), which by the same arguments as in the proof of Theorem 3.2 equals\n\n$$\nQ^{*}\\left\\{D_{z}=j\\right\\}=P\\{D=j \\mid Z=z\\}\n$$\n\nNext, consider $J_{0} \\leq j \\leq J-1$. Again for $z \\neq j,\\left\\{D_{z}=j\\right\\}$ is assigned mass only in (a), and hence as in the proof of Theorem 3.2,\n\n$$\nQ^{*}\\left\\{D_{z}=j\\right\\}=P\\{D=j \\mid Z=z\\}\n$$\n\nIt suffices to verify $Q^{*}\\left\\{D_{k}=k\\right\\}=P\\{D=k \\mid Z=k\\}$ for $J_{0} \\leq k \\leq J-1$. Note $\\left\\{D_{k} \\neq k\\right\\}$ is assigned mass only in (a) and (b) for $j \\neq k$, and\n\n$$\nQ^{*}\\left\\{D_{k} \\neq k\\right\\}=\\sum_{0 \\leq j \\leq J-1: j \\neq k} P\\{D=j \\mid Z=k\\}=P\\{D \\neq k \\mid Z=k\\}\n$$\n\nso\n\n$$\nQ^{*}\\left\\{D_{k}=k\\right\\}=1-Q^{*}\\left\\{D_{k} \\neq k\\right\\}=1-P\\{D \\neq k \\mid Z=k\\}=P\\{D=k \\mid Z=k\\}\n$$\n\nAs a result, $P=Q^{*} T^{-1}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "# B. 3 Proof of Theorem 4.1 \n\nWhen $J_{0}=0$, because for each $0 \\leq j \\leq J-1,\\left\\{B_{z}(j): z \\neq j\\right\\}$ forms a partition of $\\mathcal{Y}$,\n\n$$\n\\left(\\left\\{Y_{j} \\in B_{z}(j)\\right\\}: 0 \\leq j \\leq J-1, z \\neq j\\right)\n$$\n\nare mutually exclusive. The inequalities in (14) now follow. (15) is also obvious because $D_{k}=j$ implies $D_{j}=j$ by (3). When $J_{0}>0$, the inequalities in (14) also follow easily.\n\nWe now prove the converse when $J_{0}=0$ and note the result for $J_{0}>0$ can be proved by combining the proof of Theorem 3.2 and the arguments below (and defining $\\lambda_{j}(y)=1$ for $0 \\leq j \\leq J_{0}-1$ and following (37) for $J_{0} \\leq j \\leq J$ ). To begin, note there exists a common dominating measure $\\nu$ on $\\mathbf{R}$ such that for $z \\in \\mathcal{Z}, P\\{Y \\in \\cdot \\mid Z=z\\}$ is absolutely continuous with respect to $\\nu$. Indeed, we can simply define $\\nu$ as $\\sum_{z \\in \\mathcal{Z}} P\\{Y \\in \\cdot \\mid Z=z\\}$. Let $p_{j}(\\cdot \\mid z)$ denote the Radon-Nikodym derivative of $P\\{Y \\in \\cdot, D=j \\mid Z=z\\}$ with respect to $\\nu$. For each $y \\in \\mathcal{Y}$, let $\\left\\{z_{1}(y, j), \\ldots, z_{J}(y, j)\\right\\}=\\{0, \\ldots, J-1\\}$ be such that\n\n$$\np_{j}\\left(y \\mid z_{1}(y, j)\\right) \\leq \\cdots \\leq p_{j}\\left(y \\mid z_{J}(y, j)\\right)\n$$\n\nNote that (15) implies $p_{j}(y \\mid z)$ is maximized by $z=j$ almost everywhere, so we can take $z_{J}(y, j)=j$ everywhere. For $0 \\leq j \\leq J-1$ and each $y \\in \\mathcal{Y}$, define $\\lambda_{j}(y)$ to be an arbitrary nonnegative function such that $\\int_{\\mathcal{Y}} \\lambda_{j}(y) d \\nu(y)=1$ if $\\int_{\\mathcal{Y}}\\left(p_{j}(y \\mid j)-p_{j}\\left(y \\mid z_{J-1}(y, j)\\right) d \\nu(y)=0\\right.$, and otherwise define\n\n$$\n\\lambda_{j}(y)=\\frac{p_{j}(y \\mid j)-p_{j}\\left(y \\mid z_{J-1}(y, j)\\right)}{\\int_{\\mathcal{Y}}\\left(p_{j}(y \\mid j)-p_{j}\\left(y \\mid z_{J-1}(y, j)\\right) d \\nu(y)\\right.}\n$$\n\nNote by definition that $\\int_{\\mathcal{Y}} \\lambda_{j}(y) d \\nu(y)=1$ for $0 \\leq j \\leq J-1$.\nWe seek to construct a probability measure $Q^{*}$ that satisfies Assumption 4.1 and 2.2 as well as (3), and such that $P=Q^{*} T^{-1}$. For each $\\left(d_{0}, \\ldots, d_{J-1}\\right) \\in\\{0, \\ldots, J-1\\}^{J}$ and Borel sets $B_{0}, \\ldots, B_{J-1} \\subseteq \\mathcal{Y}$, we will use $Q_{\\left(d_{0}, \\ldots, d_{J-1}\\right)}^{*}\\left(B_{0}, \\ldots, B_{J-1}\\right)$ to denote $Q^{*}\\left\\{Y_{0} \\in B_{0}, \\ldots, Y_{J-1} \\in\\right.$ $\\left.B_{J-1}, D_{0}=d_{0}, \\ldots D_{J-1}=d_{J-1}\\right\\}$. Let $\\nu^{J}=\\nu \\times \\cdots \\times \\nu$ denote the product measure on $\\mathbf{R}^{J}$ with each marginal equal to $\\nu$. In what follows, we will construct nonnegative functions $q_{\\left(d_{0}, \\ldots, d_{J-1}\\right)}^{*}$ and for Borel sets $B_{0}, \\ldots, B_{J-1} \\subseteq \\mathcal{Y}$, we will define\n\n$$\nQ_{\\left(d_{0}, \\ldots, d_{J-1}\\right)}^{*}\\left(B_{0} \\times \\cdots \\times B_{J-1}\\right)=\\int_{B_{0}} \\cdots \\int_{B_{J-1}} q_{\\left(d_{0}, \\ldots, d_{J-1}\\right)}^{*}\\left(y_{0}, \\ldots, y_{J-1}\\right) d \\nu\\left(y_{0}\\right) \\ldots d \\nu\\left(y_{J-1}\\right)\n$$\n\nFinally, as in the proof of Theorem 3.2, we will define the joint distribution $Q^{*}$ by multiplying the marginal distribution of $Z$.\n\nOur construction for each $y \\in \\mathcal{Y}$ and $0 \\leq j \\leq J-1$ consists of the following steps:\nStep 1: for each $\\left(y_{k} \\in \\mathcal{Y}: k \\neq j\\right)$, define\n\n$$\nq_{(j, \\ldots, j)}^{*}\\left(y_{0}, \\ldots, y, \\ldots, y_{J-1}\\right)=\\prod_{k \\neq j} \\lambda_{k}\\left(y_{k}\\right) p_{j}\\left(y \\mid z_{1}(y, j)\\right)\n$$\n\nStep $\\ell$ for $2 \\leq \\ell \\leq J-1$ : for each $\\left(y_{k} \\in \\mathcal{Y}: k \\neq j\\right)$, and for $d_{z_{1}(y, j)}=z_{1}(y, j), \\ldots, d_{z_{\\ell-1}(y, j)}=$ $z_{\\ell-1}(y, j)$ and $d_{j}=j$ for $z \\notin\\left\\{z_{1}(y, j) \\ldots, z_{\\ell-1}(y, j)\\right\\}$, define\n\n$$\nq_{(d_{0}, \\ldots, d_{J-1})}^{*}\\left(y_{0}, \\ldots, y, \\ldots, y_{J-1}\\right)=\\prod_{k \\neq j} \\lambda_{k}\\left(y_{k}\\right)\\left(p_{j}\\left(y \\mid z_{\\ell}(y, j)\\right)-p_{j}\\left(y \\mid z_{\\ell-1}(y, j)\\right)\\right)\n$$\n\nCarry out these steps separately for $0 \\leq j \\leq J-1$. Define $\\mathcal{S}_{j}$ be the set of treatment vectors $\\left(d_{0}, \\ldots, d_{J-1}\\right)$ that we have assigned mass on. Finally, for each $\\left(y_{j} \\in \\mathcal{Y}: 0 \\leq j \\leq J-1\\right)$, define\n\n$$\nq_{(0, \\ldots, J-1)}^{*}\\left(y_{0}, \\ldots, y_{J-1}\\right)=\\prod_{0 \\leq j \\leq J-1} \\lambda_{j}\\left(y_{j}\\right)\\left(1-\\sum_{0 \\leq j \\leq J-1} \\int_{\\mathcal{Y}} p_{j}\\left(y \\mid z_{J-1}(y, j)\\right) d \\nu(y)\\right)\n$$\n\nwhich is nonnegative because of (14). To see it, take $B_{z}(j)=\\left\\{y \\in \\mathcal{Y}: z_{J-1}(y, j)=z\\right\\}$ for $0 \\leq j \\leq J-1$ and $j \\neq z$. As defined, for $0 \\leq j \\leq J-1, B_{z}(j)$ is measurable as a function of a finite number of functions $p_{j}(y \\mid 0), \\ldots, p_{j}(y \\mid J-1)$, and $\\left\\{B_{z}(j): z \\neq j\\right\\}$ form a partition of $\\mathcal{Y}$. Therefore,\n\n$$\n\\sum_{0 \\leq j \\leq J-1} \\int_{\\mathcal{Y}} p_{j}\\left(y \\mid z_{J-1}(y, j)\\right)=\\sum_{0 \\leq j \\leq J-1} \\sum_{0 \\leq z \\leq J-1} P\\left\\{Y \\in B_{z}(j), D=j \\mid Z=z\\right\\} \\leq 1\n$$\n\nFollowing the arguments in the proof of Theorem 3.2, it is easy to verify $Q^{*}$ is a probability measure. Next, we verify $P=Q^{*} T^{-1}$. Fix a Borel subset $B \\subseteq \\mathcal{Y}$. Integrating over all possible values $y_{k} \\in \\mathcal{Y}$ for $k \\neq j$, we get\n\n$$\n\\begin{aligned}\n& Q^{*}\\left\\{Y_{j} \\in B, D_{z}=j \\text { for } 0 \\leq z \\leq J-1\\right\\} \\\\\n& =\\int_{\\mathcal{Y} \\times \\cdots \\times B \\times \\cdots \\times \\mathcal{Y}} q_{(j, \\ldots, j)}^{*}\\left(y_{0}, \\ldots, y_{J-1}\\right) d \\nu\\left(y_{0}\\right) \\ldots d \\nu\\left(y_{J-1}\\right) \\\\\n& =\\int_{B} p_{j}\\left(y \\mid z_{1}(y, j)\\right) d \\nu(y)\n\\end{aligned}\n$$\n\nwhere the second equality follow because $\\int_{\\mathcal{Y}} \\lambda_{j}(y) d \\nu(y)=1$ for $0 \\leq j \\leq J-1$. Next,\n\n$$\nQ^{*}\\left\\{Y_{j} \\in B, D_{z}=j\\right\\}\n$$\n\n$$\n=\\int_{\\mathcal{Y} \\times \\cdots \\times B \\times \\cdots \\times \\mathcal{Y}} \\sum_{\\left(d_{0}, \\ldots, d_{J-1}\\right) \\in \\mathcal{S}_{j}} q_{\\left(d_{0}, \\ldots, d_{J-1}\\right)}^{*}\\left(y_{1}, \\ldots, y_{j}, \\ldots, y_{J-1}\\right) d \\nu\\left(y_{0}\\right) \\ldots d \\nu\\left(y_{J-1}\\right)\n$$\n\nSimilar to the proof of Theorem 3.2, for each $y_{j}$ there exists an $\\ell\\left(y_{j}, j\\right)$ such that $z_{\\ell\\left(y_{j}, j\\right)}\\left(y_{j}, j\\right)=z$. The summation inside the integral above therefore reduces to\n\n$$\n\\prod_{k \\neq j} \\lambda_{k}\\left(y_{k}\\right) p_{j}\\left(y_{j} \\mid z_{\\ell\\left(y_{j}, j\\right)}\\left(y_{j}, j\\right)\\right)=\\prod_{k \\neq j} \\lambda_{k}\\left(y_{k}\\right) p_{j}\\left(y_{j} \\mid z\\right)\n$$\n\nso that\n\n$$\n\\begin{aligned}\nQ^{*}\\left\\{Y_{j} \\in B, D_{z}=j\\right\\} & =\\int_{\\mathcal{Y} \\times \\cdots \\times B \\times \\cdots \\times \\mathcal{Y}} \\prod_{k \\neq j} \\lambda_{k}\\left(y_{k}\\right) p_{j}\\left(y_{j} \\mid z\\right) d \\nu\\left(y_{0}\\right) \\ldots d \\nu\\left(y_{J-1}\\right) \\\\\n& =\\int_{B} p_{j}\\left(y_{j} \\mid z\\right) d \\nu\\left(y_{j}\\right) \\\\\n& =P\\{Y \\in B, D=j \\mid Z=z\\}\n\\end{aligned}\n$$\n\nIt remains to verify for each Borel set $B \\subseteq \\mathcal{Y}$ and $0 \\leq k \\leq J-1$,\n\n$$\nQ^{*}\\left\\{Y_{k} \\in B, D_{k}=k\\right\\}=P\\{Y \\in B, D=k \\mid Z=k\\}\n$$\n\nTo begin with,\n\n$$\n\\begin{aligned}\nQ^{*}\\left\\{Y_{k} \\in B, D_{k}=k\\right\\}= & \\underbrace{\\sum_{j \\neq k} Q^{*}\\left\\{Y_{k} \\in B, D_{k}=k,\\left(D_{0}, \\ldots, D_{J-1}\\right) \\in \\mathcal{S}_{j}\\right\\}}_{\\text {(A) }} \\\\\n& +\\underbrace{Q^{*}\\left\\{Y_{k} \\in B, D_{k}=k,\\left(D_{0}, \\ldots, D_{J-1}\\right) \\in \\mathcal{S}_{k}\\right\\}}_{\\text {(B) }} \\\\\n& +\\underbrace{Q^{*}\\left\\{Y_{k} \\in B, D_{z}=z \\text { for } 0 \\leq z \\leq J-1\\right\\}}_{\\text {(C) }}\n\\end{aligned}\n$$\n\nwhere $\\mathcal{S}_{j}$ for $0 \\leq j \\leq J-1$ was defined in the construction. For (A), fix any $j \\neq k$. For any $y_{j}$ there exists an $\\ell\\left(y_{j}, j\\right)$ with $z_{\\ell\\left(y_{j}, j\\right)}\\left(y_{j}, j\\right)=k$, so that\n\n$$\n\\begin{aligned}\n& Q^{*}\\left\\{Y_{k} \\in B, D_{k}=k,\\left(D_{0}, \\ldots, D_{J-1}\\right) \\in \\mathcal{S}_{j}\\right\\} \\\\\n& =\\int_{y_{k} \\in B} \\int_{y_{j} \\in \\mathcal{Y}} \\int_{\\mathcal{Y} \\times \\cdots \\times \\mathcal{Y}} \\sum_{\\left(d_{0}, \\ldots, d_{J-1}\\right) \\in \\mathcal{S}_{j}} q_{\\left(d_{0}, \\ldots, d_{J-1}\\right)}^{*}\\left(y_{0}, \\ldots, y_{J-1}\\right) d \\nu\\left(y_{0}\\right) \\cdots d \\nu\\left(y_{J-1}\\right) \\\\\n& =\\int_{y_{k} \\in B} \\int_{y_{j} \\in \\mathcal{Y}} \\int_{\\mathcal{Y} \\times \\cdots \\times \\mathcal{Y}} \\prod_{j^{\\prime} \\neq j} \\lambda_{j^{\\prime}}\\left(y_{j^{\\prime}}\\right)\\left(p_{j}\\left(y_{j} \\mid z_{J-1}\\left(y_{j}, j\\right)\\right)-p_{j}\\left(y_{j} \\mid z_{\\ell\\left(y_{j}, j\\right)}\\left(y_{j}, j\\right)\\right)\\right) d \\nu\\left(y_{0}\\right) \\cdots d \\nu\\left(y_{J-1}\\right) \\\\\n& =\\int_{y_{k} \\in B} \\int_{y_{j} \\in \\mathcal{Y}} \\lambda_{k}\\left(y_{k}\\right)\\left(p_{j}\\left(y_{j} \\mid z_{J-1}\\left(y_{j}, j\\right)\\right)-p_{j}\\left(y_{j} \\mid k\\right)\\right) d \\nu\\left(y_{j}\\right) d \\nu\\left(y_{k}\\right)\n\\end{aligned}\n$$\n\nHence (A) equals\n\n$$\n\\begin{aligned}\n& \\int_{y_{k} \\in B}\\left(\\sum_{j \\neq k} \\int_{y_{j} \\in \\mathcal{Y}} \\lambda_{k}\\left(y_{k}\\right)\\left(p_{j}\\left(y_{j} \\mid z_{J-1}\\left(y_{j}, j\\right)\\right)-p_{j}\\left(y_{j} \\mid k\\right)\\right) d \\nu\\left(y_{j}\\right)\\right) d \\nu\\left(y_{k}\\right) \\\\\n& =\\int_{y_{k} \\in B}\\left(\\sum_{j \\neq k} \\int_{y \\in \\mathcal{Y}} \\lambda_{k}\\left(y_{k}\\right)\\left(p_{j}\\left(y \\mid z_{J-1}(y, j)\\right)-p_{j}(y \\mid k)\\right) d \\nu(y)\\right) d \\nu\\left(y_{k}\\right)\n\\end{aligned}\n$$\n\nFor (B), the construction yields that it equals\n\n$$\n\\begin{aligned}\n& \\int_{\\mathcal{Y} \\times \\cdots \\times B \\times \\cdots \\times \\mathcal{Y}} \\sum_{\\left(d_{0}, \\ldots, d_{J-1}\\right) \\in \\mathcal{S}_{k}} q_{\\left(d_{0}, \\ldots, d_{J-1}\\right)}^{*}\\left(y_{0}, \\ldots, y_{k}, \\ldots, y_{J-1}\\right) d \\nu\\left(y_{0}\\right) \\cdots d \\nu\\left(y_{J-1}\\right) \\\\\n& =\\int_{\\mathcal{Y} \\times \\cdots \\times B \\times \\cdots \\times \\mathcal{Y}} \\prod_{j^{\\prime} \\neq k} \\lambda_{j^{\\prime}}\\left(y_{j^{\\prime}}\\right) p_{k}\\left(y_{k} \\mid z_{J-1}\\left(y_{k}, k\\right)\\right) d \\nu\\left(y_{0}\\right) \\cdots d \\nu\\left(y_{J-1}\\right) \\\\\n& =\\int_{y_{k} \\in B} p_{k}\\left(y_{k} \\mid z_{J-1}\\left(y_{k}, k\\right)\\right) d \\nu\\left(y_{k}\\right)\n\\end{aligned}\n$$\n\nFor (C), again by construction, it equals\n\n$$\n\\int_{y_{k} \\in B} \\lambda_{k}\\left(y_{k}\\right)\\left(1-\\sum_{0 \\leq j \\leq J-1} \\int_{\\mathcal{Y}} p_{j}\\left(y \\mid z_{J-1}(y, j)\\right) d \\nu(y)\\right) d \\nu\\left(y_{k}\\right)\n$$\n\nSumming (A), (B), and (C), we have\n\n$$\nQ^{*}\\left\\{Y_{k} \\in B, D_{k}=k\\right\\}=\\int_{y_{k} \\in B} q_{k}\\left(y_{k}\\right) d \\nu\\left(y_{k}\\right)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& q_{k}\\left(y_{k}\\right) \\\\\n& =\\sum_{j \\neq k} \\int_{y \\in \\mathcal{Y}} \\lambda_{k}\\left(y_{k}\\right)\\left(p_{j}\\left(y \\mid z_{J-1}(y, j)\\right)-p_{j}(y \\mid k)\\right) d \\nu(y) \\\\\n& \\quad+\\lambda_{k}\\left(y_{k}\\right)\\left(1-\\sum_{0 \\leq j \\leq J-1} \\int_{y \\in \\mathcal{Y}} p_{j}\\left(y \\mid z_{J-1}(y, j)\\right) d \\nu(y)\\right) \\\\\n& \\quad+p_{k}\\left(y_{k} \\mid z_{J-1}\\left(y_{k}, k\\right)\\right) \\\\\n& =\\lambda_{k}\\left(y_{k}\\right) \\int_{y \\in \\mathcal{Y}}\\left(1-p_{k}\\left(y \\mid z_{J-1}(y, k)\\right)-\\sum_{j \\neq k} p_{j}(y \\mid k)\\right) d \\nu(y)+p_{k}\\left(y_{k} \\mid z_{J-1}\\left(y_{k}, k\\right)\\right) \\\\\n& =\\lambda_{k}\\left(y_{k}\\right) \\int_{y \\in \\mathcal{Y}}\\left(p_{k}(y \\mid k)-p_{k}\\left(y \\mid z_{J-1}(y, k)\\right)\\right) d \\nu(y)+p_{k}\\left(y_{k} \\mid z_{J-1}\\left(y_{k}, k\\right)\\right) \\\\\n& =p_{k}\\left(y_{k} \\mid k\\right)\n\\end{aligned}\n$$\n\nwhere the third equality follows because\n\n$$\n\\int_{\\mathcal{Y}}\\left(\\sum_{j \\neq k} p_{j}(y \\mid k)+p_{k}(y \\mid k)\\right) d \\nu(y)=1\n$$\n\nand the last equality holds regardless of whether or not denominator in the definition of $\\lambda_{k}\\left(y_{k}\\right)$ is zero. Indeed, if it is not zero, then the equality holds because of the definition of $\\lambda_{k}\\left(y_{k}\\right)$ in (37); if is zero, then $p_{k}\\left(y_{k} \\mid k\\right)=p_{k}\\left(y_{k} \\mid z_{J-1}\\left(y_{k}, k\\right)\\right)$, so the equality still holds. The desired result in (38) now follows from (39), and the proof is complete.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 19,
      "text": "# B. 4 Proof of Theorem 4.2 \n\nAs shown above, the results follow from Corollary 4.1 with the restriction that $P\\{D=j \\mid Z=z\\}=0$ for $j \\notin\\{z, 0\\}$. Here, we provide a direct proof for completeness. The necessities in both (a) and (b) are clear. To prove the sufficiency part in (a), we simply apply the construction in the proof in Appendix B.2.2, only noting that because $\\max _{z \\neq j} P\\{D=j \\mid Z=z\\}=0$ for $1 \\leq j \\leq J-1$, step (a) in that proof is omitted, whereas only $j=0$ appears in steps (b) and (c). The proof of sufficiency in (b) is also similar to the proof of Theorem 4.1, with the only difference being that for $1 \\leq j \\leq J-1$,\n\n$$\n\\lambda_{j}(y)=\\frac{p_{j}(y \\mid j)}{P\\{D=0 \\mid Z=0\\}-P\\{D=0 \\mid Z=j\\}}=\\frac{p_{j}(y \\mid j)}{1-P\\{D=0 \\mid Z=j\\}}\n$$\n\nbecause for $j \\notin\\{z, 0\\}, p_{j}(y \\mid z)=0$ as the Radon-Nikodym derivative of $P\\{Y \\in \\cdot, D=j \\mid Z=z\\}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "## C Details of the Assumptions in Behaghel et al. (2013, 2014)\n\nBehaghel et al. (2013) impose the following two requirements which they refer to as \"extended monotonicity\":\n\n$$\n\\begin{aligned}\n& Q\\left\\{I\\left\\{D_{2}=1\\right\\}=I\\left\\{D_{0}=1\\right\\} \\leq I\\left\\{D_{1}=1\\right\\}\\right\\}=1 \\\\\n& Q\\left\\{I\\left\\{D_{1}=2\\right\\}=I\\left\\{D_{0}=2\\right\\} \\leq I\\left\\{D_{2}=2\\right\\}\\right\\}=1\n\\end{aligned}\n$$\n\nWe argue that (40)-(41) imply $J_{0}=1$ in our model and are strictly stronger. Indeed, if $D_{0}=1$, then $D_{1}=1$ and $D_{2}=1$ by (40). If $D_{0}=2$, then $D_{2}=2$ and $D_{1}=2$ by (41). If $D_{0}=0$, then $D_{2} \\neq 1$ by (40), so $D_{2} \\in\\{0,2\\}=\\left\\{D_{0}, 2\\right\\}$, and similarly $D_{1} \\in\\{0,1\\}=\\left\\{D_{0}, 1\\right\\}$ by (41). In all cases, $D_{j} \\in\\left\\{D_{0}, j\\right\\}$ for $0 \\leq j \\leq 2$. Clearly, (40)-(41) are stronger than imposing $J_{0}=1$ in our model because when $J_{0}=1$, it is possible that $D_{0}=1$ but $D_{2}=2$, so that (40) is violated.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "# D Additional Simulations\n## D. 1 Simulations for $J=2$\n\nIn this subsection, we examine the sensitivity of the test in Chernozhukov et al. (2013) when $J=2$ through the simulation designs in the appendix to Mourifi\u00e9 and Wan (2017). We refer the reader to their appendix for the details of the data generating process (DGP). We again implement both the parametric and local approaches to estimate the conditional moments; this time at a few different significance levels. We only study the size properties of the test and therefore all designs satisfy the null in (19) (for both $J_{0}=0$ and $J_{0}=1$, since they are equivalent when $J=2$ ). We also consider several values of $\\rho$ in the DGP. The results are presented in Table 13. Similar to what we observe in the main text, the test using the local approach controls size poorly, while the test using the parametric approach controls size well.\n\n![table_12](table_12)\n\nTable 13: Rejection probabilities in percentages for the data generating process \"DGP2\" in the appendix to Mourifi\u00e9 and Wan (2017) using the test in Chernozhukov et al. (2013).",
      "tables": {
        "table_12": "| $n$ | parametric |  |  | local |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | $\\rho$ |  |  | $\\rho$ |  |  |\n|  | 0 | 0.3 | 0.7 | 0 | 0.3 | 0.7 |\n|  | Level $=10 \\%$ |  |  |  |  |  |\n| 200 | 6.60 | 5.55 | 6.20 | 25.80 | 19.55 | 21.85 |\n| 400 | 5.50 | 6.40 | 6.35 | 16.65 | 14.85 | 17.80 |\n| 800 | 5.45 | 5.35 | 6.00 | 10.40 | 10.70 | 14.75 |\n|  | Level $=5 \\%$ |  |  |  |  |  |\n| 200 | 2.90 | 2.80 | 3.65 | 21.65 | 14.45 | 14.10 |\n| 400 | 2.50 | 3.05 | 2.70 | 11.30 | 8.35 | 10.65 |\n| 800 | 2.60 | 2.45 | 2.90 | 6.35 | 5.60 | 8.45 |\n|  | Level $=1 \\%$ |  |  |  |  |  |\n| 200 | 0.45 | 0.80 | 0.55 | 17.35 | 8.75 | 6.60 |\n| 400 | 0.25 | 0.30 | 0.55 | 6.65 | 4.15 | 3.75 |\n| 800 | 0.25 | 0.45 | 0.45 | 2.90 | 1.20 | 2.25 |"
      },
      "images": {}
    },
    {
      "section_id": 22,
      "text": "# References \n\nAndrews, D. W. K. and Shi, X. (2013). Inference Based on Conditional Moment Inequalities. Econometrica, 81 609-666.\n\nAngrist, J., Lang, D. and Oreopoulos, P. (2009). Incentives and Services for College Achievement: Evidence from a Randomized Trial. American Economic Journal: Applied Economics, 1 $136-163$.\n\nArmstrong, T. B. and Chan, H. P. (2016). Multiscale adaptive inference on conditional moment inequalities. Journal of Econometrics, 194 24-43.\n\nBai, Y., Huang, S., Moon, S., Santos, A., Shaikh, A. M. and Vytlacil, E. J. (2024a). Inference for treatment effects conditional on generalized principal strata using instrumental variables. arXiv preprint arXiv:2411.05220.\n\nBai, Y., Huang, S., Moon, S., Shaikh, A. M. and Vytlacil, E. J. (2024b). On the Identifying Power of Monotonicity for Average Treatment Effects. ArXiv:2405.14104 [econ, stat], URL http://arxiv.org/abs/2405.14104.\n\nBalke, A. and Pearl, J. (1997a). Bounds on Treatment Effects from Studies with Imperfect Compliance. Journal of the American Statistical Association, 92 1171-1176.\n\nBalke, A. A. and Pearl, J. (1997b). Probabilistic Counterfactuals: Semantics, Computation, and Applications. Tech. rep., DTIC Document.\n\nBehaghel, L., Cr\u00c9pon, B. and GurGand, M. (2013). Robustness of the encouragement design in a two-treatment randomized control trial.\n\nBehaghel, L., Cr\u00c9pon, B. and GurGand, M. (2014). Private and Public Provision of Counseling to Job Seekers: Evidence from a Large Controlled Experiment. American Economic Journal: Applied Economics, 6 142-174.\n\nBeresteanu, A., Molchanov, I. and Molinari, F. (2012). Partial identification using random set theory. Journal of Econometrics, 166 17-32.\n\nBhattacharya, J., Shaikh, A. M. and Vytlacil, E. (2008). Treatment Effect Bounds under Monotonicity Assumptions: An Application to Swan-Ganz Catheterization. American Economic Review, 98 351-356.\n\nBhattacharya, J., Shaikh, A. M. and Vytlacil, E. (2012). Treatment effect bounds: An application to Swan-Ganz catheterization. Journal of Econometrics, 168 223-243.\n\nBhuller, M. and Sigstad, H. (2024). 2sls with multiple treatments. Journal of Econometrics, 242105785 .\n\nCanay, I. A. and Shaikh, A. M. (2017). Practical and Theoretical Advances in Inference for Partially Identified Models. In Advances in Economics and Econometrics: Eleventh World Congress (A. Pakes, B. Honor\u00e9, L. Samuelson and M. Piazzesi, eds.), vol. 2 of Econometric Society Monographs. Cambridge University Press, Cambridge, 271-306.\n\nChernozhukov, V., Kim, W., Lee, S. and Rosen, A. M. (2015). Implementing intersection bounds in stata. The Stata Journal, 15 21-44.\n\nChernozhukov, V., Lee, S. and Rosen, A. M. (2013). Intersection Bounds: Estimation and Inference. Econometrica, 81 667-737.\n\nChetverikov, D. (2018). Adaptive tests of conditional moment inequalities. Econometric Theory, 34 186-227.\n\nDuflo, E., Glennerster, R. and Kremer, M. (2007). Chapter 61 Using Randomization in Development Economics Research: A Toolkit. In Handbook of Development Economics (T. P. Schultz and J. A. Strauss, eds.), vol. 4. Elsevier, 3895-3962.\n\nFang, Z., Santos, A., Shaikh, A. M. and Torgovitsky, A. (2023). Inference for Large-Scale Linear Systems With Known Coefficients. Econometrica, 91 299-327.\n\nGoff, L. (2024). A vector monotonicity assumption for multiple instruments. Journal of Econometrics, 241105735.\n\nHeckman, J. J. and Pinto, R. (2018). Unordered monotonicity. Econometrica, 86 1-35.\nHeckman, J. J. and Vytlacil, E. (2005). Structural Equations, Treatment Effects, and Econometric Policy Evaluation. Econometrica, 73 669-738.\n\nHolland, P. W. (1988). Causal Inference, Path Analysis, and Recursive Structural Equations Models. Sociological Methodology, 18 449-484.\n\nImbens, G. W. and Angrist, J. D. (1994). Identification and Estimation of Local Average Treatment Effects. Econometrica, 62 467-475.\n\nKamat, V., Norris, S. and Pecenco, M. (2023). Identification in multiple treatment models under discrete variation. arXiv preprint arXiv:2307.06174.\n\nKirkeboen, L. J., Leuven, E. and Mogstad, M. (2016). Field of study, earnings, and selfselection. The Quarterly Journal of Economics, 131 1057-1111.\n\nKitagawa, T. (2015). A Test for Instrument Validity. Econometrica, 83 2043-2063.\nKitagawa, T. (2021). The identification region of the potential outcome distributions under instrument independence. Journal of Econometrics, 225 231-253.\n\nKline, P. and Walters, C. R. (2016). Evaluating public programs with close substitutes: The case of head start. The Quarterly Journal of Economics, 131 1795-1848.\n\nKwon, S. and Roth, J. (2024). Testing mechanisms. 2404.11739, URL https://arxiv.org/abs/2404.11739.\n\nK\u00e9dagni, D. and Mourifi\u00e9, I. (2020). Generalized instrumental inequalities: testing the instrumental variable independence assumption. Biometrika, 107 661-675.\n\nLau, C. and Torgovitsky, A. (2021). conroylau/lpinfer: v0.2.0. URL https://doi.org/10.5281/zenodo.5506545.\n\nLee, S. and Salani\u00e9, B. (2018). Identifying Effects of Multivalued Treatments. Econometrica, 86 1939-1963.\n\nLee, S. and Salani\u00e9, B. (2024). Treatment Effects with Targeting Instruments. ArXiv:2007.10432 [econ], URL http://arxiv.org/abs/2007.10432.\n\nLifshits, M. A. (1995). Gaussian Random Functions. Mathematics and Its Applications, Springer Netherlands.\n\nLuo, Y., Ponomarev, K. and Wang, H. (2024). Selecting inequalities for sharp identification in models with set-valued predictions. Working paper.\n\nMachado, C., Shaikh, A. M. and Vytlacil, E. J. (2019). Instrumental variables and the sign of the average treatment effect. Journal of Econometrics, 212 522-555.\n\nMogstad, M. and Torgovitsky, A. (2024). Instrumental Variables with Unobserved Heterogeneity in Treatment Effects. URL https://www.nber.org/papers/w32927.\n\nMogstad, M., Torgovitsky, A. and Walters, C. R. (2021). The Causal Interpretation of Two-Stage Least Squares with Multiple Instrumental Variables. American Economic Review.\n\nMourifi\u00e9, I. and Wan, Y. (2017). Testing Local Average Treatment Effect Assumptions. The Review of Economics and Statistics, 99 305-313.\n\nPowers, D. E. and Swinton, S. S. (1984). Effects of self-study for coachable test item types. Journal of Educational Psychology, 76 266-278.\n\nS\u0142oczy\u0144ski, T. (2020). When should we (not) interpret linear iv estimands as late? arXiv preprint arXiv:2011.06695.\n\nSun, Z. (2023). Instrument validity for heterogeneous causal effects. Journal of Econometrics, 237 105523 .\n\nVytlacil, E. (2002). Independence, Monotonicity, and Latent Index Models: An Equivalence Result. Econometrica, 70 331-341.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2411.09808v3",
  "authors": [
    "Yuehao Bai",
    "Shunzhuang Huang",
    "Max Tabord-Meehan"
  ],
  "categories": [
    "econ.EM"
  ],
  "abstract": "This paper studies a potential outcome model with a continuous or discrete\noutcome, a discrete multi-valued treatment, and a discrete multi-valued\ninstrument. We derive sharp, closed-form testable implications for a class of\nrestrictions on potential treatments where each value of the instrument\nencourages towards at most one unique treatment choice; such restrictions serve\nas the key identifying assumption in several prominent recent empirical papers.\nBorrowing the terminology used in randomized experiments, we call such a\nsetting an encouragement design. The testable implications are inequalities in\nterms of the conditional distributions of choices and the outcome given the\ninstrument. Through a novel constructive argument, we show these inequalities\nare sharp in the sense that any distribution of the observed data that\nsatisfies these inequalities is compatible with this class of restrictions on\npotential treatments. Based on these inequalities, we propose tests of the\nrestrictions. In an empirical application, we show some of these restrictions\nare violated and pinpoint the substitution pattern that leads to the violation.",
  "updated": "2025-03-25T20:17:51Z",
  "published": "2024-11-14T20:52:10Z"
}