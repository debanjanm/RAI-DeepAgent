{"title": "Large Deviations Inequalities for Unequal Probability Sampling Without\n  Replacement", "sections": [{"section_id": 0, "text": "#### Abstract\n\nWe provide bounds on the tail probabilities for simple procedures that generate random samples without replacement, when the probabilities of being selected need not be equal.\n\n\nLet $n \\geq k$ be two positive integers. Consider a population of size $n$ with associated relative weights $0 \\leq w^{1}, \\ldots, w^{n} \\leq 1 / k$ such that ${ }^{1} \\sum_{i=1}^{n} w^{i}=1$. We want to choose a random sample without replacement from the population $[n]=\\{1, \\ldots, n\\}$ such the probabilities of being in the sample are proportional to the weights; i.e., letting $S$ denote the random sample, we require that\n\n$$\n\\mathbb{P}[i \\in S]=k w^{i}\n$$\n\nfor every $i \\in[n]$ (this explains the constraint $w^{i} \\leq 1 / k$; the factor $k$ comes from ${ }^{2} \\sum_{i \\in[n]} \\mathbb{P}[i \\in S]=|S|=k$ ). Moreover, we want to do this in such a way\n\n[^0]\n[^0]:    *This note answers a question posed by Noam Nisan. We thank Noam Nisan and Benji Weiss for useful discussions and suggestions.\n    ${ }^{\\dagger}$ Department of Statistics, Wharton, University of Pennsylvania, Philadelphia, and Amazon, New York. e-mail: dean@foster.net web page: http://deanfoster.net\n    ${ }^{\\ddagger}$ Institute of Mathematics, Department of Economics, and Federmann Center for the Study of Rationality, The Hebrew University of Jerusalem. e-mail: hart@huji.ac.il web page: http://www.ma.huji.ac.il/hart\n    ${ }^{1}$ Superscripts are used as indices for elements (or subsets) of the population.\n    ${ }^{2}$ We write $|F|$ for the number of elements of a finite set $F$.\n\nthat we obtain \"concentration\" or \"large deviations\" inequalities, as in the case of sampling with replacement. That is, for every subset of the population $A \\subset[n]$, with high probability its proportion of the sample $|S \\cap A| / k$ does not exceed its relative weight $\\alpha=\\sum_{i \\in A} w^{i}$ by much: for $\\delta>0$,\n\n$$\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\geq \\alpha+\\delta\\right]\n$$\n\nis exponentially small in $k$ and $\\delta$.\nFor comparison, assume that the sampling is done with replacement, i.e., by $k$ i.i.d. draws from the population $[n]$, with probabilites $w^{1}, \\ldots, w^{n}$. Let $N^{i}$ be the number of times that $i$ has been selected, then $N^{A}:=\\sum_{i \\in A} N^{i}$, the number of times that an element of $A$ has been selected, is a Binomial $(k, \\alpha)$ random variable. Therefore, by the Chernoff-Hoeffding inequality (see (11) in the Appendix), for every $\\delta>0$ we have\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left[\\frac{1}{k} N^{A} \\geq \\alpha+\\delta\\right] & =\\mathbb{P}\\left[N^{A}-k \\alpha \\geq k \\delta\\right] \\\\\n& \\leq \\exp \\left(-D(\\alpha+\\delta \\| \\alpha) \\cdot k\\right) \\leq \\exp \\left(-2 \\delta^{2} k\\right)\n\\end{aligned}\n$$\n\nwhere\n\n$$\nD(q \\| p):=q \\ln \\frac{q}{p}+(1-q) \\ln \\frac{1-q}{1-p}\n$$\n\ndenotes the Kullback-Leibler divergence of from $p$ to $q$ (for $0<p, q<1$ ). (When the sampling is done without replacement each $N^{i}$ takes only the values 0 and 1 , and $N^{A}=|S \\cap A|$.)\n\nThere are various methods to generate random samples without replacement so that condition (1) holds. However, they do not immediately yield tail probability bounds such as (2). We will therefore consider martingale-based procedures, as they are naturally amenable to large deviation analysis. Such procedures have been proposed by Deville and Till\u00e9 [1998] (they call them \"splitting methods\"); in particular, Procedure $\\mathfrak{X}$ below is their \"pivotal method.\"", "tables": {}, "images": {}}, {"section_id": 1, "text": "# 1 Martingale-Based Procedures \n\nIt is convenient to rescale the weights so that they add to $k$; thus, put $\\Delta:=$ $\\left\\{x \\in[0,1]^{n}: \\sum_{i=1}^{n} x^{i}=k\\right\\}$, and let $\\Delta_{0}:=\\left\\{x \\in\\{0,1\\}^{n}: \\sum_{i=1}^{n} x^{i}=k\\right\\}$ be the set of extreme points of $\\Delta$, i.e., those weight vectors that contain $k$ ones and $n-k$ zeros. For every set $A \\subset[n]$ we write $x^{A}:=\\sum_{i \\in A} x^{i}$. A procedure that generates a random sample $S$ of size $k$ such that $\\mathbb{P}[i \\in S]=x^{i}$ for every $i \\in[n]$ is called an $x$-procedure. The vector of normalized weights $\\left(w^{1}, \\ldots, w^{n}\\right)$ yields the vector of weights $x_{0}:=k w=\\left(k w^{1}, \\ldots, k w^{n}\\right)$ in $\\Delta$.\n\nWe start with a trivial observation.\nObservation. Let $x=\\sum_{\\ell=1}^{L} \\lambda_{\\ell} x_{\\ell}$ where $x_{\\ell} \\in \\Delta$ and $\\lambda_{\\ell} \\geq 0$ for each $\\ell$, and $\\sum_{\\ell=1}^{L} \\lambda_{\\ell}=1$ (and thus $x \\in \\Delta$ as well). If $\\mathfrak{X}_{\\ell}$ is an $x_{\\ell}$-procedure for each $\\ell$, then the procedure $\\mathfrak{X}$ that with probability $\\lambda_{\\ell}$ follows $\\mathfrak{X}_{\\ell}$ is an $x$-procedure.\n\nThis is immediate by\n\n$$\n\\mathbb{P}_{\\mathfrak{X}}[i \\in S]=\\sum_{\\ell=1}^{L} \\lambda_{\\ell} \\mathbb{P}_{\\mathfrak{X}_{\\ell}}[i \\in S]\n$$\n\nIterating this observation yields a martingale: a stochastic process where at each step the (conditional) expectation of the \"value\" of the next state equals the \"value\" of the current state; in our case, these \"values\" will be weight vectors in $\\Delta$.\n\nLet thus $\\left(X_{t}\\right)_{t=0,1,2, \\ldots}$ be a $\\Delta$-valued martingale starting with the constant $X_{0}=x_{0}=k w$ and ending at a finite time $T$ with $X_{T}$ in $\\Delta_{0}$ (i.e., $X_{T}^{i}$ is either 0 or 1 for every $i$ ). ${ }^{3}$ Since for every $x$ in $\\Delta_{0}$ there is a unique $x$-procedurennamely, the deterministic choice of the sample as those $i$ whose $x^{i}$ is 1 (i.e., $\\left.S=\\left\\{i: x^{i}=1\\right\\}\\right)$-the observation above yields an $x_{0}$-procedure that uses these deterministic $X_{T}$-choices. Thus, $i$ belongs to the random sample $S$ if and only if $X_{T}^{i}=1$; and, for every set $A \\subset[n]$, the number of elements of $A$ in the sample is\n\n$$\n|S \\cap A|=\\sum_{i \\in A} X_{T}^{i}=X_{T}^{A}\n$$\n\n[^0]\n[^0]:    ${ }^{3}$ The time $T$ may well be random; for simplicity, once $\\Delta_{0}$ is reached the martingale stays constant, i.e., $X_{t}=X_{T}$ for all $t \\geq T$.\n\nThe martingale constructions below are based on moving weights around as much as possible, subject to the constraint that all weights stay between 0 and 1 .", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 2 General Procedures $\\mathfrak{X}$ \n\nWe describe a class of simple procedures that follow the \"pivotal method\" of Deville and Till\u00e9 [1998].\n\nProcedure $\\mathfrak{X}$. Start with $X_{0}=x_{0}$. At every step take two indices $i \\neq j$ such that ${ }^{4} 0<x^{i}, x^{j}<1$ (for now the order in which these steps are carried out is arbitrary, or, alternatively, random). We distinguish two cases: ${ }^{5}$\n(i) If $x^{i}+x^{j}<1$ then either the weight of $j$ is transferred to $i$ or the weight of $i$ is transferred to $j$ : with probability $x^{i} /\\left(x^{i}+x^{j}\\right)$ the new weights are $\\tilde{x}^{i}=x^{i}+x^{j}$ and $\\tilde{x}^{j}=0$, and with probability $x^{j} /\\left(x^{i}+x^{j}\\right)$ the new weights are $\\tilde{x}^{i}=0$ and $\\tilde{x}^{j}=x^{i}+x^{j}$ (the probabilities are determined by the martingale condition, i.e., the expectation of $\\tilde{x}^{i}$ being equal to $\\left.x^{i}\\right)$.\n(ii) If $x^{i}+x^{j} \\geq 1$ then either $1-x^{i}$ is transferred from $j$ to $i$ or $1-x^{j}$ is transferred from $i$ to $j$ : with probability ( $1-$ $\\left.x^{j}\\right) /\\left(2-x^{i}-x^{j}\\right)$ the new weights are $\\tilde{x}^{i}=1$ and $\\tilde{x}^{j}=x^{i}+x^{j}-1$, and with probability $\\left(1-x^{i}\\right) /\\left(2-x^{i}-x^{j}\\right)$ the new weights are $\\tilde{x}^{i}=x^{i}+x^{j}-1$ and $\\tilde{x}^{j}=1$ (the probabilities are again determined by the martingale condition, i.e., the expectation of $\\tilde{x}^{i}$ being equal to $\\left.x^{i}\\right)$.\n\nWe have in fact defined a class of procedures $\\mathfrak{X}$, depending on the order in which the active $i, j$ are chosen at every step; we will see in the next section that doing so in a consistent way allows proving better probability bounds.\n\n[^0]\n[^0]:    ${ }^{4}$ If this is step $t$, then $x^{i}$ stands for $X_{t-1}^{i}$ and $\\tilde{x}^{i}$ for $X_{t}^{i}$.\n    ${ }^{5}$ The reason that we have conveniently included $x^{i}+x^{j}=1$ in Case 2 will become clear in the next section.\n\nThe procedures $\\mathfrak{X}$ have the following properties. First, at each step (at least) one weight becomes 0 or 1 and will no longer change, which implies that the number of steps $T$ until $\\Delta_{0}$ is reached is at most $n$. Second, at each step the amount of weight that is moved is at most 1 .\n\nPut\n\n$$\n\\pi(\\eta, \\delta, k):=\\left[\\left(\\frac{\\eta}{\\eta+\\delta}\\right)^{\\eta+\\delta} \\exp (\\delta)\\right]^{k} \\leq \\exp \\left(-\\frac{\\delta^{2} / 2}{\\eta+\\delta / 3} k\\right)\n$$\n\n(as we will see below, these are probability bounds given by the Freedman inequality (13) in the Appendix).\n\nTheorem 1 The procedures $\\mathfrak{X}$ satisfy $\\mathbb{P}[i \\in S]=k w^{i}$ for every $i$ in $[n]$, and\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\geq \\alpha+\\delta\\right] & \\leq \\pi(\\eta, \\delta, k) \\text { and } \\\\\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\leq \\alpha-\\delta\\right] & \\leq \\pi(\\eta, \\delta, k)\n\\end{aligned}\n$$\n\nfor every set $A \\subset[n]$ and $\\delta \\geq 0$, where\n\n$$\n\\alpha:=\\sum_{i \\in A} w^{i}\n$$\n\nis the relative weight of $A$, and\n\n$$\n\\eta:=\\alpha-k \\sum_{i \\in A}\\left(w^{i}\\right)^{2} \\leq \\alpha\n$$\n\nProof. We will use the Freedman inequality (13). To do so we need to bound $V_{T}:=\\sum_{t=1}^{T} \\operatorname{Var}\\left(Y_{t}^{A} \\mid \\mathcal{F}_{t-1}\\right)$, where $Y_{t}^{i}:=X_{t}^{i}-X_{t-1}^{i}$ and $Y_{t}^{A}:=\\sum_{i \\in A} Y_{t}^{i}$ are the corresponding martingale differences. For each $i$ we have\n\n$$\n\\sum_{t=1}^{T} \\operatorname{Var}\\left(Y_{t}^{i} \\mid \\mathcal{F}_{t-1}\\right)=\\operatorname{Var}\\left(X_{T}^{i}\\right)=x_{0}^{i}\\left(1-x_{0}^{i}\\right)\n$$\n\nthe first equality because $X_{t}^{i}$ is a martingale, and the second because $X_{T}^{i} \\in$ $\\{0,1\\}$, and so $\\mathbb{P}\\left[X_{T}^{i}=1\\right]=X_{0}^{i}=x_{0}^{i}$ (again, since $X_{t}^{i}$ is a martingale).\n\nNext, we claim that for each $t$\n\n$$\n\\operatorname{Var}\\left(Y_{t}^{A} \\mid \\mathcal{F}_{t-1}\\right) \\leq \\sum_{i \\in A} \\operatorname{Var}\\left(Y_{t}^{i} \\mid \\mathcal{F}_{t-1}\\right)\n$$\n\nIndeed, let $i, j$ be the active indices at step $t$; if $i$ and $j$ are both outside $A$, then the two sides of (7) equal 0 ; if one of them, say $i$, is in $A$ and the other, $j$, is outside $A$, then the two sides equal $\\operatorname{Var}\\left(Y_{t}^{i} \\mid \\mathcal{F}_{t-1}\\right)$; and if $i$ and $j$ are both in $A$ then the lefthand side is 0 (because the total change in the weight of $A$, namely $Y_{t}^{A}$, is zero).\n\nAdding (7) over $t$ and then using (6) yields\n\n$$\n\\begin{aligned}\nV_{T} & =\\sum_{t=1}^{T} \\operatorname{Var}\\left(Y_{t}^{A} \\mid \\mathcal{F}_{t-1}\\right) \\leq \\sum_{t=1}^{T} \\sum_{i \\in A} \\operatorname{Var}\\left(Y_{t}^{i} \\mid \\mathcal{F}_{t-1}\\right) \\\\\n& =\\sum_{i \\in A} \\sum_{t=1}^{T} \\operatorname{Var}\\left(Y_{t}^{i} \\mid \\mathcal{F}_{t-1}\\right)=\\sum_{i \\in A} x_{0}^{i}\\left(1-x_{0}^{i}\\right) \\\\\n& =\\sum_{i \\in A} x_{0}^{i}-\\sum_{i \\in A}\\left(x_{0}^{i}\\right)^{2}=k w^{A}-k^{2} \\sum_{i \\in A}\\left(w^{i}\\right)^{2}=k \\eta\n\\end{aligned}\n$$\n\nThe Freedman's inequality (13), with $c=k \\delta$ and $v=k \\eta$, then gives\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\geq \\alpha+\\delta\\right] & =\\mathbb{P}\\left[X_{T}^{A}-X_{0}^{A} \\geq k \\delta\\right] \\leq\\left(\\frac{k \\eta}{k \\eta+k \\delta}\\right)^{k \\eta+k \\delta} \\exp (k \\delta) \\\\\n& =\\left[\\left(\\frac{\\eta}{\\eta+\\delta}\\right)^{\\eta+\\delta} \\exp (\\delta)\\right]^{k}=\\pi(\\eta, \\delta, k)\n\\end{aligned}\n$$\n\nwhich is (3).\nFinally, consider the martingale $-X_{t}^{A}$ : its differences are $-Y_{t}^{A}$, and so the sum of variances is the same $V_{T}$ above; this yields\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\leq \\alpha-\\delta\\right] & =\\mathbb{P}\\left[X_{T}^{A}-X_{0}^{A} \\leq-k \\delta\\right] \\\\\n& =\\mathbb{P}\\left[\\left(-X_{T}^{A}\\right)-\\left(-X_{0}^{A}\\right) \\geq k \\delta\\right] \\leq \\pi(\\eta, \\delta, k)\n\\end{aligned}\n$$\n\nwhich is (4).", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 3 \"In Order\" Procedures $\\mathfrak{X}^{*}$ \n\nOne issue with the general procedures $\\mathfrak{X}$ is that their number of steps is $n$ rather than $k$ (as in sampling with replacement); the Freedman inequality circumvents this by counting conditional variances rather than steps. We will now see that running a procedure $\\mathfrak{X}$ \"in order\" (see below; we call this Procedure $\\mathfrak{X}^{*}$ ) allows combining sequences of consecutive steps into \"rounds,\" in such a way that the number of rounds is at most $k$, and the martingale that corresponds to these rounds (while ignoring the individual steps) enjoys the same properties as the original (step-based) martingale used in the previous section.\n\nProcedure $\\mathfrak{X}^{*}$. Fix an order on $[n]$, say the natural order. Run procedure $\\mathfrak{X}$ with the active $i$ and $j$ that exchange weights at each step $t$ being the minimal \"yet undecided\" indices, i.e., the minimal $i \\neq j$ with ${ }^{6} 0<X_{t-1}^{i}, X_{t-1}^{j}<1$.\n\nPut\n\n$$\n\\pi^{*}(\\eta, \\delta, k):=\\exp \\left(-D\\left(\\frac{\\eta+\\delta}{1+\\eta}\\left\\|\\frac{\\eta}{1+\\eta}\\right) \\cdot k\\right)\\right.\n$$\n\nComparing with sampling with replacement, where we had $D(\\alpha+\\delta \\| \\alpha)$ in (2), we now have $D(\\tilde{\\alpha}+\\tilde{\\delta} \\| \\tilde{\\alpha})$ in (9) with $\\tilde{\\alpha}=\\alpha /(1+\\alpha)$ and $\\tilde{\\delta}=\\delta /(1+\\alpha)$ (we took $\\eta=\\alpha$ as a worst case; see the next section).\n\nTheorem 2 The procedures $\\mathfrak{X}^{*}$ satisfy $\\mathbb{P}[i \\in S]=k w^{i}$ for every $i$ in $[n]$, and\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\geq \\alpha+\\delta\\right] & \\leq \\pi^{*}(\\eta, \\delta, k) \\text { and } \\\\\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\leq \\alpha-\\delta\\right] & \\leq \\pi^{*}(\\eta, \\delta, k)\n\\end{aligned}\n$$\n\nfor every set $A \\subset[n]$ and $\\delta \\geq 0$, where $\\alpha:=\\sum_{i \\in A} w^{i}$ is the relative weight of $A$, and $\\eta:=\\alpha-k \\sum_{i \\in A}\\left(w^{i}\\right)^{2} \\leq \\alpha$.\n\n[^0]\n[^0]:    ${ }^{6}$ See Remarks (a) and (b) below.\n\nProof. When running a procedure $\\mathfrak{X}^{*}$, consider the times where some weight $i$ becomes 1 (which implies that $i$ will for sure be in the sample $S$ ). This happens $k$ times, ${ }^{7}$ specifically, whenever we are in case (ii) (i.e., when $x^{i}+x^{j} \\geq$ 1 ; note that only one weight can become 1 at a time, because $x^{i}, x^{j}<1$ implies $x^{i}+x^{j}<2$ ). Let $T_{0} \\equiv 0<T_{1}<T_{2}<\\ldots<T_{k} \\equiv T$ be these (random) stopping times, and put $Z_{\\ell}:=X_{T_{\\ell}}$ for $\\ell=0,1, \\ldots, k$. Then $\\left(Z_{\\ell}\\right)_{\\ell=0, \\ldots, k}$ is a martingale, $Z_{0}=x_{0}$, and $Z_{k}=X_{T} \\in \\Delta_{0}$. Moreover, the sum of the conditional variances is the same for the two martingales: for every set $A \\subset$ $[n]$ we have\n\n$$\n\\begin{aligned}\n\\sum_{\\ell=1}^{k} \\operatorname{Var}\\left[Z_{\\ell}^{A}-Z_{\\ell-1}^{A} \\mid \\mathcal{F}_{T_{\\ell-1}}\\right] & =\\sum_{\\ell=1}^{k} \\sum_{t=T_{\\ell-1}+1}^{T_{\\ell}} \\operatorname{Var}\\left[Y_{t}^{A} \\mid \\mathcal{F}_{t-1}\\right] \\\\\n& =\\sum_{t=1}^{T} \\operatorname{Var}\\left[Y_{t}^{A} \\mid \\mathcal{F}_{t-1}\\right] \\leq k \\eta\n\\end{aligned}\n$$\n\nwhere the inequality is by (8).\nNext, we claim that the total weight that is moved in each round is at most 1 ; i.e.,\n\n$$\n\\left|Z_{\\ell}^{A}-Z_{\\ell-1}^{A}\\right| \\leq 1\n$$\n\nfor every set $A \\subset[n]$ and $\\ell=1, \\ldots, k$. Indeed, consider the first round, and assume that it ended at step $T_{1}=t$; i.e., $Z_{1}=X_{t}$. Thus, the first $t-1$ steps were case (i), and step $t$ was case (ii); i.e., $x_{0}^{1}+x_{0}^{2}<1$ in step 1 ; then $\\left(x_{0}^{1}+x_{0}^{2}\\right)+x_{0}^{3}$ in step 2 ; and so on, up to $\\left(x_{0}^{1}+\\ldots+x_{0}^{t-1}\\right)+x_{0}^{t}<1$ in step $t-1$; and finally $\\left(x_{0}^{1}+\\ldots+x_{0}^{t}\\right)+x_{0}^{t+1} \\geq 1$ in step $t$. Let $r \\in\\{1, \\ldots, t\\}$ be the \"winner\" of the first $t-1$ steps, i.e., $X_{t-1}^{r}=x_{0}^{1}+\\ldots+x_{0}^{t}=: \\xi$ (for all other $i$ in $\\{1, \\ldots t\\}$ we have $\\left.X_{t-1}^{i}=0\\right)$. If the result of step $t$ is that $X_{t}^{r}=1$ and $X_{t}^{t+1}=\\xi+x_{0}^{t}-1$, then the only coordinate that has increased from $Z_{0}=x_{0}$ to $Z_{1}=X_{t}$ is coordinate $r$, and so the total weight that was moved is equal to the gain of $r$, which is $1-x_{0}^{r}<1$. If the result of step $t$ is that $X_{t}^{r}=\\xi+x_{0}^{t}-1$ and $X_{t}^{t+1}=1$, then $t+1$ is not included in the set of those that lost weight\n\n[^0]\n[^0]:    ${ }^{7}$ If some starting weight $x_{0}^{i}$ equals 1 then $i$ must be in the sample $S$, and so in general there are $k_{1}:=k-\\left|\\left\\{i: x_{0}^{i}=1\\right\\}\\right| \\leq k$ rounds. For simplicity assume that $x_{0}^{i}<1$ for all $i$ in $N$, and thus $k_{1}=k$.\n\n(because $t+1$ has surely gained weight), and so the total weight that was moved, which equals the total weight that was lost, is at most the sum of the weights of $1, \\ldots, t$, which is $x_{0}^{1}+\\ldots+x_{0}^{t}=\\xi<1$. Thus in every situation the total weight that was moved is $<1$, and so $\\left|Z_{1}^{A}-Z_{0}^{A}\\right|<1$ for every set $A \\subseteq[n]$. The same argument applies to every round, which proves (10).\n\nWe can therefore apply the Fan, Grama, and Liu inequality (see (12) in the Appendix) with $v=k \\eta, c=k \\delta$, and $T=k$, yielding\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\geq w^{A}+\\delta\\right] & =\\mathbb{P}\\left[X_{T}^{A}-X_{0}^{A} \\geq k \\delta\\right] \\\\\n& \\leq \\exp \\left(-D\\left(\\frac{k \\eta+k \\delta}{k \\eta+k} \\| \\frac{k \\eta}{k \\eta+k}\\right) \\cdot k\\right)\n\\end{aligned}\n$$\n\nwhich equals $\\pi^{*}(\\eta, \\delta, k)$, as claimed. Again, the martingale $-X_{t}^{A}$ provides the second inequality.\n\nRemarks. (a) One can carry out the sequence of steps of one round with a single randomization, as follows. Let the weights involved be, in order, $x^{1}, \\ldots, x^{t}, x^{t+1}$; thus, $\\xi:=x^{1}+\\ldots+x^{t}<1$ and $\\xi+x^{t+1} \\geq 1$ (cf. the proof above). Then for each $r$ in $\\{1, \\ldots, t\\}$, with probability $\\left(x^{r} / \\xi\\right) \\cdot(1-\\xi) /(2-\\xi-$ $\\left.x^{t+1}\\right)$ the weights at the end of the round are $\\tilde{x}^{r}=1, \\tilde{x}^{t+1}=\\xi+x^{t+1}-1$, and $\\tilde{x}^{i}=0$ for all other $i$ in $\\{1, \\ldots, t\\}$, and with probability $\\left(x^{r} / \\xi\\right) \\cdot\\left(1-x^{t+1}\\right) /(2-$ $\\left.\\xi-x^{t+1}\\right)$ they are $\\tilde{x}^{r}=\\xi+x^{t+1}-1, \\tilde{x}^{t+1}=1$, and $\\tilde{x}^{i}=0$. Thinking of this as \"contests\" where the \"winner\" gets as much weight as possible from the \"loser,\" the procedure first selects the winner $r$ among $\\{1, \\ldots, t\\}$ with probabilites proportional to the weights-and then the winner between $r$ (whose weight is now $\\xi=x^{1}+\\ldots+x^{t}$ ) and $t+1$-with probabilities proportional to $1-\\xi$ and $1-x^{t+1}$.\n\nDoing this for all rounds yields a procedure $\\mathfrak{X}^{* *}$ that takes at most $k$ time periods.\n(b) The \"in order\" requirement on $\\mathfrak{X}^{*}$ is used to ensure that in every round, the active pair $\\{i, j\\}$ in each step consists of the \"winner\" of the previous step together with a new element. However, as each round is considered separately, there is no need for this order to be kept between rounds (and so the round does not have to start with the winner of the previous round).\n\nThat is, the requirement on $\\mathfrak{X}^{*}$ is just to keep a consistent sequential order within each round.\n(c) The fact that the martingale $Z_{\\ell}$ of $\\mathfrak{X}^{*}$ has at most $k$ time periods in which the martingale differences are bounded by 1 (rather than $n$ time periods for the martingale $X_{t}$ ) yields an immediate exponential bound, without having to estimate the sum of variances $V_{T}$ : the Azuma-Hoeffding inequality ((12) in the Appendix, with $T=k$ ) gives\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\geq \\alpha+\\delta\\right] & =\\mathbb{P}\\left[Z_{k}^{A}-Z_{0}^{A} \\geq k \\delta\\right] \\\\\n& \\leq \\exp \\left(-\\frac{(k \\delta)^{2} / 2}{k}\\right)=\\exp \\left(-\\frac{1}{2} \\delta^{2} k\\right)\n\\end{aligned}\n$$\n\nOf course, Theorems 1 and 2 yield better bounds.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 4 Using the Bounds \n\nWe provide two comments on using the above bounds. Note that both $\\pi$ and $\\pi^{*}$ are increasing functions of their first variable ${ }^{8} \\eta$.\n\nThe best of $A$ and its complement. Let $B:=[n] \\backslash A$ be the complement of $A$; its relative weight is $w^{B}=1-w^{A}=1-\\alpha$. Since $(1 / k)|S \\cap A| \\geq$ $\\alpha+\\delta$ if and only if $(1 / k)|S \\cap B| \\leq 1-\\alpha-\\delta$, we can take the better of the two bounds $\\pi^{*}\\left(\\eta^{A}, \\delta, k\\right)$ and $\\pi^{*}\\left(\\eta^{B}, \\delta, k\\right)$ (where $\\eta^{A}:=\\alpha-k \\sum_{i \\in A}\\left(w^{i}\\right)^{2} \\leq \\alpha$ and $\\eta^{B}:=1-\\alpha-k \\sum_{i \\in B}\\left(w^{i}\\right)^{2} \\leq 1-\\alpha$ ), and get\n\n$$\n\\mathbb{P}\\left[\\frac{1}{k}|S \\cap A| \\geq \\alpha+\\delta\\right] \\leq \\pi^{*}\\left(\\min \\left\\{\\eta^{A}, \\eta^{B}\\right\\}, \\delta, k\\right)\n$$\n\n(by the monotonicity of $\\pi^{*}$ in $\\eta$ ); the same for $\\mathbb{P}[(1 / k)|S \\cap A| \\leq \\alpha-\\delta]$, and also for $\\pi$. Since $\\min \\left\\{\\eta^{A}, \\eta^{B}\\right\\} \\leq \\min \\{\\alpha, 1-a\\} \\leq 1 / 2$, we immediately get a\n\n[^0]\n[^0]:    ${ }^{8}$ This follows, for instance, from the arguments in Section 3 of Fan, Grama, and Liu [2012]: for $\\pi^{*}$ by equation (41) and Lemma 3.2, and for $\\pi$ by equation (43) (with their $v^{2} / n$ as our $\\eta$ ).\n\nuniform bound that is independent of ${ }^{9} A$ :\n\n$$\n\\pi^{*}\\left(\\frac{1}{2}, \\delta, k\\right)=\\exp \\left(-D\\left(\\frac{1}{3}+\\frac{2 \\delta}{3}\\left\\|\\frac{1}{3}\\right\\rangle \\cdot k\\right) \\leq \\exp \\left(-\\frac{8}{9} \\delta^{2} k\\right)\\right.\n$$\n\ncf. $\\exp \\left(-2 \\delta^{2} k\\right)$ in $(2)$.\nBounds on $\\eta$. One needs to estimate $\\eta=\\alpha-k \\sum_{i \\in A}\\left(w^{i}\\right)^{2}$ (see (5)); more precisely, bound it from above (because, again, $\\pi$ and $\\pi^{*}$ are increasing in $\\eta$ ). An immediate bound is of course $\\eta \\leq \\alpha$. For a better bound, assume that $A$ contains at most $m$ elements ${ }^{10}$ (i.e., $|A| \\leq m$ ); then $\\sum_{i \\in A}\\left(w^{i}\\right)^{2} \\geq$ $m(\\alpha / m)^{2}=\\alpha^{2} / m$ (by the convexity of the function $\\left.x \\mapsto x^{2}\\right)$, and so ${ }^{11}$\n\n$$\n\\eta \\leq \\alpha-\\frac{k}{m} \\alpha^{2}=: \\bar{\\eta}_{\\alpha, m}\n$$\n\n(with $\\bar{\\eta}_{\\alpha, \\infty}=\\alpha$ corresponding to the trivial bound $\\eta \\leq \\alpha$ ).\nFor example, take $k=100, \\alpha=1 / 5$, and $\\delta=1 / 3-1 / 5$; thus, we want to estimate the probability that a set that has $1 / 5$ of the weight gets more than a $1 / 3$ of the sample. The upper bounds on this probability, using $\\bar{\\eta}_{\\alpha, m}$, are as follows:\n\n![table_0](table_0)\n\n${ }^{9}$ The inequality is by Pinsker's inequality $D(q \\| p) \\geq 2(q-p)^{2}$. There are sharper inequalities; for instance, it can be shown that $D(q \\| p) /(q-p)^{2}$ is minimized at $q=1-p$, which yields\n\n$$\nD(q \\mid p) \\geq C(p)(q-p)^{2}\n$$\n\nfor\n\n$$\nC(p)=\\frac{D(1-p \\| p)}{(1-2 p)^{2}}=\\frac{1}{1-2 p} \\ln \\left(\\frac{1-p}{p}\\right)\n$$\n\nIn our case we get $D(1 / 3+2 \\delta / 3 \\| 1 / 3) \\geq \\gamma \\delta^{2}$ for $\\gamma=(2 / 3)^{2} C(1 / 3)=4 \\ln (2) / 3 \\approx 0.92$, and so $8 / 9$ may be increased to $\\gamma$.\n${ }^{10} \\mathrm{~A}$ trivial bound for $m$ is, of course, $m \\leq n$; a slightly better one is $m \\leq n-\\lceil k(1-\\alpha)\\rceil$, because the number of elements of $N \\backslash A$, whose relative weight is $w^{N \\backslash A}=1-\\alpha$, is at least $(1-\\alpha) /(1 / k)$ (since each $w^{i}$ is at most $1 / k$ ).\n${ }^{11} \\mathrm{~A}$ precise computation yields $\\eta=\\alpha-(k / m) \\alpha^{2}-(k m) \\beta$ where $m=|A|$, and $\\beta:=$ $(1 / m) \\sum_{i \\in A}\\left(w^{i}-\\alpha / m\\right)^{2}=(1 / m) \\sum_{i \\in A}\\left(w^{i}\\right)^{2}-\\alpha^{2} / m^{2}$, the variance of the weights of $A$.", "tables": {"table_0": "|  | $m=\\infty$ | $m=1000$ | $m=100$ | $m=50$ |\n| :--: | :--: | :--: | :--: | :--: |\n| With replacement $[(2)]$ | 0.0077 | 0.0077 | 0.0077 | 0.0077 |\n| Procedure $\\mathfrak{X}[\\pi]$ | 0.0249 | 0.0233 | 0.0117 | 0.0037 |\n| Procedure $\\mathfrak{X}^{*}\\left[\\pi^{*}\\right]$ | 0.0212 | 0.0198 | 0.0097 | 0.0029 |"}, "images": {}}, {"section_id": 5, "text": "# A Appendix. Tail Probability Bounds \n\nLet $\\left(X_{t}\\right)_{t \\geq 0}$ be a martingale adapted to the sequence $\\left(\\mathcal{F}_{t}\\right)_{t \\geq 0}$ of $\\sigma$-fields, let $Y_{t}:=X_{t}-X_{t-1}$ be the martingale differences, and $V_{t}:=\\sum_{s=1}^{t} \\operatorname{Var}\\left[Y_{s} \\mid \\mathcal{F}_{s-1}\\right]=$ $\\sum_{s=1}^{t} \\mathbb{E}\\left[Y_{s}^{2} \\mid \\mathcal{F}_{s-1}\\right]$ the sum of their conditional variances. Recall that $D(q \\| p)$ denotes the Kullback-Leibler divergence from $p$ to $q$. The following inequalities hold for every $T \\geq 1$ and $c, v>0$.\n\n- Chernoff-Hoeffding inequality. Let $Y_{t}$ be i.i.d. Bernoulli $(p)-p$ for some $0<p<1$ (we subtract $p$ so that ${ }^{12} \\mathbb{E}\\left[Y_{t}\\right]=0$ ), and thus $X_{T}-X_{0}$ is $\\operatorname{Binomial}(T, p)-T p$; then\n\n$$\n\\mathbb{P}\\left[X_{T}-X_{0} \\geq c\\right] \\leq \\exp \\left(-D\\left(p+\\frac{c}{T} \\| p\\right) \\cdot T\\right) \\leq \\exp \\left(-\\frac{2 c^{2}}{T}\\right)\n$$\n\nSee Theorem 1 and Example 3 in Chernoff [1952], and Theorem 1 in Hoeffding [1963].\n\n- Azuma-Hoeffding inequality. Let $\\left|Y_{t}\\right| \\leq 1$ for every $t \\geq 1$; then\n\n$$\n\\mathbb{P}\\left[X_{T}-X_{0} \\geq c\\right] \\leq \\exp \\left(-\\frac{c^{2} / 2}{T}\\right)\n$$\n\nSee Theorem 2 in Hoeffding [1963] and Azuma [1967].\n\n- Freedman inequality. Let $\\left|Y_{t}\\right| \\leq 1$ for every $t \\geq 1$; then\n\n$$\n\\mathbb{P}\\left[X_{T}-X_{0} \\geq c\\right] \\leq\\left(\\frac{v}{v+c}\\right)^{v+c} \\exp (c) \\leq \\exp \\left(-\\frac{c^{2} / 2}{v+c / 3}\\right)\n$$\n\nfor any bound $v$ on $V_{T}$ (i.e., $v \\geq V_{T}$ a.s.). See Theorem (1.6) in Freedman [1975], which gives ${ }^{13}$ the above probability bound for the event\n\n[^0]\n[^0]:    ${ }^{12}$ We are stating the inequality in a way that parallels the martingale inequalities that follow. What (11) says is that\n\n    $$\n    \\mathbb{P}\\left[\\frac{1}{T} \\operatorname{Binomial}(T, p) \\geq p+\\delta\\right] \\leq \\exp (-D(p+\\delta\\|p) T) \\leq \\exp \\left(-2 \\delta^{2} T\\right)\n    $$\n\n    for every $T \\geq 1,0<p<1$, and $\\delta>0$.\n    ${ }^{13}$ The second bound is a slight improvement over the one given in Freedman's paper; see Theorem 1.1 in Tropp [2011] or Remark 2.1 in Fan, Gram, and Liu [2012].\n\n$\\left\\{X_{t}-X_{0} \\geq c\\right.$ and $\\left.V_{t} \\leq v\\right.$ for some $\\left.t\\right\\}$, and so implies (13) when $V_{T} \\leq v$ (a.s.).\n\n- Fan, Grama, and Liu inequality. Let $Y_{t} \\leq 1$ for every $t \\geq 1$; then\n\n$$\n\\mathbb{P}\\left[X_{T}-X_{0} \\geq c\\right] \\leq \\exp \\left(-D\\left(\\frac{v+c}{v+T} \\| \\frac{v}{v+T}\\right) \\cdot T\\right)\n$$\n\nfor any bound $v$ on $V_{T}$. See Theorem 2.1 in Fan, Grama, and Liu [2012], which gives the above probability bound ${ }^{14}$ for the larger event $\\left\\{X_{t}-X_{0} \\geq c\\right.$ and $\\left.V_{t} \\leq v\\right.$ for some $\\left.t \\leq T\\right\\}$.\n\nThe final inequality (14) implies all the previous ones (see Remark 2.1 and Corollary 2.1 in Fan, Grama, and Liu [2012]), and it reduces to the Chernoff-Hoeffding inequality (11) in the i.i.d. Bernoulli case. ${ }^{15}$", "tables": {}, "images": {}}, {"section_id": 6, "text": "# References \n\nAzuma, K. [1967], \"Weighted Sums of Certain Dependent Random Variables,\" T\u00f4hoku Mathematical Journal 19 (3), 357-367.\n\nChernoff, H. [1952], \"A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the Sum of Observations,\" Annals of Mathematical Statistics 23, 493-507.\n\nDeville, J.-C. and Y. Till\u00e9 [1998], \"Unequal Probability Sampling Without Replacement Through a Splitting Method,\" Biometrika 85, 89-101.\n\nFreedman, D. A. [1975], \"On Tail Probabilites for Martingales,\" The Annals of Probability $3(1), 100-118$.\n\n[^0]which is easily seen to be the same as the righthand side of (14). We prefer the expression with the Kullback-Leibler divergence as it makes it easier to compare with, say, (11).\n${ }^{15}$ For i.i.d. $Y_{t} \\sim \\operatorname{Bernoulli}(p)-p$ apply (14) to $\\bar{Y}_{t}=Y_{t} /(1-p) \\leq 1$, with $\\hat{c}=c /(1-p)$ and $\\hat{v}=\\operatorname{Tp} /(1-p)$.\n\n\n[^0]:    ${ }^{14}$ Fan et al. write their bound as\n\n    $$\n    \\left[\\left(\\frac{v}{v+c}\\right)^{v+c}\\left(\\frac{T}{T-c}\\right)^{T-c}\\right]^{\\frac{T}{v+T}}\n    $$\n\nHoeffding, W. [1963], \"Probability Inequalities for Sums of Bounded Random Variables,\" Journal of the American Statistical Association 58 (301), 1330 .\n\nTropp, J. A. [2011], \"Freedman's Inequality for Matrix Martingales,\" Electronic Communications in Probability 16, 262-270.", "tables": {}, "images": {}}], "id": "2411.03955v1", "authors": ["Dean P. Foster", "Sergiu Hart"], "categories": ["math.PR", "cs.GT", "stat.OT"], "abstract": "We provide bounds on the tail probabilities for simple procedures that\ngenerate random samples _without replacement_, when the probabilities of being\nselected need not be equal.", "updated": "2024-11-06T14:37:22Z", "published": "2024-11-06T14:37:22Z"}