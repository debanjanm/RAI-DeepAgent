{"title": "Dynamic Factor Analysis of High-dimensional Recurrent Events", "sections": [{"section_id": 0, "text": "#### Abstract\n\nRecurrent event time data arise in many studies, including biomedicine, public health, marketing, and social media analysis. High-dimensional recurrent event data involving many event types and observations have become prevalent with advances in information technology. This paper proposes a semiparametric dynamic factor model for the dimension reduction of high-dimensional recurrent event data. The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies. A nearly rate-optimal smoothing-based estimator is proposed. An information criterion that consistently selects the number of factors is also developed. Simulation studies demonstrate the effectiveness of these inference tools. The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained.\n\n\nKeywords: Counting process; factor analysis; marginal modelling; kernel smoothing; information criterion", "tables": {}, "images": {}}, {"section_id": 1, "text": "# 1 Introduction \n\nAs information technology advances, high-dimensional recurrent event data are becoming increasingly common. For example, such data are commonly seen in market basket analysis, which often tracks customers' purchasing behaviour over time to develop personalized recommendation strategies. Here, each customer can be viewed as an observation unit. Their shopping history can be viewed as a multivariate counting process, wherein the elements of the process correspond to a large number of merchandise items, and the event times correspond to the times when the items are purchased. Another example is text data from social media platforms (e.g., Liang et al., 2018; Bogdanowicz and Guan, 2022). In such data, a user's dynamics correspond to a multivariate counting process, where event times record the occurrence of words or phrases in posts (e.g., tweets). The user dynamics are often analyzed for user profiling, opinion mining, or understanding and predicting the information cascade on a social medium. High-dimensional recurrent event data also emerge in human-computer interactions such as simulated problem-solving tasks in educational assessment (Chen, 2020), where event times are the time-stamps of different types of actions. Data of a similar structure also appear in medicine and public health, finance, and insurance (e.g., Cook and Lawless, 2007; Sun, 2006; Yang, 2022).\n\nWe propose a dynamic factor model for analyzing high-dimensional recurrent event time data. This model introduces low-dimensional time-varying factors in a continuous time domain to capture the dynamic trends underlying a multivariate counting process while keeping the constant event-type-specific parameters, known as the loadings, to strengthen the interpretability. The model is only specified based on the mean rate functions (Lin et al., 2000), allowing for a flexible conditional dependence structure among the processes. This is crucial in applications such as consumer shopping behaviour analysis, where recurrent\n\nevents could be highly dependent due to population heterogeneity. Model identification is studied, based on which rotation methods for exploratory factor analysis (Browne, 2001; Rohe and Zeng, 2023) can be applied to the current model for obtaining an interpretable factor structure. Simultaneous estimation of factors and loadings is proposed based on a kernel-smoothed pseudo-likelihood function. We further propose an information criterion for determining the number of factors. Desirable asymptotic properties are established as the number of event types and the sample size grow to infinity. In particular, we show that the proposed information criterion consistently selects the number of factors, and the estimation is consistent and nearly rate-optimal. The proposed method is applied to a large grocery shopping dataset. This analysis finds interpretable customer factors that provide insight into grocery shopping behaviours.\n\nThe proposed method is related to frailty models for recurrent event data (e.g., AbuLibdeh et al., 1990; Chen et al., 2005). These models introduce correlated event-typespecific random effects (frailties) into the intensity functions to capture the dependence among events. With many event types, the traditional frailty model has to introduce many random effects and specify their joint distribution, making the model specification and parameter estimation challenging. The proposed model is also related to dynamic factor models for irregularly spaced longitudinal data (Chen and Zhang, 2020; Lu et al., 2015; Tang et al., 2017), where the dynamic factors are treated as stochastic processes, and Bayesian or empirical Bayesian inferences are performed. The proposed method may also be viewed as an extension of high-dimensional factor analysis methods (Bai and Li, 2012; Chen et al., 2020, 2021; Liu et al., 2023; Wang et al., 2019; He et al., 2023). In these methods, the latent factors are treated as unknown parameters rather than random variables during parameter estimation, which avoids distributional assumptions on the\n\nlatent factors and makes the estimation computationally more affordable. Based on this estimation framework, information criteria are developed for determining the number of factors (Bai and Ng, 2002; Chen and Li, 2022). The current work is similar in spirit but involves a more challenging task of estimating low-dimensional functions of dynamic factors.\n\nFor a matrix $\\mathbf{X}=\\left(x_{i j}\\right)_{N \\times J}$, let $\\|\\mathbf{X}\\|_{F}=\\left(\\sum_{i, j} x_{i j}^{2}\\right)^{1 / 2}$ and $\\|\\mathbf{X}\\|_{2 \\rightarrow \\infty}=\\sup _{\\|\\alpha\\|_{2}=1}\\|X \\alpha\\|_{\\infty}$ denote its Frobenius norm and two-to-infinity norm, respectively. For two real numbers $a$ and $b$, we write $a \\wedge b=\\min \\{a, b\\}$ and $a \\vee b=\\max \\{a, b\\}$. For two sequences of real numbers $\\left\\{a_{n}\\right\\}$ and $\\left\\{b_{n}\\right\\}$, we write $a_{n} \\ll b_{n}$ or, equivalently, $a_{n}=o\\left(b_{n}\\right)$, if $\\lim _{n \\rightarrow \\infty} a_{n} / b_{n}=0$, $a_{n}=O\\left(b_{n}\\right)$ (or $a_{n} \\lesssim b_{n}$ ) if there is a positive constant $M$ independent with $n$, such that $\\left|a_{n}\\right| \\leq M\\left|b_{n}\\right|$ for all $n$, and $a_{n} \\asymp b_{n}$ if there are two positive constants $M_{1}$ and $M_{2}$ independent with $n$, such that $M_{1}\\left|b_{n}\\right| \\leq\\left|a_{n}\\right| \\leq M_{2}\\left|b_{n}\\right|$. We use the standard $O_{p}(\\cdot)$ notation for stochastic boundedness in probability. We use $L_{N \\times J}^{2}[0,1]=\\left\\{\\left(f_{i j}(t)\\right)_{N \\times J}: 0 \\leq t \\leq\\right.$ $1,\\left\\|f_{i j}\\right\\|_{L^{2}[0,1]}<\\infty$ for all $\\left.i, j\\right\\}$ to denote the space of $N \\times J$-dimensional square integrable matrix-valued functions on $[0,1]$.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 2 Proposed Method\n### 2.1 Model\n\nConsider multivariate recurrent event data from $N$ independent observation units on a standardized time interval $[0,1]$. The data from observation unit $i$ can be described by $\\mathbf{Y}_{i}(t)=\\left(Y_{i 1}(t), \\ldots, Y_{i J}(t)\\right)^{\\top}$, where $J$ is the number of event types, and each component $Y_{i j}(t)$ is a right-continuous counting process. We introduce a factor model to reduce the dimensionality of data and further identify and interpret the factors underlying the observed processes. A marginal modelling approach (Lin et al., 2000) is adopted to accommodate\n\na more flexible conditional dependence structure among the processes. This approach specifies the mean rate function for each event type $j$ as\n\n$$\nE\\left(\\mathrm{~d} Y_{i j}(t)\\right)=f\\left(X_{i j}(t)\\right) \\mathrm{d} t\n$$\n\nwhere $f: \\mathbb{R} \\rightarrow[0, \\infty)$ is a pre-specified link function, and $X_{i j}(t)$ is an unknown function with a low-dimensional structure. Specifically, $X_{i j}(t)$ is parameterized as\n\n$$\nX_{i j}(t)=\\sum_{k=1}^{r} a_{j k} \\theta_{i k}(t)\n$$\n\nwhere $\\theta_{i k}(\\cdot)$ 's are functions that may be interpreted as unobserved dynamic factors, $a_{j k}$ 's are referred to as the loading parameters, and $r$ is the number of factors. We denote $\\boldsymbol{\\Theta}(t)=\\left(\\theta_{i k}(t)\\right)_{N \\times r}, \\mathbf{A}=\\left(a_{j k}\\right)_{J \\times r}$, and $\\mathbf{X}(t)=\\left(X_{i j}(t)\\right)_{N \\times J}$. Rewriting Eq. (2) in matrix form, we have $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\top}$, where both $\\boldsymbol{\\Theta}(\\cdot)$ and $\\mathbf{A}$ are to be estimated.\n\nRemark 1 (Link function). The link function $f$ is needed to ensure the mean rate function is non-negative. For simplicity, we let $f$ be known and set $f(x)=\\exp (x)$ in the numerical analysis. Extensions to the setting with unknown $f$ can be done by estimating the link function nonparametrically via, e.g., non-negative basis function approximations.\n\nRemark 2 (Intensity formulation). Alternative to the mean rate specification (1), one can model the intensity functions as $E\\left(\\mathrm{~d} Y_{i j}(t) \\mid \\mathcal{F}_{t}\\right)=f\\left(X_{i j}(t)\\right) d t$, for a suitable right-continuous filtration $\\left\\{\\mathcal{F}_{t}\\right\\}_{0 \\leq t \\leq 1}$ that leads to a martingale structure (Andersen et al., 1993). As pointed out by Lin et al. (2000), the mean rate specification (1) is more versatile than the intensity specification in that it allows arbitrary dependence structures among recurrent events. For example, when analyzing customers' purchasing behaviour, multiple merchandise items may be purchased simultaneously and, thus, have the same event time. When analyzing users'\n\ndynamics on a social medium, multiple words or phrases often appear in the same post and, thus, have the same event time. The intensity specification implies independent increments, i.e., $\\mathrm{d} Y_{i j}(t), j=1, \\ldots, J$, are conditionally independent given $\\mathcal{F}_{t}$. As a result, $\\mathrm{d} Y_{i j}(t)=1$ can only occur to one of the $J$ event types for a specific time $t$, which does not align with the real-world situations mentioned previously. The mean rate specification does not have this restriction.\n\nRemark 3 (Connection with factor models). The proposed model is closely related to the Poisson factor model for count data. Consider a special case of (1) when $Y_{i j}(t)$, $j=1, \\ldots, J$, are independent Poisson processes with static factors $\\theta_{i k}$, i.e., $f\\left(X_{i j}(t)\\right)=$ $f\\left(\\sum_{k=1}^{r} a_{j k} \\theta_{i k}\\right)$. In this case, the counts $\\left\\{Y_{i j}(1): i=1, \\ldots, N, j=1, \\ldots, J\\right\\}$ are a sufficient statistic for the unknown parameters, with $Y_{i j}(1)$ following a Poisson distribution with rate $f\\left(\\sum_{k=1}^{r} a_{j k} \\theta_{i k}\\right)$. This model for count data is known as the Poisson factor model (Chen et al., 2020; Wedel and Kamakura, 2001), where $a_{j k}$ 's are known as the loading parameters, and $\\theta_{i k}$ 's are interpreted as the unobserved factors. In this sense, the proposed model (1)-(2) can be viewed as an extension of the Poisson factor model. The Poisson factor model can be estimated by a constrained joint maximum likelihood estimator (Chen et al., 2020), which is consistent and minimax rate optimal under suitable regularity conditions. Our model is also closely related to matrix factor models (Wang et al., 2019; He et al., 2023) in that our data at each time can be viewed as a matrix. A major difference is that our model considers a continuous time domain, with observed data being very sparse at each time. In contrast, the matrix factor models assume a discrete time domain.\n\nRemark 4 (Indeterminacy of $\\boldsymbol{\\Theta}(\\cdot)$ and $\\mathbf{A}$ and a rotated solution). Note that $\\boldsymbol{\\Theta}(\\cdot)$ and A are not determined, in the sense that for any $r \\times r$ invertible matrix $\\mathbf{Q}$, the model remains unchanged if we replace the factors by $\\boldsymbol{\\Theta}(t)\\left(\\mathbf{Q}^{\\top}\\right)^{-1}$ and replace the loadings by\n\nAQ. Similar indeterminacies also occur in other factor models (see, e.g., Bai and Li, 2012). To interpret the factor structure, one must fix the transformation $\\mathbf{Q}$, which may be done using an analytic rotation method (Browne, 2001; Rohe and Zeng, 2023). However, the current setting is slightly different from standard exploratory factor analysis settings, as factors here are functions of time $t$. To apply existing analytic rotation methods, we may first aggregate the factors by calculating $\\overline{\\boldsymbol{\\Theta}}=\\int_{0}^{1} \\boldsymbol{\\Theta}(t) d t$, and then apply an analytic rotation method to $\\overline{\\boldsymbol{\\Theta}} \\mathbf{A}^{\\top}$. In real data analysis in Section 5, a varimax rotation method (Kaiser, 1958; Rohe and Zeng, 2023) is applied to fix the transformation.\n\nRemark 5 (Time-varying loadings). The flexibility of the model can be further enhanced by letting the loading parameters be time-varying, i.e., $X_{i j}(t)=\\sum_{k=1}^{r} a_{j k}(t) \\theta_{i k}(t)$. However, this model is far less determined than the current model as $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t)(\\mathbf{A}(t))^{\\top}=$ $\\boldsymbol{\\Theta}(t)\\left(\\mathbf{Q}(t)^{\\top}\\right)^{-1}(\\mathbf{A}(t) \\mathbf{Q}(t))^{\\top}$ for any $r \\times r$ invertible matrix-valued function $\\mathbf{Q}(t)$. Determining this transformation function $\\mathbf{Q}(t)$ is more challenging than determining the timeindependent transformation discussed in Remark 4. Consequently, it is hard to identify and interpret the factor structure. In our grocery shopping application, each event type corresponds to a merchandise item, and each observation corresponds to a customer. In this context, the loading parameters can be viewed as a summary of item characteristics, and the factors can be interpreted as a summary of customer preferences. Because item characteristics tend to be stable while customer preferences often vary over time, treating the loading parameters as static and the factors as dynamic is sensible. Thus, the current paper focuses on the static loading and dynamic factor setting.", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 2.2 Estimation \n\nWe introduce a kernel-based approach for estimating the unknown parameters. Kernel smoothing borrows information from nearby time points because the observed events are very sparse at each single time point in the continuous time domain. Let $K(x)$ be a kernel function with sufficient smoothness, satisfying $K(x) \\geq 0, K(-x)=K(x)$ and $\\int_{-\\infty}^{\\infty} K(x) \\mathrm{d} x=1$. For a smoothing bandwidth $h>0$, we further define $K_{h}(x)=$ $(1 / h) K(x / h)$. We consider the following kernel-smoothed pseudo-likelihood function:\n\n$$\n\\mathcal{L}_{h}(\\boldsymbol{\\Theta}, \\mathbf{A})=\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log f\\left(X_{i j}(t)\\right)-f\\left(X_{i j}(t)\\right)\\right) \\mathrm{d} t\n$$\n\nwhere $X_{i j}(t)$ is a function of $\\boldsymbol{\\Theta}$ and $\\mathbf{A}$ as defined in (2). We consider the parameter space $\\mathcal{G}=\\left\\{(\\boldsymbol{\\Theta}, \\mathbf{A}): \\sup _{t \\in[0,1]}\\|\\boldsymbol{\\Theta}(t)\\|_{2 \\rightarrow \\infty} \\leq M^{1 / 2},\\|\\mathbf{A}\\|_{2 \\rightarrow \\infty} \\leq M^{1 / 2}\\right\\}$, where $M>0$ is a prespecified constant. We define $(\\widehat{\\boldsymbol{\\Theta}}, \\widehat{\\mathbf{A}})$ as a constrained maximizer of (3),\n\n$$\n(\\widehat{\\boldsymbol{\\Theta}}, \\widehat{\\mathbf{A}}) \\in \\arg \\max \\mathcal{L}_{h}(\\boldsymbol{\\Theta}, \\mathbf{A}), \\text { s.t. }(\\boldsymbol{\\Theta}, \\mathbf{A}) \\in \\mathcal{G}\n$$\n\nSince the parameter space $\\mathcal{G}$ is compact and $\\mathcal{L}_{h}(\\boldsymbol{\\Theta}, \\mathbf{A})$ is continuous with respect to the norm $\\|(\\boldsymbol{\\Theta}, \\mathbf{A})\\|:=\\sup _{t \\in[0,1]}\\|\\boldsymbol{\\Theta}(t)\\|_{2 \\rightarrow \\infty} \\vee\\|\\mathbf{A}\\|_{2 \\rightarrow \\infty}$, the existence of at least one solution is guaranteed. Therefore, $(\\widehat{\\boldsymbol{\\Theta}}, \\widehat{\\mathbf{A}})$ is well-defined.\n\nRemark 6. The pseudo-likelihood (3) ignores the possible dependence between event types that is allowed under the mean rate specification (1). If (1) is replaced by an intensity specification, i.e., $E\\left(\\mathrm{~d} Y_{i j}(t) \\mid \\mathcal{F}_{t}\\right)=f\\left(X_{i j}(t)\\right) \\mathrm{d} t$, and let $h$ go to 0 , then (3) becomes the log-likelihood function for recurrent event time data (Cook and Lawless, 2007).\n\nRemark 7. In practice, we can only obtain an approximate solution to (4), as the optimization involves infinite-dimensional functions. When the resolution of the approximation\n\nis carefully chosen, this approximate solution can achieve the same error rate as that of (4). More specifically, the approximate solution is obtained by a two-step procedure. In the first step, we discretize the interval $[h, 1-h]$ by equally spaced grid points $t_{1}, \\ldots, t_{q}$ and solve\n\n$$\n\\begin{aligned}\n\\left(\\widetilde{\\boldsymbol{\\Theta}}\\left(t_{1}\\right), \\ldots, \\widetilde{\\boldsymbol{\\Theta}}\\left(t_{q}\\right), \\widetilde{\\mathbf{A}}\\right) \\in \\arg \\max & \\mathcal{L}_{h}\\left(\\boldsymbol{\\Theta}\\left(t_{1}\\right), \\ldots, \\boldsymbol{\\Theta}\\left(t_{q}\\right), \\mathbf{A}\\right) \\\\\n& \\text { s.t. }\\left\\|\\boldsymbol{\\Theta}\\left(t_{l}\\right)\\right\\|_{2 \\rightarrow \\infty} \\leq M,\\|\\mathbf{A}\\|_{2 \\rightarrow \\infty} \\leq M, l=1, \\ldots, q\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\mathcal{L}_{h}\\left(\\boldsymbol{\\Theta}\\left(t_{1}\\right), \\ldots, \\boldsymbol{\\Theta}\\left(t_{q}\\right), \\mathbf{A}\\right)=\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\sum_{l=1}^{q}\\left(\\frac{\\int_{0}^{1} K_{h}\\left(t_{l}-s\\right) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}\\left(t_{l}-s\\right) \\mathrm{d} s} \\log f\\left(X_{i j}\\left(t_{l}\\right)\\right)-f\\left(X_{i j}\\left(t_{l}\\right)\\right)\\right)\n$$\n\nis the pseudo-likelihood defined on the grid points. In the second step, based on $\\widetilde{\\boldsymbol{\\Theta}}$ we find an approximation to $\\widetilde{\\boldsymbol{\\Theta}}$ on $[h, 1-h]$ by interpolation, such as a linear interpolation. By choosing the number of grid points to be inversely proportional to the error rate of (4), the approximate solution is guaranteed to achieve the same error rate. An efficient projected gradient descent algorithm is developed to obtain the approximate solution. This algorithm handles the constraints based on the two-to-infinity norms with an easy-to-compute projection operator. The details of the algorithm and the properties of its convergence can be found in the Appendix.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 2.3 Determining the Number of Factors \n\nIn practice, the number of factors $r$ is unknown and, thus, needs to be chosen. We propose an information criterion to choose $r$. To avoid ambiguity, we use $\\left(\\widetilde{\\boldsymbol{\\Theta}}^{(r)}, \\widetilde{\\mathbf{A}}^{(r)}\\right)$ to denote the estimator (4) to emphasize its dependence on the number of factors. The proposed infor-\n\nmation criterion takes the form $\\operatorname{IC}(r)=-2 \\mathcal{L}_{h}\\left(\\widehat{\\boldsymbol{\\Theta}}^{(r)}, \\widehat{\\mathbf{A}}^{(r)}\\right)+v(N, J, r)$, where $v(N, J, r)$ is a penalty term that increases with $N, J$ and $r$. The conditions on $v(N, J, r)$ for consistent model selection will be determined in Section 3.2. Given $v(N, J, r)$, we choose the number of factors by $\\widehat{r}=\\operatorname{argmin}_{r \\in \\mathcal{R}} \\operatorname{IC}(r)$, where $\\mathcal{R} \\subset \\mathbb{N}$ is a candidate set for the number of factors. As shown in Section 3, under suitable conditions on the penalty term and additional regularity conditions, $\\widehat{r}$ consistently selects the number of factors. In our implementation, the pseudo-likelihood $\\mathcal{L}_{h}\\left(\\widehat{\\boldsymbol{\\Theta}}^{(r)}, \\widehat{\\mathbf{A}}^{(r)}\\right)$ in $\\operatorname{IC}(r)$ is replaced by its discretized version as discussed in Remark 7.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 3 Theoretical properties\n### 3.1 Consistency and Rate of Convergence\n\nWe present our main theoretical results about the estimator proposed in Section 2.2. When deriving these results, the number of factors is assumed to be correctly specified. To avoid ambiguity of notation, we let $\\boldsymbol{\\Theta}^{*}(\\cdot)$ and $\\mathbf{A}^{*}$ denote the true parameters, and further let $\\mathbf{X}^{*}(t)=\\boldsymbol{\\Theta}^{*}(t)\\left(\\mathbf{A}^{*}\\right)^{\\top}$. To avoid the complications brought by the indeterminacy of $\\boldsymbol{\\Theta}(\\cdot)$ and $\\mathbf{A}$, we focus on evaluating the estimation accuracy of $\\widehat{\\mathbf{X}}(t):=\\widehat{\\boldsymbol{\\Theta}}(t) \\widehat{\\mathbf{A}}^{\\top}$. Let $m \\geq 1$ be a positive integer. We assume the following regularity conditions.\n\nCondition 1. The link function $f$ is $m$ times continuously differentiable. Moreover, for $x \\in[-M, M], f(x)$ and $f^{\\prime}(x)$ are bounded away from 0.\n\nCondition 2. The matrix function $\\mathbf{X}^{*}(\\cdot) \\in \\mathcal{G}$ is $m$ times continuously differentiable on $[0,1]$.\n\nCondition 3. The kernel function $K$ satisfies: (i) it is a Lipschitz function of order $m$ with compact support on $[-1,1]$; (ii) it attains its unique maximum at $x=0$; (iii) it is\n\ntwice continuously differentiable in a neighbourhood of 0 and $(\\log K)^{\\prime \\prime}(0)<0$.\n\nCondition 4. (i) The multivariate point processes $\\left\\{Y_{1 j}(\\cdot): j \\in[J]\\right\\}, \\ldots,\\left\\{Y_{N j}(\\cdot): j \\in[J]\\right\\}$ are independent. (ii) There exists $\\lambda>0$, such that for any $i, j, k$, and $0<s_{1}<\\ldots<$ $s_{k}<1, \\mathbb{E}\\left[\\mathrm{~d} Y_{i j}\\left(s_{1}\\right) \\cdots \\mathrm{d} Y_{i j}\\left(s_{k}\\right)\\right] \\leq \\lambda^{k} \\mathrm{~d} s_{1} \\cdots \\mathrm{~d} s_{k}$. (iii) For any $i$, there exists a partition $B_{i, 1}, \\ldots, B_{i, W_{i}}$ of $\\{1, \\ldots, J\\}$ and a function $\\phi(J)=o(J)$ satisfying $\\max _{i=1, \\ldots, N} \\max _{k=1, \\ldots, W_{i}}\\left|B_{i, k}\\right| \\leq$ $\\phi(J)$, such that $\\left\\{Y_{i j}(\\cdot): j \\in B_{i, 1}\\right\\}, \\ldots,\\left\\{Y_{i j}(\\cdot): j \\in B_{i, W_{i}}\\right\\}$ are independent.\n\nRemark 8. In Condition 1, we assume that both $f(x)$ and $f^{\\prime}(x)$ are non-zero in $[-M, M]$. This requirement is rather mild. In particular, this requirement is automatically satisfied when $f(x)$ is strictly positive and monotone increasing (or decreasing), including when $f(x)=\\exp (x)$.\n\nRemark 9. Condition 2 requires the true model to lie in the same parameter space $\\mathcal{G}$ as the one used to regularize our estimator (4). This requirement, together with Condition 1, implies that the mean rate function $f\\left(X_{i j}(t)\\right)$ is nonnegative and uniformly bounded away from zero for all $i, j$, and $t$. In the context of our grocery shopping application, it means that the proposed method is most suitable for analyzing customers who shop frequently and items that are frequently purchased. The size of this parameter space plays a key role in our theory about model estimation and selection. Condition 2 also imposes a smoothness requirement that is standard in nonparametric regression models (Gy\u00f6rfi et al., 2002).\n\nRemark 10. Conditions 3(i) and 3(ii) are standard assumptions for kernel functions. Condition 3(iii) assumes log-concavity at the maximum point.\n\nRemark 11. Condition 4(i) assumes that the observed data $\\mathbf{Y}_{i}(t)=\\left(Y_{i 1}(t), \\ldots, Y_{i J}(t)\\right)^{\\top}$ are independent across observational units $i=1, \\ldots, N$. Condition 4(ii) assumes nondegeneracy of the counting process. Condition 4(iii) assumes a blockwise independent structure, which substantially relaxes the independence assumption among different event types.\n\nNote that we restrict the maximum block size rather than assuming the blockwise independent structure to be the same across observations $i=1, \\ldots, N$. Condition 4(iii) can be further relaxed. Instead of requiring the processes in all the blocks to be independent, our theoretical results in Theorems 1 and 3 are still valid if only $\\left\\{Y_{i j}(\\cdot): j \\in B_{i, 1}\\right\\}, \\ldots,\\left\\{Y_{i j}(\\cdot)\\right.$ : $\\left.j \\in B_{i, W_{i}-1}\\right\\}$ are independent. This relaxed condition allows the processes in $B_{i, W_{i}}$ to be dependent on all the rest of the processes. We note that Condition 4(iii) can be seen as an identifiability condition for the low-rank structure $\\mathbf{X}(t)$ to be identifiable, as it excludes the noise in the data to have a low-rank structure through the independence or blockwise independence assumption. The assumption of blockwise independence is similar in spirit to the weakly dependent error assumption adopted in approximate factor models (e.g., Chamberlain and Rothschild, 1983; Bai and Ng, 2023) and may be seen as a version of the weakly dependent error assumption for factor models of high-dimensional recurrent event data.\n\nTheorem 1 (upper bound). Under Conditions 1-4:\n(i) (Dependent case) Assume $J=O(N)$ and recall $\\phi(J)$ from Condition 4(iii). For any $\\delta>0$, choose $h \\times(J / \\phi(J))^{-1 /(2 m+1)+\\delta / m}$. Then, as $N$ and $J$ go to infinity, we have\n\n$$\n\\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t=O_{p}\\left((J / \\phi(J))^{-m /(2 m+1)+\\delta}\\right)\n$$\n\n(ii) (Independent case) Assume that $\\phi(J)=1$ in Condition 4(iii) and $\\log (N \\vee J) \\ll N \\wedge J$. For any $\\delta>0$, choose $h \\asymp\\left((N \\wedge J) /\\left(\\log ^{2}(N \\wedge J)\\right)\\right)^{-1 /(2 m+1)}$. Then, as $N$ and $J$ go to infinity, we have\n\n$$\n\\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t=O_{p}\\left((N \\wedge J)^{-2 m /(2 m+1)+\\delta}\\right)\n$$\n\nTo show the near optimality of the proposed estimator, we then derive the minimax\n\nlower bound under the independent Poisson process setting in the following theorem.\n\nTheorem 2 (lower bound). Assume that the $Y_{i j}$ 's are independent Poisson point processes and $\\phi(J)=1$ in Condition 4(iii). Further assume that $\\sup _{|x| \\leq M} f^{\\prime}(x)^{2} / f(x)<\\infty$, then there is an absolute constant $C>0$, so that for any estimator $\\widehat{\\mathbf{X}}(t) \\in L_{N \\times J}^{2}[0,1]$, there exists an $\\mathbf{X}^{*}(t)$ satisfying Condition 2 such that\n\n$$\n\\operatorname{pr}\\left(\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\geq C(N \\wedge J)^{-2 m /(2 m+1)}\\right) \\geq \\frac{1}{2}\n$$\n\nHence, this information-theoretic lower bound matches the upper bound in Theorem 1 only up to an arbitrarily small exponent under the independence assumption, which implies the near minimax optimality of our estimator.\n\nRemark 12. When the blockwise independent structure is the same across observations (i.e., $W_{i}=W$, and $B_{i, w}=B_{w}, i=1, \\ldots, N, w=1, \\ldots, W$ ), we can sharpen the rate in Theorem 1(i) from $-m /(2 m+1)+\\delta$ to $-2 m /(2 m+1)+\\delta$ and establish its near minimax optimality.\n\nRemark 13. Due to the rotational indeterminacy mentioned in Remark 4, the estimated loading matrix $\\widehat{\\mathbf{A}}$ is not guaranteed to converge to the true loading matri $\\mathbf{A}^{*}$. However, it can be shown that the maximum principal angle between the column spaces of $\\widehat{\\mathbf{A}}$ and $\\mathbf{A}^{*}$ converges to zero in probability under the same conditions as Theorem 1 and an additional regularity condition on the singular values of $\\mathbf{X}^{*}(t)$. See the Appendix for more details.", "tables": {}, "images": {}}, {"section_id": 6, "text": "# 3.2 Model Selection Consistency \n\nAs introduced in Section 2.3, $v(N, J, r)$ is the penalty function in the information criterion. As $v(N, J, r)$ is required to be increasing in $r$, we denote $u(N, J, r)=v(N, J, r)-v(N, J, r-$\n\n1) $>0$. Further, for any $t \\in[0,1]$, let $\\sigma_{1, t} \\geq \\sigma_{2, t} \\geq \\ldots \\geq \\sigma_{r^{*}, t}$ be the non-zero singular values of $\\mathbf{X}^{*}(t)$. Theorem 3 provides sufficient conditions on $u(N, J, r)$ for consistent model selection.\n\nTheorem 3 (model selection consistency). Assume that the candidate set $\\mathcal{R}$ has a finite number of elements and $r^{*} \\in \\mathcal{R}$. Under Conditions 1-4:\n(i) (Dependent case) Assume $J=O(N)$ and function $u$ satisfies that $u(N, J, r)=$ $o\\left(\\int_{h}^{1-h} \\sigma_{r^{*}, t}^{2} \\mathrm{~d} t\\right)$ and $N J(J / \\phi(J))^{-m /(2 m+1)+\\delta}=o(u(N, J, r))$ for any sufficiently small $\\delta>0$ and any $r \\in \\mathcal{R}$ as $N$ and $J$ go to infinity. Choose $h \\asymp(J / \\phi(J))^{-1 /(2 m+1)+\\delta / m}$. Then $\\lim _{N, J \\rightarrow \\infty} \\operatorname{pr}\\left(\\widehat{r}=r^{*}\\right)=1$.\n(ii) (Independent case) Assume that $\\phi(J)=1$ in Condition 4(iii), $\\log (N \\vee J) \\ll N \\wedge J$ and the function $u$ satisfies that $u(N, J, r)=o\\left(\\int_{h}^{1-h} \\sigma_{r^{*}, t}^{2} \\mathrm{~d} t\\right)$ and $N J(N \\wedge J)^{-2 m /(2 m+1)+\\delta}=$ $o(u(N, J, r))$ for any sufficiently small $\\delta>0$ and any $r \\in \\mathcal{R}$ as $N$ and $J$ go to infinity. Choose $h \\asymp\\left((N \\wedge J) /\\left(\\log ^{2}(N \\wedge J)\\right)\\right)^{-1 /(2 m+1)}$. Then $\\lim _{N, J \\rightarrow \\infty} \\operatorname{pr}\\left(\\widehat{r}=r^{*}\\right)=1$.\n\nRemark 14. The two conditions on $u(N, J, r)$ in both the dependent and independent cases are needed to ensure the existence of a suitable penalty that guards against both overand under-selections of the number of factors. For such a u function to exist, $\\int_{h}^{1-h} \\sigma_{r^{*}, t}^{2} \\mathrm{~d} t$ cannot be too small. The first condition $u(N, J, r)=o\\left(\\int_{h}^{1-h} \\sigma_{r^{*}, t}^{2} \\mathrm{~d} t\\right)$ requires that $u(N, J, r)$ is smaller than the integral of the gap between non-zero singular values and zero singular values of $\\mathbf{X}^{*}(\\cdot)$. It ensures the probability of under-selecting the number of factors to be small. The second condition requires that $u(N, J, r)$ grows faster than the upper bound of estimation error, which guarantees that, with high probability, we do not over-select the number of factors.\n\nRemark 15. The results in Theorems 1 and 3 can be extended if it is of interest to use a\n\nkernel function supported on the whole real line, for example, the Gaussian kernel. In such cases, Condition 3 needs to be modified. The details are given in the Appendix.\n\nRemark 16. The results in Theorems 1 and 3 can also be extended to a missing data setting under an ignorable missingness assumption. Let $\\omega_{i j}$ be a binary random variable, indicating the missingness of $\\left\\{Y_{i j}(t): t \\in[0,1]\\right\\}$, where $\\omega_{i j}=1$ means that $\\left\\{Y_{i j}(t): t \\in[0,1]\\right\\}$ is observed and $\\omega_{i j}=0$ if $\\left\\{Y_{i j}(t): t \\in[0,1]\\right\\}$ is missing. We can still establish corresponding results in Theorems 1 and 3 under suitable conditions based on a pseudo-likelihood function that replaces the summations over all $i$ and $j$ in (3) by summations over $i$ and $j$ such that $\\omega_{i j}=1$.", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 4 Simulation Study \n\nWe evaluate the proposed estimator and information criterion with a simulation study. In this study, we generate data from the proposed model, where the number of factors is set to $r^{*}=3$, and the numbers of observation units and event types satisfy $N=2 J$. We consider three patterns regarding the dynamic component $\\boldsymbol{\\Theta}^{*}(t)$, denoted by $\\mathrm{C} 1, \\mathrm{C} 2$, and C3, in which $\\boldsymbol{\\Theta}^{*}(t)$ is constant, changes linearly, and changes periodically, respectively. We further consider two different settings for generating $\\mathbf{A}^{*}$, denoted by S 1 and S 2 , resulting in two different signal-to-noise levels, where Setting S1 has a stronger signal than Setting S2. We vary the number of event types $J$, by setting $J=100,200,400$, and 800 . Finally, we consider data generation under the dependent and independent settings in Theorem 1. The factors discussed above lead to a total of 24 simulation settings. For each setting, 50 independent replications are generated. The proposed method is compared with the Poisson factor model discussed in Remark 3 that ignores the dynamic nature of the process and only concerns the total event counts on the entire time interval. Following a similar\n\nproof as that for Theorem 1, the likelihood-based estimator under the Poisson factor model is consistent even under the dependent-event-type settings when $\\boldsymbol{\\Theta}^{*}(t)$ is constant. As the Poisson factor model involves less parameters, it is expected to be statistically more efficient than the proposed estimator under the settings when $\\boldsymbol{\\Theta}^{*}(t)$ is constant. In the other settings, the Poisson factor model has biases as it ignores the dynamic nature of the event data.\n\nWe now elaborate on the data generation and results under some settings with dependent event types. Further details about the simulation are given in the Appendix. More simulations are performed in the Appendix under additional settings, including those with independent event types, more event types than observation units, and modified specifications for $\\boldsymbol{\\Theta}^{*}(t)$ and $\\mathbf{A}^{*}$ that lead to even weaker signals. While the results vary under different settings, their patterns are consistent with the results of the current simulations below.\n\nWe set $\\phi(J)=J^{1 / 3}$ and generate data $\\left\\{Y_{i j}(t): t \\in[0,1]\\right\\}$ as follows. First, we divide event types $j=1, \\ldots, J$ into $\\lfloor J / \\phi(J)\\rfloor$ blocks of approximately equal sizes, $B_{1}, \\ldots, B_{\\lfloor J / \\phi(J)\\rfloor}$, where $\\lfloor J / \\phi(J)\\rfloor$ denotes the greatest integer less than or equal to $J / \\phi(J)$. Second, for the $k$ th block, we generate a Poisson process with intensity function $f_{k}(t):=\\max _{j \\in B_{k}} f\\left(X_{i j}^{*}(t)\\right)=$ $f\\left(\\max _{j \\in B_{k}} X_{i j}^{*}(t)\\right)$, and denote the generated event times as $0<t_{k, 1}<\\ldots<t_{k, p_{k}}<1$. Finally, using a thinning algorithm (Chen, 2016), for each $i=1, \\ldots, N$ and each $j \\in B_{k}$, we accept $t_{k, 1}, \\ldots, t_{k, p_{k}}$ with probabilities $f\\left(X_{i j}^{*}\\left(t_{k, 1}\\right)\\right) / f_{k}\\left(t_{k, 1}\\right), \\ldots, f\\left(X_{i j}^{*}\\left(t_{k, p_{k}}\\right)\\right) / f_{k}\\left(t_{k, p_{k}}\\right)$ independently and let the accepted time points to be the event times of $Y_{i j}(t)$. The resulting processes are guaranteed to follow the proposed model. We choose the Epanechnikov kernel function $K(x)=0.75\\left(1-x^{2}\\right),-1 \\leq x \\leq 1$, with kernel order $m=2$. It is easy to verify that the chosen kernel function satisfies Condition 3. The link function is $f(x)=\\exp (x)$.\n\nWe set $h=0.1(J / \\phi(J))^{-0.19}$ and $M=36$. Our estimation is based on a discretized likelihood with 31 evenly distributed time points $t_{1}, \\ldots, t_{31}$ on $[h, 1-h]$. A sensitivity analysis is performed in the Appendix regarding the number of grid points, which suggests that the choice of 31 is sufficient for our simulation settings. After the estimation, we obtain $\\widehat{\\mathbf{X}}(t)$ for $t \\in[h, 1-h]$ by a linear interpolation. The estimation error is evaluated by $\\int_{h}^{1-h}\\left\\|\\mathbf{X}^{*}(t)-\\widehat{\\mathbf{X}}(t)\\right\\|_{F}^{2} \\mathrm{~d} t / N J$. Under the Poisson factor model, we obtain $\\widehat{\\mathbf{A}}$ and $\\widehat{\\boldsymbol{\\Theta}}$. We compute $\\int_{h}^{1-h}\\left\\|\\mathbf{X}^{*}(t)-\\widehat{\\mathbf{X}}(t)\\right\\|_{F}^{2} \\mathrm{~d} t / N J$ as its estimation error, where $\\widehat{\\mathbf{X}}(t)=\\widehat{\\boldsymbol{\\Theta}} \\widehat{\\mathbf{A}}^{\\top}$ is constant over time.\n\nThe results regarding the estimation errors are given in Table 1. They show that, for each combination of $S_{i}$ and $C_{j}, i=1,2, j=1,2,3$, the estimation error of the proposed method decays as $N$ and $J$ grow. Under settings in which $\\boldsymbol{\\Theta}^{*}(t)$ is constant (i.e., C 1 ), the estimator given by the Poisson factor model has smaller errors than the proposed estimator. In the rest of the settings, the proposed estimator yields substantially smaller estimation errors than those under the Poisson factor model. In the Appendix, the two models are also compared in recovering the loading matrix $\\mathbf{A}^{*}$. Due to the rotational indeterminacy mentioned in Remark 4, we measure the accuracy by the principal angles between the subspace spanned by the column vectors of $\\mathbf{A}^{*}$ and that spanned by those of $\\widehat{\\mathbf{A}}$. The results show that the proposed method provides substantially more accurate estimates of $\\mathbf{A}^{*}$ under settings when the Poisson factor model is misspecified and similar but slightly less accurate estimates when the Poisson factor model is correctly specified.\n\nFinally, we evaluate the accuracy regarding selecting the number of factors. We set the penalty term to be $v(N, J, r)=40 r N J h^{1.99}$ and select $r$ from the candidate set $\\{1,2,3,4,5\\}$. According to our simulation results, the number of factors is always correctly selected under all the simulation settings, except for three settings where $J=100$ and the signal-to-noise\n\n![table_0](table_0)\n\nTable 1: Mean estimation error among 50 independent replications based on the proposed estimator and the estimator under the Poisson factor model under 24 simulation settings.\nlevel follows S2. For these three cases (C1-C3), 13, 29, and 10 out of 50 replications mis-select the number of factors. Overall, the proposed information criterion has effective performance.", "tables": {"table_0": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Kernel-based method | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.1006 | 0.1174 | 0.1048 | 0.1371 | 0.1606 | 0.1407 |\n| $J=200$ | 0.0536 | 0.0630 | 0.0562 | 0.0692 | 0.0806 | 0.0727 |\n| $J=400$ | 0.0291 | 0.0350 | 0.0308 | 0.0378 | 0.0437 | 0.0398 |\n| $J=800$ | 0.0159 | 0.0190 | 0.0170 | 0.0205 | 0.0240 | 0.0217 |\n| Poisson factor model | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0154 | 0.9743 | 0.7518 | 0.0192 | 0.6928 | 0.5530 |\n| $J=200$ | 0.0073 | 0.9785 | 0.7611 | 0.0091 | 0.6830 | 0.5458 |\n| $J=400$ | 0.0036 | 0.9773 | 0.7442 | 0.0046 | 0.6911 | 0.5442 |\n| $J=800$ | 0.0018 | 1.0012 | 0.7542 | 0.0023 | 0.6955 | 0.5491 |"}, "images": {}}, {"section_id": 8, "text": "# 5 Application to Grocery Shopping Data\n### 5.1 Background, Data Processing, and Analysis\n\nWe apply the proposed method to a grocery shopping dataset available at https://www. dunnhumby.com/source-files. It contains transaction records collected by a retailer in two years about its frequent shoppers. We discard the first $15 \\%$ of the observation period since the number of total transactions is significantly lower than the rest, likely due to late entries. The remaining period is then standardized to the interval $[0,1]$. After preprocessing, we obtain a dataset with $N=1,978$ shoppers and $J=2,000$ products. The dataset contains information on each product regarding its type (e.g., cheese, chips). It also contains the demographic information of 796 shoppers, including age, income, and child (having children or not). Such information is not used in the proposed model but\n\n![table_1](table_1)\n\nTable 2: Products with large positive factor loadings for each of the three factors.\nis used for validating and interpreting our results. Here, the matrix-valued function $\\boldsymbol{\\Theta}(\\cdot)$ may be interpreted as the dynamic customer factors, and the matrix $\\mathbf{A}$ may be interpreted as the attributes of the products. We apply the proposed information criterion with the candidate set $\\{1,2,3,4,5\\}$, which selects $r=3$ factors. Following the discussion in Remark 4 , we apply a varimax rotation for the selected three-factor model to obtain an interpretable factor structure.", "tables": {"table_1": "| Factor 1 | yogurt(10), salad(3), herbs(parsley, cilantro)(3), organic fruit/vegetable(3), blueberry(3), mushroom(2), tropical fruit(mango, pineapple)(2), beans(2), pepper(2), cheese(2) |\n| :--: | :--: |\n| Factor 2 | soft drink(11), cold cereal(5), hot sauce(5), refrigerated drink(4), chicken wings(4), frozen meat(3), dinner sausage(3), candy(3), frozen pizza(2), cigarette(2), potato chips(2), canned pasta(2) |\n| Factor 3 | cheese(7), milk(5), white bread(4), fruit(banana, grape, strawberry)(4), egg(4), vegetable(cucumber, celery, cabbage, corn)(4), onion(4), salad(3), soft drink(2), hamburger bun(2), beef(2), tomato(2), potato(2) |"}, "images": {}}, {"section_id": 9, "text": "# 5.2 Interpreting Factors \n\nWe interpret the factors based on the estimated loading matrix after rotation. Specifically, let $\\widetilde{\\mathbf{A}}=\\left(\\widetilde{a}_{i j}\\right)_{J \\times r}$ be the loading matrix after rotation. We say a product $j$ dominantly loads on factor $k$ if $\\widetilde{a}_{j k}^{2} /\\left(\\sum_{l=1}^{r} \\widetilde{a}_{j l}^{2}\\right)$ is large, i.e., $\\widetilde{a}_{j k}$ is dominantly larger than the rest of the loadings in magnitude. We investigate the top 60 products that dominantly load on each factor. Table 2 lists the types of these products. We note that many products with dominant loadings on the same factor tend to be of a small number of types. These types are presented only once in the table, followed by the corresponding number of products of this type in parentheses. Product types that only appear once for each factor are omitted for brevity.\n\nTable 2 shows that the items with dominant loadings on the first factor are mostly fresh\n\nand healthy food products suitable for vegetarian dietary preferences. Items with dominant loadings on the second factor contain unhealthy (e.g., soft drinks, candy, potato chips), fast food (e.g., cold cereal, frozen pizza), or budget-friendly products (e.g., frozen meat). Finally, items that load dominantly on the third factor are mostly basic food products of daily need, including bread, eggs, milk, and beef. While these products include many fresh and healthy food products similar to those loading on the first factor, they tend to be more budget-friendly. Given these features, we may interpret the three factors as \"healthy food consumption\", \"unhealthy food consumption\", and \"basic food consumption\" factors, respectively.\n\nWe further investigate the three factors by regressing them on the three demographic variables: age, income, and child. Here, age is an ordinal variable referring to the estimated age range of the shopper. For simplicity, we transform it into a binary variable, which takes value 1 if the age is above 55 and 0 otherwise. The variable income is an ordinal variable recording the household income level. We simplify it to be a variable with three categories, - under $\\$ 35,000$ (income1 $=0$, income2 $=0$ ), $\\$ 35,000$ - $\\$ 75,000$ (income1 $=1$, income2 $=0$ ), and above $\\$ 75,000$ (income1 $=0$, income $2=1$ ). Finally, the variable child is a binary variable indicating whether the shopper's household has children. We run a linear regression model for each factor by regressing the factor scores on age, income1, income2, child, and the interactions between child and income1 and income2. The interaction terms are added because it is suspected that the child effect differs between high- and low-income households.\n\nThe results from these regression models are given in Table 3, where the statistically significant coefficients and their p-values are presented, and the R-squared values of the three models are given. As the coefficients for the interaction between the dummy variable\n\nincome1 and child are insignificant in all three models, the corresponding row is not presented. All the terms are statistically significant for the first factor, except for age and the interaction between income1 and child. In particular, the coefficients associated with the summary variables for income are all positive, meaning that the consumption of healthy food increases with the household income, controlling for the rest of the variables. In addition, the coefficient for child is negative, and the coefficient for the interaction between income2 and child is positive and larger in absolute value than that of the coefficient for child. It means households with relatively lower income (less than $\\$ 75,000$ ) tend to shop less healthy food when they have children, while those with higher income (above $\\$ 75,000$ ) tend to shop more healthy food when they have children.\n\nAll the coefficients are significant for the second factor, except for those associated with the two interaction terms. The coefficient for age is negative, suggesting that the older group tends to consume less unhealthy food than the younger ones, controlling for the rest of the variables. The coefficients for income are also negative, suggesting that households with a higher income tend to consume less unhealthy food when controlling for the rest of the variables. On the other hand, the coefficient for child is positive, meaning that households with children tend to consume more unhealthy food. This may be because this food category contains most soft drinks and snacks like candy and potato chips that children often favour.\n\nRegarding the third factor, only the coefficient for age is statistically significant, and the R -squared value is quite low. The positive coefficient means that older people consume more basic food products. Combining the results for the second factor, we believe this may be because older people tend to have a healthier lifestyle. Although they do not consume more healthy food associated with the first factor, they cook more frequently using basic\n\n![table_2](table_2)\n\nTable 3: The coefficients when regressing the factors on demographic variables.\nfood products and eat less unhealthy food than the younger ones.", "tables": {"table_2": "|  | Factor $1\\left(R^{2}=0.13\\right)$ | Factor $2\\left(R^{2}=0.17\\right)$ | Factor $3\\left(R^{2}=0.02\\right)$ |\n| :--: | :--: | :--: | :--: |\n| age |  | $-0.007(p=0.00)$ | $0.006(p=0.00)$ |\n| income1 | $0.006(p=0.00)$ | $-0.005(p=0.02)$ |  |\n| income2 | $0.015(p=0.00)$ | $-0.021(p=0.00)$ |  |\n| child | $-0.007(p=0.01)$ | $0.010(p=0.00)$ |  |\n| income2 $\\times$ child | $0.009(p=0.01)$ |  |  |"}, "images": {}}, {"section_id": 10, "text": "# 5.3 Investigating Purchase Dynamics \n\nWe further investigate the dynamic trend the model captures. In particular, for each pair of consumer $i$ and product $j$, we measure the variability in the personal purchasing rate by the total variation of $X_{i j}(\\cdot)$, i.e., $\\int_{0}^{1}\\left|X_{i j}^{\\prime}(t)\\right| \\mathrm{d} t$, where $X_{i j}^{\\prime}(t)$ denotes the derivative of $X_{i j}(t)$. A larger total variation implies a higher variability. Under the estimated threefactor model, we estimate this variability based on the finite differences between $\\widehat{X}_{i j}(t)$ for time $t$ at adjacent grid points.\n\nThe variability measure is computed for 1,019 products of 18 product types that are most frequently purchased. For each product type, we look at the empirical distribution of the estimated total variations based on all the shoppers and all the products of this type and compute its $25 \\%, 50 \\%$ (i.e., median), and $75 \\%$ quartiles. The results are given in Figure 1 , where the 18 product types are organized in descending order for each quartile. The ranking of the product types is reasonably stable across the three quartiles and consistent with our understanding of their sales pattern. We remark that dimension reduction is important for the proposed method to produce the current results. One cannot obtain sensible results by averaging the sales of the products over shoppers due to the high noise level in the data.\n\nVegetables, tropical fruits, yogurt, and soft drinks are product types with consistently\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Quartiles of the variability of the most frequently purchased product types.\nhigh variability scores across all three quartiles. The price and quality of many vegetables and fruits depend on their growing seasons. In addition, tropical fruits are exported products whose price and supply depend on additional factors that fluctuate over time, such as transportation costs. Due to the previously mentioned factors, these products show higher variability in their sales. On the other hand, the higher variability of yogurt and soft drinks may be due to seasonal shifts in consumer demand. The demand for these products tends to increase during the warmer months, while it decreases during the colder months when warming foods and drinks are preferred.\n\nDairy products, eggs, beef, and candy displayed at the checkout lane are product types with consistently low variability. These are staples in many people's daily diets. Their supply and demand are typically stable throughout the year. The sales of candies displayed in the checkout lane are expected to be stable due to their constant high visibility, accessibility, and affordability, which can hardly be affected by economic conditions or other seasonal factors.", "tables": {}, "images": {"img-0.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 11, "text": "# 6 Discussions \n\nThe theoretical results of the proposed estimator under the dependent event setting may be improved. There is a gap between the error rates under the dependent and independent settings in Theorem 1, and in particular, the convergence rate is slower when $\\phi(J)$ is of a constant order in the upper bound for the dependent setting than that for the independent setting. This may be an artifact of our proof strategy, as certain random matrix results that are key to establishing the upper bound for the independent setting do not apply to the dependent setting. As discussed in Remark 12, the gap can be filled when the blockwise independent structure does not vary across individuals. Under the more general individualspecific blockwise structure in Condition 4, this gap may still be filled with a more refined analysis. We leave it for future investigation.\n\nThe current method is not particularly designed for forecasting, though it still has some prediction power. For example, we may predict events associated with the existing observation units and event types at a future time point (i.e., $t>1$ ) based on $f(\\widetilde{\\mathbf{X}}(1-h))$. This prediction is sensible if the model still holds after time 1 , and $\\mathbf{X}^{*}(1-h)$ and $\\mathbf{X}^{*}(t)$ are close to each other due to the smoothness of the function. We may improve the prediction power of the proposed method by further assuming a stochastic model (e.g., a Gaussian random field model) for the latent process $\\mathbf{X}(t)$ and estimating it based on our estimate $\\widetilde{\\mathbf{X}}(t)$. This model may allow us to better predict future events, even if they are associated with new observation units or event types not used in the model training, as long as the new observation units and event types are from the same populations as the existing ones.\n\nA useful application of the proposed method is for detecting changes in each observation unit, which may be of interest in many applications. For example, in the grocery shopping application, a change in the dynamic factor of a household may imply a structural change\n\nin their consumer behaviour, based on which individualized marketing strategy may be developed. Although we currently require each $\\theta_{i k}(t)$ to be sufficiently smooth, this requirement can be relaxed to allow each $\\theta_{i k}(t)$ to be a piecewise smooth function. Using the proposed method, changes can be detected based on the estimated functions, which is closely related to change-point detection in the nonparametric regression literature (e.g., Xia and Qiu, 2015). Methods and theories remain to be developed for optimally localizing the changes based on the estimated functions.", "tables": {}, "images": {}}, {"section_id": 12, "text": "# Appendix \n\nIn the following proof, we write $a_{N, J} \\lesssim b_{N, J}$ if there exists a constant $C$ (independent of $N$, $J$ ) such that $\\left|a_{N, J}\\right| \\leq C\\left|b_{N, J}\\right|$. The results in Theorems 1 and 3 can be extended if we want to use a kernel function supported on the whole real line, for example, the Gaussian kernel function $K(x) \\propto \\exp \\left(-x^{2} / 2\\right)$. In such cases, Condition 3 can be modified as Condition 3' as follows.\n\nCondition 3'. The kernel function $K$ satisfies: (i) it is a Lipschitz function of order $m$; (ii) it attains its unique maximum at $x=0$; (iii) it is twice continuously differentiable in a neighbourhood of 0 and $(\\log K)^{\\prime \\prime}(0)<0$; (iv) there exists a constant $\\epsilon>0$ such that for $k=1, \\cdots, m-1$, the tail bound satisfies\n\n$$\n\\max \\left\\{\\left|\\int_{x \\geq R} x^{k}|K(x)| d x\\right|,\\left|\\int_{x \\leq-R} x^{k}|K(x)| d x\\right|\\right\\}=o\\left(R^{-(m-k) / \\epsilon}\\right)\n$$\n\nwhen $R \\rightarrow \\infty$.\n\nUnder the above condition, all the limits of integration in Theorems 1 and 3 should be changed from $[h, 1-h]$ to $\\left[h^{1-\\epsilon}, 1-h^{1-\\epsilon}\\right]$.", "tables": {}, "images": {}}, {"section_id": 13, "text": "## A Proof of Theorems and Lemmas\n## A. 1 Proof of Theorem 1\n\nThe proof of Theorem 1 is based on the following two lemmas, whose proof will be provided later in Appendix B.\n\nLemma 1. For $a, b \\in[-\\alpha, \\alpha]$ and $f(x)$ satisfying Condition 1, we have\n\n$$\n(a-b)^{2} \\leq 4 \\beta_{\\alpha}\\left(f(a) \\log \\frac{f(a)}{f(b)}-(f(a)-f(b))\\right)\n$$\n\nLemma 2. Assume Condition 2 and 3, then there exists a positive constant $C_{m}$ that only depends on $m$, such that for any $t \\in(0,1)$, as long as $h \\leq \\min \\{t, 1-t\\}$, we have\n\n$$\n\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) f\\left(X_{i j}(s)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-f\\left(X_{i j}(t)\\right)\\right| \\leq C_{m} h^{m}\n$$\n\nRemark 17. If we modify Condition 3 by Condition 3', then Lemma 2 still holds for any $t \\in\\left[h^{1-\\epsilon}, 1-h^{1-\\epsilon}\\right]$. The proof of Theorem 1 and 3 remains the same under Condition 3' when we change the limits of integration from $[h, 1-h]$ to $\\left[h^{1-\\epsilon}, 1-h^{1-\\epsilon}\\right]$.\n\nProof of Theorem 1. For notational simplicity, we treat $\\widehat{\\mathbf{X}}(\\cdot)=\\widehat{\\boldsymbol{\\Theta}}(\\cdot) \\widehat{\\mathbf{A}}^{\\mathrm{T}}$ as the obtained estimator, and denote\n\n$$\nG=\\left\\{\\mathbf{X} \\in \\mathbb{R}^{N \\times J}: \\mathbf{X}=\\boldsymbol{\\Theta} \\mathbf{A}^{\\mathrm{T}}, \\boldsymbol{\\Theta} \\in \\mathbb{R}^{N \\times r}, \\mathbf{A} \\in \\mathbb{R}^{J \\times r},\\|\\boldsymbol{\\Theta}\\|_{2 \\rightarrow \\infty} \\leq M^{1 / 2},\\|\\mathbf{A}\\|_{2 \\rightarrow \\infty} \\leq M^{1 / 2}\\right\\}\n$$\n\nFor any $t \\in[0,1]$, it is easy to see that $\\mathbf{X}^{*}(t) \\in G$. We then prove the upper bound for the dependent case of Theorem 1. For given $t \\in(0,1)$ and $\\mathbf{X} \\in G$, denote\n\n$$\n\\mathcal{L}_{t, h}(\\mathbf{X})=\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left(\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log f\\left(X_{i j}\\right)-f\\left(X_{i j}\\right)\\right)\n$$\n\nBy Lemma 1 and Lemma 2, we have\n\n$$\n\\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t\n$$\n\n$$\n\\begin{aligned}\n= & \\frac{1}{N J} \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(\\widehat{X}_{i j}(t)-X_{i j}^{*}(t)\\right)^{2} \\mathrm{~d} t \\\\\n\\leqq & \\frac{4 \\beta_{M}}{N J} \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(f\\left(X_{i j}^{*}(t)\\right) \\log \\frac{f\\left(X_{i j}^{*}(t)\\right)}{f\\left(\\widehat{X}_{i j}(t)\\right)}-\\left(f\\left(X_{i j}^{*}(t)\\right)-f\\left(\\widehat{X}_{i j}(t)\\right)\\right)\\right) \\mathrm{d} t \\\\\n\\leqq & \\frac{4 \\beta_{M}}{N J} \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(\\frac{\\int_{0}^{1} K_{h}(t-s) f\\left(X_{i j}^{*}(s)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log \\frac{f\\left(X_{i j}^{*}(t)\\right)}{f\\left(\\widehat{X}_{i j}(t)\\right)}-\\left(f\\left(X_{i j}^{*}(t)\\right)-f\\left(\\widehat{X}_{i j}(t)\\right)\\right)\\right) \\mathrm{d} t \\\\\n& +\\frac{4 \\beta_{M}}{N J} \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) f\\left(X_{i j}^{*}(s)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-f\\left(X_{i j}^{*}(t)\\right)\\right|\\left|\\log \\frac{f\\left(X_{i j}^{*}(t)\\right)}{f\\left(\\widehat{X}_{i j}(t)\\right)}\\right| \\mathrm{d} t \\\\\n\\leqq & \\frac{4 \\beta_{M}}{N J} \\int_{h}^{1-h}\\left(\\mathbb{E} \\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathbb{E} \\mathcal{L}_{t, h}(\\widehat{\\mathbf{X}}(t))\\right) \\mathrm{d} t+\\frac{4 \\beta_{M}}{N J} N J \\cdot C_{m} h^{m} \\cdot 2 \\sup _{|x| \\leq M}|\\log f(x)| \\\\\n= & \\frac{4 \\beta_{M}}{N J} \\int_{h}^{1-h}\\left(\\mathbb{E} \\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathbb{E} \\mathcal{L}_{t, h}(\\widehat{\\mathbf{X}}(t))\\right) \\mathrm{d} t+8 \\beta_{M} C_{m} h^{m} \\sup _{|x| \\leq M}|\\log f(x)|\n\\end{aligned}\n$$\n\nwhere ( $i$ ) follows from Lemma 1, (ii) follows from triangle inequality, and (iii) follows from the definition of $\\mathcal{L}_{t, h}$ and Lemma 2. Note that here the expectation is taken only over the randomness of $\\left\\{Y_{i j}(t)\\right\\}_{i \\in[N], j \\in[J]}$. By the definition of $\\widehat{\\mathbf{X}}(\\cdot)$, we have\n\n$$\n\\int_{h}^{1-h} \\mathcal{L}_{t, h}(\\widehat{\\mathbf{X}}(t)) \\mathrm{d} t=\\mathcal{L}_{h}(\\widehat{\\mathbf{X}}) \\geq \\mathcal{L}_{h}\\left(\\mathbf{X}^{*}\\right)=\\int_{h}^{1-h} \\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right) \\mathrm{d} t\n$$\n\nThen we have\n\n$$\n\\begin{aligned}\n& \\int_{h}^{1-h}\\left(\\mathbb{E} \\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathbb{E} \\mathcal{L}_{t, h}(\\widehat{\\mathbf{X}}(t))\\right) \\mathrm{d} t \\\\\n\\leq & \\int_{h}^{1-h}\\left(\\mathbb{E} \\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathbb{E} \\mathcal{L}_{t, h}(\\widehat{\\mathbf{X}}(t))+\\mathcal{L}_{t, h}(\\widehat{\\mathbf{X}}(t))-\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)\\right) \\mathrm{d} t \\\\\n\\leq & 2 \\int_{h}^{1-h} \\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\mathrm{d} t\n\\end{aligned}\n$$\n\nTherefore we have\n\n$$\n\\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\lesssim \\frac{1}{N J} \\int_{h}^{1-h} \\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\mathrm{d} t+h^{m}\n$$\n\nNow we partition a $\\Delta$-net on $[0,1]$ with $\\Delta=1 /\\left(4 L_{M} h^{m+4}\\right)$ and $t_{k}=k \\Delta, k=1, \\ldots, \\widetilde{K}$, where $\\widetilde{K}=[1 / \\Delta]$. Then we get that\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\frac{1}{N J} \\int_{h}^{1-h} \\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\mathrm{d} t \\geq 2 h^{m}\\right) \\\\\n\\leq & \\sum_{k=1, \\ldots, \\widetilde{K}: t_{k} \\in[h, 1-h]} \\mathbb{P}\\left(\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t_{k}, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t_{k}, h}(\\mathbf{X})\\right| \\geq N J h^{m}\\right) \\\\\n& +\\mathbb{P}\\left(\\sup _{\\left|t-t^{\\prime}\\right| \\leq \\Delta}\\left[\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right|-\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t^{\\prime}, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t^{\\prime}, h}(\\mathbf{X})\\right|\\right] \\geq N J h^{m}\\right) \\\\\ntriangleq & I_{1}+I_{2}\n\\end{aligned}\n$$\n\nWe will then bound $I_{1}$ and $I_{2}$ separately in Step 1 and Step 2.\n\nStep 1: Bound $I_{1}$.\n\nFor matrix $\\mathbf{X}=\\left(X_{i j}\\right) \\in \\mathbb{R}^{N \\times J}$, define max norm $\\|\\cdot\\|_{\\max }$ as $\\|\\mathbf{X}\\|_{\\max } \\triangleq \\max _{i, j}\\left|X_{i j}\\right|$. Let $G^{\\prime}$ be an $h^{m+1}$ - covering of $G$ with respect to the max norm, s.t. $\\left|G^{\\prime}\\right|=N\\left(h^{m+1}, G,\\|\\cdot\\|_{\\max }\\right)$. We know that $\\forall \\mathbf{X} \\in G, \\exists \\mathbf{X}^{\\prime} \\in G^{\\prime}$ such that $\\left\\|\\mathbf{X}-\\mathbf{X}^{\\prime}\\right\\|_{\\max } \\leq h^{m+1}$. For any fixed $t \\in[h, 1-h]$, we have\n\n$$\n\\begin{aligned}\n& \\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})-\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{\\prime}\\right)+\\mathbb{E} \\mathcal{L}_{t, h}\\left(\\mathbf{X}^{\\prime}\\right)\\right| \\\\\n= & \\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\left(\\log f\\left(X_{i j}\\right)-\\log f\\left(X_{i j}^{\\prime}\\right)\\right)\\right| \\\\\n\\leq & L_{M} h^{m+1} \\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|\n\\end{aligned}\n$$\n\nHere we use an simple fact that $|\\log f(a)-\\log f(b)| \\leq L_{M}|a-b|$, if $a, b \\in[-M, M]$. Therefore, if\n\n$$\n\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right| \\leq \\frac{N J}{2 L_{M} h}\n$$\n\nand\n\n$$\n\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\geq N J h^{m}\n$$\n\nthen we have\n\n$$\n\\sup _{\\mathbf{X}^{\\prime} \\in G^{\\prime}}\\left|\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{\\prime}\\right)-\\mathbb{E} \\mathcal{L}_{t, h}\\left(\\mathbf{X}^{\\prime}\\right)\\right| \\geq \\frac{N J h^{m}}{2}\n$$\n\nHence we get\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\geq N J h^{m}\\right) \\\\\n\\leq & \\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right| \\geq \\frac{N J}{2 L_{M} h}\\right) \\\\\n& +\\mathbb{P}\\left(\\sup _{\\mathbf{X}^{\\prime} \\in G^{\\prime}}\\left|\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{\\prime}\\right)-\\mathbb{E} \\mathcal{L}_{t, h}\\left(\\mathbf{X}^{\\prime}\\right)\\right| \\geq \\frac{N J h^{m}}{2}\\right) \\\\\n\\leq & \\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right| \\geq \\frac{N J}{2 L_{M} h}\\right) \\\\\n& +N\\left(h^{m+1}, G,\\|\\cdot\\|_{\\max }\\right) \\sup _{\\mathbf{X}^{\\prime} \\in G^{\\prime}} \\mathbb{P}\\left(\\left|\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{\\prime}\\right)-\\mathbb{E} \\mathcal{L}_{t, h}\\left(\\mathbf{X}^{\\prime}\\right)\\right| \\geq \\frac{N J h^{m}}{2}\\right) \\\\\n\\leq & \\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right| \\geq \\frac{N J}{2 L_{M} h}\\right) \\\\\n& +N\\left(h^{m+1}, G,\\|\\cdot\\|_{\\max }\\right) \\sup _{\\mathbf{X} \\in G} \\mathbb{P}\\left(\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\geq \\frac{N J h^{m}}{2}\\right)\n\\end{aligned}\n$$\n\nWe then bound the two terms in Eq. (A.9) in Step 1.1 and Step 1.2, respectively.\n\nStep 1.1: Bound the first term in (A.9).\n\nFor sufficiently small $x>0$ (determined later), we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right| \\geq \\frac{N J}{2 L_{M} h}\\right) \\\\\n\\leq & \\exp \\left(-\\frac{x N J}{2 L_{M} h}\\right) \\mathbb{E}\\left[\\exp \\left(x \\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|\\right)\\right]\n\\end{aligned}\n$$\n\n$$\n=\\exp \\left(-\\frac{x N J}{2 L_{M} h}\\right) \\prod_{i=1}^{N} \\mathbb{E}\\left[\\exp \\left(x \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|\\right)\\right]\n$$\n\nFor any $1 \\leq i \\leq N$, we have the following estimation:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\exp \\left(x \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|\\right)\\right] \\\\\n\\leq & \\sum_{m=0}^{\\infty} \\frac{x^{m}}{m!} \\mathbb{E}\\left[\\left(\\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|\\right)^{m}\\right] \\\\\n\\leq & \\sum_{m=0}^{\\infty} \\frac{J^{m-1} x^{m}}{m!} \\sum_{j=1}^{J} \\mathbb{E}\\left[\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|^{m}\\right] \\\\\n\\leq & \\sum_{m=0}^{\\infty} \\frac{2^{m} J^{m-1} x^{m}}{m!} \\sum_{j=1}^{J} \\mathbb{E}\\left[\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|^{m}\\right]\n\\end{aligned}\n$$\n\nIn the last line we use the inequality $\\mathbb{E}\\left[|X-\\mathbb{E} X|^{m}\\right] \\leq 2^{m} \\mathbb{E}\\left[|X|^{m}\\right]$. Since for any $t \\in[h, 1-h]$, $\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s=\\int_{-1}^{1} K(s) \\mathrm{d} s=1$ by Condition 3 , we have\n\n$$\n\\mathbb{E}\\left[\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|^{m}\\right] \\leq \\mathbb{E}\\left[\\left(\\int_{0}^{1}\\left|K_{h}(t-s)\\right| \\mathrm{d} Y_{i j}(s)\\right)^{m}\\right]\n$$\n\nNow we derive moment bounds for $\\int_{0}^{1}\\left|K_{h}(t-s)\\right| \\mathrm{d} Y_{i j}(s)$. Denote $\\left(A_{1}, \\cdots, A_{k}\\right)$ a partition of $[m]=\\{1, \\cdots, m\\}$ into $k$ non-distinct subsets, where each $A_{i}, 1 \\leq i \\leq k$ is nonempty. Let $S(m, k)$ denote the number of such partitions, i.e., Stirling number of the second hand. It is known that\n\n$$\nS(m, k) \\leq \\frac{1}{2}\\binom{m}{k} k^{m-k} \\leq \\frac{1}{2}\\binom{m}{k} m^{m-k}\n$$\n\nDenote $S=\\left\\{\\left(s_{1}, \\cdots, s_{k}\\right) \\in[0,1]^{k}: s_{i} \\neq s_{j}, \\forall i, j\\right\\}$. Using Condition 4, we can obtain\n\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{1}\\left|K_{h}(t-s)\\right| \\mathrm{d} Y_{i j}(s)\\right)^{m}\\right]=\\sum_{k=1}^{m} \\sum_{\\left(A_{1}, \\cdots, A_{k}\\right)} \\int_{S} \\prod_{l=1}^{k}\\left|K_{h}\\left(t-s_{l}\\right)\\right|^{\\left|A_{l}\\right|} \\mathbb{E}\\left[\\prod_{l=1}^{k} \\mathrm{~d} Y_{i j}\\left(s_{l}\\right)\\right]\n$$\n\n$$\n\\begin{aligned}\n& \\leq \\sum_{k=1}^{m} \\lambda^{k} \\sum_{\\left(A_{1}, \\cdots, A_{k}\\right)} \\prod_{l=1}^{k} \\int_{0}^{1}\\left|K_{h}\\left(t-s_{l}\\right)\\right|^{\\left|A_{l}\\right|} \\mathrm{d} s_{l} \\\\\n& =\\sum_{k=1}^{m} \\lambda^{k} h^{-(m-k)} \\sum_{\\left(A_{1}, \\cdots, A_{k}\\right)} \\prod_{l=1}^{k} \\int_{\\mathbb{R}}|K(u)|^{\\left|A_{l}\\right|} \\mathrm{d} u\n\\end{aligned}\n$$\n\nWith the aid of Condition 3 and Laplace's method, we easily obtain that there is a constant $L^{\\prime}>0$ such that\n\n$$\n\\int_{\\mathbb{R}}|K(u)|^{\\left|A_{l}\\right|} \\mathrm{d} u \\leq L^{\\prime} \\frac{|K(0)|^{\\left|A_{l}\\right|}}{\\sqrt{\\left|A_{l}\\right|}}\n$$\n\nthus leading to the following estimation:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left(\\int_{0}^{1}\\left|K_{h}(t-s)\\right| \\mathrm{d} Y_{i j}(s)\\right)^{m}\\right] & \\leq \\sum_{k=1}^{m} \\lambda^{k} h^{-(m-k)} \\sum_{\\left(A_{1}, \\cdots, A_{k}\\right)} \\prod_{l=1}^{k} \\int_{\\mathbb{R}}|K(u)|^{\\left|A_{l}\\right|} \\mathrm{d} u \\\\\n& \\leq \\sum_{k=1}^{m} \\lambda^{k} L^{\\prime k}|K(0)|^{m} h^{-(m-k)} \\sum_{\\left(A_{1}, \\cdots, A_{k}\\right)} \\prod_{l=1}^{k} \\frac{1}{\\sqrt{\\left|A_{l}\\right|}} \\\\\n& \\leq \\sum_{k=1}^{m} \\lambda^{k} L^{\\prime k}|K(0)|^{m} h^{-(m-k)} S(m, k) \\\\\n& \\leq \\frac{1}{2}|K(0)|^{m} \\sum_{k=1}^{m}\\binom{m}{k} \\lambda^{k} L^{\\prime k}\\left(\\frac{m}{h}\\right)^{m-k} \\\\\n& =\\frac{1}{2}|K(0)|^{m}\\left[\\left(\\lambda L^{\\prime}+\\frac{m}{h}\\right)^{m}-\\left(\\frac{m}{h}\\right)^{m}\\right] \\\\\n& \\leq \\frac{1}{2}|K(0)|^{m}\\left(\\frac{m}{h}\\right)^{m}\\left(\\exp \\left(\\lambda L^{\\prime} h\\right)-1\\right)\n\\end{aligned}\n$$\n\nSince $m^{m} \\leq m!e^{m}$, by (A.12) there exists an absolute constant $L>0$ such that\n\n$$\n\\mathbb{E}\\left[\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|^{m}\\right] \\leq \\frac{m!}{2^{m+1}}\\left(\\frac{L}{h}\\right)^{m} h\n$$\n\nHence by (A.11), for any $0<x<h / J L$, we have\n\n$$\n\\mathbb{E}\\left[\\exp \\left(x \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|\\right)\\right] \\leq 1+\\sum_{m=1}^{\\infty} \\frac{2^{m} J^{m-1} x^{m}}{m!} \\sum_{j=1}^{J} \\frac{m!h}{2^{m+1}}\\left(\\frac{L}{h}\\right)^{m}\n$$\n\n$$\n\\leq \\frac{1}{2}\\left(1-\\frac{J L x}{h}\\right)^{-1}\n$$\n\nBy (A.10), this indicates that:\n\n$$\n\\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right| \\geq \\frac{N J}{2 L_{M} h}\\right) \\leq \\exp \\left(-\\frac{x N J}{2 L_{M} h}\\right) \\frac{1}{2^{N}}\\left(1-\\frac{J L x}{h}\\right)^{-N}\n$$\n\nHence by choosing $x=h /(2 J L)$, we have\n\n$$\n\\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right| \\geq \\frac{N J}{2 L_{M} h}\\right) \\leq \\exp \\left(-\\frac{N}{4 L L_{M}}\\right)\n$$\n\nStep 1.2: Bound the second term in (A.9).\n\nFor any $\\mathbf{X} \\in G$ and $x>0$ small enough (determined later), we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\geq \\frac{N J h^{m}}{2}\\right) \\\\\n= & \\mathbb{P}\\left(\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X}) \\geq \\frac{N J h^{m}}{2}\\right)+\\mathbb{P}\\left(-\\mathcal{L}_{t, h}(\\mathbf{X})+\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X}) \\geq \\frac{N J h^{m}}{2}\\right) \\\\\n\\leq & \\exp \\left(-\\frac{x N J h^{m}}{2}\\right) \\mathbb{E}\\left[\\exp \\left(x \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log f\\left(X_{i j}\\right)\\right)\\right] \\\\\n& +\\exp \\left(-\\frac{x N J h^{m}}{2}\\right) \\mathbb{E}\\left[\\exp \\left(-x \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log f\\left(X_{i j}\\right)\\right)\\right] \\\\\n\\leq & \\exp \\left(-\\frac{x N J h^{m}}{2}\\right) \\prod_{i=1}^{N} \\mathbb{E}\\left[\\exp \\left(x \\sum_{j=1}^{J} \\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log f\\left(X_{i j}\\right)\\right)\\right] \\\\\n& +\\exp \\left(-\\frac{x N J h^{m}}{2}\\right) \\prod_{i=1}^{N} \\mathbb{E}\\left[\\exp \\left(-x \\sum_{j=1}^{J} \\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log f\\left(X_{i j}\\right)\\right)\\right]\n\\end{aligned}\n$$\n\nBy Condition 4(iii) we have\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\exp \\left(x \\sum_{j=1}^{J} \\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log f\\left(X_{i j}\\right)\\right)\\right] \\\\\n= & \\prod_{k=1}^{W_{i}} \\mathbb{E}\\left[\\exp \\left(x \\sum_{j \\in B_{i, k}} \\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log f\\left(X_{i j}\\right)\\right)\\right] \\\\\n\\leq & \\prod_{k=1}^{W_{i}}\\left[\\frac{1}{\\left|B_{i, k}\\right|} \\sum_{j \\in B_{i, k}}\\left[\\mathbb{E} \\exp \\left(x\\left|B_{i, k}\\right| \\log f\\left(X_{i j}\\right) \\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right)\\right]\\right] \\\\\n\\leq & \\prod_{k=1}^{W_{i}}\\left[\\frac{1}{\\left|B_{i, k}\\right|} \\sum_{j \\in B_{i, k}}\\left[1+\\sum_{m=2}^{\\infty} \\frac{1}{m!} x^{m}\\left|B_{i, k}\\right|^{m} \\log ^{m} f\\left(X_{i j}\\right) \\mathbb{E}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|^{m}\\right]\\right] \\\\\n\\leq & \\prod_{k=1}^{W_{i}}\\left[\\frac{1}{\\left|B_{i, k}\\right|} \\sum_{j \\in B_{i, k}}\\left[1+\\sum_{m=2}^{\\infty} \\frac{1}{m!} x^{m}\\left|B_{i, k}\\right|^{m}\\left|\\log ^{m} f\\left(X_{i j}\\right)\\right| 2^{m} \\mathbb{E}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right|^{m}\\right]\\right] \\\\\n\\leq & \\prod_{k=1}^{W_{i}}\\left[\\frac{1}{\\left|B_{i, k}\\right|} \\sum_{j \\in B_{i, k}}\\left[1+\\sum_{m=2}^{\\infty} \\frac{1}{2} x^{m}\\left|B_{i, k}\\right|^{m}\\left|\\log ^{m} f\\left(X_{i j}\\right)\\right|\\left(\\frac{L}{h}\\right)^{m} h\\right]\\right] \\\\\n\\leq & \\prod_{k=1}^{W_{i}}\\left(1+C \\frac{x^{2}\\left|B_{i, k}\\right|^{2}}{h}\\right) \\text { when } 0 \\leq|x|\\left|B_{i, k}\\right| / h<1 / C \\\\\n\\leq & \\exp \\left(\\frac{C x^{2}}{h} \\sum_{k=1}^{W_{i}}\\left|B_{i, k}\\right|^{2}\\right) \\\\\n\\leq & \\exp \\left(C J \\phi(J) \\frac{x^{2}}{h}\\right)\n\\end{aligned}\n$$\n\nSimilarly, for $x>0$ small enough, we can get\n\n$$\n\\mathbb{E}\\left[\\exp \\left(-x \\sum_{j=1}^{J} \\frac{\\int_{0}^{1} K_{h}(t-s)\\left(\\mathrm{d} Y_{i j}(s)-f\\left(X_{i j}(s)\\right) \\mathrm{d} s\\right)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\log f\\left(X_{i j}\\right)\\right)\\right] \\leq \\exp \\left(C J \\phi(J) \\frac{x^{2}}{h}\\right)\n$$\n\nTake $x=h^{m+1} /(4 C \\phi(J))$, then $|x|\\left|B_{i, k}\\right| / h \\leq h^{m} /(4 C) \\ll 1$. Then by (A.14) we have\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\geq \\frac{N J h^{m}}{2}\\right) & \\leq 2 \\exp \\left(-\\frac{x N J h^{m}}{2}+C J \\phi(J) \\frac{x^{2}}{h}\\right) \\\\\n& \\leq 2 \\exp \\left(-\\frac{N J h^{2 m+1}}{16 C \\phi(J)}\\right)\n\\end{aligned}\n$$\n\nNote that by the low rank assumption on $G$, we have\n\n$$\nN\\left(h^{m+1}, G,\\|\\cdot\\|_{\\max }\\right) \\leq\\left(\\frac{C}{h^{m+1}}\\right)^{r(N+J)}=\\exp \\left(r(N+J)\\left(\\log C+(m+1) \\log \\left(\\frac{1}{h}\\right)\\right)\\right)\n$$\n\nHence by (A.9), (A.13), (A.15) and (A.16), for any fixed $t \\in[h, 1-h]$ we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sup _{\\mathbf{X} \\in G} \\mid \\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X}) \\mid \\geq N J h^{m}\\right) \\\\\n\\leq & 2 N\\left(h^{m+1}, G,\\|\\cdot\\|_{\\max }\\right) \\exp \\left(-\\frac{N J h^{2 m+1}}{16 C \\phi(J)}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n\\leq & 2 \\exp \\left(r(N+J)\\left(\\log C+(m+1) \\log \\left(\\frac{1}{h}\\right)\\right)-\\frac{N J h^{2 m+1}}{16 C \\phi(J)}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n= & 2 \\exp \\left(r(N+J)\\left(\\log C+\\frac{(m+1)(1-\\delta)}{2 m+1} \\log \\left(\\frac{J}{\\phi(J)}\\right)\\right)\\right. \\\\\n& \\left.-\\frac{N}{16 C}\\left(\\frac{J}{\\phi(J)}\\right)^{(2 m+1) \\delta / m}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n\\leq & \\exp \\left(\\widetilde{C} N \\log \\left(\\frac{J}{\\phi(J)}\\right)-\\frac{N}{16 C}\\left(\\frac{J}{\\phi(J)}\\right)^{(2 m+1) \\delta / m}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n\\leq & \\exp \\left(-\\frac{N}{4 L L_{M}}\\right)\n\\end{aligned}\n$$\n\nThis implies that\n\n$$\n\\begin{aligned}\nI_{1} & =\\sum_{k=1, \\ldots, \\widetilde{K}: t_{k} \\in[h, 1-h]} \\mathbb{P}\\left(\\sup _{\\mathbf{X} \\in G} \\mid \\mathcal{L}_{\\mathbf{Y}\\left(t_{k}\\right), h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{\\mathbf{Y}\\left(t_{k}\\right), h}(\\mathbf{X}) \\mid \\geq N J h^{m}\\right) \\\\\n& \\lesssim \\frac{1}{\\Delta} \\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n& \\lesssim h^{-(m+4)} \\exp \\left(-\\frac{N}{4 L L_{M}}\\right)\n\\end{aligned}\n$$\n\nStep 2: Bound $I_{2}$.\n\nSince $K(x)$ is Lipschitz, we assume the corresponding Lipschitz constant is $L_{K}$. Then $K^{\\prime}(x)$\n\nexists almost everywhere and $\\left|K^{\\prime}(x)\\right| \\leq L_{K}$, then for any $t \\in[h, 1-h]$ and any $\\mathbf{X} \\in G$ we have\n\n$$\n\\begin{aligned}\n& \\left|\\frac{\\mathrm{d}}{\\mathrm{~d} t}\\left[\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right]\\right| \\\\\n\\leq & \\sum_{i=1}^{N} \\sum_{j=1}^{J} L_{K} \\sup _{|x| \\leq M}|\\log f(x)| \\frac{\\int_{0}^{1} \\mathrm{~d} Y_{i j}(s)}{h^{2}\\left|\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s\\right|}+\\sum_{i=1}^{N} \\sum_{j=1}^{J} L_{K} \\sup _{|x| \\leq M}|\\log f(x)| \\frac{\\int_{0}^{1} f\\left(X_{i j}^{*}(s)\\right) \\mathrm{d} s}{h^{2}\\left|\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s\\right|} \\\\\n& \\lesssim \\frac{1}{h^{2}}\\left[\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left(\\int_{0}^{1} \\mathrm{~d} Y_{i j}(s)+1\\right)\\right]\n\\end{aligned}\n$$\n\nwhich is independent of $t$. This implies that $\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right|$ is a Lipschitz function with Lipschitz constant $C\\left[\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left(\\int_{0}^{1} \\mathrm{~d} Y_{i j}(s)+1\\right)\\right] / h^{2}$ on $[h, 1-h]$. We then derive the tail probability of $\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(s)$. For sufficiently small $x>0$ (determined later), we have\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(s) \\geq \\frac{N J}{4 L_{M} h}\\right) & \\leq \\exp \\left(-\\frac{x N J}{4 L_{M} h}\\right) \\mathbb{E}\\left[\\exp \\left(x \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(t)\\right)\\right] \\\\\n& =\\exp \\left(-\\frac{x N J}{4 L_{M} h}\\right) \\prod_{i=1}^{N} \\mathbb{E}\\left[\\exp \\left(x \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(t)\\right)\\right]\n\\end{aligned}\n$$\n\nFor any $1 \\leq i \\leq N$, we have the following estimation:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\exp \\left(x \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(t)\\right)\\right] & \\leq \\sum_{m=0}^{\\infty} \\frac{x^{m}}{m!} \\mathbb{E}\\left[\\left(\\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(t)\\right)^{m}\\right] \\\\\n& \\leq \\sum_{m=0}^{\\infty} \\frac{J^{m-1} x^{m}}{m!} \\sum_{j=1}^{J} \\mathbb{E}\\left[\\left(\\int_{0}^{1} \\mathrm{~d} Y_{i j}(t)\\right)^{m}\\right]\n\\end{aligned}\n$$\n\nBy similar method as in Step 1.1, there exists an absolute constant $L>0$ (for simplicity\n\nwe assume the constant $L$ is the same as in Step 1.1) such that\n\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{1} \\mathrm{~d} Y_{i j}(t)\\right)^{m}\\right] \\leq \\frac{m!}{2}[e(1+\\lambda)]^{m} \\triangleq \\frac{m!}{2} L^{m}\n$$\n\nHence we get that\n\n$$\n\\mathbb{E}\\left[\\exp \\left(x \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(t)\\right)\\right] \\leq \\sum_{m=0}^{\\infty} \\frac{J^{m-1} x^{m}}{m!} \\sum_{j=1}^{J} \\frac{m!}{2} L^{m}=\\frac{1}{2}(1-J L x)^{-1}\n$$\n\nSo for any $0<x<1 / J L$, we have the following estimation:\n\n$$\n\\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(s) \\geq \\frac{N J}{4 L_{M} h}\\right) \\leq \\exp \\left(-\\frac{x N J}{4 L_{M} h}\\right) \\frac{1}{2}(1-J L x)^{-1}\n$$\n\nChoose $x=1 /(2 J L)$, we obtain that\n\n$$\n\\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(s) \\geq \\frac{N J}{4 L_{M} h}\\right) \\leq \\exp \\left(-\\frac{N}{8 L L_{M} h}\\right)\n$$\n\nSince $\\Delta=1 /\\left(4 L_{M} h^{m+4}\\right)$, we have\n\n$$\n\\begin{aligned}\nI_{2}= & \\mathbb{P}\\left(\\sup _{\\left|t-t^{\\prime}\\right| \\leq \\Delta, t, t^{\\prime}}\\left[\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right|-\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{\\mathbf{Y}\\left(t^{\\prime}\\right), h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{\\mathbf{Y}\\left(t^{\\prime}\\right), h}(\\mathbf{X})\\right|\\right] \\geq N J h^{m}\\right) \\\\\n\\leq & \\mathbb{P}\\left(\\frac{C}{h^{2}}\\left[\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left(\\int_{0}^{1} \\mathrm{~d} Y_{i j}(s)+1\\right)\\right] \\geq \\frac{N J}{4 L_{M} h^{4}}\\right) \\\\\n\\leq & \\mathbb{P}\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(s) \\geq \\frac{N J}{4 L_{M} h}\\right) \\\\\n\\leq & \\exp \\left(-\\frac{N}{8 L L_{M} h}\\right)\n\\end{aligned}\n$$\n\nStep 3: Obtain the final bound in (A.8).\n\nBy (A.17) and (A.18) we have\n\n$$\n\\lim _{N, J \\rightarrow \\infty} \\mathbb{P}\\left(\\frac{1}{N J} \\int_{h}^{1-h} \\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\mathrm{d} t \\geq 2 h^{m}\\right)=0\n$$\n\nFinally by (A.7) and (A.8), we have proved that\n\n$$\n\\frac{1}{N J} \\sup _{t \\in[h, 1-h]}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2}=O_{p}\\left(h^{m}\\right)=O_{p}\\left((J / \\phi(J))^{-m /(2 m+1)+\\delta}\\right)\n$$\n\nNow we prove the second part of Theorem 1. We first give a polynomial bound by a similar method as in the proof of the first part in Step 1. Then we derive a sharper bound from the previous bound in Step 2 iteratively.\n\nStep 1: Obtain a polynomial bound.\n\nWe still partition a $\\Delta$-net on $[0,1]$ with $\\Delta=1 /\\left(4 L_{M} h^{m+4}\\right)$ and $t_{k}=k \\Delta, k=1, \\ldots, \\widetilde{K}$, where $\\widetilde{K}=[1 / \\Delta]$. Then by similar method as in the proof of theorem 1 , we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\geq 8 \\beta_{M}\\left(C_{m} \\sup _{|x| \\leq M}|\\log f(x)|+2\\right) h^{m}\\right) \\\\\n\\leq & \\sum_{k=1, \\ldots, \\widetilde{K}: t_{k} \\in[h, 1-h]} \\mathbb{P}\\left(\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t_{k}, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t_{k}, h}(\\mathbf{X})\\right| \\geq N J h^{m}\\right) \\\\\n& +\\mathbb{P}\\left(\\sup _{\\left|t-t^{\\prime}\\right| \\leq \\Delta, t, t^{\\prime}}\\left[\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right|-\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t^{\\prime}, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t^{\\prime}, h}(\\mathbf{X})\\right|\\right] \\geq N J h^{m}\\right) \\\\\n\\triangleq & I_{1}+I_{2}\n\\end{aligned}\n$$\n\nBy the same method as in the proof of the first part, we can get (note that $\\phi(J)=1$ under\n\nassumption):\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sup _{\\mathbf{X} \\in G}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\geq N J h^{m}\\right) \\\\\n& \\lesssim & 2 N\\left(h^{m+1}, G,\\|\\cdot\\|_{\\max }\\right) \\exp \\left(-\\frac{N J h^{2 m+1}}{16 C}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n& \\leq \\exp \\left(r(N+J)\\left(\\log C+(m+1) \\log \\left(\\frac{1}{h}\\right)\\right)-\\frac{N J h^{2 m+1}}{16 C}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n& \\leq \\exp \\left(r(N+J)\\left(\\log C+\\frac{m+1}{2 m+1} \\log (N \\wedge J)\\right)-\\frac{N J r \\log ^{2}(N \\wedge J)}{16 C(N \\wedge J)}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n& \\lesssim \\exp \\left(\\frac{m+1}{2 m+1} r(N+J) \\log (N \\wedge J)-\\frac{N J}{16 C} \\frac{r \\log ^{2}(N \\wedge J)}{N \\wedge J}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n& \\lesssim \\exp \\left(-\\frac{N J r \\log ^{2}(N \\wedge J)}{32 C(N \\wedge J)}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n& =\\exp \\left(-\\frac{(N \\vee J) r \\log ^{2}(N \\wedge J)}{32 C}\\right)+\\exp \\left(-\\frac{N}{4 L L_{M}}\\right) \\\\\n& \\lesssim \\exp \\left(-\\frac{N}{4 L L_{M}}\\right)\n\\end{aligned}\n$$\n\nwhich implies that $I_{1} \\lesssim h^{-(m+4)} \\exp \\left(-\\frac{N}{4 L L_{M}}\\right)$. By the same method as in the first part, we also have $I_{2} \\leq \\exp \\left(-\\frac{N}{8 L L_{M} h}\\right)$. So we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\frac{1}{N J} \\sup _{t \\in[h, 1-h]}\\left\\|\\tilde{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\geq 8 \\beta_{M}\\left(C_{m} \\sup _{|x| \\leq M}|\\log f(x)|+2\\right) h^{m}\\right) \\\\\n& \\lesssim h^{-(m+4)} \\exp \\left(-\\frac{N}{4 L L_{M}}\\right)+\\exp \\left(-\\frac{N}{8 L L_{M} h}\\right) \\\\\n& \\lesssim(N \\wedge J)^{\\frac{m+4}{2 m+1}} \\exp \\left(-\\frac{N \\wedge J}{4 L L_{M}}\\right) \\\\\n& \\rightarrow 0\n\\end{aligned}\n$$\n\nHence we have proved that when $h \\asymp(N \\wedge J) / \\log ^{2}(N \\wedge J))^{-1 /(2 m+1)}$, for any $\\epsilon>0$ we have\n\n$$\n\\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\tilde{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t=O_{p}\\left(h^{m}\\right)=o_{p}(N \\wedge J)^{-\\frac{m}{2 m+1}+\\epsilon}\n$$\n\nStep 2: From a presumed upper bound to a sharper one.\n\nAssume that for some $\\alpha<0$, we have proved that\n\n$$\n\\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t=o_{p}\\left((N \\wedge J)^{\\alpha}\\right)\n$$\n\nFor future convenience we denote $Z_{i j}(t)=\\log f\\left(X_{i j}(t)\\right), \\mathcal{L}_{t, h}(\\mathbf{Z})=\\mathcal{L}_{t, h}(\\mathbf{X})$, and so on. Then we also have\n\n$$\n\\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{Z}}(t)-\\mathbf{Z}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\leq \\frac{L_{M}^{2}}{N J} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t=o_{p}\\left((N \\wedge J)^{\\alpha}\\right)\n$$\n\nUsing Taylor's expansion, we have\n\n$$\n\\begin{aligned}\n0 \\leq & \\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}(\\widehat{\\mathbf{Z}}(t))-\\mathcal{L}_{t, h}\\left(\\mathbf{Z}^{*}(t)\\right)\\right) \\mathrm{d} t \\\\\n= & \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\left(\\widehat{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right)-\\left(e^{\\widehat{Z}_{i j}(t)}-e^{Z_{i j}^{*}(t)}\\right)\\right) \\mathrm{d} t \\\\\n= & \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-e^{Z_{i j}^{*}(t)}\\right)\\left(\\widehat{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right) \\mathrm{d} t \\\\\n& -\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h} \\frac{e^{\\widehat{Z}_{i j}(t)}}{2}\\left(\\widehat{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right)^{2} \\mathrm{~d} t\n\\end{aligned}\n$$\n\nwhere $\\widetilde{Z}_{i j}(t)$ is some real number between $Z_{i j}^{*}(t)$ and $\\widehat{Z}_{i j}(t)$. Now since $e^{\\widetilde{Z}_{i j}(t)} \\geq \\min \\left(f\\left(X_{i j}^{*}(t)\\right), f\\left(\\widehat{X}_{i j}(t)\\right)\\right) \\geq$ $\\inf _{|x| \\leq M} f(x)$, it follows that\n\n$$\n\\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{Z}}(t)-\\mathbf{Z}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\lesssim \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-f\\left(X_{i j}^{*}(t)\\right)\\right)\\left(\\widehat{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right) \\mathrm{d} t\n$$\n\nDenote $A_{i j}=\\left\\{\\int_{0}^{1} \\mathrm{~d} Y_{i j}(s) \\leq \\sqrt{h(N \\wedge J)} / 2 K(0)\\right\\}$. By a similar proof as in the first part of Theorem 1, we can show that there exists a constant $L>0$ such that when $0<x<1 / L$,\n\nwe have\n\n$$\n\\mathbb{E}\\left[\\exp \\left(x \\sum_{j=1}^{J} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(s) \\mathrm{d} s\\right)\\right] \\leq \\frac{1}{2}(1-L x)^{-1}\n$$\n\nTake $x=1 / 2 L$, we have\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(A_{i j}^{c}\\right) & =\\mathbb{P}\\left(\\int_{0}^{1} \\mathrm{~d} Y_{i j}(s) \\geq \\frac{\\sqrt{h(N \\wedge J)}}{2 K(0)}\\right) \\\\\n& \\lesssim \\exp \\left(-\\frac{x}{2 K(0)} \\sqrt{h(N \\wedge J)}\\right)(1-L x)^{-1} \\\\\n& \\lesssim \\exp \\left(-\\widetilde{C} h^{-m} \\log (N \\wedge J)\\right)\n\\end{aligned}\n$$\n\nSo we have\n\n$$\n\\mathbb{P}\\left(\\left(\\cup_{i=1}^{N} \\cup_{j=1}^{J} A_{i j}\\right)^{c}\\right) \\lesssim N J \\exp \\left(-\\widetilde{C} h^{-m} \\log (N \\wedge J)\\right) \\rightarrow 0\n$$\n\nNow we set\n\n$$\nM_{i j}(t)=\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} I_{A_{i j}}-\\mathbb{E}\\left[\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} I_{A_{i j}}\\right]\n$$\n\nNote that under $A_{i j}$, for any $t \\in[h, 1-h]$ we have\n\n$$\n\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right| \\leq \\frac{K(0)}{2 h} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(s) \\leq \\frac{1}{2} \\sqrt{\\frac{N \\wedge J}{h}}\n$$\n\nThis implies that $\\left|M_{i j}(t)\\right|$ has a uniform bound $\\sqrt{(N \\wedge J) / h}$ for any $t \\in[h, 1-h]$ and any $i, j$ with probability 1 . Then with probability tending to 1 , by (A.19) we have\n\n$$\n\\int_{h}^{1-h}\\left\\|\\widetilde{\\mathbf{Z}}(t)-\\mathbf{Z}^{e}(t)\\right\\|_{F}^{2} \\mathrm{~d} t\n$$\n\n$$\n\\begin{aligned}\n& \\lesssim \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h} M_{i j}(t)\\left(\\widetilde{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right) \\mathrm{d} t \\\\\n& \\quad+\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(\\mathbb{E}\\left[\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} I_{A_{i j}}\\right]-f\\left(X_{i j}^{*}(t)\\right)\\right)\\left(\\widetilde{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right) \\mathrm{d} t\n\\end{aligned}\n$$\n\nBy Lemma 2, for $t \\in(h, 1-h)$ we have the following uniform bound:\n\n$$\n\\begin{aligned}\n& \\left|\\mathbb{E}\\left[\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} I_{A_{i j}}\\right]-f\\left(X_{i j}^{*}(t)\\right)\\right| \\\\\n\\leq & \\left|\\mathbb{E}\\left[\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} I_{A_{i j}^{*}}\\right]\\right|+\\left|\\mathbb{E}\\left[\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right]-f\\left(X_{i j}^{*}(t)\\right)\\right| \\\\\n\\leq & \\mathbb{E} I_{A_{i j}^{c}}^{2} \\mathbb{E}\\left[\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right]^{2}+C_{m} h^{m} \\\\\n\\leq & \\mathbb{P}\\left(A_{i j}^{c}\\right) \\mathbb{E}\\left[\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right]^{2}+C_{m} h^{m} \\\\\n\\leq & \\exp \\left(-\\widetilde{C} h^{-m} \\log (N \\wedge J)\\right) h^{-1}+h^{m} \\\\\n& \\lesssim h^{m}\n\\end{aligned}\n$$\n\nTherefore we obtain that\n\n$$\n\\begin{aligned}\n& \\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{Z}}(t)-\\mathbf{Z}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\\\\n& \\lesssim_{p} \\frac{1}{N J} \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(M_{i j}(t)\\left(\\widetilde{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right)+h^{m}\\left|\\widetilde{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right|\\right) \\mathrm{d} t \\\\\n& \\lesssim_{p} \\frac{1}{N J} \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h} M_{i j}(t)\\left(\\log \\frac{f\\left(\\widetilde{X}_{i j}(t)\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}^{*}(t)\\right)}{f(0)}\\right) \\mathrm{d} t+\\frac{h^{m}}{\\sqrt{N J}} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{Z}}(t)-\\mathbf{Z}^{*}(t)\\right\\|_{F} \\mathrm{~d} t\n\\end{aligned}\n$$\n\nDenote\n\n$$\n\\mathcal{G}_{2}^{\\alpha}=\\left\\{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in \\mathcal{G} \\times \\mathcal{G}: \\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\mathbf{X}^{\\prime}(t)-\\mathbf{X}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\leq(N \\wedge J)^{\\alpha}\\right\\}\n$$\n\nSince $\\int_{h}^{1-h}\\left\\|\\mathbf{X}^{\\prime}(t)-\\mathbf{X}(t)\\right\\|_{F}^{2} \\mathrm{~d} t / N J=o_{p}\\left((N \\wedge J)^{\\alpha}\\right)$, we have\n\n$$\n\\begin{aligned}\n& \\frac{1}{N J} \\int_{h}^{1-h}\\left\\|\\widetilde{\\mathbf{Z}}(t)-\\mathbf{Z}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\\\\n& \\lesssim_{p} \\frac{1}{N J} \\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in \\mathcal{G}_{2}^{\\alpha}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}(t)\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}(t)\\right)}{f(0)}\\right) \\mathrm{d} t\\right|+h^{m}(N \\wedge J)^{\\alpha / 2}\n\\end{aligned}\n$$\n\nNow we fix an arbitrary constant $\\nu>0$ which is small enough. Denote $I_{0}=[0,(N \\wedge J)^{\\alpha / 2}]$ and $I_{n}=\\left((N \\wedge J)^{\\alpha / 2+(n-1) \\nu},(N \\wedge J)^{\\alpha / 2+n \\nu}\\right]$ for any $n \\in \\mathbb{N}$. Furthermore, we denote $G_{2, n}=\\left\\{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G \\times G:\\left\\|\\mathbf{X}-\\mathbf{X}^{\\prime}\\right\\|_{F} \\in I_{n}\\right\\}$. Then for the first term in (A.20), we have\n\n$$\n\\begin{aligned}\n& \\frac{1}{N J} \\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in \\mathcal{G}_{2}^{\\alpha}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}(t)\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}(t)\\right)}{f(0)}\\right) \\mathrm{d} t\\right| \\\\\n\\leq & \\frac{1}{N J} \\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in \\mathcal{G}_{2}^{\\alpha}}\\left|\\sum_{n=0}^{\\infty} \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h} \\mathbf{1}_{\\left(\\mathbf{X}(t), \\mathbf{X}^{\\prime}(t)\\right) \\in G_{2, n}} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}(t)\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}(t)\\right)}{f(0)}\\right) \\mathrm{d} t\\right| \\\\\n\\leq & \\frac{1}{N J} \\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in \\mathcal{G}_{2}^{\\alpha}} \\sum_{n=0}^{\\infty}\\left[\\int_{h}^{1-h} \\mathbf{1}_{\\left(\\mathbf{X}(t), \\mathbf{X}^{\\prime}(t)\\right) \\in G_{2, n}} \\mathrm{~d} t\\right. \\\\\n& \\left.\\times \\sup _{t \\in[h ; 1-h]} \\sup _{\\left(\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{X}}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}(t)\\left(\\log \\frac{f\\left(\\widetilde{X}_{i j}^{\\prime}(t)\\right)}{f(0)}-\\log \\frac{f\\left(\\widetilde{X}_{i j}(t)\\right)}{f(0)}\\right)\\right|\\right]\n\\end{aligned}\n$$\n\nNote that for any $\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in \\mathcal{G}_{2}^{\\alpha}$ and any $n \\in \\mathbb{N}$ we have\n\n$$\n\\int_{h}^{1-h} \\mathbf{1}_{\\left(\\mathbf{X}(t), \\mathbf{X}^{\\prime}(t)\\right) \\in G_{2, n}} \\mathrm{~d} t \\leq \\frac{\\int_{h}^{1-h}\\left\\|\\mathbf{X}^{\\prime}(t)-\\mathbf{X}(t)\\right\\|_{F}^{2} \\mathrm{~d} t}{(N \\wedge J)^{\\alpha+2(n-1) \\nu}} \\leq(N \\wedge J)^{-2(n-1) \\nu}\n$$\n\nSo by (A.21) and (A.22) we have\n\n$$\n\\frac{1}{N J} \\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in \\mathcal{G}_{2}^{\\alpha}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right) \\mathrm{d} t\\right|\n$$\n\n$$\n\\leq \\sum_{n=0}^{\\infty} \\frac{(N \\wedge J)^{-2(n-1) \\nu}}{N J} \\sup _{t \\in[h, 1-h]} \\sup _{\\left(\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{X}}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}(t)\\left(\\log \\frac{f\\left(\\widetilde{X}_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(\\widetilde{X}_{i j}\\right)}{f(0)}\\right)\\right|\n$$\n\nNow we partition a $\\Delta$-net on $[0,1]$ with $\\Delta=h^{m+3 / 2} /(N J \\sqrt{N \\wedge J})$ and $t_{k}=k \\Delta, k=$ $1, \\ldots, \\widetilde{K}$, where $\\widetilde{K}=[1 / \\Delta]$. Then we get that\n\n$$\n\\begin{aligned}\n& \\frac{1}{N J} \\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in \\mathcal{G}_{2}^{n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right) \\mathrm{d} t\\right| \\\\\n\\leq & \\sum_{n=0}^{\\infty} \\frac{(N \\wedge J)^{-2(n-1) \\nu}}{N J} \\sup _{t \\in[h, 1-h]}\\sup _{\\left(\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{X}}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}(t)\\left(\\log \\frac{f\\left(\\widetilde{X}_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(\\widetilde{X}_{i j}\\right)}{f(0)}\\right)\\right| \\\\\n\\leq & \\sum_{n=0}^{\\infty}(N \\wedge J)^{-2(n-1) \\nu}\\left[\\frac{1}{N J} \\sup _{\\left|t-t^{\\prime}\\right| \\leq \\Delta, t, t^{\\prime} \\in[h, 1-h]}\\sup _{\\left(\\widetilde{\\mathbf{X}}, \\widetilde{\\mathbf{X}}^{\\prime}\\right) \\in G_{2, n}}\\right. \\\\\n& \\left.\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left(M_{i j}(t)-M_{i j}\\left(t^{\\prime}\\right)\\right)\\left(\\log \\frac{f\\left(\\widetilde{X}_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(\\widetilde{X}_{i j}\\right)}{f(0)}\\right)\\right]\n\\end{aligned}\n$$\n\n$\\triangleq \\sum_{n=0}^{\\infty}(N \\wedge J)^{-2(n-1) \\nu} I_{1, n}+\\sum_{n=0}^{\\infty}(N \\wedge J)^{-2(n-1) \\nu} I_{2, n}$.\n\nWe then bound the first term of (A.23) in Step 2.1 and bound the second term of (A.23) in Step 2.2.\n\nStep 2.1: Bound $I_{1, n}$ for any fixed $n$.\n\nWe fix arbitrary $t \\in[h, 1-h]$. For any $p \\in \\mathbb{N}$, with the aid of Lemma 6.3 in Ledoux and Talagrand (2013), we get\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right|^{2 p}\\right] \\\\\n\\leq & 2^{2 p} \\mathbb{E}\\left[\\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\epsilon_{i j} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right|^{2 p}\\right]\n\\end{aligned}\n$$\n\nwhere $\\epsilon_{i j}$ 's are i.i.d. Rademacher random variables that are independent of the $Y_{i j}(t)$ 's. Now since $x \\mapsto \\frac{1}{L_{M}} \\log \\frac{f(x)}{f(0)}$ is a contraction on $[-M, M]$ that vanishes at $x=0$, using Theorem 4.12 in Ledoux and Talagrand (2013) gives\n\n$$\n\\begin{aligned}\n& 2^{2 p} \\mathbb{E}\\left[\\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\epsilon_{i j} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right|^{2 p}\\right] \\\\\n\\leq & 2^{2 p}\\left(2 L_{M}\\right)^{2 p} \\mathbb{E}\\left[\\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\epsilon_{i j} M_{i j}(t)\\left(X_{i j}^{\\prime}-X_{i j}\\right)\\right|^{2 p}\\right] \\\\\n\\leq & 4^{2 p} L_{M}^{2 p} \\mathbb{E}\\left[\\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}}\\|\\mathbf{E} \\circ \\mathbf{M}(t)\\|_{\\mathcal{S}_{2 p}}^{2 p}\\left\\|\\mathbf{X}^{\\prime}-\\mathbf{X}\\right\\|_{\\mathcal{S}_{2 q}}^{2 p}\\right]\n\\end{aligned}\n$$\n\nwhere $1 / 2 p+1 / 2 q=1$. Here $\\mathbf{E}=\\left(\\epsilon_{i j}\\right)_{i, j=1}^{N, J},\\|\\cdot\\|_{\\mathcal{S}_{k}}$ denote the Schatten- $k$ norm and $\\circ$ denote Hadamard product between two matrices. For $\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}$, we have\n\n$$\n\\left\\|\\mathbf{X}^{\\prime}-\\mathbf{X}\\right\\|_{\\mathcal{S}_{2 q}} \\leq(2 r)^{\\frac{1}{2 q}-\\frac{1}{2}}\\left\\|\\mathbf{X}^{\\prime}-\\mathbf{X}\\right\\|_{F} \\lesssim(2 r)^{\\frac{1}{2 q}-\\frac{1}{2}} \\sqrt{N J}(N \\wedge J)^{\\alpha / 2+n \\nu}\n$$\n\nSo we have\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2}^{n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right|^{2 p}\\right] \\\\\n& \\lesssim 4^{2 p} L_{M}^{2 p}(2 r)^{p-1}(N J)^{p}(N \\wedge J)^{\\alpha p+2 n p \\nu} \\mathbb{E}\\left[\\|\\mathbf{E} \\circ \\mathbf{M}(t)\\|_{\\mathcal{S}_{2 p}}^{2 p}\\right]\n\\end{aligned}\n$$\n\nFinally, our task is to give an upper bound on $\\mathbb{E}\\left[\\|\\mathbf{E} \\circ \\mathbf{M}(t)\\|_{\\mathcal{S}_{2 p}}^{2 p}\\right]$. By direct calculation, we have\n\n$$\n\\mathbb{E}\\left[\\epsilon_{i j}^{2} M_{i j}(t)^{2}\\right] \\leq \\mathbb{E}\\left[\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right]^{2}=O(h)\n$$\n\nUnder the context of Theorem 4.9 in Lata\u0142a et al. (2018), we have\n\n$$\n\\begin{aligned}\n\\sigma_{p, 1} & =\\left(\\sum_{i=1}^{N}\\left(\\sum_{j=1}^{J} \\mathbb{E}\\left[\\epsilon_{i j}^{2} M_{i j}(t)^{2}\\right]\\right)^{p}\\right)^{1 / 2 p}=O\\left(\\frac{N^{1 / 2 p} J^{1 / 2}}{h^{1 / 2}}\\right) \\\\\n\\sigma_{p, 1} & =\\left(\\sum_{j=1}^{J}\\left(\\sum_{i=1}^{N} \\mathbb{E}\\left[\\epsilon_{i j}^{2} M_{i j}(t)^{2}\\right]\\right)^{p}\\right)^{1 / 2 p}=O\\left(\\frac{N^{1 / 2} J^{1 / 2 p}}{h^{1 / 2}}\\right) \\\\\n\\sigma_{p}^{*} & =\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left\\|\\epsilon_{i j} M_{i j}(t)\\right\\|_{\\max }^{2 p}\\right)^{1 / 2 p}=O\\left(N^{1 / 2 p} J^{1 / 2 p} \\sqrt{\\frac{N \\wedge J}{h}}\\right)\n\\end{aligned}\n$$\n\nand $\\mathbb{E}\\left[\\|\\mathbf{E} \\circ \\mathbf{M}(t)\\|_{S_{2 p}}^{2 p}\\right] \\leq\\left(\\sigma_{p, 1}+\\sigma_{p, 2}+C \\sqrt{p} \\sigma_{p}^{*}\\right)^{2 p}$, where $C>0$ is a universal constant.\nHence for any $C^{\\prime}>0$ (determined later), by Markov's inequality we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\frac{1}{N J} \\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right|>C^{\\prime} p h^{m}(N \\wedge J)^{\\alpha / 2+3 n \\nu / 2}\\right) \\\\\n\\leq & \\left(C^{\\prime} p N J h^{m}\\right)^{-2 p}(N \\wedge J)^{-\\alpha p-3 n p \\nu} \\mathbb{E}\\left[\\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2}^{n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right|^{2 p}\\right] \\\\\n& \\lesssim\\left(\\frac{4 \\sqrt{2} L_{M}}{C^{\\prime} p}\\right)^{2 p} r^{p-1}(N J)^{-p} h^{-2 m p}\\left(\\sigma_{p, 1}+\\sigma_{p, 2}+C \\sqrt{p} \\sigma_{p}^{*}\\right)^{2 p}(N \\wedge J)^{-n p \\nu} \\\\\n& \\lesssim\\left(\\frac{12 \\sqrt{2} L_{M}}{C^{\\prime} p}\\right)^{2 p} r^{p}(N J)^{-p} h^{-2 m p}\\left(\\frac{N J^{p}}{h^{p}}+\\frac{N^{p} J}{h^{p}}+C^{2 p} p^{p} N J\\left(\\frac{N \\wedge J}{h}\\right)^{p}\\right)(N \\wedge J)^{-n p \\nu}\n\\end{aligned}\n$$\n\nNow we denote $\\widetilde{C}=C^{\\prime} / 12 \\sqrt{2} L_{M} \\sqrt{r}$ and choose $h \\asymp((N \\wedge J) / \\log ^{2}(N \\wedge J))^{-1 /(2 m+1)}$, $p=h^{-\\nu}$ (we also assume without loss of generality that $p \\geq 1$ ), then\n\n$$\n\\begin{aligned}\n& \\left(\\frac{12 \\sqrt{2} L_{M}}{C^{\\prime} p}\\right)^{2 p} r^{p}(N J)^{-p} h^{-2 m p}\\left(\\frac{N J^{p}}{h^{p}}+\\frac{N^{p} J}{h^{p}}+C^{2 p} p^{p} N J\\left(\\frac{N \\wedge J}{h}\\right)^{p}\\right) \\\\\n\\leq & \\left(\\frac{1}{\\widetilde{C}_{p}}\\right)^{2 p}\\left(N\\left(\\frac{N \\wedge J}{N \\log ^{2}(N \\wedge J)}\\right)^{p}+J\\left(\\frac{N \\wedge J}{J \\log ^{2}(N \\wedge J)}\\right)^{p}+C^{2 p} p^{p} N J\\left(\\frac{(N \\wedge J)^{2}}{N J \\log ^{2}(N \\wedge J)}\\right)^{p}\\right) \\\\\n\\leq & \\left(\\frac{1}{\\widetilde{C}_{p}}\\right)^{2 p}\\left(N+J+C^{2 p} p^{p} N J\\right)(\\log (N \\wedge J))^{-2 p} \\\\\n\\lesssim & \\left(\\frac{1}{\\widetilde{C}_{p}}\\right)^{2 p} C^{2 p} p^{p} N J\n\\end{aligned}\n$$\n\n$\\lesssim \\exp \\left(-h^{-\\nu}[2 \\log \\widetilde{C}-2 \\log C+\\nu \\log (1 / h)]+\\log N+\\log J\\right)$.\n\nChoose $C^{\\prime}=12 \\sqrt{2 r} e L_{M}$, then we have $2 \\log \\widetilde{C}-2 \\log C=2$. Since $h^{-\\nu} \\gg \\log N \\vee \\log J$ and $\\nu \\log (1 / h)>0$ for $h$ small enough, we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\frac{1}{N J} \\sup _{(\\mathbf{X}, \\mathbf{X}^{\\prime}) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right|>C^{\\prime} h^{m-\\nu}(N \\wedge J)^{\\alpha / 2+3 n \\nu / 2}\\right) \\\\\n= & \\mathbb{P}\\left(\\frac{1}{N J} \\sup _{(\\mathbf{X}, \\mathbf{X}^{\\prime}) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}(t)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right|>C^{\\prime} p h^{m}(N \\wedge J)^{\\alpha / 2+3 n \\nu / 2}\\right) \\\\\n& \\lesssim \\exp \\left(-h^{-\\nu}\\right)(N \\wedge J)^{-n p \\nu} \\\\\n& \\lesssim \\exp \\left(-(N \\wedge J)^{\\frac{\\nu}{2 m+2}}\\right)(N \\wedge J)^{-n \\nu}\n\\end{aligned}\n$$\n\nSo we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(I_{1, n}>C^{\\prime} h^{m-\\nu}(N \\wedge J)^{\\alpha / 2+3 n \\nu / 2}\\right) \\\\\n\\leq & \\sum_{t_{k} \\in[h, 1-h]} \\mathbb{P}\\left(\\frac{1}{N J} \\sup _{G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J} M_{i j}\\left(t_{k}\\right)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right|>C^{\\prime} h^{m-\\nu}(N \\wedge J)^{\\alpha / 2+3 n \\nu / 2}\\right) \\\\\n\\lesssim & \\frac{1}{\\Delta} \\exp \\left(-(N \\wedge J)^{\\frac{\\nu}{2 m+2}}\\right)(N \\wedge J)^{-n \\nu} \\\\\n\\lesssim & N J(N \\wedge J)^{\\frac{m+3 / 2}{2 m+1}+\\frac{1}{2}}(\\log (N \\wedge J))^{-\\frac{2 m+3}{2 m+1}} \\exp \\left(-(N \\wedge J)^{\\frac{\\nu}{2 m+2}}\\right)(N \\wedge J)^{-n \\nu} \\\\\n\\lesssim & N J(N \\wedge J)^{\\frac{m+3 / 2}{2 m+1}+\\frac{1}{2}-n \\nu} \\exp \\left(-(N \\wedge J)^{\\frac{\\nu}{2 m+2}}\\right)\n\\end{aligned}\n$$\n\nHence\n\n$$\n\\begin{aligned}\n& \\sum_{n=0}^{\\infty} \\mathbb{P}\\left(I_{1, n}>C^{\\prime} h^{m-\\nu}(N \\wedge J)^{\\alpha / 2+3 n \\nu / 2}\\right) \\\\\n\\leq & \\sum_{n=0}^{\\infty} N J(N \\wedge J)^{\\frac{m+3 / 2}{2 m+1}+\\frac{1}{2}-n \\nu} \\exp \\left(-(N \\wedge J)^{\\frac{\\nu}{2 m+2}}\\right)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =N J(N \\wedge J)^{\\frac{m+3 / 2}{2 m+1}+\\frac{1}{2}} \\exp \\left(-(N \\wedge J)^{\\frac{\\nu}{2 m+2}}\\right)\\left(1-(N \\wedge J)^{-\\nu}\\right)^{-1} \\\\\n& \\lesssim N J(N \\wedge J)^{\\frac{m+3 / 2}{2 m+1}+\\frac{1}{2}} \\exp \\left(-(N \\wedge J)^{\\frac{\\nu}{2 m+2}}\\right) \\\\\n& \\rightarrow 0\n\\end{aligned}\n$$\n\nwhere the last inequality holds since $N \\wedge J \\gg \\log (N \\vee J)$. This implies that\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sum_{n=0}^{\\infty}(N \\wedge J)^{-2(n-1) \\nu} I_{1, n} \\geq(N \\wedge J)^{2 \\nu+\\alpha / 2} C^{\\prime} h^{m-\\nu}\\left(1-(N \\wedge J)^{-\\nu / 2}\\right)^{-1}\\right) \\\\\n= & \\mathbb{P}\\left(\\sum_{n=0}^{\\infty}(N \\wedge J)^{-2(n-1) \\nu} I_{1, n} \\geq \\sum_{n=0}^{\\infty}(N \\wedge J)^{-2(n-1) \\nu} C^{\\prime} h^{m-\\nu}(N \\wedge J)^{\\alpha / 2+3 n \\nu / 2}\\right) \\\\\n= & \\mathbb{P}\\left(\\sum_{n=0}^{\\infty}(N \\wedge J)^{-2(n-1) \\nu} I_{1, n} \\geq C^{\\prime} h^{m-\\nu}(N \\wedge J)^{\\alpha / 2+3 \\nu / 2}\\left(1-(N \\wedge J)\\right)^{-\\nu / 2}\\right)^{-1}\\right) \\\\\n& \\rightarrow 0\n\\end{aligned}\n$$\n\nWe finally deduce that with high probability,\n\n$$\n\\begin{aligned}\n& \\sum_{n=0}^{\\infty}(N \\wedge J)^{-2(n-1) \\nu} I_{1, n} \\\\\n& \\lesssim p(N \\wedge J)^{2 \\nu+\\alpha / 2} C^{\\prime} h^{m-\\nu}\\left(1-(N \\wedge J)^{-\\nu / 2}\\right)^{-1} \\\\\n& \\lesssim(N \\wedge J)^{2 \\nu+\\alpha / 2-m /(2 m+1)+\\nu /(2 m+1)}(\\log (N \\wedge J))^{2(m-\\nu) /(2 m+1)} \\\\\n& \\lesssim(N \\wedge J)^{2 \\nu+\\alpha / 2-m /(2 m+1)+\\nu / 2 m}\n\\end{aligned}\n$$\n\nStep 2.2: Bound $I_{2, n}$ for any fixed $n$.\n\nSince $K(x)$ is $L_{K}$-Lipschitz, $K^{\\prime}(x)$ exists almost everywhere and $\\left|K^{\\prime}(x)\\right| \\leq L_{K}$. Then,\n\nunder $A_{i j}$ we have\n\n$$\n\\begin{aligned}\n\\left|\\frac{\\mathrm{d}}{\\mathrm{~d} t} \\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\right| \\leq & \\sum_{i=1}^{N} \\sum_{j=1}^{J} L_{K} \\frac{\\int_{0}^{1} \\mathrm{~d} Y_{i j}(s)}{h^{2}\\left|\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s\\right|} \\\\\n& +\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\frac{K(0) \\int_{0}^{1} \\mathrm{~d} Y_{i j}(s)}{\\left(\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s\\right)^{2}}\\left|\\frac{1}{h} K\\left(\\frac{1-t}{h}\\right)-\\frac{1}{h} K\\left(\\frac{-t}{h}\\right)\\right| \\\\\n\\leqq & \\frac{1}{h^{2}} \\int_{0}^{1} \\mathrm{~d} Y_{i j}(s) \\\\\n\\leqq & \\frac{1}{h^{3 / 2}} \\sqrt{N \\wedge J}\n\\end{aligned}\n$$\n\nThis implies that $M_{i j}(t)$ is Lipschitz continuous with Lipschitz constant $\\frac{C}{h^{3 / 2}} \\sqrt{N \\wedge J}$. Since $\\Delta=h^{m+3 / 2} /(\\sqrt{N \\wedge J} N J)$, we have\n\n$$\n\\begin{aligned}\nI_{2, n} & =\\sup _{\\left|t-t^{\\prime}\\right| \\leq \\Delta, t, t^{\\prime} \\in[h, 1-h]}\\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left(M_{i j}(t)-M_{i j}\\left(t^{\\prime}\\right)\\right)\\left(\\log \\frac{f\\left(X_{i j}^{\\prime}\\right)}{f(0)}-\\log \\frac{f\\left(X_{i j}\\right)}{f(0)}\\right)\\right| \\\\\n& \\lesssim \\sup _{\\left|t-t^{\\prime}\\right| \\leq \\Delta, t, t^{\\prime} \\in[h, 1-h]}\\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}}\\left|\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\right| M_{i j}(t)-M_{i j}\\left(t^{\\prime}\\right)\\left\\|\\left|X_{i j}^{\\prime}-X_{i j}\\right|\\right| \\\\\n& \\lesssim \\frac{\\Delta}{h^{3 / 2}} \\sqrt{N \\wedge J} \\sup _{\\left(\\mathbf{X}, \\mathbf{X}^{\\prime}\\right) \\in G_{2, n}} \\sqrt{N J}\\left\\|\\mathbf{X}^{\\prime}-\\mathbf{X}\\right\\|_{F} \\\\\n& \\lesssim \\frac{\\Delta}{h^{3 / 2}} \\sqrt{N \\wedge J} N J(N \\wedge J)^{\\alpha / 2+n \\nu} \\\\\n& \\leq h^{m}(N \\wedge J)^{\\alpha / 2+n \\nu}\n\\end{aligned}\n$$\n\nSo we have that with high probability\n\n$$\n\\sum_{n=0}^{\\infty}(N \\wedge J)^{-2(n-1) \\nu} I_{2, n} \\leq \\sum_{n=0}^{\\infty} h^{m}(N \\wedge J)^{\\alpha / 2+n \\nu-2(n-1) \\nu} \\lesssim(N \\wedge J)^{\\alpha / 2+3 \\nu-m /(2 m+1)}\n$$\n\nStep 3: Obtain the final bound.\n\nBy (A.20), (A.21), (A.23), (A.24) and (A.25), for any $\\nu>0$ small enough, we have\n\n$$\n\\frac{1}{N J} \\sup _{t \\in[h, 1-h]}\\left\\|\\widehat{\\mathbf{Z}}(t)-\\mathbf{Z}^{*}(t)\\right\\|_{F}^{2}=O_{p}\\left((N \\wedge J)^{\\alpha / 2-m /(2 m+1)+\\max \\{3 \\nu, 3 \\nu / 2+\\nu / 2 m\\}}\\right)\n$$\n\nwhich also implies that\n\n$$\n\\frac{1}{N J} \\sup _{t \\in[h, 1-h]}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2}=o_{p}\\left((N \\wedge J)^{\\alpha / 2-m /(2 m+1)+3 \\nu}\\right)\n$$\n\nTherefore, as long as $\\alpha>-2 m /(2 m+1)$, we can repeat the above procedure to obtain a sharper rate. Finally, we conclude that for any $\\delta>0$,\n\n$$\n\\frac{1}{N J} \\sup _{t \\in[h, 1-h]}\\left\\|\\widehat{\\mathbf{X}}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2}=O_{p}\\left((N \\wedge J)^{-2 m /(2 m+1)+\\delta}\\right)\n$$\n\nwhich completes the proof.\n\nCorollary 1. Let $\\sigma_{1} \\geq \\ldots \\geq \\sigma_{r}$ be the non-zero singular values of $\\int_{h}^{1-h} \\mathbf{X}^{*}(t) \\mathrm{d} t$ and let diagonal matrix $\\angle\\left(\\mathbf{A}^{*}, \\widehat{\\mathbf{A}}\\right) \\in \\mathbb{R}^{r \\times r}$ contain all $r$ principal angles between $\\mathbf{A}^{*}$ and $\\widehat{\\mathbf{A}}$ (Stewart and Sun, 1990). Under Conditions 1-4:\n(i) (Dependent case) Assume $J=O(N)$ and recall $\\phi(J)$ from Condition 4(iii). For any $\\delta>0$, choose $h \\asymp(J / \\phi(J))^{-1 /(2 m+1)+\\delta / m}$. Assume that $N J(J / \\phi(J))^{-m /(2 m+1)+\\delta}=$ $o\\left(\\sigma_{r}^{2}\\right)$. Then, as $N$ and $J$ go to infinity, we have\n\n$$\n\\left\\|\\sin \\left(\\angle\\left(\\mathbf{A}^{*}, \\widehat{\\mathbf{A}}\\right)\\right)\\right\\|_{F}=o_{p}(1)\n$$\n\n(ii) (Independent case) Assume that $\\phi(J)=1$ in Condition 4(iii) and $\\log (N \\vee J) \\ll$ $N \\wedge J$. For any $\\delta>0$, choose $h \\asymp((N \\wedge J) /\\left(\\log ^{2}(N \\wedge J)\\right))^{-1 /(2 m+1)}$. Assume that\n\n$N J(N \\wedge J)^{-2 m /(2 m+1)+\\delta}=o\\left(\\sigma_{r}^{2}\\right)$. Then, as $N$ and $J$ go to infinity, we have\n\n$$\n\\left\\|\\sin \\left(\\angle\\left(\\mathbf{A}^{*}, \\widehat{\\mathbf{A}}\\right)\\right)\\right\\|_{F}=o_{p}(1)\n$$\n\nProof of Corollary 1. We only prove part (i). By Theorem 1(i), we have\n\n$$\n\\begin{aligned}\n\\left\\|\\int_{h}^{1-h} \\mathbf{X}^{*}(t) \\mathrm{d} t-\\int_{h}^{1-h} \\widehat{\\mathbf{X}}(t) \\mathrm{d} t\\right\\|_{F} & \\leq \\int_{h}^{1-h}\\left\\|\\mathbf{X}^{*}(t)-\\widehat{\\mathbf{X}}(t)\\right\\|_{F} \\mathrm{~d} t \\\\\n& \\leq\\left(\\int_{h}^{1-h}\\left\\|\\mathbf{X}^{*}(t)-\\widehat{\\mathbf{X}}(t)\\right\\|_{F}^{2} \\mathrm{~d} t\\right)^{1 / 2} \\\\\n& =O_{p}\\left(N J(J / \\phi(J))^{-m /(2 m+1)+\\delta}\\right)^{1 / 2}\n\\end{aligned}\n$$\n\nwhich is of order $o_{p}\\left(\\sigma_{r}\\right)$ by our assumption. We note that $\\int_{h}^{1-h} \\mathbf{X}^{*}(t) \\mathrm{d} t=\\left(\\int_{h}^{1-h} \\boldsymbol{\\Theta}^{*}(t) \\mathrm{d} t\\right)\\left(\\mathbf{A}^{*}\\right)^{\\top}$ and $\\int_{h}^{1-h} \\widehat{\\mathbf{X}}(t) \\mathrm{d} t=\\left(\\int_{h}^{1-h} \\widehat{\\boldsymbol{\\Theta}}(t) \\mathrm{d} t\\right)(\\widehat{\\mathbf{A}})^{\\top}$. Hence the result is proved by applying Weyl's theorem and Wedin's $\\sin \\Theta$ theorem (Stewart and Sun, 1990).", "tables": {}, "images": {}}, {"section_id": 14, "text": "# A. 2 Proof of Theorem 2 \n\nThe proof of Theorem 2 is based on the following lemma, whose proof will be provided later in the Appendix B.\n\nLemma 3 (Varshamov-Gilbert). Let $\\Omega=\\left\\{\\omega=\\left(\\omega_{1}, \\cdots, \\omega_{N}\\right): \\omega_{j} \\in\\{0,1\\}\\right\\}$. Suppose that $N \\geq 8$. There exists $\\omega^{0}, \\cdots, \\omega^{M} \\in \\Omega$ such that (i) $\\omega^{0}=(0, \\cdots, 0)$, (ii) $M \\geq 2^{N / 8}$, and (iii) $H\\left(\\omega^{j}, \\omega^{k}\\right) \\geq N / 8$ for $0 \\leq j<k \\leq M$, here $H(\\omega, \\nu)=\\sum_{i=1}^{N} I\\left(\\omega_{i} \\neq \\nu_{i}\\right)$ is the Hamming Distance between $\\omega$ and $\\nu$. We call $\\Omega^{\\prime}=\\left\\{\\omega^{0}, \\cdots, \\omega^{M}\\right\\}$ a pruned hypercube.\n\nProof of Theorem 2. We proceed with the following two steps:\n\nStep 1. Packing Set Construction: First, set $g(t)$ to be a sufficiently smooth function\n\nwhich is supported on $[0,1]$, satisfying $\\sup _{[0,1]}|g(t)| \\leq M$ and $\\sup _{[0,1]}\\left|g^{(m)}(t)\\right| \\leq M$. Let $n \\in \\mathbb{N}$ to be determined later and define\n\n$$\ng_{k}(t)=\\frac{1}{n^{m}} g(n t-(k-1)), 1 \\leq k \\leq n\n$$\n\nThen $g_{k}(t)$ is supported on $[(k-1) / n, k / n], \\sup _{[0,1]}\\left|g_{k}(t)\\right| \\leq M$ and\n\n$$\n\\left|\\frac{\\mathrm{d}^{m}}{\\mathrm{~d} t^{m}} g_{k}(t)\\right|=\\left|g^{(m)}(n t-(k-1))\\right| \\leq M, \\forall t \\in[0,1]\n$$\n\nNow without loss of generality we assume $N \\leq J$. According to Lemma 3, as long as $n r J \\geq 8$, we can construct a pruned hypercube $\\Omega^{\\prime}$ of $\\Omega=\\{0,1\\}^{r \\times J \\times n}$, with $\\left|\\Omega^{\\prime}\\right| \\geq 2^{n r J / 8}$ and $\\forall \\omega, \\nu \\in \\Omega^{\\prime}, \\omega \\neq \\nu$, we have\n\n$$\n\\sum_{i=1}^{r} \\sum_{j=1}^{J} \\sum_{k=1}^{n} I\\left(\\omega_{i j}^{k} \\neq \\nu_{i j}^{k}\\right) \\geq \\frac{n r J}{8}\n$$\n\nNow we construct our packing set through $\\Omega^{\\prime}$ as $G_{T}^{\\prime}=\\left\\{\\mathbf{X}^{\\omega}(t): \\omega \\in \\Omega^{\\prime}\\right\\}$. We know that $\\left|G_{T}^{\\prime}\\right| \\geq 2^{n r J / 8}$. And $\\forall \\omega \\in \\Omega^{\\prime}, \\mathbf{X}^{\\omega}(t)$ is an $N \\times J$ matrix-valued function on $[0,1]$, defined element-wisely by\n\n$$\nX_{i j}^{\\omega}(t)=\\sum_{k=1}^{n} \\omega_{i(\\bmod r), j}^{k} g_{k}(t)\n$$\n\nIntuitively, for $1 \\leq i \\leq r, 1 \\leq j \\leq J$ we define the first $r \\times J$ block of $\\mathbf{X}^{\\omega}(t)$ by $X_{i j}^{\\omega}(t)=$ $\\sum_{k=1}^{n} \\omega_{i j}^{k} g_{k}(t)$, then copy this block several times until the whole matrix is defined. It's easy to verify that $\\mathbf{X}^{\\omega} \\in \\mathcal{G}$ and satisfies Condition 2, and for any $\\omega, \\nu \\in \\Omega^{\\prime}, \\omega \\neq \\nu$, we have\n\n$$\n\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\mathbf{X}^{\\omega}(t)-\\mathbf{X}^{\\nu}(t)\\right\\|_{F}^{2} \\mathrm{~d} t=\\frac{1}{N J} \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{0}^{1}\\left(X_{i j}^{\\omega}(t)-X_{i j}^{\\nu}(t)\\right)^{2} \\mathrm{~d} t\n$$\n\n$$\n\\begin{aligned}\n& \\geq \\frac{1}{N J}\\left[\\frac{N}{r}\\right] \\sum_{i=1}^{r} \\sum_{j=1}^{J} \\int_{0}^{1}\\left(\\sum_{k=1}^{n}\\left(\\omega_{i j}^{k}-\\nu_{i j}^{k}\\right) g_{k}(t)\\right)^{2} \\mathrm{~d} t \\\\\n& =\\frac{1}{N J}\\left[\\frac{N}{r}\\right] \\sum_{i=1}^{r} \\sum_{j=1}^{J} \\int_{0}^{1} \\sum_{k=1}^{n}\\left(\\omega_{i j}^{k}-\\nu_{i j}^{k}\\right)^{2} g_{k}(t)^{2} \\mathrm{~d} t \\\\\n& =\\frac{1}{N J n^{2 m+1}}\\left[\\frac{N}{r}\\right] \\sum_{i=1}^{r} \\sum_{j=1}^{J} \\sum_{k=1}^{n} I\\left(\\omega_{i j}^{k} \\neq \\nu_{i j}^{k}\\right) \\int_{0}^{1} g(t)^{2} \\mathrm{~d} t \\\\\n& \\geq \\frac{r}{8 N n^{2 m}}\\left[\\frac{N}{r}\\right] \\int_{0}^{1} g(t)^{2} \\mathrm{~d} t \\geq \\frac{1}{16 n^{2 m}} \\int_{0}^{1} g(t)^{2} \\mathrm{~d} t\n\\end{aligned}\n$$\n\nAnd similarly we can show that\n\n$$\n\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\mathbf{X}^{\\omega}(t)-\\mathbf{X}^{\\nu}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\leq \\frac{1}{n^{2 m}} \\int_{0}^{1} g(t)^{2} \\mathrm{~d} t\n$$\n\nStep 2. Utilize Fano's inequality to give a lower bound:\nLet $\\epsilon^{2}=\\int_{0}^{1} g(t)^{2} \\mathrm{~d} t /\\left(64 n^{2 m}\\right)$. For any $\\mathbf{X}^{\\omega}(t) \\in G_{T}^{\\prime}$, denote its estimator as $\\widehat{\\mathbf{X}}^{\\omega}(t)$ and we choose its $L^{2}$-projection to $G_{T}^{\\prime}$ as\n\n$$\n\\mathbf{X}^{\\omega^{*}}(t)=\\underset{\\omega^{*} \\in \\Omega^{\\prime}}{\\arg \\min } \\frac{1}{N J} \\int_{0}^{1}\\left\\|\\mathbf{X}^{\\omega^{*}}(t)-\\widehat{\\mathbf{X}}^{\\omega}(t)\\right\\|_{F}^{2} \\mathrm{~d} t\n$$\n\nWe note that if $\\mathbf{X}^{\\omega^{*}}(t) \\neq \\mathbf{X}^{\\omega}(t)$, and\n\n$$\n\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\widehat{\\mathbf{X}}^{\\omega}(t)-\\mathbf{X}^{\\omega}(t)\\right\\|_{F}^{2} \\mathrm{~d} t<\\epsilon^{2}\n$$\n\nthen we must have\n\n$$\n\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\mathbf{X}^{\\omega^{*}}(t)-\\widehat{\\mathbf{X}}^{\\omega}(t)\\right\\|_{F}^{2} \\mathrm{~d} t>\\epsilon^{2}>\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\widehat{\\mathbf{X}}^{\\omega}(t)-\\mathbf{X}^{\\omega}(t)\\right\\|_{F}^{2} \\mathrm{~d} t\n$$\n\nsince\n\n$$\n\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\mathbf{X}^{\\omega}(t)-\\mathbf{X}^{\\omega^{*}}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\geq 4 \\epsilon^{2}\n$$\n\nas proved in Step 1. However, this contradicts the definition of $\\mathbf{X}^{\\omega^{*}}(t)$ and we conclude that\n\n$$\n\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\widetilde{\\mathbf{X}}^{\\omega}(t)-\\mathbf{X}^{\\omega}(t)\\right\\|_{F}^{2} \\mathrm{~d} t<\\epsilon^{2} \\text { implies } \\mathbf{X}^{\\omega^{*}}(t)=\\mathbf{X}^{\\omega}(t)\n$$\n\nNow we proceed our proof by contradiction, assume that for any $\\mathbf{X}^{\\omega}(t) \\in G_{T}^{\\prime}$,\n\n$$\n\\mathbb{P}\\left(\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\widetilde{\\mathbf{X}}^{\\omega}(t)-\\mathbf{X}^{\\omega}(t)\\right\\|_{F}^{2} \\mathrm{~d} t<\\epsilon^{2}\\right)>\\frac{1}{2}\n$$\n\nthen we immediately obtain that\n\n$$\n\\mathbb{P}\\left(\\mathbf{X}^{\\omega^{*}}(t)=\\mathbf{X}^{\\omega}(t)\\right)>\\frac{1}{2} \\Rightarrow \\mathbb{P}\\left(\\mathbf{X}^{\\omega^{*}}(t) \\neq \\mathbf{X}^{\\omega}(t)\\right)<\\frac{1}{2}\n$$\n\nhere the probability is taken over a uniform prior on $G_{T}^{\\prime}$ and the distribution of $\\mathbf{Y}^{\\omega}(t)$, the multivariate Poisson process associated to $\\mathbf{X}^{\\omega}(t)$.\n\nSince $\\mathbf{X}^{\\omega^{*}}(t)$ only depends on $\\mathbf{Y}^{\\omega}(t)$, using Fano's inequality gives us\n\n$$\n\\mathbb{P}\\left(\\mathbf{X}^{\\omega^{*}}(t) \\neq \\mathbf{X}^{\\omega}(t)\\right) \\geq 1-\\frac{\\log 2+\\left|G_{T}^{\\prime}\\right|^{-2} \\sum_{\\omega, \\nu \\in \\Omega^{\\prime}} D_{K L}\\left(\\mathbf{Y}^{\\omega}(t)\\left\\|\\mathbf{Y}^{\\nu}(t)\\right)\\right.}{\\log \\left|G_{T}^{\\prime}\\right|}\n$$\n\nNote that for multivariate Poisson processes $\\mathbf{Y}^{\\omega}(t)$ and $\\mathbf{Y}^{\\nu}(t)$, we have\n\n$$\n\\begin{aligned}\n& D_{K L}\\left(\\mathbf{Y}^{\\omega}(t)\\left\\|\\mathbf{Y}^{\\nu}(t)\\right)=\\sum_{i=1}^{N} \\sum_{j=1}^{J} D_{K L}\\left(Y_{i j}^{\\omega}(t)\\left\\|Y_{i j}^{\\nu}(t)\\right)\\right.\\right. \\\\\n= & \\left.\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{0}^{1}\\left(X_{i j}^{\\omega}(t) \\log \\frac{X_{i j}^{\\omega}(t)}{X_{i j}^{\\nu}(t)}-\\left(X_{i j}^{\\omega}(t)-X_{i j}^{\\nu}(t)\\right)\\right) \\mathrm{d} t\\right)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& \\leq \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\sup _{|x| \\leq M} \\frac{f^{\\prime}(x)^{2}}{f(x)} \\int_{0}^{1}\\left(X_{i j}^{\\omega}(t)-X_{i j}^{\\nu}(t)\\right)^{2} \\mathrm{~d} t \\\\\n& =\\sup _{|x| \\leq M} \\frac{f^{\\prime}(x)^{2}}{f(x)} \\int_{0}^{1}\\left\\|\\mathbf{X}^{\\omega}(t)-\\mathbf{X}^{\\nu}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\leq \\sup _{|x| \\leq M} \\frac{f^{\\prime}(x)^{2}}{f(x)} \\frac{N J}{n^{2 m}} \\int_{0}^{1} g(t)^{2} \\mathrm{~d} t\n\\end{aligned}\n$$\n\nNote that $\\log \\left|G_{T}^{\\prime}\\right| \\geq n r J \\log 2 / 8$, therefore\n\n$$\n\\mathbb{P}\\left(\\mathbf{X}^{\\omega^{*}}(t) \\neq \\mathbf{X}^{\\omega}(t)\\right) \\geq 1-\\frac{8}{n r J}-\\frac{8 N \\sup _{|x| \\leq M} \\frac{f^{\\prime}(x)^{2}}{f(x)} \\int_{0}^{1} g(t)^{2} \\mathrm{~d} t}{n^{2 m+1} r \\log 2}\n$$\n\nNow as long as $r J \\geq 32$ (if $r J<32$ there are only finitely many cases) we choose\n\n$$\nn=\\left[\\left(\\frac{32}{\\log 2} \\sup _{|x| \\leq M} \\frac{f^{\\prime}(x)^{2}}{f(x)} \\int_{0}^{1} g(t)^{2} \\mathrm{~d} t\\right)^{1 /(2 m+1)}\\left(\\frac{N}{r}\\right)^{1 /(2 m+1)}\\right]+1\n$$\n\ngetting to $\\mathrm{P}\\left(\\mathbf{X}^{\\omega^{*}}(t) \\neq \\mathbf{X}^{\\omega}(t)\\right) \\geq 1 / 2$, and existence of an absolute constant $C$ such that\n\n$$\n\\epsilon^{2}=\\frac{\\int_{0}^{1} g(t)^{2} \\mathrm{~d} t}{64 n^{2 m}} \\geq C\\left(\\frac{N}{r}\\right)^{-2 m /(2 m+1)}\n$$\n\na contradiction. Therefore $\\exists \\mathbf{X}^{\\omega}(t) \\in G_{T}^{\\prime}$ such that\n\n$$\n\\mathbb{P}\\left(\\frac{1}{N J} \\int_{0}^{1}\\left\\|\\widetilde{\\mathbf{X}}^{\\omega}(t)-\\mathbf{X}^{\\omega}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\geq C\\left(\\frac{N}{r}\\right)^{-2 m /(2 m+1)}\\right) \\geq \\frac{1}{2}\n$$\n\nwhich completes the proof.", "tables": {}, "images": {}}, {"section_id": 15, "text": "# A. 3 Proof of Theorem 3 \n\nProof of Theorem 3. We first prove the first part of Theorem 3.\n\nWe only need to prove that $\\operatorname{IC}\\left(r^{*}\\right)>\\operatorname{IC}(r)$ holds with probability converging to 1 for any $r \\in \\mathcal{R}$ and $r \\neq r^{*}$. We denote $\\widetilde{\\mathbf{X}}^{(r)}$ as the estimator obtained when we fix the rank to be\n\n$r$. Then we separately discuss the case when $r$ is smaller or bigger than $r^{*}$. For notational simplicity, we define\n\n$$\n\\mathcal{J}_{t, h}(\\mathbf{X})=\\sum_{i=1}^{N} \\sum_{j=1}^{J} f\\left(X_{i j}(t)\\right) \\log f\\left(X_{i j}(t)\\right)\n$$\n\nand define\n\n$$\n\\mathcal{G}_{r}=\\left\\{\\mathbf{X}(\\cdot)=\\boldsymbol{\\Theta}(\\cdot) \\mathbf{A}^{T}: \\boldsymbol{\\Theta} \\in L_{N \\times r}^{2}[0,1], \\mathbf{A} \\in \\mathbb{R}^{J \\times r}, \\sup _{t \\in[0,1]}\\|\\boldsymbol{\\Theta}(t)\\|_{2 \\rightarrow \\infty} \\leq M^{1 / 2},\\|\\mathbf{A}\\|_{2 \\rightarrow \\infty} \\leq M^{1 / 2}\\right\\}\n$$\n\nfor fixed $r \\in \\mathcal{R}$. Now we discuss the two cases when $r$ is smaller or larger than $r^{*}$ separately.\n\nCase 1: $r<r^{*}$.\n\nWe have\n\n$$\n\\begin{aligned}\n& \\operatorname{IC}\\left(r^{*}\\right)-\\operatorname{IC}(r) \\\\\n= & -2 \\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{\\left(r^{*}\\right)}(t)\\right)-\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)\\right) \\mathrm{d} t+v\\left(N, J, r^{*}\\right)-v(N, J, r) \\\\\n\\leq & -2 \\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)\\right) \\mathrm{d} t+v\\left(N, J, r^{*}\\right)-v(N, J, r) \\\\\n\\leq & 4 \\int_{h}^{1-h} \\sup _{\\mathbf{X} \\in \\mathcal{G}_{r^{*}}}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\mathrm{d} t+4 \\sup _{\\mathbf{X} \\in \\mathcal{G}_{r^{*}}}\\left|\\int_{h}^{1-h}\\left[\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X}(t))-\\mathcal{J}_{t, h}(\\mathbf{X}(t))\\right] \\mathrm{d} t\\right| \\\\\n& -2 \\int_{h}^{1-h}\\left(\\mathcal{J}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathcal{J}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)\\right) \\mathrm{d} t+\\left(v\\left(N, J, r^{*}\\right)-v(N, J, r)\\right) \\\\\ntriangleq & 4 I_{1}+4 I_{2}-2 I_{3}+I_{4}\n\\end{aligned}\n$$\n\nIn the following steps, we bound each of $I_{1}, I_{2}, I_{3}, I_{4}$.\n\nStep 1: Bound $I_{1}$.\n\nBy the proof of the first part of Theorem 1, we have shown that $I_{1}=O_{p}\\left(N J h^{m}\\right)$.\n\nStep 2: Bound $I_{2}$.\n\nFor any $\\mathbf{X} \\in \\mathcal{G}_{r^{*}}$, by Lemma 2 we have\n\n$$\n\\begin{aligned}\n& \\left|\\int_{h}^{1-h}\\left(\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X}(t))-\\mathcal{J}_{t, h}(\\mathbf{X}(t))\\right) \\mathrm{d} t\\right| \\\\\n\\leq & \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) f\\left(X_{i j}(s)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-f\\left(X_{i j}(t)\\right)\\right|\\left|\\log f\\left(X_{i j}(t)\\right)\\right| \\mathrm{d} t \\\\\n\\leq & T N J C_{m} h^{m} \\sup _{|x| \\leq M}|\\log f(x)|\n\\end{aligned}\n$$\n\nSo we have $I_{2} \\lesssim N J h^{m}$.\n\nStep 3: For the third term, for any $t$ in $[h, 1-h]$, by Lemma 1 we have\n\n$$\n\\begin{aligned}\n\\left\\|\\widehat{\\mathbf{X}}^{(r)}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} & =\\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left(X_{i j}^{(r)}(t)-X_{i j}^{*}(t)\\right)^{2} \\\\\n& \\leq 4 \\beta_{M} \\sum_{i=1}^{N} \\sum_{j=1}^{J}\\left(f\\left(X_{i j}^{*}(t)\\right) \\log \\frac{f\\left(X_{i j}^{*}(t)\\right)}{f\\left(\\widehat{X}_{i j}^{(r)}(t)\\right)}-\\left(f\\left(X_{i j}^{*}(t)\\right)-f\\left(\\widehat{X}_{i j}^{\\left(r^{*}\\right)}(t)\\right)\\right)\\right) \\\\\n& =4 \\beta_{M}\\left(\\mathcal{J}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathcal{J}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)\\right)\n\\end{aligned}\n$$\n\nSo we have\n\n$$\n\\begin{aligned}\nI_{3} & =\\int_{h}^{1-h}\\left(\\mathcal{J}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathcal{J}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)\\right) \\mathrm{d} t \\\\\n& \\geq \\frac{1}{\\beta_{M}} \\int_{h}^{1-h}\\left\\|\\widehat{\\mathbf{X}}^{(r)}(t)-\\mathbf{X}^{*}(t)\\right\\|_{F}^{2} \\mathrm{~d} t \\\\\n& \\geq \\frac{1}{\\beta_{M}} \\int_{h}^{1-h} \\sigma_{r^{*} j}^{2} \\mathrm{~d} t\n\\end{aligned}\n$$\n\nwhere in the last step we use Weyl's inequality for singular values.\n\nStep 4: For the last term $I_{4}$, by the assumption of Theorem 3, we have\n\n$$\nI_{4}=v\\left(N, J, r^{*}\\right)-v(N, J, r)=o\\left(\\int_{h}^{1-h} \\sigma_{r^{*} j}^{2} \\mathrm{~d} t\\right)\n$$\n\nSince $N J(J / \\phi(J))^{-m /(2 m+1)+\\delta}=o(u(N, J, r))$ for some $\\delta>0$, for $h=(\\phi(J) / J)^{(1-\\delta) /(2 m+1)}$, by (A.26) we have $\\lim _{N, J \\rightarrow \\infty} \\mathbb{P}\\left(\\operatorname{IC}\\left(r^{*}\\right)>\\operatorname{IC}(r)\\right)=0$.\n\nCase 2: $r^{*}<r$.\n\nDenote maximum element in $\\mathcal{R}$ by $r_{\\max }$. Then,\n\n$$\n\\begin{aligned}\n& \\operatorname{IC}\\left(r^{*}\\right)-\\operatorname{IC}(r) \\\\\n= & -2 \\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{\\left(r^{*}\\right)}(t)\\right)-\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)\\right) \\mathrm{d} t+v\\left(N, J, r^{*}\\right)-v(N, J, r) \\\\\n\\leq & -2 \\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)\\right) \\mathrm{d} t+v\\left(N, J, r^{*}\\right)-v(N, J, r) \\\\\n\\leq & 4 \\int_{h}^{1-h} \\sup _{\\mathbf{X} \\in \\mathcal{G}_{r_{\\max }}}\\left|\\mathcal{L}_{t, h}(\\mathbf{X})-\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X})\\right| \\mathrm{d} t+4 \\sup _{\\mathbf{X} \\in \\mathcal{G}_{r_{\\max }}}\\left|\\int_{h}^{1-h}\\left(\\mathbb{E} \\mathcal{L}_{t, h}(\\mathbf{X}(t))-\\mathcal{J}_{t, h}(\\mathbf{X}(t))\\right) \\mathrm{d} t\\right| \\\\\n& +2\\left|\\int_{h}^{1-h}\\left(\\mathcal{J}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)-\\mathcal{J}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)\\right) \\mathrm{d} t\\right|-\\left(v(N, J, r)-v\\left(N, J, r^{*}\\right)\\right) \\\\\n& \\triangleq 4 I_{1}+4 I_{2}+2 I_{3}-I_{4}\n\\end{aligned}\n$$\n\nWe can use the same method as in Case 1 to prove that $I_{1} / N J=O_{p}\\left((J / \\phi(J))^{-m /(2 m+1)+\\delta}\\right)$ and $I_{2}=O\\left((J / \\phi(J))^{-m /(2 m+1)+\\delta}\\right)$. Moreover, by the proof of the first part in Theorem 1, we have $I_{3}=O_{p}\\left((J / \\phi(J))^{-m /(2 m+1)+\\delta}\\right)$. By the assumption of Theorem 3, we have\n\n$$\n\\lim _{n \\rightarrow \\infty} \\frac{1}{N J} I_{4} /(J / \\phi(J))^{-m /(2 m+1)+\\delta} \\rightarrow \\infty\n$$\n\nSo by (A.27) we have $\\lim _{N, J \\rightarrow \\infty} \\mathbb{P}\\left(\\operatorname{IC}\\left(r^{*}\\right)>\\operatorname{IC}(r)\\right)=0$. Hence by the proof in Case 1 and Case 2, we verified that\n\n$$\n\\lim _{N, J \\rightarrow \\infty} \\mathbb{P}\\left(\\widehat{r}=r^{*}\\right)=1\n$$\n\nThen we prove the second part of Theorem 3 similarly. We separately discuss the case when $r<r^{*}$ and $r^{*}<r$. The proof in the first case is the same as that of the first part of Theorem 3, so we only discuss the second case.\n\nFor convenience, we denote $Z_{i j}(t)=\\log f\\left(X_{i j}(t)\\right), \\mathcal{L}_{t, h}(\\mathbf{Z})=\\mathcal{L}_{t, h}(\\mathbf{X})$, and so on. We omit $\\widehat{\\mathbf{Z}}^{(r)}$ as $\\widehat{\\mathbf{Z}}$. By the proof of the second part in Theorem 1, for any $\\delta>0$ we have\n\n$$\n\\sup _{t \\in[h, 1-h]}\\left\\|\\widehat{\\mathbf{Z}}(t)-\\mathbf{Z}^{*}(t)\\right\\|_{F}^{2}=O_{p}\\left(N J(N \\wedge J)^{-2 m /(2 m+1)+\\delta}\\right)\n$$\n\nUsing Taylor's expansion, we have\n\n$$\n\\begin{aligned}\n& 0 \\leq \\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}(\\widehat{\\mathbf{Z}}(t))-\\mathcal{L}_{t, h}\\left(\\mathbf{Z}^{*}(t)\\right)\\right) \\mathrm{d} t \\\\\n= & \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\\left(\\widehat{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right)-\\left(f\\left(\\widehat{X}_{i j}(t)\\right)-f\\left(X_{i j}^{*}(t)\\right)\\right)\\right) \\mathrm{d} t \\\\\n= & \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h}\\left(\\frac{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} Y_{i j}(s)}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-f\\left(X_{i j}^{*}(t)\\right)\\right)\\left(\\widehat{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right) \\mathrm{d} t \\\\\n& -\\sum_{i=1}^{N} \\sum_{j=1}^{J} \\int_{h}^{1-h} \\frac{e^{\\widehat{Z}_{i j}(t)}}{2}\\left(\\widehat{Z}_{i j}(t)-Z_{i j}^{*}(t)\\right)^{2} \\mathrm{~d} t \\\\\n& \\triangleq I_{1}-I_{2}\n\\end{aligned}\n$$\n\nwhere $\\widehat{Z}_{i j}(t)$ is some real number between $Z_{i j}^{*}(t)$ and $\\widehat{Z}_{i j}(t)$. By the proof of the second part in Theorem $1, I_{1}=O_{p}\\left(N J(N \\wedge J)^{-2 m /(2 m+1)+\\delta}\\right)$. Since $e^{\\widehat{Z}_{i j}(t)} \\geq \\min \\left(f\\left(X_{i j}^{*}(t)\\right), f\\left(\\widehat{X}_{i j}(t)\\right)\\right) \\geq$ $\\inf _{|x| \\leq M} f(x)$, it follows that\n\n$$\n\\left|I_{2}\\right| \\lesssim T \\sup _{t \\in[h, 1-h]}\\left\\|\\widehat{\\mathbf{Z}}(t)-\\mathbf{Z}^{*}(t)\\right\\|_{F}^{2}=O_{p}\\left(N J(N \\wedge J)^{-2 m /(2 m+1)+\\delta}\\right)\n$$\n\nThen by (A.28) we have\n\n$$\n\\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{Z}}^{(r)}(t)\\right)-\\mathcal{L}_{t, h}\\left(\\mathbf{Z}^{*}(t)\\right)\\right) \\mathrm{d} t=O_{p}\\left(N J(N \\wedge J)^{-2 m /(2 m+1)+\\delta}\\right)\n$$\n\nSo we proved that for any $r^{*} \\leq r \\leq r_{\\max }$\n\n$$\n\\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)-\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)\\right) \\mathrm{d} t=O_{p}\\left(N J(N \\wedge J)^{-2 m /(2 m+1)+\\delta}\\right)\n$$\n\nfor $\\delta>0$ small enough. Since we have\n\n$$\n\\begin{aligned}\n& \\operatorname{IC}\\left(r^{*}\\right)-\\operatorname{IC}(r) \\\\\n= & -2 \\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{\\left(r^{*}\\right)}(t)\\right)-\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)\\right) \\mathrm{d} t+v\\left(N, J, r^{*}\\right)-v(N, J, r) \\\\\n\\leq & -2\\left[\\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{\\left(r^{*}\\right)}(t)\\right)-\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)\\right) \\mathrm{d} t-\\int_{h}^{1-h}\\left(\\mathcal{L}_{t, h}\\left(\\widehat{\\mathbf{X}}^{(r)}(t)\\right)-\\mathcal{L}_{t, h}\\left(\\mathbf{X}^{*}(t)\\right)\\right) \\mathrm{d} t\\right]+u(N, J, r) \\\\\n= & O_{p}\\left(N J(N \\wedge J)^{-2 m /(2 m+1)+\\delta}\\right)+u(N, J, r)\n\\end{aligned}\n$$\n\nand $N J(N \\wedge J)^{-2 m /(2 m+1)+\\delta}=o(u(N, J, r))$, we have $\\lim _{N, J \\rightarrow \\infty} \\mathbb{P}\\left(\\operatorname{IC}\\left(r^{*}\\right)>\\operatorname{IC}(r)\\right)=0$.\nThis completes the proof of Theorem 3.", "tables": {}, "images": {}}, {"section_id": 16, "text": "# B Proof of lemmas \n\nProof of Lemma 1. Using the well-known inequality $-\\log x \\geq 1-x$, we obtain that\n\n$$\n\\begin{aligned}\nf(a) \\log \\frac{f(a)}{f(b)}-(f(a)-f(b)) & =-2 f(a) \\log \\frac{\\sqrt{f(b)}}{\\sqrt{f(a)}}-(f(a)-f(b)) \\\\\n& \\geq 2 f(a)\\left(1-\\frac{\\sqrt{f(b)}}{\\sqrt{f(a)}}\\right)-(f(a)-f(b)) \\\\\n& =(\\sqrt{f(a)}-\\sqrt{f(b)})^{2}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =\\left(\\int_{b}^{a} \\frac{f^{\\prime}(t)}{2 \\sqrt{f(t)}} \\mathrm{d} t\\right)^{2} \\\\\n& \\geq(a-b)^{2} \\inf _{|x| \\leq \\alpha} \\frac{f^{\\prime}(x)^{2}}{4 f(x)}\n\\end{aligned}\n$$\n\nwhich implies that $(a-b)^{2} \\leq 4 \\beta_{\\alpha}\\left(f(a) \\log \\frac{f(a)}{f(b)}-(f(a)-f(b))\\right)$.\nProof of Lemma 2. Using Taylor series expansion, we obtain that\n\n$$\n\\begin{aligned}\n& \\frac{\\int_{0}^{1} K_{h}(t-s) f\\left(X_{i j}(s)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-f\\left(X_{i j}(t)\\right)=\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(f\\left(X_{i j}(s)\\right)-f\\left(X_{i j}(t)\\right)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\\\\n= & \\sum_{k=1}^{m-1} \\frac{1}{k!} \\frac{\\mathrm{d}^{k}}{\\mathrm{~d} t^{k}} f\\left(X_{i j}(t)\\right) \\frac{\\int_{0}^{1} K_{h}(t-s)(s-t)^{k} \\mathrm{~d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}+\\frac{1}{m!} \\frac{\\int_{0}^{1} K_{h}(t-s) \\frac{\\mathrm{d}^{m}}{\\mathrm{~d} s^{m}} f\\left(X_{i j}(u)\\right)(s-t)^{m} \\mathrm{~d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\n\\end{aligned}\n$$\n\nwhere $u=\\theta s+(1-\\theta) t$. Now under Condition 3, if $h \\leq t \\leq 1-h$, it's easy to show that for $k=1, \\cdots, m-1$,\n\n$$\n\\int_{0}^{1} K_{h}(t-s)(s-t)^{k} \\mathrm{~d} s=0\n$$\n\nAccording to Condition 2, $\\left|\\frac{\\mathrm{d}^{m}}{\\mathrm{~d} u^{m}} f\\left(X_{i j}(u)\\right)\\right| \\leq M$, hence we have\n\n$$\n\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) f\\left(X_{i j}(s)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-f\\left(X_{i j}(t)\\right)\\right| \\leq \\frac{M}{m!} h^{m} \\int_{-1}^{1}|K(x)||x|^{m} \\mathrm{~d} x\n$$\n\nChoose $C_{m}=\\frac{M}{m!} \\int_{-1}^{1}|K(x)||x|^{m} \\mathrm{~d} x$, we complete the proof.\n\nProof of Lemma 2 under Condition 3'. Using Taylor series expansion, we obtain that\n\n$$\n\\begin{aligned}\n& \\frac{\\int_{0}^{1} K_{h}(t-s) f\\left(X_{i j}(s)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-f\\left(X_{i j}(t)\\right)=\\frac{\\int_{0}^{1} K_{h}(t-s)\\left(f\\left(X_{i j}(s)\\right)-f\\left(X_{i j}(t)\\right)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s} \\\\\n= & \\sum_{k=1}^{m-1} \\frac{1}{k!} \\frac{\\mathrm{d}^{k}}{\\mathrm{~d} t^{k}} f\\left(X_{i j}(t)\\right) \\frac{\\int_{0}^{1} K_{h}(t-s)(s-t)^{k} \\mathrm{~d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}+\\frac{1}{m!} \\frac{\\int_{0}^{1} K_{h}(t-s) \\frac{\\mathrm{d}^{m}}{\\mathrm{~d} u^{m}} f\\left(X_{i j}(u)\\right)(s-t)^{m} \\mathrm{~d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}\n\\end{aligned}\n$$\n\nwhere $u=\\theta s+(1-\\theta) t$. Now under Condition 3', if $h^{1-\\epsilon} \\leq t \\leq 1-h^{1-\\epsilon}$, it's easy to show that\n\n$$\n\\begin{aligned}\n\\left|\\int_{0}^{1} K_{h}(t-s)(s-t)^{k} \\mathrm{~d} s\\right| & =h^{k}\\left|\\int_{\\frac{t-1}{h^{2}}}^{\\frac{t}{h}} x^{k} K(x) d x\\right| \\\\\n& =h^{k}\\left|\\int_{-\\infty}^{\\frac{t-1}{h}} x^{k} K(x) d x+\\int_{\\frac{t}{h}}^{\\infty} x^{k} K(x) d x\\right| \\\\\n& \\leq h^{k} \\int_{-\\infty}^{-h^{-\\epsilon}} x^{k}|K(x)| d x+\\int_{h^{-\\epsilon}}^{\\infty} x^{k}|K(x)| d x \\\\\n& \\lesssim h^{k} h^{m-k} \\\\\n& =h^{m}\n\\end{aligned}\n$$\n\nMoreover, it is easy to see that $\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s \\rightarrow 1$ as $h \\downarrow 0$. So we can choose constant $C_{m}>0$ such that the following holds when $h>0$ is small enough:\n\n$$\n\\left|\\frac{\\int_{0}^{1} K_{h}(t-s) f\\left(X_{i j}(s)\\right) \\mathrm{d} s}{\\int_{0}^{1} K_{h}(t-s) \\mathrm{d} s}-f\\left(X_{i j}(t)\\right)\\right| \\leq C_{m} h^{m}\n$$\n\nThis completes the proof.\n\nProof of Lemma 3. Let $D=[N / 8]$. Set $\\omega^{0}=(0, \\cdots, 0)$. Define $\\Omega_{0}=\\Omega$ and $\\Omega_{1}=\\{\\omega \\in$ $\\left.\\Omega: H\\left(\\omega, \\omega^{0}\\right)>D\\right\\}$. Let $\\omega^{1}$ be any element in $\\Omega_{1}$. We continue recursively and at the $j$-th step we get $\\Omega_{j}=\\left\\{\\omega \\in \\Omega_{j-1}: H\\left(\\omega, \\omega^{j-1}\\right)>D\\right\\}$, and $\\omega^{j} \\in \\Omega_{j}$, where $j=1, \\cdots, M$, until $\\Omega_{M+1}$ is empty. Let $n_{j}$ be the number of elements in set $A_{j}=\\left\\{\\omega \\in \\Omega_{j}: H\\left(\\omega, \\omega^{j}\\right) \\leq D\\right\\}$, $j=0, \\cdots, M$. It follows clearly that\n\n$$\nn_{j} \\leq \\sum_{i=0}^{D}\\binom{N}{i}\n$$\n\nSince the sets $A_{0}, \\cdots, A_{M}$ form a partition of $\\Omega, \\sum_{j=0}^{M} n_{j}=2^{N}$. Thus,\n\n$$\n(M+1) \\sum_{i=0}^{D}\\binom{N}{i} \\geq 2^{N}\n$$\n\nleading to\n\n$$\nM+1 \\geq \\frac{1}{\\sum_{i=0}^{D} 2^{-N}\\binom{N}{i}}=\\frac{1}{\\mathrm{P}\\left(\\sum_{i=1}^{N} Z_{i} \\leq D\\right)}\n$$\n\nwhere $Z_{1}, \\cdots, Z_{N}$ are i.i.d. $\\operatorname{Ber}(1 / 2)$ random variables. By Hoeffding's inequality,\n\n$$\n\\mathrm{P}\\left(\\sum_{i=1}^{N} Z_{i} \\leq D\\right)=\\mathrm{P}\\left(\\sum_{i=1}^{N} Z_{i} \\leq\\left[\\frac{N}{8}\\right]\\right) \\leq \\exp \\left(-\\frac{9 N}{32}\\right)<2^{-N / 4}\n$$\n\nTherefore $M \\geq 2^{N / 8}$ as long as $N \\geq 8$. And finally note that by our construction, $H\\left(\\omega^{j}, \\omega^{k}\\right) \\geq D+1>N / 8$ for any $j \\neq k$.", "tables": {}, "images": {}}, {"section_id": 17, "text": "# C Computation Algorithm \n\nWe propose an projected gradient descent algorithm for optimizing the discretized pseudolikelihood (7). To handle the constraints in our problem, a projected gradient descent update is used in each iteration. For vector $\\mathbf{x}$, we define the following projection operator:\n\n$$\n\\operatorname{Proc}_{M}(\\mathbf{x})=\\underset{\\|\\mathbf{y}\\| \\leq M}{\\arg \\min} \\|\\mathbf{y}-\\mathbf{x}\\|=\\left\\{\\begin{array}{ll}\n\\mathbf{x} & \\text { if }\\|\\mathbf{x}\\| \\leq M \\\\\nM \\mathbf{x} /\\|\\mathbf{x}\\| & \\text { if }\\|\\mathbf{x}\\|>M\n\\end{array}\\right.\n$$\n\nAlgorithm 1 (Projected gradient descent algorithm).\nInput: Data $\\left\\{Y_{i j}(t): t \\in[0,1], i=1, \\ldots, N, j=1, \\ldots, J\\right\\}$, pre-specified dimension $r$, constraint $M$.\n\nWe partition interval $[h, 1-h]$ by $q$ evenly separated time points $t_{1}, \\ldots, t_{q}$. We set iteration number $m=1$, initial values $\\boldsymbol{\\Theta}^{(0)}\\left(t_{1}\\right)=\\left(\\boldsymbol{\\theta}_{1}^{(0)}\\left(t_{1}\\right), \\ldots, \\boldsymbol{\\theta}_{N}^{(0)}\\left(t_{1}\\right)\\right), \\ldots, \\boldsymbol{\\Theta}^{(0)}\\left(t_{q}\\right)=$ $\\left(\\boldsymbol{\\theta}_{1}^{(0)}\\left(t_{q}\\right), \\ldots, \\boldsymbol{\\theta}_{N}^{(0)}\\left(t_{q}\\right)\\right)$ and $\\boldsymbol{A}^{(0)}=\\left(\\boldsymbol{a}_{1}^{(0)}, \\ldots, \\boldsymbol{a}_{J}^{(0)}\\right)$.\n\nUpdate: At the $m$-th iteration, perform:\n\nFor each respondent $i$ and time node $t_{l}$, update\n\n$$\n\\boldsymbol{\\theta}_{i}^{(m)}\\left(t_{l}\\right)=\\operatorname{Proc}_{\\sqrt{M}}\\left(\\boldsymbol{\\theta}_{i}^{(m-1)}\\left(t_{l}\\right)+\\rho \\mathbf{s}_{i J}^{(m-1)}\\left(\\boldsymbol{\\Theta}^{(m-1)}, \\boldsymbol{A}^{(m-1)}\\right)\\right)\n$$\n\nwhere\n\n$$\n\\mathbf{s}_{i J}^{(m-1)}(\\boldsymbol{\\Theta}, \\boldsymbol{A})=\\frac{\\partial}{\\partial \\boldsymbol{\\theta}_{i}\\left(t_{l}\\right)} \\mathcal{L}_{h}\\left(\\boldsymbol{\\Theta}\\left(t_{1}\\right), \\ldots, \\boldsymbol{\\Theta}\\left(t_{q}\\right), \\mathbf{A}\\right)\n$$\n\nFor each item $j$, update\n\n$$\n\\boldsymbol{a}_{j}^{(m)}=\\operatorname{Proc}_{\\sqrt{M}}\\left(\\boldsymbol{a}_{j}^{(m-1)}+\\rho \\widetilde{\\mathbf{s}}_{j}^{(m-1)}\\left(\\boldsymbol{\\Theta}^{(m-1)}, \\boldsymbol{A}^{(m-1)}\\right)\\right)\n$$\n\nwhere\n\n$$\n\\widetilde{\\mathbf{s}}_{j}^{(m-1)}(\\boldsymbol{\\Theta}, \\boldsymbol{A})=\\frac{\\partial}{\\partial \\boldsymbol{a}_{j}} \\mathcal{L}_{h}\\left(\\boldsymbol{\\Theta}\\left(t_{1}\\right), \\ldots, \\boldsymbol{\\Theta}\\left(t_{q}\\right), \\mathbf{A}\\right)\n$$\n\nThe step size $\\rho>0$ is chosen by backtracking line search. Iterate this step until convergence. Let $\\widetilde{m}$ be the last iteration number upon convergence.\n\nOutput: $\\widetilde{\\mathbf{X}}\\left(t_{l}\\right)=\\boldsymbol{\\Theta}^{(\\widetilde{m})}\\left(t_{l}\\right)\\left(\\boldsymbol{A}^{(\\widetilde{m})}\\right)^{\\mathrm{T}}$ for each $l=1, \\ldots, q$.\n\nThe following theorem guarantees the convergence of Algorithm 1 to a critical point of the discretized pseudo-likelihood.\n\nTheorem 4. Denote the parameters obtained in the $k$-th update by $\\boldsymbol{\\Theta}^{(k)}=\\left(\\boldsymbol{\\Theta}^{(k)}\\left(t_{1}\\right), \\ldots, \\boldsymbol{\\Theta}^{(k)}\\left(t_{q}\\right)\\right)$ and $\\mathbf{A}^{(k)}$. Then $\\boldsymbol{\\Theta}^{(k)} \\rightarrow \\boldsymbol{\\Theta}=\\left(\\boldsymbol{\\Theta}\\left(t_{1}\\right), \\ldots, \\boldsymbol{\\Theta}\\left(t_{q}\\right)\\right)$ and $\\mathbf{A}^{(k)} \\rightarrow \\mathbf{A}$, where $\\max _{l=1, \\ldots, q}\\left\\|\\boldsymbol{\\Theta}\\left(t_{l}\\right)\\right\\|_{2 \\rightarrow \\infty} \\leq$ $M^{1 / 2}$ and $\\|\\mathbf{A}\\|_{2 \\rightarrow \\infty} \\leq M^{1 / 2}$ and $(\\boldsymbol{\\Theta}, \\mathbf{A})$ is a critical point of $\\mathcal{L}_{h}\\left(\\boldsymbol{\\Theta}\\left(t_{1}\\right), \\ldots, \\boldsymbol{\\Theta}\\left(t_{q}\\right), \\mathbf{A}\\right)$.\n\nRemark 18. In simulation studies, $\\mathcal{L}_{h}$ usually have only one critical point, which is its unique global maximum point.\n\nProof of Theorem 4. For notational simplicity, we denote:\n\n$$\n\\mathcal{H}=\\left\\{\\boldsymbol{\\Theta}\\left(t_{1}\\right), \\ldots, \\boldsymbol{\\Theta}\\left(t_{q}\\right), \\mathbf{A}: \\max _{l=1, \\ldots, q}\\left\\|\\boldsymbol{\\Theta}\\left(t_{l}\\right)\\right\\|_{2 \\rightarrow \\infty} \\leq M^{1 / 2},\\|\\mathbf{A}\\|_{2 \\rightarrow \\infty} \\leq M^{1 / 2}\\right\\}\n$$\n\nFurthermore, denote the projection operator in algorithm by $\\mathcal{P}$. We first prove that $\\mathcal{P}$ is the projection operator on compact set $\\mathcal{H}$. For any $\\gamma=(\\boldsymbol{\\Theta}, \\mathbf{A}) \\triangleq\\left(\\alpha_{1}, \\ldots, \\alpha_{K}\\right)$, where $\\alpha_{1}, \\ldots, \\alpha_{K} \\in \\mathbb{R}^{r}$ and $K=q N+J$. Then $\\mathcal{P}\\left(\\alpha_{1}, \\ldots, \\alpha_{K}\\right)=\\left(\\operatorname{Proc}_{\\sqrt{M}}\\left(\\alpha_{1}\\right), \\ldots, \\operatorname{Proc}_{\\sqrt{M}}\\left(\\alpha_{K}\\right)\\right)$. Then for any $\\widetilde{\\gamma}=\\left(\\widetilde{\\alpha}_{1}, \\ldots, \\widetilde{\\alpha}_{K}\\right) \\in \\mathcal{H}$, we have\n\n$$\n\\begin{aligned}\n& \\left\\langle\\left(\\widetilde{\\alpha}_{1}-\\operatorname{Proc}_{\\sqrt{M}}\\left(\\alpha_{1}\\right), \\ldots, \\widetilde{\\alpha}_{K}-\\operatorname{Proc}_{\\sqrt{M}}\\left(\\alpha_{K}\\right)\\right),\\left(\\alpha_{1}-\\operatorname{Proc}_{\\sqrt{M}}\\left(\\alpha_{1}\\right), \\ldots, \\alpha_{K}-\\operatorname{Proc}_{\\sqrt{M}}\\left(\\alpha_{K}\\right)\\right)\\right\\rangle \\\\\n= & \\sum_{k=1}^{K}\\left\\langle\\widetilde{\\alpha}_{k}-\\operatorname{Proc}_{\\sqrt{M}}\\left(\\alpha_{k}\\right), \\alpha_{k}-\\operatorname{Proc}_{\\sqrt{M}}\\left(\\alpha_{k}\\right)\\right\\rangle \\\\\n\\leq & 0\n\\end{aligned}\n$$\n\nThis implies that $\\mathcal{P}$ is the projection operator on compact set $\\mathcal{H}$. By the projected gradient descent algorithm, there holds: $\\gamma^{(k)}=\\left(\\boldsymbol{\\Theta}^{(k)}, \\mathbf{A}^{(k)}\\right) \\in \\mathcal{H}$ for any $k \\in \\mathbb{N}$. Suppose $\\gamma=$ $(\\boldsymbol{\\Theta}, \\mathbf{A}) \\in \\mathcal{H}$ is a limit point of its subsequence, i.e., $\\gamma^{\\left(k_{n}\\right)} \\rightarrow \\gamma$. We first prove that $\\gamma=(\\boldsymbol{\\Theta}, \\mathbf{A})$ is a critical point, i.e.,\n\n$$\n\\nabla^{\\mathrm{T}} \\mathcal{L}_{h}(\\gamma)(\\widetilde{\\gamma}-\\gamma) \\leq 0\n$$\n\nfor any $\\widetilde{\\gamma}=(\\widehat{\\boldsymbol{\\Theta}}, \\widehat{\\mathbf{A}}) \\in \\mathcal{H}$. If not, then $\\nabla^{\\mathrm{T}} \\mathcal{L}_{h}(\\gamma)(\\widetilde{\\gamma}-\\gamma)>0$ for fixed $\\widetilde{\\gamma}=(\\widehat{\\boldsymbol{\\Theta}}, \\widehat{\\mathbf{A}}) \\in \\mathcal{H}$, Since by the algorithm, $\\mathcal{L}_{h}\\left(\\gamma^{(k)}\\right)$ is strictly increasing, we can deduce that if we input $\\gamma$ as the parameter value in the initial step, the algorithm will stop updating or update $\\gamma$ to itself.\n\nCase 1: If the algorithm stop updating, then for any $\\rho>0$ and $\\gamma_{\\rho} \\triangleq \\mathcal{P}\\left(\\gamma+\\rho \\nabla \\mathcal{L}_{h}(\\gamma)\\right)$, we have $\\mathcal{L}_{h}\\left(\\gamma_{\\rho}\\right) \\leq \\mathcal{L}_{h}(\\gamma)$. We first prove that for any $\\rho>0$, we have\n\n$$\n\\nabla^{\\mathrm{T}} \\mathcal{L}_{h}(\\gamma)\\left(\\gamma_{\\rho}-\\gamma\\right)<0\n$$\n\nIf not, i.e., $\\nabla^{\\mathrm{T}} \\mathcal{L}_{h}(\\gamma)\\left(\\gamma_{\\rho}-\\gamma\\right) \\geq 0$. Denote $\\gamma+\\rho \\nabla \\mathcal{L}_{h}(\\gamma)=\\gamma_{\\rho}+v$. Then by the projection property, we have $\\left(\\gamma_{\\rho}-\\gamma\\right)^{\\mathrm{T}} v \\geq 0$. So we have\n\n$$\n-\\left\\|\\gamma_{\\rho}-\\gamma\\right\\|_{2}^{2}=\\left(\\gamma_{\\rho}-\\gamma\\right)^{\\mathrm{T}}\\left(v-\\rho \\nabla \\mathcal{L}_{h}(\\gamma)\\right) \\geq 0\n$$\n\nwhich implies that $\\gamma_{\\rho}=\\gamma$. This falls into Case 2. Hence for any $\\rho>0$, we have $\\nabla^{\\mathrm{T}} \\mathcal{L}_{h}(\\gamma)\\left(\\gamma_{\\rho}-\\gamma\\right)<0$. Then for $\\rho>0$ small enough, we have $\\mathcal{L}_{h}(\\gamma)<\\mathcal{L}_{h}\\left(\\gamma_{\\rho}\\right)$, which leads to contradiction.\n\nCase 2: If the algorithm update $\\gamma$ to itself. Then for any $\\rho>0$, we have $\\mathcal{P}\\left(\\gamma+\\rho \\nabla \\mathcal{L}_{h}(\\gamma)\\right)=$ $\\gamma$. Then for any $\\widetilde{\\gamma} \\in \\mathcal{H}$, we have\n\n$$\n(\\widetilde{\\gamma}-\\gamma)^{\\mathrm{T}}\\left(\\gamma+\\rho \\nabla \\mathcal{L}_{h}(\\gamma)-\\gamma\\right) \\leq 0\n$$\n\nHence $\\nabla^{\\mathrm{T}} \\mathcal{L}_{h}(\\gamma)(\\widetilde{\\gamma}-\\gamma) \\leq 0$ for any $\\widetilde{\\gamma} \\in \\mathcal{H}$. So we prove that $\\gamma$ is a critical point. By this proof, we can also show that the algorithm will stop updating on a critical point. This indicates that there can be only one limit point among $\\gamma^{(k)}, k=1,2, \\ldots$. Hence we proved that $\\gamma^{(k)}$ converges to a critical point of $\\mathcal{L}_{h}(\\gamma)$.", "tables": {}, "images": {}}, {"section_id": 18, "text": "# D Details of Simulations Settings and Appended Re-\n## sults\n\nWe provide the detailed settings of the simulations in Section 4. For comparison, we also provide simulation settings with independent event types, more event types than observation units, and modified specifications on rate functions which lead to weaker signals. In each simulation setting, we consider three cases regarding the pattern of the true parameter matrix $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\mathrm{T}}$ :\n\nC1. $\\mathbf{X}(\\cdot)$ takes constant value on $[0,1]$, i.e., $\\boldsymbol{\\Theta}(\\cdot)$ takes constant value on $[0,1]$.\nC2. $\\mathbf{X}(\\cdot)$ changes linearly on $[0,1]$, i.e., $\\boldsymbol{\\Theta}(\\cdot)$ changes linearly on $[0,1]$.\nC3. $\\mathbf{X}(\\cdot)$ changes periodically on $[0,1]$, i.e., $\\boldsymbol{\\Theta}(\\cdot)$ changes periodically on $[0,1]$.\n\nAccording to the simulated rate functions, we generate $N \\times J$ independent Poisson processes in independent cases and generate $N \\times J$ Poisson processes with block-wise independence by the thinning approach mentioned in Section 4 in dependent cases, respectively. We choose the true number of factors as $r=3$ and link function as $f(x)=\\exp (x)$. In dependent cases, the block size is chosen as $\\phi(J)=J^{1 / 3}$.\n\nFor estimation, we choose $M=36$ and choose kernel function as the Epanechnikov kernel function $K(x)=0.75\\left(1-x^{2}\\right),-1 \\leq x \\leq 1$, with kernel order $m=2$. It is easy to verify that the chosen kernel function satisfies Condition 3. The smoothing bandwidth is chosen as $h=0.1\\left((N \\wedge J) /\\left(\\log ^{2}(N \\wedge J)\\right)\\right)^{-0.2}$ in independent cases and $h=0.1(J / \\phi(J))^{-0.19}$ in dependent cases, respectively. We estimate $\\mathbf{X}(t)$ based on 31 evenly distributed time points $t_{1}, \\ldots, t_{31}$ on $[h, 1-h]$ then obtain the estimates on 901 evenly distributed time points $t_{1}, \\ldots, t_{901}$ on $[h, 1-h]$ by linear interpolation. The estimation error is then evaluated by the average of $(1-2 h) \\sum_{k=1}^{901}\\left\\|\\mathbf{X}^{*}\\left(t_{k}\\right)-\\widehat{\\mathbf{X}}\\left(t_{k}\\right)\\right\\|_{F}^{2} /(901 N J)$ among all independent replications.\n\nFor model selection, we apply our proposed information criterion to select $r$ from the candidate set $\\{1,2,3,4,5\\}$ with the penalty term chosen as $v(N, J, r)=4000 r N J h^{3.99}$ in independent cases and $v(N, J, r)=40 r N J h^{1.99}$ in dependent cases, respectively. The selection accuracy is then evaluated by the number of times that the number of factors is under- or over-selected among all independent replications.", "tables": {}, "images": {}}, {"section_id": 19, "text": "# D. 1 Independent Case with Regular Event Rate \n\nThe rate functions for the Poisson processes are generated by:\n\nS1. Let $(N, J)=(200,100),(400,200),(800,400),(1600,800)$.\n\nC1. Generate $\\boldsymbol{\\Theta}, \\mathbf{A}$ by $\\Theta_{i j} \\stackrel{i i d}{\\sim} U(-1.8,1.8)$ and $A_{i j} \\stackrel{i i d}{\\sim} U(-1.8,1.8)$. Let $\\mathbf{X}(t)=\\boldsymbol{\\Theta} \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nC2. Generate $\\boldsymbol{\\Theta}(0), \\mathbf{A}$ by $\\Theta_{i j}(0) \\stackrel{i i d}{\\sim} U(-1.6,1.6)$ and $A_{i j} \\stackrel{i i d}{\\sim} U(-1.6,1.6)$. Generate linear coefficient $\\mathbf{B} \\in \\mathbb{R}^{r \\times N}$ by $B_{i j} \\stackrel{i i d}{\\sim} U(-3.6,3.6)$ and let $\\Theta_{i j}(t)=B_{i j} t+\\Theta_{i j}(0)$. Let $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nC3. Generate $\\boldsymbol{\\Theta}(0), \\mathbf{A}$ by $\\Theta_{i j}(0) \\stackrel{i i d}{\\sim} U(-1.7,1.7)$ and $A_{i j} \\stackrel{i i d}{\\sim} U(-1.7,1.7)$. Choose the period as $T_{0}=1$. Generate amplitude coefficient $\\mathbf{S} \\in \\mathbb{R}^{r \\times N}$ and phase coefficient $\\mathbf{W} \\in \\mathbb{R}^{r \\times N}$ by $S_{i j} \\stackrel{i i d}{\\sim} U(-1.2,1.2)$ and $W_{i j} \\stackrel{i i d}{\\sim} U\\left(0, T_{0}\\right)$. Let $\\Theta_{i j}(t)=\\Theta_{i j}(0)+$ $S_{i j} \\sin \\left(2 \\pi\\left(t-W_{i j}\\right) / T_{0}\\right)$ and let $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nS2. Let $(N, J)=(200,100),(400,200),(800,400),(1600,800)$.\n\nC1. Generate $\\boldsymbol{\\Theta}, \\mathbf{A}$ by $\\Theta_{i j} \\stackrel{i i d}{\\sim} U(-1.8,1.8), A_{i j} \\stackrel{i i d}{\\sim} U(-1.8,1.8)$ for $i=1, \\ldots, r-1, j=$ $1, \\ldots, J$ and $A_{i j} \\stackrel{i i d}{\\sim} U(-0.9,0.9)$ for $i=r, j=1, \\ldots, J$. Let $\\mathbf{X}(t)=\\boldsymbol{\\Theta} \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nC2. Generate $\\boldsymbol{\\Theta}(0), \\mathbf{A}$ by $\\Theta_{i j} \\stackrel{i i d}{\\sim} U(-1.6,1.6), A_{i j} \\stackrel{i i d}{\\sim} U(-1.6,1.6)$ for $i=1, \\ldots, r-$ $1, j=1, \\ldots, J$ and $A_{i j} \\stackrel{i i d}{\\sim} U(-0.8,0.8)$ for $i=r, j=1, \\ldots, J$. Generate linear coefficient $\\mathbf{B} \\in \\mathbb{R}^{r \\times N}$ by $B_{i j} \\stackrel{i i d}{\\sim} U(-3.6,3.6)$ and let $\\Theta_{i j}(t)=B_{i j} t+\\Theta_{i j}(0)$. Let $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nC3. Generate $\\boldsymbol{\\Theta}(0), \\mathbf{A}$ by $\\Theta_{i j} \\stackrel{i i d}{\\sim} U(-1.7,1.7), A_{i j} \\stackrel{i i d}{\\sim} U(-1.7,1.7)$ for $i=1, \\ldots, r-$ $1, j=1, \\ldots, J$ and $A_{i j} \\stackrel{i i d}{\\sim} U(-0.85,0.85)$ for $i=r, j=1, \\ldots, J$. Choose the period as $T_{0}=1$. Generate amplitude coefficient $\\mathbf{S} \\in \\mathbb{R}^{r \\times N}$ and phase coefficient $\\mathbf{W} \\in \\mathbb{R}^{r \\times N}$ by $S_{i j} \\stackrel{i i d}{\\sim} U(-1.2,1.2)$ and $W_{i j} \\stackrel{i i d}{\\sim} U\\left(0, T_{0}\\right)$. Let $\\Theta_{i j}(t)=\\Theta_{i j}(0)+$ $S_{i j} \\sin \\left(2 \\pi\\left(t-W_{i j}\\right) / T_{0}\\right)$ and let $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nThe $N \\times J$ independent Poisson processes are then generated according to the simulated rate functions. We also present the results when we increase the number of grid points from 31 to 121 to show that the estimation error is not sensitive to the chosen number of grid points due to the smoothness of the rate function. Moreover, we evaluate the estimation error regarding the time-independent matrix $\\mathbf{A}^{*}$. Let $\\angle\\left(\\mathbf{A}^{*}, \\widehat{\\mathbf{A}}\\right) \\in \\mathbb{R}^{r \\times r}$ be the diagonal matrix containing all $r$ principal angles between the column spaces of $\\mathbf{A}^{*}$ and $\\widehat{\\mathbf{A}}$. The estimation error is then evaluated by the element-wise product of $\\sin \\left(\\angle\\left(\\mathbf{A}^{*}, \\widehat{\\mathbf{A}}\\right)\\right)$.\n\nTable 4 presents the average estimation error of our proposed kernel-based estimator and the estimator under the Poisson factor model among the 50 independent replications for all 24 settings. Table 5 presents the average estimation error when the number of grid points changes from 31 to 121. Compared to Table 4, there is little change in the estimation error, indicating that discretization by 31 grid points is good enough for approximation. Table 6 presents the frequency that the number of factors is under- and over-selected among the 50 independent replications for all the 24 settings. Table 7 presents the average estimation error regarding time-independent matrix $\\mathbf{A}^{*}$ of our proposed kernel-based estimator and\n\nthe estimator under the Poisson factor model among the 50 independent replications for all 24 settings. In the setting where the rate function is non-constant, our proposed method still yields substantially smaller estimation errors when estimating the time-independent matrix $\\mathbf{A}^{*}$ than those under the Poisson factor model.\n\n![table_3](table_3)\n\nTable 4: (Independent case with regular event rate) Mean estimation error among 50 independent replications based on the proposed estimator and the estimator under the Poisson factor model under each of the 24 simulation settings.\n\n![table_4](table_4)\n\nTable 5: (Independent case with regular event rate and more grid points) Mean estimation error among 50 independent replications based on the proposed estimator and the estimator under the Poisson factor model under each of the 24 simulation settings.\n\n![table_5](table_5)\n\nTable 6: (Independent case with regular event rate) The number of times that the true number of factors is under- or over-selected selected among 50 independent replications under each of the 24 simulation settings.\n\n![table_6](table_6)\n\nTable 7: Mean estimation error $\\left(\\times 10^{6}\\right)$ of matrix $\\mathbf{A}^{*}$ among 50 independent replications based on the proposed estimator and the estimator under the Poisson factor model under each of the 24 simulation settings.", "tables": {"table_3": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Kernel-based method | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0714 | 0.0846 | 0.0747 | 0.0895 | 0.1012 | 0.0917 |\n| $J=200$ | 0.0373 | 0.0441 | 0.0390 | 0.0467 | 0.0535 | 0.0483 |\n| $J=400$ | 0.0198 | 0.0238 | 0.0211 | 0.0249 | 0.0289 | 0.0260 |\n| $J=800$ | 0.0108 | 0.0129 | 0.0115 | 0.0136 | 0.0156 | 0.0142 |\n| Poisson factor model | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0138 | 0.8848 | 0.7206 | 0.0170 | 0.6175 | 0.5362 |\n| $J=200$ | 0.0065 | 0.8787 | 0.7274 | 0.0082 | 0.6199 | 0.5241 |\n| $J=400$ | 0.0032 | 0.9000 | 0.7307 | 0.0040 | 0.6228 | 0.5309 |\n| $J=800$ | 0.0016 | 0.9240 | 0.7287 | 0.0020 | 0.6502 | 0.5334 |", "table_4": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Kernel-based method | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0747 | 0.0886 | 0.0782 | 0.0935 | 0.1060 | 0.0959 |\n| $J=200$ | 0.0393 | 0.0467 | 0.0411 | 0.0492 | 0.0566 | 0.0509 |\n| $J=400$ | 0.0212 | 0.0254 | 0.0225 | 0.0265 | 0.0308 | 0.0277 |\n| $J=800$ | 0.0116 | 0.0140 | 0.0124 | 0.0147 | 0.0169 | 0.0153 |\n| Poisson factor model | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0138 | 0.8848 | 0.7206 | 0.0170 | 0.6175 | 0.5362 |\n| $J=200$ | 0.0073 | 0.9785 | 0.7611 | 0.0091 | 0.6830 | 0.5458 |\n| $J=400$ | 0.0036 | 0.9773 | 0.7442 | 0.0046 | 0.6911 | 0.5442 |\n| $J=800$ | 0.0018 | 1.0012 | 0.7542 | 0.0023 | 0.6955 | 0.5491 |", "table_5": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Under-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=800$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| Over-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=800$ | 0 | 0 | 0 | 0 | 0 | 0 |", "table_6": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Kernel-based method | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 74.540 | 90.887 | 60.056 | 210.941 | 264.609 | 170.585 |\n| $J=200$ | 22.891 | 26.227 | 18.512 | 63.840 | 76.968 | 53.994 |\n| $J=400$ | 7.416 | 8.221 | 5.927 | 20.934 | 24.254 | 18.008 |\n| $J=800$ | 2.543 | 2.756 | 1.978 | 7.066 | 8.113 | 5.871 |\n| Poisson factor model | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 57.858 | 1370.073 | 329.678 | 159.902 | 2122.063 | 519.222 |\n| $J=200$ | 19.184 | 646.892 | 148.225 | 52.383 | 950.349 | 206.596 |\n| $J=400$ | 6.425 | 303.716 | 68.679 | 17.798 | 467.246 | 97.187 |\n| $J=800$ | 2.235 | 186.285 | 35.219 | 6.178 | 318.743 | 51.668 |"}, "images": {}}, {"section_id": 20, "text": "# D. 2 Dependent Case with Regular Event Rate \n\nThis is the simulation setting that is presented in Section 4. Let $(N, J)=(200,100)$, $(400,200),(800,400),(1600,800)$ and $\\phi(J)=5,6,7,9$. The rate functions for the Poisson processes are generated by the same method as in Appendix D.1. We then generate $N \\times J$ Poisson processes with block-wise independence by the thinning algorithm. We also evaluate the estimation error when we increase the number of grid points from 31 to 121 to show that the result is not sensitive to the chosen number of grid points due to the smoothness of the rate function. Table 8 presents the average estimation error of our proposed kernel-based estimator and the estimator under the Poisson factor model among the 50 independent replications for all 24 settings. Table 9 presents the average estimation\n\nerror when the number of grid points changes from 31 to 121 , which has little difference from Table 8. Table 10 presents the frequency that the number of factors is under- and over-selected among the 50 independent replications for all the 24 settings. The usage of pseudo-likelihood function instead of true likelihood function leads to slightly inferior results, compared to the independent case in Appendix D.1.\n\n![table_7](table_7)\n\nTable 8: (Dependent case with regular event rate) Mean estimation error among 50 independent replications based on the proposed estimator and the estimator under the Poisson factor model under each of the 24 simulation settings.\n\n![table_8](table_8)\n\nTable 9: (Dependent case with regular event rate and more grid points) Mean estimation error among 50 independent replications based on the proposed estimator and the estimator under the Poisson factor model under each of the 24 simulation settings.\n\n![table_9](table_9)\n\nTable 10: (Dependent case with regular event rate) The number of times that the true number of factors is under- or over-selected selected among 50 independent replications under each of the 24 simulation settings.", "tables": {"table_7": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Kernel-based method | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.1006 | 0.1174 | 0.1048 | 0.1371 | 0.1606 | 0.1407 |\n| $J=200$ | 0.0536 | 0.0630 | 0.0562 | 0.0692 | 0.0806 | 0.0727 |\n| $J=400$ | 0.0291 | 0.0350 | 0.0308 | 0.0378 | 0.0437 | 0.0398 |\n| $J=800$ | 0.0159 | 0.0190 | 0.0170 | 0.0205 | 0.0240 | 0.0217 |\n| Poisson factor model | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0154 | 0.9743 | 0.7518 | 0.0192 | 0.6928 | 0.5530 |\n| $J=200$ | 0.0073 | 0.9785 | 0.7611 | 0.0091 | 0.6830 | 0.5458 |\n| $J=400$ | 0.0036 | 0.9773 | 0.7442 | 0.0046 | 0.6911 | 0.5442 |\n| $J=800$ | 0.0018 | 1.0012 | 0.7542 | 0.0023 | 0.6955 | 0.5491 |", "table_8": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Kernel-based method | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.1084 | 0.1268 | 0.1131 | 0.1474 | 0.1729 | 0.1515 |\n| $J=200$ | 0.0588 | 0.0692 | 0.0617 | 0.0758 | 0.0885 | 0.0797 |\n| $J=400$ | 0.0326 | 0.0393 | 0.0345 | 0.0423 | 0.0489 | 0.0445 |\n| $J=800$ | 0.0181 | 0.0217 | 0.0193 | 0.0234 | 0.0274 | 0.0248 |\n| Poisson factor model | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0154 | 0.9743 | 0.7518 | 0.0192 | 0.6928 | 0.5530 |\n| $J=200$ | 0.0073 | 0.9784 | 0.7611 | 0.0091 | 0.6830 | 0.5458 |\n| $J=400$ | 0.0036 | 0.9773 | 0.7442 | 0.0046 | 0.6911 | 0.5442 |\n| $J=800$ | 0.0018 | 1.0012 | 0.7542 | 0.0023 | 0.6955 | 0.5491 |", "table_9": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Under-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=800$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| Over-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0 | 0 | 0 | 13 | 29 | 10 |\n| $J=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=800$ | 0 | 0 | 0 | 0 | 0 | 0 |"}, "images": {}}, {"section_id": 21, "text": "# D. 3 Independent Case with Lower Event Rate \n\nIn this case, we modify the generating method of the rate functions such that the total number of events is approximately half as the number of events in Appendix D.1. The rate functions of the Poisson processes are generated by:\n\nS1. Let $(N, J)=(200,100),(400,200),(800,400),(1600,800)$.\nC1. Generate $\\boldsymbol{\\Theta}, \\mathbf{A}$ by $\\Theta_{i j} \\stackrel{i i d}{\\sim} U(-1.6,1.6)$ and $A_{i j} \\stackrel{i i d}{\\sim} U(-1.6,1.6)$. Let $\\mathbf{X}(t)=\\boldsymbol{\\Theta} \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nC2. Generate $\\boldsymbol{\\Theta}(0)$, $\\mathbf{A}$ by $\\Theta_{i j}(0) \\stackrel{i i d}{\\sim} U(-1.4,1.4)$ and $A_{i j} \\stackrel{i i d}{\\sim} U(-1.4,1.4)$. Generate linear coefficient $\\mathbf{B} \\in \\mathbb{R}^{r \\times N}$ by $B_{i j} \\stackrel{i i d}{\\sim} U(-3.2,3.2)$ and let $\\Theta_{i j}(t)=B_{i j} t+\\Theta_{i j}(0)$.\n\nLet $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\nC3. Generate $\\boldsymbol{\\Theta}(0), \\mathbf{A}$ by $\\Theta_{i j}(0) \\stackrel{\\text { iid }}{\\sim} U(-1.5,1.5)$ and $A_{i j} \\stackrel{\\text { iid }}{\\sim} U(-1.5,1.5)$. Choose the period as $T_{0}=1$. Generate amplitude coefficient $\\mathbf{S} \\in \\mathbb{R}^{r \\times N}$ and phase coefficient $\\mathbf{W} \\in \\mathbb{R}^{r \\times N}$ by $S_{i j} \\stackrel{\\text { iid }}{\\sim} U(-1,1)$ and $W_{i j} \\stackrel{\\text { iid }}{\\sim} U\\left(0, T_{0}\\right)$. Let $\\Theta_{i j}(t)=\\Theta_{i j}(0)+$ $S_{i j} \\sin \\left(2 \\pi\\left(t-W_{i j}\\right) / T_{0}\\right)$ and let $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nS2. Let $(N, J)=(200,100),(400,200),(800,400),(1600,800)$.\nC1. Generate $\\boldsymbol{\\Theta}, \\mathbf{A}$ by $\\Theta_{i j} \\stackrel{\\text { iid }}{\\sim} U(-1.6,1.6), A_{i j} \\stackrel{\\text { iid }}{\\sim} U(-1.6,1.6)$ for $i=1, \\ldots, r-1, j=$ $1, \\ldots, J$ and $A_{i j} \\stackrel{\\text { iid }}{\\sim} U(-0.8,0.8)$ for $i=r, j=1, \\ldots, J$. Let $\\mathbf{X}(t)=\\boldsymbol{\\Theta} \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nC2. Generate $\\boldsymbol{\\Theta}(0), \\mathbf{A}$ by $\\Theta_{i j} \\stackrel{\\text { iid }}{\\sim} U(-1.4,1.4), A_{i j} \\stackrel{\\text { iid }}{\\sim} U(-1.4,1.4)$ for $i=1, \\ldots, r-$ $1, j=1, \\ldots, J$ and $A_{i j} \\stackrel{\\text { iid }}{\\sim} U(-0.7,0.7)$ for $i=r, j=1, \\ldots, J$. Generate linear coefficient $\\mathbf{B} \\in \\mathbb{R}^{r \\times N}$ by $B_{i j} \\stackrel{\\text { iid }}{\\sim} U(-3.2,3.2)$ and let $\\Theta_{i j}(t)=B_{i j} t+\\Theta_{i j}(0)$. Let $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nC3. Generate $\\boldsymbol{\\Theta}(0), \\mathbf{A}$ by $\\Theta_{i j} \\stackrel{\\text { iid }}{\\sim} U(-1.5,1.5), A_{i j} \\stackrel{\\text { iid }}{\\sim} U(-1.5,1.5)$ for $i=1, \\ldots, r-$ $1, j=1, \\ldots, J$ and $A_{i j} \\stackrel{\\text { iid }}{\\sim} U(-0.75,0.75)$ for $i=r, j=1, \\ldots, J$. Choose the period as $T_{0}=1$. Generate amplitude coefficient $\\mathbf{S} \\in \\mathbb{R}^{r \\times N}$ and phase coefficient $\\mathbf{W} \\in \\mathbb{R}^{r \\times N}$ by $S_{i j} \\stackrel{\\text { iid }}{\\sim} U(-1,1)$ and $W_{i j} \\stackrel{\\text { iid }}{\\sim} U\\left(0, T_{0}\\right)$. Let $\\Theta_{i j}(t)=\\Theta_{i j}(0)+$ $S_{i j} \\sin \\left(2 \\pi\\left(t-W_{i j}\\right) / T_{0}\\right)$ and let $\\mathbf{X}(t)=\\boldsymbol{\\Theta}(t) \\mathbf{A}^{\\mathrm{T}}$ for any $t \\in[0,1]$.\n\nThe $N \\times J$ independent Poisson processes are then generated according to the simulated rate functions. Table 11 presents the average estimation error of our proposed kernel-based estimator and the estimator under the Poisson factor model among the 50 independent replications for all 24 settings. Table 12 presents the frequency that the number of factors is under- and over-selected among the 50 independent replications for all 24 settings. In\n\neach setting, the total number of events is approximately half as the number of events in the corresponding setting in Appendix D.1, leading to slightly inferior but still valid results.\n\n![table_10](table_10)\n\nTable 11: (Independent case with lower event rate) Mean estimation error among 50 independent replications based on the proposed estimator and the estimator under the Poisson factor model under each of the 24 simulation settings.\n\n![table_11](table_11)\n\nTable 12: (Independent case with lower event rate) The number of times that the true number of factors is under- or over-selected selected among 50 independent replications under each of the 24 simulation settings.", "tables": {"table_10": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Kernel-based method | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0980 | 0.1103 | 0.1028 | 0.1156 | 0.1276 | 0.1191 |\n| $J=200$ | 0.0512 | 0.0594 | 0.0539 | 0.0604 | 0.0678 | 0.0630 |\n| $J=400$ | 0.0275 | 0.0320 | 0.0291 | 0.0325 | 0.0367 | 0.0340 |\n| $J=800$ | 0.0149 | 0.0174 | 0.0159 | 0.0177 | 0.0200 | 0.0186 |\n| Poisson factor model | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0187 | 0.4762 | 0.3671 | 0.0216 | 0.3452 | 0.2853 |\n| $J=200$ | 0.0089 | 0.4698 | 0.3672 | 0.0104 | 0.3348 | 0.2697 |\n| $J=400$ | 0.0044 | 0.4763 | 0.3642 | 0.0052 | 0.3387 | 0.2704 |\n| $J=800$ | 0.0022 | 0.4896 | 0.3685 | 0.0026 | 0.3468 | 0.2719 |", "table_11": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Under-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0 | 0 | 0 | 0 | 1 | 0 |\n| $J=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=800$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| Over-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=800$ | 0 | 0 | 0 | 0 | 0 | 0 |"}, "images": {}}, {"section_id": 22, "text": "# D. 4 Dependent Case with Lower Event Rate \n\nLet $(N, J)=(200,100),(400,200),(800,400),(1600,800)$ and $\\phi(J)=5,6,7,9$. The rate functions for the Poisson processes are generated by the same method as in Appendix D.3. We then generate $N \\times J$ Poisson processes with block-wise independence by the thinning algorithm. Table 13 presents the average estimation error of our proposed kernel-based estimator and the estimator under the Poisson factor model among the 50 independent replications for all 24 settings. Table 14 presents the frequency that the number of factors is under- and over-selected among the 50 independent replications for all the 24 settings.\n\n![table_12](table_12)\n\nTable 13: (Dependent case with lower event rate) Mean estimation error among 50 independent replications based on the proposed estimator and the estimator under the Poisson factor model under each of the 24 simulation settings.\n\n![table_13](table_13)\n\nTable 14: (Dependent case with lower event rate) The number of times that the true number of factors is under- or over-selected selected among 50 independent replications under each of the 24 simulation settings.", "tables": {"table_12": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Kernel-based method | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.1529 | 0.1797 | 0.1564 | 0.2380 | 0.3940 | 0.2586 |\n| $J=200$ | 0.0758 | 0.0907 | 0.0808 | 0.1035 | 0.1290 | 0.1084 |\n| $J=400$ | 0.0413 | 0.0486 | 0.0443 | 0.0534 | 0.0622 | 0.0557 |\n| $J=800$ | 0.0225 | 0.0265 | 0.0243 | 0.0285 | 0.0328 | 0.0302 |\n| Poisson factor model | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0.0208 | 0.5124 | 0.3802 | 0.0246 | 0.3895 | 0.2994 |\n| $J=200$ | 0.0099 | 0.5156 | 0.3835 | 0.0120 | 0.3740 | 0.2871 |\n| $J=400$ | 0.0050 | 0.5185 | 0.3765 | 0.0060 | 0.3713 | 0.2810 |\n| $J=800$ | 0.0025 | 0.5301 | 0.3779 | 0.0031 | 0.3752 | 0.2797 |", "table_13": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Under-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=800$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| Over-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $J=100$ | 25 | 40 | 20 | 46 | 48 | 50 |\n| $J=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $J=800$ | 0 | 0 | 0 | 0 | 0 | 0 |"}, "images": {}}, {"section_id": 23, "text": "# D. 5 Dependent Case with High-dimensional Setting \n\nLet $(N, J)=(100,800),(200,800),(400,800),(800,800),(1600,800)$ and $\\phi(J)=9,9,9$, 9, 9. The rate functions for the Poisson processes are generated by the same method as in Appendix D.1. We then generate $N \\times J$ Poisson processes with block-wise independence by the thinning algorithm. Table 15 presents the average estimation error of our proposed kernel-based method and Poisson factor model among the 50 independent replications for all 30 settings. Table 16 presents the frequency that the number of factors is under- and over-selected among the 50 independent replications for all the 30 settings. Our proposed method is still valid when $N$ is substantially smaller than $J$. Holding $J$ as constant, our method suffers less increment in the estimation error as $N$ decreases, compared to Poisson\n\nfactor model.\n\n![table_14](table_14)\n\nTable 15: (Dependent case with high-dimensional setting) Mean estimation error among 50 independent replications based on the proposed estimator and the estimator under the Poisson factor model under each of the 30 simulation settings.\n\n![table_15](table_15)\n\nTable 16: (Dependent case with high-dimensional setting) The number of times that the true number of factors is under- or over-selected selected among 50 independent replications under each of the 30 simulation settings.", "tables": {"table_14": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Kernel-based method | C1 | C2 | C3 | C1 | C2 | C3 |\n| $N=100$ | 0.0204 | 0.0221 | 0.0204 | 0.0263 | 0.0279 | 0.0260 |\n| $N=200$ | 0.0168 | 0.0190 | 0.0173 | 0.0219 | 0.0242 | 0.0222 |\n| $N=400$ | 0.0160 | 0.0188 | 0.0169 | 0.0208 | 0.0236 | 0.0216 |\n| $N=800$ | 0.0164 | 0.0195 | 0.0176 | 0.0214 | 0.0248 | 0.0224 |\n| $N=1600$ | 0.0159 | 0.0190 | 0.0170 | 0.0205 | 0.0240 | 0.0217 |\n| Poisson factor model | C1 | C2 | C3 | C1 | C2 | C3 |\n| $N=100$ | 0.0101 | 0.9409 | 0.7253 | 0.0018 | 0.9981 | 0.7287 |\n| $N=200$ | 0.0057 | 0.9595 | 0.7253 | 0.0127 | 0.6468 | 0.5293 |\n| $N=400$ | 0.0035 | 0.9688 | 0.7205 | 0.0071 | 0.6660 | 0.5286 |\n| $N=800$ | 0.0024 | 1.0007 | 0.7331 | 0.0043 | 0.6838 | 0.5289 |\n| $N=1600$ | 0.0018 | 1.0012 | 0.7542 | 0.0023 | 0.6955 | 0.5491 |", "table_15": "|  | S1 |  |  | S2 |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| Under-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $N=100$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $N=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $N=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $N=800$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $N=1600$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| Over-selection | C1 | C2 | C3 | C1 | C2 | C3 |\n| $N=100$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $N=200$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $N=400$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $N=800$ | 0 | 0 | 0 | 0 | 0 | 0 |\n| $N=1600$ | 0 | 0 | 0 | 0 | 0 | 0 |"}, "images": {}}, {"section_id": 24, "text": "# References \n\nAbu-Libdeh, H., Turnbull, B. W., and Clark, L. C. (1990). Analysis of multi-type recurrent events in longitudinal studies; application to a skin cancer prevention trial. Biometrics, $46: 1017-1034$.\n\nAndersen, P. K., Borgan, O., Gill, R. D., and Keiding, N. (1993). Statistical models based on counting processes. Springer, New York, NY.\n\nBai, J. and Li, K. (2012). Statistical analysis of factor models of high dimension. The Annals of Statistics, 40:436-465.\n\nBai, J. and Ng, S. (2002). Determining the number of factors in approximate factor models. Econometrica, 70(1):191-221.\n\nBai, J. and Ng, S. (2023). Approximate factor models with weaker loadings. Journal of Econometrics, 235(2):1893-1916.\n\nBogdanowicz, A. and Guan, C. (2022). Dynamic topic modeling of twitter data during the COVID-19 pandemic. Plos One, 17(5):1-22.\n\nBrowne, M. W. (2001). An overview of analytic rotation in exploratory factor analysis. Multivariate Behavioral Research, 36(1):111-150.\n\nChamberlain, G. and Rothschild, M. (1983). Arbitrage, factor structure, and mean-variance analysis on large asset markets. Econometrica, 51(5):1281 - 1304.\n\nChen, B. E., Cook, R. J., Lawless, J. F., and Zhan, M. (2005). Statistical methods for multivariate interval-censored recurrent events. Statistics in Medicine, 24(5):671-691.\n\nChen, L., Dolado, J. J., and Gonzalo, J. (2021). Quantile factor models. Econometrica, 89(2):875-910.\n\nChen, Y. (2016). Thinning algorithms for simulating point processes. Florida State University, Tallahassee, FL.\n\nChen, Y. (2020). A continuous-time dynamic choice measurement model for problemsolving process data. Psychometrika, 85(4):1052-1075.\n\nChen, Y. and Li, X. (2022). Determining the number of factors in high-dimensional generalized latent factor models. Biometrika, 109(3):769-782.\n\nChen, Y., Li, X., and Zhang, S. (2020). Structured latent factor analysis for large-scale data: Identifiability, estimability, and their implications. Journal of the American Statistical Association, 115(532):1756-1770.\n\nChen, Y. and Zhang, S. (2020). A latent Gaussian process model for analysing intensive longitudinal data. British Journal of Mathematical and Statistical Psychology, 73:237260 .\n\nCook, R. J. and Lawless, J. F. (2007). The statistical analysis of recurrent events. Springer, New York, NY.\n\nGy\u00f6rfi, L., Kohler, M., Krzyzak, A., and Walk, H. (2002). A distribution-free theory of nonparametric regression. Springer, New York, NY.\n\nHe, Y., Kong, X., Trapani, L., and Yu, L. (2023). One-way or two-way factor model for matrix sequences? Journal of Econometrics, 235(2):1981-2004.\n\nKaiser, H. F. (1958). The varimax criterion for analytic rotation in factor analysis. Psychometrika, 23(3):187-200.\n\nLata\u0142a, R., van Handel, R., and Youssef, P. (2018). The dimension-free structure of nonhomogeneous random matrices. Inventiones mathematicae, 214(3):1031-1080.\n\nLedoux, M. and Talagrand, M. (2013). Probability in Banach Spaces: isoperimetry and processes. Springer Science \\& Business Media.\n\nLiang, S., Zhang, X., Ren, Z., and Kanoulas, E. (2018). Dynamic embeddings for user profiling in twitter. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery 8 Data Mining, KDD '18, page 1764-1773, New York, NY, USA. Association for Computing Machinery.\n\nLin, D. Y., Wei, L.-J., Yang, I., and Ying, Z. (2000). Semiparametric regression for the mean and rate functions of recurrent events. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 62(4):711-730.\n\nLiu, W., Lin, H., Zheng, S., and Liu, J. (2023). Generalized factor model for ultra-high dimensional correlated variables with mixed types. Journal of the American Statistical Association, 118(542):1385-1401.\n\nLu, Z.-H., Chow, S.-M., Sherwood, A., and Zhu, H. (2015). Bayesian analysis of ambulatory blood pressure dynamics with application to irregularly spaced sparse data. The Annals of Applied Statistics, 9(3):1601 - 1620.\n\nRohe, K. and Zeng, M. (2023). Vintage factor analysis with varimax performs statistical inference. Journal of the Royal Statistical Society Series B (Statistical Methodology), 85(4):1037-1060.\n\nStewart, G. and Sun, J. (1990). Matrix Perturbation Theory. Computer Science and Scientific Computing. Elsevier Science.\n\nSun, J. (2006). The statistical analysis of interval-censored failure time data. Springer, New York, NY.\n\nTang, N., Chow, S.-M., Ibrahim, J. G., and Zhu, H. (2017). Bayesian sensitivity analysis of a nonlinear dynamic factor analysis model with nonparametric prior and possible nonignorable missingness. Psychometrika, 82(4):875-903.\n\nWang, D., Liu, X., and Chen, R. (2019). Factor models for matrix-valued high-dimensional time series. Journal of Econometrics, 208(1):231-248.\n\nWedel, M. and Kamakura, W. A. (2001). Factor analysis with (mixed) observed and latent variables in the exponential family. Psychometrika, 66(4):515-530.\n\nXia, Z. and Qiu, P. (2015). Jump information criterion for statistical inference in estimating discontinuous curves. Biometrika, 102(2):397-408.\n\nYang, L. (2022). Nonparametric copula estimation for mixed insurance claim data. Journal of Business $\\mathcal{E}$ Economic Statistics, 40(2):537-546.", "tables": {}, "images": {}}], "id": "2405.19803v2", "authors": ["Fangyi Chen", "Yunxiao Chen", "Zhiliang Ying", "Kangjie Zhou"], "categories": ["stat.ME", "math.ST", "stat.TH"], "abstract": "Recurrent event time data arise in many studies, including biomedicine,\npublic health, marketing, and social media analysis. High-dimensional recurrent\nevent data involving many event types and observations have become prevalent\nwith advances in information technology. This paper proposes a semiparametric\ndynamic factor model for the dimension reduction of high-dimensional recurrent\nevent data. The proposed model imposes a low-dimensional structure on the mean\nintensity functions of the event types while allowing for dependencies. A\nnearly rate-optimal smoothing-based estimator is proposed. An information\ncriterion that consistently selects the number of factors is also developed.\nSimulation studies demonstrate the effectiveness of these inference tools. The\nproposed method is applied to grocery shopping data, for which an interpretable\nfactor structure is obtained.", "updated": "2025-04-01T05:51:20Z", "published": "2024-05-30T08:12:21Z"}