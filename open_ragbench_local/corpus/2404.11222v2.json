{"title": "On the algebra of equal-input matrices in time-inhomogeneous Markov\n  flows", "sections": [{"section_id": 0, "text": "#### Abstract\n\nMarkov matrices of equal-input type constitute a widely used model class. The corresponding equal-input generators span an interesting subalgebra of the real matrices with zero row sums. Here, we summarise some of their amazing properties and discuss the corresponding Markov embedding problem, both homogeneous and inhomogeneous in time. In particular, we derive exact and explicit solutions for time-inhomogeneous Markov flows with non-commuting generator families of equal-input type and beyond.", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1. InTRODUCTION\n\nMarkov processes are fundamental throughout probability theory and its applications. They often show up as Markov chains, both in discrete and in continuous time; see [18] for a comprehensive exposition. When the state space is finite, one most effectively works with Markov matrices (non-negative matrices with unit row sums) or Markov semigroups. In the case of continuous time, the formulation is usually based on a Markov generator, $Q$ say, and the relevant (commutative) semigroup is written as $\\{M(t): t \\geqslant 0\\}$ with $M(t)=\\mathrm{e}^{t Q}$. Markov generators have non-negative off-diagonal entries and zero row sums, and are also known as rate matrices (we will use both terms synonymously). When only a single Markov matrix $M$ is given, it is then a natural question whether it can occur in such a semigroup, which is known as the Markov embedding problem; see $[11,16]$ for early work and $[10,3]$ and references therein for some recent developments.\n\nWhile this already is a hard problem, and far from being solved in sufficient generality, it has one drawback in that it only asks for the embedding into a time-homogeneous process. In reality, there is often no good reason for the assumption that a time-independent generator should exist. Further, in the time-dependent case of commuting matrices, one faces the issue of common eigenvectors, and hence strong restrictions on the possible equilibria. Indeed, some experimental evidence [8] around non-stationarity thus points towards the need for timedependent generators that do not commute with one another. Consequently, one should also look at the embeddability into a time-inhomogeneous process with, in general, non-commuting generators. For finite state spaces, a partial answer is given by Johansen's theorem [15], which states that any Markov matrix that is embeddable in such a Markov flow can be approximated arbitrarily well by the product of finitely many homogeneously embeddable matrices.\n\n[^0]\n[^0]:    2010 Mathematics Subject Classification. 60J27, 34A05, 15A16.\n    Key words and phrases. Markov matrices and generators, matrix algebras, embedding problem.\n\nWhile analysing part of this in previous work [4, 5], it became clear that there currently is a rather limited understanding of the structure of Markov flows, and increasingly so with growing dimension. In particular, when we look at the Cauchy problem $\\dot{M}=M Q$ with $M(0)=\\mathbb{1}$, which is the Kolmogorov forward equation of the underlying process, there are hardly any explicit results beyond the case where the matrices $Q(t)$ all commute with one another, in which case the flow is given by $M(t)=\\exp \\left(\\int_{0}^{t} Q(\\tau) \\mathrm{d} \\tau\\right)$ with $t \\geqslant 0$, as is straightforward to verify. Once again, there is usually no good reason for the commutativity assumption, and the main goal of this paper is to derive some solvable cases that go beyond, where we start from some models that are widely used in applications. Here, we also aim at a better understanding of the corresponding Markov flows.\n\nNow, the existence of a solution to the Cauchy problem, as well as its uniqueness under some standard continuity assumptions on the generator family, is guaranteed by the PicardLindel\u00f6f theorem of ordinary differential equations (ODEs); see [1, 20] for classic sources. Also, one can use the Peano-Baker series (PBS) to write down a convergent series expansion of the solution, which can be quite helpful in understanding various aspects of the solution; see [2] and references therein for a summary. In addition, there is another approach via the Magnus expansion (ME), see [17, 6], which harvests the observation that, since $M(0)=\\mathbb{1}$ possesses a real matrix logarithm (in fact, the principal one), this will remain true at least for small values of $t>0$. So, one can write $M(t)=\\exp (R(t))$ for some interval $[0, T]$, with $T>0$ and $R(0)=\\mathbb{0}$, and derive an ODE for the matrix function $R$ from it.\n\nThis is possible via some underlying Lie theory, as summarised in [6], and the relevant ODE is based on a series expansion in terms of iterated matrix commutators (or Lie brackets). As with the PBS, beyond the trivial case of commuting generators, it is rarely possible to compute this series explicitly, but there are instances known where it is; compare [6] and references therein. It is perhaps surprising that our featured model class of equal-input matrices, which we take from applications in phylogenetics [19], has a sufficiently strong algebraic structure (see Fact 2.3 below) that leads to a natural and wide class of non-commuting families where an explicit solution is also possible. In fact, already in [4], we highlighted this structure, when we demonstrated that the equal-input matrices allow for an explicit formula of the BakerCampbell-Hausdorff (BCH) formula in this class, where we refer to the WIKIPIDIA for some background on BCH. Below, we employ the PBS and the ME to derive some explicit solutions in closed form.\n\nThe paper is organised as follows. We collect some notions and results, in particular around equal-input matrices, in Section 2, before we solve the Cauchy problem for the case that the generator family consists solely of general equal-input matrices (Section 3). Generically, these matrices do not commute with one another, but have a sufficiently strong algebraic structure to enable an explicit solution. A close inspection of the algebraic structure reveals that even more is possible, due to a particular ideal within the algebra of zero row sum matrices. This leads to two more general classes where we obtain a full solution, which are treated step by step in Section 4. As we proceed, for the sake of readability, we sometimes employ a style of\n\npresentation where we first derive the results prior to their formal statement as a theorem. Some material on the PBS and the ME is briefly summarised in the Appendix.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 2. Notation and Preliminaries \n\nFor a row vector $\\boldsymbol{x}=\\left(x_{1}, \\ldots, x_{d}\\right) \\in \\mathbb{R}^{d}$, we define the equal-rows matrix\n\n$$\nC_{\\boldsymbol{x}}:=\\left(\\begin{array}{ccc}\nx_{1} & \\cdots & x_{d} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nx_{1} & \\cdots & x_{d}\n\\end{array}\\right)\n$$\n\nwhich has rank 0 for $\\boldsymbol{x}=\\mathbf{0}$ and rank 1 otherwise. Its spectrum is $\\sigma\\left(C_{\\boldsymbol{x}}\\right)=\\{x, 0, \\ldots, 0\\}$ in multi-set notation, with\n\n$$\nx:=\\operatorname{tr}\\left(C_{\\boldsymbol{x}}\\right)=x_{1}+\\cdots+x_{d}\n$$\n\ndenoting the summatory parameter of $C_{\\boldsymbol{x}}$, while $\\boldsymbol{x}$ is called the parameter vector of $C_{\\boldsymbol{x}}$. Note that $x=0$ is possible for $\\boldsymbol{x} \\neq \\mathbf{0}$. An important relation is\n\n$$\nC_{x} C_{\\boldsymbol{y}}=x C_{\\boldsymbol{y}}\n$$\n\nwhich holds for arbitrary $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{d}$ and has the following consequence.\nFact 2.1. Any matrix $C_{\\boldsymbol{x}}$ with $x=\\operatorname{tr}\\left(C_{\\boldsymbol{x}}\\right)=0$ is nilpotent, with $C_{0}=\\mathbb{0}$ and nilpotency degree 2 in all other cases. When $\\boldsymbol{x} \\neq \\mathbf{0}$, the null space of $C_{\\boldsymbol{x}}$ has dimension $d-1$. Whenever $x \\neq 0$, $C_{\\boldsymbol{x}}$ is diagonalisable. Otherwise, if $\\boldsymbol{x} \\neq \\mathbf{0}$ but $d \\geqslant 2$ and $x=0$, the Jordan normal form of $C_{\\boldsymbol{x}}$ has precisely one elementary Jordan block of the form $\\left(\\begin{array}{ll}0 & 1 \\\\ 0 & 0\\end{array}\\right)$ and is diagonal otherwise.\n\nProof. The first claim follows immediately from Eq. (1) by taking $\\boldsymbol{y}=\\boldsymbol{x}$. When $\\boldsymbol{x} \\neq \\mathbf{0}$, the relation $\\left(v_{1}, \\ldots, v_{d}\\right) C_{\\boldsymbol{x}}=\\mathbf{0}$ is satisfied precisely by all $\\boldsymbol{v}$ with $v_{1}+\\ldots+v_{d}=0$, which form a subspace of $\\mathbb{R}^{d}$ of dimension $(d-1)$. When $x \\neq 0$, hence also $\\boldsymbol{x} \\neq \\mathbf{0}$, we get one extra eigenvector for the eigenvalue $x$, and thus diagonalisability of $C_{\\boldsymbol{x}}$. In the remaining case, the consequence on the Jordan normal form is clear.\n\nWhen $x=1$ and all entries of $\\boldsymbol{x}$ are non-negative, $C_{\\boldsymbol{x}}$ is a Markov matrix, but of little interest as it is singular for $d \\geqslant 2$. A more interesting set of Markov matrices for $d \\geqslant 2$, with numerous applications in phylogeny [19] for instance, is defined by\n\n$$\nM_{\\boldsymbol{x}}=(1-x) \\mathbb{1}+C_{\\boldsymbol{x}}\n$$\n\nwhen $\\boldsymbol{x} \\geqslant \\mathbf{0}$ (meaning $x_{i} \\geqslant 0$ for all $1 \\leqslant i \\leqslant d$ ) with $1+x_{i} \\geqslant x$ for all $i$. By slight abuse of notation, $x$ is called the summatory parameter of $M_{\\boldsymbol{x}}$. The condition that all row sums are 1 is satisfied automatically. These are the well-known equal-input Markov matrices, which have $d$ degrees of freedom and form a closed convex set. Let us describe them in a little more detail. The non-negativity condition of the matrix elements restricts $x$ to the interval $\\left[0, \\frac{d}{d-1}\\right]$, with the maximal value being attained at $x_{1}=x_{2}=\\ldots=x_{d}=\\frac{1}{d-1}$. This implies the following result, where $\\mathbf{1}:=(1, \\ldots, 1)$ and $\\boldsymbol{e}_{i}$ denotes the standard unit row vector with 1 in position $i$ and 0 everywhere else; see [4, Lemma 2.8].\n\nFact 2.2. The equal-input Markov matrices for fixed $d \\geqslant 2$ form a d-dimensional convex set that is closed and has $d+2$ extremal elements, namely the matrices $C_{\\boldsymbol{e}_{i}}$ with $1 \\leqslant i \\leqslant d$ together with $\\mathbb{1}$ and $\\frac{1}{d-1}\\left(C_{1}-\\mathbb{1}\\right)$.\n\nThe matrices $M_{x}$ where all entries of $\\boldsymbol{x}$ are equal are called the constant-input matrices. Note that in Eq. (2), independently of whether $M_{x}$ is Markov or not, one always has the spectrum $\\sigma\\left(M_{x}\\right)=\\{1,1-x, \\ldots, 1-x\\}$ and thus $\\operatorname{det}\\left(M_{x}\\right)=(1-x)^{d-1}$.\n\nOf particular interest are the equal-input counterparts with zero row sums, as defined by\n\n$$\nQ_{x}=-x \\mathbb{1}+C_{x}\n$$\n\nwhich satisfy $\\lambda Q_{x}=Q_{\\lambda x}$ for all $\\lambda \\in \\mathbb{R}$ and $\\boldsymbol{x} \\in \\mathbb{R}^{d}$. These matrices all lie in the non-unital, real matrix algebra of zero row sum matrices,\n\n$$\n\\mathcal{A}_{0}=\\mathcal{A}_{0}^{(d)}:=\\left\\{A \\in \\operatorname{Mat}(d, \\mathbb{R}): \\sum_{j=1}^{d} A_{i j}=0 \\text { for all } 1 \\leqslant i \\leqslant d\\right\\}\n$$\n\nIn fact, all matrices of the form (3),\n\n$$\n\\mathcal{E}_{0}=\\mathcal{E}_{0}^{(d)}:=\\left\\{Q_{x}: \\boldsymbol{x} \\in \\mathbb{R}^{d}\\right\\}\n$$\n\nform a $d$-dimensional subalgebra of $\\mathcal{A}_{0}^{(d)}$ with interesting properties. ${ }^{1}$ Since $\\mathbb{0} \\in \\mathcal{E}_{0}$, the vector space property of $\\mathcal{E}_{0}$ is clear, while (1) and (3) together give\n\n$$\nQ_{x} Q_{y}=-y Q_{x}\n$$\n\nwhich implies that $\\mathcal{E}_{0}$ is an algebra. It is not unital, because it has no two-sided unit, though it has right units [7], which are precisely the matrices $Q_{x}$ with $x=-1$, as follows from (4). Within $\\mathcal{E}_{0}$, one also has the special class of constant-input matrices that are the multiples of $J_{d}-\\mathbb{1}$, where $J_{d}=\\frac{1}{d} C_{1}$. They will show up a number of times. Another important subset, comprising nilpotent elements only, is\n\n$$\n\\mathcal{E}_{0}^{\\prime}:=\\left\\{Q_{x} \\in \\mathcal{E}_{0}: x=0\\right\\}=\\left\\{C_{x}: \\boldsymbol{x} \\in \\mathbb{R}^{d} \\text { and } \\operatorname{tr}\\left(C_{x}\\right)=0\\right\\}\n$$\n\nwhich is a subalgebra of $\\mathcal{A}_{0}$. Clearly, for any $C_{x} \\in \\mathcal{E}_{0}^{\\prime}$, one has $A C_{x}=\\mathbb{0} \\in \\mathcal{E}_{0}^{\\prime}$ for all $A \\in \\mathcal{A}_{0}$. Also, if we write $A \\in \\mathcal{A}_{0}$ as a collection of column vectors, $A=\\left(\\boldsymbol{a}_{1}^{\\top}, \\ldots, \\boldsymbol{a}_{d}^{\\top}\\right)$, we get $C_{x} A=C_{y}$ with $y_{i}=\\boldsymbol{x} \\cdot \\boldsymbol{a}_{i}^{\\top}$. Since $\\operatorname{tr}\\left(C_{x} A\\right)=\\operatorname{tr}\\left(A C_{x}\\right)=\\operatorname{tr}(\\mathbb{0})=0$, we see that $C_{y} \\in \\mathcal{E}_{0}^{\\prime}$. Further, for elements from $\\mathcal{E}_{0}^{\\prime}$, Eq. (1) simplifies to $C_{x} C_{y}=\\mathbb{0}$. Together, this shows the following.\n\nFact 2.3. The subalgebra $\\mathcal{E}_{0}^{\\prime}$ is a two-sided nil ideal, both in $\\mathcal{E}_{0}$ and in $\\mathcal{A}_{0}$. Further, for any $A \\in \\mathcal{A}_{0}$ and $C \\in \\mathcal{E}_{0}^{\\prime}$, one has $A C=\\mathbb{0}$.\n\nA special case of (4) is $Q_{x}^{2}=-x Q_{x}$, and hence $Q_{x}^{n}=(-x)^{n-1} Q_{x}$ for $n \\in \\mathbb{N}$, which results in a simple formula for its time-scaled exponential,\n\n$$\n\\begin{aligned}\nM(t) & :=\\mathrm{e}^{t Q_{x}}=\\mathbb{1}+\\sum_{n=1}^{\\infty} \\frac{t^{n}}{n!}(-x)^{n-1} Q_{x}=\\mathbb{1}+\\frac{1-\\mathrm{e}^{-t x}}{x} Q_{x} \\\\\n& =\\mathrm{e}^{-t x} \\mathbb{1}+\\frac{1-\\mathrm{e}^{-t x}}{x} C_{x}=M_{\\boldsymbol{c}(t)}\n\\end{aligned}\n$$\n\n[^0]\n[^0]:    ${ }^{1}$ From now on, whenever the dimension is arbitrary but fixed, we shall simply write $\\mathcal{A}_{0}$ and $\\mathcal{E}_{0}$.\n\nwith $\\boldsymbol{c}(t)=\\frac{1-\\mathrm{e}^{-t x}}{x} \\boldsymbol{x}$ and $c(t)=1-\\mathrm{e}^{-t x}$. This formula also holds when $x=0$, then with the appropriate limits via de l'Hospital's rule. So, each $M(t)$ is an equal-input matrix with row sum 1 and (time-dependent) summatory parameter $c(t)=1-\\mathrm{e}^{-t x}$. In particular, when $Q_{\\boldsymbol{x}}$ is a Markov generator, which happens if and only if $\\boldsymbol{x} \\geqslant \\mathbf{0}$, the set $\\{M(t): t \\geqslant 0\\}$ defines a (commutative) semigroup of Markov matrices. Since $M(0)=\\mathbb{1}$, it is actually a monoid.\n\nLet us mention an interesting property of equal-input matrices in the context of the BCH formula. Given $Q_{\\boldsymbol{x}}, Q_{\\boldsymbol{y}} \\in \\mathcal{E}_{0}$, they will generally not commute, and neither will their exponentials. However, the product $\\mathrm{e}^{Q_{\\boldsymbol{x}}} \\mathrm{e}^{Q_{\\boldsymbol{y}}}$ is still of equal-input type and has a real logarithm, which can be given explicitly as a linear combination in $Q_{\\boldsymbol{x}}$ and $Q_{\\boldsymbol{y}}$; see [4, Thm. 2.15]. To formulate it, we define the positive function $h$ on $\\mathbb{R}$ by $h(u)=\\frac{1-\\mathrm{e}^{-u}}{u}$ for $u \\neq 0$ and $h(0)=1$, which is the continuous extension to $u=0$.\n\nFact 2.4. Let $Q_{\\boldsymbol{x}}, Q_{\\boldsymbol{y}} \\in \\mathcal{E}_{0}$. Then, the product of their exponentials has a unique real logarithm of equal-input type. The latter is the principal matrix logarithm and reads\n\n$$\n\\log \\left(\\mathrm{e}^{Q_{\\boldsymbol{x}}} \\mathrm{e}^{Q_{\\boldsymbol{y}}}\\right)=\\frac{1}{h(x+y)}\\left(h(x) \\mathrm{e}^{-y} Q_{\\boldsymbol{x}}+h(y) Q_{\\boldsymbol{y}}\\right)\n$$\n\nProof. Since $\\mathrm{e}^{Q_{\\boldsymbol{x}}}=\\mathbb{1}+h(x) Q_{\\boldsymbol{x}}$ and $\\mathrm{e}^{Q_{\\boldsymbol{y}}}=\\mathbb{1}+h(y) Q_{\\boldsymbol{y}}$ by (5), with our previous convention for $x$ and $y$, one finds\n\n$$\n\\mathrm{e}^{Q_{\\boldsymbol{x}}} \\mathrm{e}^{Q_{\\boldsymbol{y}}}=\\mathbb{1}+h(x) Q_{\\boldsymbol{x}}+h(y) Q_{\\boldsymbol{y}}+h(x) h(y) Q_{\\boldsymbol{x}} Q_{\\boldsymbol{y}} \\stackrel{(4)}{=} \\mathbb{1}+h(x) \\mathrm{e}^{-y} Q_{\\boldsymbol{x}}+h(y) Q_{\\boldsymbol{y}}=\\mathbb{1}+Q_{\\boldsymbol{z}}\n$$\n\nwith the new parameters\n\n$$\n\\boldsymbol{z}=h(x) \\mathrm{e}^{-y} \\boldsymbol{x}+h(y) \\boldsymbol{y} \\quad \\text { and } \\quad z=1-\\mathrm{e}^{-(x+y)}\n$$\n\nRecall that $Q_{\\boldsymbol{z}}^{n}=(-z)^{n-1} Q_{\\boldsymbol{z}}$ holds for $n \\in \\mathbb{N}$. Then, for $|z|<1$, which holds when $x+y \\geqslant 0$, we can compute\n\n$$\n\\log \\left(\\mathbb{1}+Q_{\\boldsymbol{z}}\\right)=\\sum_{n \\geqslant 1} \\frac{z^{n-1}}{n} Q_{\\boldsymbol{z}}=\\frac{\\log (1-z)}{-z} Q_{\\boldsymbol{z}}=\\frac{1}{h(x+y)} Q_{\\boldsymbol{z}}=Q_{\\boldsymbol{z} / h(x+y)}\n$$\n\nwhich is of the form claimed. This now has to be extended to all $x$ and $y$.\nSince $\\sigma\\left(\\mathbb{1}+Q_{\\boldsymbol{z}}\\right)=\\left\\{1, \\mathrm{e}^{-(x+y)}, \\ldots, \\mathrm{e}^{-(x+y)}\\right\\} \\subset \\mathbb{R}_{+}$, we immediately know from Culver's theorem [9, Thm. 1] that a real logarithm exists. In particular, the principal matrix logarithm is well defined for these matrices, which can be given in integral form [13, Thm. 11.1] as\n\n$$\n\\log \\left(\\mathbb{1}+Q_{\\boldsymbol{z}}\\right)=\\int_{0}^{1} Q_{\\boldsymbol{z}}\\left(\\mathbb{1}+\\tau Q_{\\boldsymbol{z}}\\right)^{-1} \\mathrm{~d} \\tau\n$$\n\nThe resulting matrix is again $Q_{\\boldsymbol{z} / h(x+y)}$, as follows from our previous calculation in conjunction with a standard analytic continuation argument. Further, the claimed uniqueness follows from the observation that $Q_{\\boldsymbol{a}}=Q_{\\boldsymbol{b}}$ in (3) is only possible for $\\boldsymbol{a}=\\boldsymbol{b}$.\n\nThe real logarithm of $\\mathbb{1}+Q_{\\boldsymbol{z}}$ in the last proof will not be unique when $d>3$, as there are degeneracies in the spectrum, but no other real logarithm can be of equal-input form, which follows from [4, Lemma 2.14].\n\nThis is a rare case of non-commuting matrices where one can give a closed expression for the BCH formula. ${ }^{2}$ Indeed, via this route, one would find\n\n$$\n\\log \\left(\\mathrm{e}^{Q_{x}} \\mathrm{e}^{Q_{y}}\\right)=\\left(1-\\frac{y}{2}+\\frac{y(y-x)}{12}+\\frac{x y^{2}}{24}+\\ldots\\right) Q_{x}+\\left(1+\\frac{x}{2}+\\frac{x(x-y)}{12}-\\frac{x^{2} y}{24}+\\ldots\\right) Q_{y}\n$$\n\nwhere the dots indicate higher order terms that would stem from fourfold or higher commutators in the BCH expansion. The Taylor expansion of the formula in Fact 2.4 to third order agrees with this, as it must, because no other logarithm of equal-input type can exist.", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 3. InHOMOGENEOUS FLOW \n\nConsider the Cauchy problem defined by $\\dot{M}=M Q$ with $M(0)=\\mathbb{1}$, where $Q=Q(t)$ with $t \\geqslant 0$ is a (possibly piecewise) continuous family of equal-input matrices from $\\mathcal{E}_{0}$.\n\nLemma 3.1. Assume that $Q(t) \\in \\mathcal{E}_{0}$ for all $t \\geqslant 0$, and that $Q(t)$ is piecewise continuous. Then, the Cauchy problem\n\n$$\n\\dot{M}=M Q \\quad \\text { with } \\quad M(0)=\\mathbb{1}\n$$\n\nhas a unique solution, where each $M(t)$ with $t \\geqslant 0$ is a real matrix with row sum 1 that has equal-input form. Further, if all $Q(t)$ are rate matrices, $\\{M(t): t \\geqslant 0\\}$ is a flow of equal-input Markov matrices.\n\nProof. Assume first that $Q(t)$ is continuous, but not necessarily of equal-input type. When $[Q(t), Q(s)]=\\mathbb{0}$ for all $t, s \\geqslant 0$, we get the solution in closed form as\n\n$$\nM(t)=M(0) \\exp \\left(\\int_{0}^{t} Q(\\tau) \\mathrm{d} \\tau\\right)=\\exp \\left(\\int_{0}^{t} Q(\\tau) \\mathrm{d} \\tau\\right)\n$$\n\nby classic ODE theory $[1,20]$. Within $\\mathcal{E}_{0}$, Eq. (3) implies that the easy case of commuting matrices only occurs when $Q(t)=\\mu(t) Q_{0}$ with a fixed $Q_{0}$. Then, the solution (6) simplifies to $M(t)=\\exp \\left(u(t) Q_{0}\\right)$ with $u(t)=\\int_{0}^{t} \\mu(\\tau) \\mathrm{d} \\tau$, and the claim on the equal-input nature of $M(t)$ is obvious from the fact that $Q_{0}^{2}$ is a scalar multiple of $Q_{0}$.\n\nAlternatively, one can see it from the convergent series expansion of the matrix exponential in (6). Indeed, when all $Q(t)$ lie in $\\mathcal{E}_{0}$, we have $\\int_{0}^{t} Q(\\tau) \\mathrm{d} \\tau \\in \\mathcal{E}_{0}$ for every $t \\geqslant 0$, and if all $Q(\\tau)$ are rate matrices, then so is the integral. Since the algebra $\\mathcal{E}_{0}$ is closed under multiplication and under taking limits, the equal-input structure is preserved and then inherited by $M(t)$.\n\nIn the general case with non-commuting $Q(t)$, we can represent the solution of the Cauchy problem by the PBS [2]; see [5] for a formulation that is tailored to our present setting, and the Appendix for a brief summary. Each summand of the PBS (except the first, which is $\\mathbb{1}$ ) lies in $\\mathcal{E}_{0}$. As the series is compactly converging, it then admits the same line of conclusions.\n\nWhen $Q(t)$ is piecewise continuous, the locations of the potential jumps are isolated, and one can solve the Cauchy problem for each continuity stretch individually, then with the last value of $M(t)$ as the new initial condition, modifying (6) or the PBS in the obvious way. Harvesting the semigroup property of the (convex) set of equal-input Markov matrices from [4], we can put the pieces together and obtain the claimed properties for the entire flow.\n\n[^0]\n[^0]:    ${ }^{2}$ We refer to the WIKIPEDI\u00c1 entry on the BCH formula for a suitable summary with references.\n\nWith this result, we can use the parametrising vectors to reduce the problem to an ODE for vectors in $\\mathbb{R}^{d}$ as follows. Assume that we have $Q(t)=Q_{\\boldsymbol{q}(t)}=-q(t) \\mathbb{1}+C_{\\boldsymbol{q}(t)}$ for $t \\geqslant 0$, with $\\boldsymbol{q}(t)$ being piecewise continuous. By Lemma 3.1, we know that the solution exists and must be of the form $M(t)=M_{\\boldsymbol{x}(t)}=(1-x(t)) \\mathbb{1}+C_{\\boldsymbol{x}(t)}$, for all $t \\geqslant 0$. Clearly, we then have $\\dot{M}(t)=-\\dot{x}(t) \\mathbb{1}+C_{\\boldsymbol{x}(t)}$, which is to be compared with\n\n$$\nM(t) Q(t)=(x(t)-1) q(t) \\mathbb{1}+C_{\\boldsymbol{q}(t)-q(t) \\boldsymbol{x}(t)}\n$$\n\nas follows from a short calculation that uses $C_{\\boldsymbol{x}} C_{\\boldsymbol{q}}=x C_{\\boldsymbol{q}}$. Since $\\mathbb{1} \\notin \\mathcal{E}_{0}$, this leads to the simpler Cauchy problem\n\n$$\n\\dot{\\boldsymbol{x}}+q \\boldsymbol{x}=\\boldsymbol{q} \\quad \\text { with } \\quad \\boldsymbol{x}(0)=\\mathbf{0}\n$$\n\nThis inhomogeneous, linear first order ODE can be solved by the standard variation of constants method [1, Thm. 11.13 and Rem. 11.14], which results in\n\n$$\n\\boldsymbol{x}(t)=\\exp \\left(-\\int_{0}^{t} q(\\rho) \\mathrm{d} \\rho\\right) \\int_{0}^{t} \\boldsymbol{q}(\\tau) \\exp \\left(\\int_{0}^{\\tau} q(\\sigma) \\mathrm{d} \\sigma\\right) \\mathrm{d} \\tau\n$$\n\nClearly, this gives $\\boldsymbol{x}(0)=\\mathbf{0}$, and also the scalar counterpart of the vector-valued ODE, namely $\\dot{x}+q x=q$ together with $x(0)=0$, which is also needed to satisfy the original Cauchy problem. We have thus established the following result.\n\nProposition 3.2. The Cauchy problem of Lemma 3.1 has the solution $M(t)=M_{\\boldsymbol{x}(t)}$ for $t \\geqslant 0$, with the $\\mathbb{R}^{d}$-valued vector function $\\boldsymbol{x}$ from (7).\n\nHere, we have $\\operatorname{det}(M(t))=(1-x(t))^{d-1}$, where $x(0)=0$ matches $\\operatorname{det}(M(0))=\\operatorname{det}(\\mathbb{1})=1$. Since $\\frac{\\mathrm{d}}{\\mathrm{d} t} \\operatorname{det}(M(t))=\\operatorname{det}(M(t)) \\operatorname{tr}(Q(t))$ by Liouville's theorem [1, Prop. 11.4], we get\n\n$$\n\\operatorname{det}(M(t))=\\exp \\left(\\int_{0}^{t} \\operatorname{tr}(Q(\\tau)) \\mathrm{d} \\tau\\right)\n$$\n\nwhich never vanishes. But this implies that $x(t)$, which starts at 0 , can never take the value 1 , as this would make $M(t)$ singular. With $x(0)=0$, this implies $x(t)<1$ for all $t \\geqslant 0$. Note that all eigenvalues of $M(t)$ are positive real numbers in this case.\n\nRemark 3.3. Liouville's theorem actually implies that $\\operatorname{det}(M(t))$ is either identically 0 or never vanishes [1, Cor. 11.5]. Indeed, when we admit more general initial conditions, the formula from (8) is simply replaced by\n\n$$\n\\operatorname{det}(M(t))=\\operatorname{det}(M(0)) \\exp \\left(\\int_{0}^{t} \\operatorname{tr}(Q(\\tau)) \\mathrm{d} \\tau\\right)\n$$\n\nwhere $Q(t)=-q(t) \\mathbb{1}+C_{\\boldsymbol{q}(t)}$ with $\\operatorname{tr}\\left(C_{\\boldsymbol{q}(t)}\\right)=q(t)$, hence $\\operatorname{tr}(Q(t))=-(d-1) q(t)$. In particular, the exponential factor is strictly positive for all $t \\geqslant 0$. Recall that $\\operatorname{det}(M(0))=(1-x(0))^{d-1}$. Consequently, the three possible cases now are $x(0)<1$ (which means $\\operatorname{det}(M(t))>0$ for all $t \\geqslant 0$ and includes the case treated above), $x(0)=1$ (forcing $\\operatorname{det}(M(t)) \\equiv 0$ ), and $x(0)>1$. The last case either implies $\\operatorname{det}(M(t))>0$ or $\\operatorname{det}(M(t))<0$ for all $t \\geqslant 0$, depending on whether $d$ is odd or even, respectively. This distinction naturally occurs in the treatment of equal-input matrices [3]. If the determinant is negative, it must stay so, which is consistent\n\nwith the grading of the monoid of equal-input Markov matrices by the sign of $1-x$, and thus by the determinant for even $d$, as detailed in [4, Prop. 2.6].\n\nWhen all $Q(t)=Q_{\\boldsymbol{q}(t)}$ are rate matrices, which is equivalent with $\\boldsymbol{q}(t) \\geqslant \\mathbf{0}$, we also know that $\\boldsymbol{x}(t) \\geqslant \\mathbf{0}$ together with $x(0)=0$ from (7), and then get the stronger inequality $|x(t)|<1$ for all $t \\geqslant 0$. Consequently, the spectral radius of $A(t)=M(t)-\\mathbb{1}$ is always less than 1 , and\n\n$$\nR(t):=\\log (\\mathbb{1}+A(t))=\\sum_{n=1}^{\\infty} \\frac{(-1)^{n-1}}{n} A(t)^{n}\n$$\n\nis a convergent series. Observing that $A(t)^{n}=(-x(t))^{n-1} A(t)$ holds for all $n \\in \\mathbb{N}$, one finds\n\n$$\nR(t)=\\frac{\\log (1-x(t))}{-x(t)} A(t)=\\frac{-\\log (1-x(t))}{x(t)} Q_{\\boldsymbol{x}(t)}\n$$\n\nwith $R(0)=\\mathbb{0}$ and $R(t)=\\mathbb{0}$ whenever $x(t)=0$, via an argument of de l'Hospital type, because the latter case is only possible for $\\boldsymbol{x}(t)=\\mathbf{0}$ under our assumption that $Q$ is a family of rate matrices. Further, $R(t)$ is a Markov generator if $\\boldsymbol{x}(t) \\geqslant \\mathbf{0}$, as $Q_{\\boldsymbol{x}(t)}$ then is a rate matrix and the prefactor is non-negative. Here, $R(t)$ is a real logarithm of the solution function, hence $M(t)=\\mathrm{e}^{R(t)}$, and it is actually the principal logarithm; see [12, 13, 14] for background.\n\nMore generally, the principal matrix logarithm exists for all (real or complex) matrices with positive spectrum, and is defined as the unique logarithm whose eigenvalues all lie in the strip $\\{z \\in \\mathbb{C}:-\\pi<\\operatorname{Im}(z)<\\pi\\}$. One general formula follows from [13, Thm. 11.1] and reads\n\n$$\n\\log (M(t))=\\int_{0}^{1}(M(t)-\\mathbb{1})(\\tau(M(t)-\\mathbb{1})+\\mathbb{1})^{-1} \\mathrm{~d} \\tau\n$$\n\nwhich is applicable for all of the above cases with $x(t)<1$. It is a consequence of analytic continuation that the formula from (9) is the correct one also in this more general case.\n\nSince $M(t)$ is of equal-input form, its spectrum will generally be degenerate (certainly for $d \\geqslant 3$ ). In view of Fact 2.1, $M(t)$ can never be cyclic for $d \\geqslant 3$ (meaning that characteristic and minimal polynomial cannot agree), though for $d=3$ with $x=0$, we can have (nonMarkov) equal-input matrices with Jordan normal form $1 \\oplus\\left(\\begin{array}{ll}1 & 1 \\\\ 0 & 1\\end{array}\\right)$, in which case the real logarithm is still unique. In general, however, there will be other real logarithms, compare [9], but none of equal-input form. Putting the pieces together, the derived result reads as follows; see [4] for previous results on the embedding problem.\n\nTheorem 3.4. The Cauchy problem of Lemma 3.1 defines a forward flow $\\{M(t): t \\geqslant 0\\}$ of equal-input matrices with unit row sums, where each $M(t)$ possesses a real logarithm. In particular, we have $M(t)=\\mathrm{e}^{R(t)}$ with the matrix function $R$ from (9) and (7), where $R(t)$ is the unique real logarithm of $M(t)$ of equal-input form.\n\nFurther, when all $Q(t)$ are Markov generators (of equal-input type), so $Q(t)=Q_{\\boldsymbol{q}(t)}$ with $\\boldsymbol{q}(t) \\geqslant \\mathbf{0}$ for all $t \\geqslant 0$, the forward flow consists of Markov matrices only. Then, every $R(t)$ is a Markov generator of equal-input type as well, and each individual $M(t)$ is also embeddable in a time-homogeneous Markov semigroup that is generated by an equal-input Markov generator.\n\nLet us look at two particular limiting cases that we will need again later for a comparison of our two different example classes at their intersection. First, if we assume $Q(t) \\in \\mathcal{E}_{0}^{\\prime}$ for all $t \\geqslant 0$, we have $Q(t)=Q_{\\boldsymbol{q}(t)}=C_{\\boldsymbol{q}(t)}$ with $q(t) \\equiv 0$. Then, (7) simplifies to $\\boldsymbol{x}(t)=\\int_{0}^{t} \\boldsymbol{q}(\\tau) \\mathrm{d} \\tau$ with $x(t) \\equiv 0$, and (9) becomes\n\n$$\nR(t)=Q_{\\boldsymbol{x}(t)}=C_{\\boldsymbol{x}(t)}\n$$\n\nafter an application of de l'Hospital's rule. This is one special limiting case.\nNext, and more generally, let us write $R(t)$ from (9) in a different way. Each $Q \\in \\mathcal{E}_{0}$ can uniquely be written as a sum of a constant-input matrix (compare (3) and the paragraph before it) and one from the ideal $\\mathcal{E}_{0}^{\\prime}$, namely as $Q=Q_{\\boldsymbol{q}}=q J_{d}+C_{\\boldsymbol{r}}$ with $J_{d}=\\frac{1}{d} C_{1}$ and $\\boldsymbol{r}=\\boldsymbol{q}-\\frac{2}{d} \\mathbf{1}$. Setting $Q_{0}=J_{d}$, we now write\n\n$$\nQ(t)=Q_{\\boldsymbol{q}(t)}=\\mu(t) Q_{0}+C_{\\boldsymbol{r}(t)}\n$$\n\nwith the scalar function $\\mu(t)=q(t)$ and $\\boldsymbol{r}(t)=\\boldsymbol{q}(t)-\\frac{q(t)}{d} \\mathbf{1}$. In particular, one then has $r(t) \\equiv 0$. Setting $u(t)=\\int_{0}^{t} \\mu(\\tau) \\mathrm{d} \\tau$, so that $u(0)=0$, Eq. (7) simplifies to\n\n$$\n\\boldsymbol{x}(t)=\\mathrm{e}^{-u(t)} \\int_{0}^{t} \\mathrm{e}^{u(\\tau)} \\boldsymbol{q}(\\tau) \\mathrm{d} \\tau\n$$\n\nIts scalar counterpart then is $x(t)=1-\\mathrm{e}^{-u(t)}$, where we used that $\\int_{0}^{t} \\mathrm{e}^{u(\\tau)} \\mu(\\tau) \\mathrm{d} \\tau=\\mathrm{e}^{u(t)}-1$. With $f(x)=\\frac{x}{\\mathrm{e}^{x}-1}$, which will appear many times from now on, Eq. (9) turns into\n\n$$\nR(t)=f(u(t)) \\mathrm{e}^{u(t)} Q_{\\boldsymbol{x}(t)}=f(u(t)) \\int_{0}^{t} \\mathrm{e}^{u(\\tau)} Q_{\\boldsymbol{q}(\\tau)} \\mathrm{d} \\tau=f(u(t)) \\int_{0}^{t} \\mathrm{e}^{u(\\tau)} Q(\\tau) \\mathrm{d} \\tau\n$$\n\nThis gives $R(t)$ as a weighted integral over the original generator family from (11), which thus is a direct generalisation of Fact 2.4.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 4. Two generalisations and their exact solutions \n\nThe decomposition of Eq. (11) suggests that a little more might be possible than such a sum with $Q_{0}$ a constant-input matrix. Indeed, we will establish that the algebraic structure of equal-input matrices from Fact 2.3 is strong enough to allow for a significant extension. So, let $Q_{0}$ now be any fixed rate matrix, and consider the matrix family defined by\n\n$$\nQ(t)=\\mu(t) Q_{0}+C_{\\boldsymbol{q}(t)}\n$$\n\nsubject to the assumption that $q(t)=\\operatorname{tr}\\left(C_{\\boldsymbol{q}(t)}\\right)=0$ for all $t \\geqslant 0$, so $C_{\\boldsymbol{q}(t)} \\in \\mathcal{E}_{0}^{\\prime}$. Here, $\\mu$ is a strictly positive scalar function, assumed continuous, which has the interpretation of a time-dependent global rate change for $Q_{0}$, and $C_{\\boldsymbol{q}(t)}$ is a time-dependent modification in the form of a traceless equal-input matrix, also assuming that $\\boldsymbol{q}$ is continuous. ${ }^{3}$ Here, in line with Fact 2.3, we have\n\n$$\nQ_{0} C_{\\boldsymbol{q}(t)}=\\mathbb{O}, \\quad C_{\\boldsymbol{q}(t)} C_{\\boldsymbol{q}\\left(t^{\\prime}\\right)}=q(t) C_{\\boldsymbol{q}\\left(t^{\\prime}\\right)}=\\mathbb{O} \\quad \\text { and } \\quad C_{\\boldsymbol{q}(t)} Q_{0} \\in \\mathcal{E}_{0}^{\\prime}\n$$\n\n[^0]\n[^0]:    ${ }^{3}$ The continuity assumptions can later be generalised to local integrability of $\\mu$ and $\\boldsymbol{q}$, if one replaces the ODE for the flow with the corresponding Volterra integral equation. Also, the strict positivity of $\\mu$ can be slightly relaxed, but we suppress further details on this aspect.\n\nfor all $t, t^{\\prime} \\geqslant 0$ under our assumptions. This implies that we can compute the summands $I_{n}(t)$ of the PBS for the solution of the Cauchy problem $\\dot{M}=M Q$ with $M(0)=\\mathbb{1}$ more explicitly.\n\nLet us begin with the following observation.\nFact 4.1. Let $\\mu$ be a locally integrable function and define $u(t)=\\int_{0}^{t} \\mu(\\tau) \\mathrm{d} \\tau$. Then,\n\n$$\n\\int_{0}^{t} u(\\tau)^{n} \\mu(\\tau) \\mathrm{d} \\tau=\\frac{u(t)^{n+1}}{n+1}\n$$\n\nholds for all $n \\in \\mathbb{N}$, and one also obtains the iterated integral\n\n$$\n\\int_{0}^{t} \\int_{0}^{t_{1}} \\cdots \\int_{0}^{t_{n-1}} \\mu\\left(t_{1}\\right) \\mu\\left(t_{2}\\right) \\cdots \\mu\\left(t_{n}\\right) \\mathrm{d} t_{n} \\cdots \\mathrm{~d} t_{2} \\mathrm{~d} t_{1}=\\frac{u(t)^{n}}{n!}\n$$\n\nProof. For the first claim, since $u(0)=0$, we employ integration by parts to obtain\n\n$$\n\\int_{0}^{t} u(\\tau)^{n} \\mu(\\tau) \\mathrm{d} \\tau=u(t)^{n+1}-n \\int_{0}^{t} u(\\tau)^{n} \\mu(\\tau) \\mathrm{d} \\tau\n$$\n\nand solve for the integral, where the integrability of $u^{n} \\mu$ is clear by standard arguments.\nThe second claim is true for $n=1$, by the very definition of $u(t)$. Then, for $n \\in \\mathbb{N}$, we get\n\n$$\n\\begin{aligned}\n\\int_{0}^{t} \\int_{0}^{t_{1}} & \\cdots \\int_{0}^{t_{n}} \\mu\\left(t_{1}\\right) \\mu\\left(t_{2}\\right) \\cdots \\mu\\left(t_{n+1}\\right) \\mathrm{d} t_{n+1} \\cdots \\mathrm{~d} t_{2} \\mathrm{~d} t_{1} \\\\\n& =\\int_{0}^{t} \\mu\\left(t_{1}\\right) \\frac{u\\left(t_{1}\\right)^{n}}{n!} \\mathrm{d} t_{1}=\\frac{u(t)^{n+1}}{(n+1)!}\n\\end{aligned}\n$$\n\nwhich uses the first claim, and settles the second inductively.\nFor the PBS, we have $I_{1}(t)=\\int_{0}^{t} Q(\\tau) \\mathrm{d} \\tau=u(t) Q_{0}+C_{\\boldsymbol{q}^{(1)}(t)}$ with $\\boldsymbol{q}^{(1)}(t)=\\int_{0}^{t} \\boldsymbol{q}(\\tau) \\mathrm{d} \\tau$. Then, defining $\\boldsymbol{q}^{(n+1)}(t)=\\int_{0}^{t} \\mu(\\tau) \\boldsymbol{q}^{(n)}(\\tau) \\mathrm{d} \\tau$ for $n \\in \\mathbb{N}$, one inductively finds\n\n$$\nI_{n+1}(t)=\\int_{0}^{t} I_{n}(\\tau) Q(\\tau) \\mathrm{d} \\tau=\\frac{u(t)^{n+1}}{(n+1)!} Q_{0}^{n+1}+C_{\\boldsymbol{q}^{(n+1)}(t)} Q_{0}^{n}\n$$\n\nby an application of Fact 4.1 and the observation that all other terms vanish as a result of Eq. (14). Note that the last term in (15), for any $t \\geqslant 0$, is an element of $\\mathcal{E}_{0}^{\\prime}$ due to Fact 2.3. Putting all this together, we get the following result.\n\nProposition 4.2. Consider the Cauchy problem $\\dot{M}=M Q$ with $M(0)=\\mathbb{1}$ for the continuous matrix family defined by Eq. (13). Then, the PBS for its solution has the additive form $M(t)=\\mathbb{1}+A(t)=\\mathbb{1}+A_{0}(t)+A_{\\triangle}(t)$, with $Q_{0}^{0}=\\mathbb{1}$ and\n\n$$\nA_{0}(t)=\\mathrm{e}^{u(t) Q_{0}}-\\mathbb{1} \\quad \\text { and } \\quad A_{\\triangle}(t)=\\sum_{n=1}^{\\infty} C_{\\boldsymbol{q}^{(n)}(t)} Q_{0}^{n-1}\n$$\n\nwhere $A_{0}(t) \\in \\mathcal{A}_{0}$ and $A_{\\triangle}(t) \\in \\mathcal{E}_{0}^{\\prime}$ for all $t \\geqslant 0$. The infinite sum is compactly converging, and the spectral radius of $A(t)$, for all sufficiently small $t$, satisfies $\\varrho_{A(t)}<1$.\n\nProof. One has $M(t)=\\mathbb{1}+\\sum_{n \\geqslant 1} I_{n}(t)$ from the PBS, which is compactly converging by [2, Thm. 1]. Evaluating the sum with the terms from (15) gives the decomposition into contributions from $\\mathcal{A}_{0}$ and $\\mathcal{E}_{0}^{\\prime}$ as stated.\n\nSince $A(0)=\\mathbb{0}$, the claim on the spectral radius is clear by continuity.\nSince $u(t)=\\mathcal{O}(t)$ as $t \\searrow 0$, the result on the spectral radius implies that $M(t)$, at least for all sufficiently small $t$, possesses a real logarithm in the form of the convergent series\n\n$$\nR(t)=\\log (M(t))=\\log (\\mathbb{1}+A(t))=\\sum_{n=1}^{\\infty} \\frac{(-1)^{n-1}}{n} A(t)^{n}\n$$\n\nwhich defines the principal matrix logarithm. Consequently, we know that $M(t)=\\exp (R(t))$, at least for small $t$. Since $A_{0}(t) \\in \\mathcal{A}_{0}$ and $A_{\\triangleleft}(t) \\in \\mathcal{E}_{0}^{\\prime}$, we obtain $A_{0}(t) A_{\\triangleleft}(t)=\\mathbb{0}$ and $A_{\\triangleleft}(t)^{2}=\\mathbb{0}$ from Fact 2.3. This implies\n\n$$\nA(t)^{n}=\\left(A_{0}(t)+A_{\\triangleleft}(t)\\right)^{n}=A_{0}(t)^{n}+A_{\\triangleleft}(t) A_{0}(t)^{n-1}\n$$\n\nfor all $n \\in \\mathbb{N}$ and $t \\geqslant 0$. But this gives $R(t)=R_{0}(t)+R_{\\triangleleft}(t)$ with\n\n$$\n\\begin{aligned}\n& R_{\\triangleleft}(t)=\\sum_{n=1}^{\\infty} \\frac{(-1)^{n-1}}{n} A_{\\triangleleft}(t) A_{0}(t)^{n-1} \\quad \\text { and } \\\\\n& R_{0}(t)=\\sum_{n=1}^{\\infty} \\frac{(-1)^{n-1}}{n} A_{0}(t)^{n}=\\log \\left(\\mathbb{1}+A_{0}(t)\\right)=u(t) Q_{0}\n\\end{aligned}\n$$\n\nfor sufficiently small $t$, where one then always has $R_{\\triangleleft}(t) \\in \\mathcal{E}_{0}^{\\prime}$. Note that the latter property will be preserved when an extension of $t$ beyond the circle of convergence is possible, for instance via analytic continuation.\n\nCorollary 4.3. For all sufficiently small $t \\geqslant 0$, the solution $M(t)$ from Proposition 4.2 has the form $M(t)=\\mathrm{e}^{R(t)}$ with $R(t)=u(t) Q_{0}+R_{\\triangleleft}(t)$ and $R_{\\triangleleft}(t) \\in \\mathcal{E}_{0}^{\\prime}$.\n\nTo determine $R_{\\triangleleft}(t)$, we employ tools from the Magnus expansion; see the Appendix for a short summary and further references. In view of the switched order of the matrices in our ODEs due to the row sum convention for Markov matrices and generators, we use the (twisted) adjoint of two matrices,\n\n$$\n\\widetilde{\\operatorname{ad}}_{A}(B):=[B, A]=-[A, B]\n$$\n\nHere, we get\n\n$$\n\\begin{aligned}\n\\widetilde{\\operatorname{ad}}_{R(t)}(Q(t)) & =\\left[\\mu(t) Q_{0}+C_{\\boldsymbol{q}(t)}, u(t) Q_{0}+R_{\\triangleleft}(t)\\right] \\\\\n& =-\\mu(t) R_{\\triangleleft}(t) Q_{0}+u(t) C_{\\boldsymbol{q}(t)} Q_{0}=(u(t) Q(t)-\\mu(t) R(t)) Q_{0}\n\\end{aligned}\n$$\n\nwhere all additional terms in the second step vanish due to Eq. (14), while the last step uses $R_{0}(t)=u(t) Q_{0}$ from above. Next, we need to calculate powers of the adjoint, where\n\n$\\widetilde{\\operatorname{ad}}_{A}^{n+1}:=\\widetilde{\\operatorname{ad}}_{A} \\circ \\widetilde{\\operatorname{ad}}_{A}^{n}$ for $n \\geqslant 1$. One can now repeat the above type of calculation inductively, with the same kind of cancellations, to obtain\n\n$$\n\\widetilde{\\operatorname{ad}}_{R(t)}^{n}(Q(t))=(u(t) Q(t)-\\mu(t) R(t)) u(t)^{n-1} Q_{0}^{n}\n$$\n\nfor $n \\in \\mathbb{N}$, where it is clear from Fact 2.3 that this expression is always an element of $\\mathcal{E}_{0}^{\\prime}$, as can be verified by writing the right-hand side in terms of $C_{\\boldsymbol{q}(t)}$ and $R_{\\triangle}(t)$.\n\nNow, with (17) and Eq. (25) from the Appendix, we get\n\n$$\n\\dot{R}(t)=\\sum_{n=0}^{\\infty} \\frac{b_{n}}{n!} \\widetilde{\\operatorname{ad}}_{R(t)}^{n}(Q(t))=Q(t)+(u(t) Q(t)-\\mu(t) R(t)) \\frac{f\\left(u(t) Q_{0}\\right)-\\mathbb{1}}{u(t)}\n$$\n\nwith the well-known meromorphic function\n\n$$\nf(x)=\\frac{x}{\\mathrm{e}^{x}-1}=\\sum_{n=1}^{\\infty} \\frac{b_{n}}{n!} x^{n}, \\quad \\text { with } f(0)=0\n$$\n\nwhere the $b_{n}$ denote the Bernoulli numbers, here with $b_{1}=-\\frac{1}{2}$ and $b_{2 m+1}=0$ for all $m \\in \\mathbb{N}$. The power series of $f$ has $2 \\pi$ as its radius of convergence, so $f\\left(u(t) Q_{0}\\right)$ is well defined for small values of $t$, as is $\\left(f\\left(u(t) Q_{0}\\right)-\\mathbb{1}\\right) / u(t)$, where we assume $u(t)=\\int_{0}^{t} \\mu(\\tau) \\mathrm{d} \\tau>0$ for all $t>0$, which really is a consequence of our assumptions on $\\mu$.\n\nObserving that $R(t)=R_{0}(t)+R_{\\triangle}(t)$ with $\\dot{R}_{0}(t)=\\mu(t) Q_{0}$ leads to several cancellations, one arrives at the inhomogeneous linear ODE\n\n$$\n\\dot{R}_{\\triangle}(t)+\\frac{\\mu(t)}{u(t)} R_{\\triangle}(t)\\left(f\\left(u(t) Q_{0}\\right)-\\mathbb{1}\\right)=C_{\\boldsymbol{q}(t)} f\\left(u(t) Q_{0}\\right)\n$$\n\nwhich can again be solved by the standard methods used earlier. The result is\n\n$$\n\\begin{aligned}\nR_{\\triangle}(t)= & \\int_{0}^{t} C_{\\boldsymbol{q}(\\tau)} f\\left(u(\\tau) Q_{0}\\right) \\exp \\left(\\int_{0}^{\\tau} \\frac{\\mu(\\sigma)}{u(\\sigma)}\\left(f\\left(u(\\sigma) Q_{0}\\right)-\\mathbb{1}\\right) \\mathrm{d} \\sigma\\right) \\mathrm{d} \\tau \\\\\n& \\cdot \\exp \\left(-\\int_{0}^{t} \\frac{\\mu(\\rho)}{u(\\rho)}\\left(f\\left(u(\\rho) Q_{0}\\right)-\\mathbb{1}\\right) \\mathrm{d} \\rho\\right) \\\\\n= & \\int_{0}^{t} C_{\\boldsymbol{q}(\\tau)} f\\left(u(\\tau) Q_{0}\\right) \\exp \\left(\\int_{0}^{u(\\tau)} \\frac{f\\left(\\vartheta Q_{0}\\right)-\\mathbb{1}}{\\vartheta} \\mathrm{~d} \\vartheta\\right) \\mathrm{d} \\tau \\cdot \\exp \\left(-\\int_{0}^{u(t)} \\frac{f\\left(\\eta Q_{0}\\right)-\\mathbb{1}}{\\eta} \\mathrm{~d} \\eta\\right)\n\\end{aligned}\n$$\n\nwhere the second expression emerges from a standard substitution, which is justified when $u$ is strictly increasing.\n\nThe expression for $R_{\\triangle}(t)$ can still be simplified further. One idea how to achieve this comes from the formula\n\n$$\n\\int_{0}^{t} \\frac{f(\\tau)-1}{\\tau} \\mathrm{~d} \\tau=-t+\\log \\frac{\\mathrm{e}^{t}-1}{t}=-t-\\log (f(t))\n$$\n\nwhich holds for $t \\geqslant 0$. Carefully inspecting the corresponding identities for matrix-valued integrals, one can come to the following helpful identity.\n\nLemma 4.4. Let $B \\in \\operatorname{Mat}(d, \\mathbb{R})$ be fixed. Then, for sufficiently small $t \\geqslant 0$, one has\n\n$$\n\\int_{0}^{t} \\frac{f(\\tau B)-\\mathbb{1}}{\\tau} \\mathrm{d} \\tau=\\log \\left(f(t B)^{-1} \\mathrm{e}^{-t B}\\right)\n$$\n\nwhere $\\log$ refers to the principal matrix logarithm.\nProof. Clearly, both sides evaluate to $\\mathbb{0}$ for $t=0$, and the spectrum of $t B$ lies inside the circle of convergence for the power series of $f$, as long as $t$ is small enough. Also, again for sufficiently small $t$, the matrix argument of the logarithm cannot have any negative eigenvalues, so the principal logarithm is well defined.\n\nNow, we need to check that both sides have the same derivative. On the left, one has\n\n$$\n\\frac{1}{t}(f(t B)-\\mathbb{1})=\\left.\\frac{f(z)-1}{z}\\right|_{z=t B} \\cdot B=\\left.\\frac{1+z-\\mathrm{e}^{z}}{z\\left(\\mathrm{e}^{z}-1\\right)}\\right|_{z=t B} \\cdot B\n$$\n\nwith the usual understanding of matrix functions via converging power series. In comparison, the right-hand side gives\n\n$$\n\\left.\\frac{\\mathrm{d}}{\\mathrm{~d} z} \\log \\frac{\\mathrm{e}^{-z}}{f(z)}\\right|_{z=t B} \\cdot B=\\left.\\frac{z \\mathrm{e}^{z}}{\\mathrm{e}^{z}-1} \\frac{\\mathrm{~d}}{\\mathrm{~d} z} \\frac{1-\\mathrm{e}^{-z}}{z}\\right|_{z=t B} \\cdot B=\\left.\\frac{1+z-\\mathrm{e}^{z}}{z\\left(\\mathrm{e}^{z}-1\\right)}\\right|_{z=t B} \\cdot B\n$$\n\nwhich agrees with the previous expression and completes the argument.\nUsing $f(t) \\mathrm{e}^{t}=f(-t)$ and inserting the formula from Lemma 4.4 into our solution for $R_{\\triangle}(t)$ then gives the significantly simpler expression\n\n$$\nR_{\\triangle}(t)=\\int_{0}^{t} C_{\\boldsymbol{q}(\\tau)} \\mathrm{e}^{-u(\\tau) Q_{0}} \\mathrm{~d} \\tau \\cdot f\\left(-u(t) Q_{0}\\right)\n$$\n\nLet us sum up as follows.\nTheorem 4.5. Consider the Cauchy problem $\\dot{M}=M Q$ with $M(0)=\\mathbb{1}$ for the matrix family $Q(t)=\\mu(t) Q_{0}+C_{\\boldsymbol{q}(t)}$ from (13), where $\\mu$ is a positive scalar function and $\\boldsymbol{q}$ an $\\mathbb{R}^{d}$-valued function with $q(t)=0$ for all $t \\geqslant 0$, both assumed continuous. This problem has a unique solution in the form of the PBS. For small enough $t$, it also satisfies $M(t)=\\mathrm{e}^{R(t)}$ with $R(t)=u(t) Q_{0}+R_{\\triangle}(t)$ and the $R_{\\triangle}(t) \\in \\mathcal{E}_{0}^{\\prime}$ from (19), where $u(t)=\\int_{0}^{t} \\mu(\\tau) \\mathrm{d} \\tau$.\n\nFurther, when all $Q(t)$ are Markov generators, the set $\\{M(t): t \\geqslant 0\\}$ defines a flow of Markov matrices. For all sufficiently small $t$, the real logarithm $R(t)$ is then a Markov generator, which means that $M(t)$, for any fixed $t$, is a Markov matrix that is also embeddable into a time-homogeneous Markov semigroup.\n\nLet us comment on the condition with small $t$. In our derivation, we have used the approach to matrix functions via convergent Taylor series as in [13, Thm. 4.7], which limits the eigenvalues of the matrix argument in $f\\left( \\pm u(t) Q_{0}\\right)$ to be less than $2 \\pi$ in modulus. However, $f$ is a meromorphic function on $\\mathbb{C}$, with poles of first order on the imaginary axis, namely at $2 n \\pi \\mathrm{i}$ for all $0 \\neq n \\in \\mathbb{Z}$. As long as the spectrum of $u(t) Q_{0}$ avoids these poles (which is the generic case), the solution formula (19) remains valid, as can be seen via analytic continuation.\n\nLet us make two consistency calculations. First, with $Q_{0}=\\mathbb{0}$ in (13), we get $Q(t) \\in \\mathcal{E}_{0}^{\\prime}$ for all $t \\geqslant 0$, and (19) simplifies to\n\n$$\nR_{\\triangle}(t)=\\int_{0}^{t} C_{\\boldsymbol{q}(\\tau)} \\mathrm{d} \\tau=C_{\\boldsymbol{x}(t)}\n$$\n\nwith $\\boldsymbol{x}(t)=\\int_{0}^{t} \\boldsymbol{q}(\\tau) \\mathrm{d} \\tau$. Since $R(t)=R_{\\triangle}(t)$ in this case, we are back to Eq. (10).\nA little less obvious is the case where $Q_{0}$ is a constant-input matrix. Let us thus consider $Q(t)=\\mu(t) Q_{0}+C_{\\boldsymbol{r}(t)}$ with $Q_{0}=J_{d}$ and $r(t) \\equiv 0$. Then, $Q_{0}^{2}=-Q_{0}$ and $C_{\\boldsymbol{r}(t)} Q_{0}=-C_{\\boldsymbol{r}(t)}$, both as a result of Eq. (4). Consequently, for any function $\\phi$ that is analytic around 0 and all $t, s \\geqslant 0$, we get\n\n$$\nC_{\\boldsymbol{r}(t)} \\phi\\left(u(s) Q_{0}\\right)=\\phi(-u(s)) C_{\\boldsymbol{r}(t)}\n$$\n\nwith the standard restriction on the arguments of $\\phi$ in relation to its radius of convergence, which can later be lifted by analytic continuation. With this, the formula for $R_{\\triangle}(t)$ gives\n\n$$\n\\begin{aligned}\nR_{\\triangle}(t) & =\\int_{0}^{t} C_{\\boldsymbol{r}(\\tau)} \\exp \\left(-u(\\tau) Q_{0}\\right) \\mathrm{d} \\tau \\cdot f\\left(-u(t) Q_{0}\\right) \\\\\n& =\\int_{0}^{t} \\mathrm{e}^{u(\\tau)} C_{\\boldsymbol{r}(\\tau)} f\\left(-u(t) Q_{0}\\right) \\mathrm{d} \\tau=f(u(t)) \\int_{0}^{t} \\mathrm{e}^{u(\\tau)} C_{\\boldsymbol{r}(\\tau)} \\mathrm{d} \\tau\n\\end{aligned}\n$$\n\nSince $R_{0}(t)=u(t) Q_{0}$ and\n\n$$\nu(t)=f(u(t))\\left(\\mathrm{e}^{u(t)}-1\\right)=f(u(t)) \\int_{0}^{t} \\mathrm{e}^{u(\\tau)} \\mu(\\tau) \\mathrm{d} \\tau\n$$\n\nwe thus get the simplified formula\n\n$$\nR(t)=f(u(t)) \\int_{0}^{t} \\mathrm{e}^{u(\\tau)}\\left(\\mu(\\tau) Q_{0}+C_{\\boldsymbol{r}(\\tau)}\\right) \\mathrm{d} \\tau=f(u(t)) \\int_{0}^{t} \\mathrm{e}^{u(\\tau)} Q(\\tau) \\mathrm{d} \\tau\n$$\n\nwhich agrees with Eq. (12), as it must.\nIt is possible to generalise our result from Theorem 4.5 even a little further. To do so, we consider the (assumed continuous) matrix family\n\n$$\nQ(t)=Q_{0}(t)+C_{\\boldsymbol{q}(t)}\n$$\n\nwith $q(t)=\\operatorname{tr}\\left(C_{\\boldsymbol{q}(t)}\\right) \\equiv 0$, where the $Q_{0}(t) \\in \\mathcal{A}_{0}$ define a commuting matrix family, that is, they satisfy $\\left[Q_{0}(t), Q_{0}(s)\\right]=\\mathbb{0}$ for all $t, s \\geqslant 0$. We now define the integral $R_{0}(t)=\\int_{0}^{t} Q_{0}(\\tau) \\mathrm{d} \\tau$, which satisfies $R_{0}(0)=\\mathbb{0}$ and $\\left[R_{0}(t), Q_{0}(s)\\right]=\\mathbb{0}$ for all $t, s \\geqslant 0$. The PBS for the Cauchy problem with the matrix family from (20) leads to $I_{1}(t)=R_{0}(t)+Q_{\\triangle}^{(1)}(t)$, where one has $Q_{\\triangle}^{(1)}(t)=\\int_{0}^{t} C_{\\boldsymbol{q}(\\tau)} \\mathrm{d} \\tau \\in \\mathcal{E}_{0}^{\\prime}$, and then inductively to\n\n$$\nI_{n+1}(t)=\\frac{R_{0}^{n+1}(t)}{(n+1)!}+Q_{\\triangle}^{(n+1)}(t) \\quad \\text { with } \\quad Q_{\\triangle}^{(n+1)}(t)=\\int_{0}^{t} Q_{\\triangle}^{(n)}(\\tau) Q_{0}(\\tau) \\mathrm{d} \\tau \\in \\mathcal{E}_{0}^{\\prime}\n$$\n\nfor $n \\in \\mathbb{N}$. The derivation of the leading term uses the integration identity\n\n$$\n\\int_{0}^{t} R_{0}^{n}(\\tau) Q_{0}(\\tau) \\mathrm{d} \\tau=\\frac{R_{0}^{n+1}(t)}{n+1}\n$$\n\nwhich holds for $n \\in \\mathbb{N}_{0}$ and is a matrix-valued analogue of Fact 4.1. As $\\left[R_{0}(t), Q_{0}(t)\\right]=\\mathbb{O}$, the claimed formula can be verified by differentiation, with $\\dot{R}_{0}=Q_{0}$. As before, this leads to the solution of the Cauchy problem in the form $M(t)=\\mathbb{1}+A_{0}(t)+A_{\\triangle}(t)$ with $A_{0}(t)=\\mathrm{e}^{R_{0}(t)}-\\mathbb{1}$ and $A_{\\triangle}(t)=\\sum_{n \\geqslant 1} Q_{\\triangle}^{(n)}(t)$, which is compactly converging. Here, one has $A_{0}(t) \\in \\mathcal{A}_{0}$ and $A_{\\triangle}(t) \\in \\mathcal{E}_{0}^{\\prime}$ for all $t \\geqslant 0$, and we get the same formula for the powers as in Eq. (16).\n\nSetting $M(t)=\\exp (R(t))$, which is at least possible for small $t$, one gets the decomposition $R(t)=R_{0}(t)+R_{\\triangle}(t)$, compare Corollary 4.3, again with $R_{\\triangle}(t)=\\sum_{n \\in \\mathbb{N}} \\frac{(-1)^{n-1}}{n} A_{\\triangle}(t) A_{0}^{n-1}(t)$. Dropping the notation indicating explicit time dependence for a moment, one can now calculate the iterated adjoint using exactly the same algebraic steps as around (19) together with $\\left[R_{0}(t), Q_{0}(t)\\right]=\\mathbb{O}$. This leads to\n\n$$\n\\widetilde{\\operatorname{ad}}_{R}^{n}(Q)=\\left(Q_{\\boldsymbol{q}} R_{0}-R_{\\triangle} Q_{0}\\right) R_{0}^{n-1}=\\left(Q R_{0}-R Q_{0}\\right) R_{0}^{n-1}\n$$\n\nfor $n \\in \\mathbb{N}$, together with $\\widetilde{\\operatorname{ad}}_{R}^{0}(Q)=Q$. This gives the ODE for $R$ via Eq. (25) as\n\n$$\n\\dot{R}=\\sum_{n=1}^{\\infty} \\frac{b_{n}}{n!} \\widetilde{\\operatorname{ad}}_{R}^{n}(Q)=Q_{0}+C_{\\boldsymbol{q}} f\\left(R_{0}\\right)-R_{\\triangle} Q_{0} g\\left(R_{0}\\right)\n$$\n\nwith $g(z)=\\frac{f(z)-1}{z}$. This $g$ is a meromorphic function on $\\mathbb{C}$ with simple poles at the same places as $f$, and with $g(0)=-\\frac{1}{2}$. Since $\\dot{R}_{0}=Q_{0}$, we can split off the part for $R_{\\triangle}$, which gives the inhomogeneous linear ODE\n\n$$\n\\dot{R}_{\\triangle}+R_{\\triangle} Q_{0} g\\left(R_{0}\\right)=C_{\\boldsymbol{q}} f\\left(R_{0}\\right)\n$$\n\nwith the solution\n\n$$\nR_{\\triangle}(t)=\\int_{0}^{t} C_{\\boldsymbol{q}(\\tau)} f\\left(R_{0}(\\tau)\\right) \\exp \\left(\\int_{0}^{\\tau} Q_{0}(\\sigma) g\\left(R_{0}(\\sigma)\\right) \\mathrm{d} \\sigma\\right) \\mathrm{d} \\tau \\cdot \\exp \\left(-\\int_{0}^{t} Q_{0}(\\rho) g\\left(R_{0}(\\rho)\\right) \\mathrm{d} \\rho\\right)\n$$\n\nWith $h(z)=\\int_{0}^{z} g(x) \\mathrm{d} x=-z-\\log (f(z))$, which satisfies $h(0)=0$, and Eq. (18), we then get\n\n$$\n\\int_{0}^{t} Q_{0}(\\tau) g\\left(R_{0}(\\tau)\\right) \\mathrm{d} \\tau=h\\left(R_{0}(t)\\right)\n$$\n\nas can easily be verified by differentiation. This is a variant of Lemma 4.4, which now gives\n\n$$\n\\exp \\left( \\pm h\\left(R_{0}(t)\\right)\\right)=\\exp \\left(\\mp R_{0}(t)\\right) f\\left(R_{0}(t)\\right)^{\\mp 1}\n$$\n\nInserting this into the solution formula for $R_{\\triangle}(t)$, and using $f(x) \\mathrm{e}^{x}=f(-x)$ again, we obtain\n\n$$\nR_{\\triangle}(t)=\\int_{0}^{t} C_{\\boldsymbol{q}(\\tau)} \\mathrm{e}^{-R_{0}(\\tau)} \\mathrm{d} \\tau \\cdot f\\left(-R_{0}(t)\\right)\n$$\n\nIn principle, $R_{\\triangle}(t)$ is of the form $C_{\\boldsymbol{x}(t)}$ for some suitable function $\\boldsymbol{x}$. However, no simple general formula seems possible, unless one makes further assumptions on $Q_{0}$. We have thus derived the following extension of Theorem 4.5.\n\nCorollary 4.6. Consider the Cauchy problem $\\dot{M}=M Q$ with $M(0)=\\mathbb{1}$ for the matrix family $\\{Q(t): t \\geqslant 0\\}$ of Eq. (20), with $Q_{0}(t)$ being commuting matrices from $\\mathcal{A}_{0}$ and $C_{\\boldsymbol{q}(t)}$ having zero trace. Then, at least for small $t$, the solution is of the form $M(t)=\\exp (R(t))$ with $R(t)=R_{0}(t)+R_{\\triangle}(t)$, where $R_{0}(t)=\\int_{0}^{t} Q_{0}(\\tau) \\mathrm{d} \\tau$ and $R_{\\triangle}(t)$ is the matrix from (21).\n\nOne obvious special case emerges via $\\boldsymbol{q}(t) \\equiv \\mathbf{0}$. Then, we have $R_{\\triangle} \\equiv \\mathbb{O}$, and the solution boils down to\n\n$$\nM(t)=\\exp \\left(R_{0}(t)\\right)=\\exp \\left(\\int_{0}^{t} Q_{0}(\\tau) \\mathrm{d} \\tau\\right)\n$$\n\nas it must, because this is the easy case of commuting matrices.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# Appendix A. Peano-Baker series and Magnus expansion \n\nHere, we give a quick summary of two helpful tools for the solution of non-autonomous matrix-valued Cauchy problems of the form\n\n$$\n\\dot{X}(t)=X(t) A(t) \\quad \\text { with } \\quad X\\left(t_{0}\\right)=X_{0}\n$$\n\nwhere we consider the forward flow for $t \\geqslant t_{0}$. We use this version (with $A(t)$ on the right) in view of the applications to Markov flows. In the simple case where $[A(t), A(s)]=\\mathbb{O}$ for all $t, s \\geqslant t_{0}$, which includes the case that $A(t)$ is a constant matrix, one gets the solution as\n\n$$\nX(t)=X_{0} \\exp \\left(\\int_{t_{0}}^{t} A(\\tau) \\mathrm{d} \\tau\\right)\n$$\n\nwith uniqueness under the usual constraints on $A(t)$. This solution is natural from the Volterra integral equation point of view, where (22) is replaced by the integral version\n\n$$\nX(t)=X_{0}+\\int_{t_{0}}^{t} X(\\tau) A(\\tau) \\mathrm{d} \\tau\n$$\n\nWhen $A(t)=A$ for all $t$, the standard Picard iteration leads to the well-known formula\n\n$$\nX(t)=X_{0}\\left(\\mathbb{1}+\\left(t-t_{0}\\right) A+\\frac{\\left(t-t_{0}\\right)^{2}}{2} A^{2}+\\frac{\\left(t-t_{0}\\right)^{3}}{6} A^{3}+\\ldots\\right)=X_{0} \\mathrm{e}^{\\left(t-t_{0}\\right) A}\n$$\n\nwhich is a simple special case of (23).\nThe situation becomes more complicated when the matrices $A(t)$ no longer commute. There are still two helpful approaches, namely the Peano-Baker series (PBS, which emerges from a careful application of the Picard iteration) and the Magnus expansion (ME, which is related to the Baker-Campbell-Hausdorff formula and employs some techniques from Lie theory).\n\nLet us begin with the PBS. Here, one finds a compactly converging series representation of the solution in the form $X(t)=X_{0} \\cdot \\sum_{n=0}^{\\infty} I_{n}(t)$ with\n\n$$\nI_{0}(t)=\\mathbb{1} \\quad \\text { and } \\quad I_{n+1}(t)=\\int_{t_{0}}^{t} I_{n}(\\tau) A(\\tau) \\mathrm{d} \\tau \\quad \\text { for } n \\in \\mathbb{N}_{0}\n$$\n\nsee [2] and references therein for background and proofs, and [5] for a formulation in our present context and a comparison with the time-ordered exponential used in physics, where it is sometimes called the Dyson series. While it is rare that one can calculate the $I_{n}$ explicitly (unless the $A(t)$ commute), the PBS is still useful for structural insight, in particular if the matrices $A(t)$ come from an algebra.\n\nAnother tool is the ME, which approaches linear ODEs via an exponential solution; see [6] and references therein for an extensive exposition. Here, we again look at the matrix-valued\n\nCauchy problem from (22) where the $A(t)$ constitute some (sufficiently nice) matrix family, but need not commute. The flow, at least for small times, is of the form $X(t)=X_{0} \\exp \\left(R\\left(t_{0}, t\\right)\\right)$, where we now simply write $R(t)$ and take $t_{0}=0$ for convenience.\n\nThe Poincar\u00e9-Hausdorff identity from matrix groups now states that\n\n$$\n\\mathrm{e}^{-R(t)} \\frac{\\mathrm{d}}{\\mathrm{~d} t} \\mathrm{e}^{R(t)}=A(t)\n$$\n\nfrom the left-hand side of which one can derive an expression for $\\dot{R}(t)$ in the form\n\n$$\n\\dot{R}(t)=f\\left(\\overrightarrow{\\mathrm{ad}}_{R(t)}\\right)(A(t))\n$$\n\nwhere $f$ is the meromorphic function defined by $f(x)=\\frac{x}{\\mathrm{e}^{x}-1}$ and $\\widehat{\\mathrm{ad}}_{C}(B):=[B, C]$. This unusual (twisted) version of the standard adjoint, $\\operatorname{ad}_{C}(B)=[C, B]$, is taken to match the order of matrix multiplication we have used in the ODE (22). Then, powers of ad are defined recursively, so $\\widehat{\\mathrm{ad}}_{C}^{0}(B)=B$ and $\\widehat{\\mathrm{ad}}_{C}^{n+1}(B)=\\left[\\widehat{\\mathrm{ad}}_{C}^{n}(B), C\\right]$ for $n \\geqslant 0$. The operator in (25) is then defined via the power series of $f$ around 0 , which is\n\n$$\nf(x)=\\sum_{n=0}^{\\infty} \\frac{b_{n}}{n!} x^{n}=1-\\frac{x}{2}+\\frac{x^{2}}{12}+\\mathcal{O}\\left(x^{4}\\right)\n$$\n\nwhere the $b_{n}$ are the Bernoulli numbers.\nWhile the right-hand side of (25) usually cannot be calculated in closed terms, and is then employed approximately via suitable truncations, this paper has examined several special situations where the series can be worked out, and then admits the exact computation of $R(t)$ via an explicit integration step.", "tables": {}, "images": {}}, {"section_id": 6, "text": "# Acknowledgements \n\nThe authors thank an anonymous referee for a number of useful suggestions, which helped us to improve our presentation. This work was supported by the German Research Foundation (DFG, Deutsche Forschungsgemeinschaft) under SFB 1283/2 2021 - 317210226.", "tables": {}, "images": {}}, {"section_id": 7, "text": "## REFERENCES\n\n[1] H. Amann, Ordinary Differential Equations, de Gruyter, Berlin (1990).\n[2] M. Baake and U. Schl\u00e4gel, The Peano-Baker series, Proc. Steklov Inst. Math. 275 (2011) 167-171; arXiv:1011.1775.\n[3] M. Baake and J. Sumner, Notes on Markov embedding, Lin. Alg. Appl. 594 (2020) 262-299; arXiv:1903.08736.\n[4] M. Baake and J. Sumner, On equal-input and monotone Markov matrices, J. Appl. Probab. 54 (2022) 460-492; arXiv:2007.11433.\n[5] M. Baake and J. Sumner, Embedding of Markov matrices for $d \\leqslant 4$, J. Math. Biol. 89 (2024) 23:1-45; arXiv:2311.02596.\n[6] S. Blanes, F. Casas, J.A. Oteo and J. Ros, The Magnus expansion and some of its applications, Phys. Rep. 470 (2009) 151-238; arXiv:0810.5488.\n[7] L. Cooper and J. Sumner, Uniformization stable Markov models and their Jordan algebraic structure, SIAM J. Matrix Anal. Appl. 44 (2023) 1822-1851; arXiv:2105.03558.\n\n[8] C.J. Cox, P.G. Foster, R.P. Hirt, S.R. Harris and T.M. Embley, The archaebacterial origin of eucaryotes, Proc. Nat. Acad. Sci. 105 (2008) 20356-20361.\n[9] W.J. Culver, On the existence and uniqueness of the real logarithm of a matrix, Proc. Amer. Math. Soc. 17 (1966) 1146-1151.\n[10] E.B. Davies, Embeddable Markov matrices, Electronic J. Probab. 15 (2010) paper 47, 1474-1486; arXiv:1001.1693.\n[11] G. Elfving, Zur Theorie der Markoffschen Ketten, Acta Soc. Sci. Fennicae A2 (1937) 1-17.\n[12] F.R. Gantmacher, Matrizentheorie, Springer, Berlin (1986).\n[13] N.J. Higham, Functions of Matrices: Theory and Computation, SIAM, Philadelphia, PA (2008).\n[14] R.A. Horn and C.R. Johnson, Matrix Analysis, 2nd ed., Cambridge University Press, Cambridge (2013).\n[15] S. Johansen, The Bang-Bang problem for stochastic matrices, Z. Wahrscheinlichkeitsth. Verw. Geb. 26 (1973) 191-195.\n[16] J.F.C. Kingman, The imbedding problem for finite Markov chains, Z. Wahrscheinlichkeitsth. Verw. Geb. 1 (1962) 14-24.\n[17] W. Magnus, On the exponential solution of differential equations for a linear operator, Commun. Pure Appl. Math. VII (1954) 649-673.\n[18] J.R. Norris, Markov Chains, reprint, Cambridge University Press, Cambridge (2005).\n[19] M. Steel, Phylogeny\u2014Discrete and Random Processes in Evolution, SIAM, Philadelphia, PA (2016).\n[20] W. Walter, Ordinary Differential Equations, GTM 182, Springer, New York (1998).\nFakult\u00e4t f\u00fcr Mathematik, Universit\u00e4t Bielefeld, Postfach 100131, 33501 Bielefeld, Germany\n\nSchool of Natural Sciences, Discipline of Mathematics, University of Tasmania, Private Bag 37, Hobart, TAS 7001, Australia", "tables": {}, "images": {}}], "id": "2404.11222v2", "authors": ["Michael Baake", "Jeremy Sumner"], "categories": ["math.PR", "math.CA"], "abstract": "Markov matrices of equal-input type constitute a widely used model class. The\ncorresponding equal-input generators span an interesting subalgebra of the real\nmatrices with zero row sums. Here, we summarise some of their amazing\nproperties and discuss the corresponding Markov embedding problem, both\nhomogeneous and inhomogeneous in time. In particular, we derive exact and\nexplicit solutions for time-inhomogeneous Markov flows with non-commuting\ngenerator families of equal-input type and beyond.", "updated": "2025-04-15T07:54:02Z", "published": "2024-04-17T10:11:01Z"}