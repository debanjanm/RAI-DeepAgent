{
  "title": "Understanding the Cluster LP for Correlation Clustering",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nIn the classic Correlation Clustering problem introduced by Bansal, Blum, and Chawla [BBC04], the input is a complete graph where edges are labeled either + or - , and the goal is to find a partition of the vertices that minimizes the sum of the +edges across parts plus the sum of the -edges within parts. In recent years, Chawla, Makarychev, Schramm and Yaroslavtsev [CMSY15] gave a 2.06-approximation by providing a near-optimal rounding of the standard LP, and Cohen-Addad, Lee, Li, and Newman [CLN22, CLLN23] finally bypassed the integrality gap of 2 for this LP giving a 1.73-approximation for the problem.\n\nWhile introducing new ideas for Correlation Clustering, their algorithm is more complicated than typical approximation algorithms in the following two aspects: (1) It is based on two different relaxations with separate rounding algorithms connected by the round-or-cut procedure. (2) Each of the rounding algorithms has to separately handle seemingly inevitable correlated rounding errors, coming from correlated rounding of Sherali-Adams and other strong LP relaxations [GS11, BRS11, RT12].\n\nIn order to create a simple and unified framework for Correlation Clustering similar to those for typical approximate optimization tasks, we propose the cluster $L P$ as a strong linear program that might tightly capture the approximability of Correlation Clustering. It unifies all the previous relaxations for the problem. It is exponential-sized, but we show that it can be $(1+\\varepsilon)$-approximately solved in polynomial time for any $\\varepsilon>0$, providing the framework for designing rounding algorithms without worrying about correlated rounding errors; these errors are handled uniformly in solving the relaxation.\n\nWe demonstrate the power of the cluster LP by presenting a simple rounding algorithm, and providing two analyses, one analytically proving a 1.49-approximation and the other solving a factor-revealing SDP to show a 1.437-approximation. Both proofs introduce principled methods by which to analyze the performance of the algorithm, resulting in a significantly improved approximation guarantee.\n\nFinally, we prove an integrality gap of $4 / 3$ for the cluster LP, showing our 1.437-upper bound cannot be drastically improved. Our gap instance directly inspires an improved NP-hardness of approximation with a ratio $24 / 23 \\approx 1.042$; no explicit hardness ratio was known before.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "## 1 Introduction\n\nClustering is a classic problem in unsupervised machine learning and data mining. Given a set of data elements and pairwise similarity information between the elements, the task is to find a partition of the data\n\n[^0]\n[^0]:    *Supported by NSF grant CCF-2008422.\n    ${ }^{\\dagger}$ Supported in part by NSF grant CCF-2236669 and Google.\n    ${ }^{\\ddagger}$ Affiliated with the Department of Computer Science and Technology in Nanjing University, and supported by the State Key Laboratory for Novel Software Technology, and the New Cornerstone Science Laboratory.\n    \u00a7 Supported by the Swiss National Science Foundation project 200021-184656 \"Randomness in Problem Instances and Randomized Algorithms\".\n\nelements into clusters to achieve (often contradictory) goals of placing similar elements in the same cluster and separating different elements in different clusters. Introduced by Bansal, Blum, and Chawla [BBC04], Correlation Clustering elegantly models such tension and has become one of the most widely studied formulations for graph clustering. The input of the problem consists of a complete graph $\\left(V, E^{+} \\uplus E^{-}\\right)$, where $E^{+} \\uplus E^{-}=\\binom{V}{2}, E^{+}$representing the so-called positive edges and $E^{-}$the so-called negative edges. The goal is to find a clustering (partition) of $V$, namely $\\left(V_{1}, \\ldots, V_{k}\\right)$, that minimizes the number of unsatisfied edges, namely the +edges between different clusters and the -edges within the same cluster. Thanks to the simplicity and modularity of the formulation, Correlation Clustering has found a number of applications, e.g., finding clustering ensembles [BGU13], duplicate detection [ARS09], community mining [CSX12], disambiguation tasks [KCMNT08], automated labelling [AHK^09, CKP08] and many more.\n\nThis problem is APX-Hard [CGW05], and various $O(1)$-approximation algorithms [BBC04, CGW05] have been proposed in the literature. Ailon, Charikar and Newman introduced an influential pivot-based algorithm, which leads to a combinatorial 3 -approximation and a 2.5 -approximation with respect to the standard LP relaxation [ACN08]. The LP-based rounding was improved by Chawla, Makarychev, Schramm and Yaroslavtsev to a 2.06-approximation [CMSY15], nearly matching the LP integrality gap of 2 presented in [CGW05].\n\nIt turns out that (a high enough level of) the Sherali-Adams hierarchy can be used to design a strictly better than 2-approximation. Cohen-Addad, Lee, and Newman [CLN22] showed that $O\\left(1 / \\varepsilon^{2}\\right)$ rounds of the Sherali-Adams hierarchy have an integrality gap of at most $(1.994+\\varepsilon)$. This approximation ratio was improved by Cohen-Addad, Lee, Li, and Newman [CLLN23] to $(1.73+\\varepsilon)$ in $n^{\\operatorname{poly}(1 / \\varepsilon)}$-time, which combines pivot-based rounding and set-based rounding.\n\nOne undesirable feature of [CLLN23] is the lack of a single convex relaxation with respect to which the approximation ratio is analyzed. For technical reasons, it combines the two rounding algorithms via a generic round-or-cut framework. Given $x \\in[0,1]^{E}$, each of the two rounding algorithms outputs either an integral solution with some guarantee or a hyperplane separating $x$ from the convex hull of integral solutions; if both algorithms output integral solutions, one of them is guaranteed to achieve the desired approximation factor. Though each of the rounding procedures is based on some LP relaxations, they are different, so there is no single relaxation that can be compared to the value of the final solution.\n\nIn this work, we propose the cluster $L P$ as a single relaxation that captures all of the existing algorithmic results. Based on this new unified framework, we design a new rounding algorithm as well as principled tools for the analysis that significantly extend the previous ones, ultimately yielding a new approximation ratio of $1.437+\\varepsilon$. The study of the cluster LP sheds light on the hardness side as well, as we prove a $4 / 3 \\approx 1.33$ gap for the cluster LP and a $24 / 23 \\approx 1.042$ NP-hardness of approximation.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 1.1 Our Results \n\nWe first state the cluster LP here. It is similar to configuration LPs used for scheduling and assignment problems [BS06, FGMS06]. In the cluster LP, we have a variable $z_{S}$ for every $S \\subseteq V, S \\neq \\emptyset$, that indicates if $S$ is a cluster in the output clustering or not. As usual, $x_{u v}$ for every $u v \\in\\binom{V}{2}$ indicates if $u$ and $v$ are separated in the clustering or not. For any $x \\in[0,1]^{\\binom{V}{2}}$, we define $\\operatorname{obj}(x):=\\sum_{u v \\in E^{+}} x_{u v}+\\sum_{u v \\in E^{-}}\\left(1-x_{u v}\\right)$ to be the fractional number of edges in disagreement in the solution $x$.\n\n$$\n\\begin{aligned}\n& \\min \\quad \\operatorname{obj}(x) \\quad \\text { s.t. } \\\\\n& \\sum_{S \\ni u} z_{S}=1 \\\\\n& \\forall u \\in V \\\\\n& \\sum_{S \\geq\\{u, v\\}} z_{S}=1-x_{u v} \\\\\n& \\forall u v \\in\\binom{ V}{2} \\\\\n& z_{S} \\geq 0 \\\\\n& \\forall S \\subseteq V, S \\neq \\emptyset\n\\end{aligned}\n$$\n\nThe objective of the LP is to minimize $\\operatorname{obj}(x)$, which is a linear function. (1) requires that every vertex $u$ appears in exactly one cluster, (2) gives the definition of $x_{u v}$ using $z$ variables.\n\nThe idea behind this LP was used in [CLLN23] to design their set-based rounding algorithm, though the LP was not formulated explicitly in that paper. Moreover, the paper did not provide an efficient algorithm to solve it approximately. Our first result shows that we can approximately solve the cluster LP in polynomial time, despite it having an exponential number of variables. We remark that unlike the configuration LPs for many problems, we do not know how to solve the cluster LP simply by considering its dual.\n\nTheorem 1. Let $\\varepsilon>0$ be a small enough constant and opt be the cost of the optimum solution to the given Correlation Clustering instance. In time $n^{\\operatorname{poly}(1 / \\varepsilon)}$, we can output a solution $\\left(\\left(z_{S}\\right)_{S \\subseteq V},\\left(x_{u v}\\right)_{u v \\in\\binom{V}{2}}\\right.$ to the cluster $L P$ with $\\operatorname{obj}(x) \\leq(1+\\varepsilon)$ opt, described using a list of non-zero coordinates. ${ }^{1}$\n\nThe cluster LP is the most powerful LP that has been considered for the problem. Indeed, previous algorithms in [CLN22] and [CLLN23] can be significantly simplified if one is given a $(1+\\varepsilon)$-approximate solution to the LP. A large portion of the algorithms and analysis in [CLN22] and [CLLN23] is devoted to handle the additive errors incurred by the correlated rounding procedure, which is inherited from the Raghavendra-Tan rounding technique [RT12]. Instead, we move the complication of handling rounding errors into the procedure of solving the cluster LP relaxation.\n\nWith this single powerful relaxation, we believe that Theorem 1 provides a useful framework for future work that may use more ingenious rounding of the exponential-sized cluster LP without worrying about errors. Indeed, the constraints in the cluster LP imply that the matrix $\\left(1-x_{u v}\\right)_{u, v \\in V}$ is $\\operatorname{PSD},{ }^{2}$ and thus the LP is at least as strong as the natural SDP for the problem. For the complementary version of maximizing the number of correct edges, the standard SDP is known to give a better approximation guarantee of 0.766 [Swa04, CGW05]. For the minimization version, the standard SDP has integrality gap at least 1.5 (see Appendix C), but it is still open whether this program has an integrality gap strictly below 2 or not.\n\nWe demonstrate the power of the cluster LP by presenting and analyzing the following algorithm, significantly improving the previous best 1.73 -approximation.\n\nTheorem 2. There exists a $(1.49+\\varepsilon)$-approximation algorithm for Correlation Clustering that runs in time $O\\left(n^{\\operatorname{poly}(1 / \\varepsilon)}\\right)$.\n\nThis is achieved by a key modification of the pivot-based rounding algorithm that is used in conjunction with the set-based algorithm as in [CLLN23]. In combination with more careful analysis, which involves principled methods to obtain the best budget function, we obtain a significantly improved approximation ratio.\n\nIn order to obtain an even tighter analysis of the same algorithm, we introduce the new factor revealing $S D P$ that searches over possible global distributions of triangles in valid Correlation Clustering instances. By numerically solving such an SDP, we can further improve the approximation ratio of the same algorithm.\n\nTheorem 3. There exists a $(1.437+\\varepsilon)$-approximation algorithm for Correlation Clustering that runs in time $O\\left(n^{\\operatorname{poly}(1 / \\varepsilon)}\\right)$.\n\nWhile the proof includes a feasible solution to a large SDP and is not human-readable, we prove that our SDP gives an upper bound on the approximation ratio, so it is a complete proof modulo the SDP feasibility of the solution. Our program and solution can be found at https://github.com/correlationClusteringSDP/SDP1437code/.\n\nWe also study lower bounds and prove the following lower bound on the integrality gap of the cluster LP.\nTheorem 4. For any $\\varepsilon>0$, the integrality gap of the cluster $L P$ is at least $4 / 3-\\varepsilon$.\nThis integrality gap for the cluster LP, after some (well-known) loss, directly translates to NP-hardness. Apart from the APX-hardness [CGW05], it is the first hardness with an explicit hardness ratio.\n\n[^0]\n[^0]:    ${ }^{1}$ We remark that $\\operatorname{obj}(x)$ given by the theorem is at most $1+\\varepsilon$ times opt, instead of the value of the cluster LP. This is sufficient for our purpose. One should also be able to achieve the stronger guarantee of $(1+\\varepsilon)$-approximation to the optimum fractional solution. Instead of dealing with the optimum clustering $\\mathcal{C}^{*}$ in the analysis, we deal with the optimum fractional clustering to the LP. For simplicity, we choose to prove the theorem with the weaker guarantee.\n    ${ }^{2}$ Consider the matrix $Y \\in[0,1]^{V \\times V}$ where $y_{u v}=1-x_{u v}$ for every $u, v \\in V\\left(Y_{u u}=1, \\forall u \\in V\\right)$. For every $w \\in \\mathbb{R}^{V}$, we have $w^{T} Y w=\\sum_{u, v \\in V} y_{u v} w_{u} w_{v}=\\sum_{u, v} \\sum_{S \\supseteq\\{u, v\\}} z_{S} w_{u} w_{v}=\\sum_{u, v} \\sum_{S \\subseteq V} z_{S} \\cdot\\left(w_{u} \\cdot 1_{u \\in S}\\right) \\cdot\\left(w_{v} \\cdot 1_{v \\in S}\\right)=$ $\\sum_{S \\subseteq V} z_{S}\\left(\\sum_{u \\in S} w_{u}\\right)\\left(\\sum_{v \\in S} w_{v}\\right) \\geq 0$.\n\nTheorem 5. Unless $\\mathbf{P}=\\mathbf{B P P}$, for any $\\varepsilon>0$, there is no $(24 / 23-\\varepsilon)$-approximation algorithm for Correlation Clustering.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 1.2 Further Related Work \n\nThe weighted version of Correlation Clustering, where each pair of vertices has an associated weight and unsatisfied edges contribute a cost proportional to their weight to the objective, is shown to be equivalent to the Multicut problem [DEF106], implying that there is an $O(\\log n)$-approximation but no constant factor approximation is possible under the Unique Games Conjecture $\\left[\\mathrm{CKK}^{+}\\right.$06].\n\nIn the unweighted case, a PTAS exists when the number of clusters is a fixed constant [GG06, KS09]. Much study has been devoted to the minimization version of Correlation Clustering in various computational models, for example in the online setting [MSS10, $\\mathrm{LMV}^{+}$21, CLMP22], as well as in other practical settings such as distributed, parallel or streaming [CDK14, $\\mathrm{ACG}^{+} 15, \\mathrm{CLM}^{+} 21, \\mathrm{PPO}^{+} 15$, CCMU21, Vel22, VGW18, AW22, BCMT22, BCMT23, CHS24, CM23, CKL ${ }^{+}$24]. Other recent work involves settings with fair or local guarantees [AN23, DMN23, HIA23].",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "## 2 Algorithmic Framework and Setup for Analysis\n\nIn this section, we describe our algorithm for obtaining the improved approximation ratio for Correlation Clustering. We solve the cluster LP using Theorem 1 to get a fractional solution $z=\\left(z_{S}\\right)_{S \\subseteq V}$, which determines $x \\in[0,1]^{\\binom{1}{2}}$ as in (2): $x_{u v}:=1-\\sum_{S \\supseteq\\{u, v\\}} z_{S}$ for every $u v \\in\\binom{V}{2}$. We have $\\operatorname{obj}(x) \\leq(1+\\varepsilon)$ opt. The theorem will be proved in Section 4. With $z$, we then run two procedures: the cluster-based rounding and the pivot-based rounding with threshold $1 / 3$. We select the better result as the final clustering. The two procedures are defined in Algorithms 1 and 2 respectively. We use $N^{+}(u)$ and $N^{-}(u)$ to denote the sets of + and -neighbors of a vertex $u \\in V$ respectively.\n\n```\nAlgorithm 1 Cluster-Based Rounding\n    \\(\\mathcal{C} \\leftarrow \\emptyset, V^{\\prime} \\leftarrow V\\)\n    while \\(V^{\\prime} \\neq \\emptyset\\) do\n        randomly choose a cluster \\(S \\subseteq V\\), with probabilities \\(\\frac{z_{S}}{\\sum_{S^{\\prime}} z_{S^{\\prime}}}\\)\n        if \\(V^{\\prime} \\cap S \\neq \\emptyset\\) then \\(\\mathcal{C} \\leftarrow \\mathcal{C} \\cup\\left\\{V^{\\prime} \\cap S\\right\\}, V^{\\prime} \\leftarrow V^{\\prime} \\backslash S\\)\n    return \\(\\mathcal{C}\\)\n```\n\n```\nAlgorithm 2 Pivot-Based Rounding with Threshold \\(1 / 3\\)\n    \\(\\mathcal{C} \\leftarrow \\emptyset, V^{\\prime} \\leftarrow V\\)\n    while \\(V^{\\prime} \\neq \\emptyset\\) do\n        randomly choose a pivot \\(u \\in V^{\\prime}\\)\n        \\(C \\leftarrow\\left\\{v \\in V^{\\prime} \\cap N^{+}(u): x_{u v} \\leq \\frac{1}{3}\\right\\}\\)\n        for every \\(v \\in V^{\\prime} \\cap N^{-}(u)\\) do independently add \\(v\\) to \\(C\\) with probability \\(1-x_{u v}\\)\n        randomly choose a set \\(S \\ni u\\), with probabilities \\(z_{S} \\quad \\triangleright\\) We have \\(\\sum_{S \\ni u} z_{S}=1\\)\n        \\(C \\leftarrow C \\cup\\left(S \\cap V^{\\prime} \\cap N^{+}(u)\\right), \\mathcal{C} \\leftarrow \\mathcal{C} \\cup\\{C\\}, V^{\\prime} \\leftarrow V^{\\prime} \\backslash C\\)\n    return \\(\\mathcal{C}\\)\n```\n\nAnalysis of Cluster-Based Rounding Procedure. The cluster-based rounding procedure is easy to analyze. The following lemma suffices.\n\nLemma 6. For every $u v \\in\\binom{V}{2}$, the probability that $u$ and $v$ are separated in the clustering $\\mathcal{C}$ output by the cluster-based rounding procedure is $\\frac{2 x_{u v}}{1+x_{u v}}$. So the probability they are in the same cluster is $\\frac{1-x_{u v}}{1+x_{u v}}$.\n\nProof. We consider the first set $S$ chosen in the cluster-based rounding algorithm such that $\\{u, v\\} \\cap S \\neq \\emptyset$. $u$ and $v$ will be separated iff $|S \\cap\\{u, v\\}|=1$. The probability that this happens is precisely $\\frac{\\sum_{S \\cap\\{u, v\\}=1} \\delta S}{\\sum_{S \\cap\\{u, v\\} \\neq \\emptyset} \\delta^{S}}=$ $\\frac{2 x_{u v}}{1+x_{u v}}$.\n\nTherefore, a +edge $u v$ will incur a cost of $\\frac{2 x_{u v}}{1+x_{u v}}$ in expectation in the cluster-based rounding procedure, and a -edge will incur a cost of $\\frac{1-x_{u v}}{1+x_{u v}}$. The approximation ratios for a + edge $u v$ and a - edge $u v$ are respectively $\\frac{2}{1+x_{u v}}$ and $\\frac{1}{1+x_{u v}}$. Notice that the latter quantity is at most 1 .\n\nNotations and Analysis for Pivot-Based Rounding Procedure. We now proceed to the pivot-based rounding procedure in Algorithm 2. We remark that to recover the correlated rounding algorithm in [CLN22] and [CLLN23], we can use $C \\leftarrow \\emptyset$ in Step 4. Then we can obtain their approximation ratios without the complication of handling rounding errors. The errors are handled in [CLN22] by distinguishing between the short, median and long + edges. In our algorithm, we also distinguish between short + edges (those with $x_{u v} \\leq \\frac{1}{3}$ ) and long + edges (those with $x_{u v}>\\frac{1}{3}$ ); however, the purpose of this distinction is to get an improved approximation ratio, instead of to bound the rounding errors.\n\nOur high-level setup of the analysis follows from [CLN22, CLLN23], which in turn is based on [ACN08] and [CMSY15]. We consider a general budget for every edge. We shall define two budget functions:\n\n- $b^{+}:[0,1] \\rightarrow \\mathbb{R}_{\\geq 0}$ and $b^{-}:[0,1] \\rightarrow \\mathbb{R}_{\\geq 0}$.\n\nThey determine the budget $b_{u v}$ for the edge $u v$ : if $u v \\in E^{+}$, then $b_{u v}:=b^{+}\\left(x_{u v}\\right)$, and if $u v \\in E^{-}$, then $b_{u v}:=b^{-}\\left(x_{u v}\\right)$.\n\nWe now focus on one iteration of the while loop in Algorithm 2. Suppose $u, v, w \\in V^{\\prime}$ at the beginning of the iteration, and let $C$ be the cluster constructed at the end. We use $u$ to denote the event that $u$ is chosen as the pivot. We say $v w$ incurs a cost in the iteration, if $v w \\in E^{+}$and $|C \\cap\\{v, w\\}|=1$, or $v w \\in E^{-}$ and $\\{v, w\\} \\subseteq C$. Then, we define\n\n$$\n\\operatorname{cost}_{u}(v, w):=\\operatorname{Pr}[v w \\text { incurs a cost } \\mid u]\n$$\n\nand\n\n$$\n\\Delta_{u}(v, w):=\\operatorname{Pr}[C \\cap\\{v, w\\} \\neq \\emptyset \\mid u] \\cdot b_{v w}\n$$\n\n$\\operatorname{cost}_{u}(v, w)$ is the probability that $v w$ incurs a cost conditioned on the event $u$. When an edge $v w$ disappears, we say $v w$ releases its budget. So, $\\Delta_{u}(v, w)$ is the expected budget released by $v w$ in the iteration when $u$ is the pivot. Notice that both $\\operatorname{cost}_{u}(v, w)$ and $\\Delta_{u}(v, w)$ do not depend on $V^{\\prime}$, provided that $u, v, w \\in V^{\\prime}$.\n\nWe call a set of three distinct vertices a triangle. A set of two distinct vertices is called a degenerate triangle. For triangle $(u, v, w)$, let\n$\\operatorname{cost}(u, v, w):=\\operatorname{cost}_{u}(v, w)+\\operatorname{cost}_{v}(u, w)+\\operatorname{cost}_{w}(u, v), \\quad$ and $\\quad \\Delta(u, v, w):=\\Delta_{u}(v, w)+\\Delta_{v}(u, w)+\\Delta_{w}(u, v)$.\nFor degenerate triangle $(u, v)$, let\n\n$$\n\\operatorname{cost}(u, v):=\\operatorname{cost}_{u}(u, v)+\\operatorname{cost}_{v}(u, v), \\quad \\text { and } \\quad \\Delta(u, v):=\\Delta_{u}(u, v)+\\Delta_{v}(u, v)\n$$\n\nLemma 7. Suppose that for every $V^{\\prime} \\subseteq V$, we have\n\n$$\n\\sum_{(u, v, w) \\in\\binom{V^{\\prime}}{3}} \\operatorname{cost}(u, v, w)+\\sum_{(u, v) \\in\\binom{V^{\\prime}}{3}} \\operatorname{cost}(u, v) \\leq \\sum_{(u, v, w) \\in\\binom{V^{\\prime}}{3}} \\Delta(u, v, w)+\\sum_{(u, v) \\in\\binom{V^{\\prime}}{2}} \\Delta(u, v)\n$$\n\nThen, the expected cost of the clustering output by Algorithm 2 is at most $\\sum_{u v \\in\\binom{V}{3}} b_{u v}$.\n\nProof. Focus on any iteration of Algorithm 2; $V^{\\prime}$ is the $V^{\\prime}$ at the beginning of the iteration.\nThe expected cost incurred by all edges in the iteration is\n\n$$\n\\frac{1}{\\left|V^{\\prime}\\right|} \\sum_{u \\in V^{\\prime}} \\sum_{(v, w) \\in\\binom{V^{\\prime}}{2}} \\operatorname{cost}_{u}(v, w) \\frac{1}{\\left|V^{\\prime}\\right|} \\sum_{(u, v, w) \\in\\binom{V^{\\prime}}{3}} \\operatorname{cost}(u, v, w)+\\frac{1}{\\left|V^{\\prime}\\right|} \\sum_{(u, v) \\in\\binom{V^{\\prime}}{2}} \\operatorname{cost}(u, v)\n$$\n\nThe expected budget released at this iteration is\n\n$$\n\\frac{1}{\\left|V^{\\prime}\\right|} \\sum_{u \\in V^{\\prime}} \\sum_{(v, w) \\in\\binom{V^{\\prime}}{2}} \\Delta_{u}(v, w)=\\frac{1}{\\left|V^{\\prime}\\right|} \\sum_{(u, v, w) \\in\\binom{V^{\\prime}}{3}} \\Delta(u, v, w)+\\frac{1}{\\left|V^{\\prime}\\right|} \\sum_{(u, v) \\in\\binom{V^{\\prime}}{2}} \\Delta(u, v)\n$$\n\nTherefore, if the condition of the lemma holds, then at every iteration of Algorithm 2, the expected cost incurred is at most the expected budget released. Overall, the expected cost of the final clustering is at most the expected total budget released by all edges during the whole procedure, which is $\\sum_{u v \\in\\binom{V}{2}} b_{u v}$. This finishes the proof of the lemma.\n\nTo obtain an approximation ratio of $\\alpha \\in[1,2)$, we consider a variant of our algorithm, in which we run the cluster-based rounding procedure (Algorithm 1) with probability $\\frac{\\alpha}{2}$, and the pivot-based rounding procedure with threshold $1 / 3$ (Algorithm 2) with the remaining probability $1-\\frac{\\alpha}{2}$. Clearly, the actual algorithm that picks the better of the two clusterings generated can only be better. We set up the budget functions $b^{+}$and $b^{-}$such that every edge pays a cost of at most $\\alpha$ times its LP cost in expectation. That is, the following properties are satisfied for every $x \\in[0,1]$ :\n\n$$\n\\frac{\\alpha}{2} \\cdot \\frac{2 x}{1+x}+\\left(1-\\frac{\\alpha}{2}\\right) b^{+}(x)=\\alpha x, \\quad \\frac{\\alpha}{2} \\cdot \\frac{1-x}{1+x}+\\left(1-\\frac{\\alpha}{2}\\right) b^{-}(x)=\\alpha(1-x)\n$$\n\nThis gives us the following definitions:\n\n$$\nb_{\\alpha}^{+}(x):=\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{x^{2}}{1+x}, \\quad \\text { and } \\quad b_{\\alpha}^{-}(x):=\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{(1+2 x)(1-x)}{2(1+x)}, \\quad \\forall x \\in[0,1]\n$$\n\nLemma 8. If the budget functions $b_{\\alpha}^{+}$and $b_{\\alpha}^{-}$satisfy (4) for some $\\alpha \\in[1,2)$, then our algorithm has an approximation ratio of $\\alpha$.\n\nProof. Consider the variant of the algorithm where we run the cluster-based rounding procedure with probability $\\frac{\\alpha}{2}$, and the pivot-based procedure with threshold $1 / 3$ with the remaining probability of $1-\\frac{\\alpha}{2}$. By Lemma 7 , the expected cost of the clustering given by the variant is at most\n\n$$\n\\begin{aligned}\n& \\sum_{u v \\in E^{+}}\\left(\\frac{\\alpha}{2} \\cdot \\frac{2 x_{u v}}{1+x_{u v}}+\\left(1-\\frac{\\alpha}{2}\\right) \\cdot b_{\\alpha}^{+}\\left(x_{u v}\\right)\\right)+\\sum_{u v \\in E^{-}}\\left(\\frac{\\alpha}{2} \\cdot \\frac{1-x_{u v}}{1+x_{u v}}+\\left(1-\\frac{\\alpha}{2}\\right) \\cdot b_{\\alpha}^{-}\\left(x_{u v}\\right)\\right) \\\\\n= & \\alpha\\left(\\sum_{u v \\in E^{+}} x_{u v}+\\sum_{u v \\in E^{-}}\\left(1-x_{u v}\\right)\\right)=\\alpha \\cdot \\operatorname{obj}(x)\n\\end{aligned}\n$$\n\nThe actual algorithm we run can only be better than this variant.\nAs a baseline, we provide a per-triangle analysis leading to an approximation ratio of 1.5 in Section 5:\nLemma 9. For budget functions $b^{+} \\equiv b_{1.5}^{+}$and $b^{-} \\equiv b_{1.5}^{-}$, we have $\\operatorname{cost}(T) \\leq \\Delta(T)$ for every triangle $T$.\nClearly, the lemma implies that (4) holds for $b^{+} \\equiv b_{1.5}^{+}$and $b^{-} \\equiv b_{1.5}^{-}$. By Lemma 8 , our algorithm gives an approximation ratio of 1.5 . We remark that 1.5 is the best possible ratio we can achieve using the per-triangle analysis. For a ++ triangle with length $\\frac{1}{2}$ for +edges and length 1 for the -edge, we need to pay a factor of 2 for each of the $\\frac{1}{2}$-length +edge. Then the cluster-based rounding algorithm gives factors of\n\n2 and $\\frac{4}{3}$ for + edges of lengths 0 and $\\frac{1}{2}$ respectively. For the pivot-based rounding algorithm, the factors are at least 0 and 2 . A combination of the two algorithms can only lead to a factor of 1.5 .\n\nTo get a better approximation ratio, we provide two analyses that use global distributions of triangles. The former is purely analytic and the latter relies on solving a factor-revealing SDP. The following two lemmas are proved in Sections 6 and 7 respectively.\n\nLemma 10. (4) holds for budget functions $b^{+} \\equiv b_{1.49}^{+}$and $b^{-} \\equiv b_{1.49}^{-}$.\nLemma 11. (4) holds for budget functions $b^{+} \\equiv b_{1.437}^{+}$and $b^{-} \\equiv b_{1.437}^{-}$.\nCombined with Lemma 8, the two lemmas imply Theorems 2 and 3 respectively.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 3 Overview of Techniques \n\nIn this section, we provide overviews of the techniques used in our results.\nSimpler and Better Preclustering Procedure. The concept of preclustering was introduced in [CLLN23]. In a preclustered instance, we predetermine the fate of some edges: for some edges $u v, u$ and $v$ must be in the same cluster; for some other edges $u v, u$ and $v$ must be separated. Since the relation of being in the same cluster is transitive, we define a preclustered instance using a pair $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$, where $\\mathcal{K}$ is a partition of $V$ into so called atoms and $E_{\\text {adm }} \\subseteq\\binom{V}{2}$ is a set of admissible edges. An atom can not be broken. If $u$ and $v$ are not in the same atom and $u v \\notin E_{\\text {adm }}$, then $u$ and $v$ must be separated. [CLLN23] showed how to construct a preclustered instance $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$, losing only a $(1+\\varepsilon)$ factor in the optimum cost, while at the same time guaranteeing that $\\left|E_{\\text {adm }}\\right| \\leq O\\left(\\right.$ opt $\\left./ \\varepsilon^{12}\\right)$. This is crucial for their correlated rounding algorithm, as it loses an additive error depending on $\\left|E_{\\text {adm }}\\right|$. In this work, we still need the preclustering procedure to bound the rounding error, but now it is inside the procedure of solving the cluster LP.\n\nWe greatly simplify the preclustering procedure from [CLLN23], and as a result, we achieve a much better bound of $O\\left(\\right.$ opt $\\left./ \\varepsilon^{2}\\right)$ on $\\left|E_{\\text {adm }}\\right|$. [CLLN23] used the agreement graph to construct the atoms; roughly speaking, two vertices are in agreement if their neighborhood sets are similar to each other. The analysis uses many technical structural lemmas from $\\left[\\mathrm{CLM}^{+}\\right.$21], which solves Correlation Clustering in the online setting. In contrast, our construction of atoms is simple: we construct an $O(1)$-approximate clustering $\\mathcal{C}$, mark vertices whose costs are large, and then $\\mathcal{K}$ is obtained from $\\mathcal{C}$ by removing marked vertices and creating singletons for them. The set of admissible edges is roughly defined as follows: we construct a graph $\\left(V, E^{1}\\right)$ where two vertices are neighbors if their + degrees are similar. Then an edge $u v$ is admissible if $u$ and $v$ have many common neighbors in $E^{+} \\cap E^{1}$.\n\nSolving Cluster LP by Preclustering. As we mentioned, we move the complication of handling rounding errors to the step of solving the cluster LP. As in [CLLN23], we construct a preclustered instance $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$, and formulate an LP relaxation aimed at finding the $(1+\\varepsilon)$-approximate good clustering for $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$, that we call the bounded sub-cluster $L P$. In contrast to [CLLN23], which solves many instances of this LP embedded in their round-or-cut framework, we only solve the LP once, therefore avoiding this heavy framework. With a solution $(x, y)$ to the LP, we run a procedure that constructs a single cluster $C$ randomly. The probability that any vertex is in $C$ is precisely $1 / y_{\\emptyset}$, where $y_{\\emptyset}$ is the fractional number of clusters in $y$. The probabilities that exactly one of $u$ and $v$ is in $C$, and both of them are in $C$, are respectively $\\frac{x_{u v}}{y_{\\emptyset}}$ and $\\frac{1-x_{u v}}{y_{\\emptyset}}$ up to some error terms arising from the Raghavendra-Tan rounding procedure. As usual, $x_{u v}$ is the extent in which $u$ and $v$ are separated.\n\nTo construct the solution $z=\\left(z_{S}\\right)_{S \\subseteq V}$ for the cluster LP, we generate $y_{\\emptyset} \\Delta$ many clusters $C$ independently, for a large enough polynomial $\\Delta$. Roughly speaking, the solution $z$ is $\\frac{1}{\\Delta}$ times the multi-set of clusters $C$ we generated. The error incurred by the Raghavendra-Tan rounding procedure can be bounded in terms of $\\left|E_{\\text {adm }}\\right|$, and the error from sampling can be bounded using concentration bounds.\n\n1.49-approximation. We start with the algorithm of [CLLN23], but make several key modifications both in the design and in the analysis. This allows us to significantly improve the approximation ratio, first to 1.5 and, eventually, to 1.49 , which shows that, perhaps surprisingly, even the rather low approximation factor of 1.5 is not tight for Correlation Clustering. The first key ingredient is to use a principled budget function for the pivot-based rounding procedure, defined earlier in (5), which is designed to optimally balance the approximation factor of edges between the two rounding procedures. This new budget function is better than the one used in [CLLN23], but does not allow us to reach 1.5 without changing the algorithm. Indeed, the budget for the short +edges in +++ triangles is still too low to reach the approximation ratio 1.5. Thus, the second key ingredient is to add the threshold step to the pivot-based rounding procedure for the short +edges (i.e., +edges $u v$ with $x_{u v} \\leq 1 / 3$ ). By adding this threshold step, the cost of the triangles containing such edges decreases; for example, a +++ triangle with all short edges now has cost zero. This allows us to use the new budget function and still reach 1.5. Notice that making the threshold too large would result in too much cost for $++$ - triangles.\n\nFinally, we observe that, analogous to the correlated rounding approach of [CLN22], only the bad triangles are tight, meaning their cost equals their budget. Roughly speaking, a bad triangle is a $++$ - triangle whose two +edges have value very close to half and whose -edge has value close to one. This allows us to apply a charging argument, in which tight triangles have part of their cost paid for by triangles that are not tight (i.e., that have extra budget). Now there are no tight triangles (i.e., all triangles have some unused budget), and we can decrease the $\\alpha$ in the budget function from 1.5 to $70 / 47$. As previously [ACN08, CMSY15, CLN22, CLLN23], the analysis necessary to reach 1.5 and go below requires a case-by-case analysis of triangle types to ensure that the budget allocated to each triangle covers its cost. Both the new threshold step and the new budget functions result in an analysis that is more involved than what was required in [CLLN23], but is still feasible.\n1.437-approximation. The above charging argument between different types of triangles can be more systematically expressed by a factor-revealing SDP. Given a cluster LP solution $z_{S}$ and vertices $u, v, w$, let us define $y_{u v}:=\\sum_{S \\supseteq\\{u, v\\}} z_{S}\\left(\\right.$ resp. $\\left.y_{u v w}:=\\sum_{S \\supseteq\\{u, v, w\\}} z_{S}\\right)$ be the probability that $u, v$ (resp. $u, v, w$ ) are in the same cluster. Given any quadruple $T=(a, b, c, d) \\in[0,1]^{4}$, let $\\eta_{T}$ represent the number of triangles $(u, v, w)$ such that of $y_{u v}=a, y_{u w}=b, y_{v w}=c, y_{u v w}=d$. The above 1.49-approximation analysis can be regarded as putting one constraint on the distribution of $\\eta_{T}$. To enhance the approximation ratio and reduce the budget function, we opt for a more detailed categorization of triangles, imposing stronger constraints on $\\eta_{T}$.\n\nConsider an imaginary rounding procedure, where given a pivot $u$, the cluster $C$ that contains $u$ is simply chosen with probability $z_{C}$ (note that $\\sum_{C \\ni u} z_{C}=1$ ). Let $X_{v}$ denote the event that node $v$ is included in the cluster of node $u$ in this rounding. We can show $\\mathbb{E}\\left[X_{v} \\cdot X_{w}\\right]=y_{u v w}$ and $\\mathbb{E}\\left[X_{v}\\right] \\cdot \\mathbb{E}\\left[X_{w}\\right]=$ $y_{u v} y_{u w}$. The covariance matrix $C O V_{u}$, where $C O V_{u}(v, w)=\\mathbb{E}\\left[X_{v} \\cdot X_{w}\\right]-\\mathbb{E}\\left[X_{v}\\right] \\cdot \\mathbb{E}\\left[X_{w}\\right]=y_{u v w}-y_{u v} y_{u w}$, must be positive semidefinite (PSD). This PSD constraint on the covariance matrix enforces a stronger constraint on $\\eta_{T}$. For instance, if all non-degenerate triangles centered at $u$ are $++$ triangles with $y$ value $\\left(y_{u v}=0.5, y_{u w}=0.5, y_{w v}=0, y_{u v w}=0\\right)$, then the covariance matrix of $C O V_{u}$ cannot be PSD because $C O V_{u}(v, w)=y_{u v w}-y_{u v} y_{u w}=-0.25$ for almost all non-diagonal entries.\n\nFor a triangle $T=\\left(y_{u v}, y_{u w}, y_{v w}, y_{u v w}\\right)$, we discretize $y_{u v}, y_{u w}, y_{v w}$ to incorporate the PSD constraint. We partition the interval $[0,1]$ into numerous subintervals $I_{1}, I_{2}, \\ldots, I_{t}$. Each triangle with $y$ value $\\left(y_{u v} \\in I_{i}, y_{u w} \\in\\right.$ $\\left.I_{j}, y_{v w} \\in I_{k}, y_{u v w}\\right)$ is placed in one of these interval combinations. We can rearrange $C O V_{u}$ as $Q_{u} \\in \\mathbb{R}^{t \\times t}$, where $Q_{u}\\left(I_{i}, I_{j}\\right)=\\sum_{y_{u v} \\in I_{i}, y_{u w} \\in I_{j}}\\left(y_{u v w}-y_{u v} y_{u w}\\right)$. Considering $Q=\\sum_{u \\in V} Q_{u}$, we can represent $Q$ using $T$ and $\\eta_{T}$. The PSD property of $Q_{u}$ implies $Q$ is PSD, thus enforcing a constraint on $\\eta_{T}$.\n\nDespite there being infinitely many types of triangles in each range $I_{i}, I_{j}, I_{k}$, our key observation is that $y_{u v w}-y_{u v} y_{u w}$ is multi-linear. Therefore, we only need a few triangles in each range to represent all possible triangles. We want to mention the triangles we need are fixed so can be precomputed and the only unsure variable is $\\eta_{T}$. To compute a lower bound $\\sum \\eta_{T}(\\Delta(T)-\\operatorname{cost}(T))$, we set up a semi-definite program (SDP) under the constraint that $Q$ is PSD. This SDP is independent of cluster LP and relies on the chosen interval and budget function. By employing a practical SDP solver, we demonstrate that $\\sum \\eta_{T}(\\Delta(T)-\\operatorname{cost}(T)) \\geq 0$.\n\nGaps and Hardness. A high-level intuition for the cluster LP is the following: (any) LPs cannot distinguish between a random graph and a nearly bipartite graph. For the cluster LP, given a complete graph $H=\\left(V_{H}, E_{H}\\right)$ with $n=\\left|V_{H}\\right|$, our Correlation Clustering instance is $G=\\left(V_{G}, E_{G}\\right)$ where $V_{G}=E_{H}$ and $e, f \\in V_{G}$ have a plus edge in $G$ if they share a vertex in $V$. Consider vertices of $H$ as ideal clusters in $G$ containing their incident edges. The LP fractionally will think that it is nearly bipartite, implying that the entire $E_{H}$ can be partitioned into $n / 2$ ideal clusters of the same size. Of course, integrally, such a partition is not possible in complete graphs.\n\nFor the cluster LP, it suffices to consider a complete graph instead of a random graph. We believe (but do not prove) that such a gap instance can be extended to stronger LPs (e.g., Sherali-Adams strengthening of the cluster LP), because it is known that Sherali-Adams cannot distinguish a random graph and a nearly bipartite graph [CMM09].\n\nThe idea for the NP-hardness of approximation is the same. The main difference, which results in a worse factor here, is that other polynomial-time algorithms (e.g., SDPs) can distinguish between random and nearly bipartite graphs! So, we are forced to work with slightly more involved structures.\n\nStill, we use a similar construction for 3 -uniform hypergraphs; let $H=\\left(V_{H}, E_{H}\\right)$ be the underlying 3 -uniform hypergraph and $G=\\left(V_{G}, E_{G}\\right)$ be the plus graph of the final Correlation Clustering instance where $V_{G}=E_{H}$ and $e, f \\in E_{H}$ has an edge in $G$ if they share a vertex in $H$. We use the hardness result of Cohen-Addad, Karthik, and Lee [CAL22] that shows that it is hard to distinguish whether $H$ is nearly bipartite, which implies that half of the vertices intersect every hyperedge, or close to a random hypergraph.\n\nOrganization. We show how to solve the cluster LP in Section 4, proving Theorem 1. It uses our improved preclustering procedure, which will be described in Section A. We prove Lemmas 9, 10 and 11 in Sections 5, 6 and 7 respectively; they will prove Theorems 2 and 3. We give the $\\left(\\frac{4}{3}-\\varepsilon\\right)$-integrality gap of the cluster LP (Theorem 4) in Section 8, and the improved hardness of $24 / 23-\\varepsilon$ (Theorem 5) in Section 9.\n\nGlobal Notations. For two sets $A$ and $B$, we use $A \\triangle B=(A \\backslash B) \\cup(B \\backslash A)$ to denote the symmetric difference between $A$ and $B$. We used $N_{u}^{+}$and $N_{u}^{-}$to denote the sets of + and -neighbors of a vertex $u$ respectively in the Correlation Clustering instance. For a clustering $\\mathcal{C}$ of $V$, we define $\\operatorname{obj}(\\mathcal{C})$ to be the objective value of $\\mathcal{C}$. For any $x \\in[0,1]^{\\binom{V}{2}}$, we already defined $\\operatorname{obj}(x)=\\sum_{u v \\in E^{+}} x_{u v}+\\sum_{u v \\in E^{-}}\\left(1-x_{u v}\\right)$. Recall that we defined $\\operatorname{cost}_{u}(v, w), \\Delta_{u}(v, w), \\operatorname{cost}(T)$ and $\\Delta(T)$ for a triangle $T=(u, v, w)$ or a degenerate triangle $T=(u, v)$ in Section 2; they depend on the budget functions $b^{+}$and $b^{-}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "# 4 Solving Cluster LP Relaxation Approximately \n\nIn this section, we show how to solve the cluster LP in polynomial time, by proving Theorem 1, which is repeated below.\n\nTheorem 1. Let $\\varepsilon>0$ be a small enough constant and opt be the cost of the optimum solution to the given Correlation Clustering instance. In time $n^{\\operatorname{poly}(1 / \\varepsilon)}$, we can output a solution $\\left(\\left(z_{S}\\right)_{S \\subseteq V},\\left(x_{u v}\\right)_{u v \\in\\binom{V}{2}}\\right)$ to the cluster LP with $\\operatorname{obj}(x) \\leq(1+\\varepsilon)$ opt, described using a list of non-zero coordinates.\n\nWe define some global parameters used across this section. Let $\\varepsilon_{1}=\\varepsilon^{3}, \\varepsilon_{\\mathrm{rt}}=\\varepsilon_{1}^{2}=\\varepsilon^{6}$, and $r=$ $\\Theta\\left(1 / \\varepsilon_{\\mathrm{rt}}^{2}\\right)=\\Theta\\left(1 / \\varepsilon^{12}\\right)$ be an integer, with some large enough hidden constant. The subscript \"rt\" stands for Raghavendra-Tan.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "### 4.1 Preclustering\n\nWe use the definition of a preclustered instance from [CLLN23], with some minor modifications.\nDefinition 12. Given a Correlation Clustering instance $\\left(V, E^{+} \\uplus E^{-}\\right)$, a preclustered instance is defined by a pair $\\left(\\mathcal{K}, E_{\\mathrm{adm}}\\right)$, where $\\mathcal{K}$ is a partition of $V$ (which can also be viewed as a clustering), and $E_{\\mathrm{adm}} \\subseteq\\binom{V}{2}$ is a set of pairs such that for every $u v \\in E_{\\mathrm{adm}}, u$ and $v$ are not in a same set in $\\mathcal{K}$.\n\nEach set $K \\in \\mathcal{K}$ is called an atom. An (unordered) pair uv between two vertices $u$ and $v$ in a same $K \\in \\mathcal{K}$ is called an atomic edge; in particular, a self-loop uu is an atomic edge. A pair that is neither an atomic nor an admissible edge is called a non-admissible edge.\n\nThere are two minor differences between our definition and the one in [CLLN23]. First, we require that $\\mathcal{K}$ forms a partition; this can be guaranteed by adding singletons. Second, we do not require an edge between two different non-singleton atoms to be non-admissible. Our construction can guarantee this condition, but it is not essential.\n\nDefinition 13. Given a preclustered instance $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$ for some Correlation Clustering instance $\\left(V, E^{+} \\uplus\\right.$ $E^{-}$), a clustering $\\mathcal{C}$ of $V$ is called good with respect to $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$ if\n\n- $u$ and $v$ are in the same cluster in $\\mathcal{C}$ for an atomic edge $u v$, and\n- $u$ and $v$ are not in the same cluster in $\\mathcal{C}$ for a non-admissible edge $u v$.\n\nThe following theorem with a worse bound on $\\left|E_{\\text {adm }}\\right|$ was proved in [CLLN23]. We give a cleaner proof of the theorem in Section A; as a byproduct, it achieves a better bound on $\\left|E_{\\text {adm }}\\right|$.\n\nTheorem 14. For any sufficiently small $\\varepsilon>0$, there exists a poly $\\left(n, \\frac{1}{\\varepsilon}\\right)$-time algorithm that, given a Correlation Clustering instance $\\left(V, E^{+} \\uplus E^{-}\\right)$with optimal value opt (which is not given to us), produces a preclustered instance $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$ such that\n\n- there exists a good clustering w.r.t $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$, whose cost is at most $(1+\\varepsilon)$ opt, and\n- $\\left|E_{\\text {adm }}\\right| \\leq O\\left(\\frac{1}{\\varepsilon^{2}}\\right) \\cdot$ opt.\n\nWe can assume in the preclustered instance $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$, the edges between two different atoms $K$ and $K^{\\prime}$ are all admissible, or all non-admissible. If one edge between them is non-admissible, we can change all other edges to non-admissible edges. This will not change the set of good clusterings, and it will decrease $\\left|E_{\\text {adm }}\\right|$.\n\nWe apply Theorem 14 to obtain a preclustered instance $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$, with the unknown good clustering $\\mathcal{C}_{1}^{*}$. We define $K_{u}$ to be the atom that contains $u$, and $k_{u}=\\left|K_{u}\\right|$. We shall use $N_{\\text {adm }}(u)$ to be the set of vertices $v$ such that $u v \\in E_{\\text {adm }} ;$ so $N_{\\text {adm }}(u)=N_{\\text {adm }}(v)$ if $v \\in K_{u}$. We further process the good clustering $\\mathcal{C}_{1}^{*}$ using the following procedure in [CLLN23]. This procedure is not a part of our algorithm; it is only for analysis purpose.\n\n```\nwhile there exists some \\(K_{u}\\) in a cluster \\(C \\in \\mathcal{C}_{1}^{*}\\) with \\(k_{u}<|C| \\leq k_{u}+\\varepsilon_{1} \\cdot\\left|N_{\\text {adm }}(u)\\right|\\) do\n    \\(\\mathcal{C}_{1}^{*} \\leftarrow \\mathcal{C}_{1}^{*} \\backslash\\{C\\} \\cup\\left\\{K_{u}, C \\backslash K_{u}\\right\\}\\)\n```\n\nClaim 15. The procedure increases $\\operatorname{obj}\\left(\\mathcal{C}_{1}^{*}\\right)$ by at most $2 \\varepsilon_{1} \\cdot\\left|E_{\\text {adm }}\\right|$.\nProof. Whenever we break $C$ into $K_{u}$ and $C \\backslash K_{u}$ in the procedure, the cost increase is at most $k_{u} \\cdot\\left(|C|-k_{u}\\right) \\leq$ $k_{u} \\cdot \\varepsilon_{1} \\cdot\\left|N_{\\text {adm }}(u)\\right|=\\varepsilon_{1} \\sum_{v \\in K_{u}}\\left|N_{\\text {adm }}(v)\\right|$. We separate each atom $K_{u}$ at most once. Therefore, the total cost increase is at most $\\varepsilon_{1} \\sum_{v \\in V}\\left|N_{\\text {adm }}(v)\\right|=2 \\varepsilon_{1} \\cdot\\left|E_{\\text {adm }}\\right|$.\n\nSo, the cost of $\\mathcal{C}_{1}^{*}$ after the procedure will be at most $(1+\\varepsilon)$ opt $+O\\left(\\varepsilon_{1}\\right)\\left|E_{\\text {adm }}\\right|$. Crucially, the following property is satisfied:\n(A1) For every $u \\in V, K_{u}$ is either a cluster in $\\mathcal{C}_{1}^{*}$, or in a cluster of size more than $k_{u}+\\varepsilon_{1} \\cdot\\left|N_{\\text {adm }}(u)\\right|$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "# 4.2 Bounded Sub-Cluster LP Relaxation for Preclustered Instances \n\nFollowing [CLLN23], we form an LP relaxation aiming at finding the good clustering $\\mathcal{C}_{1}^{*}$. In the LP, we have a variable $y_{S}^{*}$, for every $s \\in[n]$, and $S \\subseteq V$ of size at most $r$ (recall that $r=\\Theta\\left(1 / \\varepsilon^{12}\\right)$ ), that denotes the number of clusters in $\\mathcal{C}_{1}^{*}$ of size $s$ containing $S$ as a subset. When $S \\neq \\emptyset$, there is at most one such cluster and thus $y_{S}^{*} \\in\\{0,1\\}$ indicates if $S$ is a subset of a cluster of size $s$ in $\\mathcal{C}_{1}^{*}$. For every $S \\subseteq V$ of size at most $r$,\n\nlet $y_{S}:=\\sum_{s} y_{S}^{*}$ denote the number of clusters (of any size) in $\\mathcal{C}_{1}^{*}$ containing $S$ as a subset. Again, if $S \\neq \\emptyset$, then $y_{S} \\in\\{0,1\\}$ indicates if $S$ is a subset of a cluster in $\\mathcal{C}_{1}^{*}$. For every $u v \\in\\binom{V}{2}$, we have a variable $x_{u v}$ indicating if $u$ and $v$ are separated or not in $\\mathcal{C}_{1}^{*}$. We call the LP the bounded sub-cluster LP relaxation, as we have variables indicating if a small set $S$ is a subset of a cluster or not.\n\nWe use the following type of shorthand: $y_{u}^{s}$ for $y_{\\{u\\}}^{s}, y_{u v}^{s}$ for $y_{\\{u, v\\}}^{s}$, and $y_{S u}^{s}$ for $y_{S \\cup\\{u\\}}^{s}$. The bounded sub-cluster LP is defined as follows. In the description, we always have $s \\in[n], u \\in V$ and $u v \\in\\binom{V}{2}$. For convenience, we omit the restrictions. By default, any variable of the form $y_{S}$ or $y_{S}^{s}$ has $|S| \\leq r$; if not, we do not have the variable and the constraint involving it.\n\n$$\n\\begin{aligned}\n& \\min \\quad \\operatorname{obj}(x) \\\\\n& \\sum_{s=1}^{n} y_{S}^{s}=y_{S} \\quad \\forall S \\\\\n& y_{u}=1 \\quad \\forall u \\\\\n& y_{u v}+x_{u v}=1 \\quad \\forall u v \\\\\n& \\frac{1}{s} \\sum_{u} y_{S u}^{s}=y_{S}^{s} \\quad \\forall s, S \\\\\n& y_{S}^{s} \\geq 0 \\quad \\forall s, S\n\\end{aligned}\n$$\n\n(bounded sub-cluster LP)\n\n$$\n\\begin{aligned}\n& x_{u v}=0 \\quad \\forall u, v \\text { in a same } K \\in \\mathcal{K} \\\\\n& x_{u v}=1 \\quad \\forall \\text { non-admissible edge } u v \\\\\n& y_{u}^{s}=0 \\quad \\forall u, s \\in\\left[k_{u}-1\\right] \\cup\\left[k_{u}+1, k_{u}+\\varepsilon_{1}\\left|N_{\\text {adm }}(u)\\right|\\right] \\\\\n& \\sum_{T^{\\prime} \\subseteq T}(-1)^{\\left|T^{\\prime}\\right|} y_{S \\cup T^{\\prime}}^{s} \\in\\left[0, y_{S}^{s}\\right] \\quad \\forall s, S \\cap T=\\emptyset \\\\\n& y_{S}^{s} \\geq 0 \\quad \\forall s, S\n\\end{aligned}\n$$\n\n(6) gives the definition of $y_{S}$, (7) requires $u$ to be contained in some cluster, and (8) gives the definition of $x_{u v}$. (9) says if $y_{S}^{s}=1$, then there are exactly $s$ elements $u \\in V$ with $y_{S u}^{s}=1$. (An exception is when $S=\\emptyset$; but the equality also holds.) (10) is the non-negativity constraint. (11) and (12) follows from that $\\mathcal{C}_{1}^{*}$ is a good clustering, and (13) follows from (A1). The left side of (14) is the number of clusters of size $s$ containing $S$ but does not contain any vertex in $T$. So the inequality holds. This corresponds to a SheraliAdams relaxation needed for the correlated rounding [RT12], see Lemma 16. The running time for solving the LP is $n^{O(r)}=n^{O\\left(1 / \\varepsilon^{12}\\right)}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 4.3 Sampling One Cluster Using LP Solution to the Bounded Sub-Cluster LP \n\nWe solve the bounded sub-cluster LP to obtain the $y$ and $x$ vectors. Given $y$, we can use the procedure construct-cluster described in Algorithm 3, which is from [CLLN23], to produce a random cluster $C$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "## Algorithm 3 construct-cluster $(y)$\n\nrandomly choose a cardinality $s$, so that $s$ is chosen with probability $\\frac{y_{S}^{s}}{y_{S}}$\n2: randomly choose a vertex $u \\in V$, so that $u$ is chosen with probability $\\frac{y_{u}^{s}}{s y_{u}^{s}}$\n3: define a vector $y^{\\prime}$ such that $y_{S}^{\\prime}=\\frac{y_{S u}^{s}}{y_{S}}$ for every $S \\subseteq V$ of size at most $r-1$\n4: apply the Raghavendra-Tan correlated rounding technique over the fractional set $y^{\\prime}$ to construct a cluster $C \\subseteq V$ that does not break any atom, and return $C$\n\nWith (14), the Raghavendra-Tan technique can be applied:\nLemma 16 ([RT12]). In Step 4 of Algorithm 3, one can sample a set $C \\subseteq V$ that does not break atoms in time $n^{O(r)}$ such that\n\n- For each $v \\in V, \\operatorname{Pr}[v \\in C]=y_{v}^{\\prime}$.\n- $\\frac{1}{\\left|N_{\\text {adm }}(u)\\right|^{2}} \\sum_{v, w \\in N_{\\text {adm }}(u)}\\left|\\operatorname{Pr}[v, w \\in C]-y_{v w}^{\\prime}\\right| \\leq \\varepsilon_{\\mathrm{rt}}$.\n\nRecall that $\\varepsilon_{\\mathrm{rt}}=\\Theta(1 / \\sqrt{r})$ and the hidden constant inside $\\Theta(\\cdot)$ is large enough.\n\nAs in [CLLN23], we define $\\operatorname{err}_{v w \\mid u}^{s}$ to be the error generated by the procedure when we choose $s$ as the cardinality and $u$ as the pivot:\n\n$$\n\\operatorname{err}_{v w \\mid u}^{s}:=\\left|\\operatorname{Pr}[v, w \\in C \\mid s, u]-\\frac{y_{u v w}^{s}}{y_{u}^{s}}\\right|, \\forall v w \\in\\binom{ V}{2}\n$$\n\nand\n\n$$\n\\operatorname{err}_{v w}^{s}:=\\frac{1}{s y_{\\emptyset}^{s}} \\sum_{u \\in V} y_{u}^{s} \\cdot \\operatorname{err}_{v w \\mid u}^{s} \\text { and } \\operatorname{err}_{v w}:=\\sum_{s} \\frac{y_{\\emptyset}^{s}}{y_{\\emptyset}} \\cdot \\operatorname{err}_{v w}^{s}\n$$\n\nas the error for $v w$ conditioned on $s$, and the unconditioned error. Notice that all these quantities are expectations of random variables, and thus deterministic.\n\nThe following two lemmas can be proved using the same arguments as in [CLLN23].\nLemma 17 ([CLLN23]). For any $v \\in V$, we have $\\operatorname{Pr}[v \\in C]=\\frac{1}{y_{\\emptyset}}$.\nLemma 18 ([CLLN23]). Focus on an edge $v w \\in\\binom{ V}{2}$.\n\n1. $\\operatorname{Pr}[v \\in C, w \\notin C] \\leq \\frac{1}{y_{\\emptyset}} \\cdot x_{v w}+\\operatorname{err}_{v w}$.\n2. $\\operatorname{Pr}[|\\{v, w\\} \\subseteq C] \\leq \\frac{1}{y_{\\emptyset}} \\cdot y_{v w}+\\operatorname{err}_{v w}$.\n\nA similar lemma to the following is proved in [CLLN23]. The parameters we use here are slightly different and we provide a proof for completeness.\n\nLemma 19. $\\sum_{v w \\in\\binom{ V}{2}} \\operatorname{err}_{v w} \\leq O\\left(\\varepsilon_{1}\\right) \\cdot \\frac{1}{y_{\\emptyset}}\\left|E_{\\text {adm }}\\right|$.\nProof. Throughout the proof, we assume $u, v, w$ are all in $V, v w$ and $u w$ are in $\\binom{V}{2}$.\nFix some $s \\in[n], u \\in V$ with $y_{u}^{s}>0$, and we now bound $\\sum_{v w} \\operatorname{err}_{v w \\mid u}^{s}$. If $s=k_{u}$, then $C=K_{u}$; no errors will be created and the quantity is 0 . Assume $s>k_{u}$. By (13), we have that $s>k_{u}+\\varepsilon_{1} \\cdot\\left|N_{\\text {adm }}(u)\\right|$, since otherwise we shall $y_{u}^{s}=0$. By the second property of Lemma 16, we have $\\sum_{v w} \\operatorname{err}_{v w \\mid u}^{s} \\leq \\frac{\\varepsilon_{1}}{2}\\left|N_{\\text {adm }}(u)\\right|^{2}$. (Notice that if one of $v$ and $w$ is not in $N_{\\text {adm }}(u)$, then $\\operatorname{err}_{v w \\mid u}^{s}=0$.) Recall that $\\varepsilon_{\\mathrm{rt}}=\\varepsilon_{1}^{2}$. Therefore,\n\n$$\n\\begin{aligned}\n\\sum_{v w \\in\\binom{ V}{2}} \\operatorname{err}_{v w \\mid u}^{s} & \\leq \\frac{\\varepsilon_{\\mathrm{rt}}}{2} \\cdot\\left|N_{\\mathrm{adm}}(u)\\right|^{2} \\leq \\frac{\\varepsilon_{\\mathrm{rt}}}{2 \\varepsilon_{1}} \\cdot\\left|N_{\\mathrm{adm}}(u)\\right| \\cdot\\left(s-k_{u}\\right) \\\\\n& =\\frac{\\varepsilon_{1}}{2} \\cdot\\left|N_{\\mathrm{adm}}(u)\\right| \\cdot \\sum_{v \\in N_{\\mathrm{adm}}(u)} \\frac{y_{u v}^{s}}{y_{u}^{s}}=\\frac{\\varepsilon_{1}}{2} \\cdot \\sum_{v, w \\in N_{\\mathrm{adm}}(u)} \\frac{y_{u v}^{s}}{y_{u}^{s}}\n\\end{aligned}\n$$\n\nThe first equality is by (9) and $y_{u v}^{s}=y_{u}^{s}$ for every $v \\in K_{u}$. (To see this, notice that $y_{u v}^{s} \\leq y_{u}^{s}$ is implied by (14). We have $y_{u v}=\\sum_{s} y_{u v}^{s}, y_{u}=\\sum_{s} y_{u}^{s}$, and $y_{u v}=y_{u}=1$ if $v \\in K_{u}$.)\n\nConsidering the inequalities over all $u \\in V$, we have\n\n$$\n\\begin{aligned}\n\\sum_{v w} \\operatorname{err}_{v w}^{s} & =\\frac{1}{s y_{\\emptyset}^{s}} \\sum_{u} y_{u}^{s} \\cdot \\sum_{v w} \\operatorname{err}_{v w \\mid u}^{s} \\leq \\frac{1}{s y_{\\emptyset}^{s}} \\sum_{u} y_{u}^{s} \\cdot \\sum_{v, w \\in N_{\\text {adm }}(u)} \\frac{\\varepsilon_{1}}{2} \\cdot \\frac{y_{u v}^{s}}{y_{u}^{s}}=\\frac{\\varepsilon_{1}}{2} \\cdot \\frac{1}{s y_{\\emptyset}^{s}} \\cdot \\sum_{u \\in V, v, w \\in N_{\\text {adm }}(u)} y_{u v}^{s} \\\\\n& =\\frac{\\varepsilon_{1}}{2} \\cdot \\sum_{v \\in V} \\frac{y_{v}^{s}}{s y_{\\emptyset}^{s}} \\sum_{u \\in N_{\\text {adm }}(v), w \\in N_{\\text {adm }}(u)} \\frac{y_{u v}^{s}}{y_{v}^{s}} \\leq \\frac{\\varepsilon_{1}}{2} \\cdot \\sum_{v \\in V} \\frac{y_{v}^{s}}{s y_{\\emptyset}^{s}} \\sum_{u w \\in E_{\\text {adm }}}\\left(\\frac{y_{u v}^{s}+y_{v w}^{s}}{y_{v}^{s}}\\right) \\\\\n& \\leq \\varepsilon_{1} \\cdot \\sum_{v \\in V} \\frac{y_{v}^{s}}{s y_{\\emptyset}^{s}} \\sum_{u w \\in E_{\\text {adm }}} \\operatorname{Pr}[C \\cap\\{u, w\\} \\neq \\emptyset \\mid s, v \\text { is pivot }] \\\\\n& =\\varepsilon_{1} \\sum_{u w \\in E_{\\text {adm }}} \\operatorname{Pr}[C \\cap\\{u, w\\} \\neq \\emptyset \\mid s] .\n\\end{aligned}\n$$\n\nTo see the last inequality, notice that $\\frac{y_{v w}^{s}}{y_{v}^{s}}=\\operatorname{Pr}[u \\in C \\mid s, v$ is pivot $] \\leq \\operatorname{Pr}[C \\cap\\{u, w\\} \\neq \\emptyset \\mid s, v$ is pivot $]$. The same inequality holds for $\\frac{y_{v w}^{s}}{y_{v}^{s}}$.\n\nFinally, we take all $s$ into consideration:\n\n$$\n\\begin{aligned}\n\\sum_{v w} \\operatorname{err}_{v w} & =\\sum_{s} \\frac{y_{\\emptyset}^{s}}{y_{\\emptyset}} \\cdot \\sum_{v w} \\operatorname{err}_{v w}^{s} \\leq \\varepsilon_{1} \\cdot \\sum_{s} \\frac{y_{\\emptyset}^{s}}{y_{\\emptyset}} \\sum_{u w \\in E_{\\text {adm }}} \\operatorname{Pr}[C \\cap\\{u, w\\} \\neq \\emptyset \\mid s] \\\\\n& =\\varepsilon_{1} \\cdot \\sum_{u w \\in E_{\\text {adm }}} \\operatorname{Pr}[C \\cap\\{u, w\\} \\neq \\emptyset] \\leq \\frac{2 \\varepsilon_{1}}{y_{\\emptyset}}\\left|E_{\\text {adm }}\\right|+3 \\varepsilon_{1} \\sum_{u w \\in\\binom{ v}{s}} \\operatorname{err}_{u w}\n\\end{aligned}\n$$\n\nTo see the last inequality, we notice that $C \\cap\\{u, w\\} \\neq \\emptyset$ is the union of the 3 disjoint events: $u \\in C$ and $w \\notin C, u \\notin C$ and $w \\in C$, and $\\{u, w\\} \\notin C$. By Lemma 18, we have $\\operatorname{Pr}[C \\cap\\{u, w\\} \\neq \\emptyset] \\leq \\frac{2 \\varepsilon_{v w}+y_{v w}}{y_{\\emptyset}}+3 \\cdot \\operatorname{err}_{u w} \\leq$ $\\frac{2}{y_{\\emptyset}}+3 \\cdot \\operatorname{err}_{u w}$. So, we have $\\sum_{v w} \\operatorname{err}_{v w} \\leq \\frac{1}{1-3 \\varepsilon_{1}} \\cdot \\frac{2 \\varepsilon_{1}}{y_{\\emptyset}}\\left|E_{\\text {adm }}\\right|$. This proves the lemma.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "# 4.4 Construction of Solution to the Cluster LP Using Independently Sampled Clusters \n\nWith all the ingredients, we can now describe our algorithm for solving the cluster LP approximately, finishing the proof of Theorem 1. Let $\\Delta=\\Theta\\left(\\frac{n^{2} \\log n}{\\varepsilon_{1}^{2}\\left\\|E_{\\text {adm }}\\right\\|}\\right)$ with a large enough hidden constant, and $\\Delta y_{\\emptyset}$ being an integer. (We assume $\\left|E_{\\text {adm }}\\right| \\geq 1$ since otherwise the preclustered instance is trivial.) We run Algorithm 3 $\\Delta y_{\\emptyset}$ times independently to obtain clusters $C_{1}, C_{2}, \\cdots, C_{\\Delta y_{\\emptyset}}$.\n\nWe use the following variant of Chernoff bound.\nTheorem 20. Let $X_{1}, X_{2}, X_{3}, \\cdots, X_{n}$ be independent (not necessarily iid) random varibles which take values in $[0,1]$. Let $X=\\sum_{i=1}^{n} X_{i}, \\mu=\\mathbb{E}[X]$, and $\\mu^{\\prime} \\geq \\mu$ be a real. Then for any $\\delta \\in(0,1)$, we have\n\n$$\n\\operatorname{Pr}[X<(1-\\delta) \\mu]<e^{-\\delta^{2} \\mu / 2} \\quad \\text { and } \\quad \\operatorname{Pr}\\left[X>\\mu+\\delta \\mu^{\\prime}\\right]<e^{-\\delta^{2} \\mu^{\\prime} / 3}\n$$\n\nFor every $u \\in V$, let $R_{u}=\\left\\{t: u \\in C_{t}\\right\\}$. Notice that $\\Delta y_{\\emptyset} \\cdot \\frac{\\left|E_{\\text {adm }}\\right|}{y_{\\emptyset} n^{2}}=\\Theta\\left(\\frac{\\log n}{\\varepsilon_{1}^{2}}\\right)$, with a large enough hidden constant. Using Chernoff bound and union bound, we can prove that with probability at least $1-1 / n$, the following conditions hold.\n\n- For every $u \\in V$, we have $\\left|R_{u}\\right| \\geq\\left(1-\\varepsilon_{1}\\right) \\Delta y_{\\emptyset} \\cdot \\frac{1}{y_{\\emptyset}}=\\left(1-\\varepsilon_{1}\\right) \\Delta$.\n- For every $u, v \\in V$ such that $u v \\in E^{+}$, we have\n\n$$\n\\begin{aligned}\n\\left|R_{u} \\backslash R_{v}\\right| & \\leq \\Delta y_{\\emptyset}\\left(\\frac{x_{u v}}{y_{\\emptyset}}+\\operatorname{err}_{u v}+\\varepsilon_{1} \\cdot \\max \\left\\{\\frac{x_{u v}}{y_{\\emptyset}}+\\operatorname{err}_{u v}, \\frac{\\left|E_{\\text {adm }}\\right|}{y_{\\emptyset} n^{2}}\\right\\}\\right) \\\\\n& \\leq\\left(1+\\varepsilon_{1}\\right) \\Delta\\left(x_{u v}+y_{\\emptyset} \\operatorname{err}_{u v}\\right)+\\frac{\\varepsilon_{1} \\Delta\\left|E_{\\text {adm }}\\right|}{n^{2}}\n\\end{aligned}\n$$\n\n- For every $u v \\in E^{-}$, we have\n\n$$\n\\begin{aligned}\n\\left|R_{u} \\cap R_{v}\\right| & \\leq \\Delta y_{\\emptyset}\\left(\\frac{y_{u v}}{y_{\\emptyset}}+\\operatorname{err}_{u v}+\\varepsilon_{1} \\cdot \\max \\left\\{\\frac{y_{u v}}{y_{\\emptyset}}+\\operatorname{err}_{u v}, \\frac{\\left|E_{\\text {adm }}\\right|}{y_{\\emptyset} n^{2}}\\right\\}\\right) \\\\\n& \\leq\\left(1+\\varepsilon_{1}\\right) \\Delta\\left(y_{u v}+y_{\\emptyset} \\operatorname{err}_{u v}\\right)+\\frac{\\varepsilon_{1} \\Delta\\left|E_{\\text {adm }}\\right|}{n^{2}}\n\\end{aligned}\n$$\n\nFrom now on we assume the conditions hold. For every $u \\in V$, we let $R_{u}^{\\prime}$ be the set of the $[(1-\\varepsilon) \\Delta]$ smallest indices in $R_{u}$. Clearly, $\\left|R_{u}^{\\prime} \\cap R_{v}^{\\prime}\\right| \\leq\\left|R_{u} \\cap R_{v}\\right|$. We show $\\left|R_{u}^{\\prime} \\backslash R_{v}^{\\prime}\\right|$ is still upper bounded by (15).\nClaim 21. For every $u v \\in E^{+}$we have $\\max \\left\\{\\left|R_{u}^{\\prime} \\backslash R_{v}^{\\prime}\\right|,\\left|R_{v}^{\\prime} \\backslash R_{u}^{\\prime}\\right|\\right\\} \\leq\\left(1+\\varepsilon_{1}\\right) \\Delta\\left(x_{u v}+y_{\\emptyset} \\operatorname{err}_{u v}\\right)+\\frac{\\varepsilon_{1} \\Delta\\left|E_{\\text {adm }}\\right|}{n^{2}}$.\n\nProof. For convenience, we use $B$ to denote the upper bound $\\left(1+\\varepsilon_{1}\\right) \\Delta\\left(x_{u v}+y_{\\emptyset} \\operatorname{err}_{u v}\\right)+\\frac{\\varepsilon_{1} \\Delta\\left|E_{\\text {adm }}\\right|}{n^{2}}$. We think of $R_{u}^{\\prime}$ ( $R_{v}^{\\prime}$ resp.) as obtained from the set $R_{u}$ ( $R_{v}$ resp.) by removing the largest indices one by one. Wlog we assume $\\left|R_{u}\\right| \\geq\\left|R_{v}\\right|$; and thus initially $\\left|R_{v} \\backslash R_{u}\\right| \\leq\\left|R_{u} \\backslash R_{v}\\right| \\leq B$. We remove the elements from $R_{u}$ and $R_{v}$ in two stages.\n\nIn the first stage we do the following. While $\\left|R_{u}\\right|>\\left|R_{v}\\right|$, we remove the largest index from $R_{u}$. This can not increase $\\left|R_{u} \\backslash R_{v}\\right|$. After the first stage, we have $\\left|R_{u} \\backslash R_{v}\\right|=\\left|R_{v} \\backslash R_{u}\\right| \\leq B$.\n\nIn the second stage we do the following. While $\\left|R_{u}\\right|=\\left|R_{v}\\right|>\\lceil(1-\\varepsilon) \\Delta\\rceil$, we remove the largest index in $R_{u}$ from $R_{u}$, and do the same for $R_{v}$. Consider one iteration of the while loop. If the two indices are the same, then $\\left|R_{u} \\backslash R_{v}\\right|=\\left|R_{v} \\backslash R_{u}\\right|$ does not change. Otherwise, wlog we assume the index we removed from $R_{u}$ is larger. Then removing the index in $R_{u}$ will decrease $\\left|R_{u} \\backslash R_{v}\\right|$. So the iteration can not increase $\\left|R_{u} \\backslash R_{v}\\right|=\\left|R_{v} \\backslash R_{u}\\right|$.\n\nThen, for every $t \\in\\left[1, \\Delta y_{\\emptyset}\\right]$, we define $C_{t}^{\\prime}=\\left\\{u: t \\in R_{u}^{\\prime}\\right\\} \\subseteq C_{t}$; then every $v$ is contained in $C_{t}^{\\prime}$ for exactly $\\lceil(1-\\varepsilon) \\Delta\\rceil$ values of $t$. We define $z_{S}=\\frac{1}{\\lceil(1-\\varepsilon) \\Delta\\rceil} \\cdot\\left|\\left\\{t: C_{t}^{\\prime}=S\\right\\}\\right|$ for every $S \\subseteq V$ with $S \\neq \\emptyset$. Define $\\tilde{x}_{u v}=1-\\sum_{\\{u, v\\} \\subseteq S} z_{S}$ for every $u v \\in\\binom{V}{2}$. Then $(\\tilde{x}, z)$ is a valid solution to the cluster LP.\n\nFor a $u v \\in E^{+}$, we have\n\n$$\n\\tilde{x}_{u v}=\\frac{1}{\\lceil(1-\\varepsilon) \\Delta\\rceil} \\cdot\\left|R_{u}^{\\prime} \\backslash R_{v}^{\\prime}\\right| \\leq \\frac{1+\\varepsilon_{1}}{1-\\varepsilon}\\left(x_{u v}+y_{\\emptyset} \\operatorname{err}_{u v}\\right)+\\frac{\\varepsilon_{1}\\left|E_{\\mathrm{adm}}\\right|}{(1-\\varepsilon) n^{2}}\n$$\n\nFor a $u v \\in E^{-}$, we have\n\n$$\n\\left(1-\\tilde{x}_{u v}\\right)=\\frac{1}{\\lceil(1-\\varepsilon) \\Delta\\rceil} \\cdot\\left|R_{u}^{\\prime} \\cap R_{v}^{\\prime}\\right| \\leq \\frac{1+\\varepsilon_{1}}{1-\\varepsilon}\\left(1-x_{u v}+y_{\\emptyset} \\operatorname{err}_{u v}\\right)+\\frac{\\varepsilon_{1}\\left|E_{\\mathrm{adm}}\\right|}{(1-\\varepsilon) n^{2}}\n$$\n\nTherefore,\n\n$$\n\\begin{aligned}\n\\operatorname{obj}(\\tilde{x}) & \\leq(1+O(\\varepsilon))\\left(\\operatorname{obj}(x)+y_{\\emptyset} \\sum_{u v \\in\\binom{V}{2}} \\operatorname{err}_{u v}\\right)+O\\left(\\varepsilon_{1}\\right)\\left|E_{\\mathrm{adm}}\\right| \\leq(1+O(\\varepsilon)) \\operatorname{obj}(x)+O\\left(\\varepsilon_{1}\\right)\\left|E_{\\mathrm{adm}}\\right| \\\\\n& \\leq(1+O(\\varepsilon)) \\cdot \\operatorname{opt}+O\\left(\\varepsilon^{3}\\right) \\cdot O\\left(\\frac{1}{\\varepsilon^{2}}\\right) \\cdot \\operatorname{opt}=(1+O(\\varepsilon)) \\text { opt. }\n\\end{aligned}\n$$\n\nThe second inequality is due to Lemma 19, and the third one used that $\\left|E_{\\text {adm }}\\right| \\leq O\\left(\\frac{1}{\\varepsilon^{2}}\\right) \\cdot$ opt. By scaling $\\varepsilon$, the upper bound can be made to $(1+\\varepsilon)$ opt. This finishes the proof of Theorem 1.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# 5 Triangle Analysis used in 1.5-Approximation: Proof of Lemma 9 \n\nThe goal of this section is to prove Lemma 9, which is restated here.\nLemma 9. For budget functions $b^{+} \\equiv b_{1.5}^{+}$and $b^{-} \\equiv b_{1.5}^{-}$, we have $\\operatorname{cost}(T) \\leq \\Delta(T)$ for every triangle $T$.\nThe proof of Lemma 9 follows from Lemma 25, 26, 27, 28, 29 which we prove in this section. First, we recall some notation.\n\nNotations and Useful Observations We define some notation and make some observations that will be useful in this section and in Sections 6 and 7. Recall that we are given a polynomial-sized LP solution $\\left(z=\\left(z_{S}\\right)_{S \\subseteq V}, x \\in[0,1]\\binom{V}{2}\\right.$ to (cluster LP). For a +edge $u v$, we say $u v$ is short if $x_{u v} \\leq \\frac{1}{3}$ and long if $x_{u v}>\\frac{1}{3}$. Notice that the pivot-based rounding procedure (Algorithm 1) treat short and long +edges differently. For vertices $u, v, w \\in V$, we define\n\n$$\ny_{u v}:=\\sum_{S \\supseteq\\{u, v\\}} z_{S} \\in[0,1], \\quad y_{u v w}:=\\sum_{S \\supseteq\\{u, v, w\\}} z_{S} \\in[0,1], \\quad y_{u v \\mid w}:=y_{u v}-y_{u v w} \\in[0,1]\n$$\n\n$$\ny_{u[v] w}:=1-y_{u v \\mid w}-y_{u w \\mid v}-y_{v w \\mid u}-y_{u v w}=1-y_{u v}-y_{u w}-y_{v w}+2 y_{u v w} \\in[0,1]\n$$\n\nTo see that $y_{u[v] w} \\in[0,1]$, notice that $y_{u v}+y_{u w}+y_{v w}-2 y_{u v w}=\\sum_{S} z_{S} \\cdot\\left(1_{\\{u, v\\} \\subseteq S}+1_{\\{u, w\\} \\subseteq S}+1_{\\{v, w\\} \\subseteq S}-\\right.$ $\\left.2 \\cdot 1_{\\{u, v, w\\} \\subseteq S}\\right)$. The quantity after $z_{S}$ is either 0 or 1 , depending on the size of $S \\cap\\{u, v, w\\}$. In an integral solution $z, y_{u v}, y_{u v w}, y_{u v \\mid w}$ and $y_{u[v] w}$, respectively, indicate if $u$ and $v$ are in the same cluster, if $u, v$ and $w$ are all in the same cluster, if $u$ and $v$ are in the same cluster not containing $w$, and if $u, v, w$ are in three different clusters. These definitions also hold when $u, v$ and $w$ are not distinct vertices.\n\nIn the following two claims, we focus on one iteration of the while loop in Algorithm 2. $V^{\\prime}$ is the vertex set at the beginning of the iteration, and $C$ is the cluster obtained at the end. We use $u$ to denote the event that $u$ is the chosen pivot.\n\nClaim 22. Let $u, v \\in V^{\\prime}$ (it is possible that $u=v$ ). Then\n\n$$\n\\operatorname{Pr}[v \\in C \\mid u]= \\begin{cases}1 & \\text { if } u v \\text { is a short }+ \\text { edge } \\\\ 1-x_{u v}=y_{u v} & \\text { otherwise }\\end{cases}\n$$\n\nClaim 23. Let $u, v, w \\in V^{\\prime}$ with $v \\neq w$. Then\n\n$$\n\\begin{aligned}\n\\operatorname{Pr}[\\{v, w\\} \\subseteq C \\mid u] & = \\begin{cases}y_{u v w} & \\text { if both } u v \\text { and } u w \\text { are long }+ \\text { edges } \\\\\n\\operatorname{Pr}[v \\in C \\mid u] \\cdot \\operatorname{Pr}[w \\in C \\mid u] & \\text { otherwise }\\end{cases} \\\\\n\\operatorname{Pr}[v \\in C \\wedge w \\notin C \\mid u] & = \\begin{cases}y_{u v \\mid w} & \\text { if both } u v \\text { and } u w \\text { are long }+ \\text { edges } \\\\\n\\operatorname{Pr}[v \\in C \\mid u] \\cdot(1-\\operatorname{Pr}[w \\in C \\mid u]) & \\text { otherwise }\\end{cases}\n\\end{aligned}\n$$\n\nWe now prove some useful facts about the budget functions defined in Section 2,\n\n$$\nb_{\\alpha}^{+}(x)=\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{x^{2}}{1+x}, \\quad \\text { and } \\quad b_{\\alpha}^{-}(x)=\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{(1+2 x)(1-x)}{2(1+x)}, \\quad \\forall x \\in[0,1]\n$$\n\nLemma 24. For any $\\alpha \\in[1,2), b_{\\alpha}^{+}(x)$ is convex and decreasing. Moreover, $b_{\\alpha}^{-}(x)$ is increasing.\nProof. Consider the first and second derivative of $b_{\\alpha}^{+}(x)$,\n\n$$\n\\begin{aligned}\n\\frac{d}{d x} b_{\\alpha}^{+}(x) & =\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{x(2+x)}{(1+x)^{2}} \\geq 0 \\\\\n\\frac{d^{2}}{d^{2} x} b_{\\alpha}^{+}(x) & =\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{2}{(x+1)^{3}} \\geq 0\n\\end{aligned}\n$$\n\nHere, we used that $\\frac{\\alpha}{1-\\alpha / 2} \\geq 0$ and $x \\geq 0$. Similar, for $b_{\\alpha}^{-}(x)$,\n\n$$\n\\frac{d}{d x} b_{\\alpha}^{-}(x)=\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{-x(x+2)}{(1+x)^{2}} \\leq 0\n$$\n\nNow we start the proof of Lemma 9. Note that for $\\alpha=1.5$, the budget functions simplify to\n\n$$\nb^{+}(x):=\\frac{6 x^{2}}{1+x}, \\quad \\text { and } \\quad b^{-}(x):=\\frac{3(1+2 x)(1-x)}{(1+x)}, \\quad \\forall x \\in[0,1]\n$$\n\nWe will prove the lemma by considering each possible triangle type separately. That is, degenerate triangles, triangles with no +edge (--- triangles), triangles with one +edge (+-- triangles), triangles with two +edges (++- triangles) and lastly, triangles with three +edges (+++ triangles). Moreover, recall that the pivot rounding algorithm treats short + edges differently than long + edges. Therefore, for each type of triangle, we consider different cases that specify how many short and long + edges the triangle contains.\n\nDegenerate triangles For the case when two of the vertices $u, v, w$ are identical, we want to make sure that $\\operatorname{cost}(u, v) \\leq \\Delta(u, v)$.\n\nLemma 25. Fix budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-} \\equiv b_{\\alpha}^{-}$, where $\\alpha \\geq 4 / 3$. For any degenerate triangle $T$, we have $\\operatorname{cost}(T) \\leq \\Delta(T)$.\n\nProof. For a -edge with value $x$ this is equivalent to\n\n$$\n(1-x) \\leq \\frac{\\alpha(1+2 x)(1-x)}{(2-\\alpha)(1+x)}\n$$\n\nThis holds true since $\\frac{\\alpha(1+2 x)}{(2-\\alpha)(1+x)} \\geq \\frac{\\alpha}{(2-\\alpha)} \\geq 1$, for any $\\alpha \\geq 1$. For short + edges the cost is 0 and the inequality holds trivially. For long + edges with value $x>\\frac{1}{3}$ we need,\n\n$$\nx \\leq \\frac{2 \\alpha x^{2}}{(2-\\alpha)(1+x)}\n$$\n\nThis holds true since $\\frac{2 \\alpha x^{2}}{(2-\\alpha)(1+x)} \\geq \\frac{2 \\alpha / 3}{(2-\\alpha)(1+1 / 3)} \\geq 1$ for any $\\alpha \\geq 4 / 3$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# 5.1 +++ Triangles \n\nLemma 26. Fix the budget function $b^{+} \\equiv b_{\\alpha}^{+}$, where $\\alpha=1.5$. For any +++ triangle $T$, we have $\\operatorname{cost}(T) \\leq$ $\\Delta(T)$.\n\nProof. We distinguish between the following cases: the triangle has three short edges, two short edges, one short edge or no short edge.\n(s,s,s) $\\operatorname{cost}(T)=0$ and $\\Delta(T) \\geq 0$.\n(s,s,l) For a +++ triangle $T=(u, v, w)$, assume that $u v$ is long and $u w, v w$ are short:\n\n$$\n\\begin{aligned}\n& \\operatorname{cost}(T)=2 \\cdot x_{u v} \\\\\n& \\quad \\Delta(T)=b^{+}\\left(x_{u v}\\right)+b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right) \\geq b^{+}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u w}+x_{v w}}{2}\\right) \\geq b^{+}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u v}}{2}\\right)\n\\end{aligned}\n$$\n\nHere, we used that the function $b^{+}$is convex (Lemma 24) together with the triangle inequality, $x_{u v} \\leq$ $x_{u w}+x_{v w}$. For $x_{u v}>1 / 3$, we have that\n\n$$\n\\Delta(T) \\geq b^{+}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u v}}{2}\\right) \\geq \\frac{3}{2} x_{u v}+\\frac{7}{6} x_{u v} \\geq 2 \\cdot x_{u v}=\\operatorname{cost}(T)\n$$\n\n(s,l,l) For a +++ triangle $T=(u, v, w)$, assume that $u v$ is short and $u w$ and $v w$ are long:\n\n$$\n\\begin{aligned}\n\\operatorname{cost}(T) & =x_{u w}+x_{v w}+y_{v w}+y_{u w}-2 \\cdot y_{u v w} \\\\\n\\Delta(T) & =b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right)+b^{+}\\left(x_{u v}\\right) \\cdot\\left(y_{v w}+y_{u w}-y_{u v w}\\right) \\\\\n& \\geq b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right)\n\\end{aligned}\n$$\n\nBecause $x_{u w}>1 / 3$,\n\n$$\nb^{+}\\left(x_{u w}\\right) \\geq \\frac{3}{2} x_{u w}=\\frac{3}{2}\\left(1-y_{u w}\\right)\n$$\n\nHence,\n\n$$\n\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{2}\\left(x_{u w}+x_{v w}\\right)-y_{v w}-y_{u w}+2 \\cdot y_{u v w}\n$$\n\n$$\n\\begin{aligned}\n& =1-\\frac{3}{2}\\left(y_{v w}+y_{u w}\\right)+2 \\cdot y_{u v w} \\\\\n& \\geq y_{u v}+y_{v w}+y_{u w}-2 \\cdot y_{u v w}-\\frac{3}{2}\\left(y_{v w}+y_{u w}\\right)+2 \\cdot y_{u v w} \\\\\n& =y_{u v}-\\frac{1}{2}\\left(y_{v w}+y_{u w}\\right) \\\\\n& \\geq 0\n\\end{aligned}\n$$\n\nFor the first inequality we used $1-\\left(y_{u v}+y_{v w}+y_{u w}-2 \\cdot y_{u v w}\\right) \\geq 0$ and for the second that $y_{v w}, y_{u w} \\leq \\frac{2}{3}$ and $y_{u v} \\geq \\frac{2}{3}$.\n(1,1,1) For a +++ triangle $T$ where we use correlated rounding, CLN22] show that $\\frac{\\operatorname{cost}(T)}{\\Delta(T)} \\leq 1$, when we use budget $\\frac{3}{2} x$ for each +edge. We have $b^{+}(x) \\geq \\frac{3}{2} x$ for $x \\geq 1 / 3$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# 5.2 --- Triangles \n\nLemma 27. Fix the budget function $b^{-} \\equiv b_{\\alpha}^{-}$, where $\\alpha \\geq 1$. For any --- triangle $T$, we have $\\operatorname{cost}(T) \\leq$ $\\Delta(T)$.\n\nProof. We have that $b_{\\alpha}^{-}(x)=\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{(1+2 x)(1-x)}{2(1+x)} \\geq(1-x)$. As before, $\\frac{\\alpha(1+2 x)}{(2-\\alpha)(1+x)} \\geq \\frac{\\alpha}{(2-\\alpha)} \\geq 1$, for any $\\alpha \\geq 1$. [CLLN23] shows that the inequality holds true if the coefficient is 1 .\n\n$$\n\\begin{aligned}\n\\operatorname{cost}(T) & =y_{u v} y_{u w}+y_{u v} y_{v w}+y_{u w} y_{v w} \\\\\n\\Delta(T) & \\geq\\left(y_{u v}+y_{u w}-y_{u v} y_{u w}\\right) y_{v w}+\\left(y_{u v}+y_{v w}-y_{u v} y_{v w}\\right) y_{u w}+\\left(y_{u w}+y_{v w}-y_{u w} y_{v w}\\right) y_{u v} \\\\\n& =2\\left(y_{u v} y_{u w}+y_{u v} y_{v w}+y_{u w} y_{v w}\\right)-3 y_{u v} y_{u w} y_{v w} \\geq y_{u v} y_{u w}+y_{u v} y_{v w}+y_{u w} y_{v w}\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "### 5.3 +-- Triangles\n\nLemma 28. Fix budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-} \\equiv b_{\\alpha}^{-}$, where $\\alpha \\geq \\frac{4}{3}$. For any $+--$triangle $T$, we have $\\operatorname{cost}(T) \\leq \\Delta(T)$.\n\nProof. We have to prove the following two cases.\nshort +edge For a $+--$ triangle $T=(u, v, w)$, assume that $u v$ is a short +edge and $u w$ and $v w$ are -edges.\n\n$$\nb_{\\alpha}^{-}(x)=\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{(1+2 x)(1-x)}{2(1+x)} \\geq 2(1-x)\n$$\n\nTherefore, we have for $\\operatorname{cost}(T)$ and $\\Delta(T)$,\n\n$$\n\\begin{aligned}\n\\operatorname{cost}(T) & =y_{u w}+y_{v w}+y_{u w}+y_{v w}-2 \\cdot y_{u w} y_{v w} \\leq 2\\left(y_{u w}+y_{v w}\\right) \\\\\n\\Delta(T) & \\geq b^{-}\\left(x_{u w}\\right)+b^{-}\\left(x_{v w}\\right) \\geq 2\\left(y_{u w}+y_{v w}\\right)\n\\end{aligned}\n$$\n\nlong +edge For a $+--$ triangle $T=(u, v, w)$, assume that $u v$ is a long +edge and $u w$ and $v w$ are -edges. For $x \\geq 1 / 3$, we can lower bound the budget of the +edge,\n\n$$\nb_{\\alpha}^{+}(x)=\\frac{\\alpha}{1-\\alpha / 2} \\cdot \\frac{x^{2}}{1+x} \\geq \\frac{2 \\alpha / 3}{(2-\\alpha)(1+1 / 3)} x \\geq x\n$$\n\nAs above, $b^{-}(x) \\geq 2(1-x)$. The inequality is true if the coefficient for the +edge is 1 and for the - edge is 2, [CLLN23].\n\n$$\n\\begin{aligned}\n\\operatorname{cost}(T) & =y_{u w} y_{u v}+y_{v w} y_{u v}+y_{u w}+y_{v w}-2 y_{u w} y_{v w}=\\left(2-x_{u v}\\right)\\left(y_{u w}+y_{v w}\\right)-2 y_{u w} y_{v w} \\\\\n\\Delta(T) & \\geq\\left(y_{u w}+y_{v w}-y_{u w} y_{v w}\\right) x_{u v}+2\\left(y_{u w}+y_{u v}-y_{u w} y_{u v}\\right) y_{v w}+2\\left(y_{v w}+y_{u v}-y_{v w} y_{u v}\\right) y_{u w} \\\\\n& =\\left(y_{u w}+y_{v w}-y_{u w} y_{v w}\\right) x_{u v}+2\\left(1-x_{u v}+y_{u w} x_{u v}\\right) y_{v w}+2\\left(1-x_{u v}+y_{v w} x_{u v}\\right) y_{u w} \\\\\n& =\\left(2-x_{u v}\\right)\\left(y_{u w}+y_{v w}\\right)+3 y_{u w} y_{v w} x_{u v}\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "# 5.4 ++ Triangles \n\nLemma 29. Fix budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-} \\equiv b_{\\alpha}^{-}$, where $\\alpha=\\frac{3}{2}$. For any $+--$ triangle $T$, we have $\\operatorname{cost}(T) \\leq \\Delta(T)$.\n\nProof. Again, we distinguish cases depending on the number of short + edges.\n(s,s) For a ++ triangle $T=(u, v, w)$, assume that $u v$ is a - edge and $u w, v w$ are short + edges.\n\n$$\n\\begin{aligned}\n\\operatorname{cost}(T) & =2 \\cdot x_{u v}+1 \\\\\n\\Delta(T) & =b^{-}\\left(x_{u v}\\right)+b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right) \\geq b^{-}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u w}+x_{v w}}{2}\\right) \\geq b^{-}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u v}}{2}\\right)\n\\end{aligned}\n$$\n\nAs before, we used the convexity of $b^{+}$(Lemma 24) together with the triangle inequality, $x_{u v} \\leq x_{u w}+x_{v w}$. The derivative of (17) w.r.t. $x_{u v}$ is less than 0 for $x_{u v} \\geq 0$,\n\n$$\n-\\frac{6 x_{u v}\\left(4+3 x_{u v}\\right)}{\\left(1+x_{u v}\\right)^{2}\\left(2+x_{u v}\\right)^{2}} \\leq 0\n$$\n\nHence, (17) is decreasing with $x_{u v}$. Observe that $x_{u v} \\leq x_{u w}+x_{v w} \\leq \\frac{2}{3}$. Since the cost is increasing with $x_{u v}$ we only need to check the case $x_{u v}=\\frac{2}{3}$,\n\n$$\n\\Delta(T)-\\operatorname{cost}(T) \\geq b^{-}\\left(\\frac{2}{3}\\right)+2 \\cdot b^{+}\\left(\\frac{1}{3}\\right)-\\frac{7}{3}=\\frac{1}{15}\n$$\n\n(s,1) For a ++ triangle $T=(u, v, w)$, assume that $u v$ is a - edge, $u w$ is a short + edge and $v w$ is a long +edge.\n\n$$\n\\begin{aligned}\n\\operatorname{cost}(T) & =x_{u v}+y_{v w}+y_{v w}+y_{u v}-2 y_{v w} y_{u v}=1+2 y_{w v}\\left(1-y_{u v}\\right)=1+2\\left(1-x_{w v}\\right) x_{u v} \\\\\n\\Delta(T) & =b^{+}\\left(x_{v w}\\right)+b^{-}\\left(x_{u v}\\right)+b^{+}\\left(x_{u w}\\right)\\left(y_{v w}+y_{u v}-y_{v w} y_{u v}\\right) \\\\\n& =b^{+}\\left(x_{v w}\\right)+b^{-}\\left(x_{u v}\\right)+b^{+}\\left(x_{u w}\\right)\\left(2-x_{v w}-x_{u v}-\\left(1-x_{v w}\\right)\\left(1-x_{u v}\\right)\\right)\n\\end{aligned}\n$$\n\nCase $1 x_{v w} \\geq x_{u v}$.\n\n$$\n\\Delta(T) \\geq b^{+}\\left(x_{u v}\\right)+b^{-}\\left(x_{u v}\\right)=3\n$$\n\n$\\operatorname{cost}(T)$ is always less than 3 .\n\nCase 2 $x_{v w} \\leq x_{u v}$. We have $x_{u w} \\geq x_{u v}-x_{v w}$ by triangle inequality. Since $b^{+}(x)$ is increasing by Lemma 24 ,\n\n$$\n\\Delta(T) \\geq b^{+}\\left(x_{v w}\\right)+b^{-}\\left(x_{u v}\\right)+b^{+}\\left(x_{u v}-x_{v w}\\right)\\left(2-x_{v w}-x_{u v}-\\left(1-x_{v w}\\right)\\left(1-x_{u v}\\right)\\right)\n$$\n\nFor a fixed $z:=x_{u v}$, define $x:=x_{v w} \\geq \\frac{1}{3}$. We need to argue that the difference\n\n$$\n\\Delta(T)-\\operatorname{cost}(T)=b^{+}(x)+b^{-}(z)+b^{+}(z-x)(2-x-z-(1-x)(1-z))-1-2(1-x) z\n$$\n\nis non-negative.\nClaim 30. (18) is increasing with $x$.\nProof. The derivative of (18) w.r.t. $x$ is\n\n$$\n\\begin{gathered}\n\\frac{2\\left(6 x^{5} z+x^{4}(4-15 z) z+4 x^{3} z\\left(-3-5 z+3 z^{2}\\right)+x^{2} z\\left(-11+7 z+22 z^{2}-3 z^{3}\\right)\\right)}{(1+x)^{2}(1-x+z)^{2}} \\\\\n+\\frac{2\\left(-z\\left(5+z+2 z^{2}+3 z^{3}\\right)+x\\left(12+6 z+14 z^{2}+8 z^{3}-6 z^{4}\\right)\\right)}{(1+x)^{2}(1-x+z)^{2}}\n\\end{gathered}\n$$\n\nwhich is non-negative as long as the numerator is non-negative. The numerator is a concave function in $x$. Indeed, the second derivative of the numerator in (19) w.r.t. $x$ is\n\n$$\n4 z\\left(-11+60 x^{3}+x^{2}(24-90 z)+7 z+22 z^{2}-3 z^{3}+12 x\\left(-3-5 z+3 z^{2}\\right)\\right)\n$$\n\nWe want to show that (20) is non-positive. This is equivalent to\n\n$$\n-11+60 x^{3}+x^{2}(24-90 z)+7 z+22 z^{2}-3 z^{3}+12 x\\left(-3-5 z+3 z^{2}\\right) \\leq 0\n$$\n\nNote that (21) is a convex function in $x$. Indeed, the second derivative w.r.t. $x$ of (21) is equal to\n\n$$\n12(4+30 x-15 z) \\geq 0\n$$\n\nThe inequality holds true since either $z \\geq 2 / 3$ and $x \\geq z-\\frac{1}{3}$ or $z \\leq 2 / 3$. Hence, (21) is maximized for either $x=z, x=1 / 3$ or $x=z-1 / 3$.\n\n$$\n\\begin{aligned}\nx=z, & (21)=-11-29 z-14 z^{2}+3 z^{3} \\leq 0 \\\\\nx=\\frac{1}{3}, & (21)=-\\frac{163}{9}-23 z+34 z^{2}-3 z^{3} \\leq 0 \\\\\nx=z-\\frac{1}{3}, & (21)=\\frac{13}{9}-15 z-26 z^{2}+3 z^{3} \\leq 0\n\\end{aligned}\n$$\n\nAll inequalities above hold for $\\frac{1}{3} \\leq z \\leq 1$. Thus, the numerator of (19) is concave and minimized for either $x=1, x=1 / 3$ or $x=z-1 / 3$.\n\n$$\n\\begin{aligned}\nx=z, & 2 z\\left(7+5 z+z^{2}\\right) \\geq 0 \\\\\nx=\\frac{1}{3}, & \\frac{2}{27}\\left(108-124 z+95 z^{2}+96 z^{3}-144 z^{4}\\right) \\geq 0 \\\\\nx=z-\\frac{1}{3}, & -8+\\frac{688}{81} z+\\frac{86}{9} z^{2}+\\frac{22}{3} z^{3}+\\frac{14}{3} z^{4} \\geq 0\n\\end{aligned}\n$$\n\nThe first two inequalities hold for $\\frac{1}{3} \\leq z \\leq 1$. The last inequality holds for $\\frac{2}{3} \\leq z \\leq 1$. We can conclude that (18) is indeed increasing with $x$.\n\nTherefore, we can assume that $x=1 / 3$ if $z \\leq 2 / 3$ or $x=z-1 / 3$.\n\nCase 2.1 $x=1 / 3$.\n\n$$\n(18)=\\frac{1}{2}+b^{-}(z)+b^{+}\\left(z-\\frac{1}{3}\\right)\\left(1-\\frac{1}{3} z\\right)-1-\\frac{4}{3} z\n$$\n\nThe derivative of (22) is equal to\n\n$$\n\\frac{-2\\left(-22+124 z+494 z^{2}+510 z^{3}+153 z^{4}\\right)}{3(1+z)^{2}(2+3 z)^{2}}\n$$\n\nFor $z \\geq x=1 / 3$, we have that,\n\n$$\n-22+124 z+494 z^{2}+510 z^{3}+153 z^{4} \\geq 95\n$$\n\nHence, (22) is decreasing with $z$. For $z=2 / 3,(22)=2 / 5$.\nCase 2.2 $x=z-1 / 3$.\n\n$$\n(18)=b^{+}\\left(z-\\frac{1}{3}\\right)+b^{-}(z)+\\frac{1}{2}\\left(\\frac{7}{3}-2 z-\\left(\\frac{4}{3}-z\\right)(1-z)\\right)-1-2\\left(\\frac{4}{3}-z\\right) z\n$$\n\nThe derivative of (23) is equal to\n\n$$\n\\frac{-80-148 z-65 z^{2}+72 z^{3}+135 z^{4}+54 z^{5}}{2(1+z)^{2}(2+3 z)^{2}}\n$$\n\nFor $0 \\leq z \\leq 1$, we have that,\n\n$$\n-80-148 z-65 z^{2}+72 z^{3}+135 z^{4}+54 z^{5} \\leq 0\n$$\n\nHence, (23) is decreasing with $z$. For $z=1,(23)=\\frac{1}{10}$.\n(1,1) For a $++$ triangle $T=(u, v, w)$, assume that $u v$ is a -edge and $u w, v w$ are long + edges. We will proceed similar to [CLLN23]. For - edges we use a coefficient of $3, b^{-}(x) \\geq 3(1-x)$. For + edges define the coefficient $f(x)=\\frac{5 x}{1+x}, b^{+}(x)=f(x) \\cdot x$.\n\n$$\n\\begin{aligned}\n\\Delta(T) \\geq & 3\\left(y_{u w}+y_{v w}-y_{u v w}\\right) y_{u v}+f\\left(x_{v w}\\right)\\left(y_{u w}+y_{u v}-y_{u w} y_{u v}\\right)\\left(1-y_{v w}\\right) \\\\\n& +f\\left(x_{u w}\\right)\\left(y_{v w}+y_{u v}-y_{v w} y_{u v}\\right)\\left(1-y_{u w}\\right) \\\\\n= & f\\left(x_{v w}\\right) y_{u w}+f\\left(x_{u w}\\right) y_{v w}+\\left(f\\left(x_{u w}\\right)+f\\left(x_{a, c}\\right)\\right) y_{u v}+\\left(3-f\\left(x_{u w}\\right)-f\\left(x_{v w}\\right)\\right)\\left(y_{u w}+y_{v w}\\right) y_{u v} \\\\\n& -\\left(f\\left(x_{u w}\\right)+f\\left(x_{v w}\\right)\\right) y_{u w} y_{v w}\\left(1-y_{u v}\\right)-3 y_{u v w} y_{u v} . \\\\\n\\operatorname{cost}(T)= & y_{u v w}+y_{u w}+y_{u v}-2 y_{u w} y_{u v}+y_{v w}+y_{u v}-2 y_{v w} y_{u v}=y_{u v w}+y_{u w}+y_{v w}+2 y_{u v}-2\\left(y_{u w}+y_{v w}\\right) y_{u v}\n\\end{aligned}\n$$\n\nThe difference between the cost and the budget is at least,\n\n$$\n\\begin{gathered}\n\\Delta(T)-\\operatorname{cost}(T) \\geq\\left(f\\left(x_{v w}\\right)-1\\right) y_{u w}+\\left(f\\left(x_{u w}\\right)-1\\right) y_{v w}+\\left(f\\left(x_{u w}\\right)+f\\left(x_{a, c}\\right)-2\\right) y_{u v} \\\\\n+\\left(5-f\\left(x_{u w}\\right)-f\\left(x_{v w}\\right)\\right)\\left(y_{u w}+y_{v w}\\right) y_{u v}-\\left(f\\left(x_{u w}\\right)+f\\left(x_{v w}\\right)\\right) y_{u w} y_{v w}\\left(1-y_{u v}\\right)-3 y_{u v w} y_{u v}-y_{u v w}\n\\end{gathered}\n$$\n\nWe can decrease $y_{u v}$ until $y_{u v}=y_{u v w}$.\nClaim 31. (24) is increasing with $y_{u v}$.\n\nProof. We will show that the derivative of (24) w.r.t. $y_{u v}$ is non-negative. The derivative of (24) w.r.t. $y_{u v}$ is equal to\n\n$$\n\\left(5-f\\left(x_{u w}\\right)-f\\left(x_{v w}\\right)\\right)\\left(y_{u w}+y_{v w}\\right)+\\left(f\\left(x_{u w}\\right)+f\\left(x_{v w}\\right)-2\\right)+\\left(f\\left(x_{u w}\\right)+f\\left(x_{v w}\\right)\\right)\\left(y_{u w} y_{v w}\\right)-3 y_{u v w}\n$$\n\nObserve that $f(x) \\leq 3$ and $f(x) \\geq 2$ if $x \\geq \\frac{1}{2}$. Hence, if $x_{u w}, x_{v w} \\geq \\frac{1}{2}$,\n\n$$\n(25) \\geq-\\left(y_{u w}+y_{v w}\\right)+2+4\\left(y_{u w} y_{v w}\\right)-3 y_{u v w} \\geq 1+4 y_{u v w}^{2}-3 y_{u v w} \\geq 0\n$$\n\nNote that $f(x) \\leq 2$ if $x \\leq \\frac{1}{2}$ and $f(x) \\geq \\frac{3}{2}$ if $x \\geq \\frac{1}{3}$. Hence, If $\\frac{1}{3} \\leq x_{u w} \\leq \\frac{1}{2}$ and $x_{v w} \\geq \\frac{1}{3}$, then,\n\n$$\n(25) \\geq 1+3\\left(y_{u w} y_{v w}\\right)-3 y_{u v w} \\geq 1+3 y_{u v w}^{2}-3 y_{u v w}+1+3 y_{u v w}\\left(1-y_{u v w}\\right) \\geq 0\n$$\n\nSubstituting $y_{u v w}=y_{u v}$ into (24) yields,\n\n$$\n\\begin{aligned}\n& \\left(f\\left(x_{v w}\\right)-1\\right) y_{u w}+\\left(f\\left(x_{u w}\\right)-1\\right) y_{v w}+\\left(5-f\\left(x_{u w}\\right)-f\\left(x_{v w}\\right)\\right)\\left(y_{u w}+y_{v w}\\right) y_{u v} \\\\\n& \\quad+\\left(f\\left(x_{u w}\\right)+f\\left(x_{v w}\\right)-3\\right) y_{u v}-\\left(f\\left(x_{u w}\\right)+f\\left(x_{v w}\\right)\\right) y_{u w} y_{v w}\\left(1-y_{u v}\\right)-3 y_{u v}^{2}\n\\end{aligned}\n$$\n\nWe have to show that (26) is non-negative. For $y_{u v}$ we have the constraints $y_{u v}=y_{u v w} \\leq \\min \\left\\{y_{u w}, y_{v w}\\right\\}$ and $y_{u v} \\geq\\left(y_{u w}+y_{v w}-1\\right)_{+}$. (26) is a quadratic in $y_{u v}$ with leading coefficient -3 . Hence, (26) is minimized either by $y_{u v}=\\left(y_{u w}+y_{v w}-1\\right)_{+}$or by $y_{u v}=\\min \\left\\{y_{u w}, y_{v w}\\right\\}$.\n\nCase 1: $y_{u v}=0$. In this case,\n\n$$\n(26)=\\left(f\\left(x_{v w}\\right)-1\\right) y_{u w}+\\left(f\\left(x_{u w}\\right)-1\\right) y_{v w}-\\left(f\\left(x_{u w}\\right)+f\\left(x_{v w}\\right)\\right) y_{u w} y_{v w}\n$$\n\nIf $x_{u w}, x_{v w} \\geq \\frac{1}{2}$, then,\n\n$$\n\\begin{aligned}\n(27) & =f\\left(x_{v w}\\right)\\left(y_{u w}-y_{u w} y_{v w}\\right)+f\\left(x_{u w}\\right)\\left(y_{v w}-y_{u w} y_{v w}\\right)-\\left(y_{u w}+y_{v w}\\right) \\\\\n& \\geq 2\\left(y_{u w}-y_{u w} y_{v w}\\right)+2\\left(y_{v w}-y_{u w} y_{v w}\\right)-\\left(y_{u w}+y_{v w}\\right) \\\\\n& =y_{u w}+y_{v w}-4 y_{u w} y_{v w} \\\\\n& \\geq 0\n\\end{aligned}\n$$\n\nFor the last inequality we used that $y_{u w}, y_{v w} \\leq \\frac{1}{2}$ and hence, $y_{u w} \\geq 2 y_{u w} y_{v w}$. Otherwise, we can assume w.l.o.g. that $x:=x_{v w} \\leq \\frac{1}{2} \\leq 1-x \\leq x_{u w}=: y$, since $1 \\geq y_{u v}+y_{u w}+y_{v w}-2 y_{u v w}=y_{u v}+y_{u w}$.\n\nClaim 32. (27) is increasing with $y=x_{u w}$.\nProof. The derivative of (27) w.r.t. $y$ is\n\n$$\n\\frac{\\left(2 y+y^{2}\\right)\\left(7+x-12 x^{2}\\right)+6 x^{2}-x-1}{(y+1)^{2}(x+1)} \\geq 0\n$$\n\nThe inequality holds since for $y \\geq \\frac{1}{2}$ and $x \\leq \\frac{1}{2}$, the numerator is bounded by\n\n$$\n\\left(2 y+y^{2}\\right)\\left(7+x-12 z^{2}\\right)+6 x^{2}-x-1 \\geq \\frac{31}{4}+\\frac{1}{4} x-9 x^{2} \\geq 0\n$$\n\nHence, (27) is minimized for $y=(1-x)$,\n\n$$\n(27) \\geq(f(1-y)-1)(1-y)+(f(y)-1) y-(f(y)+f(1-y)) y(1-y)\n$$\n\nClaim 33. (28) is decreasing with $y$.\nProof. The derivative of (28) w.r.t $y$ is\n\n$$\n\\frac{6(2 y-1)\\left(2 y^{4}-4 y^{3}-6 y^{2}+8 y+5\\right)}{(y-2)^{2}(y+1)^{2}} \\geq 0\n$$\n\nsince $(2 y-1) \\geq 0$ for $y \\geq \\frac{1}{2}$ and $\\left(2 y^{4}-4 y^{3}-6 y^{2}+8 y+5\\right) \\geq 0$.\nThus, (28) is minimized for $y=\\frac{1}{2}$. We have that $y=x_{u w}, x=x_{v w}=\\frac{1}{2}$. As before,\n\n$$\n(28)=\\left(f\\left(\\frac{1}{2}\\right)-1\\right) \\frac{1}{2}+\\left(f\\left(\\frac{1}{2}\\right)-1\\right) \\frac{1}{2}-\\left(f\\left(\\frac{1}{2}\\right)+\\left(\\frac{1}{2}\\right)\\right) \\frac{1}{4}=0\n$$\n\nCase 2: $y_{u w}+y_{v w} \\geq 1$ and $y_{u v}=y_{u w}+y_{v w}-1$. In this case,\n\n$$\n\\begin{aligned}\n(26)=\\left(f\\left(x_{v w}\\right)-1\\right) y_{u w}+\\left(f\\left(x_{u w}\\right)-1\\right) y_{v w}+\\left(5-f\\left(x_{u w}\\right)-f\\left(x_{v w}\\right)\\right)\\left(y_{u w}+y_{v w}\\right)\\left(y_{u w}+y_{v w}-1\\right) \\\\\n+\\left(f\\left(x_{u w}\\right)+f\\left(x_{v w}\\right)-3\\right)\\left(y_{u w}+y_{v w}-1\\right) \\\\\n-\\left(f\\left(x_{u w}\\right)+f\\left(x_{v w}\\right)\\right) y_{u w} y_{v w}\\left(2-\\left(y_{u w}+y_{v w}\\right)\\right)-3\\left(y_{u w}+y_{v w}-1\\right)^{2}\n\\end{aligned}\n$$\n\nSetting $y:=x_{u w}$ and $x:=x_{v w}$, we get\n\n$$\n\\begin{aligned}\n(29)= & \\left(\\frac{6 x}{1+x}-1\\right)(1-y)+\\left(\\frac{6 y}{1+y}-1\\right)(1-y)+\\left(5-\\frac{6 y}{1+y}-\\frac{6 x}{1+x}\\right)(2-y-x)(1-y-x) \\\\\n& +\\left(\\frac{6 y}{1+y}+\\frac{6 x}{1+x}-3\\right)(1-y-x)-\\left(\\frac{6 y}{1+y}+\\frac{6 x}{1+x}\\right)(1-y)(1-x)(y+x)-3(1-y-x)^{2} \\\\\n= & \\frac{2-3 x+3 x^{2}+2 x^{3}-2 y^{3}\\left(-1+2 x+6 x^{2}\\right)+y^{2}\\left(3+7 x-8 x^{2}-12 x^{3}\\right)-y\\left(3+4 x-7 x^{2}+4 x^{3}\\right)}{(1+y)(1+x)}\n\\end{aligned}\n$$\n\nSince $x+y \\leq 1$, we can assume w.l.o.g. that $y \\leq \\frac{1}{2}$.\nClaim 34. (30) is decreasing with $y$.\nProof. The derivative of (30) w.r.t. $y$ is equal to\n\n$$\n\\frac{4 y^{3}\\left(-1+2 x+6 x^{2}\\right)+y^{2}\\left(-9+5 x+44 x^{2}+12 x^{3}\\right)+2 y\\left(-3-7 x+8 x^{2}+12 x^{3}\\right)+5+x-4 x^{2}+6 x^{3}}{-(1+y)^{2}(1+x)}\n$$\n\n(31) is less than or equal to 0 if and only if the numerator is non-negative. Since $x \\geq \\frac{1}{3}$,\n\n$$\n\\begin{gathered}\n\\left(-1+2 x+6 x^{2}\\right) \\geq \\frac{1}{3} \\\\\n\\left(-9+5 x+44 x^{2}+12 x^{3}\\right) \\geq-2 \\\\\n\\left(-3-7 x+8 x^{2}+12 x^{3}\\right) \\geq-4 \\\\\n\\left(5+x-4 x^{2}+6 x^{3}\\right) \\geq \\frac{46}{9}\n\\end{gathered}\n$$\n\nThus, the numerator of (31) is lower bounded by\n\n$$\n\\frac{4}{3} y^{3}-2 y^{2}-8 y+\\frac{46}{9} \\geq 0\n$$\n\nThe inequality holds for $y \\leq \\frac{1}{2}$.\n(30) is minimized for $y=1-x$. This implies that $y_{u v}=1-x_{u w}-x_{v w}=1-x-y=0$ as in Case 1.\n\nCase 3: $y_{u v}=\\min \\left\\{y_{u w}, y_{v w}\\right\\}$. Assume that $y_{u v}=y_{u w} \\leq y_{v w}$. We will use the lower bound $f(x) \\geq \\frac{3}{2} x$ for $x \\geq \\frac{1}{3}$\n\n$$\n(26) \\geq \\frac{1}{2} y_{u w}+\\frac{1}{2} y_{v w}+2\\left(y_{u w}+y_{v w}\\right) y_{u w}-3 y_{u w} y_{v w}\\left(1-y_{u w}\\right)-3 y_{u w}^{2}\n$$\n\nThe coefficient of $y_{v w}$ in (32) is $\\frac{1}{2}-y_{u w}+3 y_{u w}^{2} \\geq 0$. Hence, (32) is minimized for $y_{v w}=y_{u w}$,\n\n$$\n(32) \\geq y_{u w}-2 y_{u w}^{2}+3 y_{u w}^{3} \\geq 0\n$$\n\nfor $y_{u w} \\geq 0$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# 6 Triangle Analysis used in 1.49-Approximation: Proof of Lemma 10 \n\nIn this section we will prove Lemma 10. The only type of triangles for which the budget functions $b_{1.5}^{+}$and $b_{1.5}^{-}$are tight (i.e., for which $\\Delta(T)=\\operatorname{cost}(T)$ ), are ++ triangles with value 0.5 for the + edges and value 1 for the -edge. If we start to decrease $\\alpha$ to be less than 1.5 , the inequality $\\Delta(T) \\geq \\operatorname{cost}(T)$ for budgets $b_{\\alpha}^{+}$ and $b_{\\alpha}^{-}$will be violated on tight triangles $T$. However, similar to [CLN22], we will charge these bad triangles (i.e., triangles that are close to the tight case) to other triangles that have some slack (i.e., triangles for which $\\Delta(T)-\\operatorname{cost}(T)>0)$. Therefore, using a charging argument, we can decrease the approximation ratio from 1.5 to $\\alpha=\\frac{70}{47} \\approx 1.49$. This motivates the following definitions used in [CLN22].\n\nDefinition 35 (Bad and chargeable triangles). Let $\\eta=1 / 12$,\n\n- $A++$ triangle $T=(u, v, w)$ is a bad triangle centered at $u$ if $x_{u v}, x_{u w} \\in[0.5-\\eta, 0.5+\\eta]$ and $x_{v w} \\geq 1-\\eta$.\n- A triangle $T=(u, v, w)$ is a chargeable triangle centered at $u$ if it has two + edges with value $x_{u v}, x_{u w} \\in$ $[0.5-\\eta, 0.5+\\eta]$ and a third edge with value $x_{v w} \\leq 1-\\eta$. A chargeable triangle is either a ++ triangle or a +++ triangle.\n- Note that chargeable ++ triangles and bad triangles are only centered at one vertex, while a chargeable +++ triangle can be centered at either one or all three vertices.\n- $A+$ edge $u v$ is a chargeable degenerate triangle if $x_{u v} \\in[0.5-\\eta, 0.5+\\eta]$. We say that the degenerate triangle $u v$ is centered at both $u$ and $v$.\n\nThe following lemma says that the number of chargeable triangles is close to the number of bad triangles. This enables us to charge the loss we incur on bad triangles (i.e., the value $\\Delta(T)-\\operatorname{cost}(T)<0$ ) to chargeable triangles with value $\\Delta(T)-\\operatorname{cost}(T)>0$.\n\nLemma 36 ([CLN22]). For any vertex $v$, the number of bad triangles centered at $v$ is at most the number of chargeable triangles (degenerate and non-degenerate) centered at $v$.\n\nStarting with triangles that are neither bad nor chargeable, we can show that for these triangles, $\\Delta(T)$ remains larger than or equal to $\\operatorname{cost}(T)$ when lowering $\\alpha$.\n\nLemma 37. Fix budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-} \\equiv b_{\\alpha}^{-}$, where $\\alpha=\\frac{70}{47}$. For any triangle $T$ that is neither chargeable nor bad, we have $\\operatorname{cost}(T) \\geq \\Delta(T)$.\n\nFor bad triangles this is not the case, however, we can give a lower bound which is negative.\nLemma 38. Fix budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-} \\equiv b_{\\alpha}^{-}$, where $\\alpha=\\frac{70}{47}$. For any bad triangle $T$, we have $\\Delta(T)-\\operatorname{cost}(T) \\geq-\\frac{1}{36}$.\n\nNext, we have to deal with chargeable triangles.\nLemma 39. Fix budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-} \\equiv b_{\\alpha}^{-}$, where $\\alpha=\\frac{70}{47}$.\n\n- For any chargeable +++ triangle $T$, we have $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{12}$.\n- For any chargeable ++- triangle $T$, we have $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{36}$.\n- For any chargeable degenerate triangle $T$, we have $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{18}$.\n\nBefore proving the statements above, we show how to combine them to prove Lemma 10.\nProof of Lemma 10. We have to show the following inequality.\n\n$$\n(4)=\\sum_{u v w \\in\\binom{V^{\\prime}}{3}} \\operatorname{cost}(u, v, w)+\\sum_{u v \\in\\binom{V^{\\prime}}{2}} \\operatorname{cost}(u, v) \\leq \\sum_{u v w \\in\\binom{V^{\\prime}}{3}} \\Delta(u, v, w)+\\sum_{u v \\in\\binom{V^{\\prime}}{2}} \\Delta(u, v)\n$$\n\nLet $C^{+++} \\subseteq\\binom{V^{\\prime}}{3}, C^{++-} \\subseteq\\binom{V^{\\prime}}{3}$ and $C^{+} \\subseteq\\binom{V^{\\prime}}{2}$ be the set of chargeable +++ , chargeable ++ - and chargeable degenerate triangles, respectively. Define $C:=C^{+++} \\cup C^{++-} \\cup C^{+}$to be the set of chargeable triangles and define $B \\subseteq\\binom{V^{\\prime}}{3}$ to be the set of bad triangles. For all triangles $T$ that are neither bad nor chargeable, we have by Lemma 37 that $\\Delta(T) \\geq \\operatorname{cost}(T)$. Thus,\n\n$$\n\\sum_{T \\in\\binom{V^{\\prime}}{3}}(\\Delta(T)-\\operatorname{cost}(T))+\\sum_{u v \\in\\binom{V^{\\prime}}{2}}(\\Delta(u, v)-\\operatorname{cost}(u, v)) \\geq \\sum_{T \\in B}(\\Delta(T)-\\operatorname{cost}(T))+\\sum_{T \\in C}(\\Delta(T)-\\operatorname{cost}(T))\n$$\n\nBy Lemma 38 and Lemma 39,\n\n$$\n\\begin{aligned}\n& \\sum_{T \\in B}(\\Delta(T)-\\operatorname{cost}(T))+\\sum_{T \\in C}(\\Delta(T)-\\operatorname{cost}(T)) \\\\\n\\geq & \\sum_{T \\in B}-\\frac{1}{36}+\\sum_{T \\in C^{+++}} \\frac{1}{12}+\\sum_{T \\in C^{++-}} \\frac{1}{36}+\\sum_{T \\in C^{+}} \\frac{1}{18}\n\\end{aligned}\n$$\n\nFor a fixed vertex $v$, let $B_{v}, C_{v}^{+++}, C_{v}^{++-}, C_{v}^{+}$be the bad triangles, chargeable +++ triangles, chargeable ++ - triangles and chargeable degenerate triangles centered at $v$, respectively. Remember that bad triangles and chargeable ++ - triangles are centered at one vertex. Moreover, degenerate triangles are centered at their endpoints and +++ triangles can be centered at each of their three vertices.\n\n$$\n\\begin{aligned}\n& \\sum_{T \\in B}-\\frac{1}{36}+\\sum_{T \\in C^{+++}} \\frac{1}{12}+\\sum_{T \\in C^{++-}} \\frac{1}{36}+\\sum_{T \\in C^{+}} \\frac{1}{18} \\\\\n\\geq & \\sum_{v \\in V^{\\prime}} \\sum_{T \\in B(v)}-\\frac{1}{36}+\\frac{1}{3} \\sum_{T \\in C_{v}^{+++}} \\frac{1}{12}+\\sum_{T \\in C_{v}^{++-}} \\frac{1}{36}+\\frac{1}{2} \\sum_{T \\in C_{v}^{+}} \\frac{1}{18} \\geq 0\n\\end{aligned}\n$$\n\nThe last inequality holds by Lemma 36 .\nIn order to prove Lemmas 37, 38 and 39 we will again consider each type of triangle individually. The case of $---$ triangles, $+--$ triangles and (non-chargeable) degenerate triangles are already handled in Section 5. In particular, Lemma 27, Lemma 28 and Lemma 25 show that in these cases $\\Delta(T) \\geq \\operatorname{cost}(T)$ as long as $\\alpha \\geq \\frac{4}{3}$. Hence, we only need to improve the analysis for +++ , ++ - triangles and chargeable degenerate triangles. For $\\alpha=\\frac{70}{37}$, the budget functions are equal to,\n\n$$\nb^{+}(x):=\\frac{70}{12} \\frac{x^{2}}{1+x}, \\quad \\text { and } \\quad b^{-}(x):=\\frac{35}{12} \\frac{(1+2 x)(1-x)}{(1+x)}, \\quad \\forall x \\in[0,1]\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "# Degenerate triangles \n\nLemma 40. Fix the budget function $b^{+} \\equiv b_{\\alpha}^{+}$, where $\\alpha=\\frac{70}{47}$. For any chargeable degenerate triangle $T$, we have that $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{18}$.\n\nProof. Let $T=(u, v)$ be a chargeable degenerate triangle. By definition, $u v$ is a +edge and we have that $x_{u v} \\geq 0.5-\\eta$. Thus,\n\n$$\n\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{175}{102} x-x \\geq \\frac{365}{1224} \\geq \\frac{1}{18}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 19,
      "text": "# 6.1 +++ triangles\n## (s,s,s) $\\operatorname{cost}(T)=0$ and $\\Delta(T) \\geq 0$.\n## (s,s,l)\n\nLemma 41. Fix the budget function $b^{+} \\equiv b_{\\alpha}^{+}$, where $\\alpha=\\frac{70}{47}$. For any +++ triangle $T$ with one long + edge, we have $\\operatorname{cost}(T) \\leq \\Delta(T)$.\n\nProof. For a +++ triangle $T=(u, v, w)$, assume that $u v$ is long and $u w, v w$ are short:\n\n$$\n\\begin{aligned}\n& \\operatorname{cost}(T)=2 \\cdot x_{u v} \\\\\n& \\quad \\Delta(T)=b^{+}\\left(x_{u v}\\right)+b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right) \\geq b^{+}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u w}+x_{v w}}{2}\\right) \\geq b^{+}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u v}}{2}\\right)\n\\end{aligned}\n$$\n\nHere, we used Lemma 24 together with the triangle inequality, $x_{u v} \\leq x_{u w}+x_{v w}$. For $x_{u v} \\geq 1 / 3$, we have that,\n\n$$\nb^{+}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u v}}{2}\\right) \\geq\\left(\\frac{70}{12} \\cdot \\frac{x_{u v}}{1+x_{u v}}+\\frac{70}{12} \\cdot \\frac{\\frac{x_{u v}}{2}}{1+\\frac{x_{u v}}{2}}\\right) x_{u v} \\geq \\frac{55}{24} x_{u v} \\geq 2 x_{u v}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "## (s,l,l)\n\nLemma 42. Fix the budget function $b^{+} \\equiv b_{\\alpha}^{+}$, where $\\alpha=\\frac{70}{47}$. For any +++ triangle $T$ with two long + edges, we have $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{12}$.\n\nProof. For a +++ triangle $T=(u, v, w)$, assume that $u v$ is short and $u w$ and $v w$ are long:\n\n$$\n\\begin{aligned}\n\\operatorname{cost}(T) & =x_{u w}+x_{v w}+y_{v w}+y_{u w}-2 \\cdot y_{u v w}=2-2 \\cdot y_{u v w} \\\\\n\\Delta(T) & =b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right)+b^{+}\\left(x_{u v}\\right) \\cdot\\left(y_{v w}+y_{u w}-y_{u v w}\\right) \\\\\n\\Delta(T)-\\operatorname{cost}(T) & =b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right)+b^{+}\\left(x_{u v}\\right) \\cdot\\left(y_{v w}+y_{u w}-y_{u v w}\\right)-2+2 \\cdot y_{u v w}\n\\end{aligned}\n$$\n\nThe derivative of (33) w.r.t. $y_{u v w}$ is\n\n$$\n2-b^{+}\\left(x_{u v}\\right) \\geq 2-\\frac{35}{12} \\cdot \\frac{1}{6} \\geq 0\n$$\n\nFor the first inequality we used that $x_{u v} \\leq \\frac{1}{3}$. Hence, (33) is increasing with $y_{u v w}$. Since $y_{u v w} \\geq$ $\\frac{y_{u v}+y_{u w}+y_{v w}-1}{2}$, (33) is minimized for $y_{u v w}=\\max \\left\\{\\frac{y_{u v}+y_{u w}+y_{v w}-1}{2}, 0\\right\\}$.\n\nCase 1: $y_{u v}+y_{u w}+y_{v w}-1 \\geq 0$ and $y_{u v w}=\\frac{y_{u v}+y_{u w}+y_{v w}-1}{2}$.\n\n$$\n\\begin{aligned}\n(33) & \\geq b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right)+b^{+}\\left(x_{u v}\\right) \\cdot\\left(y_{v w}+y_{u w}-\\frac{y_{u v}+y_{u w}+y_{v w}-1}{2}\\right)-2+y_{u v}+y_{u w}+y_{v w}-1 \\\\\n& \\geq \\frac{35}{24} \\cdot x_{u w}+\\frac{35}{24} \\cdot x_{v w}+b^{+}\\left(x_{u v}\\right) \\cdot\\left(1-\\frac{x_{v w}+x_{u w}-x_{u v}}{2}\\right)-\\left(x_{u v}+x_{u w}+x_{v w}\\right)\n\\end{aligned}\n$$\n\nFor the last inequality we used that for $x \\geq 1 / 3$,\n\n$$\nb^{+}(x) \\geq \\frac{35}{24} \\cdot x_{u w}\n$$\n\nThe derivative of (35) w.r.t. $x_{u w}$ is\n\n$$\n\\frac{35}{24}-\\frac{1}{2} \\cdot b^{+}\\left(x_{u v}\\right)-1 \\geq \\frac{35}{24}-\\frac{1}{2} \\cdot \\frac{35}{72}-1 \\geq 0\n$$\n\nFor the first inequality we used that $x_{u v} \\leq \\frac{1}{3}$ and that $b^{+}(x)$ is increasing with $x$. Thus, (35) is increasing with $x_{u w}$. By symmetry, the same is true for $x_{v w}$. Therefore, (35) is minimized for $x_{u w}=x_{v w}=\\frac{1}{3}$.\n\n$$\n(35) \\geq \\frac{35}{36}+b^{+}\\left(x_{u v}\\right) \\cdot\\left(1-\\frac{1}{2}+\\frac{x_{u v}}{2}\\right)-\\left(\\frac{2}{3}+x_{u v}\\right)=\\frac{1}{16}\\left(105 x_{u v}^{2}-36 x_{u v}+11\\right) \\geq \\frac{277}{560} \\geq \\frac{1}{12}\n$$\n\nCase 2: $y_{u v w}=0$.\n\n$$\n(33)=b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right)+b^{+}\\left(x_{u v}\\right) \\cdot\\left(y_{v w}+y_{u w}\\right)-2\n$$\n\n(36) is increasing with $x_{u v}$. Moreover, $x_{u v}$ is lower bounded by $2-\\left(x_{u w}+x_{v w}\\right)$ since $1 \\geq y_{u v}+y_{u w}+y_{v w}-$ $2 y_{u v w}=y_{u v}+y_{u w}+y_{v w}$. Hence,\n\n$$\n\\begin{aligned}\n(36) & \\geq b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right)+b^{+}\\left(2-\\left(x_{u w}+x_{v w}\\right)\\right) \\cdot\\left(y_{v w}+y_{u w}\\right)-2 \\\\\n& \\geq 2 b^{+}\\left(\\frac{x_{u w}+x_{v w}}{2}\\right)+b^{+}\\left(2-\\left(x_{u w}+x_{v w}\\right)\\right) \\cdot\\left(2-\\left(x_{v w}+x_{u w}\\right)\\right)-2\n\\end{aligned}\n$$\n\nFor the last inequality we used Lemma 24. Substituting $z=x_{u w}+x_{v w}$,\n\n$$\n\\begin{aligned}\n(36) & \\geq 2 b^{+}\\left(\\frac{z}{2}\\right)+b^{+}(2-z) \\cdot(2-z)-2 \\\\\n& =\\frac{35 z^{4}-105 z^{3}-117 z^{2}+572 z-488}{6(z-3)(z+2)} \\geq \\frac{1}{12}\n\\end{aligned}\n$$\n\nThe inequality holds for $2 \\geq z=x_{u w}+x_{v w} \\geq 2-x_{u v} \\geq 2-\\frac{1}{3}=\\frac{5}{3}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "# (1,1,1) \n\nLemma 43. Fix the budget function $b^{+} \\equiv b_{\\alpha}^{+}$, where $\\alpha \\geq \\frac{34}{17} \\approx 1.41$. For any +++ triangles $T$ with three long +edges, we have $\\operatorname{cost}(T) \\leq \\Delta(T)$.\n\nProof. For $\\alpha=\\frac{24}{17}$ the budget function $b^{+}(x)$ is equal to $4.8 \\cdot \\frac{x^{2}}{1+x}$. Let $x=x_{u v}, y=x_{u w}$ and $z=x_{v w}$. We use the following notation: $a=y_{u v \\mid w}, b=y_{u w \\mid v}, c=y_{v w \\mid u}, p=y_{u v w}$ and $q=y_{a \\mid v \\mid w}$. Notice that $a+b+c+p+q=1$. We assume that $x \\leq y \\leq z$. Our goal is to show that $\\operatorname{cost}(T) \\leq \\Delta(T)$. For a +++ triangle $T=(u, v, w)$, assume that $x=u v, y=u w$ and $z=v w$ and $1 / 3<x \\leq y \\leq z$.\n$\\operatorname{cost}(T)=y_{u v}+y_{u w}-2 \\cdot y_{u v w}+y_{u v}+y_{v w}-2 \\cdot y_{u v w}+y_{u w}+y_{v w}-2 \\cdot y_{u v w}=2 \\cdot\\left(y_{u v}+y_{u w}+y_{v w}\\right)-6 y_{u v w}$\n\n$$\n\\Delta(T)=b^{+}\\left(x_{v w}\\right) \\cdot\\left(y_{u v}+y_{u w}-y_{u v w}\\right)+b^{+}\\left(x_{u w}\\right) \\cdot\\left(y_{u v}+y_{v w}-y_{u v w}\\right)+b^{+}\\left(x_{u v}\\right) \\cdot\\left(y_{u v}+y_{u w}-y_{u v w}\\right)\n$$\n\nSince $1 / 3<x=1-a-p$, we have the constraint that $a+p<2 / 3$. Our goal is to show\n\n$$\n\\frac{4.8 \\cdot \\operatorname{cost}(T)}{\\Delta(T)}=\\frac{2(1-p-q)}{\\frac{(1-a-p)^{2}(1-a-q)}{(2-a-p)}+\\frac{(1-b-p)^{2}(1-b-q)}{(2-b-p)}+\\frac{(1-c-p)^{2}(1-c-q)}{(2-c-p)}} \\leq 4.8\n$$\n\nWe can fix $p+q$, which means that $a+b+c=1-p-q$. We want to argue that the denominator of the LHS of (38) is minimized when $r:=a=b=c=(1-p-q) / 3$.\n\nClaim 44. For fixed $p \\geq 0$ and fixed $q$, the function $f(a):=\\frac{(1-a-p)^{2}(1-a-q)}{(2-a-p)}$ is convex when $a+p<2 / 3$, $a, p, q \\geq 0$ and $a+p+q \\leq 1$.\n\nProof. We want to show that the second derivative is positive on the specified ranges for $a, p$ and $q$.\n\n$$\nf^{\\prime \\prime}(a)=\\frac{2\\left(a^{3}+3 a^{2}(p-2)+3 a(p-2)^{2}+p^{3}-6 p^{2}+11 p+q-7\\right)}{(a+p-2)^{3}}\n$$\n\nSince $a+p<2 / 3$, the denominator is always negative. So we need to show that the numerator is also negative.\n\n$$\n\\begin{aligned}\n& 2\\left(a^{3}+3 a^{2}(p-2)+3 a(p-2)^{2}+p^{3}-6 p^{2}+11 p+q-7\\right)<0 \\\\\n& \\Longleftrightarrow a^{3}+3 a^{2}(p-2)+3 a(p-2)^{2}+p^{3}-6 p^{2}+11 p+q<7\n\\end{aligned}\n$$\n\nWe can replace $q=1-a-p$ since this is the maximum value for $q$, so we have\n\n$$\na^{3}+3 a^{2}(p-2)+3 a(p-2)^{2}+p^{3}-6 p^{2}+10 p-a<6\n$$\n\nWe want to prove (39) when $a+p<2 / 3$ and $a, p \\geq 0$. Define\n\n$$\nh(a):=a^{3}+3 a^{2}(p-2)+3 a(p-2)^{2}+p^{3}-6 p^{2}+10 p-a\n$$\n\nWe have\n\n$$\nh^{\\prime}(a)=3 a^{2}+6 a p+3 p^{2}+11-12 a-12 p\n$$\n\nWe want to show that $h^{\\prime}(a)>0$ when $a+p<2 / 3$ and $a, p \\geq 0$. We have\n\n$$\n3 a^{2}+6 a p+3 p^{2}+11>8>12(a+p)\n$$\n\nso we conclude that $h^{\\prime}(a)>0$ on the desired domain. Therefore, we can maximize the value of $a$ in order to maximize $h(a)$. We set $a=2 / 3-p$. This gives\n\n$$\n\\begin{aligned}\n& (2 / 3-p)^{3}+3(2 / 3-p)^{2}(p-2)+3(2 / 3-p)(p-2)^{2}+p^{3}-6 p^{2}+10 p-(2 / 3-p) \\\\\n= & \\frac{29 p}{3}+3(2 / 3-p)^{2}(p-2)+(2-3 p)(p-2)^{2}-4 p^{2}-\\frac{10}{27} \\\\\n= & \\frac{134}{27}-p<6\n\\end{aligned}\n$$\n\nas desired. We conclude that the function $f$ is convex on the relevant range of $a$ and $p$.\nBy Claim 44, we know that $f(a)+f(b)+f(c) \\geq 3 \\cdot f\\left(\\frac{a+b+c}{3}\\right)$, so the denominator of the LHS of (38) is minimized when $a=b=c$, so set $r=a=b=c$. Then we have\n\n$$\n\\frac{\\operatorname{cost}(T)}{\\Delta(T)}=\\frac{2(1-p-q)}{f(a)+f(b)+f(c)} \\leq \\frac{2(1-p-q)}{3 \\cdot f\\left(\\frac{a+b+c}{3}\\right)} \\leq \\frac{2(1-p-q)}{3 \\frac{(1-r-p)^{2}(1-r-q)}{(2-r-p)}}\n$$\n\nSetting $r:=(1-p-q) / 3$, we obtain\n\n$$\n\\frac{\\operatorname{cost}(T)}{\\Delta(T)} \\leq \\frac{6(1-p-q)(5-2 p+q)}{(2-2 p+q)^{2}(2-2 q+p)}\n$$\n\nNotice that $a+p=r+p=(1-p-q) / 3+p<2 / 3$ implies $q<1$, since $p \\geq 0$.\nClaim 45. For fixed $0 \\leq p<2 / 3$ and for $0 \\leq q<1-p$, the function $\\ell(q):=\\frac{6(1-p-q)(5-2 p+q)}{(2-2 p+q)^{2}(2-2 q+p)}$ is maximized when $q=0$.\n\nProof. We show that the derivative of $\\ell$ is negative for the appropriate range of $q$.\n\n$$\n\\ell^{\\prime}(q)=-\\frac{6\\left(14 p^{3}-3 p^{2}(5 q+16)+18 p(2 q+1)+2(q-1)^{2}(q+8)\\right)}{(p-2 q+2)^{2}(-2 p+q+2)^{3}}\n$$\n\nNotice that the denominator is always positive for the appropriate range of $p$ and $q$. So we need to show the following holds for $0 \\leq p \\leq 2 / 3$ and $0 \\leq q<1$.\n\n$$\n\\ell_{2}(q):=14 p^{3}-3 p^{2}(5 q+16)+18 p(2 q+1)+2(q-1)^{2}(q+8)>0\n$$\n\nIf the following inequality holds, then we can conclude that $\\ell_{2}$ is minimized when $q$ is minimized. Taking the derivative with respect to $q$, we want to show\n\n$$\n\\ell_{2}^{\\prime}(q)=-15 p^{2}+36 p+6\\left(q^{2}+4 q-5\\right)<0\n$$\n\nSince $\\ell_{2}^{\\prime \\prime}(q)=12 q+24>0$, we can replace $q$ with its maximum value $1-p$. Then we have\n\n$$\n\\ell_{2}^{\\prime}(q) \\leq-15 p^{2}+36 p+6\\left((1-p)^{2}+4(1-p)-5\\right)=-9 p^{2} \\leq 0\n$$\n\nSo when $q<1$, Inequality (41) holds. Thus, we have that $\\ell_{2}$ is minimizes when $q$ is minimizes, which is when $q=0$. So we have\n\n$$\n\\ell_{2}(q) \\geq \\ell_{2}(0)=14 p^{3}-48 p^{2}+18 p+16\n$$\n\nTaking the first and second derivatives of this function with respect to $p$, we have $42 p^{2}-96 p+18$ and $84 p-96<0$ for $p \\in[0,1]$, so we conclude that the function is concave on this domain. Thus, the minimum values occur at the extreme values, which are $p=0$ and $p=2 / 3$. So we have $\\ell_{2}(q)>\\min \\{16,10.8\\}$. Thus, we conclude that the Inequality (40) holds.\n\nNow we can set $q=0$ and define\n\n$$\nh(p):=\\frac{6(1-p)(5-2 p)}{(2-2 p)^{2}(2+p)}=\\frac{3(5-2 p)}{(2-2 p)(2+p)}\n$$\n\nWe are looking for the maximum value of $h(p)$ for $0 \\leq p<2 / 3$. We have\n\n$$\nh^{\\prime}(p)=\\frac{-6 p^{2}+30 p+3}{2\\left(p^{2}+p-2\\right)^{2}} \\text { and } h^{\\prime \\prime}(p)=\\frac{6 p^{3}-45 p^{2}-9 p-33}{\\left(p^{2}+p-2\\right)^{3}}\n$$\n\nand $h^{\\prime}(p)>0$, when $p \\in[0,1]$. Thus, $h$ is an increasing function of $p$. Notice that if $q=0$, then $p<1 / 2$ (and $r=1 / 6$ ), because we have $p+r=p+(1-p) / 3<2 / 3$. Thus, we can conclude that the maximum value $h$ have and not violate any of the constraints is $h(1 / 2)=4.8$. This concludes the proof of Lemma 43.\n\nLemma 46. Given budget functions $b^{+} \\equiv b_{\\alpha}^{+}$where $\\alpha=\\frac{70}{47}$. For $a+++$ triangles $T$ with three long +edges where furthermore, two of the +edges have distance $x \\in[0.5-\\eta, 0.5+\\eta]$, we have $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{12}$.\n\nProof. By Lemma 43, we know that $\\operatorname{cost}(T) \\leq 4.8 \\cdot \\frac{12}{70} \\cdot \\Delta(T)$. Here, we use that Lemma 43 only requires a coefficient of 4.8 but we have a coefficient of $\\frac{70}{12}$ since the lemma provides a strictly bigger $\\alpha=\\frac{70}{47}>\\frac{24}{17}$. Thus,\n\n$$\n\\Delta(T)-\\operatorname{cost}(T) \\geq\\left(1-4.8 \\cdot \\frac{12}{70}\\right) \\Delta(T)=\\frac{31}{175} \\Delta(T)\n$$\n\nWe will conclude the prove by lower bounding $\\Delta(T)$. For $T=(u, v, w)$, let $x_{u v}, x_{u w} \\in[0.5-\\eta, 0.5+\\eta]$.\n\n$$\n\\begin{aligned}\n\\Delta(T) & \\geq b^{+}\\left(x_{u v}\\right) \\cdot\\left(y_{u v}+y_{v w}-y_{u v w}\\right)+b^{+}\\left(x_{u w}\\right) \\cdot\\left(y_{u w}+y_{v w}-y_{u v w}\\right) \\\\\n& \\geq b^{+}\\left(x_{u v}\\right) \\cdot\\left(y_{u w}+y_{v w}-y_{u v w}\\right)+b^{+}\\left(x_{u w}\\right) \\cdot\\left(y_{u v}+y_{v w}-y_{u v w}\\right) \\\\\n& \\geq b^{+}\\left(x_{u v}\\right) \\cdot y_{u w}+b^{+}\\left(x_{u w}\\right) \\cdot y_{u v} \\\\\n& \\geq 2 \\cdot b^{+}(0.5-\\eta) \\cdot(0.5-\\eta)=\\frac{4375}{7344}\n\\end{aligned}\n$$\n\nWe have that $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{31}{175} \\cdot \\frac{4375}{7344}=\\frac{775}{7344} \\geq \\frac{1}{12}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 22,
      "text": "# $6.2++$ triangles \n\nObserve that $++$ - triangles have to contain two long + edges to be bad or chargeable. Hence, for ++ triangles that contain at most one long + edge, it is enough to prove that $\\Delta(T) \\geq \\operatorname{cost}(T)$.\n\nLemma 47. Fix budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-} \\equiv b_{\\alpha}^{-}$, where $\\alpha=\\frac{70}{47}$. For any $++$ triangles $T$ with at most one long + edge, we have $\\operatorname{cost}(T) \\leq \\Delta(T)$.\n\nProof. We distinguish two cases, the triangle contains two short + edges or one short + edge.\n(s,s) For a ++ triangle $T=(u, v, w)$, assume that $u v$ is a -edge and $u w, v w$ are short + edges:\n\n$$\n\\begin{aligned}\n\\operatorname{cost}(T) & =2 x_{u v}+1 \\\\\n\\Delta(T) & =b^{-}\\left(x_{u v}\\right)+b^{+}\\left(x_{u w}\\right)+b^{+}\\left(x_{v w}\\right) \\geq b^{-}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u w}+x_{v w}}{2}\\right) \\geq b^{-}\\left(x_{u v}\\right)+2 \\cdot b^{+}\\left(\\frac{x_{u v}}{2}\\right)\n\\end{aligned}\n$$\n\nAs above, we used that $b^{+}(x)$ is convex and increasing together with the triangle inequality, $x_{u v} \\leq x_{u w}+x_{v w}$. The derivative of (43) w.r.t. $x_{u v}$ is less than 0 for $x_{u v} \\geq 0$,\n\n$$\n\\frac{70}{12} \\cdot \\frac{x_{u v}\\left(4+3 x_{u v}\\right)}{\\left(1+x_{u v}\\right)^{2}\\left(2+x_{u v}\\right)^{2}} \\leq 0\n$$\n\nHence, (43) is decreasing with $x_{u v}$. Observe that $x_{u v} \\leq x_{u w}+x_{v w} \\leq \\frac{2}{3}$. Since the cost is increasing with $x_{u v}$ we only need to check the case $x_{u v}=\\frac{2}{3}$.\n\n$$\n\\Delta(T)-\\operatorname{cost}(T) \\geq b^{-}\\left(\\frac{2}{3}\\right)+2 \\cdot b^{+}\\left(\\frac{1}{3}\\right)-\\frac{7}{3}=0\n$$\n\n(s,l) For a ++ triangle $T=(u, v, w)$, assume that $u v$ is a -edge and $u w$ is a short + edge and $v w$ is a long + edge:\n\n$$\n\\begin{aligned}\n\\operatorname{cost}(T) & =x_{u v}+y_{v w}+y_{v w}+y_{u v}-2 y_{v w} y_{u v}=1+2 y_{w v}\\left(1-y_{u v}\\right)=1+2\\left(1-x_{w v}\\right) x_{u v} \\\\\n\\Delta(T) & =b^{+}\\left(x_{v w}\\right)+b^{-}\\left(x_{u v}\\right)+b^{+}\\left(x_{u w}\\right)\\left(y_{v w}+y_{u v}-y_{v w} y_{u v}\\right) \\\\\n& =b^{+}\\left(x_{v w}\\right)+b^{-}\\left(x_{u v}\\right)+b^{+}\\left(x_{u w}\\right)\\left(2-x_{v w}-x_{u v}-\\left(1-x_{v w}\\right)\\left(1-x_{u v}\\right)\\right)\n\\end{aligned}\n$$\n\nCase $1 \\quad x_{v w} \\geq x_{u v}$.\n\n$$\n\\begin{aligned}\n\\Delta(T) & \\geq b^{+}\\left(x_{u v}\\right)+b^{-}\\left(x_{u v}\\right)=\\frac{35}{12} \\\\\n\\operatorname{cost}(T) & =1+2\\left(1-x_{v w}\\right) x_{u v} \\leq 1+\\frac{4}{3}=\\frac{7}{3} \\leq \\frac{35}{12}\n\\end{aligned}\n$$\n\nFor the second inequality, we used that $x_{v w} \\geq \\frac{1}{3}$.\nCase $2 \\quad x_{v w} \\leq x_{u v}$. By triangle inequality we have $x_{u w} \\geq x_{u v}-x_{v w}$. Remember that by Lemma $24 b^{+}(x)$ is increasing with $x$,\n\n$$\n\\Delta(T) \\geq b^{+}\\left(x_{v w}\\right)+b^{-}\\left(x_{u v}\\right)+b^{+}\\left(x_{u v}-x_{v w}\\right)\\left(2-x_{v w}-x_{u v}-\\left(1-x_{v w}\\right)\\left(1-x_{u v}\\right)\\right)\n$$\n\nFor a fixed $z:=x_{u v}$, define $x:=x_{v w} \\geq \\frac{1}{3}$. We need to argue that the difference\n\n$$\n\\begin{aligned}\n\\Delta(T)-\\operatorname{cost}(T) & =b^{+}(x)+b^{-}(z)+b^{+}(z-x)(2-x-z-(1-x)(1-z))-1-2(1-x) z \\\\\n& -1-2(1-x) z\n\\end{aligned}\n$$\n\nis non-negative. To this end, we want to prove that (45) is increasing with $x$. The derivative of (45) w.r.t. $x$ is\n\n$$\n\\begin{aligned}\n& \\frac{70 x^{5} z-x^{4} z(175 z-47)+2 x^{3} z\\left(70 z^{2}-117 z-70\\right)-x^{2} z\\left(35 z^{3}-257 z^{2}-81 z+129\\right)}{6(x+1)^{2}(x-z-1)^{2}} \\\\\n& +\\frac{-2 x\\left(35 z^{4}-47 z^{3}-82 z^{2}-35 z-70\\right)-z\\left(35 z^{3}+23 z^{2}+11 z+58\\right)}{6(x+1)^{2}(x-z-1)^{2}}\n\\end{aligned}\n$$\n\nWe will show that the numerator of (46) is non-negative,\n\n$$\n\\begin{gathered}\n70 x^{5} z-x^{4} z(175 z-47)+2 x^{3} z\\left(70 z^{2}-117 z-70\\right)-x^{2} z\\left(35 z^{3}-257 z^{2}-81 z+129\\right) \\\\\n-2 x\\left(35 z^{4}-47 z^{3}-82 z^{2}-35 z-70\\right)-z\\left(35 z^{3}+23 z^{2}+11 z+58\\right) \\geq 0\n\\end{gathered}\n$$\n\n(47) is concave in $x$. Indeed, consider the second derivative,\n\n$$\n2 z\\left(-129+700 x^{3}+81 z+257 z^{2}-35 z^{3}-6 x^{2}(-47+175 z)+6 x\\left(-70-117 z+70 z^{2}\\right)\\right)\n$$\n\n(48) is convex in $x$. The second derivative of (48) is equal to\n\n$$\n24 z(47+350 x-175 z)\n$$\n\n(49) is positive since either $x \\geq 2 / 3$ or $z \\leq x+\\frac{1}{3}$. Hence, to show that (48) is non-positive we need to check $x=z, x=\\frac{1}{3}$ and $x=z-\\frac{1}{3}$.\n\n$$\n\\begin{aligned}\nx & =z, \\quad(48)=2 z\\left(-129-339 z-163 z^{2}+35 z^{3}\\right) \\leq 0 \\\\\nx & =\\frac{1}{3}, \\quad(48)=-\\frac{2}{27} z\\left(5717+7281 z-10719 z^{2}+945 z^{3}\\right) \\leq 0 \\\\\nx & =z-\\frac{1}{3}, \\quad(48)=\\frac{2}{27} z\\left(443-4761 z-8181 z^{2}+945 z^{3}\\right) \\leq 0\n\\end{aligned}\n$$\n\nFor all cases we assumed that $\\frac{1}{3} \\leq z \\leq 1$. Since we now know that (47) is concave in $x$, we only need to check $x=z, x=\\frac{1}{3}$ and $x=z-\\frac{1}{3}$.\n\n$$\n\\begin{aligned}\nx=z, & (47)=z\\left(82+59 z+12 z^{2}\\right) \\geq 0 \\\\\nx=\\frac{1}{3}, & (47)=\\frac{1}{243}\\left(11340-12956 z+10167 z^{2}+10224 z^{3}-15120 z^{4}\\right) \\geq 0 \\\\\nx=z-\\frac{1}{3}, & (47)=\\frac{1}{243}\\left(-11340+12104 z+13737 z^{2}+10539 z^{3}+6615 z^{4}\\right) \\geq 0\n\\end{aligned}\n$$\n\nFor the first two cases, we assumed $0 \\leq z \\leq 1$. For the third case $2 / 3 \\leq z \\leq 1$. We can conclude that (45) is increasing with $x$. Hence, we can assume that either $x=\\frac{1}{3}$ or $x=z-\\frac{1}{3}$.\n\nCase 2.1 $x=\\frac{1}{3}$.\n\n$$\n\\begin{aligned}\n(45) & =\\frac{35}{72}+b^{-}(z)+b^{+}\\left(z-\\frac{1}{3}\\right)\\left(1-\\frac{5}{3} z\\right)-1-\\frac{4}{3} z \\\\\n& =\\frac{1458-221 z-443 z^{2}-1284 z^{3}-1260 z^{4}}{216(1+z)(2+3 z)} \\geq 0\n\\end{aligned}\n$$\n\nThe inequality is true since the numerator is non-negative for $z \\leq x+\\frac{1}{3}=\\frac{2}{3}$.\nCase 2.2 $x=z-1 / 3$.\n\n$$\n\\begin{aligned}\n(45) & =b^{+}\\left(z-\\frac{1}{3}\\right)+b^{-}(z)+\\frac{35}{72}\\left(\\frac{7}{3}-2 z-\\left(\\frac{4}{3}-z\\right)(1-z)\\right)-1-2\\left(\\frac{4}{3}-z\\right) z \\\\\n& =\\frac{1458-587 z-1754 z^{2}+12 z^{3}+981 z^{4}}{216(1+z)(2+3 z)} \\geq 0\n\\end{aligned}\n$$\n\nThe inequality holds true since the numerator is non-negative for $0 \\leq z \\leq 1$.\n(1,1) The only case that is left are ++ triangles with two long +edges. This case includes the bad triangles as well as chargeable triangles. For a ++ triangle $T=(a, b, c)$, assume that $b c$ is a -edge and $a b, a c$ are long + edges. Before we start to distinguish between bad triangles, chargeable triangles and triangles that are neither, we will prove three claims that will be used in each of the three cases. For - edges we use a coefficient of $\\frac{35}{12}, b^{-}(x):=\\frac{35}{12}(1-x) \\leq b_{\\alpha}^{-}(x)$. For + edges define the coefficient $f(x):=\\frac{70}{12} \\cdot \\frac{x}{1+x}, b^{+}(x)=f(x) \\cdot x$.\nClaim 48. For a ++ triangle $T=(a, b, c), \\Delta(T)-\\operatorname{cost}(T)$ is minimized for $y_{b c}=y_{a b c}$.\nProof.\n\n$$\n\\begin{aligned}\n\\Delta(T) & =\\frac{35}{12}\\left(y_{a b}+y_{a c}-y_{a b c}\\right) y_{b c}+f\\left(x_{a c}\\right)\\left(y_{a b}+y_{b c}-y_{a b} y_{b c}\\right)\\left(1-y_{a c}\\right)+f\\left(x_{a b}\\right)\\left(y_{a c}+y_{b c}-y_{a c} y_{b c}\\right)\\left(1-y_{a b}\\right) \\\\\n& =f\\left(x_{a c}\\right) y_{a b}+f\\left(x_{a b}\\right) y_{a c}+\\left(f\\left(x_{a b}\\right)+f\\left(x_{a, c}\\right)\\right) y_{b c}+\\left(\\frac{35}{12}-f\\left(x_{a b}\\right)-f\\left(x_{a c}\\right)\\right)\\left(y_{a b}+y_{a c}\\right) y_{b c} \\\\\n& -\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)\\right) y_{a b} y_{a c}\\left(1-y_{b c}\\right)-\\frac{35}{12} y_{a b c} y_{b c} . \\\\\n\\operatorname{cost}(T) & =y_{a b c}+y_{a b}+y_{b c}-2 y_{a b} y_{b c}+y_{a c}+y_{b c}-2 y_{a c} y_{b c}=y_{a b c}+y_{a b}+y_{a c}+2 y_{b c}-2\\left(y_{a b}+y_{a c}\\right) y_{b c}\n\\end{aligned}\n$$\n\nThe difference between the cost and the budget is at least,\n\n$$\n\\begin{aligned}\n\\Delta(T)-\\operatorname{cost}(T) & \\geq\\left(f\\left(x_{a c}\\right)-1\\right) y_{a b}+\\left(f\\left(x_{a b}\\right)-1\\right) y_{a c}+\\left(f\\left(x_{a b}\\right)+f\\left(x_{a, c}\\right)-2\\right) y_{b c}-\\frac{35}{12} y_{a b c} y_{b c}-y_{a b c} \\\\\n& +\\left(\\frac{35}{12}+2-f\\left(x_{a b}\\right)-f\\left(x_{a c}\\right)\\right)\\left(y_{a b}+y_{a c}\\right) y_{b c}-\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)\\right) y_{a b} y_{a c}\\left(1-y_{b c}\\right)\n\\end{aligned}\n$$\n\nWe can decrease $y_{b c}$ until $y_{b c}=y_{a b c}$. (50) is increasing with $y_{b c}$. Indeed, we will show that the derivative of (50) w.r.t. $y_{b c}$ is non-negative. The derivative of (50) w.r.t. $y_{b c}$ is equal to\n\n$$\n\\left(\\frac{35}{12}+2-f\\left(x_{a b}\\right)-f\\left(x_{a c}\\right)\\right)\\left(y_{a b}+y_{a c}\\right)+\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)-2\\right)+\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)\\right)\\left(y_{a b} y_{a c}\\right)-\\frac{35}{12} y_{a b c}\n$$\n\nObserve that $f(x) \\leq \\frac{35}{12}$ and $f(x) \\geq \\frac{35}{18}$ if $x \\geq 1 / 2$. Hence, if $x_{a b}, x_{a c} \\geq \\frac{1}{2}$,\n\n$$\n(51) \\geq\\left(2-\\frac{35}{12}\\right)\\left(y_{a b}+y_{a c}\\right)+\\left(\\frac{35}{9}-2\\right)+\\left(\\frac{35}{9}\\right)\\left(y_{a b} y_{a c}\\right)-\\frac{35}{12} y_{a b c}\n$$\n\n$$\n\\geq \\frac{35}{36}+\\left(\\frac{35}{9}\\right) y_{a b c}^{2}-\\frac{35}{12} y_{a b c} \\geq 0\n$$\n\nFor the second inequality we used that $y_{a b}+y_{a c} \\leq 1$ and $y_{a b c} \\leq y_{a b}$, yac. Note that $f(x) \\leq \\frac{35}{18}$ if $x \\leq \\frac{1}{2}$ and $f(x) \\geq \\frac{35}{24}$ if $x \\geq \\frac{1}{3}$. Hence, If $\\frac{1}{3} \\leq x_{a b} \\leq \\frac{1}{2}$ and $x_{a c} \\geq \\frac{1}{3}$, then,\n\n$$\n\\begin{aligned}\n& \\left(\\frac{35}{12}+2-\\frac{15}{9}\\right)\\left(y_{a b}+y_{a c}\\right)+\\left(\\frac{35}{12}-2\\right)+\\left(\\frac{35}{12}\\right)\\left(y_{a b} y_{a c}\\right)-\\frac{35}{12} y_{a b c} \\\\\n= & \\left(\\frac{13}{4}\\right)\\left(y_{a b}+y_{a c}\\right)+\\frac{11}{12}+\\left(\\frac{35}{12}\\right)\\left(y_{a b} y_{a c}\\right)-\\frac{35}{12} y_{a b c} \\\\\n\\geq & \\frac{11}{12}+\\frac{35}{12} y_{a b c}^{2}+\\frac{43}{12} y_{a b c} \\geq 0\n\\end{aligned}\n$$\n\nFor the second inequality we used that $y_{a b c} \\leq y_{a b}, y_{a c}$.\nSubstituting $y_{a b c}=y_{b c}$ into (50) yields,\n\n$$\n\\begin{aligned}\n\\Delta(T)-\\operatorname{cost}(T) \\geq(f\\left(x_{a c}\\right)-1) y_{a b}+ & (f\\left(x_{a b}\\right)-1) y_{a c}+\\left(2+\\frac{35}{12}-f\\left(x_{a b}\\right)-f\\left(x_{a c}\\right)\\right)\\left(y_{a b}+y_{a c}\\right) y_{b c} \\\\\n& +\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)-3\\right) y_{b c}-\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)\\right) y_{a b} y_{a c}\\left(1-y_{b c}\\right)-\\frac{35}{12} y_{b c}^{2}\n\\end{aligned}\n$$\n\nWe want to determine a lower bound for (52). For $y_{b c}$ we have the constraints $y_{b c}=y_{a b c} \\leq \\min \\left\\{y_{a b}, y_{a c}\\right\\}$ and $y_{b c} \\geq\\left(y_{a b}+y_{a c}-1\\right)_{+}$. (52) is a quadratic in $y_{b c}$ with leading coefficient $-\\frac{35}{12}$. Hence, (52) is minimized at the boundaries.\n\nClaim 49. For $a++$ triangle $T=(a, b, c)$ with $y_{b c}=\\min \\left\\{y_{a b}, y_{a c}\\right\\}$,\n\n$$\n\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{5}{6} y_{b c}-\\frac{11}{6} y_{b c}^{2}+\\frac{35}{12} y_{b c}^{3} \\geq 0\n$$\n\nProof. Assume that $y_{b c}=y_{a b} \\leq y_{a c}$. We will use the lower bound $f(x) \\geq \\frac{35}{24}$ for $x \\geq \\frac{1}{3}$,\n\n$$\n(52) \\geq\\left(\\frac{35}{24}-1\\right) y_{a b}+\\left(\\frac{35}{24}-1\\right) y_{a c}+2\\left(y_{a b}+y_{a c}\\right) y_{a b}+\\left(\\frac{35}{12}-3\\right) y_{a b}-\\frac{35}{12} y_{a b} y_{a c}\\left(1-y_{a b}\\right)-\\frac{35}{12} y_{a b}^{2}\n$$\n\nThe coefficient of $y_{a c}$ in (53) is\n\n$$\n\\frac{11}{24}-\\frac{11}{12} y_{a b}+\\frac{35}{12} y_{a b}^{2} \\geq 0\n$$\n\nHence, (53) is minimized for $y_{a c}=y_{a b}$,\n\n$$\n(53) \\geq \\frac{5}{6} y_{a b}-\\frac{11}{6} y_{a b}^{2}+\\frac{35}{12} y_{a b}^{3} \\geq 0\n$$\n\nClaim 50. Let $L \\in[0,1]$. For $a++$ triangle $T=(a, b, c)$ with $y_{b c}=\\left(y_{a b}+y_{a c}-1\\right) \\geq L, \\Delta(T)-\\operatorname{cost}(T)$ is minimized for $y_{b c}=L$.\n\nProof. Set $x:=x_{a b}$ and $z:=x_{a c}$. In this case,\n\n$$\n\\begin{aligned}\n(52)= & (f(z)-1)(1-x)+(f(x)-1)(1-z)+\\left(2+\\frac{35}{12}-f(x)-f(z)\\right)(2-x-z)(1-x-z) \\\\\n& +(f(x)+f(z)-3)(1-x-z)-(f(x)+f(z))(1-x)(1-z)(x+z)-\\frac{35}{12}(1-x-z)^{2}\n\\end{aligned}\n$$\n\nThis simplifies to\n\n$$\n\\begin{aligned}\n(52) & =-\\frac{-23+36 z-35 z^{2}-24 z^{3}+x\\left(36+47 z-83 z^{2}+46 z^{3}\\right)}{(12(1+x)(1+z))} \\\\\n& -\\frac{x^{2}\\left(-35-83 z+92 z^{2}+140 z^{3}\\right)+2 x^{3}\\left(-12+23 z+70 z^{2}\\right)}{(12(1+x)(1+z))}\n\\end{aligned}\n$$\n\nSince $x+z \\leq 1-L \\leq 1$, we can assume that $x \\leq \\frac{1}{2}$. We will show that (54) is decreasing with $x$. The derivative of (54) w.r.t. $x$ is equal to\n\n$$\n\\begin{gathered}\n-\\frac{59+11 z-48 z^{2}+70 z^{3}+4 x^{3}\\left(-12+23 z+70 z^{2}\\right)}{12(1+x)^{2}(1+z)} \\\\\n-\\frac{2 x\\left(-35-83 z+92 z^{2}+140 z^{3}\\right)+x^{2}\\left(-107+55 z+512 z^{2}+140 z^{3}\\right)}{12(1+x)^{2}(1+z)}\n\\end{gathered}\n$$\n\nWe will show that the numerator of (55) is non-negative.\n\n$$\n\\begin{gathered}\n59+11 z-48 z^{2}+70 z^{3} \\geq \\frac{1618}{27} \\\\\n-12+23 z+70 z^{2} \\geq \\frac{31}{9} \\\\\n-35-83 z+92 z^{2}+140 z^{3} \\geq-\\frac{1276}{27} \\\\\n-107+55 z+512 z^{2}+140 z^{3} \\geq-\\frac{718}{27}\n\\end{gathered}\n$$\n\nFor all inequalities we used that $\\frac{1}{3}$. The numerator of (55) is lower bounded by\n\n$$\n\\frac{2}{27}\\left(809-1276 x-359 x^{2}+186 x^{3}\\right) \\geq 0\n$$\n\nThe inequality holds for $0 \\leq x \\leq \\frac{1}{2}$. We can conclude that (54) is decreasing with $x$ and hence, minimized for $x=1-L-z$. This implies that $y_{b c}=1-x_{a b}-x_{a c}=L$.\n\nNow, we are ready to prove the lower bounds for bad $++$ - triangles, chargeable $++$ - triangles and $++$ - triangles that are neither. We will start with bad $++$ - triangles and since they are the worst case we will give a lower bound for all $++$ - triangles.\nLemma 51. Given budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-}(x) \\equiv \\frac{35}{12}(1-x) \\leq b_{\\alpha}^{-}(x)$, where $\\alpha=\\frac{70}{47}$. For any ++ triangles $T$ with two long +edges, we have $\\Delta(T)-\\operatorname{cost}(T) \\geq-\\frac{1}{36}$.\nProof. By Claim 48, we know that\n\n$$\n\\begin{aligned}\n\\Delta(T)-\\operatorname{cost}(T) \\geq(f\\left(x_{a c}\\right)-1) y_{a b}+ & \\left(f\\left(x_{a b}\\right)-1\\right) y_{a c}+\\left(2+\\frac{35}{12}-f\\left(x_{a b}\\right)-f\\left(x_{a c}\\right)\\right)\\left(y_{a b}+y_{a c}\\right) y_{b c} \\\\\n& +\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)-3\\right) y_{b c}-\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)\\right) y_{a b} y_{a c}\\left(1-y_{b c}\\right)-\\frac{35}{12} y_{b c}^{2}\n\\end{aligned}\n$$\n\nAgain, this is a quadratic function with leading coefficient $-\\frac{35}{12}$ and thus minimized for either $y_{b c}=y_{a b c}=$ $\\min \\left\\{y_{a b}, y_{a c}\\right\\}$ or $y_{b c}=\\left(y_{a b}+y_{a c}-1\\right)_{+}$. If $y_{b c}=y_{a b c}=\\min \\left\\{y_{a b}, y_{a c}\\right\\}$, then by Claim 49, $\\Delta(T)-\\operatorname{cost}(T) \\geq 0$. By Claim 50, we know that the case $y_{b c}=\\left(y_{a b}+y_{a c}-1\\right) \\geq 0$ reduces to the case $y_{b c}=0$,\n\n$$\n(56) \\geq\\left(f\\left(x_{a c}\\right)-1\\right) y_{a b}+\\left(f\\left(x_{a b}\\right)-1\\right) y_{a c}-\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)\\right) y_{a b} y_{a c}\n$$\n\nIf $x \\geq \\frac{1}{2}$, then $f(x) \\geq \\frac{35}{18}$. Thus, for $x_{a b}, x_{a c} \\geq \\frac{1}{2}$,\n\n$$\n(57)=f\\left(x_{a c}\\right)\\left(y_{a b}-y_{a b} y_{a c}\\right)+f\\left(x_{a b}\\right)\\left(y_{a c}-y_{a b} y_{a c}\\right)-\\left(y_{a b}+y_{a c}\\right)\n$$\n\n$$\n\\begin{aligned}\n& \\geq \\frac{35}{18}\\left(y_{a b}-y_{a b} y_{a c}\\right)+\\frac{35}{18}\\left(y_{a c}-y_{a b} y_{a c}\\right)-\\left(y_{a b}+y_{a c}\\right) \\\\\n& =\\frac{17}{18} y_{a b}+\\frac{35}{18} y_{a c}-\\frac{35}{9} y_{a b} y_{a c} \\\\\n& \\geq \\frac{34}{9} y_{a b} y_{a c}-\\frac{35}{9} y_{a b} y_{a c} \\\\\n& =-\\frac{1}{9} y_{a b} y_{a c} \\\\\n& \\geq-\\frac{1}{36}\n\\end{aligned}\n$$\n\nFor the second inequality we used that $y_{a b}, y_{a c} \\leq \\frac{1}{2}$ and hence, $y_{a b} \\geq 2 y_{a b} y_{a c}$. For the last inequality, we used that $y_{a b}, y_{a c} \\leq \\frac{1}{2}$. Otherwise, assume w.l.o.g. that $z:=x_{a c} \\leq \\frac{1}{2} \\leq 1-z \\leq x_{a b}=x$.\nClaim 52. (57) is increasing with $x$ if $z \\leq \\frac{1}{2}+\\eta$.\nProof.\n\n$$\n(57)=\\frac{-12-6 z+41 z^{2}+x^{2}\\left(41+6 z-70 z^{2}\\right)+6 x\\left(-1+z^{2}\\right)}{(1+x)(1+z)}\n$$\n\nThe derivative of (58) w.r.t $x$ is equal to\n\n$$\n\\frac{6+6 z-35 z^{2}+x\\left(82+12 z-140 z^{2}\\right)+x^{2}\\left(41+6 z-70 z^{2}\\right)}{6(1+x)^{2}(1+z)}\n$$\n\nWe will show that the numerator of (59) is non-negative.\n\n$$\n\\begin{gathered}\n6+6 z-35 z^{2} \\geq-\\frac{347}{144} \\\\\n82+12 z-140 z^{2} \\geq \\frac{1489}{36} \\\\\n41+6 z-70 z^{2} \\geq \\frac{1}{2} \\cdot \\frac{1489}{36}\n\\end{gathered}\n$$\n\nFor all inequalities we used that $\\frac{1}{3} \\leq z \\leq 0.5+\\eta$. The numerator of (59) is lower bounded by\n\n$$\n\\frac{-347+5956 x+2978 x^{2}}{144} \\geq 0\n$$\n\nThe inequality holds for $x \\geq \\frac{1}{3}$.\nHence, (58) is minimized for $x=(1-z)$,\n\n$$\n(58) \\geq(f(1-x)-1)(1-x)+(f(x)-1) x-(f(x)+f(1-x)) x(1-x)\n$$\n\nThe derivative of (60) is\n\n$$\n\\frac{70}{12} \\cdot \\frac{\\left(-5+2 x+22 x^{2}-8 x^{3}-10 x^{4}+4 x^{5}\\right)}{(2-z)^{2}(1+x)^{2}} \\geq 0\n$$\n\nThe inequality holds since\n\n$$\n-5+2 x+22 x^{2}-8 x^{3}-10 x^{4}+4 x^{5} \\geq 0\n$$\n\nfor $x \\geq \\frac{1}{2}$. Thus, (58) is minimized for $x=\\frac{1}{2}$. We have that $x=x_{a b}, z=x_{a c}=\\frac{1}{2}$.\n\n$$\n(58)=(f(1 / 2)-1) \\frac{1}{2}+(f(1 / 2)-1) \\frac{1}{2}-(f(1 / 2)+f(1 / 2)) \\frac{1}{4}=-\\frac{1}{36}\n$$\n\nNext, we will deal with ++ - triangles that are chargeable. As soon as the distance of the -edge is bounded away from 1 , we can improve the lower bound.\n\nLemma 53. Given budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-}(x) \\equiv \\frac{35}{12}(1-x) \\leq b_{\\alpha}^{-}(x)$, where $\\alpha=\\frac{70}{47}$. For $a++-$ triangles $T$ with two long + edges and distance at most $1-\\eta$ for the - edge, we have $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{36}$.\n\nProof. By Claim 48, we know that\n\n$$\n\\begin{aligned}\n& \\Delta(T)-\\operatorname{cost}(T) \\geq\\left(f\\left(x_{a c}\\right)-1\\right) y_{a b}+\\left(f\\left(x_{a b}\\right)-1\\right) y_{a c}+\\left(2+\\frac{35}{12}-f\\left(x_{a b}\\right)-f\\left(x_{a c}\\right)\\right)\\left(y_{a b}+y_{a c}\\right) y_{b c} \\\\\n&+\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)-3\\right) y_{b c}-\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)\\right) y_{a b} y_{a c}\\left(1-y_{b c}\\right)-\\frac{35}{12} y_{b c}^{2}\n\\end{aligned}\n$$\n\nAs before, (61) is minimized at the boundaries, i.e. for either $y_{b c}=y_{a b c}=\\min \\left\\{y_{a b}, y_{a c}\\right\\}$ or $y_{b c}=\\max \\left\\{y_{a b}+\\right.$ $\\left.y_{a c}-1, \\eta\\right\\}$. If $y_{b c}=y_{a b c}=\\min \\left\\{y_{a b}, y_{a c}\\right\\}$, then by Claim 49,\n\n$$\n\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{5}{6} y_{b c}-\\frac{11}{6} y_{b c}^{2}+\\frac{35}{12} y_{b c}^{3} \\geq \\frac{1211}{20736} \\geq \\frac{1}{36}\n$$\n\nThe inequality holds since $y_{b c} \\geq \\eta$. By Claim 50 we know that the case $y_{b c}=\\left(y_{a b}+y_{a c}-1\\right) \\geq \\eta$ reduces to the case $y_{b c}=\\eta$,\n\n$$\n\\begin{gathered}\n(61) \\geq\\left(f\\left(x_{a c}\\right)-1\\right) y_{a b}+\\left(f\\left(x_{a b}\\right)-1\\right) y_{a c}+\\left(2+\\frac{35}{12}-f\\left(x_{a b}\\right)-f\\left(x_{a c}\\right)\\right)\\left(y_{a b}+y_{a c}\\right) \\eta \\\\\n+\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)-3\\right) \\eta-\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)\\right) y_{a b} y_{a c}(1-\\eta)-\\frac{35}{12} \\eta^{2} \\\\\n=(f(z)-1)(1-x)+(f(x)-1)(1-z)+\\left(2+\\frac{35}{12}-f(x)-f(z)\\right)(2-x-z) \\eta \\\\\n+(f(x)+f(z)-3) \\eta-(f(x)+f(z))(1-x)(1-z)(1-\\eta)-\\frac{35}{12} \\eta^{2}\n\\end{gathered}\n$$\n\nCase 1: $x, z \\geq 1 / 2+\\eta$. We have that $f(x), f(z) \\geq 2$,\n\n$$\n\\begin{gathered}\n(62) \\geq(1-x)+(1-z)+\\left(\\frac{35}{12}-2\\right)(2-x-z) \\eta+\\eta-4(1-x)(1-z)(1-\\eta)-\\frac{35}{12} \\eta^{2} \\\\\n\\geq \\eta-\\frac{35}{12} \\eta^{2}=\\frac{109}{1728} \\geq \\frac{1}{36}\n\\end{gathered}\n$$\n\nCase 2: $z \\leq 1 / 2+\\eta$.\nClaim 54. (62) is increasing for $x$ if $z \\leq 0.5+\\eta$.\nProof.\n\n$$\n(62)=\\frac{-\\left(2507+1487 z-11100 z^{2}+x\\left(1487+467 z-1860 z^{2}\\right)+60 x^{2}\\left(-185-31 z+308 z^{2}\\right)\\right)}{(1728(1+x)(1+z))}\n$$\n\nThe derivative of (63) w.r.t $x$ is equal to\n\n$$\n5 \\frac{17+17 z-154 z^{2}+x\\left(370+62 z-616 z^{2}\\right)+x^{2}\\left(185+31 z-308 z^{2}\\right)}{144(1+x)^{2}(1+z)}\n$$\n\nWe will lower bound the numerator of (64). Observe that\n\n$$\n17+17 z-154 z^{2} \\geq-\\frac{1835}{72}\n$$\n\n$$\n\\begin{aligned}\n370+62 z-616 z^{2} & \\geq \\frac{1769}{9} \\\\\n185+31 z-308 z^{2} & \\geq \\frac{1769}{18}\n\\end{aligned}\n$$\n\nWe used that $\\frac{1}{3} \\leq z \\leq 1 / 2+\\eta$ for all of the above inequalities. Hence, we have that the numerator of (64) is lower bounded by\n\n$$\n1 / 72\\left(-1835+14152 x+7076 x^{2}\\right) \\geq 0\n$$\n\nThe inequality is true for $x \\geq 1 / 3$.\nThus, we can assume that $x=1-\\eta-z$ since $x+z \\geq 1-\\eta$.\n\n$$\n\\begin{aligned}\n& (62) \\geq(f(1-\\eta-z)-1)(\\eta+z)+(f(1-\\eta-z)-1)(1-z)+\\left(2+\\frac{35}{12}-f(1-\\eta-z)-f(z)\\right)(1+\\eta) \\eta \\\\\n& \\quad+(f(1-\\eta-z)+f(z)-3) \\eta-(f(1-\\eta-z)+f(z))(\\eta+z)(1-z)(1-\\eta)-\\frac{35}{12} \\eta^{2} \\\\\n& =\\frac{-32742+115291 z-32602 z^{2}-203280 z^{3}+110880 z^{4}}{864(1+z)(-23+12 z)} \\geq \\frac{1343}{20736} \\geq \\frac{1}{36}\n\\end{aligned}\n$$\n\n(68) is minimized for $z=0.5-\\eta / 2$. (68) is convex and first derivative is 0 for $z=z=0.5-\\eta / 2$. Indeed, the derivative of (68) is\n\n$$\n\\frac{35(-11+24 z)\\left(7823+11132 z-9482 z^{2}-5808 z^{3}+3168 z^{4}\\right)}{864(1+z)^{2}(-23+12 z)^{2}}\n$$\n\nThe second derivative of (68) is equal to\n\n$$\n\\frac{385}{18}-\\frac{3815}{432(1+z)^{3}}+\\frac{15260}{(-23+12 z)^{3}} \\geq 0\n$$\n\nThe inequality holds for $0 \\leq z \\leq 1$.\nIf a ++ - triangle is neither chargeable nor bad, it is again enough to show that $\\Delta(T) \\geq \\operatorname{cost}(T)$. Thus, we complete the previous lemma with the following.\nLemma 55. Given budget functions $b^{+} \\equiv b_{\\alpha}^{+}$and $b^{-}(x) \\equiv \\frac{35}{12}(1-x) \\leq b_{\\alpha}^{-}(x)$, where $\\alpha=\\frac{70}{17}$. For a ++- triangles $T$ with two long + edges where one of the + edges has distance $x \\notin[0.5-\\eta, 0.5+\\eta]$, we have $\\Delta(T)-\\operatorname{cost}(T) \\geq 0$.\n\nProof. By Claim 48, we know that\n\n$$\n\\begin{aligned}\n\\Delta(T)-\\operatorname{cost}(T) \\geq\\left(f\\left(x_{a c}\\right)-1\\right) y_{a b}+ & \\left(f\\left(x_{a b}\\right)-1\\right) y_{a c}+\\left(2+\\frac{35}{12}-f\\left(x_{a b}\\right)-f\\left(x_{a c}\\right)\\right)\\left(y_{a b}+y_{a c}\\right) y_{b c} \\\\\n& +\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)-3\\right) y_{b c}-\\left(f\\left(x_{a b}\\right)+f\\left(x_{a c}\\right)\\right) y_{a b} y_{a c}\\left(1-y_{b c}\\right)-\\frac{35}{12} y_{b c}^{2}\n\\end{aligned}\n$$\n\nAgain, this is a quadratic function with leading coefficient $-\\frac{35}{12}$ and thus minimized for either $y_{b c}=y_{a b c}=$ $\\min \\left\\{y_{a b}, y_{a c}\\right\\}$ or $y_{b c}=\\left(y_{a b}+y_{a c}-1\\right)_{+}$. If $y_{b c}=y_{a b c}=\\min \\left\\{y_{a b}, y_{a c}\\right\\}$, then by Claim $49, \\Delta(T)-\\operatorname{cost}(T) \\geq 0$. By Claim 50 we know that the case $y_{b c}=\\left(y_{a b}+y_{a c}-1\\right) \\geq 0$ reduces to the case $y_{b c}=0$. Set $x=x_{a b}$ and $z=x_{a c}$.\n\n$$\n(69)=(f(z)-1)(1-x)+(f(x)-1)(1-z)-(f(x)+f(z))(1-x)(1-z)\n$$\n\nSince $x+z \\geq 1$ we have either, $x \\geq 0.5+\\eta$ or $z \\geq 0.5+\\eta$. Assume that $x \\geq 0.5+\\eta$. We distinguish the following two cases.\n\nCase 1: $x, z \\geq \\frac{1}{2}+\\eta$. For $x \\geq \\frac{1}{2}+\\eta, f(x) \\geq 2$. Thus,\n$(f(z)-1)(1-x)+(f(x)-1)(1-z)-(f(x)+f(z))(1-x)(1-z) \\geq(1-x)+(1-z)-4(1-x)(1-z) \\geq 0$.\nFor the last inequality we used that $(1-z),(1-x) \\leq \\frac{1}{2}$.\nCase 2: $x \\geq 0.5+\\eta$ and $z \\leq 0.5+\\eta$. By Claim 52 we know that (70) is increasing with $x$. Hence, we have either $x=0.5+\\eta$ and $z \\geq 0.5-\\eta$ or $x=1-z$ and $z \\leq 0.5-\\eta$. For $x=1-z$,\n\n$$\n\\begin{gathered}\n(70) \\geq(f(z)-1) z+(f(1-z)-1)(1-z)-(f(1-z)+f(z)) z(1-z) \\\\\n=\\frac{23-76 z+6 z^{2}+140 z^{3}-70 z^{4}}{12+6 z-6 z^{2}} \\geq 0\n\\end{gathered}\n$$\n\nThe inequality holds since both the numerator and the denominator are non-negative for $z \\leq \\frac{1}{2}-\\eta$. For $x=0.5+\\eta$,\n\n$$\n(70) \\geq \\frac{-223-570 z+2978 z^{2}}{1368(1+z)} \\geq 0\n$$\n\nThe numerator is non-negative for $z \\geq \\frac{1}{2}-\\eta$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 23,
      "text": "# 6.3 Wrapping up: Deferred proofs of Lemma 37, 38, 39 \n\nProof of Lemma 37. We have to show that $\\Delta(T) \\geq \\operatorname{cost}(T)$ for all triangles $T$ that are neither chargeable nor bad. This is true for --- triangles, +-- triangles and degenerate triangles by Lemma 27, Lemma 28 and Lemma 25. Let $T$ be a +-- triangles where $u v$, uw are + edges and $v w$ is a -edge. If $T$ contains at most one long + edge, then the inequality holds by Lemma 47. Otherwise, assume that both + edges are long. $T$ is neither bad nor chargeable if and only if $x_{v w} \\leq 1-\\eta$ or at least one of $x_{u v}, x_{u w}$ is not in $[0.5-\\eta, 0.5+\\eta]$. In the first case $\\Delta(T) \\geq \\operatorname{cost}(T)$ by Lemma 53 and in the second case by Lemma 55. For +++ triangles the statement holds true by Lemma 41 for triangles with one long + edge, by Lemma 42 for triangles with two long + edges and by Lemma 43 for triangles with three long + edges. The cost is 0 in the case where all + edges are short.\n\nProof of Lemma 38. A bad triangle $T$ is always a ++- triangle with two long + edges. Thus, the bound follows from Lemma 51 .\n\nProof of Lemma 39. Consider a chargeable ++- triangle $T$. Both + edges of $T$ are long. Moreover, the -edge has value $x \\leq 1-\\eta$. Thus, $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{36}$ by Lemma 53. A chargeable +++ triangle $T$ has either two long + edges or three long + edges. In the first case we have $\\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{12}$ by Lemma 42 and in the second case by Lemma 46. For chargeable degenerate triangles $T, \\Delta(T)-\\operatorname{cost}(T) \\geq \\frac{1}{18}$ by Lemma 40 .",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 24,
      "text": "## 7 1.437-Approximation: Computer-assisted proof\n\nIn this section, by introducing and solving the factor-revealing $S D P$, we prove Lemma 11, which implies that our algorithm archives a 1.437-approximation. Let $\\mathcal{T}$ be defined as the set of all possible triangles, represented by tuples of values $\\left(y_{u v}, y_{u w}, y_{v w}, y_{u v w}\\right)$ satisfying the condition $y_{u v} \\leq y_{u w} \\leq y_{v w}$. We consider positive degenerate triangle $u v$ as a +++ triangle and negative degenerate triangle $u v$ as a $--+$ triangle, both having the same $y$ value $\\left(y_{u v}, y_{u v}, 1, y_{u v}\\right)$. This substitution simplifies our computations, and we will later show that it doesn't increase the value of $\\Delta(T)-\\operatorname{cost}(T)$ for degenerate triangles. Consequently, $\\mathcal{T}$ includes both degenerate and non-degenerate triangles and it can be used to lower bound $\\Delta(T)-\\operatorname{cost}(T)$. Given graph $G$ and the cluster LP solution $z_{S}$, along with their corresponding $y$ values, we define $\\eta_{T}(y)$ for any triangle $T \\in \\mathcal{T}$ as the number of such triangles present in graph $G$.\n\nIn section 5, we demonstrate that, for any triangle $T \\in \\mathcal{T}$, the inequality $\\Delta(T)-\\operatorname{cost}(T) \\geq 0$ holds. In section 6, we establish that, for any solution $y$, it is possible to determine a reduced budget such that the summation $\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T)) \\geq 0$. The key insight is that $\\eta_{T}(y)$ for a given solution $y$ cannot take arbitrary values. In this section, we elucidate how stronger constraints can be imposed on $\\eta_{T}(y)$, enabling the use of a smaller budget for $\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T))$.\n\nIn this section, we will first outline the constraint we intend to impose on $\\eta_{T}$. Subsequently, we will employ discretization techniques to encompass all possible triangles $T$. We will then formulate a semidefinite program to demonstrate that it is possible to achieve $\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T)) \\geq 0$ within a reduced budget.\n\nCovariance Constraint. Given cluster LP solution $z_{S}$ and $y$, for each node $u$, the covariance matrix $C O V_{u} \\in \\mathbb{R}^{V \\times V}$ where\n\n$$\nC O V_{u}(v, w)=y_{u v w}-y_{u v} y_{u w}\n$$\n\nis positive semi-definite(PSD). Actually, we can even show a stronger version of the covariance constraint.\nLemma 56. We define $y_{S}=\\sum_{S^{\\prime} \\supseteq S} z_{S^{\\prime}}$ for every $S \\subseteq V$. For any $T \\subseteq V, T \\neq \\emptyset$, the matrix $M \\in \\mathbb{R}^{V \\times V}$ where $M_{u v}=y_{T \\cup\\{u, v\\}}-y_{T \\cup\\{u\\}} y_{T \\cup\\{v\\}}$ is PSD.\n\nProof. Fix any vector $a \\in \\mathbb{R}^{V}$. For every $S \\subseteq V$, we define $a(S):=\\sum_{v \\in S} a_{v}$. Then\n\n$$\n\\begin{aligned}\na^{\\top} M a & =\\sum_{u, v \\in V}\\left(y_{T \\cup\\{u, v\\}}-y_{T \\cup\\{u\\}} y_{T \\cup\\{v\\}}\\right) a_{u} a_{v} \\\\\n& =\\sum_{u, v \\in V} a_{u} a_{v}\\left(\\sum_{S \\supseteq T \\cup\\{u, v\\}} z_{S}-\\left(\\sum_{S \\supseteq T \\cup\\{u\\}} z_{S}\\right)\\left(\\sum_{S^{\\prime} \\supseteq T \\cup\\{v\\}} z_{S^{\\prime}}\\right)\\right) \\\\\n& =\\sum_{S \\supseteq T} z_{S} \\cdot a(S) \\cdot a(S)-\\sum_{S \\supseteq T} \\sum_{S^{\\prime} \\supseteq T} z_{S} \\cdot z_{S^{\\prime}} \\cdot a(S) \\cdot a\\left(S^{\\prime}\\right) \\\\\n& \\geq \\sum_{S \\supseteq T, S^{\\prime} \\supseteq T} z_{S} z_{S^{\\prime}} a^{2}(S)-\\sum_{S \\supseteq T} \\sum_{S^{\\prime} \\supseteq T} z_{S} \\cdot z_{S^{\\prime}} \\cdot a(S) \\cdot a\\left(S^{\\prime}\\right) \\\\\n& =\\sum_{S \\supseteq T, S^{\\prime} \\supseteq T} z_{S} z_{S^{\\prime}}\\left(\\frac{1}{2} a^{2}(S)+\\frac{1}{2} a^{2}\\left(S^{\\prime}\\right)-a(S) a\\left(S^{\\prime}\\right)\\right) \\geq 0\n\\end{aligned}\n$$\n\nThe first inequlaity used that $\\sum_{S^{\\prime} \\supseteq T} z_{S^{\\prime}} \\leq 1$ as $T \\neq \\emptyset$. Therefore, $M$ is PSD.\nLemma 56 places a significant constraint on the distribution of triangles. We want to mention that the covariance matrix is implied by cluster LP, so we do not need to add extra constraints to the linear programming.\n\nTo leverage the property of the covariance matrix being positive semi-definite (PSD), we begin by partitioning the interval $[0,1]$ into numerous sub-intervals denoted as $I_{1}, I_{2}, \\ldots, I_{t}$ and each sub-interval $I_{j} \\in[0,1]$ is continuous. Given any $y_{u v} \\in[0,1]$, we use $I\\left(y_{u v}\\right)$ to represent the interval containing $y_{u v}$. Let $l\\left(I_{j}\\right)$ and $r\\left(I_{j}\\right)$ be the left and right boundary of interval $I_{j}$. Given a node $u$, consider the matrix $Q_{u} \\in \\mathbb{R}^{t \\times t}$, where $Q_{u}\\left(I_{j}, I_{k}\\right)$ is defined as\n\n$$\nQ_{u}\\left(I_{j}, I_{k}\\right)=\\sum_{\\substack{y_{u v} \\in I_{j}, y_{u w} \\in I_{k} \\\\ v \\in V, w \\in V}} y_{u v w}-y_{u v} y_{u w}\n$$\n\nWhen $v=u$ or $w=u$, we have $y_{u v w}-y_{u v} y_{u w}=0$ and $Q_{u}$ remains well-defined. We first show that $Q_{u}$ is also PSD.\n\nLemma 57. Let $Q_{u}$ be the matrix defined above, then $Q_{u} \\succeq 0$.\n\nProof. For any given vector $a \\in \\mathbb{R}^{t}$, let $a\\left(I_{j}\\right)$ be the value for index $I_{j}$.\n\n$$\n\\begin{aligned}\na^{\\top} \\cdot Q_{u} \\cdot a & =\\sum_{I_{j}, I_{k}} a\\left(I_{j}\\right) Q_{u}\\left(I_{j}, I_{k}\\right) a\\left(I_{k}\\right) \\\\\n& =\\sum_{I_{j}, I_{k}} \\sum_{y_{u v} \\in I_{j}, y_{u w} \\in I_{k}} a\\left(I_{j}\\right) a\\left(I_{k}\\right)\\left(y_{u v w}-y_{u v} y_{u w}\\right) \\\\\n& =\\sum_{v, w} \\sum_{I_{j} \\ni y_{u v}, I_{k} \\ni y_{u w}} a\\left(I_{j}\\right) a\\left(I_{k}\\right)\\left(y_{u v w}-y_{u v} y_{u w}\\right) \\\\\n& =\\sum_{v, w} \\sum_{y_{u v}, y_{u w}} a\\left(I\\left(y_{u v}\\right)\\right) a\\left(I\\left(y_{u w}\\right)\\right)\\left(y_{u v w}-y_{u v} y_{u w}\\right) \\\\\n& =b^{\\top} \\cdot C O V_{u} \\cdot b \\geq 0\n\\end{aligned}\n$$\n\nwhere $b \\in \\mathbb{R}^{n}$ and each entry of $b$ is defined as $b(v)=a\\left(I\\left(y_{u v}\\right)\\right)$. The last inequality is using the fact that the covariance matrix is PSD.\n\nGiven a cluster LP solution $y$, let $Q(y)=\\sum_{u} Q_{u}$ be the sum of $Q_{u}$ for all vertices $u$ in the set $V$. According to Lemma 57, we can assert that $Q(y) \\succeq 0$.\n\nWe can now express the matrix $Q(y)$ using triangles $T \\in \\mathcal{T}$ and the corresponding counts $\\eta_{T}(y)$. To achieve this, we need to consider the increase caused by $T$ in the matrices $Q_{u}, Q_{v}$, and $Q_{w}$.\n\nSpecifically, a non-degenerate triangle $T$ increases the entries $Q_{u}\\left(I\\left(y_{u v}\\right), I\\left(y_{u w}\\right)\\right)$ and $Q_{u}\\left(I\\left(y_{u w}\\right), I\\left(y_{u v}\\right)\\right)$ by $y_{u v w}-y_{u v} y_{u w}$ for the matrix $Q_{u}$. Similar computations are performed for $Q_{v}$ and $Q_{w}$. For degenerate triangle $T=\\left(y_{u v}, y_{u v}, 1, y_{u v}\\right)$, it increases $Q_{u}\\left(I\\left(y_{u v}\\right), I\\left(y_{u v}\\right)\\right)$ by $\\left(y_{u v}-y_{u v}^{2}\\right)$, increases $Q_{v}\\left(I\\left(y_{u v}\\right), I\\left(y_{u v}\\right)\\right)$ by $\\left(y_{u v}-y_{u v}^{2}\\right)$ and doesn't affect and $Q_{w}$. In either case, we increase $Q\\left(I\\left(y_{u v}\\right), I\\left(y_{u w}\\right)\\right)$ and $Q_{u}\\left(I\\left(y_{u w}\\right), I\\left(y_{u v}\\right)\\right)$ by $y_{u v w}-y_{u v} y_{u w}, Q\\left(I\\left(y_{v u}\\right), I\\left(y_{v w}\\right)\\right)$ and $Q\\left(I\\left(y_{v w}\\right), I\\left(y_{v u}\\right)\\right)$ by $y_{u v w}-y_{v u} y_{v w}, Q\\left(I\\left(y_{w u}\\right), I\\left(y_{w v}\\right)\\right)$ and $Q\\left(I\\left(y_{w v}\\right), I\\left(y_{w u}\\right)\\right)$ by $y_{u v w}-y_{w u} y_{w v}$.\n\nMore formally, to compute $Q(y)$. Let $C(T) \\in \\mathbb{R}^{6 \\times 6}$ be matrix that\n\n$$\nC(T)=\\operatorname{diag}\\left(C^{(1)}(T), C^{(2)}(T), \\ldots, C^{(6)}(T)\\right)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& C^{(1)}(T)=C^{(2)}(T)=C O V_{u}(v, w)=y_{u v w}-y_{u v} y_{u w} \\\\\n& C^{(3)}(T)=C^{(4)}(T)=C O V_{v}(u, w)=y_{u v w}-y_{v u} y_{v w} \\\\\n& C^{(5)}(T)=C^{(6)}(T)=C O V_{w}(v, u)=y_{u v w}-y_{w v} y_{w u}\n\\end{aligned}\n$$\n\nLet $\\operatorname{Head}(T) \\in \\mathbb{R}^{6 \\times t}$ be a binary matrix, defined as $\\operatorname{Head}(T)(i, I(a))=1$ when:\n\n$$\n(i, a) \\in\\left\\{\\left(1, y_{u v}\\right),\\left(2, y_{u w}\\right),\\left(3, y_{v u}\\right),\\left(4, y_{v w}\\right),\\left(5, y_{w u}\\right),\\left(6, y_{w v}\\right)\\right\\}\n$$\n\nand 0 otherwise. Similarly, we define $\\operatorname{Tail}(T) \\in \\mathbb{R}^{6 \\times t}$ as a binary matrix, defined as $\\operatorname{Tail}(T)(i, I(a))=1$ when:\n\n$$\n(i, a) \\in\\left\\{\\left(1, y_{u w}\\right),\\left(2, y_{u v}\\right),\\left(3, y_{v w}\\right),\\left(4, y_{v u}\\right),\\left(5, y_{w v}\\right),\\left(6, y_{w u}\\right)\\right\\}\n$$\n\nWe can then express $Q(y)$ as:\n\n$$\nQ(y)=\\sum_{T \\in \\mathcal{T}} \\eta_{T}(y) \\cdot\\left(\\operatorname{Head}^{\\top}(T) \\cdot C(T) \\cdot \\operatorname{Tail}(T)\\right)\n$$\n\nLet $\\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)=\\left\\{\\left(y_{u v}, y_{u w}, y_{v w}, y_{u v w}\\right) \\in \\mathcal{T} \\mid y_{u v} \\leq y_{u w} \\leq y_{v w}, y_{u v} \\in I_{i}, y_{v w} \\in I_{j}, y_{u v} \\in I_{k}\\right\\}$ be the set of triangles within the intervals $I_{i}, I_{j}, I_{k}$. For any two triangles $T$ with $\\left(y_{u v}=a, y_{u w}=b, y_{v w}=c\\right)$ and $T^{\\prime}$ with $\\left(y_{u v}=a^{\\prime}, y_{u w}=b^{\\prime}, y_{v w}=c^{\\prime}\\right)$, if $T$ and $T^{\\prime}$ are located within the same interval, meaning $I(a)=I\\left(a^{\\prime}\\right)$, $I(b)=I\\left(b^{\\prime}\\right)$, and $I(c)=I\\left(c^{\\prime}\\right)$, then we have $\\operatorname{Head}(T)=\\operatorname{Head}\\left(T^{\\prime}\\right)$ and $\\operatorname{Tail}(T)=\\operatorname{Tail}\\left(T^{\\prime}\\right)$. Given $I_{i}, I_{j}, I_{k}$, define $\\operatorname{Head}\\left(I_{i}, I_{j}, I_{k}\\right)=\\operatorname{Head}(T)$ (or $\\operatorname{Tail}\\left(I_{i}, I_{j}, I_{k}\\right)=\\operatorname{Tail}(T)$ ), where $T$ is an arbitrary triangle such that $T \\in \\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)$. We can then express $Q(y)$ as:\n\n$$\nQ(y)=\\sum_{I_{i}, I_{j}, I_{k}} \\operatorname{Head}^{\\mathrm{T}}\\left(I_{i}, I_{j}, I_{k}\\right) \\cdot\\left(\\sum_{T \\in \\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)} \\eta_{T}(y) \\cdot C(T)\\right) \\cdot \\operatorname{Tail}\\left(I_{i}, I_{j}, I_{k}\\right)\n$$\n\nSince $Q(y) \\succeq 0$, it provides a constraint for $\\eta_{T}(y)$. Our target is to show that $\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T)) \\geq$ 0 under the constraint that $Q(y) \\succeq 0$. We will consider all possible triangles in each range $\\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)$ and there will be at most $O\\left(t^{3}\\right)$ different range. For any solution $y$, the triangle $T=\\left\\{\\left(y_{u v}, y_{u w}, y_{v w}, y_{u v w}\\right) \\mid\\right.$ $y_{u v} \\leq y_{u w} \\leq y_{v w}\\}$ satisfies the triangle inequality, that is $1-y_{u w}+1-y_{v w} \\geq 1-y_{u v}$. Therefore, given range $I_{i}, I_{j}, I_{k}$, we assume that $l\\left(I_{i}\\right) \\geq l\\left(I_{j}\\right)+l\\left(I_{k}\\right)-1, r\\left(I_{j}\\right) \\leq 1+l\\left(I_{i}\\right)-l\\left(I_{k}\\right)$ and $r\\left(I_{k}\\right) \\leq 1+l\\left(I_{i}\\right)-l\\left(I_{j}\\right)$, since there won't be any triangles in the range when the above inequality doesn't hold. If necessary, we can truncate intervals to ensure that these inequalities hold true.\n\nIn each range $I_{i}, I_{j}, I_{k}$, we will use $\\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right)=\\min _{T \\in \\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)}(\\Delta(T)-\\operatorname{cost}(T))$ to establish a lower bound on $\\Delta(T)-\\operatorname{cost}(T) . \\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right)$ can be precomputed and there will be at most $O\\left(t^{3}\\right)$ different $\\tilde{d}$ value. We first show that $\\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right)$ is a lower bound even for degenerate triangle. Recall that we treat degenerate triangle as triangle with $y$ value $\\left(y_{u v}, y_{u v}, 1, y_{u v}\\right)$.\n\nLemma 58. Let $T=(u, v)$ be a degenerate triangle, for any budget function $b^{+}, b^{-}$such that $b^{+}(0)=0$, then\n\n$$\n\\operatorname{cost}(u, v)-\\Delta(u, v) \\geq \\tilde{d}\\left(I\\left(y_{u v}\\right), I\\left(y_{u v}\\right), I(1)\\right)\n$$\n\nProof. Let $T_{1}$ represent the +++ triangle with $\\left(y_{u v}, y_{u v}, 1, y_{u v}\\right)$.If $(u, v)$ is a positive edge and $y_{u v} \\geq 2 / 3$, then $\\operatorname{cost}(u, v)=0$ and $\\Delta(u, v)=2 b^{+}\\left(x_{u v}\\right)$. In this case, $\\operatorname{cost}\\left(T_{1}\\right)=0$ and $\\Delta\\left(T_{1}\\right)=2 b^{+}\\left(x_{u v}\\right)+b^{+}(0)$. If $(u, v)$ is a positive edge and $y_{u v}<2 / 3$, we have $\\operatorname{cost}(u, v)=2\\left(1-y_{u v}\\right)$ and $\\Delta(u, v)=2 b^{+}\\left(x_{u v}\\right)$. For $T_{1}$ in this scenario, $\\operatorname{cost}\\left(T_{1}\\right)=2\\left(1-y_{u v}\\right)$ and $\\Delta\\left(T_{1}\\right)=2 b^{+}\\left(x_{u v}\\right)+b^{+}(0) y_{u v}$. In either case, we observe that $\\Delta(u, v)-\\operatorname{cost}(u, v)=\\Delta\\left(T_{1}\\right)-\\operatorname{cost}\\left(T_{1}\\right) \\geq \\tilde{d}\\left(I\\left(y_{u v}\\right), I\\left(y_{u v}\\right), I(1)\\right)$.\n\nLet $T_{2}$ be the $--+$triangle with $\\left(y_{u v}, y_{u v}, 1, y_{u v}\\right)$. If $(u, v)$ is a negative edge, then $\\operatorname{cost}(u, v)=2 y_{u v}$ and $\\Delta(u, v)=2 b^{-}\\left(x_{u v}\\right)$. For $T_{2}$, we have $\\operatorname{cost}\\left(T_{2}\\right)=2 y_{u v}+2 y_{u v}-2 y_{u v}^{2}$ and $\\Delta\\left(T_{2}\\right)=2 b^{-}\\left(x_{u v}\\right)+b^{+}(0)\\left(2 y_{u v}-y_{u v}^{2}\\right)$. Therefore, we conclude that $\\Delta(u, v)-\\operatorname{cost}(u, v) \\geq \\Delta\\left(T_{2}\\right)-\\operatorname{cost}\\left(T_{2}\\right) \\geq \\tilde{d}\\left(I\\left(y_{u v}\\right), I\\left(y_{u v}\\right), I(1)\\right)$.\n\nWe still need to represent $Q$ using $\\eta_{T}$. The problem is there might be infinite types of triangles in each range $\\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)$, and we are not able to list every possible triangle. To solve this problem, our last observation is that, for $i \\in[1,6], C^{(i)}(T)$ is a multi-linear function in terms of $y_{u v}, y_{u w}, y_{v w}$ and $y_{u v w}$.\n\nSince $C^{(i)}(T)$ is multi-linear, we only need to consider 16 different triangles in each range to cover all possible triangles, where those triangles' $y$ value are\n\n$$\n\\begin{aligned}\ny_{u v} & \\in\\left\\{l\\left(I_{i}\\right), r\\left(I_{i}\\right)\\right\\}, y_{u w} \\in\\left\\{l\\left(I_{j}\\right), r\\left(I_{j}\\right)\\right\\}, y_{v w} \\in\\left\\{l\\left(I_{k}\\right), r\\left(I_{k}\\right)\\right\\} \\\\\ny_{u v w} & \\in\\left\\{\\max \\left(0,\\left(l\\left(I_{i}\\right)+l\\left(I_{j}\\right)+l\\left(I_{k}\\right)-1\\right) / 2\\right), r\\left(I_{k}\\right)\\right\\}\n\\end{aligned}\n$$\n\nThe $y_{u v w}$ constraints comes from the fact that $y_{u v w} \\leq y_{v w}$ and $y_{u w}+y_{u v}+y_{v w}-2 y_{u v w} \\leq 1$. Each triangle might be considered in different ranges multiple times and we shall treat them as different types of triangles. Let $\\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}\\right)$ be the set of triangles containing this 16 triangles in $\\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)$ and $\\mathcal{T}_{\\mathcal{D}}=$ $\\cup_{I_{i}, I_{j}, I_{k}} \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}\\right)$, we can set up a semi-definite programming to lower bound $\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T))$. In factor-revealing SDP, the variables are $\\eta_{T}$ that represent the ratio of triangles $T \\in \\mathcal{T}_{\\mathcal{D}}$ and $N \\cdot \\eta_{T}$ is the\n\nnumber of triangle of $T$, where $N=\\frac{n(n-1)(n-2)}{6}+\\frac{n(n-1)}{2}=\\frac{n(n-1)(n+1)}{6} . \\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right)$, Head $^{\\mathrm{T}}\\left(I_{i}, I_{j}, I_{k}\\right), C(T)$ and $\\operatorname{Tail}\\left(I_{i}, I_{j}, I_{k}\\right)$ are constant and can be pre-computed.\n\n$$\n\\begin{aligned}\n& \\min \\sum_{I_{i}, I_{j}, I_{k}} \\sum_{T \\in \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}\\right)} \\eta_{T} \\cdot \\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right) \\quad \\text { s.t. } \\\\\n& Q=\\sum_{I_{i}, I_{j}, I_{k}} \\operatorname{Head}^{\\mathrm{T}}\\left(I_{i}, I_{j}, I_{k}\\right)\\left(\\sum_{T \\in \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}\\right)} \\eta_{T} \\cdot C(T)\\right) \\cdot \\operatorname{Tail}\\left(I_{i}, I_{j}, I_{k}\\right) \\succeq 0 \\\\\n& \\eta_{T} \\geq 0 \\\\\n& \\sum_{T \\in \\mathcal{T}_{\\mathcal{D}}} \\eta_{T}=1\n\\end{aligned}\n$$\n\nOur main theorem regarding the SDP program is that\nTheorem 59. For any cluster $L P$ solution $z_{S}$ and $y$, let $\\mathrm{OPT}_{\\mathrm{ SDP}}$ be the optimal solution for factor-revealing SDP, then\n\n$$\n\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T)) \\geq N \\cdot \\mathrm{OPT}_{\\mathrm{SDP}}\n$$\n\nwhere $N=\\frac{n(n-1)(n+1)}{6}$ is the number of degenerate and non-degenerate triangles.\nProof. Note that $\\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right)$ is the lower bound of $\\Delta(T)-\\operatorname{cost}(T)$ for any $T \\in \\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)$. Hence, we have:\n\n$$\n\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T)) \\geq \\sum_{I_{i}, I_{j}, I_{k}} \\sum_{T \\in \\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)} \\eta_{T}(y) \\cdot \\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right)\n$$\n\nThe difficulty of the proof arises from the fact that we only consider triangles from $\\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}\\right)$, which contains only 16 triangles. We will show that we can use triangles from $\\mathcal{T}_{\\mathcal{D}}$ to represent $Q$ and the objective value is at most $\\sum_{I_{i}, I_{j}, I_{k}} \\sum_{T \\in \\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)} \\eta_{T}(y) \\cdot \\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right)$. For any triangle $T$ with $\\left(y_{u v} \\in I_{i}, y_{u w} \\in I_{j}, y_{v w} \\in\\right.$ $\\left.I_{k}\\right)$ and $l\\left(I_{i}\\right) \\leq l\\left(I_{j}\\right) \\leq l\\left(I_{k}\\right)$, let $T_{1}, T_{2}, \\ldots, T_{16}$ be the 16 triangles from $\\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}\\right)$. We will later show that there exist $\\lambda_{i} \\geq 0$ such that $C(T)=\\sum_{i \\in[1,16]} \\lambda_{i} \\cdot C\\left(T_{i}\\right)$ and $\\sum_{i \\in[1,16]} \\lambda_{i}=1$. Therefore, we can use $T_{1}, T_{2}, \\ldots, T_{16}$ to replace $T$ without affecting the $Q$ value. As we use $\\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right)$ in the objective function and $\\sum_{i \\in[1,16]} \\lambda_{i}=1$, this substitution doesn't impact the objective value either.\n\nCombining these findings, for any solution $y$ in cluster LP, we can deduce the existence of a feasible solution $\\eta_{T}$ from factor-revealing SDP such that $\\sum \\eta_{t}=1$ and $N \\cdot Q\\left(\\eta_{T}\\right)=Q(y)$. Therefore, we have\n\n$$\n\\begin{aligned}\n\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T)) & \\geq \\sum_{I_{i}, I_{j}, I_{k}} \\sum_{T \\in \\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)} \\eta_{T}(y) \\cdot \\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right) \\\\\n& \\geq \\sum_{I_{i}, I_{j}, I_{k}} \\sum_{T \\in \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}\\right)} N \\cdot \\eta_{T} \\cdot \\tilde{d}\\left(I_{i}, I_{j}, I_{k}\\right) \\geq N \\cdot \\mathrm{OPT}_{\\mathrm{SDP}}\n\\end{aligned}\n$$\n\nWe still need to demonstrate the existence of $\\lambda_{i} \\geq 0$ such that $C(T)=\\sum_{i \\in[1,16]} \\lambda_{i} \\cdot C\\left(T_{i}\\right)$ and $\\sum_{i \\in[1,16]} \\lambda_{i}=$ 1. For any triangle $T=\\left(y_{u v}, y_{u w}, y_{v w}, y_{u v w}\\right)$, consider triangle $T_{1}=\\left(l\\left(I_{i}\\right), y_{u w}, y_{v w}, y_{u v w}\\right)$ and $T_{2}=$ $\\left(r\\left(I_{i}\\right), y_{u w}, y_{v w}, y_{u v w}\\right)$ with $\\lambda=\\frac{y_{u w}-l\\left(I_{i}\\right)}{r\\left(I_{i}\\right)-l\\left(I_{i}\\right)}$. We then have $C^{(i)}(T)=(1-\\lambda) C^{(i)}\\left(T_{1}\\right)+\\lambda C^{(i)}\\left(T_{2}\\right)$ for $i \\in[1,6]$. We can apply the same transformation to $y_{u w}, y_{v w}, y_{u v w}$ since $C^{(i)}(T)$ is multilinear. This completes the proof.\n\nTheorem 59 provides a valuable method for establishing a lower bound on $\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T))$ for any given solution and budget function. To proceed with the computation of this lower bound, we need to solve factor-revealing SDP. It's important to emphasize that solving factor-revealing SDP is completely independent of cluster LP. The value of $\\mathrm{OPT}_{\\mathrm{ SDP}}$ is determined solely by the chosen budget function and the selected intervals $I_{1}, I_{2}, \\ldots, I_{t}$.\n\nThe estimation errors, which is the difference between $\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T))$ and $\\mathrm{OPT}_{\\mathrm{ SDP}}$, mainly arise from two sources. First, we use $\\hat{d}\\left(I_{i}, I_{j}, I_{k}\\right)$ as an estimated value for all triangles within $T_{D}\\left(I_{i}, I_{j}, I_{k}\\right)$. Second, some of the triangles we consider may violate the triangle inequality, and there's the possibility that $y_{u v w}$ could violate its constraint as defined in cluster LP.\n\nGiven the budget function, the accuracy of our estimation improves with a greater number of chosen intervals, but this also leads to longer computation times for solving $\\mathrm{OPT}_{\\text {SDP }}$. Additionally, to enhance the accuracy of computing $\\mathrm{OPT}_{\\mathrm{ SDP}}$, we employ two key techniques. Firstly, we introduce an additional SDP constraint, and secondly, we discretize the values of $y_{u v w}$.\n\nFrequency Constraint. We can set up another constraint for $\\eta_{T}$. Similarly, given a node $u$, consider the frequency matrix\n\n$$\nF_{u}\\left(I_{j}, I_{k}\\right)=\\left|\\left\\{v \\in V, w \\in V \\mid y_{u v} \\in I_{j}, y_{u w} \\in I_{k}\\right\\}\\right|=\\sum_{y_{u v} \\in I_{j}, y_{u w} \\in I_{k}} 1\n$$\n\n$F_{u}\\left(I_{j}, I_{k}\\right)$ is the number of pairs such that $y_{u v} \\in I_{j}$ and $y_{u w} \\in I_{k}$. We want to mention $v$ and $w$ might be $u$. Let freq $\\in \\mathbb{R}^{t}$ such that $\\operatorname{freq}\\left(I_{j}\\right)=\\left|\\left\\{v \\in V \\mid y_{u v} \\in I_{j}\\right\\}\\right|$, then we can represent $F_{u}$ as\n\n$$\nF_{u}=\\operatorname{freq}^{\\top} \\cdot \\operatorname{freq}\n$$\n\nwhich implies $F_{u}$ is PSD, too. We will follow the discretization strategy of $Q$ matrix. Let $F=\\sum_{u} F_{u}$ and $A \\in \\mathbb{R}^{6 \\times 6}$ is defined as\n\n$$\nA=\\operatorname{diag}(1,1,1,1,1,1)\n$$\n\nThen the increase of $F$ causing by $T$ is\n\n$$\n\\operatorname{Head}^{\\top}(T) \\cdot A \\cdot \\operatorname{Tail}(T)\n$$\n\nIt's worth noting this increase holds even for degenerate triangles. For any degenerate triangle $(u, v)$, it raises $F\\left(I\\left(y_{u v}\\right), I\\left(y_{u v}\\right)\\right), F\\left(I\\left(y_{u v}\\right), I(1)\\right)$, and $F\\left(I(1), I\\left(y_{u v}\\right)\\right)$ by 2 , echoing the increase seen in the triangle with the $y$ value $\\left(y_{u v}, y_{u v}, 1, y_{u v}\\right)$. However, unlike the $Q_{u}$ vector, $F_{u}$ also considers cases where $v=u$ and $w=u$. In the $Q_{u}$ vector, this case doesn't impact $Q_{u}$ due to $y_{u u u}-y_{u u} y_{u u}=0$. We need to increment $F(I(1), I(1))$ by $n$ since $\\eta(y)$ does not account for $v=u$ and $w=u$. Fortunately, we already include the $+++$ triangle $T_{3}=(1,1,1,1)$; given that $T_{3}$ doesn't influence the matrix $Q$ value and $\\Delta\\left(T_{3}\\right)-\\operatorname{cost}\\left(T_{3}\\right)=0$, each occurrence of $T_{3}$ raises $F(I(1), I(1))$ by 6 . By increasing $\\eta_{T_{3}}(y)$ by $n / 6$, we can effectively represent $F$ as:\n\n$$\nF(y)=\\sum_{I_{i}, I_{j}, I_{k}} \\operatorname{Head}^{\\top}\\left(I_{i}, I_{j}, I_{k}\\right)\\left(\\sum_{T \\in \\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)} \\eta_{T}(y) \\cdot A\\right) \\cdot \\operatorname{Tail}\\left(I_{i}, I_{j}, I_{k}\\right)\n$$\n\nGiven that $A$ is a constant matrix, any $T$ can be utilized to encompass all triangles from $\\mathcal{T}\\left(I_{i}, I_{j}, I_{k}\\right)$ for $F$.\nDiscretization for $y_{u v w}$. Another technique that enhances estimation accuracy involves discretizing the $y_{u v w}$ values. We achieve this by subdividing the interval $\\left[\\max \\left(0,\\left(l\\left(I_{i}\\right)+l\\left(I_{j}\\right)+l\\left(I_{k}\\right)-1\\right) / 2\\right), r\\left(I_{k}\\right)\\right]$ into numerous sub-intervals. The specific manner in which we partition this interval depends on both the interval's length and its significance in the overall estimation.\n\nHowever, it's crucial to note that discretizing $y_{u v w}$ results in a substantial increase in the number of triangles to be considered. This surge may render the final SDP impractical to solve. The formulation of our ultimate factor-revealing SDP is provided below:\n\n$$\n\\begin{aligned}\n& \\min \\sum_{I_{i}, I_{j}, I_{k}, I_{l}} \\sum_{T \\in \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right)} \\eta_{T} \\cdot \\tilde{d}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right) \\quad \\text { s.t. } \\quad \\text { (factor-revealing SDP) } \\\\\n& Q=\\sum_{I_{i}, I_{j}, I_{k}} \\operatorname{Head}^{\\boldsymbol{\\tau}}\\left(I_{i}, I_{j}, I_{k}\\right)\\left(\\sum_{I_{l}, T \\in \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right)} \\eta_{T} \\cdot C(T)\\right) \\cdot \\operatorname{Tail}\\left(I_{i}, I_{j}, I_{k}\\right) \\succeq 0 \\\\\n& F=\\sum_{I_{i}, I_{j}, I_{k}} \\operatorname{Head}^{\\boldsymbol{\\tau}}\\left(I_{i}, I_{j}, I_{k}\\right)\\left(\\sum_{I_{l}, T \\in \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right)} \\eta_{T} \\cdot A\\right) \\cdot \\operatorname{Tail}\\left(I_{i}, I_{j}, I_{k}\\right) \\succeq 0 \\\\\n& \\eta_{T} \\geq 0 \\quad \\forall T \\in \\mathcal{T}_{\\mathcal{D}} \\\\\n& \\sum_{T \\in \\mathcal{T}_{\\mathcal{D}}} \\eta_{T}=1\n\\end{aligned}\n$$\n\nwhere $\\tilde{d}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right)=\\min _{T \\in T\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right)}(\\Delta(T)-\\operatorname{cost}(T))$, and $\\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right)$ is the set of triangles such that\n\n$$\ny_{u v} \\in\\left\\{l\\left(I_{i}\\right), r\\left(I_{i}\\right)\\right\\}, y_{u w} \\in\\left\\{l\\left(I_{j}\\right), r\\left(I_{j}\\right)\\right\\}, y_{v w} \\in\\left\\{l\\left(I_{k}\\right), r\\left(I_{k}\\right)\\right\\}, y_{u v w} \\in\\left\\{l\\left(I_{l}\\right), r\\left(I_{l}\\right)\\right\\}\n$$\n\nBy appropriately setting intervals, we can demonstrate that $\\mathrm{OPT}_{\\mathrm{ SDP}}$ remains non-negative, as affirmed in Lemma 60, even for a reduced budget function. The primary challenge lies in achieving a balance between estimation error and the running time of solving factor-revealing SDP in practice. For a more in-depth explanation of the specific settings of Lemma 60, please refer to the details provided in Appendix B.\nLemma 60. For budget functions $b^{+} \\equiv b_{1.437}^{+}$and $b^{-} \\equiv b_{1.437}^{-}$Let $\\mathrm{OPT}_{\\mathrm{SDP}}$ be the optiamal value for factor-revealing SDP, there exist intervals $I_{1}, I_{2}, \\ldots, I_{t}$ and intervals for $y_{u v w}$ such that $\\mathrm{OPT}_{\\mathrm{SDP}}=0$.\n\nProof of Lemma 11. For budget functions $b^{+} \\equiv b_{1.437}^{+}$and $b^{-} \\equiv b_{1.437}^{-}$, For degenerate and non-degenerate triangles, based on Theorem 59 and Lemma 60, we know that\n\n$$\n\\sum_{T} \\eta_{T}(y) \\cdot(\\Delta(T)-\\operatorname{cost}(T)) \\geq N \\cdot \\mathrm{OPT}_{\\mathrm{SDP}}=0\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 25,
      "text": "# 8 1.33-gap for Cluster LP \n\nIn this section, we show that the cluster LP has a gap of $4 / 3$, proving Theorem 4 restated below.\nTheorem 4. For any $\\varepsilon>0$, the integrality gap of the cluster $L P$ is at least $4 / 3-\\varepsilon$.\nThe graph of the plus edges of our gap instance is based on the line graph of a base graph; given a based graph $H=\\left(V_{H}, E_{H}\\right)$, our correlation clustering instance is $G=\\left(V_{G}, E_{G}\\right)$ where $V_{G}=E_{H}$ and $e, f \\in V_{G}$ have a plus edge in $G$ if they share a vertex in $V_{H}$.\n\nA high-level intuition is the following: LPs cannot distinguish between a random graph and a nearly bipartite graph. Consider vertices of $H$ as ideal clusters in $G$ containing their incident edges. Given a random graph $H$, the LP fractionally will think that it is nearly bipartite, implying that the almost entire $E_{H}$ can be partitioned into $n / 2$ ideal clusters. Of course, integrally, such a partition is not possible in random graphs. For the cluster LP, it suffices to consider a complete graph instead of a random graph. We believe (but do not prove) that such a gap instance can be extended to stronger LPs (e.g., Sherali-Adams strengthening of the cluster LP), because it is known that Sherali-Adams cannot distinguish a random graph and a nearly bipartite graph [CMM09].\n\nProof of Theorem 4. Let $H=\\left(V_{H}, E_{H}\\right)$ be a complete graph on $n$ vertices. Let $d=n-1$ be the degree of $H$. Our correlation clustering instance $G=\\left(V_{G}, E_{G}\\right)$ is the line graph of $H ; V_{G}=E_{H}$ and $e, f \\in E_{H}$ has + edge in $G$ if and only if they share a vertex in $H$. The + degree of each $e \\in E_{H}$ in $G$ is $2 d-2$.\n\nConsider the following solution for the cluster LP: for every $v \\in V_{H}$, let $E_{v} \\subseteq E_{H}$ be the $d$ edges containing $v$. The cluster LP has $z_{E_{v}}=1 / 2$ for every $v \\in v_{H}$. Each $e \\in E_{H}$ belongs to two fractional clusters, each of which has its $d-1$ plus neighbors, so fractionally $d-1$ plus edges incident on it are violated. Since each violated edge is counted twice, the LP value is $\\binom{n}{2}(d-1) / 2$.\n\nLet us consider the integral optimal correlation clustering of $G$. Consider a cluster $C$ in the clustering. Note that every vertex in $C$ has at least $|C| / 2$ plus neighbors in $C$, which implies $|C| \\leq 4 d$. We apply the following procedure to $C$ to partition it further.\n\nClaim 61. There is a partition of $C$ into $C_{1}, \\ldots, C_{r}$ such that (1) each $C_{i}$ is a subset of $E_{v}$ for some $v \\in V_{H}$, and (2) replacing $C$ by $C_{1}, \\ldots, C_{r}$ in the correlation clustering solution increases the objective function by at most $35|C|$.\n\nProof. For $v \\in V_{H}$, let $n_{v}:=\\left|C \\cap E_{v}\\right|$. Note that $\\sum_{v} n_{v}=2|C|$. Without loss of generality, assume $V_{H}=\\left\\{v_{1}, \\ldots, v_{n}\\right\\}$ with $n_{v_{1}} \\geq \\cdots \\geq n_{v_{n}}$. If $e=\\left(v_{i}, v_{j}\\right) \\in C$ has $i, j>8$, then the number of its plus neighbors in $C$ is $n_{v_{i}}+n_{v_{j}}<2 \\cdot \\frac{1}{8} \\cdot 2|C|=|C| / 2$, so it should not exist in $C$. So, every edge is incident on $v_{i}$ for some $i \\leq 8$.\n\nLet us make at most $\\binom{8}{2}=28$ edges in $C$ between $v_{1}, \\ldots, v_{8}$ as singleton clusters; the objective function increases by at most $28|C|$. Then partition the remaining $C$ into $E_{1}, \\ldots, E_{8}$ where $E_{i}:=C \\cap E_{v_{i}}$. Each $e \\in E_{i}$ has at most seven plus neighbors in $\\cup_{j \\neq i} E_{j}$, so the objective function increases by at most $7|C|$. So, we partitioned $C$ into $C_{1}, \\ldots, C_{r}$ where all the edges in $C_{i}$ share a common endpoint. We increased the objective function by at most $35|C|$.\n\nAfter we apply the above procedure to every cluster $C$, we increased the cost by at most $35\\left|V_{H}\\right| \\leq 35 n^{2}$ and all the edges in a cluster $C$ share a common endpoint. For $v \\in V_{H}$, let $C_{v}$ be the cluster in the solution whose common endpoint is $v$. (If there are many of them, merging them will strictly improve the objective function value.) Without loss of generality, there are $t$ such clusters $C_{v_{1}}, \\ldots, C_{v_{t}}$ and let $n_{i}:=\\left|C_{v_{i}}\\right|$ such that $n_{1} \\geq \\cdots \\geq n_{t}$.\nClaim 62. $\\sum_{i=1}^{t} n_{i}^{2} \\leq n^{3} / 3$.\nProof. The LHS is monotone in $\\left(n_{1}, \\ldots, n_{t}\\right)$, and if there is an edge $\\left(v_{i}, v_{j}\\right) \\in C_{j}$ with $j>i$ (which implies $n_{i} \\geq n_{j}$ ), the LHS strictly improves by moving $\\left(v_{i}, v_{j}\\right)$ to $C_{i}$. Therefore, the configuration that maximizes the LHS is when $t=n$ and $C_{v_{i}}$ contains all the edges of $H$ not incident on $v_{1}, \\ldots, v_{i-1}$. In that case, the LHS is\n\n$$\n\\sum_{i=1}^{n-1}(n-i)^{2}=n^{3} \\sum_{i=1}^{n-1}\\left(\\frac{n-i}{n}\\right)^{2} \\cdot \\frac{1}{n} \\leq n^{3} \\int_{0}^{1}(1-x)^{2} d x=n^{3}\\left[x-x^{2}+x^{3} / 3\\right]_{0}^{1}=n^{3} / 3\n$$\n\nas desired.\nUsing this, we can prove a lower bound on the cost of our near-optimal clustering. Note that every cluster is a clique of +edges. Thus, the only edges violated are +edges. Moreover, there are at most $\\sum_{t \\in[t]} n_{i}^{2} / 2 \\leq n^{3} / 6$ correctly clustered + edges. The cost of our near-optimal clustering is the total number of +edges of $G$ minus the number of correctly clustered + edges, namely at most $\\binom{n}{2}(d-1)-n^{3} / 6=n^{3} / 3-o\\left(n^{3}\\right)$. Since the cost of the optimal clustering is at most $35 n^{2}$ lower than ours, it is still $n^{3} / 3-o\\left(n^{3}\\right)$. The fractional solution has the value at most $n^{3} / 4$, so the gap is at least $4 / 3-o(1)$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 26,
      "text": "# 9 1.04-NP Hardness \n\nIn this section, we show that it is NP-hard (under randomized reductions) to obtain an algorithm with an approximation ratio of $24 / 23 \\geq 1.043$, proving Theorem 5 restated below.\n\nThe idea is similar to the gap for the cluster LP in Section 8, which is based on the fact that the LPs generally cannot distinguish nearly bipartite graphs and random graphs. The main difference, which results in a worse factor here, is that other polynomial-time algorithms (e.g., SDPs) can distinguish between them! So, we are forced to work with slightly more involved structures.\n\nStill, we use a similar construction for 3 -uniform hypergraphs; let $H=\\left(V_{H}, E_{H}\\right)$ be the underlying 3uniform hypergraph and $G=\\left(V_{G}, E_{G}\\right)$ be the plus graph of the final Correlation Clustering instance where $V_{G}=E_{H}$ and $e, f \\in E_{H}$ has an edge in $G$ if they share a vertex in $H$. We use the following hardness result of Cohen-Addad, Karthik, and Lee [CAL22] that shows that it is hard to distinguish whether $H$ is nearly bipartite or close to a random hypergraph.\n\nTheorem 63. For any $\\varepsilon>0$, there exists a randomized polynomial-time algorithm that receives a 3-CNF formula $\\phi$ as input and outputs a simple 3 -uniform hypergraph $H=\\left(V_{H}, E_{H}\\right)$ where the degree of each vertex is $(1 \\pm o(1)) d$ for some $d=\\omega\\left(\\left|V_{H}\\right|\\right)$ such that the following properties are satisfied with high probability.\n\n- (YES) If $\\phi$ is satisfiable, there exists $U \\subseteq V_{H}$ with $|U|=\\left|V_{H}\\right| / 2$ that intersects every hyperedge in $E_{H}$. Moreover, for every $u \\in U,\\left|\\left\\{e \\in E_{H}: e \\cap U=\\{u\\}\\right\\}\\right| \\geq(1 / 2-\\varepsilon) d$.\n- (NO) If $\\phi$ is unsatisfiable, any set of $\\gamma\\left|V_{H}\\right|$ vertices $(\\gamma \\in[0,1])$ do not intersect at least a $(1-\\gamma)^{3}-\\varepsilon$ fraction of hyperedges in $E_{H}$.\n\nProof. The same reduction in Theorem 4.1 of (the arXiv version of) [CAL22] yields the desired hardness. In the following, we highlight the difference between the statement of Theorem 4.1 of [CAL22] and our Theorem 63 and briefly explain how our additional properties are satisfied by their reduction.\n\n1. Regularity of $H$ : Section 4.5 of [CAL22], based on an earlier weighted hard instance, constructs the final hard instance $H=\\left(V_{H}, E_{H}\\right)$ as a certain random hypergraph where the degree of each vertex $v$ is the sum of independent $\\{0,1\\}$ variables with the same expected value. This expected value is $\\Theta\\left(\\left|V_{H}\\right|^{1.5}\\right)$, so the standard Chernoff and union bound argument will show that the degree of each vertex is almost the same with high probability.\n2. In the (YES) case, for every $u \\in U,\\left|\\left\\{e \\in E_{H}: e \\cap U=\\{u\\}\\right\\}\\right| \\geq(1 / 2-\\varepsilon) d$ : It follows from their construction in Section 4.1. The construction is analogous to H\u00e5stad's celebrated result on Max3SAT [H\u00e5s01] where in the (YES) case, almost three quarters of the clauses have one true literal and almost one quarter have three true literals, so that for each true literal $\\ell$, roughly half of the clauses containing $\\ell$ has it as the only true literal.\n3. In the (NO) case: the guarantee holds for any value of $\\gamma \\in[0,1]$ instead of just 0.5 : One can simply change $1 / 2$ to $1-\\gamma$ in the proof of Lemma 4.4 in Section 4.3. It is analogous to the fact that all nontrivial Fourier coefficients vanish in H\u00e5stad's result on Max-3SAT and Max-3LIN [H\u00e5s01].\n\nGiven such $H=\\left(V_{H}, E_{H}\\right)$, let $n:=\\left|V_{H}\\right|$. Our correlation clustering instance $G=\\left(V_{G}, E_{G}\\right)$ is the line graph of $H ; V_{G}=E_{H}$ and $e, f \\in E_{H}$ have a plus edge in $G$ if they share a vertex in $H$. This means that every $e \\in V_{G}$ has $(3 \\pm o(1)) d$ plus edges incident on it; we used the fact that $d=\\omega(n)$ and $e$ has at most $O(n)$ other hyperedges that intersect with $e$ with at least two points (which causes double counting).\n\nYES case. Consider $U \\subseteq V_{H}$ guaranteed in Theorem 63. Our (randomized) clustering is the following: randomly permute vertices to obtain $U=\\left\\{v_{1}, \\ldots, v_{n / 2}\\right\\}$, and let $E_{i}:=\\left\\{e \\in E_{H}: v_{i} \\in e\\right.$ and $\\left.e \\cap\\left\\{v_{1}, \\ldots, v_{i-1}\\right\\}=\\right.$ $\\emptyset\\}$. Since $U$ intersects every $e \\in E_{H},\\left(E_{1}, \\ldots, E_{n / 2}\\right)$ forms a partition of $E_{H}$.\n\nWe analyze the expected cost of this clustering. For each $e \\in E_{H}$, let save(e) be (the number of plus neighbors in the same cluster) minus (the number of minus neighbors in the same cluster). Intuitively, it is\n\nthe amount of saved cost between $e$ and its neighbors, compared to the situation where $e$ is a singleton cluster. Then, the cost of our clustering is the total number of plus edges of $G$, namely $\\left|E_{H}\\right| \\cdot \\frac{3(1 \\pm o(1)) d}{2}=n d^{2} \\cdot \\frac{(1 \\pm o(1))}{2}$, minus $\\sum_{e \\in E_{H}}$ save $(e) / 2$.\n\nFix $v \\in U$ and let $E_{v}:=\\left\\{e \\in E_{H}: v \\in e\\right\\}, E_{v}^{\\prime}:=\\left\\{e \\in E_{H}: e \\cap U=\\{v\\}\\right\\}, E_{v}^{\\prime \\prime}:=E_{v} \\backslash E_{v}^{\\prime}$. Then $\\left|E_{v}\\right|=(1 \\pm o(1)) d$ and $\\left|E_{v}^{\\prime}\\right| \\geq(1 / 2-\\varepsilon) d$. We would like to compute $\\mathbb{E}\\left[\\left|E_{i}\\right|^{2}\\right]$ over random permutations where $i$ is defined such that $v_{i}=v$. It is clear that $E_{v}^{\\prime} \\subseteq E_{i}$. For each $e \\in E_{v}^{\\prime \\prime}$, the probability that $e \\in E_{i}$ is at least $1 / 3$ (when $v$ comes before the other two vertices of $e$ in the random permutation). And two hyperedges $e, f \\in E_{v}^{\\prime \\prime}$, the probability that both are in $E_{i}$ is at least $1 / 5$ (when $v$ comes first among $|e \\cup f| \\leq 5$ vertices). Therefore,\n\n$$\n\\mathbb{E}\\left[\\left|E_{i}\\right|^{2}\\right] \\geq\\left|E_{i}^{\\prime}\\right|^{2}+2\\left|E_{i}^{\\prime}\\right|\\left|E_{i}^{\\prime \\prime}\\right| / 3+\\left|E_{i}^{\\prime \\prime}\\right|^{2} / 5 \\geq d^{2}(1 / 4+1 / 6+1 / 20-O(\\varepsilon))=d^{2}(7 / 15-O(\\varepsilon))\n$$\n\nTherefore, the total saving is at least $n d^{2}(7 / 30-O(\\varepsilon))$ and the final cost is at most $n d^{2}(1 / 2-7 / 60+O(\\varepsilon))=$ $n d^{2}(23 / 60+O(\\varepsilon))$.\n\nNO case. Our analysis will be similar to that of the gap instance, slightly more complicated by the fact that we are working with a non-complete hypergraph. Consider the optimal correlation clustering and consider one cluster $C$. For $e \\in C$, it has at most $(3 \\pm o(1)) d$ plus edges in $G$, so $|C| \\leq(6+o(1)) d$; otherwise, it is better to make $e$ a singleton cluster. We prove that if $C$ is large, then we can partition $C$ into smaller clusters where each cluster consists of hyperedges sharing the same vertex in $H$. For $v \\in E_{H}$, let $E_{v} \\subseteq E_{H}$ be the set of hyperedges containing $v$.\n\nClaim 64. There is a partition of $C$ into $C_{1}, \\ldots, C_{r}$ such that (1) each $C_{i}$ is a subset of $E_{v}$ for some $v \\in V_{H}$, and (2) replacing $C$ by $C_{1}, \\ldots, C_{r}$ in the correlation clustering solution increases the objective function by at most $O(n|C|)$.\n\nProof. Without loss of generality, assume $V_{H}=\\left\\{v_{1}, \\ldots, v_{n}\\right\\}$ and define $n_{i}:=\\left|C \\cap E_{v_{i}}\\right|$ such that $n_{1} \\geq \\cdots \\geq$ $n_{n}$. Note that $\\sum_{i} n_{i}=3|C|$.\n\nIf $e=\\left(v_{i}, v_{j}, v_{k}\\right)$ with $i, j, k>20$, then $n_{i}+n_{j}+n_{k}<3 \\cdot(3|C| / 20)<|C| / 2$, which implies that $e$ has more minus neighbors than plus neighbors in $C$, leading to contradiction. So, every hyperedge is incident on $v_{i}$ for some $i \\leq 20$.\n\nSince two vertices of $H$ have at most $n$ hyperedges containing both of them, let us make at most $n \\cdot\\binom{10}{2}$ hyperedges in $C$ that contain at least two of $v_{1}, \\ldots, v_{20}$ as singleton clusters; the objective function increases by at most $n \\cdot\\binom{10}{2} \\cdot|C|$. Then partition the remaining $C$ into $E_{1}, \\ldots, E_{20}$ where $E_{i}:=C \\cap E_{v_{i}}$. Each $e \\in E_{i}$ has at most $2 \\cdot 20 \\cdot n$ plus edges in $\\cup_{j \\neq i} E_{j}$ ( 20 choices for $v_{j}, 2$ choices for a vertex in $e \\ni\\left\\{v_{i}\\right\\}$, and $n$ choices for hyperedges containing both vertices), so the objective function increases by at most $O(n|C|)$. So, we partitioned $C$ into $C_{1}, \\ldots, C_{r}$ where all the hyperedges in $C_{i}$ share a common endpoint. In total, we increased the objective function by at most $O(n|C|)$.\n\nApplying the above procedure for every cluster $C$ increases the objective function by at most $O\\left(n \\cdot\\left|E_{H}\\right|\\right)=$ $O\\left(n^{2} d\\right)$. Then, we have a clustering where all the edges in a cluster $C$ share a common endpoint. $C$ forms a clique in $H$. For $v \\in V_{H}$, let $C_{v}$ be the cluster in the solution whose common endpoint is $v$. (If there are many of them, merging them will strictly improve the objective function value.) Without loss of generality, there are $t$ such clusters $C_{v_{1}}, \\ldots, C_{v_{t}}$ and let $c_{i}:=\\left|C_{v_{i}}\\right|$ such that $c_{1} \\geq \\cdots \\geq c_{t}$.\n\nClaim 65. $\\sum_{i=1}^{t} c_{i}^{2} \\leq d^{2} n(0.2+O(\\sqrt{\\varepsilon}))$, where $\\varepsilon$ is the parameter from Theorem 63.\nProof. Here, we use the NO case guarantee from Theorem 63: for any $\\gamma \\in[0,1]$ and choice of $\\gamma n$ vertices, it covers at most $1-(1-\\gamma)^{3}+\\varepsilon=3 \\gamma-3 \\gamma^{2}+\\gamma^{3}+\\varepsilon$ fraction of the edges, which is equivalent to: for every $i \\in[n]$,\n\n$$\n\\sum_{j=1}^{i} c_{i} \\leq\\left(3(i / n)-3(i / n)^{2}+(i / n)^{2}+\\varepsilon\\right)\\left|E_{H}\\right|\n$$\n\nLet $\\delta=o(1)$ be such that every vertex of $H$ has degree at most $(1+\\delta) d$, which means that $(1+\\delta) d \\geq c_{1} \\geq$ $\\cdots \\geq c_{t}$. And let $f_{i / n}:=c_{i} /((1+\\delta) d)$. Then (71) becomes\n\n$$\n\\frac{1}{n} \\sum_{j=1}^{i} f_{j / n} \\leq\\left(3(i / n)-3(i / n)^{2}+(i / n)^{2}+\\varepsilon\\right) \\frac{\\left|E_{H}\\right|}{(1+\\delta) d n} \\leq\\left(3(i / n)-3(i / n)^{2}+(i / n)^{2}+\\varepsilon\\right) / 3\n$$\n\n(Note that $\\left|E_{H}\\right| \\leq(1+\\delta) d n / 3$.) Interpreting $\\frac{1}{n} \\sum_{j=1}^{i} f_{j / n}$ as $\\int_{0}^{1} f(x) d x$ where $f(x)=c_{\\lceil x n\\rceil}$, we have that $\\sum_{i=1}^{t}\\left|c_{i}\\right|^{2} \\leq(1+\\delta)^{2} d^{2} n \\max _{f} \\int_{0}^{1} f(x)^{2} d x$, where the maximum is taken over functions $f:[0,1] \\rightarrow[0,1]$ with the constraints that\n\n1. For all $y \\in[0,1]$,\n\n$$\n\\int_{x=0}^{y} f(x) d x \\leq y-y^{2}+y^{3} / 3+\\varepsilon / 3\n$$\n\n(Compared to (72), we add more constraints for every $y \\in[0,1]$, but it is valid to do so since the step function $f(\\cdot)$ defined above satisfies all these constraints; if (73) is violated for some value $y \\in$ $(i / n,(i+1) / n)$ for some integer $i$, (72) is violated at $(i+1) / n$ because $f(y)$ stays the same in the interval while the upper bound increases strictly less than linearly.)\n2. $f$ decreasing with $f(0) \\leq 1$.\n\nThen one see that the optimal $f$ satisfies either $f(y)=1$ or $\\int_{x=0}^{y} f(x)=y-y^{2}+y^{3}+\\varepsilon / 3$ for every $y \\in[0,1)$. If it is not satisfied at some $y$, we can increase $f(y)$ while decreasing $f(z)$ for some $z>y$, which will still satisfy the constraints and increase $\\int_{0}^{1} f(x)^{2} d x$. Therefore, we can conclude that $f(y)=1$ for $y \\leq \\tau$ and\n\n$$\n\\int_{x=0}^{y} f(x) d x=y-y^{2}+y^{3} / 3+\\varepsilon / 3 \\Rightarrow f(y)=\\left(y-y^{2}+y^{3} / 3+\\varepsilon / 3\\right)^{\\prime}=1-2 y+y^{2}\n$$\n\nfor $y>\\tau$, where $\\tau=\\Theta(\\sqrt{\\varepsilon})$ is the solution of $\\tau=\\tau-\\tau^{2}+\\tau^{3}+\\varepsilon / 3$. Then, we can bound\n\n$$\n\\int_{x=0}^{1} f(x)^{2} d x \\leq O(\\sqrt{\\varepsilon})+\\int_{x=0}^{1}\\left(1-2 x+x^{2}\\right)^{2} d x \\leq 0.2+O(\\sqrt{\\varepsilon})\n$$\n\nwhich implies that $\\sum_{i} c_{i}^{2} \\leq d^{2} n(0.2+O(\\sqrt{\\varepsilon}))$.\nUsing this, we can prove a lower bound on the cost of our near-optimal clustering. Note that every cluster is a clique of +edges. Thus, the only edges violated are +edges. Moreover, there are at most $\\sum_{i \\in[t]} c_{i}^{2} / 2 \\leq d^{2} n(0.1+O(\\sqrt{\\varepsilon})$ correctly clustered +edges. The cost of our near-optimal clustering is the total number of +edges of $G$ minus the number of correctly clustered +edges, namely at least $n d^{2}(1 / 2-$ $0.1-O(\\sqrt{\\varepsilon}))=n d^{2}(0.4-O(\\sqrt{\\varepsilon}))$. Since the cost of the optimal clustering is at most $O\\left(n^{2} d\\right)$ lower than ours, it is still $n d^{2}(0.4-O(\\sqrt{\\varepsilon}))$ using $d=\\omega(n)$.\n\nSince the value in the YES case is at most $(23 / 60+O(\\varepsilon)) n d^{2}$, so the gap is almost $\\frac{24}{23} \\geq 1.043$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 27,
      "text": "# 10 Open Problems \n\nWe highlight several problems left open by this work.\n\n- Our algorithm computes a $(1+\\epsilon)$-approximate solution for Cluster LP in $n^{\\operatorname{poly}(1 / \\epsilon)}$ time. An intriguing question is whether Cluster LP can be solved in time $f(\\epsilon) \\cdot \\operatorname{poly}(n)$, for some function $f$. (e.g., $\\left.O\\left(2^{\\operatorname{poly}(1 / \\epsilon)} n^{100}\\right)\\right)$.\n- Our analysis of the rounding procedure, which achieves a 1.437-approximate ratio, currently requires computer assistance. One challenge is to determine whether a similar approximate ratio can be achieved without computer assistance. Additionally, although computer assistance enables us to impose more constraints on $\\eta_{T}$-thereby slightly reducing the approximate ratio-it remains substantially above the theoretical lower bound. Exploring the possibility of achieving an approximate ratio of $4 / 3+\\epsilon$, or even surpassing the $4 / 3$ approximate ratio, represents a critical area for future research.\n\n- Recent findings by Cohen-Addad, Lolck, Pilipczuk, Thorup, Yan, and Zhang [CALP+24] demonstrate that a 1.847-approximate ratio is achievable in $\\tilde{O}(m)$ time, where $m$ is the number of +edges. Can we show some kind of tradeoff between the running time and the approximate ratio?",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 28,
      "text": "# References \n\n[ACG+15] Kook Jin Ahn, Graham Cormode, Sudipto Guha, Andrew McGregor, and Anthony Wirth. Correlation clustering in data streams. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 2237-2246, 2015. 4\n[ACN08] Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: Ranking and clustering. Journal of the ACM, 55(5):1-27, 2008. 2, 5, 8, 51\n[AHK+09] Rakesh Agrawal, Alan Halverson, Krishnaram Kenthapadi, Nina Mishra, and Panayiotis Tsaparas. Generating labels from clicks. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 172-181, 2009. 2\n[AN23] Sara Ahmadian and Maryam Negahbani. Improved approximation for fair correlation clustering. In International Conference on Artificial Intelligence and Statistics, pages 9499-9516. PMLR, 2023. 4\n[ARS09] Arvind Arasu, Christopher R\u00e9, and Dan Suciu. Large-scale deduplication with constraints using dedupalog. In Proceedings of the 25th IEEE International Conference on Data Engineering (ICDE), pages 952-963, 2009. 2\n[AW22] Sepehr Assadi and Chen Wang. Sublinear time and space algorithms for correlation clustering via sparse-dense decompositions. In Proceedings of the 13th Conference on Innovations in Theoretical Computer Science (ITCS), volume 215 of LIPIcs, pages 10:1-10:20, 2022. 4\n[BBC04] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine learning, $56(1): 89-113,2004.1,2$\n[BCMT22] Soheil Behnezhad, Moses Charikar, Weiyun Ma, and Li-Yang Tan. Almost 3-approximate correlation clustering in constant rounds. In Proceedings of 63rd Annual IEEE Symposium on Foundations of Computer Science, (FOCS), pages 720-731, 2022. 4\n[BCMT23] Soheil Behnezhad, Moses Charikar, Weiyun Ma, and Li-Yang Tan. Single-pass streaming algorithms for correlation clustering. In Proceedings of the 2023 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 819-849, 2023. 4\n[BGU13] Francesco Bonchi, Aristides Gionis, and Antti Ukkonen. Overlapping correlation clustering. Knowledge and Information Systems, 35(1):1-32, 2013. 2\n[BRS11] Boaz Barak, Prasad Raghavendra, and David Steurer. Rounding semidefinite programming hierarchies via global correlation. In Proceedings of 52nd Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 472-481, 2011. 1\n[BS06] Nikhil Bansal and Maxim Sviridenko. The Santa Claus problem. In Proceedings of the 38th Annual ACM Symposium on Theory of Computing (STOC), pages 31-40, 2006. 2\n[CAL22] Vincent Cohen-Addad and Euiwoong Lee. Johnson coverage hypothesis: Inapproximability of k-means and k-median in $l_{p}$-metrics. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1493-1530. SIAM, 2022. 9, 45\n[CALP+24] Vincent Cohen-Addad, David Rasmussen Lolck, Marcin Pilipczuk, Mikkel Thorup, Shuyi Yan, and Hanwen Zhang. Combinatorial correlation clustering, 2024. 48\n\n[CCMU21] M\u00e9lanie Cambus, Davin Choo, Havu Miikonen, and Jara Uitto. Massively parallel correlation clustering in bounded arboricity graphs. In 35th International Symposium on Distributed Computing (DISC), volume 209 of LIPIcs, pages 15:1-15:18, 2021.4\n[CDK14] Flavio Chierichetti, Nilesh Dalvi, and Ravi Kumar. Correlation clustering in MapReduce. In Proceedings of the 20th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD), pages 641-650, 2014. 4\n[CGW05] Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. Clustering with qualitative information. Journal of Computer and System Sciences, 71(3):360-383, 2005. 2, 3, 55\n[CHS24] Nairen Cao, Shang-En Huang, and Hsin-Hao Su. Breaking 3-factor approximation for correlation clustering in polylogarithmic rounds. In Proceedings of the 2024 ACM-SIAM Symposium on Discrete Algorithms (SODA), 2024. 4\n[CKK ${ }^{+}$06] Shuchi Chawla, Robert Krauthgamer, Ravi Kumar, Yuval Rabani, and D. Sivakumar. On the hardness of approximating multicut and sparsest-cut. Computational Complexity, 15(2):94-114, 2006. 4\n$\\left[\\mathrm{CKL}^{+}\\right.$24] M\u00e9lanie Cambus, Fabian Kuhn, Etna Lindy, Shreyas Pai, and Jara Uitto. A (3+ $\\varepsilon$ )-Approximate Correlation Clustering Algorithm in Dynamic Streams. In Proceedings of the 2024 ACM-SIAM Symposium on Discrete Algorithms (SODA), 2024. 4\n[CKP08] Deepayan Chakrabarti, Ravi Kumar, and Kunal Punera. A graph-theoretic approach to webpage segmentation. In Proceedings of the 17th International conference on World Wide Web (WWW), pages 377-386, 2008. 2\n[CLLN23] Vincent Cohen-Addad, Euiwoong Lee, Shi Li, and Alantha Newman. Handling correlated rounding error via preclustering: A 1.73-approximation for correlation clustering. In Proceedings of the 64rd Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2023. $1,2,3,5,7,8,9,10,11,12,17,18,20,52$\n[CLM ${ }^{+}$21] Vincent Cohen-Addad, Silvio Lattanzi, Slobodan Mitrovic, Ashkan Norouzi-Fard, Nikos Parotsidis, and Jakub Tarnawski. Correlation clustering in constant many parallel rounds. In Proceedings of the 38th International Conference on Machine Learning (ICML), pages 2069-2078, 2021. 4,7\n[CLMP22] Vincent Cohen-Addad, Silvio Lattanzi, Andreas Maggiori, and Nikos Parotsidis. Online and consistent correlation clustering. In Proceedings of International Conference on Machine Learning (ICML), pages 4157-4179, 2022. 4\n[CLN22] Vincent Cohen-Addad, Euiwoong Lee, and Alantha Newman. Correlation clustering with Sherali-Adams. In Proceedings of 63rd Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 651-661, 2022. 1, 2, 3, 5, 8, 17, 23\n[CM23] Sayak Chakrabarty and Konstantin Makarychev. Single-pass pivot algorithm for correlation clustering. Keep it simple! In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 4\n[CMM09] Moses Charikar, Konstantin Makarychev, and Yury Makarychev. Integrality gaps for SheraliAdams relaxations. In Proceedings of the 41st Annual ACM Symposium on Theory of Computing (STOC), pages 283-292, 2009. 9, 43\n[CMSY15] Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. Near optimal LP rounding algorithm for correlation clustering on complete and complete $k$-partite graphs. In Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC), pages 219-228, 2015. 1, 2, 5, 8\n\n[CSX12] Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering sparse graphs. In Advances in Neural Information Processing Systems (Neurips), pages 2204-2212, 2012.\n[DEFI06] Erik D. Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. Correlation clustering in general weighted graphs. Theoretical Computer Science, 361(2-3):172-187, 2006.\n[DMN23] Sami Davies, Benjamin Moseley, and Heather Newman. Fast combinatorial algorithms for min max correlation clustering. arXiv preprint arXiv:2301.13079, 2023.\n[FGMS06] Lisa Fleischer, Michel X. Goemans, Vahab S. Mirrokni, and Maxim Sviridenko. Tight approximation algorithms for maximum general assignment problems. In Proceedings of the 17th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 611-620, 2006.\n[GG06] Ioannis Giotis and Venkatesan Guruswami. Correlation clustering with a fixed number of clusters. Theory of Computing, 2:249-266, 2006.\n[GS11] Venkatesan Guruswami and Ali Kemal Sinop. Lasserre hierarchy, higher eigenvalues, and approximation schemes for graph partitioning and quadratic integer programming with PSD objectives. In Proceedings of the 52nd Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 482-491, 2011.\n[H\u00e0s01] Johan H\u00e5stad. Some optimal inapproximability results. Journal of the ACM, 48(4):798-859, 2001. 45\n[HIA23] Holger Heidrich, Jannik Irmai, and Bjoern Andres. A 4-approximation algorithm for min max correlation clustering. arXiv preprint arXiv:2310.09196, 2023.\n[KCMNT08] Dmitri V. Kalashnikov, Zhaoqi Chen, Sharad Mehrotra, and Rabia Nuray-Turan. Web people search via connection analysis. IEEE Transactions on Knowledge and Data Engineering, $20(11): 1550-1565,2008$.\n[KS09] Marek Karpinski and Warren Schudy. Linear time approximation schemes for the GaleBerlekamp game and related minimization problems. In Proceedings of the 41st Annual ACM Symposium on Theory of Computing (STOC), pages 313-322, 2009.\n[LMV+21] Silvio Lattanzi, Benjamin Moseley, Sergei Vassilvitskii, Yuyan Wang, and Rudy Zhou. Robust online correlation clustering. In Advances in Neural Information Processing Systems (Neurips), pages 4688-4698, 2021.\n[MSS10] Claire Mathieu, Ocan Sankur, and Warren Schudy. Online correlation clustering. In Proceedings of 27th International Symposium on Theoretical Aspects of Computer Science (STACS), pages $573-584,2010$.\n$\\left[\\mathrm{PPO}^{+} 15\\right]$ Xinghao Pan, Dimitris S. Papailiopoulos, Samet Oymak, Benjamin Recht, Kannan Ramchandran, and Michael I. Jordan. Parallel correlation clustering on big graphs. In Advances in Neural Information Processing Systems (Neurips), pages 82-90, 2015.\n[RT12] Prasad Raghavendra and Ning Tan. Approximating CSPs with global cardinality constraints using SDP hierarchies. In Proceedings of the 23d Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 373-387, 2012. 1, 3, 11\n[Swa04] Chaitanya Swamy. Correlation clustering: Maximizing agreements via semidefinite programming. In Proceedings of the 15th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 526-527, 2004. 3, 55\n[Vel22] Nate Veldt. Correlation clustering via strong triadic closure labeling: Fast approximation algorithms and practical lower bounds. In International Conference on Machine Learning (ICML), pages 22060-22083, 2022.\n\n[VGW18] Nate Veldt, David F. Gleich, and Anthony Wirth. A correlation clustering framework for community detection. In Proceedings of the 2018 ACM World Wide Web Conference (WWW), pages 439-448, 2018. 4",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 29,
      "text": "# A Preclustering \n\nThe goal of this section is to prove Theorem 14, which is repeated below:\nTheorem 14. For any sufficiently small $\\varepsilon>0$, there exists a poly $(n, \\frac{1}{\\varepsilon})$-time algorithm that, given a Correlation Clustering instance $\\left(V, E^{+} \\uplus E^{-}\\right)$with optimal value opt (which is not given to us), produces a preclustered instance $\\left(\\mathcal{K}, E_{\\mathrm{adm}}\\right)$ such that\n\n- there exists a good clustering w.r.t $\\left(\\mathcal{K}, E_{\\mathrm{adm}}\\right)$, whose cost is at most $(1+\\varepsilon)$ opt, and\n- $\\left|E_{\\mathrm{adm}}\\right| \\leq O\\left(\\frac{1}{\\varepsilon^{2}}\\right) \\cdot$ opt.\n\nFor convenience, we assume every $u$ has a self-loop in $E^{+}$. Let $N_{u}^{+}$be the set of + neighbors of $u$; so we have $u \\in N_{u}^{+}$. Let $d_{u}^{+}=\\left|N_{u}^{+}\\right|$be its + degree. Let $\\mathcal{C}^{*}$ be the optimum clustering, and opt $=\\operatorname{obj}\\left(\\mathcal{C}^{*}\\right)$ be its cost. We assume $\\varepsilon>0$ is at most a small enough constant; we define $\\beta=0.1$ to be an absolute constant.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 30,
      "text": "## A. 1 Constructing Atoms\n\nIn the first step of the algorithm for the proof of Theorem 14, we define the set $\\mathcal{K}$ of atoms. We use any known $O(1)$-approximation algorithm for the Correlation Clustering problem to obtain a clustering; for example, we can use the 3 -approximation combinatorial algorithm of [ACN08] to obtain a clustering $\\mathcal{C}$ with $\\operatorname{obj}(\\mathcal{C}) \\leq 3 \\cdot$ opt.\n\nOur $\\mathcal{K}$ is obtained from $\\mathcal{C}$ by marking some vertices and creating singletons for them. We view $\\mathcal{K}$ both as a clustering and as the set of atoms. The algorithm is described in Algorithm 4.\n\n```\nAlgorithm 4 Construction of \\(\\mathcal{K}\\)\n    for every non-singleton \\(C \\in \\mathcal{C}\\) do\n        for every \\(u \\in C\\) do: mark \\(u\\) if \\(\\left|N_{u}^{+} \\triangle C\\right|>\\frac{\\beta}{2} \\cdot|C|\\)\n        if at least \\(\\frac{\\beta|C|}{3}\\) vertices in \\(C\\) are marked then mark all vertices in \\(C\\)\n    let \\(\\mathcal{K}\\) be the clustering obtained from \\(\\mathcal{C}\\) by removing marked vertices and creating a singleton cluster for\n        each of them\n    return \\(\\mathcal{K}\\)\n```\n\nLemma 66. $\\operatorname{obj}(\\mathcal{K}) \\leq O(1) \\cdot$ opt, where we view $\\mathcal{K}$ as a clustering.\nProof. If a cluster $C$ has less than $\\frac{\\beta|C|}{3}$ marked vertices before Step 3, then the cost incurred by separating the marked vertices in $C$ is at most $\\frac{\\beta}{3}=O(1)$ times the cost of edges incident to these vertices in $\\mathcal{C}$. On the other hand, if $C$ has at least $\\frac{\\beta|C|}{3}$ marked vertices before Step 3, the cost of edges incident to $C$ in $\\mathcal{C}$ is at least $\\frac{\\beta}{2} \\cdot|C| \\cdot \\frac{\\beta|C|}{3} \\cdot \\frac{1}{2}=\\Omega\\left(|C|^{2}\\right)$. The cost incurred by breaking $C$ into singletons is at most $\\binom{|C|}{2}$. As every edge is charged at most twice, the cost incurred by creating singletons for all marked vertices is at most $O(1) \\cdot \\operatorname{obj}(\\mathcal{C})$. So $\\operatorname{obj}(\\mathcal{K}) \\leq O(1) \\cdot$ opt as $\\operatorname{obj}(\\mathcal{C}) \\leq 3 \\cdot$ opt.\n\nLemma 67. For every non-singleton $K \\in \\mathcal{K}$, and every $u \\in K$, we have $\\left|N_{u}^{+} \\triangle K\\right|<\\beta|K|$.\nProof. Assume $K \\subseteq C$ for some $C \\in \\mathcal{C}$. So the vertices in $C \\backslash K$ are marked, the vertices in $K$ are unmarked, and $|C \\backslash K|<\\frac{\\beta|C|}{3}$. For every $u \\in K$, we have $\\left|N_{u}^{+} \\triangle C\\right| \\leq \\frac{\\beta}{2} \\cdot|C|$ because it is unmarked. Then $\\left|N_{u}^{+} \\triangle K\\right| \\leq \\frac{\\beta}{2} \\cdot|C|+\\frac{\\beta|C|}{3}=\\frac{5 \\beta}{6} \\cdot|C| \\leq \\frac{5 \\beta}{6} \\cdot \\frac{|K|}{1-\\beta / 3} \\leq \\beta|K|$ for our choice of $\\beta$.\n\nLemma 68. Consider the optimum clustering $\\mathcal{C}^{*}$. Any atom $K \\in \\mathcal{K}$ is completely inside a cluster in $\\mathcal{C}^{*}$.\n\nProof. The lemma holds trivially if $K$ is a singleton. Assume towards the contradiction that $K$ is a nonsingleton and not inside a cluster in $\\mathcal{C}^{*}$. By Lemma 67, we have that $\\left|N_{u}^{+} \\triangle K\\right|<\\beta|K|$ for every $u \\in K$. In particular, this implies that $\\left|K \\backslash N_{u}^{+}\\right|<\\beta|K|$ and $\\left|N_{u}^{+} \\backslash K\\right|<\\beta|K|$. We consider two cases as in [CLLN23].\n\nFirst, suppose no cluster in $\\mathcal{C}^{*}$ contains at least $\\frac{2|K|}{3}$ vertices in $K$. We consider the operation of removing all vertices in $K$ from their respective clusters in $\\mathcal{C}^{*}$, and creating a single cluster $K$. The saving in cost is at least $\\frac{1}{2} \\cdot|K| \\cdot\\left(\\frac{|K|}{3}-\\beta|K|\\right)-\\frac{1}{2} \\cdot|K| \\cdot \\beta|K|=\\left(\\frac{1}{6}-\\beta\\right)|K|^{2}>0$, as $\\beta=0.1$. This contradicts that $\\mathcal{C}^{*}$ is optimum.\n\nThen, consider the other case where there is some $C \\in \\mathcal{C}$ with $|C \\cap K| \\geq \\frac{2|K|}{3}$. Let $u$ be any vertex in $K \\backslash C$; it must exist as $K$ is not completely inside $C$. Then, we consider the operation of moving $u$ from its cluster to $C$. The saving in cost is at least $\\left(\\frac{2}{3}-2 \\beta\\right)|K|-\\frac{|K|}{3}=\\left(\\frac{1}{3}-2 \\beta\\right)|K|>0$ as $\\beta=0.1$, contradicting that $\\mathcal{C}^{*}$ is optimum.\n\nWith Lemma 68, we now restrict ourselves to clusterings that do not break atoms. As a result, we can then assume all the edges between two vertices in a same atom $K \\in \\mathcal{K}$ are +edges. The assumption only decreases opt and can only make the two properties of Theorem 14 harder to satisfy. We use $K_{u}$ for every $u \\in V$ to denote the atom that contains $u$. We let $k_{u}=\\left|K_{u}\\right|$.\n\nIt is convenient for us to distribute the + edges incident to an atom $K \\in \\mathcal{K}$ equally to the vertices in $K$. For every $u, v \\in V$, we define\n\n$$\nw_{u v}:=\\frac{1}{k_{u} k_{v}} \\sum_{u^{\\prime} \\in K_{u}, v^{\\prime} \\in K_{v}} 1_{u^{\\prime} v^{\\prime} \\in E^{+}}\n$$\n\nto be the probability that an edge between a random vertex in $K_{u}$ and a random vertex in $K_{v}$ is a + edge. So $w_{u v} \\in[0,1]$ for every pair $u v$; in particular, if $v \\in K_{u}$, we have $w_{u v}=1$. We shall call $w_{u v}$ the weight of the edge $u v$. For any set $u \\in V, V^{\\prime} \\subseteq V$, we define $w\\left(u, V^{\\prime}\\right):=\\sum_{v \\in V^{\\prime}} w_{u v}$ be the total weight of edges between $u$ and $V^{\\prime}$. Let $w_{u}:=w(u, V)$ be the total weight of all edges incident to $u$. In this new instance, $w_{u v}$ fraction of edge $u v$ has a + sign, and the remaining $1-w_{u v}$ fraction has a - sign; $w_{u}$ is the total fractional number of + edges incident to $u$, which can be treated as the + degree of $u$. This new instance is equivalent to the original one. Till the end of this section we focus on this instance and call it the averaged instance to distinguish it from the original one.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 31,
      "text": "# A. 2 Defining Admissible Edges \n\nIn this section, we give a $(1+O(\\varepsilon))$-approximate clustering $\\mathcal{C}_{1}^{*}$ that does not break atoms. Then we define the set $E_{\\text {adm }}$ of admissible edges so that all edges between two different clusters in $\\mathcal{C}_{1}^{*}$ are admissible. We shall bound $\\left|E_{\\text {adm }}\\right|$ in terms of $\\operatorname{obj}(\\mathcal{K})$, which is at most $O(1) \\cdot$ opt. Notice that the construction of $\\mathcal{C}_{1}^{*}$ is not a part of our algorithm for Theorem 14, but the definition of admissible edges is.\n\nLemma 69. There is a clustering $\\mathcal{C}_{1}^{*}$ of cost at most $(1+O(\\varepsilon))$ opt that does not break any atom $K \\in \\mathcal{K}$, and satisfies the following condition: For every $u, C$ with $K_{u} \\subsetneq C \\in \\mathcal{C}_{1}^{*}$, we have $w_{u, C}>\\frac{|C|}{2}+\\varepsilon \\cdot w_{u}$.\n\nProof. Start with $\\mathcal{C}_{1}^{*}=\\mathcal{C}^{*}$; notice that it does not break any atom. While the condition does not hold for some $C \\in \\mathcal{C}_{1}^{*}$ and $u \\in C$, i.e., $w_{u, C} \\leq \\frac{|C|}{2}+\\varepsilon \\cdot w_{u}$, we update $\\mathcal{C}_{1}^{*} \\leftarrow \\mathcal{C}_{1}^{*} \\backslash\\{C\\} \\cup\\left\\{C \\backslash K_{u}, K_{u}\\right\\}$. This finishes the construction of $\\mathcal{C}_{1}^{*}$; clearly it does not break any atom.\n\nWe then consider how much cost increment the procedure incurs. Focus on any iteration of the while loop. At the beginning of the iteration, the cost of edges incident to any vertex in $K_{u}$ in the clustering $\\mathcal{C}_{1}^{*}$ (w.r.t the averaged instance) is\n\n$$\n|C|-w(u, C)+w(u, V \\backslash C)=w(u, V)+|C|-2 w(u, C) \\geq w_{u}+|C|-2\\left(\\frac{|C|}{2}+\\varepsilon \\cdot w_{u}\\right)=(1-2 \\varepsilon) w_{u}\n$$\n\nThe inequalilty is by the condition of while loop.\n\nThe cost increment incurred by separating $K_{u}$ and $C \\backslash K_{u}$ in the iteration is\n\n$$\n\\begin{gathered}\nk_{u} \\cdot\\left(w\\left(u, C \\backslash K_{u}\\right)-\\left(\\left|C \\backslash K_{u}\\right|-w\\left(u, C \\backslash K_{u}\\right)\\right)\\right)=k_{u}\\left(2 w\\left(u, C \\backslash K_{u}\\right)-\\left|C \\backslash K_{u}\\right|\\right) \\\\\n\\leq k_{u} \\cdot(2 w(u, C)-|C|) \\leq 2 \\varepsilon k_{u} \\cdot w_{u}\n\\end{gathered}\n$$\n\nAgain, the second inequality is by the condition of the while loop.\nWe can charge the cost increment using the $(1-2 \\varepsilon) k_{u} \\cdot w_{u}$ cost from edges incident to $u$ : every unit cost is used to charge $\\frac{2 \\varepsilon}{1-2 \\varepsilon}$ unit cost increment. Notice that every edge is charged at most twice. So, we have $\\operatorname{obj}\\left(\\mathcal{C}_{1}^{*}\\right)-\\operatorname{obj}(\\mathcal{C}) \\leq \\frac{4 \\varepsilon}{1-2 \\varepsilon} \\cdot \\operatorname{obj}\\left(\\mathcal{C}_{1}^{*}\\right)$. This implies that $\\operatorname{obj}\\left(\\mathcal{C}_{1}^{*}\\right) \\leq(1+O(\\varepsilon)) \\operatorname{obj}\\left(\\mathcal{C}^{*}\\right)=(1+O(\\varepsilon))$ opt.\n\nWe let $\\mathcal{C}_{1}^{*}$ be the $(1+O(\\varepsilon))$-approximate clustering satisfying the properties of Lemma 69. We now proceed to the definition of $E_{\\text {adm }}$; this should be independent of $\\mathcal{C}_{1}^{*}$.\n\nLemma 70. For every cluster $C \\in \\mathcal{C}_{1}^{*}$ and two vertices $u, v \\in C$, we have $w_{u}>\\varepsilon w_{v}$.\nProof. Assume $v \\notin K_{u}$ since otherwise $w_{u}=w_{v}$. So $C$ contains both $K_{u}$ and $K_{v}$. Lemma 69 implies $w_{u} \\geq w_{u, C}>\\frac{|C|}{2}$. On the other hand, it implies $\\varepsilon \\cdot w_{v}<w(v, C)-\\frac{|C|}{2} \\leq|C|-\\frac{|C|}{2}=\\frac{|C|}{2}$. Therefore, $w_{u}>\\varepsilon w_{v}$.\n\nThen, we define a set $E^{1}$ of edges so that $u v \\in E^{1}$ if $\\varepsilon w_{v}<w_{u}<\\frac{w_{u}}{\\varepsilon}$; so $\\left(V, E^{1}\\right)$ is undirected and contains a self-loop $(u, u)$ for every $u \\in V$. By Lemma 70, we have\nCorollary 71. Every $C \\in \\mathcal{C}_{1}^{*}$ forms a clique (with self-loops) in $\\left(V, E^{1}\\right)$.\nAlso, notice that $E^{1}$ may not be a subset of $E^{+}$. We use $N_{u}^{1}$ to denote the neighbor sets of $u$ in the graph $\\left(V, E^{1}\\right)$.\nLemma 72. For any cluster $C \\in \\mathcal{C}_{1}^{*}$, any two vertices $u, v \\in C$ with $v \\notin K_{u}$ have $\\sum_{p \\in N_{u}^{1} \\cap N_{v}^{1}} w_{u p} w_{v p}>$ $\\varepsilon \\cdot\\left(w_{u}+w_{v}\\right)$.\nProof. Notice that $K_{u}, K_{v} \\subseteq C$. By Corollary 71, we have $C \\subseteq N_{u}^{1}$ and $C \\subseteq N_{v}^{1}$. By Lemma 69, we have $w(u, C)>\\frac{|C|}{2}+\\varepsilon w_{u}$, and $w(v, C)>\\frac{|C|}{2}+\\varepsilon w_{v}$. Therefore,\n\n$$\n\\begin{aligned}\n\\sum_{p \\in N_{u}^{1} \\cap N_{v}^{1}} w_{u p} w_{v p} & \\geq \\sum_{p \\in C} w_{u p} w_{v p} \\geq \\sum_{p \\in C}\\left(w_{u p}+w_{v p}-1\\right)=w(u, C)+w(v, C)-|C| \\\\\n& >\\frac{|C|}{2}+\\varepsilon w_{u}+\\frac{|C|}{2}+\\varepsilon w_{v}-|C|=\\varepsilon\\left(w_{u}+w_{v}\\right)\n\\end{aligned}\n$$\n\nThe second inequality is by that $w_{v p}, w_{u p} \\in[0,1]$.\nDefine a set $E^{2}$ of edges so that $u v \\in E^{2}$ if and only if $v \\in K_{u}$, or the following holds: $u v \\in E^{1}$ and $\\sum_{p \\in N_{u}^{1} \\cap N_{v}^{1}} w_{u p} w_{v p}>\\varepsilon\\left(w_{u}+w_{v}\\right)$. Notice that $E^{2} \\subseteq E^{1}$. Corollary 71 and Lemma 72 implies\nCorollary 73. Every $C \\in \\mathcal{C}_{1}^{*}$ forms a clique (with self-loops) in $\\left(V, E^{2}\\right)$.\nLet $d_{u}^{2}$ and $N_{u}^{2}$ denote the degree and neighbor set of $u$ in the graph $\\left(V, E^{2}\\right)$ respectively.\nLemma 74. $d_{u}^{2}-k_{u} \\leq O\\left(\\frac{1}{\\varepsilon^{2}}\\right)\\left(w_{u}-k_{u}\\right)$.\nProof. Notice that $d_{u}^{2}-k_{u}$ is precisely the number of edges $u v \\in E^{2}$ with $v \\notin K_{u}$. Consider the graph $\\left(V, E^{1}\\right)$ with edge weights $w$ : Every edge $v p \\in E^{1}$ has weight $w_{v p}$. We define the weight of a 2 -edge path $u-p-v$ in $\\left(V, E^{1}\\right)$ to be $w_{u p} w_{p v}$. By the definition of $E^{2}$, if $v \\notin K_{u}$, then $u v \\in E^{2}$ only if the total weight of 2 -edge paths of the form $u-p-v$ is at least $\\varepsilon\\left(w_{u}+w_{v}\\right)>\\varepsilon w_{u}$.\n\nFirst, consider the paths $u-p-v$ with $p \\notin K_{u}$. The total weight of all such paths (over all $p$ and $v$ ) is at most $\\left(w_{u}-k_{u}\\right) \\cdot \\frac{w_{u}}{\\varepsilon}$. This holds since the total weight of edges between $u$ and $V \\backslash K_{u}$ in the complete graph is $w_{u}-k_{u}$, and any neighbor $p$ of $u$ in $\\left(V, E^{1}\\right)$ has $w_{p}<\\frac{w_{u}}{\\varepsilon}$ by the definition of $E^{1}$. Then consider the paths\n\n$u-p-v$ with $p \\in K_{u}$ and $v \\notin K_{u}$. The total weight of all such paths is at most $k_{u}\\left(w_{u}-k_{u}\\right)$ as any $p \\in K_{u}$ has $w_{p}=w_{u}$ and $k_{p}=k_{u}$.\n\nTherefore, the total number of verticies $v \\notin K_{u}$ with $u v \\in E^{2}$ is at most\n\n$$\n\\frac{\\left(w_{u}-k_{u}\\right) \\cdot \\frac{w_{u}}{\\varepsilon}+k_{u}\\left(w_{u}-k_{u}\\right)}{\\varepsilon w_{u}} \\leq \\frac{1}{\\varepsilon^{2}}\\left(w_{u}-k_{u}\\right)+\\frac{1}{\\varepsilon}\\left(w_{u}-k_{u}\\right)=O\\left(\\frac{1}{\\varepsilon^{2}}\\right)\\left(w_{u}-k_{u}\\right)\n$$\n\nTherefore, we can define $u v$ to be admissible, i.e., in $E_{\\text {adm }}$, if $u v \\in E^{2}$ and $v \\notin K_{u}$. This finishes the description of the preclustered instance $\\left(\\mathcal{K}, E_{\\text {adm }}\\right)$. The total number of admissible edges is\n\n$$\n\\left|E_{\\mathrm{adm}}\\right|=\\frac{1}{2} \\sum_{u \\in V}\\left(d_{u}^{2}-k_{u}\\right) \\leq O\\left(\\frac{1}{\\varepsilon^{2}}\\right) \\sum_{u \\in V}\\left(w_{u}-k_{u}\\right)=O\\left(\\frac{1}{\\varepsilon^{2}}\\right) \\cdot \\mathrm{obj}(\\mathcal{K}) \\leq O\\left(\\frac{1}{\\varepsilon^{2}}\\right) \\cdot \\mathrm{opt}\n$$\n\nThe second equality is by that $\\operatorname{obj}(\\mathcal{K})=\\frac{1}{2} \\sum_{u \\in V}\\left(w_{u}-k_{u}\\right)$ and the last inequality is by Lemma 66. We can scale $\\varepsilon$ down by a constant at the beginning, so that the cost of $\\mathcal{C}_{i}^{*}$ is at most $(1+\\varepsilon)$ opt. This finishes the proof of Theorem 14.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 32,
      "text": "# B Detail of Lemma 60 \n\nWe repeat the factor-revealing SDP for convenience.\n\n$$\n\\begin{aligned}\n& \\min \\sum_{I_{i}, I_{j}, I_{k}, I_{l}} \\sum_{T \\in \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right)} \\eta_{T} \\cdot \\bar{d}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right) \\quad \\text { s.t. } \\\\\n& Q=\\sum_{I_{i}, I_{j}, I_{k}} \\operatorname{Head}^{\\boldsymbol{\\tau}}\\left(I_{i}, I_{j}, I_{k}\\right)\\left(\\sum_{I_{l}, T \\in \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right)} \\eta_{T} \\cdot C(T)\\right) \\cdot \\operatorname{Tail}\\left(I_{i}, I_{j}, I_{k}\\right) \\succeq 0 \\\\\n& F=\\sum_{I_{i}, I_{j}, I_{k}} \\operatorname{Head}^{\\boldsymbol{\\tau}}\\left(I_{i}, I_{j}, I_{k}\\right)\\left(\\sum_{I_{l}, T \\in \\mathcal{T}_{\\mathcal{D}}\\left(I_{i}, I_{j}, I_{k}, I_{l}\\right)} \\eta_{T} \\cdot A\\right) \\cdot \\operatorname{Tail}\\left(I_{i}, I_{j}, I_{k}\\right) \\succeq 0 \\\\\n& \\eta_{T} \\geq 0 \\\\\n& \\quad \\sum_{T \\in \\mathcal{T}_{\\mathcal{D}}} \\eta_{T}=1\n\\end{aligned}\n$$\n\nWe aim to demonstrate that $\\mathrm{OPT}_{\\mathrm{SDP}}=0$ for the budget functions $b^{+} \\equiv b_{1,437}^{+}$and $b^{-} \\equiv b_{1,437}^{-}$, where $\\mathrm{OPT}_{\\mathrm{SDP}}$ is the optimal value for factor-revealing SDP. We describe the specific interval choices and small tricks employed to expedite the computation of factor-revealing SDP.\n\nWe opt to divide the interval $[0,1]$ into 37 sub-intervals with the following splitting points:\n\n$$\n\\begin{gathered}\n{[0.0,0.1,0.15,0.2,0.3,0.32,1 / 3,0.34,0.36,0.38,0.4,0.42,0.43,0.44,0.45,0.46,0.47,0.48,0.49,0.5,0.51} \\\\\n0.52,0.53,0.54,0.56,0.58,0.6,0.62,0.65,0.7,0.8,0.9,0.95,0.96,0.97,0.98,0.99,1]\n\\end{gathered}\n$$\n\nFor intervals $I_{i}, I_{j}, I_{k}$ where $I_{i} \\in[0.42,0.60]$ and $I_{j} \\in[0.42,0.60]$, we discretize the $y_{u v w}$ value. For each $I_{i}, I_{j}, I_{k}$, we split the interval\n\n$$\nI_{l}=\\left[\\max \\left(0,\\left(l\\left(I_{i}\\right)+l\\left(I_{j}\\right)+l\\left(I_{k}\\right)-1\\right) / 2\\right), r\\left(I_{k}\\right)\\right]\n$$\n\ninto 10 sub-intervals, each with the same length. For triangles in different ranges but with the same $y_{u v}, y_{u w}, y_{v w}$ value, they are treated as different triangles since they affect different locations in the $Q$ matrix. However, if two triangles have the same $y_{u v}, y_{u w}, y_{v w}, y_{u v w}$ and are located in the same $I_{i}, I_{j}, I_{k}$, but are located in different $I_{l}$ intervals, they can be treated as the same triangle since they affect $Q$ in the same\n\nway. We may need to use different $\\tilde{d}$ values to lower bound $T$ when $T$ is in a different range. Assume $I_{l}^{(x)}$ and $I_{l}^{(r+1)}$ share $y_{u v w}=r\\left(I_{l}^{(r)}\\right)=l\\left(I_{l}^{(r+1)}\\right)$ value. To lower bound $\\Delta(T)-\\operatorname{cost}(T)$ in different ranges, we set $\\tilde{d}(T)=\\min _{T_{1} \\in \\mathcal{T}\\left(I_{1}, I_{j}, I_{u}, I_{l}^{(r)}\\right) \\cup \\mathcal{T}\\left(I_{1}, I_{j}, I_{u}, I_{l}^{(r+1)}\\right)} \\Delta\\left(T_{1}\\right)-\\operatorname{cost}\\left(T_{1}\\right)$. This trick reduces the number of triangles to consider when discretizing $y_{u v w}$, considering only 12 triangles instead of 20 .\n\nWe choose to create numerous subintervals, especially around 0.5 , to obtain precise estimates for ++ triangles with $y$ values of $\\left(y_{u v}=0.5, y_{u w}=0.5, y_{v w}=0, y_{u v w}=0\\right)$ and +++ triangles with $y$ values of $\\left(y_{u v}=0.5, y_{u w}=0.5, y_{v w}=0.5, y_{u v w}=0.5\\right)$. When determining the intervals, our strategy involves setting up a budget function and identifying instances where $\\mathrm{OPT}_{\\mathrm{SDP}}$ becomes negative, allowing us to refine the interval splitting for these problematic cases.\n\nApproximately 274,000 triangles are considered for the current interval configuration. The SDP solver is executed on Google Cloud with N2 CPUs, taking around 44,000 seconds to produce the $\\mathrm{OPT}_{\\mathrm{SDP}}$. It's important to note that loading all triangles into memory poses a bottleneck, requiring approximately 200 GB of memory.\n\nLower bound of the factor-revealing SDP One might wonder whether using more intervals and denser splitting of $y_{u v w}$ 's interval could demonstrate $\\mathrm{OPT}_{\\mathrm{SDP}} \\geq 0$ for a smaller budget function. Unfortunately, even with more subintervals, a significant improvement is not achievable if we only use $Q$ and $F$ as constraints.\nLemma 75. For the budget functions $b^{+} \\equiv b_{1.421}^{+}$and $b^{-} \\equiv b_{1.421}^{-}$, there does not exist intervals $I_{1}, I_{2}, \\ldots, I_{t}$ and intervals for $y_{u v w}$ such that $\\mathrm{OPT}_{\\mathrm{SDP}} \\geq 0$.\nProof. Consider the $++$ - triangle $T_{1}$ with $y$ value $\\left(y_{u v}=0.5, y_{u w}=0.5, y_{v w}=0, y_{u v w}=0\\right)$ and +++ triangles $T_{2}$ with $y$ values of $\\left(y_{u v}=0.5, y_{u w}=0.5, y_{v w}=0.5, y_{u v w}=0.5\\right)$. If $\\eta_{T_{1}}=0.75$ and $\\eta_{T_{2}}=0.25$, both $Q$ and $F$ are PSD. Additionally, $\\Delta\\left(T_{1}\\right)-\\operatorname{cost}\\left(T_{1}\\right) \\approx-0.1819$ and $\\Delta\\left(T_{2}\\right)-\\operatorname{cost}\\left(T_{2}\\right) \\approx 0.54519$, leading to $\\mathrm{OPT}_{\\mathrm{SDP}} \\leq \\eta_{T_{1}}\\left(\\Delta\\left(T_{1}\\right)-\\operatorname{cost}\\left(T_{1}\\right)\\right)+\\eta_{T_{2}}\\left(\\Delta\\left(T_{2}\\right)-\\operatorname{cost}\\left(T_{2}\\right)\\right)<0$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 33,
      "text": "# C Standard SDP Relaxation \n\nThe following is a natural SDP relaxation previously used in approximation algorithms for the maximization problem [CGW05, Swa04] augmented with triangle inequality from the standard LP relaxation.\n\n$$\n\\begin{aligned}\n\\min \\quad \\operatorname{obj}(x)=\\sum_{i j \\in E^{+}} & x_{i j}+\\sum_{i j \\in E^{-}}\\left(1-x_{i j}\\right) \\\\\n& x_{i j}=1-v_{i} \\cdot v_{j} \\quad \\forall i, j \\in V \\\\\nv_{i} \\cdot v_{j} & \\geq 0 \\quad \\forall i, j \\in V \\\\\nv_{i} \\cdot v_{i} & =1 \\quad \\forall i \\in V \\\\\nx_{i j} & \\leq x_{i k}+x_{j k} \\quad \\forall i, j, k \\in V \\\\\nv_{i} & \\in \\mathbb{R}^{n} \\quad \\forall i \\in V\n\\end{aligned}\n$$\n\nLemma 76. The integrality gap of (SDP) is at least 1.5 .\nProof. Let $S_{k}$ be a star with $k$ leaves and one center vertex with degree $k$. Here, the $\\operatorname{sdp}_{k}$ value is for the basic relaxation. Then the value $\\operatorname{opt}_{k}=k-1$. Consider the sdp solution where there are $k$ unit vectors, one for each leaf, each with a 1 in the $i^{\\text {th }}$ position. The vector corresponding to the center vertex has length $k$ and each entry is $1 / \\sqrt{k}$. Then the integrality gap is at least\n\n$$\n\\frac{\\operatorname{opt}_{k}}{\\operatorname{sdp}_{k}}=\\frac{k-1}{k-\\sqrt{k}}\n$$\n\nNotice that for $k=2,3$, the above vector configuration leads to distances that violate the triangle inequality. Thus the $k$ yielding the largest integrality gap that also does not violate the triangle inequality is $k=4$, which gives a gap of $3 / 2$.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2404.17509v2",
  "authors": [
    "Nairen Cao",
    "Vincent Cohen-Addad",
    "Euiwoong Lee",
    "Shi Li",
    "Alantha Newman",
    "Lukas Vogl"
  ],
  "categories": [
    "cs.DS"
  ],
  "abstract": "In the classic Correlation Clustering problem introduced by Bansal, Blum, and\nChawla~(FOCS 2002), the input is a complete graph where edges are labeled\neither $+$ or $-$, and the goal is to find a partition of the vertices that\nminimizes the sum of the +edges across parts plus the sum of the -edges within\nparts. In recent years, Chawla, Makarychev, Schramm and Yaroslavtsev~(STOC\n2015) gave a 2.06-approximation by providing a near-optimal rounding of the\nstandard LP, and Cohen-Addad, Lee, Li, and Newman~(FOCS 2022, 2023) finally\nbypassed the integrality gap of 2 for this LP giving a $1.73$-approximation for\nthe problem.\n  In order to create a simple and unified framework for Correlation Clustering\nsimilar to those for {\\em typical} approximate optimization tasks, we propose\nthe {\\em cluster LP} as a strong linear program that might tightly capture the\napproximability of Correlation Clustering. It unifies all the previous\nrelaxations for the problem.\n  We demonstrate the power of the cluster LP by presenting a simple rounding\nalgorithm, and providing two analyses, one analytically proving a\n1.49-approximation and the other solving a factor-revealing SDP to show a\n1.437-approximation. Both proofs introduce principled methods by which to\nanalyze the performance of the algorithm, resulting in a significantly improved\napproximation guarantee.\n  Finally, we prove an integrality gap of $4/3$ for the cluster LP, showing our\n1.437-upper bound cannot be drastically improved. Our gap instance directly\ninspires an improved NP-hardness of approximation with a ratio $24/23 \\approx\n1.042$; no explicit hardness ratio was known before.",
  "updated": "2025-04-09T16:04:21Z",
  "published": "2024-04-26T16:23:53Z"
}