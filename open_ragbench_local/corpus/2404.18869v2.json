{
  "title": "Learning Mixtures of Gaussians Using Diffusion Models",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nWe give a new algorithm for learning mixtures of $k$ Gaussians (with identity covariance in $\\mathbb{R}^{n}$ ) to TV error $\\varepsilon$, with quasi-polynomial $\\left(O\\left(n^{\\operatorname{poly} \\log \\left(\\frac{n+k}{\\varepsilon}\\right)}\\right)\\right)$ time and sample complexity, under a minimum weight assumption. Our results extend to continuous mixtures of Gaussians where the mixing distribution is supported on a union of $k$ balls of constant radius. In particular, this applies to the case of Gaussian convolutions of distributions on low-dimensional manifolds, or more generally sets with small covering number, for which no sub-exponential algorithm was previously known. Unlike previous approaches, most of which are algebraic in nature, our approach is analytic and relies on the framework of diffusion models. Diffusion models are a modern paradigm for generative modeling, which typically rely on learning the score function (gradient log-pdf) along a process transforming a pure noise distribution, in our case a Gaussian, to the data distribution. Despite their dazzling performance in tasks such as image generation, there are few end-to-end theoretical guarantees that they can efficiently learn nontrivial families of distributions; we give some of the first such guarantees. We proceed by deriving higherorder Gaussian noise sensitivity bounds for the score functions for a Gaussian mixture to show that that they can be inductively learned using piecewise polynomial regression (up to polylogarithmic degree), and combine this with known convergence results for diffusion models.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "# Contents \n\n1 Introduction and main results ..... 3\n1.1 Main results ..... 3\n1.2 Related work ..... 7\n1.3 Notation ..... 9\n2 Proof overview ..... 10\n2.1 Learning with diffusion models ..... 11\n2.2 Learning the score for a single cluster ..... 12\n2.3 Learning the score for multiple clusters ..... 13\n3 Diffusion models ..... 14\n3.1 Convergence guarantees for diffusion models ..... 14\n3.2 Score function computation for Gaussian mixture ..... 15\n4 Learning the score for a single cluster ..... 16\n4.1 Calculation of $\\mathscr{L}^{m} f$ ..... 17\n4.2 Bounding $\\mathscr{L}^{m} f$ ..... 19\n4.3 Approximation with polynomial ..... 21\n5 From one to multiple clusters ..... 24\n5.1 Estimation with the \"nearby\" score ..... 25\n5.2 Approximation within a Voronoi cell ..... 27\n5.3 Piecewise polynomial regression ..... 29\n5.3.1 Moment and high-probability bounds ..... 31\n5.3.2 Generalization gap ..... 38\n5.4 Maintaining warm starts ..... 39\n5.5 Proof of Theorem 1.2 \\& Corollary 1.3 ..... 42\nA Proof of Theorem 3.1 ..... 48\nB Inequalities ..... 51\nC Bounding $T V\\left(P_{0}, P_{\\sigma^{2}}\\right)$ ..... 52\nD Sub-exponential random variables ..... 52\nE Derivation of Tweedie's formula ..... 53",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 1 Introduction and main results \n\nWe address the problem of learning generalized mixture of Gaussians (with identity covariance) from samples, using the framework of diffusion models. Formally, we wish to learn the following distribution on $\\mathbb{R}^{n}$ from iid samples:\n\n$$\nP_{0}=Q_{0} * \\mathcal{N}\\left(0, \\sigma_{0}^{2} I_{n}\\right)\n$$\n\nwhich we think of as a (possibly continuous) mixture of Gaussians, where $Q_{0}$ is the distribution of the means. We assume that the support of $Q_{0}$ is contained within $k$ Euclidean balls, each holding a non-trivial amount of mass. A precise definition will be provided shortly in Assumption 1.1. In the special case that $Q_{0}=\\sum_{j=1}^{k} \\alpha_{j} \\delta_{\\mu_{j}}$, this is exactly a mixture of $k$ Gaussians; however, our results hold for any generalized mixture of Gaussians under Assumption 1.1. Note that if the covariance is known but non-identity, we can first transform the data to be in this setting. Our goal is distribution learning, to output samples from a distribution $\\varepsilon$-close in TV distance to the actual one.\n\nOur motivation for this class is twofold: First, as mixtures of Gaussians are one of the simplest but nevertheless challenging mixture models, this is a classic learning problems in statistics and computer science. As a special case, our result also obtains a completely new method distinct from the common algebraic approaches for learning a discrete mixtures of $k$ Gaussians, e.g., Diakonikolas and Kane (2020) which obtains quasi-polynomial complexity for that setting (see Section 1.2 for a discussion of this and other related work). Second, diffusion models are an empirically successful paradigm for generative modeling, which work well for learning multimodal distributions in practice but for which theoretical guarantees are lacking. By applying diffusion models to the problem of learning generalized Gaussian mixtures, our work is the first to give theoretical grounding to the success of diffusion models by fully learning a highly non-trivial class of distributions without assuming oracle access to the score function estimates. This necessitates solving the problem of learning the score function in sub-exponential time. Interestingly, our approach to learning the score across multiple noise levels-essential for executing the diffusion process-actually leverages the diffusion process itself via maintaining a set of \"warm-starts.\"",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "### 1.1 Main results\n\nFor $x_{0} \\in \\mathbb{R}^{n}$, let $B_{R}\\left(x_{0}\\right)=\\left\\{x \\in \\mathbb{R}^{n}:\\left|x-x_{0}\\right| \\leq R\\right\\}$ denote the closed ball of radius $R$ around $x$. We make the following assumptions on $Q_{0}$.\n\nAssumption 1.1 ( $k$-locality): Fix $R_{0} \\geq 1, D$, and $k$. The following hold:\n\n1. For every point $\\mu$ in the support of $Q_{0}$, we have $Q_{0}\\left(B_{R_{0}}(\\mu)\\right) \\geq \\alpha_{\\min }$.\n2. There exist $\\bar{\\mu}_{1}, \\ldots, \\bar{\\mu}_{k}$ such that the support of $Q_{0}$ is contained in $\\bigcup_{i=1}^{k} B_{R_{0}}\\left(\\bar{\\mu}_{i}\\right)$.\n3. $Q_{0}\\left(B_{D}(0)\\right)=1$.\n\nNote that generalized mixture models are a strict generalization of a mixture of $k$ Gaussians: we permit mixtures of $k$ arbitrary distributions supported on balls of radius $R_{0}$, convolved with Gaussians. Our main theorem is that these mixtures can be learned with quasi-polynomial time and samples with an algorithm based on diffusion models.\n\nTheorem 1.2. Given $\\varepsilon>0$ with $\\varepsilon \\leq \\min \\left\\{\\frac{1}{2}, \\frac{\\sigma_{0}}{R_{0}}, \\frac{1}{D}, \\frac{1}{n}, \\alpha_{\\min }\\right\\}$, and given Assumption 1.1, Algorithm 1 learns a distribution that is $\\varepsilon$-close in TV distance to $P_{0}$ with time and sample complexity\n\n$$\n\\left(n \\ln \\left(\\frac{1}{\\delta}\\right)\\right)^{O}\\left(\\left(\\ln \\left(\\frac{1}{\\varepsilon}\\right)^{3}+\\left(\\frac{R_{0}}{\\sigma_{0}}\\right)^{6}\\right) \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4}\\right)\n$$\n\nwith probability $\\geq 1-\\delta$.\nNote that because of the restriction on $\\varepsilon, \\ln \\left(\\frac{1}{\\varepsilon}\\right)$ implicitly has logarithmic dependence on $D$, $n$, and $\\frac{1}{\\alpha_{\\min }} \\geq k$. In the case where $\\frac{R_{0}}{\\sigma_{0}}$ is a constant, we can remove the dependence on $\\frac{R_{0}}{\\sigma_{0}}$. We further remark that in the case of a (discrete) mixture of $k$ Gaussians, a straightforward SVD preprocessing step can replace the dependence on $n$ to $\\min \\{n, k\\}$, at an extra additive cost polynomial in $n$ (see e.g., Vempala and Wang (2004)).\n\nWe note that as a non-parametric family of distributions, it is already highly non-trivial that a $k$ local generalized mixture of Gaussians has sample complexity that is sub-exponential in dimension. This is an interesting example of a class which does not suffer from the curse of dimensionality in either the sample or time complexity.\n\nAlgorithm 1 consists of two parts: The \"learning\" part involves learning the score functions of distributions that bridge the data distribution with a pure noise (Gaussian) distribution. Once these scores are obtained, the \"generation\" part uses these learned score functions and can generate as many samples as desired. The high-probability bound is over the learning part: with high probability, the learned score functions are such that the generation process satisfies the TV distance bound.\n\nFor the learning part, a step size schedule is chosen that would allow generation with specified error; the algorithm sequentially learns the score functions from large to small $t_{\\ell}$ (time or noise level). As the noise level $t_{\\ell}$ decreases, a set of warm starts (or cluster centers) $\\mathcal{C}_{\\ell}$ at the current resolution is maintained and updated. Initially (at the highest noise level $t_{N_{\\text {step }}}$ ), all points belong to the same cluster, so $\\mathcal{C}_{N_{\\text {step }}}=\\{0\\}$. At each time step, following the recipe for learning diffusion models, we cast the score estimation problem for $P_{t_{\\ell}}=P_{0} * \\mathcal{N}\\left(0, t_{\\ell} I_{n}\\right)$ as a supervised learning problem involving denoising data points. The score function is learned within the family of piecewise low-degree polynomials, whose regions are given by the Voronoi diagram of the cluster centers. This is a polynomial regression problem that can be efficiently solved. Whenever the noise level $t_{\\ell}$ is reduced by a constant factor, the set of warm starts $\\mathcal{C}_{\\ell}$ is refined using Algorithm 2. Algorithm 2 uses the current score estimate to denoise the data points and obtain estimates of the means, accurate at the current resolution. These means are clustered to obtain $\\mathcal{C}_{\\ell-1}$. Finally, the generation part follows the generation procedure for a diffusion model: start with a Gaussian sample, and follow the reverse SDE with the score estimate to obtain a sample from the learned data distribution.\n\nA special case of Theorem 1.2 is the problem of learning a distribution that is equal to a distribution on a low-dimensional manifold convolved with a Gaussian. The \"manifold\" assumption that we need is much weaker: simply that it has can be covered by $C^{l}$ balls of radius $R_{0}$, for some constant $C$, where $l>0$ is the parameter that governs the sample complexity. It is straightforward to obtain the following.\n\nCorollary 1.3. Fix $\\sigma_{0}=1$ and constants $R_{0}, C>1$. Let $0<\\varepsilon<\\frac{1}{2}$. Suppose that $Q_{0}$ is supported on a set $M$ such that $M$ has radius $D, M$ can be covered with $C^{l}$ balls of radius $R_{0}$, and such that for every point $\\mu$ in the support of $Q_{0}, Q_{0}\\left(B_{R_{0}}(\\mu)\\right) \\geq \\frac{\\varepsilon}{C^{l}}$. Then Algorithm 1 learns a distribution\n\n```\nAlgorithm 1 Learning Gaussian mixture with diffusion model\n    Input: Error \\(\\varepsilon\\), failure probability \\(\\delta\\), sample access to mixture \\(P_{0}\\) satisfying Assumption 1.1.\n    \\(\\triangleright\\) Learning\n    Let \\(t_{1}=\\frac{\\varepsilon^{2} \\sigma_{0}^{2}}{2 \\sqrt{n}}\\) and choose a step size schedule \\(t_{1}<\\cdots<t_{N_{\\text {step }}}\\) as in Theorem 3.1.\n    Let \\(d=\\Theta\\left(\\left(\\ln \\left(\\frac{1}{\\varepsilon}\\right)^{3}+\\left(\\frac{R_{0}}{\\sigma_{0}}\\right)^{6}\\right) \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4}\\right)\\) for an appropriate constant. \\(\\triangleright\\) Degree of polynomial\n    approximation\n    Set \\(\\mathcal{C}_{N_{\\text {step }}}=\\{0\\}\n                            \\(\\triangleright\\) Set of warm starts\n    for \\(\\ell\\) from \\(N_{\\text {step }}\\) to 1 do\n        Let \\(V_{1}, \\ldots, V_{k_{\\ell}}\\) be the Voronoi partition induced by \\(\\mathcal{C}_{\\ell}=\\left\\{\\widehat{\\mu}_{1}, \\ldots, \\widehat{\\mu}_{k_{\\ell}}\\right\\}\\).\n        Draw \\(N=\\left(n \\ln \\left(\\frac{1}{\\delta}\\right)\\right)^{\\Theta(d)}\\) samples \\(x_{1}, \\ldots, x_{N} \\sim P_{0}\\) and let \\(y_{i}=x_{i}+\\sqrt{t_{\\ell}} \\cdot \\xi_{i}, \\xi_{i} \\sim \\mathcal{N}\\left(0, I_{n}\\right)\\).\n    \\(\\triangleright\\) Construct the dataset for the denoising objective, for learning the score function.\n        for \\(j\\) from 1 to \\(k_{\\ell}\\) do\n            Let \\(\\sigma_{\\ell}^{2}=t_{\\ell}+\\sigma_{0}^{2}\\) and\n```\n\n$$\n\\left.\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}=\\underset{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k}} \\leq d \\in B_{D}(0)}{\\arg \\min }\\sum_{i: y_{i} \\in V_{j}}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right)-\\left(\\left(1-\\frac{\\sigma_{\\ell}^{2}}{t_{\\ell}}\\right) y_{i}+\\frac{\\sigma_{\\ell}^{2}}{t_{\\ell}} x_{i}\\right)\\right|^{2}\n$$\n\nwhere $h_{\\mathbf{k}}$ is the multivariate Hermite polynomial with multi-index $\\mathbf{k}$ and variance $\\sigma^{2}$ (see Section 1.3). $\\triangleright$ Perform piecewise polynomial regression according to Voronoi partition, for the denoising objective.\n10: end for\n11: Define $\\widehat{y}_{\\ell}(y):=\\sum_{i=1}^{k_{\\ell}} \\mathbb{1}_{V_{i}}(y) \\sum_{|\\mathbf{k}| \\leq d} \\widehat{h}_{\\mathbf{k}}^{(i)} h_{\\mathbf{k}}\\left(y-\\widehat{\\mu}_{i}\\right)$ and $s_{\\ell}(y)=\\frac{\\widehat{y}_{\\ell}(y)-y}{\\sigma_{\\ell}^{2}}$.\n$\\triangleright$ Define the piecewise polynomial and the score estimate.\n12: if time $t_{\\ell}$ has halved since last computation of warm starts then\n13: $\\quad$ Run Algorithm 2 with fresh samples, and failure probability $\\frac{\\delta}{2 N_{\\text {step }}}$ to obtain $\\mathcal{C}_{\\ell-1}$.\n$\\triangleright$ Recompute warm starts by clustering data points denoised using the score estimate.\n14: else let $\\mathcal{C}_{\\ell-1}=\\mathcal{C}_{\\ell}$\n15: end if\n16: end for\n17: Output: Score functions $s_{1}, \\ldots, s_{N_{\\text {step }}}$.\n18: Input: Score functions $s_{1}, \\ldots, s_{N_{\\text {step }}}$ for times $t_{1}<\\cdots<t_{N_{\\text {step }}}$ $\\triangleright$ Generation\n19: Draw $\\widehat{y}_{t_{N_{\\text {step }}}} \\sim \\mathcal{N}\\left(0, t_{N_{\\text {step }}} I_{n}\\right)$.\n20: for $\\ell$ from $N$ to 2 do\n21: Let $\\widehat{y}_{t_{\\ell-1}}=\\widehat{y}_{t_{\\ell}}+2\\left[\\left(t_{\\ell}-1\\right)-\\sqrt{\\left(t_{\\ell-1}-1\\right)\\left(t_{\\ell}-1\\right)}\\right] s_{t_{\\ell}}\\left(\\widehat{y}_{t_{\\ell}}\\right)+\\sqrt{t_{\\ell}-t_{\\ell-1}} \\cdot \\xi_{t_{\\ell}}$ where $\\xi_{t_{\\ell}} \\sim$ $\\mathcal{N}\\left(0, I_{n}\\right)$\n$\\triangleright$ Discretization of reverse SDE\n22: end for\n23: Output: $\\widehat{y}_{t_{1}}$\nthat is $\\varepsilon$-close in TV distance to $P_{0}$ with time and sample complexity $\\left(n \\ln \\left(\\frac{1}{\\delta}\\right)\\right)^{O\\left(l+\\ln \\left(\\frac{n D}{\\varepsilon}\\right)\\right)^{*}}$ with probability $\\geq 1-\\delta$.\n\nWe note that this is a setting where diffusion models can provably learn under a manifold\n\nassumption, but in contrast to most prior work, the distribution cannot be learned using straightforward methods such as through binning or kernel density estimation. For example, if $M$ can be covered with $\\left(\\frac{C}{\\varepsilon}\\right)^{l}$ balls of radius $\\varepsilon$, then learning a distribution exactly supported on $M$ can be done to Wasserstein distance $\\varepsilon$ with complexity $\\widetilde{O}\\left(\\left(\\frac{C}{\\varepsilon}\\right)^{l}\\right)$ simply with a binning procedure. However, we consider learning a distribution on $M$ convolved with a Gaussian, which is a more challenging problem.\n\nAdditionally, the complexity of our algorithm (stated in Theorem 1.2) for the special case of discrete mixtures of $k$ Gaussians is known using a completely different algorithm based on algebraic methods (Diakonikolas and Kane, 2020) (without the dependence on $\\alpha_{\\min }$ or $D$ and with better exponents). More precisely, their result learns a distribution $\\varepsilon$-close in TV distance to the mixture with time and sample complexity poly $(n k / \\varepsilon)+(k / \\varepsilon)^{O\\left(\\ln ^{2} k\\right)}$, and outputs a density function. Though their algorithm does obtain better dependencies, their result relies essentially on a discrete mixture structure, while our result holds for the much larger, non-parametric family of generalized mixtures; to our knowledge the extension to a generalized mixture is novel. (Because their algorithm relies on finding an $\\varepsilon$-cover of possible parameters, and an $\\varepsilon$-cover of a constant-radius ball is exponential in dimension, it seems unlikely that their methods extend to this setting.) It is interesting to note that the generic framework of diffusion models allows us to match (up to polylogarithmic factors in the exponent) guarantees obtainable using more specialized algebraic procedures.\n\nNote that we do not proceed by learning the density function; instead, \"learning\" the distribution means that we have a procedure to generate an additional sample, each step of which involves evaluation of a learned score function. It may be possible to upgrade this guarantee to a guarantee of learning the density (Qin and Risteski, 2023).\n\nWe leave open the questions of removing the requirement on $\\alpha_{\\min }$ and improving the polynomial dependence on $\\ln \\left(\\frac{1}{\\varepsilon}\\right)$ (in Theorem 1.2) and $d$ (in Corollary 1.3). The question remains of whether the complexity of learning mixtures of Gaussians (of equal known covariance) is truly quasi-polynomial, or is actually polynomial. Since our methods work just as well in the more general setting of continuous mixtures, we believe that doing better than quasi-polynomial complexity would require an algorithm specific to a mixture of $k$ Gaussians. Further structure, e.g., hierarchical structure, could also make the problem easier, though our current analysis does not benefit from such assumptions.\n\nWe note that mixtures of Gaussians are particularly suited to learning with diffusion models because diffusions preserve the Gaussian. We expect that similar results are possible in other settings with a \"match\" between the family and the diffusion, e.g., mixtures of product distributions on the hypercube where diffusion is a random walk on the hypercube (i.e. bit-flip noise). It would be interesting to find other families of distributions which be learned using diffusion models, including families of conditional distributions such as mixtures of linear regressions.\n\nFinally, we note that in contrast to our algorithm based on piecewise polynomial regression, in practice the score function is typically learned with a neural network. The score function for a Gaussian mixture is exactly represented by a softmax neural network, which raises the question of whether it can be learned by a neural network with gradient descent. Understanding neural network training dynamics for diffusion models is an important open direction.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 1.2 Related work \n\nLearning mixtures of Gaussians. The problem of learning a mixture of Gaussians from samples has an illustrious history (Titterington et al., 1985). We first distinguish between several types of results. First, one can ask for parameter learning or distribution learning-either outputting parameters that are close to the ground-truth parameters, or simply a distribution that is close (e.g., in TV distance) to the ground-truth distribution. For distribution learning, the learning can be improper, that is, the output need not be a mixture of Gaussians. Second, one can either study the problem from an information theoretic perspective - the minimum number of samples required to get within a specified error regardless of computational cost-or computational complexity perspective - where the emphasis is on the running time of the algorithm. Below, $n$ denotes the ambient dimension, $k$ the number of components, and $\\varepsilon$ the target accuracy.\n\nMost earlier works go through parameter learning and require the means to be sufficiently separated. For identity-covariance Gaussians, Dasgupta and Schulman (2000); Arora and Kannan (2005); Vempala and Wang (2004) show that spectral methods work with a separation of at least $\\widetilde{\\Omega}\\left(\\min \\{n, k\\}^{1 / 4}\\right)$. Allowing arbitrary covariances, Moitra and Valiant (2010) show that whenever the mixture is \" $\\varepsilon$-statistically learnable,\" it can be learned with running time and sample complexity $\\exp (k) \\operatorname{poly}\\left(n, \\frac{1}{\\varepsilon}\\right)$. Several works use the sum-of-squares method (Kothari et al., 2018; Hopkins and Li, 2018) to learn a mixture with separation $\\Omega\\left(k^{\\gamma}\\right)$ in time $n^{\\operatorname{poly}(1 / \\gamma)}$. These methods extend to a broader class of mixture distributions, where components have moments which can be certifiably bounded. Liu and Li (2022) obtain a polynomial-time algorithm whenever the separation is $\\Omega\\left(\\ln \\frac{1}{2}+c k\\right)$ for constant $c>0$.\n\nSeparation conditions are unavoidable for sample-efficient parameter learning. Regev and Vijayaraghavan (2017) shows that the threshold for efficient parameter learning is $\\Theta(\\sqrt{\\ln k})$ : with separation $\\Omega(\\sqrt{\\ln k})$, polynomially many samples suffice (information-theoretically), while with separation $o(\\sqrt{\\ln k})$, super-polynomially many samples are required. Doss et al. (2020) conducts a more finegrained study of this problem.\n\nHence, any sample-efficient algorithm for learning mixtures of Gaussians without separation cannot go through parameter learning. The optimal information theoretic complexity is known up to logarithmic factors: Ashtiani et al. (2018) show a sample complexity bound in TV distance of $\\widetilde{\\Theta}\\left(k n^{2} / \\varepsilon^{2}\\right)$ for a mixture of $k$ Gaussians in $\\mathbb{R}^{n}$ (with any variance) and $\\widetilde{\\Theta}\\left(k n / \\varepsilon^{2}\\right)$ for axis-aligned Gaussians. However, their algorithms are based on brute-force search and have exponential running time. Acharya et al. (2017) give an improper nearly linear-time algorithm based on polynomial interpolation which learns a mixture of Gaussians with arbitrary variances in 1 dimension, with $\\widetilde{O}\\left(k / \\varepsilon^{2}\\right)$ samples. Most relevant for us, for a mixture of Gaussians with identity covariance, a breakthrough work by Diakonikolas and Kane (2020) uses algebraic geometry to obtains a time and sample complexity of $(k / \\varepsilon)^{O\\left(\\ln ^{2} k\\right)}$ plus polynomial factors.\n\nIn the statistics literature, the model is referred to as the Gaussian location mixture. Saha and Guntuboyina (2020); Kim and Guntuboyina (2022) consider arbitrary mixing measures $Q_{0}$ and give finite-sample bounds using non-parametric MLE (maximum likelihood estimation) for squared Hellinger risk, which scale as $\\frac{\\log N}{N}$ in terms of the number of samples $N$. The risk depends on the volume of an approximate support of $Q_{0}$, but also include a constant with unspecified dependence on the dimension $n$.\n\nWe note that the picture is more complicated when variances are unknown: Diakonikolas et al. (2017) obtain statistical query (SQ) lower bounds ( $2^{n^{D(1)}}$ queries of fixed polynomial precision) based on a connection with non-Gaussian component analysis, with a \"parallel pancake\" construction in\n\na unknown direction. Gupte et al. (2022) show that $\\log n$ components are enough to obtain a super-polynomial lower bound assuming exponential hardness of the classical LWE problem.\n\nFinally, a recent line of work considers the problem of robust learning of Gaussian mixtures (Liu and Moitra, 2021; Bakshi et al., 2022), i.e., under adversarial corruption of some fraction of samples; these methods have complexity $n^{O(k)}$.\n\nConcurrent work. During the preparation of this manuscript, we were made aware of independent and concurrent work by Chen, Kontonis, and Shah (Chen et al., 2024) which also gave guarantees for learning Gaussian mixtures using diffusion models and piecewise polynomial regression for score estimation. They consider the more general case where covariances are well-conditioned but arbitrary, but their runtime scales exponentially in $\\operatorname{poly}(k / \\epsilon)$ rather than poly $\\log (k / \\epsilon)$, which is unavoidable by SQ lower bounds (Diakonikolas et al., 2017). In contrast to our work, their work uses a different approach to polynomial approximation and does one-shot learning of parameters via spectral methods.\n\nDiffusion models. Diffusion models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Song et al., 2020) are a modern generative modeling paradigm which involves defining a forward noising process which turns a data distribution into a pure noise (e.g., Gaussian) distribution, and then learning to simulate the reverse process. For diffusion models based on SDEs (stochastic differential equations), the reverse process involves the score function (gradient of log-pdf) of the intermediate distributions; hence they are also called score-based generative models (SGM). See Tang and Zhao (2024) for a technical tutorial. We note that diffusion models are essentially a reparameterization of stochastic localization (Eldan, 2013) as pointed out by Montanari (2023). Stochastic localization has been independently studied in the probability literature and been used to obtain new results in sampling (Chen and Eldan, 2022; El Alaoui et al., 2022).\n\nTheoretical work has focused on two problems: (1) When is it possible to efficiently learn the score? (2) Given a learned ( $L^{2}$-accurate) score function, what guarantees can we obtain for sampling from the data distribution? Answers to these two questions together would give an end-to-end result for learning via diffusion models.\n\nAddressing (2), it is a remarkable fact that having a $L^{2}$-accurate score function for the sequence of distributions is sufficient for sampling under minimal distributional assumptions, allowing even multimodal distributions which cause slow mixing for local MCMC algorithms (Lee et al., 2023; Chen et al., 2023b,a). Benton et al. (2023) show that it suffices to have a number of steps linear in the dimension.\n\nQuestion (1) has proved to be thornier; it has been a challenge to obtain end-to-end results for non-trivial settings. Several works consider the problem of representability by neural networks, such as Cole and Lu (2024) for distributions whose log-density relative to a Gaussian can be represented by a low-complexity neural network, or Mei and Wu (2023) for graphical models. Following the work on neural network function approximation for smooth functions, Oko et al. (2023) give nearly minimax optimal estimation rates for densities in Besov spaces. Wibisono et al. (2024) relates score learning to kernel density estimation. The manifold assumption is another popular setting for analysis: De Bortoli (2022) considers generalization error, and Chen et al. (2023b) give learning guarantees when the distribution is supported on a subspace (however, this family of distributions can be trivially learned by first recovering the subspace). Finally, we note that score matching can be used to learn exponential families for which sampling is difficult (Koehler et al., 2022;\n\nPabbaraju et al., 2024); however, these methods only use the score for the data distribution, rather than a sequence of distributions as in a diffusion model.\n\nShah et al. (2023) consider using diffusion models to learn Gaussian mixtures, and show that diffusion models can do as well as the EM algorithm. However, they either require $k=2$, or the components to be well-separated and $O(1)$-warm starts to be given for all the means. Gaussian mixtures are a popular toy model for understanding various aspects or behavior of diffusion models, including learning behavior (Cui et al., 2023), sample complexity (Biroli and M\u00e9zard, 2023), guided diffusion (Wu et al., 2024), and critical windows (Li and Chen, 2024).\n\nAnalytic conditions for learning functions. We rely on the noise sensitivity/stability framework (Klivans et al., 2008), which shows that Gaussian noise stability (or small Gaussian surface area) implies approximability by a low-degree polynomial, giving an efficient \"low-degree algorithm\" for learning. We note a similar-in-spirit result that under the Gaussian distribution, intersections of $k$ halfspaces can be learned in time $n^{O(\\ln k)}$. The noise sensitivity framework was previously developed for boolean functions on the hypercube (Benjamini et al., 1999) and applied to learning function classes such as functions of halfspaces (Klivans et al., 2004; Kalai et al., 2008).\n\nLearnability by neural networks can also be related to complex analytic properties of the function: Schwab and Zech (2023) relate a radius of analyticity to the Hermite expansion which gives results on representability by neural networks. Learnability by neural networks for multi-index models (Bietti et al., 2023)-functions depending on the projection of the input to a few dimensions (such as the score function of a Gaussian mixture with $k \\ll n$ )\u2014is also related to the Hermite expansion of the function.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 1.3 Notation \n\nWe let $\\gamma_{\\mu, \\sigma^{2}}$ denote the density of $\\mathcal{N}\\left(\\mu, \\sigma^{2} I_{n}\\right)$, and abbreviate $\\gamma_{\\sigma^{2}}=\\gamma_{0, \\sigma^{2}}$. We abbreviate this as $\\gamma$ when $\\sigma$ is understood. In general, we denote probability measures by uppercase letters and their corresponding densities by lowercase letters, though sometimes we will conflate the two.\n\nWe use $|v|=|v|_{2}$ to denote the norm of a vector $v \\in \\mathbb{R}^{n}$, to avoid confusion with function norms. For a measure $\\nu$ on $\\Omega$, let $\\|f\\|_{L^{p}(\\nu)}=\\left(\\int_{\\Omega} f^{p} d \\nu\\right)^{1 / p}$. When the measure is clear, we may simply write $\\|f\\|_{p}$. Let $\\|f\\|_{\\nu}:=\\|f\\|_{L^{2}(\\nu)}$. For a $\\mathbb{R}^{n}$-valued function, we write $\\|f\\|_{L^{p}(\\nu)}$ to mean $\\||f|\\|_{L^{p}(\\nu)}$. For $x_{0} \\in \\mathbb{R}^{n}$, let $B_{R}\\left(x_{0}\\right)=\\left\\{x \\in \\mathbb{R}^{n}:\\left|x-x_{0}\\right| \\leq R\\right\\}$, and let $B_{R}=B_{R}(0)$. Let $\\binom{S}{k}$ denote the set of subsets of $S$ of size $k$, and $\\binom{S}{<k}$ denote the set of subsets of $S$ of size at most $k$. Let $h_{k, 1}$ be the Hermite polynomial of degree $k$, which satisfies the following recursive equation for $z \\in \\mathbb{R}$,\n\n$$\nh_{k, 1}(z)=z h_{k-1,1}(z)-h_{k-1,1}^{\\prime}(z), \\quad h_{0,1}(z)=1\n$$\n\nand define the rescaled version (with variance $\\sigma^{2}$ ) to be $h_{k, \\sigma^{2}}(z)=h_{k, 1}(z / \\sigma)$. We use the notation $\\mathbf{k}$ to denote a multi-index in $\\mathbb{N}_{0}^{n}$. The multivariate Hermite polynomials, indexed by $\\mathbf{k} \\in \\mathbb{N}_{0}^{n}$ is defined as $h_{\\mathbf{k}, \\sigma^{2}}(z)=\\prod_{i=1}^{n} h_{\\mathbf{k}_{i}, \\sigma^{2}}\\left(z_{i}\\right)$, where $z \\in \\mathbb{R}^{n}$. It is well-known that $\\left(h_{\\mathbf{k}, \\sigma^{2}}\\right)_{\\mathbf{k} \\in \\mathbb{N}_{0}^{n}}$ forms an orthogonal basis for $\\mathbb{R}^{n}$ with respect to $\\gamma_{\\sigma^{2}}$.\n\nWe give some further background and notation on Markov semigroups and generators in Section 4.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "# 2 Proof overview \n\nDiffusion models give a way to reduce the problem of learning a probability distribution $P_{0}$ from samples, to the problem of estimating score functions of $P_{0} * \\mathcal{N}\\left(0, t I_{n}\\right)$ for a sequence of noise levels $t$. A sequence of works (Lee et al., 2023; Chen et al., 2023b,a; Benton et al., 2023) shows that this reduction works for general distributions, with explicit error bounds.\n\nOur proof then proceeds by sequentially learning the score functions for a decreasing sequence of $t$ 's. The proof hinges on the fact that the score function for a mixture of Gaussians is approximable by a piecewise low-degree polynomial function. We first give some intuition as to why we expect this to be true. Consider a mixture of two Gaussians $P=\\frac{1}{2} \\mathcal{N}(-\\mu, 1)+\\frac{1}{2} \\mathcal{N}(\\mu, 1)$. The score function for this distribution is\n\n$$\n\\nabla \\ln \\left(e^{-\\frac{(x-\\mu)^{2}}{2}}+e^{-\\frac{(x+\\mu)^{2}}{2}}\\right)=-x+\\mu \\tanh (\\mu x)\n$$\n\nConsider two cases:\n\n1. When $\\mu$ is bounded, the two Gaussians have non-negligible overlap. Using smoothness properties of $\\tanh$, we can approximate the score function uniformly with a low-degree polynomial on $[-\\mu-C, \\mu+C]$.\n2. When $\\mu \\rightarrow \\infty$, then we can no longer uniformly approximate the score function with a lowdegree polynomial on $[-\\mu-C, \\mu+C]$, because $\\tanh (\\mu x)$ is very steep around 0 and close to flat for an interval whose length approaches $\\infty$. However, importantly, we don't need to: we only care about error with respect to $P$, which is mostly supported on $[-\\mu-C,-\\mu+C] \\cup$ $[\\mu-C, \\mu+C]$. On each of these intervals, the score function is close to flat-because the other mixture component has negligible effect-and so well-approximated by a polynomial.\n\nIn general, we can hope that we can cluster the Gaussians, show that the score function for each cluster can be approximated by a polynomial, and each cluster has negligible effect on the other clusters, giving the piecewise polynomial structure.\n\nBefore giving a detailed proof sketch, we we highlight some key techniques used in our proof which may be more generally useful.\n\n- Higher-order noise sensitivity: We first consider the case of a single cluster, that is, all the means in the mixture are within a small ball. Let $f$ be the function we wish to estimate. We use the technique of Gaussian noise sensitivity: by bounding the norm of $\\mathscr{L} f$ with respect to a Gaussian $\\gamma$, where $\\mathscr{L}$ is a differential operator, namely the generator of the Ornstein-Uhlenbeck (OU) process, we can show that $f$ is approximable by a low-degree polynomial with respect to $\\gamma$. However, this only allows approximation by a polynomial of degree $O(1 / \\varepsilon)$, leading to a sample and time complexity of $n^{O(1 / \\varepsilon)}$. To overcome this, we instead prove higher-order noise sensitivity bounds, bounding $\\left\\|\\mathscr{L}^{m} f\\right\\|_{\\gamma}$ for $m$ logarithmically large. To our knowledge, this is the first time that higher-order noise sensitivity has been considered. The high level of smoothness required to bound $\\left\\|\\mathscr{L}^{m} f\\right\\|_{\\gamma}$ derives from the fact that the score function has a nice representation in terms of a posterior expectation; we bound its derivatives with careful bookkeeping.\n- Higher-order smoothing: A particularly delicate part of our proof is a change-of-measure argument. Gaussian noise sensitivity gives error bounds under the Gaussian measure $\\gamma$;\n\nhowever, we care about the error under the true data distribution $\\gamma^{\\prime}$, which is a mixture. If $f$ is the true function (related to the score function) and $g$ is the estimate, we have by the CauchySchwarz inequality that $\\|f-g\\|_{\\gamma^{\\prime}}^{2} \\leq\\|f-g\\|_{\\gamma}^{2}+\\|f-g\\|_{L^{4}(\\gamma)}^{2} \\chi^{2}\\left(\\gamma^{\\prime} \\| \\gamma\\right)^{1 / 2}$. However, this means that we need $\\|f-g\\|_{L^{4}(\\gamma)}$ to be small, while we only have control over $\\|f-g\\|_{L^{3}(\\gamma)}$. A standard way that we can bound a higher $L^{p}$ norm by a lower one is to use hypercontractivity: smooth $f$ by the OU semigroup to obtain $\\mathscr{P}_{t} f$. Again, this \"first-order\" smoothing turns out to be insufficient, and we introduce a higher-order smoothing obtained from a higher-order finite-differencing.\n\n- Using the diffusion model for maintaining clusters: In the case of a single cluster, the above argument shows the existence of a logarithmic-degree polynomial approximation of the score function, which can be efficiently learned using polynomial regression. In the general case, by localizing the effect on the score function from different clusters, there exists a piecewise polynomial approximation. The pieces can be taken to be the Voronoi cells of a suitable set of cluster centers, and the score function can be efficiently learned if these cluster centers were known. The key observation is that having an estimate of the score function at the previous (higher) noise level allows this approximate clustering: by Tweedie's formula, the score function exactly points in the direction of denoising a data point, i.e., the posterior of the Gaussian mean that the point came from! Thus, the denoising principle of the diffusion model is an integral part of inductively allowing us to maintain the clusters as the noise level $t$ decreases. As $t$ decreases, we obtain more accurate estimates of the means, which allows us to refine the clusters to the current resolution.\n\nThe first two techniques are already present in the single-cluster setting, while the last one is a key part of extending the argument to the multiple-cluster setting.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 2.1 Learning with diffusion models \n\nThe idea behind diffusion models is to first define a forward process based on a SDE that takes the data distribution to a pure noise distribution, in our case a Gaussian. Then using a result on reversing a SDE (Anderson, 1982), we can write down the reverse process which involves the score function. We apply this in the case when the forward process is simply Brownian motion,\n\n$$\nd x_{t}=d W_{t}, \\quad x_{0} \\sim P_{0}\n$$\n\nto obtain that this process on $[0, T]$ is equivalently described by\n\n$$\nd x_{t}=\\nabla \\ln p_{t}\\left(x_{t}\\right)+d \\widetilde{W}_{t}, \\quad 0 \\leq t \\leq T, \\quad x_{T} \\sim P_{T}\n$$\n\nwhere $x_{t}$ has distribution $p_{t}$ and $\\widetilde{W}_{t}$ is reverse Brownian motion. We choose $T$ large enough so that $P_{T}$ is close to Gaussian. Hence, if we learn the score functions $\\nabla \\ln p_{t}\\left(x_{t}\\right)$, then we can approximately simulate the reverse process. In Section 3, we make this precise by adapting known convergence results on diffusion models, which show that we can approximately sample from the data distribution given a $L^{2}$-accurate score function.\n\nBy Tweedie's formula, the score function admits an interpretation in terms of the posterior mean given an observation (see (4)), so the score matching objective $\\mathbb{E}_{p_{t}}\\left|\\nabla \\ln p_{t}-s\\right|^{2}$ can be rewritten as the supervised denoising auto-encoder objective (see Section 3.2). The problem is now reduced to that of learning the score function $\\nabla \\ln p_{t}$ for each time $t$. For this, we will show that it resides in a low-dimensional function class we can optimize over.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "# 2.2 Learning the score for a single cluster \n\nIn Section 4, we show that in the special case where all centers are close together, i.e. $Q_{0}$ is supported on a small Euclidean ball, there is a polynomial of low (poly $\\log k$ ) degree that approximates the score function with respect to the mixture distribution. The key enabling result is Lemma 4.1, which shows that Gaussian noise stability implies low-degree polynomial approximability, by considering the expansion in Hermite polynomials (eigenfunctions of $\\mathscr{L}$ ). In contrast to existing literature, we employ a higher-order version of noise stability which involves bounding the $L^{2}$-norm of $\\mathscr{L}^{m} f$ iterates of the generator $\\mathscr{L}$ of the Ornstein-Uhlenbeck process. Specifically, any function $f$ can be approximated by a polynomial $g$ of degree $<d$ such that\n\n$$\n\\|f-g\\|_{L^{2}\\left(\\mathcal{N}\\left(0, I_{n}\\right)\\right)} \\leq \\frac{\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{2}\\left(\\mathcal{N}\\left(0, I_{n}\\right)\\right)}}{d^{m}}\n$$\n\nAs long as we can bound $\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{2}\\left(\\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)\\right)}$ by $L^{m}$ for reasonable $L$, then the dependence of the necessary degree $d$ on the desired accuracy is $O(\\operatorname{poly} \\log (1 / \\varepsilon))$ rather than $O(1 / \\varepsilon)$.\n\nThe interpretation of the score function as a posterior mean gives us a handle on computing (Lemma 4.2 in Section 4.1) and bounding (Lemma 4.4 in Section 4.2) its derivatives. For convenience, we consider a function $f$ which is linearly related to the score function, and bound $\\mathscr{L}^{2 m} f$. Specifically, $f(y)=\\langle x\\rangle_{y}$, where $\\langle\\cdot\\rangle_{y}:=\\mathbb{E}_{P_{x \\mid y}}$ is an expectation of the posterior distribution of the mean of the Gaussian that $y$ originated from. Differentiating this creates \"replicas,\" giving expectations of higher moments under the posterior. Arguments using Jensen's and H\u00f6lder's inequalities then bounds this using Gaussian moments.\n\nHowever, one key problem remains: Lemma 4.4 gives us low-degree approximability with respect to the Gaussian $\\gamma$, not the actual mixture $\\gamma^{\\prime}=P_{t}$. A standard change-of-measure argument can bound the $L^{2}$ error under $\\gamma^{\\prime}$ by a higher $L^{p}$ error under $\\gamma$ : by the Cauchy-Schwarz inequality,\n\n$$\n\\|f-g\\|_{\\gamma^{\\prime}}^{2} \\leq\\|f-g\\|_{\\gamma}^{2}+\\|f-g\\|_{L^{4}(\\gamma)}^{2} \\chi^{2}\\left(\\gamma^{\\prime}\\|\\gamma\\right)^{1 / 2}\n$$\n\nHere, $f$ is the actual function and $g$ is the polynomial approximation. However, we are not able to directly obtain a higher $L^{p}$ polynomial approximation of $f$. The problem is that the degree of the polynomial required depends poly-logarithmically on the desired $L^{2}$ accuracy $\\varepsilon$, while passing to a higher $L^{p}$ norm results in a multiplicative factor exponential in the degree, resulting in an error $\\varepsilon e^{\\ln ^{C}(1 / \\varepsilon)} \\gg 1$.\n\nBy hypercontractivity, we know that we can bound higher $L^{p}$ norms of $\\mathscr{P}_{t} f$-the smoothing of $f$ by the Ornstein-Uhlenbeck operator for time $t$-by $\\|f\\|_{L^{2}(\\gamma)}$. We can approximate $f \\approx \\mathscr{P}_{t} f$ and then approximate $\\mathscr{P}_{t} f$ by the polynomial $g$. In order for $\\left\\|f-\\mathscr{P}_{t} f\\right\\|_{\\gamma^{\\prime}} \\leq \\varepsilon$, we need $t=O(\\varepsilon)$. However, this turns out to be not enough smoothing. We would like to write $f$ approximately as a reasonably-sized linear combination of $\\mathscr{P}_{t} f$ (for different $t$ 's), for $t$ 's that are as large as possible. The insight is that $f-\\mathscr{P}_{t} f \\approx t \\cdot \\frac{d}{d t} \\mathscr{P}_{t} f \\|_{t=0}$; higher-order finite differencing gives\n\n$$\n\\left(\\mathrm{id}-\\mathscr{P}_{t}\\right)^{m} f \\approx t^{m} \\frac{d^{m}}{d t^{m}} \\mathscr{P}_{t} f \\|_{t=0}\n$$\n\nHence, by taking $m \\sim \\ln \\left(\\frac{1}{\\varepsilon}\\right)$, we can take $t$ much larger-independent of $\\varepsilon$-and still obtain an $\\varepsilon$ approximation. This is carried out in Lemma 4.5 in Section 4.3. In summary, we approximate $f$ by a higher-order smoothing by the Ornstein-Uhlenbeck process before the polynomial approximation.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 2.3 Learning the score for multiple clusters \n\nIn Section 5 we consider the general setting of multiple clusters. Recall that as we would like to run the backwards diffusion process from pure Gaussian noise back to our mixture, we want to learn the score function $\\nabla \\ln p_{t}\\left(x_{t}\\right)$ at various times $t=t_{\\ell}$. It is no longer true that a single polynomial of low degree approximates the score function with respect to the mixture distribution. We must deal with two challenges:\n\n1. The score function now has a more complex structure, as it is the posterior mean with respect to multiple clusters of $Q_{0}$.\n2. We care about the error with respect to a mixture measure which is concentrated around $k$ clusters.\n\nHow can we overcome these issues?\nFirst assume we are given a set of \"warm starts\" $\\left\\{\\widehat{\\mu}_{i}\\right\\}_{i=1}^{k_{\\ell}}$ to the mixture measure $P_{t}=Q_{0} *$ $\\mathcal{N}\\left(0,\\left(\\sigma_{0}^{2}+t\\right) I_{n}\\right)$, in the sense that balls of radius roughly $\\widehat{O}(\\sqrt{t})$ around these warm starts cover the support of $Q_{0}$ (Definition 5.1). Then, we hope to approximate the score function by a suitable piecewise polynomial function with respect to the Voronoi diagram of the warm starts, such that, in every Voronoi cell $V_{i}$, the corresponding polynomial approximates well the score function restricted to that cell:\n\n$$\n\\widehat{g}_{\\ell}(y):=\\sum_{i=1}^{k_{\\ell}} \\mathbb{1}_{V_{i}}(y) \\sum_{|\\mathbf{k}| \\leq d} \\widehat{b}_{\\mathbf{k}}^{(i)} h_{\\mathbf{k}}\\left(y-\\widehat{\\mu}_{i}\\right)\n$$\n\nHere, $h_{\\mathbf{k}}$ is the Hermite polynomial with multi-index $\\mathbf{k}$. To reduce the problem of approximating the score in a Voronoi cell in the multiple cluster case to the single cluster case, we prove that there is a neighborhood $S_{i}$ of the Voronoi cell $V_{i}$, such that, viewing the score function as the posterior mean, we can approximate it in $V_{i}$ by \"pretending\" that the prior distribution has been restricted to the mixing measure $Q_{0}$ restricted to $S_{i}$ (Lemma 5.3 in Section 5.1). Given that the radius of this neighborhood is small, we can use this argument to overcome challenge 1. For challenge 2, we bound the R\u00e9nyi divergence of the mixture measure with respect to the Gaussian centered at the warm start point in the Voronoi cells (Lemma 5.5 in Section 5.2), and use that for a change-of-measure argument given by Lemma 4.5, to bound the approximation error under the mixture measure in terms of the approximation error under the Gaussian. Having shown the existence of the piecewise polynomial approximation by combining the Hermite approximations within all the Voronoi cells (Lemma 5.6 in Section 5.2), we can then efficiently approximate the score by applying a polynomial regression with suitable choice of degree restricted to each Voronoi cell. We analyze the sample complexity of piecewise polynomial regression in Section 5.3 by bounding the generalization gap of the squared loss of low-degree polynomial functions in predicting the score (Lemma 5.8).\n\nThe remaining problem is to obtain a set of warm starts. It turns out that we can exploit the score function learned in the previous iteration of the algorithm (for a larger $t$ ) to generate these warm-start points (Lemma 5.13, Algorithm 2 in Section 5.4); the idea is that the score function exactly points toward the \"denoising\" direction, i.e. approximately towards points sampled form the mixing measure $Q_{0}$. Therefore, adding the score function estimate $s$ to sufficiently many noisy samples $y_{i}$ is sufficient for obtaining a suitable set of warm starts:\n\n$$\n\\widehat{\\mu}_{i}=y_{i}+\\sigma^{2} s\\left(y_{i}\\right)\n$$\n\nThe $\\widehat{\\mu}_{i}$ can be greedily clustered and subselected to keep the set of warm starts small. Based on this observation, we propose an inductive recipe to iteratively estimate the score functions for different noise levels, while maintaining a warm-start set.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# 3 Diffusion models \n\nUsing the framework of diffusion models, the algorithm and analysis consist of two main parts: showing that we can learn this score function efficiently, and showing that an accurate score function allows generating more samples from the distribution. In this section, we focus on the second part, which has been well-studied.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "### 3.1 Convergence guarantees for diffusion models\n\nTwo popular parameterizations of diffusion models are the variance-exploding (VE) and variancepreserving (VP) processes. The diffusion models are also known as score matching with Langevin dynamics (SMLD) and denoising diffusion probabilistic models (DDPM), respectively. The forward SDE for the VE process is simply Brownian motion, while the forward SDE for the VP process is the Ornstein-Uhlenbeck process. Below, we lay out the forward SDE for each of these process, and the backward SDE obtained by Anderson (1982):\n\n$$\n\\begin{array}{ll}\n\\text { VE: } & \\text { VP: } \\\\\nd y_{t}=d W_{t} & d x_{t}=-x_{t} d t+\\sqrt{2} d W_{t} \\\\\nd y_{t}=-\\nabla \\ln p_{t}\\left(y_{t}\\right) d t+d \\widetilde{W}_{t} & d x_{t}=\\left(-x_{t}-2 \\nabla \\ln p_{t}\\left(x_{t}\\right)\\right) d t+\\sqrt{2} d \\widetilde{W}_{t} \\\\\nd y_{t}^{*-}=\\nabla \\ln p_{T-t}\\left(y_{t}^{*-}\\right) d t+d W_{t}^{\\prime} & d x_{t}^{*-}=\\left(x_{t}^{*-}+2 \\nabla \\ln p_{T-t}\\left(x_{t}^{*-}\\right)\\right) d t+\\sqrt{2} d W_{t}^{\\prime}\n\\end{array}\n$$\n\nwhere $p_{t}$ is the density of the either $x_{t}$ or $y_{t}, \\widetilde{W}_{t}$ is reverse Brownian motion, and $W_{t}, W_{t}^{\\prime}$ are usual Browian motions, and the reverse processes are initialized at $y_{0}^{*-} \\sim p_{T}, x_{0}^{*-} \\sim p_{T}$ and we can match up trajectories $y_{t}^{*-}=y_{T-t}, x_{t}^{*-}=x_{T-t}$ for $t \\in[0, T)$. For convenience, we work with the VE process. As most convergence guarantees in the literature are for the VP process, we need to adapt those results. Note that as continuous processes we have $x_{t}=e^{-t} y_{e^{2 t}-1}$, but some care is required when matching up the discretizations.\n\nThe following is an adaptation of (Benton et al., 2023, Theorem 1), after reparameterizing the VP into the VE process.\n\nTheorem 3.1 (Reverse KL guarantee for variance-exploding diffusion models). Let $0<t_{1}<t_{2}<$ $\\ldots<t_{N_{\\text {step }}}=T$ and $\\zeta_{k}=t_{k+1}-t_{k}$. Suppose $T \\geq 1$. Assume the following.\n\n1. We have a score function estimate $s_{t_{k}}(y)$ for every $t_{k}$ such that\n\n$$\n\\mathbb{E}_{P_{t_{k}}}\\left|\\nabla \\ln p_{t_{k}}(y)-s_{t_{k}}(y)\\right|^{2} \\leq \\varepsilon_{k}^{2}\n$$\n\n2. The data distribution has bounded second moment $M_{2}=\\mathbb{E}_{P_{0}}|y|^{2}$.\n3. For some $\\kappa<1$, the step size schedule satisfies\n\n$$\nt_{k}+1 \\geq\\left(t_{k+1}+1\\right) \\max \\left\\{e^{-2 \\kappa},\\left(t_{k+1}+1\\right)^{-\\kappa}\\right\\}\n$$\n\nLet $\\widehat{p}_{t}$ denote the distribution of the following discretization of the reverse process when initialized at $\\widehat{p}_{T}=\\mathcal{N}\\left(0,(T+1) \\cdot I_{n}\\right):^{1}$\n\n$$\n\\widehat{y}_{t_{k}}=\\widehat{y}_{t_{k+1}}+2\\left[\\left(t_{k+1}-1\\right)-\\sqrt{\\left(t_{k}-1\\right)\\left(t_{k+1}-1\\right)}\\right] s_{t_{k+1}}\\left(\\widehat{y}_{t_{k+1}}\\right)+\\sqrt{\\zeta_{k}} \\cdot \\xi_{t_{k}}\n$$\n\nThen\n\n$$\n\\mathrm{KL}\\left(p_{t_{1}} \\mid \\widehat{p}_{t_{1}}\\right) \\lesssim \\frac{n+M_{2}}{T+1}+\\sum_{k=1}^{N_{\\text {step }}-1} \\ln \\left(\\frac{t_{k+1}+1}{t_{k}+1}\\right) \\cdot \\frac{1}{t_{k}+1} \\cdot \\varepsilon_{k+1}^{2}+\\kappa n \\ln (T+1)+\\kappa^{2} n N_{\\text {step }}+\\kappa M_{2}\n$$\n\nMoreover, we can choose a schedule of length $N_{\\text {step }}=O\\left(\\frac{1}{\\kappa} \\ln \\left(\\frac{T+1}{t_{1}}\\right)\\right)$ to make assumption 3 hold. If we choose $T=\\frac{M_{2}+n}{\\varepsilon^{2}}$ and $\\kappa=\\frac{\\varepsilon^{2}}{M_{2}+n \\ln (T+1)}$, and we have $\\varepsilon_{k}^{2} \\leq \\frac{\\varepsilon^{2}\\left(t_{k}+1\\right)}{\\ln (T+1)}$ for each $k$, then the error is $O\\left(\\varepsilon^{2}\\right)$.\n\nThe proof is deferred to Appendix A.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# 3.2 Score function computation for Gaussian mixture \n\nTo learn a distribution using a standard diffusion model, we need to learn its score function (gradient of the log of the pdf) under Gaussian convolution, with varying levels of noise. For Gaussian mixtures, the score function has a particularly nice form, which we now derive.\n\nFirst, we let $P_{t}=P_{0} * \\mathcal{N}\\left(0, t I_{n}\\right)$ and $Q_{t}=Q_{0} * \\mathcal{N}\\left(0, t I_{n}\\right)$. Then $P_{t}=Q_{0} * \\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)=Q_{\\sigma^{2}}$ where $\\sigma^{2}=\\sigma_{0}^{2}+t$. We consider the following probabilistic model where $\\mu, \\xi_{1}, \\xi_{2}$ are drawn independently:\n\n$$\n\\mu \\sim Q_{0}, \\quad \\xi_{1}, \\xi_{2} \\sim \\mathcal{N}\\left(0, I_{n}\\right), \\quad X=\\mu+\\sigma_{0} \\xi_{1}, \\quad Y=X+\\sqrt{t} \\xi_{2}=\\mu+\\sigma_{0} \\xi_{1}+\\sqrt{t} \\xi_{2}\n$$\n\nThen $X \\sim P_{0}$ and $Y \\sim P_{t}$. Letting $p_{t}, q_{t}$ be the corresponding densities and $V_{t}=\\ln p_{t}$, we have by Tweedie's formula Robbins (1992) (see Appendix E for a derivation) that\n\n$$\n\\begin{gathered}\n\\nabla V_{t}(y)=\\frac{1}{\\sigma^{2}} \\mathbb{E}[\\mu-y \\mid Y=y]=\\frac{1}{\\sigma^{2}}\\left(-y+\\frac{\\int_{\\mathbb{R}^{n}} \\mu \\exp \\left(\\frac{\\langle y, \\mu\\rangle}{\\sigma^{2}}-\\frac{|\\mu|^{2}}{2 \\sigma^{2}}\\right) d Q_{0}(\\mu)}{\\int_{\\mathbb{R}^{n}} \\exp \\left(\\frac{\\langle y, \\mu\\rangle}{\\sigma^{2}}-\\frac{|\\mu|^{2}}{2 \\sigma^{2}}\\right) d Q_{0}(\\mu)}\\right) \\\\\nf_{\\sigma^{2}}(y):=y+\\sigma^{2} \\nabla V_{t}(y)=\\mathbb{E}[\\mu \\mid Y=y]=\\frac{\\int_{\\mathbb{R}^{n}} \\mu \\exp \\left(\\frac{\\langle y, \\mu\\rangle}{\\sigma^{2}}-\\frac{|\\mu|^{2}}{2 \\sigma^{2}}\\right) d Q_{0}(\\mu)}{\\int_{\\mathbb{R}^{n}} \\exp \\left(\\frac{\\langle y, \\mu\\rangle}{\\sigma^{2}}-\\frac{|\\mu|^{2}}{2 \\sigma^{2}}\\right) d Q_{0}(\\mu)}\n\\end{gathered}\n$$\n\nNote that in the same way we have\n\n$$\n\\begin{gathered}\n\\nabla V_{t}(y)=\\frac{1}{t} \\mathbb{E}[X-y \\mid Y=y]=\\frac{1}{t}\\left(-y+\\frac{\\int_{\\mathbb{R}^{n}} x \\exp \\left(\\frac{\\langle y, x\\rangle}{t}-\\frac{|x|^{2}}{2 t}\\right) d P_{0}(x)}{\\int_{\\mathbb{R}^{n}} \\exp \\left(\\frac{\\langle y, x\\rangle}{t}-\\frac{|x|^{2}}{2 t}\\right) d P_{0}(x)}\\right) \\\\\ny+t \\nabla V_{t}(y)=\\mathbb{E}[X \\mid Y=y]=\\frac{\\int_{\\mathbb{R}^{n}} x \\exp \\left(\\frac{\\langle y, x\\rangle}{t}-\\frac{|x|^{2}}{2 t}\\right) d P_{0}(x)}{\\int_{\\mathbb{R}^{n}} \\exp \\left(\\frac{\\langle y, x\\rangle}{t}-\\frac{|x|^{2}}{2 t}\\right) d P_{0}(x)}\n\\end{gathered}\n$$\n\n[^0]\n[^0]:    ${ }^{1}$ It would be more natural to have $h_{k}$ as the coefficient in front of the score term. This coefficient comes from doing a change-of-variable from Theorem A. 1 for the DDPM process, rather than reproving the theorem for this process.\n\nBecause of this identity, the score function can be learned as the minimal mean squared estimator in a (supervised) denoising problem of estimating $X$ given $Y$, known as the denoising auto-encoder (DAE) objective Vincent (2011). (3)-(4) will be useful for analysis while (5)-(6) is used for the actual learning algorithm. Because of the nice form of (4), we will actually aim to learn $f_{\\sigma^{2}}$. We remark that in the case of a finite mixture, (4) shows that $f_{\\sigma^{2}}$ is represented by a softmax neural network with 1 hidden layer of $k$ units.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# 4 Learning the score for a single cluster \n\nWe will follow the approach in Klivans et al. (2008), though with functions that are $\\mathbb{R}^{n}$-valued rather than $\\{0,1\\}$-valued.\n\nFirst, we give some background on the Ornstein-Uhlenbeck process. Let $\\gamma:=\\gamma_{\\sigma^{2}}$ denote the density of $\\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)$. The generator $\\mathscr{L}:=\\mathscr{L}_{\\sigma^{2}}$ of the scaled Ornstein-Uhlenbeck process with variance $\\sigma^{2}$, also known as Langevin dynamics for $\\gamma$, is\n\n$$\n\\mathscr{L} f(x)=-\\frac{1}{\\sigma^{2}}\\langle x, \\nabla f(x)\\rangle+\\Delta f(x)\n$$\n\nFor a $\\mathbb{R}^{d}$-valued function $f$, we interpret this componentwise, i.e.,\n\n$$\n\\mathscr{L} f(x)=-\\frac{1}{\\sigma^{2}} D f(x) x+\\sum_{i=1}^{d} \\partial_{i i} f\n$$\n\nThe eigenfunctions are the (suitably scaled) Hermite polynomials $\\left(h_{\\mathbf{k}}\\right)_{\\mathbf{k} \\in \\mathbb{N}_{0}^{n}}$, with $h_{\\mathbf{k}}:=h_{\\mathbf{k}, \\sigma^{2}}$, as defined in Section 1.3, having eigenvalue $-\\frac{|\\mathbf{k}|}{\\sigma^{2}}$. Here we use $|\\mathbf{k}|$ to denote $|\\mathbf{k}|_{1}$. The Hermite polynomials form a complete orthogonal basis for $L^{2}(\\gamma)$. The scaled Ornstein-Uhlenbeck process can be described by the SDE\n\n$$\nd x_{t}=-\\frac{1}{\\sigma^{2}} x_{t}+\\sqrt{2} d W_{t}\n$$\n\nLet $\\left(\\mathscr{P}_{t}\\right)_{t \\geq 0}$ be the Markov semigroup of the scaled Ornstein-Uhlenbeck process, with generator given by (7). That is, we have\n\n$$\n\\begin{aligned}\n\\mathscr{P}_{t} f(x) & =\\mathbb{E}_{x_{0}=x} f\\left(x_{t}\\right) \\text { when } x_{t} \\text { solves }(\\text { 8), } \\\\\n\\mathscr{L} f & =\\lim _{t \\rightarrow 0^{+}} \\frac{\\mathscr{P}_{t} f-f}{t} .\n\\end{aligned}\n$$\n\nThe following encapsulates the technique of noise sensitivity/stability: a bound on the $L^{2}(\\gamma)$ norm of $\\mathscr{L}^{m} f$ implies approximability of $f$ by a low-degree polynomial. To our knowledge, only the case $m=1$ has been considered in the literature; however, similarly to how bounds on higher moments imply better tail bounds, bounding $\\mathscr{L}^{m} f$ for larger $m$ can give better approximation.\n\nLemma 4.1 (Noise stability implies low-degree approximability). Let $\\gamma$ denote the density of $\\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)$. Suppose that $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{n}$ satisfies $\\|f\\|_{L^{2}(\\gamma)}<\\infty$, and that $m \\in \\mathbb{N}, L$ are such that $\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{2}(\\gamma)} \\leq L^{m}$. For $d \\in \\mathbb{N}$, there exists a polynomial $g$ of degree $<d$ such that\n\n$$\n\\|f-g\\|_{L^{2}(\\gamma)} \\leq\\left(\\frac{L \\sigma^{2}}{d}\\right)^{m}\n$$\n\nProof. Expand $f$ in the eigenfunction basis of $\\mathscr{L}_{\\sigma^{2}}$ as $f=\\sum_{\\mathbf{k} \\in \\mathbb{N}_{0}^{d}} a_{\\mathbf{k}} h_{\\mathbf{k}}$ where $a_{\\mathbf{k}} \\in \\mathbb{R}^{d}$. Then\n\n$$\n\\mathscr{L}^{m} f=\\sum_{\\mathbf{k} \\in \\mathbb{N}_{0}^{d}} \\frac{|\\mathbf{k}|^{m}}{\\sigma^{2 m}} a_{\\mathbf{k}} h_{\\mathbf{k}}\n$$\n\nHence\n\n$$\n\\frac{d^{2 m}}{\\sigma^{4 m}} \\sum_{|\\mathbf{k}| \\geq d}\\left|a_{\\mathbf{k}}\\right|^{2} \\leq \\sum_{\\mathbf{k} \\in \\mathbb{N}_{0}^{d}} \\frac{|\\mathbf{k}|^{2 m}}{\\sigma^{4 m}}\\left|a_{\\mathbf{k}}\\right|^{2} \\leq L^{2 m} \\Longrightarrow \\sum_{|\\mathbf{k}| \\geq d}\\left|a_{\\mathbf{k}}\\right|^{2} \\leq\\left(\\frac{L \\sigma^{2}}{d}\\right)^{2 m}\n$$\n\nTaking $g=\\sum_{|\\mathbf{k}|<d} a_{\\mathbf{k}} h_{\\mathbf{k}}$, we have that $\\|f-g\\|_{L^{2}(\\gamma)}^{2}=\\sum_{|\\mathbf{k}| \\geq d}\\left|a_{\\mathbf{k}}\\right|^{2}$, which gives the desired bound.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# 4.1 Calculation of $\\mathscr{L}^{m} f$ \n\nLet $\\nu$ be a measure on $\\mathbb{R}^{n}$ with all moments finite. Consider generating $y=x+\\sigma \\xi$ where $x \\sim \\nu$ and $\\xi \\sim \\mathcal{N}\\left(0, I_{n}\\right)$. Let $\\langle\\cdot\\rangle=\\langle\\cdot\\rangle_{y}:=\\mathbb{E}_{P(x \\mid y)}$ where $P(x \\mid y)$ is the posterior distribution given by\n\n$$\n\\frac{d P(\\cdot \\mid y)}{d \\nu}(x) \\propto \\exp \\left(\\frac{\\langle y, x\\rangle}{\\sigma^{2}}-\\frac{\\|x\\|^{2}}{2 \\sigma^{2}}\\right)\n$$\n\n(This is not to be confused with the inner product.) Then letting $f=f_{\\sigma^{2}}$ and $\\nu=Q_{0}$ as in (4), we have $f(y):=f_{\\sigma^{2}}(y)=\\langle x\\rangle_{y}$.\n\nWe first derive some formulas for differentiating posterior expectations $\\langle\\cdot\\rangle$. Let $\\widetilde{x}=x-\\langle x\\rangle_{y}$ denote the centered random variable. For a function $g: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, we have the following general formula for differentiation with respect to $y_{i}$ :\n\n$$\n\\begin{aligned}\n& \\sigma^{2} \\partial_{i}\\langle g(x)\\rangle_{y}=\\frac{\\int_{\\mathbb{R}^{n}} g(x) x_{i} \\exp \\left(\\frac{\\langle y, x\\rangle}{\\sigma^{2}}-\\frac{|x|^{2}}{2 \\sigma^{2}}\\right) \\nu(d x)}{\\int_{\\mathbb{R}^{n}} \\exp \\left(\\frac{\\langle y, x\\rangle}{\\sigma^{2}}-\\frac{|x|^{2}}{2 \\sigma^{2}}\\right) \\nu(d x)} \\\\\n&-\\frac{\\int_{\\mathbb{R}^{n}} g(x) \\exp \\left(\\frac{\\langle y, x\\rangle}{\\sigma^{2}}-\\frac{|x|^{2}}{2 \\sigma^{2}}\\right) \\nu(d x) \\int_{\\mathbb{R}^{n}} x_{i} \\exp \\left(\\frac{\\langle y, x\\rangle}{\\sigma^{2}}-\\frac{|x|^{2}}{2 \\sigma^{2}}\\right) \\nu(d x)}{\\left(\\int_{\\mathbb{R}^{n}} \\exp \\left(\\frac{\\langle y, x\\rangle}{\\sigma^{2}}-\\frac{|x|^{2}}{2 \\sigma^{2}}\\right) \\nu(d x)\\right)^{2}} \\\\\n&=\\left\\langle g(x) \\widetilde{x}_{i}\\right\\rangle_{y} \\\\\n& \\nabla\\langle g(x)\\rangle_{y}=\\frac{1}{\\sigma^{2}}\\langle g(x) \\widetilde{x}\\rangle_{y} .\n\\end{aligned}\n$$\n\nMore generally, let $\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right)\\right\\rangle_{y}$ denote $\\mathbb{E} g\\left(x^{(1)}, \\ldots, x^{(r)}\\right)$ where $x^{(1)}, \\ldots, x^{(r)}$ are independent draws from the posterior $p(\\cdot \\mid y)$. We use the trick of replacing the means with independent copies of the random variable (see e.g., Talagrand (2010)) to obtain\n\n$$\n\\begin{aligned}\n\\sigma^{2} \\nabla\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right)\\right\\rangle & =\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right)\\left(\\widetilde{x}^{(1)}+\\cdots+\\widetilde{x}^{(r)}\\right)\\right\\rangle \\\\\n& =\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right)\\left(x^{(1)}+\\cdots+x^{(r)}-r x^{(r+1)}\\right)\\right\\rangle\n\\end{aligned}\n$$\n\nand for a function $h: \\mathbb{R}^{r} \\rightarrow \\mathbb{R}$,\n\n$$\n\\begin{aligned}\n& \\nabla\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right) h\\left(\\left\\langle x^{(1)}, y\\right\\rangle, \\ldots,\\left\\langle x^{(r)}, y\\right\\rangle\\right)\\right\\rangle \\\\\n& =\\frac{1}{\\sigma^{2}}\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right) h\\left(\\left\\langle x^{(1)}, y\\right\\rangle, \\ldots,\\left\\langle x^{(r)}, y\\right\\rangle\\right)\\left(x^{(1)}+\\cdots+x^{(r)}-r x^{(r+1)}\\right)\\right\\rangle \\\\\n& \\quad+\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right) \\sum_{j} \\partial_{j} h\\left(\\left\\langle x^{(1)}, y\\right\\rangle, \\ldots,\\left\\langle x^{(r)}, y\\right\\rangle\\right) x^{(j)}\\right\\rangle\n\\end{aligned}\n$$\n\nNow suppose $h$ is a homogeneous polynomial of degree $s$. Then\n\n$$\n\\begin{aligned}\n& D\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right) h\\left(\\left\\langle x^{(1)}, y\\right\\rangle, \\ldots,\\left\\langle x^{(r)}, y\\right\\rangle\\right)\\right\\rangle y \\\\\n& =\\frac{1}{\\sigma^{2}}\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right) h\\left(\\left\\langle x^{(1)}, y\\right\\rangle, \\ldots,\\left\\langle x^{(r)}, y\\right\\rangle\\right)\\left(\\left\\langle x^{(1)}, y\\right\\rangle+\\cdots+\\left\\langle x^{(r)}, y\\right\\rangle-r\\left\\langle x^{(r+1)}, y\\right\\rangle\\right)\\right\\rangle \\\\\n& \\quad+s\\left\\langle g\\left(x^{(1)}, \\ldots, x^{(r)}\\right) h\\left(\\left\\langle x^{(1)}, y\\right\\rangle, \\ldots,\\left\\langle x^{(r)}, y\\right\\rangle\\right)\\right\\rangle\n\\end{aligned}\n$$\n\nby Euler's formula $\\sum_{j=1}^{r} \\partial_{j} h\\left(x_{1}, \\ldots, x_{j}\\right) x_{j}=s \\cdot h$.\nNow consider $h\\left(z_{1}, \\ldots, z_{r}\\right)=\\prod_{\\ell=1}^{t} z_{j_{\\ell}}$. Let $u\\left(x^{(1)}, \\ldots, x^{(r)}\\right)=g\\left(x^{(1)}, \\ldots, x^{(r)}\\right) h\\left(\\left\\langle x^{(1)}, y\\right\\rangle, \\ldots,\\left\\langle x^{(r)}, y\\right\\rangle\\right)$. We have\n\n$$\n\\begin{aligned}\n& \\Delta\\left\\langle u\\left(x^{(1)}, \\ldots, x^{(r)}\\right)\\right\\rangle=\\frac{1}{\\sigma^{4}}\\left\\langle u \\cdot\\left\\langle x^{(1)}+\\cdots+x^{(r)}-r x^{(r+1)}, x^{(1)}+\\cdots+x^{(r+1)}-(r+1) x^{(r+2)}\\right\\rangle\\right\\rangle \\\\\n& +\\frac{2}{\\sigma^{2}}\\left\\langle g \\cdot \\sum_{\\ell^{\\prime}=1}^{s} \\prod_{\\ell \\neq \\ell^{\\prime}}\\left\\langle x^{\\left(i_{\\ell}\\right)}, y\\right\\rangle \\cdot\\left\\langle x^{\\left(i_{\\ell^{\\prime}}\\right)}, x^{(1)}+\\cdots+x^{(r)}-r x^{(r+1)}\\right\\rangle\\right\\rangle \\\\\n& \\quad+\\left\\langle g \\cdot \\sum_{\\substack{1 \\leq \\ell^{\\prime}, \\ell^{\\prime \\prime} \\leq s \\\\\n\\ell^{\\prime} \\neq \\ell^{\\prime \\prime}}}\\prod_{\\ell \\neq \\ell^{\\prime}, \\ell^{\\prime \\prime}}\\left\\langle x^{\\left(i_{\\ell}\\right)}, y\\right\\rangle \\cdot\\left\\langle x^{\\left(i_{\\ell^{\\prime}}\\right)}, x^{\\left(i_{\\ell^{\\prime \\prime}}\\right)}\\right\\rangle\\right\\rangle\n\\end{aligned}\n$$\n\nApplying the above to monomials $g\\left(x^{(1)}, \\ldots, x^{(r)}\\right)=\\prod_{\\ell=1}^{s}\\left\\langle x^{\\left(i_{\\ell}\\right)}, x^{\\left(i_{\\ell}^{\\prime}\\right)}\\right\\rangle$ and using induction, we have the following.\n\nLemma 4.2 (Probabilistic interpretation of $\\mathscr{L}^{m} f$ ). Let $f=f_{\\sigma^{2}}$ be as in (4). We have\n\n$$\n\\mathscr{L}^{m} f(y)=\\left\\langle x^{(1)} \\sum_{s+t \\leq m} \\sum_{i, i^{\\prime} \\in[2 m+1]^{s}, j \\in[2 m+1]^{t}} a_{i, i^{\\prime}, j} \\sigma^{-2(s+t+m)} \\prod_{\\ell=1}^{s}\\left\\langle x^{\\left(i_{\\ell}\\right)}, x^{\\left(i_{\\ell}^{\\prime}\\right)}\\right\\rangle \\prod_{\\ell=1}^{t}\\left\\langle x^{\\left(j_{\\ell}\\right)}, y\\right\\rangle\\right\\rangle\n$$\n\nwhere $\\sum_{i, i^{\\prime}, j}\\left|a_{i, i^{\\prime}, j}\\right| \\leq 30^{m} m!^{2}$.\nProof. Recall that $\\mathscr{L} f(x)=-\\frac{1}{\\sigma^{2}} D f(x) x+\\Delta f(x)$. We induct on $m$. Suppose that the lemma holds for $m-1$; each term has at most a number of replicas $r \\leq 2 m-1$ and $s, t \\leq m-1$. We consider the effect of the map $f \\mapsto \\frac{1}{\\sigma^{2}} D f(y) \\cdot y(10)$ and $\\Delta(11)$ on a single term in (12). First, note that in each case, we obtain a factor $\\frac{1}{\\sigma^{2}}$, and a factor of $\\frac{1}{\\sigma^{2}}$ for each additional $\\left\\langle x^{\\left(i_{\\ell}\\right)}, x^{\\left(i_{\\ell}^{\\prime}\\right)}\\right\\rangle$ term as well as $\\left\\langle x^{\\left(i_{\\ell}\\right)}, y\\right\\rangle$ term (with a $\\sigma^{2}$ factor if we remove a term); this justifies the $\\sigma^{-2(s+t+m)}$ factor.\n\n1. In (10), the number of replicas $r$ in the resulting terms increases by at most $1, t$ increases by at most 1 , and the sum of absolute value of coefficients is at most\n\n$$\n2 r+s \\leq 2(2 m-1)+(m-1) \\leq 5 m-3\n$$\n\n2. In (11), the number of replicas $r$ in the resulting terms increases by at most $2, s$ increases by at most 1 , and the sum of absolute value of coefficients is at most\n\n$$\n\\begin{aligned}\n& 2 r \\cdot 2(r+1)+2 \\cdot s \\cdot 2 r+s(s-1) \\\\\n& \\leq 4 \\cdot(2 m-1)(2 m+1)+2 \\cdot(m-1) \\cdot 2(2 m-1)+(m-1)(m-2) \\\\\n& \\leq 16 m^{2}+8 m^{2}+m^{2} \\leq 25 m^{2}\n\\end{aligned}\n$$\n\nThe sum of absolute value of coefficients multiplies by at most $30 m^{2}$. This finishes the induction step.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "# 4.2 Bounding $\\mathscr{L}^{m} f$ \n\nFrom Lemma 4.1, we know that that a bound on $\\left\\|\\mathscr{L}^{2 m} f\\right\\|_{L^{p}\\left(\\gamma_{\\sigma^{2}}\\right)}$ implies a low-degree approximation for $f$. Here we would like to bound $\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{p}\\left(\\gamma_{\\sigma^{2}}\\right)}$ for all $m$. The kind of growth we get in $m$ is captured by the following definition.\n\nDefinition 4.3: We say $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n^{\\prime}}$ is a $(r, \\sigma)$-Gaussian-noise-sensitive function if for all $m \\in \\mathbb{N}$ and $p \\geq 1$,\n\n$$\n\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{p}\\left(\\gamma_{\\sigma^{2}}\\right)} \\leq r \\sigma\\left(\\frac{r m^{2}}{\\sigma^{2}}\\right)^{m} \\max \\{r, \\sqrt{m p}\\}^{m}\n$$\n\nIn the following lemma, we bound the moments of higher iterates of the Ornstein-Uhlenbeck operator $\\mathscr{L}$ applied to the score function $f$ under the Gaussian measure. This shows the Gaussian-noise-sensitivity property for $f$ in the case where $Q_{0}$ is supported on a ball of radius $R$, forming a single cluster. We begin by switching from the Gaussian measure to the mixture measure using Lemma B.3, and then leverage the probabilistic interpretation of $\\mathscr{L}^{m} f$ derived in Lemma 4.2, which is based on moments of replicas of the posterior distribution.\n\nLemma 4.4 (Control on iterates of the OU operator). Let $f=f_{\\sigma^{2}}$ be as in (4). Suppose $Q_{0}$ is supported on $B_{R}(0), R \\geq \\sigma$. Let $P$ be the density of $Q_{0} * \\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)$. Then for any $m \\in \\mathbb{N}$, we have the following:\n\n$$\n\\begin{aligned}\n\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{p}(P)} & =R \\cdot O\\left(\\left(\\frac{1}{\\sigma}\\right)^{2}\\left(\\frac{m R}{\\sigma}\\right)^{2}\\left(1+\\frac{(m p)^{1 / 2} \\sigma}{R}\\right)\\right)^{m} \\\\\n\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{p}\\left(\\gamma_{\\sigma^{2}}\\right)} & \\leq R \\cdot O\\left(\\left(\\frac{1}{\\sigma}\\right)^{2}\\left(\\frac{m^{2} R}{\\sigma}\\right) \\max \\left\\{\\frac{R}{\\sigma},(m p)^{1 / 2}\\right\\}\\right)^{m}\n\\end{aligned}\n$$\n\nTherefore, $f$ is $\\left(O\\left(\\frac{R}{\\sigma}\\right), \\sigma\\right)$-Gaussian-noise-sensitive.\n\nProof. In (12) in Lemma 4.2,\n\n$$\n\\left|\\prod_{\\ell=1}^{s}\\left\\langle x^{\\left(i_{\\ell}\\right)}, x^{\\left(i_{\\ell}^{\\prime}\\right)}\\right\\rangle\\right| \\leq R^{2 s} \\leq R^{2 m}\n$$\n\nNote that the joint distribution of $\\left(x^{(j)}, y\\right)$ is the same for any $j$, namely, it is the distribution when $x^{(j)} \\sim p_{0}$ and $y=x^{(j)}+\\sigma \\xi^{(j)}$ when $\\xi \\sim \\mathcal{N}\\left(0, I_{n}\\right)$ is independent of $x^{(j)}$. Then for $p \\in \\mathbb{N}$,\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left\\langle\\left|\\prod_{\\ell=1}^{t}\\left\\langle x^{\\left(j_{\\ell}\\right)}, y\\right\\rangle\\right|\\right\\rangle^{p} \\\\\n& \\leq \\mathbb{E}_{x^{(1)} \\sim p_{0}, y=x^{(1)}+\\sigma \\xi}\\left|\\prod_{\\ell=1}^{t}\\left\\langle x^{(j \\ell)}, y\\right\\rangle\\right|^{p} \\quad \\text { by Jensen's inequality } \\\\\n& \\leq \\prod_{\\ell=1}^{t}\\left[\\mathbb{E}\\left|\\left\\langle x^{\\left(j_{\\ell}\\right)}, y\\right\\rangle\\right|^{t p}\\right]^{1 / t} \\quad \\text { by H\u00f6lder's inequality } \\\\\n& \\leq \\mathbb{E}\\left|\\left\\langle x^{(1)}, x^{(1)}+\\sigma \\xi^{(1)}\\right\\rangle\\right|^{t p} \\quad\\left(x^{(j)}, y\\right) \\stackrel{d}{=}\\left(x^{(1)}, x^{(1)}+\\sigma \\xi^{(1)}\\right) \\\\\n& \\leq \\mathbb{E} \\sum_{k=0}^{t p}\\binom{t p}{k}\\left|x^{(1)}\\right|^{2(t p-k)} \\sigma^{k}\\left|\\left\\langle x^{(1)}, \\xi^{(1)}\\right\\rangle\\right|^{k} \\quad \\text { Binomial theorem } \\\\\n& \\leq \\sum_{k=0}^{t p}\\binom{t p}{k} R^{2(t p-k)} \\sigma^{k} R^{k} \\mathbb{E}_{X \\sim \\mathcal{N}(0,1)}|X|^{k} \\\\\n& \\leq \\sum_{k=0}^{t p}\\binom{t p}{k} R^{2(t p-k)} \\sigma^{k} R^{k}(k-1)!! \\quad \\text { Gaussian moment bound } \\\\\n& \\leq \\sum_{k=0}^{t p}\\binom{t p}{k} R^{2(t p-k)} \\sigma^{k} R^{k}(t p)^{k / 2} \\leq R^{2 t p}\\left(1+\\frac{(t p)^{1 / 2} \\sigma}{R}\\right)^{t p}\n\\end{aligned}\n$$\n\nConsider one term $a_{i, i^{\\prime}, j}\\left\\langle x^{(1)} \\sigma^{-2(s+t+m)} \\prod_{\\ell=1}^{s}\\left\\langle x^{\\left(i_{\\ell}\\right)}, x^{\\left(i_{\\ell}^{\\prime}\\right)}\\right\\rangle \\prod_{\\ell=1}^{t}\\left\\langle x^{\\left(j_{\\ell}\\right)}, y\\right\\rangle\\right\\rangle$ in (12). We have by Jensen's inequality that\n\n$$\n\\begin{aligned}\n& \\left\\|\\left\\langle x^{(1)} \\sigma^{-2(s+t+m)} \\prod_{\\ell=1}^{s}\\left\\langle x^{\\left(i_{\\ell}\\right)}, x^{\\left(i_{\\ell}^{\\prime}\\right)}\\right\\rangle \\prod_{\\ell=1}^{t}\\left\\langle x^{\\left(j_{\\ell}\\right)}, y\\right\\rangle\\right\\rangle\\right\\|_{L^{p}(P)} \\\\\n& \\leq\\left[\\mathbb{E}\\left\\langle | x^{(1)} \\sigma^{-2(s+t+m)} \\prod_{\\ell=1}^{s}\\left\\langle x^{\\left(i_{\\ell}\\right)}, x^{\\left(i_{\\ell}^{\\prime}\\right)}\\right\\rangle \\prod_{\\ell=1}^{t}\\left\\langle x^{\\left(j_{\\ell}\\right)}, y\\right\\rangle\\right|^{p}\\right\\rangle\\left.\\right]\\right]^{\\frac{1}{p}} \\\\\n& \\leq \\sigma^{-2(s+t+m)} R^{2 s+1} R^{2 t}\\left(1+\\frac{(t p)^{1 / 2} \\sigma}{R}\\right)^{t} \\\\\n& =O\\left(\\frac{R}{\\sigma^{2 m}}\\left(\\frac{R}{\\sigma}\\right)^{2(s+t)}\\left(1+\\frac{(t p)^{1 / 2} \\sigma}{R}\\right)^{t}\\right)=O\\left(\\frac{R}{\\sigma^{2 m}}\\left(\\frac{R}{\\sigma}\\right)^{2 m}\\left(1+\\frac{(m p)^{1 / 2} \\sigma}{R}\\right)^{m}\\right)\n\\end{aligned}\n$$\n\nusing $s+t \\leq m$. Then\n\n$$\n\\begin{aligned}\n\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{p}(P)} & \\leq 30^{m} m!^{2} \\cdot O\\left(\\frac{R}{\\sigma^{2 m}}\\left(\\frac{R}{\\sigma}\\right)^{2 m}\\left(1+\\frac{(m p)^{1 / 2} \\sigma}{R}\\right)^{m}\\right) \\\\\n& =R \\cdot O\\left(\\left(\\frac{1}{\\sigma}\\right)^{2 m}\\left(\\frac{m R}{\\sigma}\\right)^{2 m}\\left(1+\\frac{(m p)^{1 / 2} \\sigma}{R}\\right)^{m}\\right)\n\\end{aligned}\n$$\n\nBy H\u00f6lder's inequality and Lemma B. 4 (appropriately scaled),\n\n$$\n\\begin{aligned}\n\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{p}\\left(\\gamma_{\\sigma^{2}}\\right)} & =\\left(\\int_{\\mathbb{R}^{n}}\\left|\\mathscr{L}^{m} f\\right|^{p} \\frac{d \\gamma_{\\sigma^{2}}}{d P} d P\\right)^{\\frac{1}{p}} \\leq\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{p(1+q)}\\left(\\gamma_{\\sigma^{2}}\\right)}\\left\\|\\frac{d \\gamma_{\\sigma^{2}}}{d P}\\right\\|_{L^{1+\\frac{1}{2}}(P)}^{\\frac{1}{p}} \\\\\n& \\leq O\\left(\\left(\\frac{1}{\\sigma}\\right)^{2}\\left(\\frac{m R}{\\sigma}\\right)^{2}\\left(1+\\frac{(m p(q+1))^{1 / 2} \\sigma}{R}\\right)\\right)^{m} e^{\\frac{(R / \\sigma)^{2}}{2 p q}}\n\\end{aligned}\n$$\n\nTo optimize this bound, set $q=\\frac{(R / \\sigma)^{2}}{p m}$. Then by H\u00f6lder's inequality,\n\n$$\n\\begin{aligned}\n\\left\\|\\mathscr{L}^{m} f\\right\\|_{L^{p}\\left(\\gamma_{\\sigma^{2}}\\right)} & \\leq R e^{\\frac{(R / \\sigma)^{2}}{2 p q}} \\cdot O\\left(\\left(\\frac{1}{\\sigma}\\right)^{2}\\left(\\frac{m R}{\\sigma}\\right)^{2}\\left(1+\\frac{m p(q+1) \\sigma}{R}\\right)\\right)^{m} \\\\\n& \\leq R e^{m / 2} \\cdot O\\left(\\left(\\frac{1}{\\sigma}\\right)^{2}\\left(\\frac{m R}{\\sigma}\\right)^{2}\\right)^{m} \\cdot\\left[1+O\\left(\\frac{(m p q)^{1 / 2} \\sigma}{R}\\right)^{m}+O\\left(\\frac{(m p)^{1 / 2} \\sigma}{R}\\right)^{m}\\right]\n\\end{aligned}\n$$\n\nThe first two terms give $R \\cdot O\\left(\\left(\\frac{1}{\\sigma}\\right)^{2}\\left(\\frac{m R}{\\sigma}\\right)^{2}\\right)^{m}$, while the last term gives $R \\cdot O\\left(\\left(\\frac{1}{\\sigma}\\right)^{2}\\left(\\frac{m^{2} R}{\\sigma}\\right)(m p)^{1 / 2}\\right)$. This completes the proof.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "# 4.3 Approximation with polynomial \n\nSuppose we want to approximate $f=f_{\\sigma^{2}}$ (in (4)) with a low-degree polynomial on the mixture distribution $\\gamma^{\\prime}$. We seek a low-degree polynomial $g$ such that $\\|f-g\\|_{\\gamma^{\\prime}}^{2}$ is small. To do this, we first smooth $f$ by the Ornstein-Uhlenbeck semigroup to obtain $\\mathscr{P}_{t} f$ (see (9)), and then find a $g$ that approximates $\\mathscr{P}_{t} f$. We can bound using Cauchy-Schwarz that\n\n$$\n\\begin{aligned}\n\\|f-g\\|_{\\gamma^{\\prime}}^{2} & \\leq 2\\left(\\left\\|f-\\mathscr{P}_{t} f\\right\\|_{\\gamma^{\\prime}}^{2}+\\left\\|\\mathscr{P}_{t} f-g\\right\\|_{\\gamma^{\\prime}}^{2}\\right) \\\\\n& \\leq 2\\left(\\left\\|f-\\mathscr{P}_{t} f\\right\\|_{\\gamma^{\\prime}}^{2}+\\left\\|\\mathscr{P}_{t} f-g\\right\\|_{L^{2}(\\gamma)}^{2}+\\left\\|\\mathscr{P}_{t} f-g\\right\\|_{L^{4}(\\gamma)}^{2} \\chi^{2}\\left(\\gamma^{\\prime}\\right\\| \\gamma\\right)^{1 / 2}\\right)\n\\end{aligned}\n$$\n\nDoing the smoothing ensures that we have better control over the term with higher $p$-norm, $\\left\\|\\mathscr{P}_{t} f-g\\right\\|_{L^{4}(\\gamma)}$. Choosing $t=\\varepsilon, \\mathscr{P}_{t} f$ has exponential decay in coefficients with rate $\\varepsilon$, so we can approximate $\\mathscr{P}_{t} f$ with a degree- $\\Theta\\left(\\frac{1}{\\varepsilon}\\right)$ polynomial. To bound $\\left\\|f-\\mathscr{P}_{t} f\\right\\|_{\\gamma^{\\prime}}^{2}$, we bound its derivative:\n\n$$\n\\begin{aligned}\n\\frac{d}{d t}\\left\\|f-\\mathscr{P}_{t} f\\right\\|_{\\gamma^{\\prime}}^{2} & \\leq \\int_{\\mathbb{R}^{n}} 2\\left(f-\\mathscr{P}_{t} f\\right)\\left(-\\mathscr{L} \\mathscr{P}_{t} f\\right) d \\gamma^{\\prime} \\\\\n& \\leq 2\\left\\|f-\\mathscr{P}_{t} f\\right\\|_{\\gamma^{\\prime}}\\left\\|\\mathscr{L} \\mathscr{P}_{t} f\\right\\|_{\\gamma^{\\prime}} \\\\\n\\Longrightarrow \\frac{d}{d t}\\left\\|f-\\mathscr{P}_{t} f\\right\\|_{\\gamma^{\\prime}} & \\leq\\left\\|\\mathscr{L} \\mathscr{P}_{t} f\\right\\|_{\\gamma^{\\prime}}\n\\end{aligned}\n$$\n\nFocusing on the dependence of total error on $\\varepsilon$, we can bound this with a change-of-measure inequality and Lemma 4.4, which, after integrating in $t$, gives error $O(\\varepsilon)$. In order to obtain polylogarithmic degree, we need a higher-order version of this argument. In preparation for Section 5, when we need to consider the norm with respect to a different measure, we state the following more generally.\nLemma 4.5. Suppose $f$ is $(O(r), \\sigma)$-Gaussian-noise-stable and $\\gamma^{\\prime}$ is a measure such that for all $a \\geq 0,\\left\\|\\frac{d \\gamma^{\\prime}}{d \\gamma}\\right\\|_{L^{1+a}(\\gamma)} \\leq e^{\\frac{\\omega^{2}}{2}}$ (for $\\gamma:=\\gamma_{\\sigma^{2}}$ ) (e.g., from Lemma B.4, $Q * \\mathcal{N}\\left(0, I_{n}\\right)$ where $Q$ is supported on $B_{r}(0)$ ), where $r \\geq 1$. There is a polynomial $g$ of degree at most $O\\left(r^{4} \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4} \\max \\left\\{r^{2}, \\ln \\left(\\frac{1}{\\varepsilon}\\right)\\right\\}\\right)$ such that\n\n$$\n\\|f-g\\|_{\\gamma^{\\prime}}^{2} \\leq \\varepsilon^{2} r^{2} \\sigma^{2} \\quad \\text { and } \\quad\\|g\\|_{\\gamma}^{2} \\leq\\|f\\|_{\\gamma}^{2}\n$$\n\nProof. To get a better bound, we use a more clever smoothing strategy. For some $\\widetilde{f}$ to be defined, we will bound\n\n$$\n\\|f-g\\|_{\\gamma^{\\prime}}^{2} \\leq 2\\left(\\|f-\\widetilde{f}\\|_{\\gamma^{\\prime}}^{2}+\\left\\|\\widetilde{f}-g\\right\\|_{\\gamma^{\\prime}}^{2}\\right)\n$$\n\nwhere $g$ is a polynomial approximation of $\\widetilde{f}$ obtained by truncating the Hermite expansion.\nTo define $\\widetilde{f}$, we approximate $f$ with a numerical differentiation formula for a higher derivative. Define the finite difference by $\\Delta_{x, h} g(x)=\\Delta_{h} g(x):=g(x+h)-g(x)$. We can write this as $\\Delta_{h} g=T_{h} g-g$, where $T_{h} g(x):=g(x+h)$. Suppose that $g \\in C^{m}$. By the Binomial Theorem on $T_{h}$ - id and Taylor's Theorem,\n\n$$\n\\begin{aligned}\n\\Delta_{h}^{m} g(0) & =\\sum_{j=0}^{m}\\binom{m}{j}(-1)^{m-j}\\left[\\sum_{i=0}^{m-1} g^{(i)}(0) \\frac{(h j)^{i}}{i!}+\\frac{g^{(m)}\\left(\\xi_{j}\\right)}{m!}(h j)^{m}\\right] \\text { for some } \\xi_{j} \\in[0, h j] \\\\\n& =\\left.\\sum_{i=0}^{m-1} \\frac{g^{(i)}(0)}{i!} \\Delta_{x, h}^{m}\\left(x^{i}\\right)\\right|_{x=0}+h^{m} \\sum_{j=0}^{m}\\binom{m}{j}(-1)^{m-j} \\frac{g^{(j)}\\left(\\xi_{j}\\right)}{m!} j^{m} \\\\\n& =h^{m} \\sum_{j=0}^{m}\\binom{m}{j}(-1)^{m-j} \\frac{g^{(j)}\\left(\\xi_{j}\\right)}{m!} j^{m}\n\\end{aligned}\n$$\n\nwhere in the last step we use the fact that finite differencing reduces the degree of a polynomial by 1 , the $m$ th finite difference of polynomials of degree $<m$ is 0 . Hence\n\n$$\n\\left|\\Delta_{h}^{m} g(0)\\right| \\leq h^{m} \\sum_{j=0}^{m}\\binom{m}{j}\\left(\\frac{e j}{m}\\right)^{m} \\max _{\\xi \\in[0, m h]}\\left|f^{(m)}(\\xi)\\right| \\leq h^{m}(2 e)^{m} \\max _{\\xi \\in[0, m h]}\\left|f^{(m)}(\\xi)\\right|\n$$\n\nLet $\\widetilde{f}=\\sum_{j=1}^{m}(-1)^{j+1}\\binom{m}{j} \\mathscr{P}_{j h} f$. Note $f-\\widetilde{f}=(-1)^{m} \\Delta_{t, h}^{m}\\left(\\mathscr{P}_{t} f\\right) \\|_{t=0}$. We then have (noting that $\\mathscr{P}_{s}$ and $\\mathscr{L}$ commute)\n\n$$\n\\begin{aligned}\n\\left\\|f-\\widetilde{f}\\right\\|_{\\gamma^{\\prime}}^{2} & \\leq(2 h e)^{2 m} \\int_{\\mathbb{R}^{n}} \\max _{t \\in[0, m h]}\\left(\\frac{d^{m}}{d t^{m}} \\mathscr{P}_{t} f(x)\\right)^{2} d \\gamma^{\\prime}(x) \\\\\n& \\leq(2 h e)^{2 m} \\int_{\\mathbb{R}^{n}}\\left(\\frac{d^{m}}{d t^{m}} \\mathscr{P}_{t} f(x)\\right|_{t=0}+\\int_{0}^{m h}\\left|\\frac{d^{m+1}}{d s^{m+1}} \\mathscr{P}_{s} f(x)\\right| d s\\right)^{2} d \\gamma^{\\prime}(x) \\\\\n& \\leq 2(2 h e)^{2 m}\\left(\\int_{\\mathbb{R}^{n}}\\left|\\mathscr{L}^{m} f(x)\\right|^{2} d \\gamma^{\\prime}(x)+m h \\int_{0}^{m h} \\int_{\\mathbb{R}^{n}}\\left|\\mathscr{P}_{s} \\mathscr{L}^{m+1} f(x)\\right|^{2} d \\gamma^{\\prime}(x) d s\\right)\n\\end{aligned}\n$$\n\nFirst we bound the first term and the second term when $s=0$. We use the Gaussian noise sensitivity assumption, the assumption on $\\gamma^{\\prime}$, and H\u00f6lder's inequality to obtain\n\n$$\n\\begin{aligned}\n\\int_{\\mathbb{R}^{n}}\\left|\\mathscr{L}^{l} f\\right|^{2} d \\gamma^{\\prime} & \\leq\\left\\|\\mathscr{L}^{l} f\\right\\|_{L^{(p+1)}(\\gamma)}^{2}\\left\\|\\frac{d \\gamma^{\\prime}}{d \\gamma}\\right\\|_{L^{\\frac{p+1}{p}}(\\gamma)} \\\\\n& \\leq r^{2} \\sigma^{2} O\\left(\\frac{r l^{2}}{\\sigma^{2}}\\right)^{2 l} \\max \\left\\{r^{2}, l(1+p)\\right\\}^{l} e^{\\frac{r^{2}}{2 p}}\n\\end{aligned}\n$$\n\nTo get a bound for the second term when $s>0$, we bound the following derivative, again using the Gaussian noise sensitivity assumption and H\u00f6lder's inequality:\n\n$$\n\\begin{aligned}\n\\frac{d}{d s} \\int_{\\mathbb{R}^{n}}\\left|\\mathscr{P}_{s} \\mathscr{L}^{l} f\\right|^{2} d \\gamma^{\\prime} & \\leq \\int_{\\mathbb{R}^{n}} 2\\left\\langle\\mathscr{P}_{s} \\mathscr{L}^{l} f, \\mathscr{P}_{s} \\mathscr{L}^{l+1} f\\right\\rangle \\frac{d \\gamma^{\\prime}}{d \\gamma} d \\gamma \\\\\n& \\leq 2\\left\\|\\mathscr{P}_{s} \\mathscr{L}^{l} f\\right\\|_{L^{2(p+1)}(\\gamma)}\\left\\|\\mathscr{P}_{s} \\mathscr{L}^{l+1} f\\right\\|_{L^{2(p+1)}(\\gamma)}\\left\\|\\frac{d \\gamma^{\\prime}}{d \\gamma}\\right\\|_{L^{\\frac{p+1}{p}}(\\gamma)} \\\\\n& \\leq 2\\left\\|\\mathscr{L}^{l} f\\right\\|_{L^{2(p+1)}(\\gamma)}\\left\\|\\mathscr{L}^{l+1} f\\right\\|_{L^{2(p+1)}(\\gamma)}\\left\\|\\frac{d \\gamma^{\\prime}}{d \\gamma}\\right\\|_{L^{\\frac{p+1}{p}}(\\gamma)} \\\\\n& \\leq r^{2} \\sigma^{2} O\\left(\\frac{r l^{2}}{\\sigma^{2}}\\right)^{2 l} \\max \\left\\{r^{2}, l(1+p)\\right\\}^{l} e^{\\frac{r^{2}}{2 p}}\n\\end{aligned}\n$$\n\nwhere in (17) we use the fact that for $q \\geq 1,\\left\\|\\mathscr{P}_{s} g\\right\\|_{L^{q}(\\gamma)}$ is monotonically decreasing in $s$. In both (16) and (18), we optimize the bound by taking $p=\\frac{r^{2}}{l}$ to obtain\n\n$$\n\\begin{aligned}\n\\int_{\\mathbb{R}^{n}}\\left|\\mathscr{L}^{l} f\\right|^{2} d \\gamma^{\\prime} & \\leq r^{2} \\sigma^{2} O\\left(\\frac{r l^{2}}{\\sigma^{2}}\\right)^{2 l} \\max \\left\\{r^{2}, l\\right\\}^{l}, & l=m, m+1 \\\\\n\\frac{d}{d s} \\int_{\\mathbb{R}^{n}}\\left|\\mathscr{L}^{l} \\mathscr{P}_{s} f\\right|^{2} d \\gamma^{\\prime} & \\leq r^{2} \\sigma^{2} O\\left(\\frac{r l^{2}}{\\sigma^{2}}\\right)^{2 l} \\max \\left\\{r^{2}, l\\right\\}^{l}, & l=m+1\n\\end{aligned}\n$$\n\nIntegrating the last inequality twice and substituting into (15) gives\n\n$$\n\\begin{aligned}\n\\left\\|f-\\widetilde{f}\\right\\|_{\\gamma^{\\prime}}^{2} & \\lesssim r^{2} \\sigma^{2} O(h)^{2 m}\\left(\\left(1+(m h)^{3}\\right)\\left(\\frac{r m^{2}}{\\sigma^{2}}\\right)^{2 m} \\max \\left\\{r^{2}, m\\right\\}^{m}\\right) \\\\\n& \\lesssim r^{2} \\sigma^{2} O\\left(\\frac{h r m^{2} \\max \\{r, \\sqrt{m}\\}}{\\sigma^{2}}\\right)^{2 m}\n\\end{aligned}\n$$\n\nwhen $m h=O(1)$. We choose $m \\sim \\ln \\left(\\frac{1}{\\varepsilon}\\right)$ and $h \\leq \\frac{c \\sigma^{2}}{r m^{2} \\max \\{r, \\sqrt{m}\\}}$ for an appropriate constant $c$ to get\n\n$$\n\\left\\|f-\\widetilde{f}\\right\\|_{\\gamma^{\\prime}}^{2} \\lesssim \\varepsilon^{2} r^{2} \\sigma^{2}\n$$\n\nWrite $f=\\sum_{\\mathbf{k} \\in \\mathbb{N}_{0}^{d}} a_{\\mathbf{k}} h_{\\mathbf{k}}$ where $a_{\\mathbf{k}} \\in \\mathbb{R}^{d}$, and let $p_{l}=\\sum_{|\\mathbf{k}|=l} a_{\\mathbf{k}} h_{\\mathbf{k}}$, so that $f=\\sum_{l=0}^{\\infty} p_{l}$. Then\n\n$$\n\\widetilde{f}=\\sum_{\\mathbf{k} \\in \\mathbb{N}_{0}^{n}} b_{\\mathbf{k}} h_{\\mathbf{k}} \\quad \\text { for } \\quad b_{\\mathbf{k}}=\\sum_{j=1}^{m}(-1)^{j+1}\\binom{m}{j} e^{-j h|\\mathbf{k}| / \\sigma^{2}} a_{\\mathbf{k}}=\\left[1-\\left(1-e^{-h|\\mathbf{k}| / \\sigma^{2}}\\right)^{m}\\right] a_{\\mathbf{k}}\n$$\n\nLet $q_{l}=\\sum_{|\\mathbf{k}|=l} b_{\\mathbf{k}} h_{\\mathbf{k}}$. Note\n\n$$\n\\left\\|q_{l}\\right\\|_{\\gamma}^{2}=\\sum_{|\\mathbf{k}|=l}\\left|b_{\\mathbf{k}}\\right|^{2}=\\sum_{|\\mathbf{k}|=l}\\left[1-\\left(1-e^{-h|\\mathbf{k}| / \\sigma^{2}}\\right)^{m}\\right]^{2}\\left|a_{\\mathbf{k}}\\right|^{2} \\leq m^{2} e^{-2 h l / \\sigma^{2}}\\left\\|p_{l}\\right\\|_{\\gamma}^{2}\n$$\n\nWe approximate $\\tilde{f}$ with $g=\\sum_{|\\mathbf{k}|<L} b_{\\mathbf{k}} h_{\\mathbf{k}}$ where $L \\geq C h^{-1} \\ln \\left(\\frac{1}{\\varepsilon}\\right)$ for an appropriate constant $C$. We have by Cauchy-Schwarz and Lemma B. 3 that\n\n$$\n\\begin{aligned}\n\\|\\tilde{f}-g\\|_{\\gamma^{\\prime}}^{2} & \\leq \\int_{\\mathbb{R}^{n}}\\left|\\sum_{l \\geq L} q_{l}\\right|^{2} d \\gamma^{\\prime} \\\\\n& \\leq \\sum_{l \\geq L} e^{-h l} \\cdot \\sum_{l \\geq L} e^{h l} \\int_{\\mathbb{R}^{n}}\\left|q_{l}\\right|^{2} d \\gamma^{\\prime} \\\\\n& \\leq 1 \\cdot \\sum_{l \\geq L} e^{h l}\\left\\|q_{l}\\right\\|_{\\gamma}^{2} e^{2 \\sqrt{l} r} \\\\\n& \\leq \\sum_{l \\geq L} e^{h l} m^{2} e^{-2 h l / \\sigma^{2}}\\left\\|p_{l}\\right\\|_{\\gamma}^{2} e^{2 \\sqrt{l} r} \\\\\n& \\leq \\max _{l \\geq L} m^{2} e^{2 \\sqrt{l} r-h l / \\sigma^{2}}\\|f\\|_{\\gamma}^{2} \\leq \\varepsilon^{2}\\|f\\|_{\\gamma}^{2}\n\\end{aligned}\n$$\n\nwhen we take $L \\geq C \\frac{\\sigma^{4} \\varepsilon^{2}}{h^{2}} \\asymp r^{4} \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4} \\max \\left\\{r^{2}, \\ln \\left(\\frac{1}{\\varepsilon}\\right)\\right\\}$ for an appropriate constant $C$. Note that the Gaussian-noise-sensitive property also implies $\\|f\\|_{\\gamma}^{2} \\leq r^{2} \\sigma^{2}$. Plugging into (14) the inequalities (19) and (21),\n\n$$\n\\|f-g\\|_{\\gamma^{\\prime}}^{2} \\lesssim\\|f-\\tilde{f}\\|_{\\gamma^{\\prime}}^{2}+\\|\\tilde{f}-g\\|_{\\gamma^{\\prime}}^{2} \\lesssim r^{2} \\sigma^{2} \\varepsilon^{2}\n$$\n\nChoosing constants appropriately then gives the desired bound. Finally note that Equation (20) implies $\\left|a_{\\mathbf{k}}\\right| \\leq\\left|b_{\\mathbf{k}}\\right|$ for all $\\mathbf{k} \\in \\mathbb{N}_{0}^{n}$. Hence\n\n$$\n\\|g\\|_{\\gamma}^{2} \\leq\\left\\|\\tilde{f}\\right\\|_{\\gamma}^{2}=\\sum_{\\mathbf{k} \\in \\mathbb{N}_{0}^{n}}\\left|b_{\\mathbf{k}}\\right|^{2} \\leq \\sum_{\\mathbf{k} \\in \\mathbb{N}_{0}^{n}}\\left|a_{\\mathbf{k}}\\right|^{2}=\\|f\\|_{\\gamma}^{2}\n$$\n\nCombining Lemmas 4.4 and 4.5 implies a low degree approximation of order poly $\\left(\\frac{R}{\\sigma}, \\ln \\left(\\frac{1}{\\varepsilon}\\right)\\right)$ for the single cluster case.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# 5 From one to multiple clusters \n\nTo handle the general case of multiple clusters, we approximate the score function using a piecewise low-degree polynomial over a set of warm-starts. Before applying Lemma 4.5, we locally approximate the score within the Voronoi cell corresponding to each warm-start $\\hat{\\mu}$ by $f_{\\text {loc }}$. We can then approximate $f_{\\text {loc }}$ using a low-degree polynomial by controlling the terms $\\left\\|\\mathscr{L}^{m} f_{\\text {loc }}(\\cdot+\\hat{\\mu})\\right\\|_{L^{p}\\left(\\gamma_{\\alpha^{2}}\\right)}$, where $f_{\\text {loc }}$ is shifted by the warm start $\\hat{\\mu}$. We start with the definition of the warm starts.\n\nDefinition 5.1: Let $\\mathcal{C} \\subset \\mathbb{R}^{n}$. We say $\\mathcal{C}$ is a complete set of $R$-warm starts for $Q_{0}$ if\n\n$$\nQ_{0}\\left(\\bigcup_{\\widetilde{\\mu} \\in \\mathcal{C}} B_{R}(\\widetilde{\\mu})\\right)=1\n$$\n\nIn other words, for all $\\mu$ in the support of $Q_{0}$, there exists $\\widetilde{\\mu} \\in \\mathcal{C}$ such that $\\|\\widetilde{\\mu}-\\mu\\| \\leq R$.\nDefinition 5.2: Let $\\mathcal{C}=\\left\\{\\widetilde{\\mu}_{1}, \\ldots, \\widetilde{\\mu}_{k^{\\prime}}\\right\\}$. The Voronoi partition $V_{1}, \\ldots, V_{k^{\\prime}}$ corresponding to $\\mathcal{C}$ is defined by $V_{j}=\\left\\{x \\in \\mathbb{R}^{n}:\\left|x-\\widetilde{\\mu}_{j}\\right|=\\min _{1 \\leq j^{\\prime} \\leq k^{\\prime}}\\left|x-\\widetilde{\\mu}_{j^{\\prime}}\\right|\\right\\}$.\n\nNote that up to the boundaries (which are a measure 0 set), this induces a partition of $\\mathbb{R}^{n}$.\nTo tackle the multiple cluster setting, we first suppose that we have a complete set of $R$ warm starts $\\mathcal{C}=\\left\\{\\widetilde{\\mu}_{1}, \\ldots, \\widetilde{\\mu}_{k^{\\prime}}\\right\\}$, and let $V_{1}, \\ldots, V_{k^{\\prime}}$ be the corresponding Voronoi partition. For a probability measure $P$ and set $S$, define the unnormalized restriction by $P_{S}(A)=P(A \\cap S)$ and the normalized restriction by $\\left.P\\right|_{S}(A)=\\frac{P(A \\cap S)}{P(S)}$. Define $P^{S}=\\left(Q_{0}\\right)_{S} * \\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)$-that is, we do the restriction before the convolution. Define\n\n$$\nf_{S, \\sigma^{2}}(y)=\\mathbb{E}_{\\substack{\\mu \\sim Q_{0} \\mid S \\\\ Y=\\mu+\\sigma \\xi, \\xi \\sim \\mathcal{N}\\left(0, I_{n}\\right)}}[\\mu \\mid Y=y]=y+\\sigma^{2} \\nabla \\ln p^{S}(y)\n$$\n\nWhen $\\sigma$ is understood, we omit it from the subscript.\nWe need to show that we still have a good polynomial approximation for $f$ under the measure $\\left.P\\right|_{V_{i}}=\\left.\\left(Q_{0} * \\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)\\right)\\right|_{V_{i}}$. By the analysis for one cluster, we have good approximation of $f_{V_{i}, \\sigma^{2}}$ under $P^{V_{i}}$. To obtain the result for multiple clusters, we need to show that this approximation is preserved even when we consider $f$ instead of $f_{V_{i}, \\sigma^{2}}$ and $\\left.P\\right|_{V_{i}}$ instead of $P^{V_{i}}$, i.e., deal with the leakage into $V_{i}$ from the other Voronoi cells after $Q_{0}$ is convolved with $\\mathcal{N}\\left(0, \\sigma^{2} I\\right)$, in both the score function and the measure which the norm is with respect to. We do this in Sections 5.1 and 5.2, respectively.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "# 5.1 Estimation with the \"nearby\" score \n\nWe will actually bound $\\left\\|f-f_{S_{i}}\\right\\|_{L^{2}\\left(P_{V_{i}}\\right)}$, for an expanded neighborhood $S_{i}$ of $\\widetilde{\\mu}_{i}$, allowing an extra \"buffer region\" where mass is allowed to leak in.\n\nLemma 5.3. Suppose that $\\mathcal{C}=\\left\\{\\widetilde{\\mu}_{1}, \\ldots, \\widetilde{\\mu}_{k^{\\prime}}\\right\\}$ is a complete set of $R$-warm starts for $Q_{0}$, and let $V_{1}, \\ldots, V_{k^{\\prime}}$ be the corresponding Voronoi partition. Suppose $Q_{0}$ is supported on $B_{M}(0)$. Given $R^{\\prime}>R$, let $S_{i}=B_{R^{\\prime}}\\left(\\widetilde{\\mu}_{i}\\right)$ and define\n\n$$\n\\begin{aligned}\nf_{\\mathrm{loc}, \\sigma^{2}}^{\\mathcal{C}, R^{\\prime}}(y) & =f_{S_{i}, \\sigma^{2}}(y) \\text { when } y \\in V_{i} \\\\\n\\text { where } f_{S_{i}, \\sigma^{2}}(y) & =\\mathbb{E}_{\\substack{\\mu \\sim Q_{0} \\mid S_{i} \\\\\nY=\\mu+\\sigma \\xi, \\xi \\sim \\mathcal{N}\\left(0, I_{n}\\right)}}[\\mu \\mid Y=y]=y+\\sigma^{2} \\nabla \\ln p^{S_{i}}(y)\n\\end{aligned}\n$$\n\nWe write $f_{\\text {loc }, \\sigma^{2}}$ when $\\mathcal{C}, R^{\\prime}$ are clear. Then for $R^{\\prime}=3 R+2 \\sqrt{2} \\sigma \\sqrt{\\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)}$,\n\n$$\n\\left\\|f_{\\sigma^{2}}-f_{\\mathrm{loc}, \\sigma^{2}}\\right\\|_{L^{2}(P)}^{2} \\leq\\left(32 \\sigma^{2}+6 R^{\\prime 2}\\right) \\varepsilon \\lesssim\\left(R^{2}+\\sigma^{2} \\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)\\right) \\varepsilon\n$$\n\nNote that\n\n$$\n\\left\\|f_{\\sigma^{2}}-f_{\\operatorname{loc}, \\sigma^{2}}\\right\\|_{L^{2}(P)}^{2}=\\sum_{i=1}^{k^{\\prime}}\\left\\|f_{\\sigma^{2}}-f_{S_{i}, \\sigma^{2}}\\right\\|_{L^{2}\\left(P_{V_{i}}\\right)}^{2}\n$$\n\nso this gives a bound for the $L^{2}$ norm within each Voronoi cell.\nTo prove this, we first show that with high probability, $Y=\\mu+\\sigma \\xi$ does not stray too far from the Voronoi cell of $\\mu$.\n\nLemma 5.4. Let $\\mathcal{C}=\\left\\{\\widehat{\\mu}_{1}, \\ldots, \\widehat{\\mu}_{k^{\\prime}}\\right\\}$ and $V_{1}, \\ldots, V_{k^{\\prime}}$ be the corresponding Voronoi partition. Suppose that $\\left|\\mu-\\widehat{\\mu}_{i}\\right| \\leq R$. Define $Y=\\mu+\\sigma \\xi$ where $\\xi \\sim \\mathcal{N}\\left(0, I_{n}\\right)$, and let $i^{\\prime}$ be such that $Y \\in V_{i^{\\prime}}$. Then with probability $\\geq 1-\\varepsilon$,\n\n$$\n\\left|\\mu-\\widehat{\\mu}_{i^{\\prime}}\\right| \\leq 3 R+2 \\sqrt{2} \\sigma \\sqrt{\\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)}\n$$\n\nIn other words, with high probability, even if $Y=\\mu+\\sigma \\xi \\in V_{i^{\\prime}}$ for $i^{\\prime} \\neq i$ (i.e., adding a Gaussian brings the point to a different Voronoi cell), $\\mu$ will not be too far from the center of the new cell $\\widehat{\\mu}_{i^{\\prime}}$.\n\nProof. Let $v_{i i^{\\prime}}=\\frac{\\widehat{\\mu}_{i^{\\prime}}-\\widehat{\\mu}_{i}}{\\left|\\widehat{\\mu}_{i^{\\prime}}-\\widehat{\\mu}_{i}\\right|}$ be the unit vector pointing from $\\widehat{\\mu}_{i}$ toward $\\widehat{\\mu}_{i^{\\prime}}$. Since $\\mu \\in V_{i}$, we have\n\n$$\n\\left\\langle\\mu-\\widehat{\\mu}_{i}, v_{i i^{\\prime}}\\right\\rangle \\leq \\frac{\\left|\\widehat{\\mu}_{i^{\\prime}}-\\widehat{\\mu}_{i}\\right|}{2}\n$$\n\nBy Gaussian tail bounds and a union bound, with probability $\\geq 1-\\varepsilon$, we have that for all $i^{\\prime}$,\n\n$$\n\\left\\langle Y-\\widehat{\\mu}_{i}, v_{i i^{\\prime}}\\right\\rangle=\\left\\langle\\mu+\\sigma \\xi-\\widehat{\\mu}_{i}, v_{i i^{\\prime}}\\right\\rangle \\leq\\left\\langle\\mu-\\widehat{\\mu}_{i}, v_{i i^{\\prime}}\\right\\rangle+\\sqrt{2} \\sigma \\sqrt{\\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)} \\leq R+\\sqrt{2} \\sigma \\sqrt{\\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)}\n$$\n\nTherefore, if $y \\in V_{i^{\\prime}}$, then we have\n\n$$\n\\frac{\\left|\\widehat{\\mu}_{i^{\\prime}}-\\widehat{\\mu}_{i}\\right|}{2} \\leq\\left\\langle Y-\\widehat{\\mu}_{i}, v_{i i^{\\prime}}\\right\\rangle \\leq R+\\sqrt{2} \\sigma \\sqrt{\\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)}\n$$\n\nand\n\n$$\n\\left|\\mu-\\widehat{\\mu}_{i^{\\prime}}\\right| \\leq\\left|\\mu-\\widehat{\\mu}_{i}\\right|+\\left|\\widehat{\\mu}_{i}-\\widehat{\\mu}_{i^{\\prime}}\\right| \\leq 3 R+2 \\sqrt{2} \\sigma \\sqrt{\\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)}\n$$\n\nProof of Lemma 5.3. Given $\\mu$, let $i$ be such that $y \\in V_{i}$. We note\n\n$$\n\\begin{aligned}\nf_{\\sigma^{2}}(y) & =\\mathbb{E}\\left[\\mathbb{1}_{S_{i}}(\\mu) \\mu \\mid Y=y\\right]+\\mathbb{E}\\left[\\mathbb{1}_{S_{i}^{\\circ}}(\\mu) \\mu \\mid Y=y\\right] \\\\\nf_{\\operatorname{loc}, \\sigma^{2}}(y) & =\\mathbb{P}\\left(\\mu \\in S_{i} \\mid Y=y\\right) f_{\\operatorname{loc}, \\sigma^{2}}(y)+\\mathbb{P}\\left(\\mu \\notin S_{i} \\mid Y=y\\right) f_{\\operatorname{loc}, \\sigma^{2}}(y) \\\\\n& =\\mathbb{P}\\left(\\mu \\in S_{i} \\mid Y=y\\right) \\frac{\\mathbb{E}\\left[\\mathbb{1}_{S_{i}}(\\mu) \\mu \\mid Y=y\\right]}{\\mathbb{E}\\left[\\mathbb{1}_{S_{i}}(\\mu) \\mid Y=y\\right]}+\\mathbb{P}\\left(\\mu \\notin S_{i} \\mid Y=y\\right) f_{\\operatorname{loc}, \\sigma^{2}}(y) \\\\\n& =\\mathbb{E}\\left[\\mathbb{1}_{S_{i}}(\\mu) \\mu \\mid Y=y\\right]+\\mathbb{E}\\left[\\mathbb{1}_{S_{i}^{\\circ}}(\\mu) \\mathbb{E}_{\\mu^{\\prime} \\sim Q_{0} \\mid S_{i}}\\left[\\mu^{\\prime} \\mid Y=y\\right] \\mid Y=y\\right]\n\\end{aligned}\n$$\n\nHence, using the fact that $\\mathbb{P}_{(\\mu, y)}\\left(\\mu \\notin S_{i}\\right) \\leq \\varepsilon$, we have\n\n$$\n\\begin{aligned}\n\\left\\|f_{\\sigma^{2}}-f_{\\operatorname{loc}, \\sigma^{2}}\\right\\|_{L^{2}(P)}^{2} & =\\mathbb{E}_{y}\\left|\\mathbb{E}_{\\mu \\sim Q_{0}}\\left[\\mathbb{1}_{S_{i}^{c}}(\\mu)\\left(\\mu-\\mathbb{E}_{\\mu^{\\prime} \\sim Q_{0} \\mid S_{i}}\\left|\\mu^{\\prime}\\right| Y=y\\right]\\right)\\right| Y=y\\left] \\|^{2} \\\\\n& \\leq \\mathbb{E}_{(\\mu, y)}\\left[\\mathbb{1}_{S_{i}^{c}}(\\mu)\\left|\\mu-\\mathbb{E}_{\\mu^{\\prime} \\sim Q_{0} \\mid S_{i}}\\left|\\mu^{\\prime}\\right| Y=y\\right|\\right|^{2}\\right] \\\\\n& \\leq 2 \\mathbb{E}_{(\\mu, y)}\\left[\\mathbb{1}_{S_{i}^{c}}(\\mu)\\left|\\mu-\\widehat{\\mu}_{i}\\right|^{2}+\\mathbb{1}_{S_{i}^{c}}(\\mu)\\left|\\widehat{\\mu}_{i}-\\mathbb{E}_{\\mu^{\\prime} \\sim Q_{0} \\mid S_{i}}\\left|\\mu^{\\prime}\\right| Y=y\\right|\\right|^{2}\\right] \\\\\n& \\leq 2 \\mathbb{E}_{(\\mu, y)}\\left[\\mathbb{1}_{S_{i}^{c}}(\\mu)\\left|\\mu-\\widehat{\\mu}_{i}\\right|^{2}\\right]+2 \\varepsilon R^{\\prime 2} \\\\\n& \\leq 2 \\int_{0}^{\\infty} \\mathbb{P}_{(\\mu, y)}\\left(\\left|\\mu-\\widehat{\\mu}_{i}\\right|^{2} \\geq 2 R^{\\prime 2}+a\\right) d a+6 \\varepsilon R^{\\prime 2} \\\\\n& \\leq 2 \\int_{0}^{\\infty} \\mathbb{P}_{(\\mu, y)}\\left(\\left|\\mu-\\widehat{\\mu}_{i}\\right| \\geq R^{\\prime}+\\sqrt{\\frac{a}{2}}\\right) d a+6 \\varepsilon R^{\\prime 2}\n\\end{aligned}\n$$\n\nBut using Lemma 5.4, we have\n\n$$\n\\mathbb{P}\\left(\\left|\\mu-\\widehat{\\mu}_{i}\\right| \\geq 3 R+r\\right) \\leq k^{\\prime} e^{-\\frac{r^{2}}{8 \\sigma^{2}}}\n$$\n\nwhich implies\n\n$$\n\\mathbb{P}_{(\\mu, y)}\\left(\\left|\\mu-\\widehat{\\mu}_{i}\\right| \\geq R^{\\prime}+\\sqrt{\\frac{a}{2}}\\right) \\leq k^{\\prime} e^{-\\frac{\\left(2 \\sqrt{2} \\sigma \\sqrt{\\ln \\left(\\frac{k^{\\prime}}{7}\\right)}+\\sqrt{\\frac{a}{2}}\\right)^{2}}{8 \\sigma^{2}}} \\leq k^{\\prime} e^{-\\frac{8 \\sigma^{2} \\ln \\left(\\frac{k^{\\prime}}{7}\\right)+a / 2}{8 \\sigma^{2}}}=\\varepsilon e^{-\\frac{a}{16 \\sigma^{2}}}\n$$\n\nPlugging Equation (23) into Equation (22),\n\n$$\n\\left\\|f_{\\sigma^{2}}-f_{\\operatorname{loc}, \\sigma^{2}}\\right\\|_{L^{2}(P)}^{2} \\leq 2 \\int_{0}^{\\infty} \\varepsilon e^{-\\frac{a}{16 \\sigma^{2}}} d a+6 \\varepsilon R^{\\prime 2}=32 \\varepsilon \\sigma^{2}+6 \\varepsilon R^{\\prime 2}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 19,
      "text": "# 5.2 Approximation within a Voronoi cell \n\nIn the multiple cluster case, in order to apply Lemma 4.4 for approximating the score function in the Voronoi cell of a warm-start $\\widehat{\\mu}_{i}$ by a low-degree polynomial, we also need to bound the R\u00e9nyi divergence of the mixture measure $P$ restricted to that Voronoi cell of $\\widehat{\\mu}_{i}$, with respect to the Gaussian measure around $\\widehat{\\mu}_{i}$.\n\nLemma 5.5. Let $\\mathcal{C}=\\left\\{\\widehat{\\mu}_{1}, \\ldots, \\widehat{\\mu}_{k^{\\prime}}\\right\\}$ and $V_{1}, \\ldots, V_{k^{\\prime}}$ be the corresponding Voronoi partition. Suppose that $\\mathcal{C}$ is a complete set of $R$-warm starts. Then\n\n$$\n\\int_{V_{i}}\\left(\\frac{d P_{V_{i}}}{d \\gamma_{\\widehat{\\mu}_{i}, 1}}\\right)^{p+1} d \\gamma_{\\widehat{\\mu}_{i}, 1} \\leq \\exp \\left(\\frac{p R^{2}}{2}\\right)\n$$\n\nProof. We can write\n\n$$\nP_{V_{i}}=Q_{0}\\left(V_{i}\\right) \\frac{P^{V_{i}}}{Q_{0}\\left(V_{i}\\right)}+Q_{0}\\left(V_{i}^{c}\\right) \\frac{P^{V_{i}^{c}}}{Q_{0}\\left(V_{i}^{c}\\right)}\n$$\n\nnote that $Q_{0}\\left(V_{i}\\right)$ and $Q_{0}\\left(V_{i}^{c}\\right)$ are the normalizing constants for the respective probability measures. By convexity, it suffices to show that (24) holds for $P^{V_{i}}$ and $P^{V_{i}^{c}}$ in place of $P_{V_{i}}$.\n\nFor $P^{V_{i}}$, (24) follows directly from Lemma B.4. For $P^{V_{i}^{c}}$, noting that it is a convex mixture of $\\mathcal{N}\\left(\\mu_{j}, I_{n}\\right)$ for $\\mu_{j} \\notin V_{i}$, again by convexity it suffices to show that (24) holds for one such $\\mathcal{N}\\left(\\mu_{j}, I_{n}\\right)$. Suppose that $\\mu_{j} \\in V_{j}, j \\neq i$. Note that by definition of the Voronoi cell, $\\left|\\mu_{j}-\\widehat{\\mu}_{i}\\right| \\geq\\left|\\mu_{j}-\\widehat{\\mu}_{j}\\right|$ so $\\frac{d \\gamma_{\\widehat{\\mu}_{j}, 1}}{d \\gamma_{\\widehat{\\mu}_{j}, 1}} \\leq 1$ on $V_{i}$. Then\n\n$$\n\\begin{aligned}\n\\int_{V_{i}}\\left(\\frac{d \\gamma_{\\mu_{j}, 1}}{d \\gamma_{\\widehat{\\mu}_{i}, 1}}\\right)^{p+1} d \\gamma_{\\widehat{\\mu}_{i}, 1} & =\\int_{V_{i}}\\left(\\frac{d \\gamma_{\\mu_{j}, 1}}{d \\gamma_{\\widehat{\\mu}_{i}, 1}}\\right)^{p} d \\gamma_{\\mu_{j}, 1} \\\\\n& \\leq \\int_{V_{i}}\\left(\\frac{d \\gamma_{\\mu_{j}, 1}}{d \\gamma_{\\widehat{\\mu}_{j}, 1}}\\right)^{p} d \\gamma_{\\mu_{j}, 1} \\\\\n& =\\int_{V_{i}}\\left(\\frac{d \\gamma_{\\mu_{j}, 1}}{d \\gamma_{\\widehat{\\mu}_{j}, 1}}\\right)^{p+1} d \\gamma_{\\widehat{\\mu}_{j}, 1} \\leq e^{\\frac{p R^{2}}{2}}\n\\end{aligned}\n$$\n\nwhere the last step follows from Lemma B.4, as $\\left\\|\\mu_{j}-\\widehat{\\mu}_{j}\\right\\| \\leq R$.\nLemma 5.6 (Existence of low-degree polynomial). Suppose that $f=f_{\\sigma^{2}}$ as in (4), $Q_{0}$ is supported on $B_{D}, \\mathcal{C}=\\left\\{\\widehat{\\mu}_{1}, \\ldots, \\widehat{\\mu}_{k^{\\prime}}\\right\\}$ is a complete set of $R$-warm starts for $Q_{0}$, and let $V_{1}, \\ldots, V_{k^{\\prime}}$ be the corresponding Voronoi partition. For each $i$, there is a polynomial $g_{i}$ of degree at most $O\\left(\\left(\\frac{R}{\\sigma}+\\sqrt{\\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)}\\right)^{6} \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4}\\right)$ such that\n\n$$\n\\left\\|f-g_{i}\\right\\|_{P_{V_{i}}}^{2} \\lesssim \\varepsilon^{2}\\left(R^{2}+\\sigma^{2} \\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)\\right) \\quad \\text { and } \\quad\\left\\|g_{i}\\right\\|_{\\gamma_{\\widehat{\\mu}_{i}, \\sigma^{2}}} \\leq D\n$$\n\nProof. Let $R^{\\prime}=3 R+2 \\sqrt{2} \\sigma \\sqrt{\\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)}$. First, note that from Lemma 5.3,\n\n$$\n\\sum_{i=1}^{k^{\\prime}}\\left\\|f_{\\sigma^{2}}-f_{S_{i}, \\sigma^{2}}\\right\\|_{L^{2}\\left(P_{V_{i}}\\right)}^{2}=\\left\\|f_{\\sigma^{2}}-f_{\\operatorname{loc}, \\sigma^{2}}\\right\\|_{L^{2}(P)}^{2} \\lesssim\\left(R^{2}+\\sigma^{2} \\ln \\left(\\frac{k^{\\prime}}{\\varepsilon}\\right)\\right) \\varepsilon^{2}\n$$\n\nwhere $S_{i}$ is the expanded neighborhood in Lemma 5.3. By Lemma 4.5 applied to $P_{V_{i}}, f_{\\text {loc, } \\sigma^{2}}$ is $\\left(O\\left(R^{\\prime} / \\sigma\\right), \\sigma\\right)$-Gaussian-noise-sensitive. By Lemma 5.5, for each $p>0,\\left\\|\\frac{d P_{V_{i}}}{d \\gamma_{\\widehat{\\mu}_{i}, \\sigma^{2}}}\\right\\|_{L^{1+p}\\left(\\gamma_{\\widehat{\\mu}_{i}, \\sigma^{2})}\\right.} \\leq$ $e^{\\frac{\\alpha(R / \\sigma)^{2}}{2}}$. Hence $P_{V_{i}}$ satisfies the conditions of Lemma 4.5, and there exists a polynomial $g_{i}$ of degree at most\n\n$$\nO\\left(\\left(\\frac{R^{\\prime}}{\\sigma}\\right)^{4} \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4} \\max \\left\\{\\left(\\frac{R^{\\prime}}{\\sigma}\\right)^{2}, \\ln \\left(\\frac{1}{\\varepsilon}\\right)\\right\\}\\right)=O\\left(\\left(\\frac{R^{\\prime}}{\\sigma}\\right)^{6} \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4}\\right)\n$$\n\nsuch that\n\n$$\n\\left\\|f_{\\operatorname{loc}, \\sigma^{2}}-g_{i}\\right\\|_{P_{V_{i}}}^{2}=\\left\\|f^{S_{i}}-g_{i}\\right\\|_{P_{V_{i}}}^{2} \\leq \\varepsilon^{2} R^{\\prime 2}\n$$\n\nCombining (26) and (27) gives the result. Finally, from Lemma 4.5,\n\n$$\n\\left\\|g_{i}\\right\\|_{\\gamma_{\\widehat{\\mu}_{i}, \\sigma^{2}}} \\leq\\left\\|f_{\\operatorname{loc}, \\sigma^{2}}\\right\\|_{P_{V_{i}}} \\leq D\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "# 5.3 Piecewise polynomial regression \n\nThus far, we have demonstrated the existence of low-degree polynomials in the Voronoi cells corresponding to the warm starts, that provide a good approximation of the score function with respect to the mixture measure in each cell. Now we investigate how many samples we need, so that applying polynomial regression separately in each Voronoi cell enables us to recover the coefficients of these low-degree polynomials accurately. For $\\left(x_{i}, y_{i}\\right)$ generated according to (2), by (6) we wish to find the least-squares solution to\n\n$$\ny_{i}+t s_{t}\\left(y_{i}\\right) \\approx x_{i}\n$$\n\nwhere by (4) we parameterize\n\n$$\nf_{\\sigma^{2}}(y)=y+\\sigma^{2} s_{t}(y)=\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}(y-\\widehat{\\mu})\n$$\n\nwhere $\\widehat{\\mu}$ is a warm start. Equivalently, we wish to find the least-squares solution to\n\n$$\n\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}\\right) \\approx y_{i}+\\frac{\\sigma^{2}}{t}\\left(x_{i}-y_{i}\\right)=\\left(1-\\frac{\\sigma^{2}}{t}\\right) y_{i}+\\frac{\\sigma^{2}}{t} x_{i}\n$$\n\nWe can restrict to $\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d} \\in B_{R}\\left(\\right.$ in $\\left.\\mathbb{R}^{n\\binom{|n|}{\\leq d}}\\right)$, so we solve\n\n$$\n\\underset{\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d} \\in B_{R}}{\\arg \\min } \\widehat{L}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right) \\quad \\text { where } \\quad \\widehat{L}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)=\\frac{1}{N} \\sum_{i=1}^{N}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}\\right)-\\left(\\left(1-\\frac{\\sigma^{2}}{t}\\right) y_{i}+\\frac{\\sigma^{2}}{t} x_{i}\\right)\\right|^{2}\n$$\n\nLet $\\bar{x}_{i}=\\mathbb{E}\\left[X \\mid Y=y_{i}\\right]=y_{i}+t \\nabla V_{t}\\left(y_{i}\\right)$ and $x_{i}=\\bar{x}_{i}+\\zeta_{i}$, so that conditioned on $y_{i}, \\zeta_{i}$ is mean-zero noise. Let $\\eta_{i}=\\frac{\\sigma^{2}}{t} \\zeta_{i}$. Let $z_{i}=\\mathbb{E}\\left[\\mu \\mid Y=y_{i}\\right]=\\left(1-\\frac{\\sigma^{2}}{t}\\right) y_{i}+\\frac{\\sigma^{2}}{t} \\bar{x}_{i}$. Then, for each Voronoi cell $V_{j}$, we calculate the empirical loss for samples where $y_{i}$ falls into $V_{j}$,\n\n$$\n\\widehat{L}^{(j)}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)=\\frac{1}{N} \\sum_{i: y_{i} \\in V_{j}}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right)-\\left(z_{i}+\\eta_{i}\\right)\\right|^{2}\n$$\n\nand the empirical risk minimizer (ERM) for the $j$ th Voronoi cell as\n\n$$\n\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}:=\\underset{\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d} \\in B_{M}}{\\arg \\min } \\widehat{L}^{(j)}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)\n$$\n\nWe then combine these ERM solutions for all Voronoi cells and define the following piecewise polynomial function to approximate the score:\n\n$$\n\\widehat{g}(y)=\\sum_{j=1}^{k^{\\prime}} \\mathbb{1}_{V_{j}}(y) \\sum_{|\\mathbf{k}| \\leq d} \\widehat{b}_{\\mathbf{k}}^{(j)} h_{\\mathbf{k}}\\left(y-\\widehat{\\mu}_{j}\\right)\n$$\n\nwhere $k^{\\prime}$ is the total number of Voronoi cells. To show that $\\widehat{g}$ is a decent approximation for the score function $f_{\\sigma^{2}}$, we need to bound the generalization error of these ERM solutions (28). To do this, we derive moment bounds for the features and the noise. Let\n\n$$\nL^{(j)}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)=\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} \\mathbb{1}_{V_{j}}(Y)\\left(b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(Y-\\widehat{\\mu}_{j}\\right)-(Z+\\eta)\\right)\\right|^{2}\n$$\n\ndenote the population risk in $V_{j}$. We further define the shifted error of the empirical loss with respect to the population loss as\n\n$$\n\\begin{aligned}\n\\operatorname{err}^{(j)}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right):= & \\left(\\widehat{L}^{(j)}-L^{(j)}\\right)\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)+\\frac{1}{N} \\sum_{i: y_{i} \\in V_{j}}\\left\\langle\\sum_{|\\mathbf{k}| \\leq d} z_{i}, \\eta_{i}\\right\\rangle-\\frac{1}{N} \\sum_{i: y_{i} \\in V_{j}}\\left(\\left|\\eta_{i}\\right|^{2}-\\mathbb{E}\\left|\\eta_{i}\\right|^{2}\\right) \\\\\n= & \\frac{1}{N} \\sum_{i: y_{i} \\in V_{j}}\\left\\langle\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right), \\eta_{i}\\right\\rangle \\\\\n& +\\frac{1}{N} \\sum_{i: y_{i} \\in V_{j}}\\left(\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right)-z_{i}\\right|^{2}-\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right)-z_{i}\\right|^{2}\\right)\n\\end{aligned}\n$$\n\nThe following lemma is standard and relates the generalization gap of ERM to $\\operatorname{err}^{(j)}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)$. As we see shortly, the terms in the definition (30) that do not depend on $\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}$ cancel out in the calculation of the generalization gap.\nLemma 5.7 (Uniform convergence $\\Rightarrow$ Generalization gap). The generalization gap for the ERM solution can be bounded as\n\n$$\nL^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}\\right)-\\min _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}} L^{(j)}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right) \\leq 2 \\max _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}}\\left|\\operatorname{err}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)\\right|\n$$\n\nProof. To bound the generalization gap of the ERM solution $\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d_{i}, 1 \\leq j \\leq n}$ output by Algorithm 1.1, we compare its loss with the loss of an arbitrary point $\\left(\\widehat{b}_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}$ in $B_{L}(0)$ :\n\n$$\n\\begin{aligned}\nL^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}\\right)-L^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)= & L^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}\\right)-\\widehat{L}^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}\\right) \\\\\n& +\\widehat{L}^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}\\right)-\\widehat{L}^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right) \\\\\n& +\\widehat{L}^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)-L^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)\n\\end{aligned}\n$$\n\nNote that $\\widehat{L}^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}\\right)-\\widehat{L}^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right) \\leq 0$ by the definition of $\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}$ as a minimizer of $\\widehat{L}^{(j)}$. Furthermore, by (30),\n\n$$\n\\begin{aligned}\n& L^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}\\right)-\\widehat{L}^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}\\right)+\\widehat{L}^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)-L\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right) \\\\\n& =\\operatorname{err}^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)-\\operatorname{err}^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}^{(j)}\\right)_{|\\mathbf{k}| \\leq d}\\right) \\leq 2 \\max _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}(0)}\\left|\\operatorname{err}^{(j)}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)\\right|\n\\end{aligned}\n$$\n\nBased on Lemma 5.7 and Equation (30), to bound the generalization gap it suffices to bound\n\n$$\n\\begin{aligned}\n& \\max _{\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d} \\in B_{M}}\\left|\\operatorname{err}\\left(\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right)\\right| \\leq \\max _{\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d} \\in B_{M}} \\frac{1}{N} \\sum_{i: y_{i} \\in V_{j}}\\left\\langle\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right), \\eta_{i}\\right\\rangle \\\\\n& \\quad+\\max _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}} \\frac{1}{N} \\sum_{i: y_{i} \\in V_{j}}\\left(\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right)-z_{i}\\right|^{2}-\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right)-z_{i}\\right|^{2}\\right)\n\\end{aligned}\n$$\n\nLet $h(y) \\in \\mathbb{R}\\binom{[n]}{\\leq d}$ be given by $h(y)_{\\mathbf{k}}=h_{\\mathbf{k}}(y)$. Let $B \\in \\mathbb{R}^{n \\times\\binom{[n]}{\\leq d}}$ with columns $b_{\\mathbf{k}}$. Then $\\left|\\left(b_{\\mathbf{k}}\\right)_{|\\mathbf{k}| \\leq d}\\right|=\\|B\\|_{F}$ so\n\n$$\n\\begin{aligned}\n\\max _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}} \\sum_{i: y_{i} \\in V_{j}} \\sum_{|\\mathbf{k}| \\leq d}\\left\\langle b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right), \\eta_{i}\\right\\rangle & =\\max _{\\|B\\|_{F} \\leq M} \\operatorname{Tr}\\left(B \\sum_{i: y_{i} \\in V_{j}} h\\left(y_{i}-\\widehat{\\mu}_{j}\\right) \\eta_{i}^{\\top}\\right) \\\\\n& =M\\left\\|\\sum_{i: y_{i} \\in V_{j}} h\\left(y_{i}-\\widehat{\\mu}_{j}\\right) \\eta_{i}^{\\top}\\right\\|_{F}\n\\end{aligned}\n$$\n\nWe carefully bound the moments of each of the terms in Equation (33) in Lemmas 5.9 and 5.11, leading to high-probability bounds given in Lemmas 5.10 and 5.12, respectively. From this, we obtain the following high probability bound on the generalization gap.\n\nLemma 5.8 (Generalization gap). Suppose $Q_{0}$ is supported on $B_{D}(0)$ and $\\mathcal{C}$ is a complete set of $R$-warm starts for $Q_{0}$ with $\\left|\\mathcal{C}^{\\prime}\\right|=k^{\\prime}$ and $d \\geq(R / \\sigma)^{2}$. Given $N$ pairs of samples $\\left(\\mu_{i}, y_{i}\\right)$ distributed as $y_{i}=\\mu_{i}+\\xi_{i}$, for $\\mu_{i} \\sim Q_{0}$ and $\\xi_{i} \\sim \\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)$ with\n\n$$\nN=\\Omega\\left(\\frac{n\\left(k^{\\prime}(1+M+D)^{2}\\right)^{2}\\left(\\ln (k / \\delta) \\sqrt{n / d+1}\\right)^{4(d+1)}}{\\varepsilon^{4}}\\right)\n$$\n\nwe have with probability at least $1-\\delta$ that\n\n$$\n\\max _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}(0)}\\left|\\operatorname{err}^{(j)}\\left(\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right)\\right| \\leq \\frac{\\varepsilon^{2}}{k^{\\prime}}\n$$\n\nFurthermore, the piecewise polynomial approximation of the score defined in Equation (29) satisfies\n\n$$\n\\|\\widehat{g}-f\\|_{L^{2}(P)}^{2}-\\|\\widehat{g}-f\\|_{L^{2}(P)}^{2} \\leq \\varepsilon^{2}\n$$\n\nfor any other piece-wise polynomial $\\widetilde{g}$ on the Voronoi cells whose coefficients satisfy $\\left(\\widetilde{b}_{\\mathbf{k}}^{(j)}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}$ for each cell $j$.\n\nWe defer the proof to Section 5.3.2.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "# 5.3.1 Moment and high-probability bounds \n\nFor the following lemmas, we assume the following: $Q_{0}$ is supported on $B_{D}(0), \\mathcal{C}$ is a complete set of $R$-warm starts for $Q_{0}$, and $V_{1}, \\ldots, V_{k^{\\prime}}$ is the corresponding Voronoi partition. We are given iid samples $\\left(\\mu_{i}, y_{i}\\right), i=1, \\ldots, N$, where $y_{i}=\\mu_{i}+\\xi_{i}, \\mu_{i} \\sim Q_{0}$, and $\\xi_{i} \\sim \\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)$. Recall that $h_{\\mathbf{k}}$ are the $\\sigma$-rescaled Hermite polynomials and $h(y)=\\left(h_{\\mathbf{k}}(y)\\right)_{|\\mathbf{k}| \\leq d}$. We moreover fix the Voronoi cell $V_{j}$ corresponding to $\\widehat{\\mu}_{j}$, and (to simplify notation) suppose that $\\widehat{\\mu}_{j}=0$.\n\nLemma 5.9 (Moment bound for first term). We have the following uniform convergence bound for $p$ even:\n\n$$\n\\left\\|\\left\\|\\sum_{i=1}^{N} \\mathbb{1}_{V_{j}}\\left(y_{i}\\right) h\\left(y_{i}\\right) \\eta_{i}^{\\top}\\right\\|_{F}\\right\\|_{2 p}=O\\left(D n^{1 / 2}\\left(\\frac{n}{d}+1\\right)^{d / 2} \\sqrt{p}(e(p-1))^{d} \\sqrt{N}\\right)\n$$\n\nWe will use properties of sub-exponential random variables; see Appendix D for background.\nProof. In the following, the expectations without index refer to the mixture measure. Let $\\mu_{i}^{(2)}$ be independent and identically distributed to $\\mu_{i} \\mid Y=y_{i}$. Without loss of generality suppose the samples in which $y_{i}$ fall into $V_{j}$ are exactly $y_{1}, \\ldots, y_{N_{j}}$. Using the symmetrization technique with Jensen's inequality:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right) \\eta_{i}^{\\top}\\right\\|_{F}^{2 p} & =\\mathbb{E}\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mu_{i}-\\mathbb{E}\\left[\\mu_{i}\\left|Y=y_{i}\\right|\\right)^{\\top}\\right]\\right\\|_{F}^{2 p} \\\\\n& =\\mathbb{E}\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mathbb{E}\\left[\\mu_{i}\\left|Y=y_{i}\\right|\\right)^{\\top}\\right]\\right\\|_{F}^{2 p} \\\\\n& =\\mathbb{E}\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mathbb{E}_{\\mu_{i}}\\left[\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)\\left|Y=y_{i}\\right|\\right)^{\\top}\\right]\\right\\|_{F}^{2 p} \\\\\n& \\leq \\mathbb{E}\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)^{\\top}\\right\\|_{F}^{2 p}\n\\end{aligned}\n$$\n\nNote that conditioned on any fixed value of $y_{i}$, the variable $\\mu_{i}^{(2)}-\\mu_{i}$ is the difference between two iid random variables, each of whose norm is bounded by $D$. Therefore, given a fixed $\\left\\{y_{i}\\right\\}_{i=1}^{N_{j}}$, the variable $\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)^{\\top}\\right\\|_{F}^{2}$ is a quadratic form of a sub-Gaussian vector. From the properties of sub-Gaussian variables, for every $1 \\leq k \\leq n$ and $|\\mathbf{k}| \\leq d$,\n\n$$\n\\left(\\sum_{i=1}^{N_{j}} h_{\\mathbf{k}}\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)_{k}\\right)^{2}\n$$\n\nis sub-exponential with parameter $\\left(O\\left(\\left(\\sum_{i=1}^{N_{j}} h_{\\mathbf{k}}\\left(y_{i}\\right)^{2} D^{2}\\right)^{2}\\right), O\\left(\\sum_{i=1}^{N_{j}} h_{\\mathbf{k}}\\left(y_{i}\\right)^{2} D^{2}\\right)\\right)$. Therefore, noting we do not have independence when we enumerate the variables over $\\mathbf{k}$, by Proposition D.4, $\\sum_{\\mathbf{k}}\\left(\\sum_{i=1}^{N_{j}} h_{\\mathbf{k}}\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)_{k}\\right)^{2}$ is sub-exponential with parameters\n\n$$\n\\left(\\binom{n+d}{d} O\\left(\\sum_{|\\mathbf{k}| \\leq d}\\left(\\sum_{i=1}^{N_{j}} h_{\\mathbf{k}}\\left(y_{i}\\right)^{2} D^{2}\\right)^{2}\\right),\\binom{n+d}{d} O\\left(\\max _{|\\mathbf{k}| \\leq d} \\sum_{i=1}^{N_{j}} h_{\\mathbf{k}}\\left(y_{i}\\right)^{2} D^{2}\\right)\\right)\n$$\n\nFinally summing over $1 \\leq k \\leq n$, from the independence of the coordinates of $\\left(\\mu^{(2)}-\\mu\\right)$, by Proposition D.3, the variable\n\n$$\nX:=\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)^{\\top}\\right\\|_{F}^{2}=\\sum_{k=1}^{n} \\sum_{|\\mathbf{k}| \\leq d}\\left(\\sum_{i=1}^{N_{j}} h_{\\mathbf{k}}\\left(y_{i}\\right)\\left(\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)^{\\top}\\right)_{k}\\right)^{2}\n$$\n\nis sub-exponential with parameters\n\n$$\n\\left(v^{2}, \\alpha\\right):=\\left(n\\binom{n+d}{d} O\\left(\\sum_{|\\mathbf{k}| \\leq d}\\left(\\sum_{i=1}^{N_{j}} h_{\\mathbf{k}}\\left(y_{i}\\right)^{2} D^{2}\\right)^{2}\\right), n\\binom{n+d}{d} O\\left(\\max _{|\\mathbf{k}| \\leq d} \\sum_{i=1}^{N_{j}} h_{\\mathbf{k}}\\left(y_{i}\\right)^{2} D^{2}\\right)\\right)\n$$\n\nand has expectation\n\n$$\n\\mathbb{E}\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)^{\\top}\\right\\|_{F}^{2}=\\sum_{i=1}^{N_{j}}\\left|h\\left(y_{i}\\right)\\right|^{2} \\mathbb{E}\\left|\\mu_{i}-\\mu_{i}^{(2)}\\right|^{2}=4 D^{2} \\sum_{i=1}^{N_{j}}\\left|h\\left(y_{i}\\right)\\right|^{2}\n$$\n\nBut this implies\n\n$$\n\\mathbb{P}(X-\\mathbb{E}[X] \\geq t) \\leq e^{-t / \\kappa}\n$$\n\nfor parameter\n\n$$\n\\kappa=O(\\max (v, \\alpha))=O\\left(D^{2} n\\binom{n+d}{d} \\sum_{i=1}^{N_{j}}\\left|h\\left(y_{i}\\right)\\right|^{2}\\right)\n$$\n\nThus, using the second property for sub-exponential variables in Proposition D.2, for some constant $c_{1}$ we have\n\n$$\n\\mathbb{E}\\left[\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)^{\\top}\\right\\|_{F}^{2 p}\\left|\\left\\{y_{i}\\right\\}_{i=1}^{N_{j}}\\right] \\leq O(p \\kappa+\\mathbb{E} X)^{p}=\\left(c_{1} p D^{2} n\\binom{n+d}{d} \\sum_{i=1}^{N_{j}}\\left|h\\left(y_{i}\\right)\\right|^{2}\\right)^{p}\n$$\n\nThen by Lemma B.3,\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)^{\\top}\\right\\|_{F}^{2 p} & =\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right)\\left(\\mu_{i}^{(2)}-\\mu_{i}\\right)^{\\top}\\right\\|_{F}^{2 p}\\left|\\left\\{y_{i}\\right\\}_{i=1}^{N_{j}}\\right]\\right]\\right] \\\\\n& \\leq \\mathbb{E}\\left(c_{1} p D^{2} n\\binom{n+d}{d} \\sum_{i=1}^{N_{j}}\\left|h\\left(y_{i}\\right)\\right|^{2}\\right)^{p}\n\\end{aligned}\n$$\n\nTherefore, given $\\gamma=\\gamma_{\\widehat{\\mu}_{j}, \\sigma^{2}}$ is the Gaussian around the warm start point $\\widehat{\\mu}_{j}$, and Lemma B. 3 applied\n\nto $|h|^{p}$ (which is a polynomial as $p$ is even) and Lemma B.2,\n\n$$\n\\begin{aligned}\n& \\left\\|\\left\\|\\sum_{i=1}^{N} \\mathbb{1}_{V_{j}}\\left(y_{i}\\right) h\\left(y_{i}\\right) \\eta_{i}^{\\top}\\right\\|_{F}\\right\\|_{2 p} \\leq \\sqrt{c}_{1} \\sqrt{p} D\\left(n\\binom{n+d}{d}^{1 / 2} \\sqrt{\\left\\|\\sum_{i=1}^{N_{j}}\\left|h\\left(y_{i}\\right)\\right|^{2}\\right\\|_{p}}\\right. \\\\\n& \\leq \\sqrt{c}_{1} \\sqrt{p} D\\left(n\\binom{n+d}{d}^{1 / 2} \\sqrt{\\sum_{i=1}^{N_{j}}\\left\\|\\left|h\\left(y_{i}\\right)\\right|^{2}\\right\\|_{p}}\\right. \\\\\n& \\leq \\sqrt{c}_{1} \\sqrt{p} D\\left(n\\binom{n+d}{d}^{1 / 2} \\sqrt{\\sum_{i=1}^{N_{j}} e^{2 \\sqrt{d / p}(R / \\sigma)}\\left\\|\\left|h\\left(y_{i}\\right)\\right|^{2}\\right\\|_{L^{p}(\\gamma)}}\\right. \\\\\n& \\leq \\sqrt{c}_{1} \\sqrt{p} D\\left(n\\binom{n+d}{d}^{1 / 2} \\sqrt{\\sum_{i=1}^{N_{j}} e^{2 \\sqrt{d / p}(R / \\sigma)}(p-1)^{2 d}\\left\\|\\left|h\\left(y_{i}\\right)\\right|^{2}\\right\\|_{L^{2}(\\gamma)}\\right. \\\\\n& \\leq c_{2} \\sqrt{p} D n^{1 / 2}\\left(\\frac{n}{d}+1\\right)^{d / 2}(e(p-1))^{d} \\sqrt{N},\n\\end{aligned}\n$$\n\nfor some universal constant $c_{2}$, and we used the upper bound $\\binom{n}{k} \\leq \\frac{1}{e}\\left(\\frac{c n}{k}\\right)^{k}$.\nLemma 5.10 (High probability bound for first term). We have\n\n$$\n\\mathbb{P}\\left(\\left\\|\\sum_{i=1}^{N} \\mathbb{1}_{V_{j}}\\left(y_{i}\\right) h\\left(y_{i}\\right) \\eta_{i}^{\\top}\\right\\|_{F} \\geq t\\right) \\leq \\exp \\left(-\\Omega\\left(\\left(\\frac{n}{d}+1\\right)^{-1 / 2}\\left(\\frac{t}{D \\sqrt{N} n^{1 / 2}}\\right)^{1 /(d+1)}\\right)\\right)\n$$\n\nProof. By Lemma 5.9 and Markov's inequality,\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(\\left\\|\\sum_{i=1}^{N} \\mathbb{1}_{V_{j}}\\left(y_{i}\\right) h\\left(y_{i}\\right) \\eta_{i}^{\\top}\\right\\|_{F} \\geq t\\right) & =\\mathbb{P}\\left(\\left\\|\\sum_{i=1}^{N} \\mathbb{1}_{V_{j}}\\left(y_{i}\\right) h\\left(y_{i}\\right) \\eta_{i}^{\\top}\\right\\|_{F}^{2 p} \\geq t^{2 p}\\right) \\\\\n& \\leq \\frac{\\mathbb{E}\\left\\|\\sum_{i=1}^{N} \\mathbb{1}_{V_{j}}\\left(y_{i}\\right) h\\left(y_{i}\\right) \\eta_{i}^{\\top}\\right\\|_{F}^{2 p}}{t^{2 p}} \\\\\n& \\leq\\left(\\frac{D n^{1 / 2}\\left(\\frac{n}{d}+1\\right)^{d / 2}(e(p-1))^{d+1} \\sqrt{N}}{t}\\right)^{2 p}\n\\end{aligned}\n$$\n\nPicking\n\n$$\np-1=\\Theta\\left(\\frac{\\left(t /\\left(D \\sqrt{N} n^{1 / 2}\\right)\\right)^{1 /(d+1)}}{\\left(\\frac{n}{d}+1\\right)^{1 / 2}}\\right)\n$$\n\nwith appropriate constant, we have\n\n$$\n\\mathbb{P}\\left(\\left\\|\\sum_{i=1}^{N} \\mathbb{1}_{V_{j}}\\left(y_{i}\\right) h\\left(y_{i}\\right) \\eta_{i}^{\\top}\\right\\|_{F} \\geq t\\right) \\leq \\exp \\left(-\\Omega\\left(\\left(\\frac{n}{d}+1\\right)^{-1 / 2}\\left(\\frac{t}{D \\sqrt{N} n^{1 / 2}}\\right)^{1 /(d+1)}\\right)\\right)\n$$\n\nLemma 5.11 (Moment bound for second term). Assume $d \\geq(R / \\sigma)^{2}$. We have the following uniform convergence bound:\n\n$$\n\\begin{aligned}\n& \\left\\|\\max _{\\left(b_{\\mathbf{k}}\\right) \\mid \\mathbf{k} \\leq d \\in B_{M}} \\sum_{i=1}^{N} \\mathbb{1}_{V_{j}}\\left(y_{i}\\right)\\left(\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}-\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}\\right)\\right\\|_{2 p} \\\\\n& =O\\left(M(M+D) n^{1 / 2} e^{d}\\left(\\frac{n}{d}+1\\right)^{d} p(4(4 p-1))^{2 d} \\sqrt{N}+\\sqrt{p N} D^{2}\\right)\n\\end{aligned}\n$$\n\nProof. Without loss of generality, suppose the first $N_{j}$ samples $y_{1}, \\ldots, y_{N_{j}}$ are in $V_{j}$. Again using symmetrization for the $y_{i}$ 's,\n\n$$\n\\begin{aligned}\n& \\left\\|\\max _{\\left(b_{\\mathbf{k}}\\right) \\mid \\mathbf{k} \\leq d \\in B_{M}} \\sum_{i=1}^{N_{j}}\\left(\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}-\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}\\right)\\right\\|_{2 p} \\\\\n& \\leq\\left\\|\\max _{\\left(b_{\\mathbf{k}}\\right) \\mid \\mathbf{k} \\leq d \\in B_{M}} \\sum_{i=1}^{N_{j}}\\left(\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}^{(2)}\\right)-z_{i}^{(2)}\\right|^{2}-\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}\\right)\\right\\|_{2 p} \\\\\n& \\leq\\left\\|\\max _{\\|B\\|_{F} \\leq M} \\operatorname{tr}\\left(B\\left(\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right) h\\left(y_{i}\\right)^{\\top}-\\sum_{i=1}^{N_{j}} h\\left(y_{i}^{(2)}\\right) h\\left(y_{i}^{(2)}\\right)^{\\top}\\right) B^{\\top}\\right)\\right\\|_{2 p} \\\\\n& \\quad+\\left\\|\\max _{\\|B\\|_{F} \\leq M}\\left\\{\\sum_{i=1}^{N_{j}} z_{i}^{\\top} B h\\left(y_{i}\\right)-\\sum_{i=1}^{N_{j}} z_{i}^{(2)^{\\top}} B h\\left(y_{i}^{(2)}\\right)\\right\\}\\right\\|_{2 p}+\\left\\|\\sum_{i=1}^{N_{j}}\\left(\\left|z_{i}^{(2)}\\right|^{2}-\\left|z_{i}\\right|^{2}\\right)\\right\\|_{2 p} \\\\\n& \\leq\\left\\|M^{2}\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right) h\\left(y_{i}\\right)^{\\top}-\\sum_{i=1}^{N_{j}} h\\left(y_{i}^{(2)}\\right) h\\left(y_{i}^{(2)}\\right)^{\\top}\\right\\|_{o p}\\right\\|_{2 p} \\\\\n& \\quad+\\left\\|\\max _{\\|B\\|_{F} \\leq M}\\left\\langle B, \\sum_{i=1}^{N_{j}} z_{i} h\\left(y_{i}\\right)^{\\top}-\\sum_{i=1}^{N_{j}} z_{i}^{(2)} h\\left(y_{i}^{(2)}\\right)^{\\top}\\right\\rangle\\right\\|_{2 p}+\\left\\|\\sum_{i=1}^{N_{j}}\\left(\\left|z_{i}^{(2)}\\right|^{2}-\\left|z_{i}\\right|^{2}\\right)\\right\\|_{2 p} \\\\\n& \\leq M^{2}\\left\\|\\left\\|\\sum_{i=1}^{N_{j}} h\\left(y_{i}\\right) h\\left(y_{i}\\right)^{\\top}-\\sum_{i=1}^{N_{j}} h\\left(y_{i}^{(2)}\\right) h\\left(y_{i}^{(2)}\\right)^{\\top}\\right\\|_{F}\\right\\|_{2 p} \\\\\n& \\quad+M\\left\\|\\left\\|\\sum_{i=1}^{N_{j}} z_{i} h\\left(y_{i}\\right)^{\\top}-\\sum_{i=1}^{N_{j}} z_{i}^{(2)} h\\left(y_{i}^{(2)}\\right)^{\\top}\\right\\|_{F}\\right\\|_{2 p}+\\left\\|\\sum_{i=1}^{N_{j}}\\left(\\left|z_{i}^{(2)}\\right|^{2}-\\left|z_{i}\\right|^{2}\\right)\\right\\|_{2 p}\n\\end{aligned}\n$$\n\nFor the third term, note that for each $1 \\leq i \\leq N$ :\n\n$$\n\\left|\\left|z_{i}^{(2)}\\right|^{2}-\\left|z_{i}\\right|^{2}\\right| \\leq 2 D^{2}\n$$\n\nand this has mean zero. Hence the third term is sub-Gaussian with parameter $O\\left(D^{2} \\sqrt{N}\\right)$. Therefore\n\n$$\n\\left\\|\\sum_{i=1}^{N_{j}}\\left(\\left|z_{i}^{(2)}\\right|^{2}-\\left|z_{i}\\right|^{2}\\right)\\right\\|_{2 p} \\leq O\\left(\\sqrt{p N} D^{2}\\right)\n$$\n\nFor the second term, note that for all $1 \\leq i \\leq N_{j}, 1 \\leq k \\leq n$, and $|\\mathbf{k}| \\leq\\binom{n}{d}$, given $\\gamma=\\gamma_{\\widehat{\\mu}_{j}, \\sigma^{2}}$ is the Gaussian around the warm start point $\\widehat{\\mu}_{j}$; by Cauchy-Schwarz, Lemma B. 3 applied to $h_{\\mathbf{k}}^{p}$, Lemma B.2, and the assumption that $d \\geq(R / \\sigma)^{2}$,\n\n$$\n\\begin{aligned}\n\\left\\|z_{i k} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i k}^{(2)} h_{\\mathbf{k}}\\left(y_{i}^{(2)}\\right)\\right\\|_{p} & \\leq 2\\left\\|z_{i k} h_{\\mathbf{k}}\\left(y_{i}\\right)\\right\\|_{p} \\\\\n& \\leq 2\\left\\|z_{i k}\\right\\|_{2 p}\\left\\|h_{\\mathbf{k}}\\left(y_{i}\\right)\\right\\|_{2 p} \\\\\n& \\leq 2 D \\cdot 2 \\sqrt{d /(2 p)(R / \\sigma)}\\left\\|h_{\\mathbf{k}}\\left(y_{i}\\right)\\right\\|_{L^{2 p}(\\gamma)} \\\\\n& \\leq 2 D \\cdot 2 \\sqrt{d /(2 p)(R / \\sigma)}(2 p-1)^{d}\\left\\|h_{\\mathbf{k}}\\left(y_{i}\\right)\\right\\|_{L^{2}(\\gamma)} \\\\\n& \\leq 2 D \\cdot(2(2 p-1))^{d}\n\\end{aligned}\n$$\n\nTherefore, from the Rosenthal inequality Ibragimov and Sharakhmetov (2001) with constant given by Pinelis Nagaev and Pinelis (1978), there is a universal constant $c$ such that\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left|\\sum_{i=1}^{N_{j}} z_{i k} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i k}^{(2)} h_{\\mathbf{k}}\\left(y_{i}^{(2)}\\right)\\right|^{p} \\\\\n& \\leq(c p)^{p} \\max \\left\\{\\sum_{i=1}^{N_{j}} \\mathbb{E}\\left|z_{i k} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i k}^{(2)} h_{\\mathbf{k}}\\left(y_{i}^{(2)}\\right)\\right|^{p},\\left(\\sum_{i=1}^{N_{j}} \\mathbb{E}\\left|z_{i k} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i k}^{(2)} h_{\\mathbf{k}}\\left(y_{i}^{(2)}\\right)\\right|^{2}\\right)^{p / 2}\\right\\} \\\\\n& \\leq(c p)^{p} \\max \\left\\{N_{j} 2^{p} D^{p}(2(2 p-1))^{p d},\\left(4 N_{j} D^{2} 6^{2 d}\\right)^{p / 2}\\right\\} \\\\\n& \\leq(2 c p(6(2 p-1))^{d} \\sqrt{N} D)^{p}\n\\end{aligned}\n$$\n\nwhich according to the norm property of $p$ th norm $\\|\\cdot\\|_{p}$ implies\n\n$$\n\\begin{aligned}\n\\left\\|\\sum_{|\\mathbf{k}| \\leq d} \\sum_{k=1}^{n}\\left(\\sum_{i=1}^{N_{j}} z_{i k} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i k}^{(2)} h_{\\mathbf{k}}^{(2)}\\right)^{2}\\right\\|_{p} & \\leq \\sum_{|\\mathbf{k}| \\leq d} \\sum_{k=1}^{n}\\left\\|\\left(\\sum_{i=1}^{N_{j}} z_{i k} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i k}^{(2)} h_{\\mathbf{k}}^{(2)}\\right)^{2}\\right\\|_{p} \\\\\n& \\leq n\\binom{n+d}{d}(4 c p(6(4 p-1))^{d} \\sqrt{N} D)^{2}\n\\end{aligned}\n$$\n\nTherefore, using $\\binom{n}{k} \\leq \\frac{1}{e}\\left(\\frac{e n}{k}\\right)^{k}$,\n\n$$\n\\left\\|\\left\\|\\sum_{i=1}^{N_{j}} z_{i} h\\left(y_{i}\\right)^{\\top}-\\sum_{i=1}^{N_{j}} z_{i}^{(2)} h\\left(y_{i}^{(2)}\\right)^{\\top}\\right\\|_{F}\\right\\|_{2 p} \\leq n^{1 / 2}\\left(e\\left(\\frac{n}{d}+1\\right)\\right)^{d / 2} 4 c p(6(4 p-1))^{d} \\sqrt{N} D\n$$\n\nFor the first term, for all $\\left|\\mathbf{k}_{1}\\right|,\\left|\\mathbf{k}_{2}\\right| \\leq d$, again by Cauchy-Schwarz, Lemma B. 3 applied to $h_{\\mathbf{k}}^{p}$,\n\nLemma B.2, and the assumption that $d \\geq(R / \\sigma)^{2}$,\n\n$$\n\\begin{aligned}\n\\left\\|h_{\\mathbf{k}_{1}}\\left(y_{i}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)-h_{\\mathbf{k}_{1}}\\left(y_{i}^{(2)}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}^{(2)}\\right)\\right\\|_{p} & \\leq 2\\left\\|h_{\\mathbf{k}_{1}}\\left(y_{i}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)\\right\\|_{p} \\\\\n& \\leq 2\\left\\|h_{\\mathbf{k}_{1}}\\left(y_{i}\\right)\\right\\|_{2 p}\\left\\|h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)\\right\\|_{2 p} \\\\\n& \\leq 2^{2 \\sqrt{d /(2 p})(R / \\sigma)+1}\\left\\|h_{\\mathbf{k}_{1}}\\left(y_{i}\\right)\\right\\|_{L^{2 p}(\\gamma)}\\left\\|h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)\\right\\|_{L^{2 p}(\\gamma)} \\\\\n& \\leq 2^{2 \\sqrt{d /(2 p})(R / \\sigma)+1}(2 p-1)^{2 d}\\left\\|h_{\\mathbf{k}_{1}}\\left(y_{i}\\right)\\right\\|_{L^{2}(\\gamma)}\\left\\|h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)\\right\\|_{L^{2}(\\gamma)} \\\\\n& \\leq(4(2 p-1))^{2 d}\n\\end{aligned}\n$$\n\nTherefore, again using Rosenthal's inequality,\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left|\\sum_{i=1}^{N_{j}}\\left(h_{\\mathbf{k}_{1}}\\left(y_{i}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)-h_{\\mathbf{k}_{1}}\\left(y_{i}^{(2)}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}^{(2)}\\right)\\right)\\right|^{p} \\\\\n& \\leq(c p)^{p} \\max \\left\\{\\sum_{i=1}^{N_{j}} \\mathbb{E}\\left|h_{\\mathbf{k}_{1}}\\left(y_{i}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)-h_{\\mathbf{k}_{1}}\\left(y_{i}^{(2)}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}^{(2)}\\right)\\right|^{p},\\left(\\sum_{i=1}^{N_{j}} \\mathbb{E}\\left|h_{\\mathbf{k}_{1}}\\left(y_{i}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)-h_{\\mathbf{k}_{1}}\\left(y_{i}^{(2)}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}^{(2)}\\right)\\right|^{2}\\right)^{p / 2}\\right\\} \\\\\n& \\leq(c p(4(2 p-1))^{2 d} \\sqrt{N})^{p}\n\\end{aligned}\n$$\n\nwhich implies\n\n$$\n\\begin{aligned}\n& \\left\\|\\sum_{\\left|\\mathbf{k}_{1}\\right|,\\left|\\mathbf{k}_{2}\\right| \\leq d}\\left(\\sum_{i=1}^{N_{j}}\\left(h_{\\mathbf{k}_{1}}\\left(y_{i}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)-h_{\\mathbf{k}_{1}}\\left(y_{i}^{(2)}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}^{(2)}\\right)\\right)\\right)^{2}\\right\\|_{p} \\\\\n& \\leq \\sum_{\\left|\\mathbf{k}_{1}\\right|,\\left|\\mathbf{k}_{2}\\right| \\leq d}\\left\\|\\left(\\sum_{i=1}^{N_{j}}\\left(h_{\\mathbf{k}_{1}}\\left(y_{i}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}\\right)-h_{\\mathbf{k}_{1}}\\left(y_{i}^{(2)}\\right) h_{\\mathbf{k}_{2}}\\left(y_{i}^{(2)}\\right)\\right)\\right)^{2}\\right\\|_{p} \\\\\n& \\leq\\binom{ n+d}{d}^{2}\\left(2 c p(4(4 p-1))^{2 d} \\sqrt{N}\\right)^{2}\n\\end{aligned}\n$$\n\nTherefore, using $\\binom{n}{k} \\leq \\frac{1}{e}\\left(\\frac{e n}{k}\\right)^{k}$,\n\n$$\n\\left\\|\\left\\|\\sum_{i=1}^{N_{j}}\\left(h\\left(y_{i}\\right) h\\left(y_{i}\\right)^{\\top}-h\\left(y_{i}^{(2)}\\right) h\\left(y_{i}^{(2)}\\right)^{\\top}\\right)\\right\\|_{F}\\right\\|_{2 p} \\leq\\left(e\\left(\\frac{n}{d}+1\\right)\\right)^{d} 2 c p(4(4 p-1))^{2 d} \\sqrt{N}\n$$\n\nCombining Equations (42), (41), and (40) completes the proof.\nLemma 5.12 (High-probability bound for second term). Assume $d \\geq(R / \\sigma)^{2}$. We have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\max _{\\left(b_{\\mathbf{k}}\\right) \\mid \\mathbf{k} \\leq d \\in B_{M}(0)} \\sum_{i=1}^{N}\\left(\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}-\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}\\right) \\geq t\\right)\\right. \\\\\n& \\leq \\exp \\left(-\\Omega\\left(\\left(\\frac{n}{d}+1\\right)^{-1 / 2}\\left(\\frac{t}{n^{1 / 2} M(M+D) \\sqrt{N}}\\right)^{1 /(2(d+1))} \\wedge\\left(\\frac{t}{D^{2} \\sqrt{N}}\\right)^{2}\\right)\\right)\n\\end{aligned}\n$$\n\nProof. We have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\max _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}(0)} \\sum_{i=1}^{N}\\left(\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}-\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}\\right) \\geq t\\right)\\right. \\\\\n& =\\frac{\\mathbb{E}\\left|\\max _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}(0)} \\sum_{i=1}^{N}\\left(\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}-\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}\\right)-z_{i}\\right|^{2}\\right)\\right|^{2 p}}{t^{2 p}} \\\\\n& \\leq\\left(\\frac{n^{1 / 2} M(M+D) e^{d}\\left(\\frac{n}{d}+1\\right)^{d} p(4(4 p-1))^{2 d} \\sqrt{N}+O\\left(\\sqrt{p N} D^{2}\\right)}{t}\\right)^{2 p}\n\\end{aligned}\n$$\n\nPicking\n\n$$\np=O\\left(\\frac{\\left(c^{\\prime} t /\\left(n^{1 / 2} M(M+D) \\sqrt{N}\\right)\\right)^{1 /(2(d+1))}}{\\left(\\frac{n}{d}+1\\right)^{1 / 2}} \\wedge\\left(\\frac{c^{\\prime} t}{D^{2} \\sqrt{N}}\\right)^{2}\\right)\n$$\n\nfor universal constant $c^{\\prime}$ small enough, the proof is complete.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 22,
      "text": "# 5.3.2 Generalization gap \n\nProof of Lemma 5.8. Note that from Equation (30), for every fixed Voronoi cell $V_{j}$ :\n\n$$\n\\begin{aligned}\n& \\max _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}}\\left|\\operatorname{err}^{(j)}\\left(\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right)\\right| \\leq \\frac{1}{N}\\left\\|\\sum_{i: y_{i} \\in V_{j}} h\\left(y_{i}-\\widehat{\\mu}_{j}\\right) \\eta_{i}^{\\top}\\right\\|_{F} \\\\\n& \\quad+\\max _{\\left(b_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d} \\in B_{M}} \\frac{1}{N} \\sum_{i: y_{i} \\in V_{j}}\\left(\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right)-z_{i}\\right|^{2}-\\mathbb{E}\\left|\\sum_{|\\mathbf{k}| \\leq d} b_{\\mathbf{k}} h_{\\mathbf{k}}\\left(y_{i}-\\widehat{\\mu}_{j}\\right)-z_{i}\\right|^{2}\\right)\n\\end{aligned}\n$$\n\nBut from Lemma 5.10, given\n\n$$\nN=\\Omega\\left(\\frac{k^{\\prime 2} \\ln ^{2(d+1)}\\left(k^{\\prime} / \\delta\\right) D^{2} n(n / d+1)^{(d+1)}}{\\varepsilon^{4}}\\right)\n$$\n\nsamples, with probability at least $1-\\delta /\\left(2 k^{\\prime}\\right)$ the absolute value of the first term in Equation (43) is at most $\\varepsilon^{2} /\\left(2 k^{\\prime}\\right)$. Furthermore, based on Lemma 5.12, given\n\n$$\nN=\\Omega\\left(\\frac{k^{\\prime 2} \\ln ^{4(d+1)}\\left(k^{\\prime} / \\delta\\right) n(M+D)^{4}(n / d+1)^{2(d+1)}}{\\varepsilon^{4}}\\right)\n$$\n\nwith probability at least $1-\\delta /\\left(2 k^{\\prime}\\right)$ the second term is bounded by $\\varepsilon^{2} /\\left(2 k^{\\prime}\\right)$. Applying a union bound, the sum of the first and second terms is bounded by $\\varepsilon^{2}$ with probability at least $1-\\delta / k^{\\prime}$. This shows the first claim. To show the second claim, note that\n\n$$\n\\begin{aligned}\nL^{(j)}\\left(\\left(\\widehat{b}_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right) & =\\mathbb{E}_{(\\mu, Y)}\\left|\\mathbb{1}_{V_{j}}(Y)(\\widehat{g}(Y)-(Z+\\eta))\\right|^{2} \\\\\n& =\\mathbb{E}_{(\\mu, Y)}\\left|\\mathbb{1}_{V_{j}}(Y)(\\widehat{g}(Y)-Z)\\right|^{2}+\\mathbb{E}\\left[\\mathbb{1}_{V_{j}}(Y)|\\eta|^{2}\\right]\n\\end{aligned}\n$$\n\nwhere $\\widetilde{g}$ is defined as in (29) and similarly\n\n$$\nL^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right)=\\mathbb{E}_{(\\mu, Y)}\\left|\\mathbb{1}_{V_{j}}(Y)(\\widetilde{g}(Y)-Z)\\right|^{2}+\\mathbb{E}\\left[\\mathbb{1}_{V_{j}}(Y)|\\eta|^{2}\\right]\n$$\n\nwhere $\\widetilde{g}$ is defined analogously. Therefore\n\n$$\nL^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right)-L^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right)=\\mathbb{E}_{(\\mu, Y)}\\left|\\mathbb{1}_{V_{j}}(Y)(\\widetilde{g}(Y)-Z)\\right|^{2}-\\mathbb{E}_{(\\mu, Y)}\\left|\\mathbb{1}_{V_{j}}(Y)(\\widetilde{g}(Y)-Z)\\right|^{2}\n$$\n\nSumming Equation (44) for $1 \\leq j \\leq k^{\\prime}$ implies\n\n$$\n\\sum_{j=1}^{k^{\\prime}} L^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right)-L^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right)=\\|\\widetilde{g}-f\\|_{L^{2}(P)}^{2}-\\|\\widetilde{g}-f\\|_{L^{2}(P)}^{2}\n$$\n\nBut from the previous claim and union bound, we know with probability at least $1-\\delta$ each $L^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right)-L^{(j)}\\left(\\left(\\widetilde{b}_{\\mathbf{k}}\\right)_{\\mid \\mathbf{k} \\mid \\leq d}\\right)$ is bounded by $\\varepsilon^{2} / k^{\\prime}$. This completes the proof.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 23,
      "text": "# 5.4 Maintaining warm starts \n\nWe would like to maintain warm starts to the centers of all Gaussians (of non-negligible mass) as we decrease the noise level. By choosing the highest noise level large enough, $\\mathbf{0}$ will be a warm start. The key observation is that with high probability, the score function points in a direction close to a mean; this remains true with the estimated score when the error is small.\n\n```\nAlgorithm 2 Picking set of warm starts\n    Input: Score estimate \\(s\\), noise level \\(\\sigma\\), data points \\(y_{1}, \\ldots, y_{N} \\sim Q_{\\sigma^{2}}\\left(N=\\Omega\\left(\\frac{\\ln (1 / \\delta) k}{\\alpha_{\\min }}\\right)\\right)\\), min\n    weight \\(\\alpha_{\\min}\\), failure probability \\(\\delta\\).\n    For each \\(i\\), let \\(\\widehat{\\mu}_{i}=y_{i}+\\sigma^{2} s\\left(y_{i}\\right)\\).\n    Let \\(U=\\left\\{\\widehat{\\mu}_{1}, \\ldots, \\widehat{\\mu}_{N}\\right\\} \\quad \\triangleright\\) Uncovered means\n    Let \\(\\widehat{R}=C\\left(R_{0}+\\sigma \\sqrt{\\ln \\left(\\frac{1}{\\alpha_{\\min }}\\right)}\\right)\\) for an appropriate constant \\(C\\).\n    for \\(t=1\\) to \\(C^{\\prime} k \\ln \\left(\\frac{1}{\\alpha_{\\min }}\\right)\\) do \\(\\triangleright\\) Greedy set cover\n        Let \\(\\widehat{\\mu}=\\arg \\max _{\\mu \\in U}\\left|B_{\\mu}(\\widehat{R}) \\cap U\\right|\\).\n        Let \\(D=B_{\\widehat{\\mu}}(\\widehat{R}) \\cap U\\).\n        \\(\\widehat{\\mathcal{C}} \\leftarrow \\widehat{\\mathcal{C}} \\cup\\{\\widehat{\\mu}\\}, U \\leftarrow U \\backslash D\\).\n    end for\n    Output: Set of warm starts \\(\\widehat{\\mathcal{C}}\\)\n```\n\nWe first show that we do not lose too much in the score estimate if we only consider the means that are close to the mean that a data point came from.\n\nLemma 5.13 (Good score estimation $\\Longrightarrow$ Warm starts). Consider a mixture of Gaussians satisfying Assumption 1.1 with $\\sigma_{0}^{2}=1$. There is a universal constant such that the following holds. Let $f_{\\sigma^{2}}(y)=y+\\sigma^{2} \\nabla \\ln q_{\\sigma^{2}}(y)$. Suppose we are given a function $g$ satisfying\n\n$$\n\\|f-g\\|_{L^{2}\\left(Q_{\\sigma^{2}}\\right)}^{2} \\leq\\left(R_{0}+\\sigma\\right)^{2} \\alpha_{\\min }\n$$\n\nSuppose we have $N=\\Omega\\left(\\frac{\\ln (1 / \\delta) k}{\\alpha_{\\min }}\\right)$ samples sampled from $q_{\\sigma^{2}}$. Let $\\mathcal{C}$ be the output of Algorithm 2. Then with probability $\\geq 1-\\delta$, for radius $\\widetilde{R}=C\\left(R_{0}+\\sigma \\sqrt{\\ln \\left(\\frac{1}{\\alpha_{\\min }}\\right)}\\right)$ with universal constant $C$, the support of $Q_{0}$ is contained in $\\bigcup_{\\widetilde{R} \\in \\mathcal{C}} B_{\\widetilde{R}}(\\widetilde{\\mu})$.\n\nProof. By (45) and Chebyshev's inequality,\n\n$$\nQ_{\\sigma^{2}}\\left(|f-g| \\geq 4\\left(R_{0}+\\sigma\\right)\\right)=Q_{\\sigma^{2}}\\left(|f-g|^{2} \\geq 16\\left(R_{0}+\\sigma\\right)^{2}\\right) \\leq \\frac{\\|f-g\\|_{L^{2}\\left(q_{\\sigma}^{2}\\right)}^{2}}{16\\left(R_{0}+\\sigma\\right)^{2}} \\leq \\frac{\\alpha_{\\min }}{16}\n$$\n\nConsider drawing $\\mu \\sim Q_{0}, \\xi \\sim \\mathcal{N}\\left(0, I_{n}\\right)$, and $Y=\\mu+\\sigma \\xi$. Let $\\mathcal{C}=\\left\\{\\bar{\\mu}_{1}, \\ldots, \\bar{\\mu}_{k}\\right\\}$ be as in Assumption 1.1. Let $f_{\\text {loc }, \\sigma^{2}}=f_{\\text {loc }, \\sigma^{2}}^{\\mathcal{C}, R^{\\prime}}$ be as in Lemma 5.3, where $R^{\\prime}=3 R_{0}+2 \\sqrt{2} \\sigma \\sqrt{\\ln \\left(\\frac{k}{\\varepsilon^{\\prime}}\\right)}$. Then\n\n$$\n\\mathbb{E}\\left|f_{\\sigma^{2}}(y)-f_{\\text {loc }, \\sigma^{2}}(y)\\right|^{2} \\lesssim\\left(R_{0}^{2}+\\sigma^{2} \\ln \\left(\\frac{k}{\\varepsilon^{\\prime}}\\right)\\right) \\varepsilon^{\\prime}\n$$\n\nBy choosing $\\varepsilon^{\\prime}=\\frac{\\alpha_{\\min }}{\\ln \\left(\\frac{1}{\\alpha_{\\min }}\\right)}$ for a small enough constant $c$ and noting $\\alpha_{\\min } \\leq \\frac{1}{k}$, we obtain that $\\mathbb{E}\\left|f_{\\sigma^{2}}(y)-f_{\\text {loc }, \\sigma^{2}}(y)\\right|^{2} \\leq\\left(R_{0}+\\sigma\\right)^{2} \\alpha_{\\min }$. Again by Chebyshev's inequality,\n\n$$\n\\mathbb{P}\\left(\\left|f(y)-f_{\\text {loc }, \\sigma^{2}}(y)\\right| \\geq 4\\left(R_{0}+\\sigma\\right)\\right) \\leq \\frac{\\alpha_{\\min }}{16}\n$$\n\nHence\n\n$$\n\\mathbb{P}\\left(\\left|g(y)-f_{\\text {loc }, \\sigma^{2}}(y)\\right| \\geq 8\\left(R_{0}+\\sigma\\right)\\right) \\leq \\frac{\\alpha_{\\min }}{8}\n$$\n\nLet $V_{1}, \\ldots, V_{k}$ be the Voronoi partition corresponding to $\\mathcal{C}$. Letting $i$ be such that $\\mu \\in V_{i}$, we have that $\\left|f_{\\text {loc }, \\sigma^{2}}(y)-\\bar{\\mu}_{i}\\right| \\leq R^{\\prime}$. Hence, under the event in (46), by the triangle inequality,\n\n$$\n\\left|g(y)-\\bar{\\mu}_{i}\\right| \\leq R^{\\prime}+8\\left(R_{0}+\\sigma\\right)\n$$\n\nBy assumption, $Q_{0}\\left(B_{R_{0}}\\left(\\bar{\\mu}_{i}\\right)\\right) \\geq \\alpha_{\\min }$, so letting $R^{\\prime \\prime}=R^{\\prime}+8\\left(R_{0}+\\sigma\\right)$,\n\n$$\n\\begin{aligned}\nQ_{\\sigma^{2}}\\left(g(y) \\in B_{R^{\\prime \\prime}}\\left(\\bar{\\mu}_{i}\\right)\\right) & \\geq \\frac{7 \\alpha_{\\min }}{8} \\\\\nQ_{\\sigma^{2}}\\left(g(y) \\in \\bigcup_{i=1}^{k} B_{R^{\\prime \\prime}}\\left(\\bar{\\mu}_{i}\\right)\\right) & \\geq 1-\\frac{\\alpha_{\\min }}{8}\n\\end{aligned}\n$$\n\nBy the Chernoff bounds, for independent $Z_{1}, \\ldots, Z_{N} \\sim \\operatorname{Bernoulli}(p)$, for $c \\geq 0$,\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\frac{Z_{1}+\\cdots+Z_{N}}{N} \\leq(1-c) p\\right) \\leq e^{-\\frac{c^{2} p N}{2}} \\\\\n& \\mathbb{P}\\left(\\frac{Z_{1}+\\cdots+Z_{N}}{N} \\geq(1+c) p\\right) \\leq e^{-\\frac{c^{2} p N}{2+c}}\n\\end{aligned}\n$$\n\nFor fixed $c$, it suffices to have $N=\\Omega\\left(\\frac{\\ln (1 / \\delta)}{p}\\right)$ to make this $\\leq \\delta$. Applying this to $p=\\frac{2 \\alpha_{\\min }}{\\delta}$, $(1-c) p=\\frac{3 \\alpha_{\\min}}{4}$ and $p=\\frac{\\alpha_{\\min}}{8},(1+c) p=\\frac{\\alpha_{\\min}}{4}$ respectively, by a union bound, given $N=\\Omega\\left(\\frac{\\ln (k / \\delta)}{\\alpha_{\\min}}\\right)$ iid draws $\\mu_{1}, \\ldots, \\mu_{m} \\sim Q_{\\sigma^{2}}$, we have\n\n$$\n\\mathbb{P}\\left(\\begin{array}{c}\n\\forall 1 \\leq i \\leq k,\\left|\\left\\{j: g\\left(y_{j}\\right) \\in B_{R^{\\prime \\prime}}\\left(\\bar{\\mu}_{i}\\right)\\right\\}\\right| \\geq \\frac{3 \\alpha_{\\min}}{4} N \\\\\n\\text { and }\\left|\\left\\{j: g\\left(y_{j}\\right) \\in \\bigcup_{i=1}^{k} B_{R^{\\prime \\prime}}\\left(\\bar{\\mu}_{i}\\right)\\right\\}\\right| \\geq\\left(1-\\frac{\\alpha_{\\min}}{4}\\right) N\n\\end{array}\\right) \\geq 1-\\delta\n$$\n\nSuppose this event holds. Consider the sets $B_{R^{\\prime \\prime}+R_{0}}\\left(g\\left(y_{j}\\right)\\right)$. Choose $C$ such that $\\widetilde{R}=C\\left(R_{0}+\\sigma \\sqrt{\\ln \\left(\\frac{1}{\\alpha_{\\min }}\\right)}\\right) \\geq R^{\\prime \\prime}+R_{0}$. For each $i$, choose $j(i)$ such that $g\\left(y_{j(i)}\\right) \\in B_{R^{\\prime \\prime}}\\left(\\bar{\\mu}_{i}\\right)$. Then $B_{R_{0}}\\left(\\bar{\\mu}_{i}\\right) \\subseteq B_{R^{\\prime \\prime}+R_{0}}\\left(y_{j(i)}\\right)$ so by the second event, these $k$ sets cover $1-\\frac{\\alpha_{\\min}}{4}$ proportion of the $g\\left(y_{j}\\right), 1 \\leq j \\leq N$. To finish, apply Lemma 5.14 to obtain that the output of the algorithm covers $1-\\frac{\\alpha_{\\min}}{2}$ proportion of the $g\\left(y_{j}\\right)$. In light of the first event, for each $1 \\leq i \\leq k$, it must contain some $g\\left(y_{j}\\right) \\in B_{R^{\\prime \\prime}}\\left(\\bar{\\mu}_{i}\\right)$. This finishes the proof.\n\nLemma 5.14. Let $\\mathcal{S}$ be a set of subsets of $X$. Let $k$ be the minimum number of sets in $\\mathcal{S}$ required to cover $(1-\\varepsilon)|X|$ elements of $X$. Consider the greedy algorithm where at each step, we take the set containing the most uncovered elements, as in Algorithm 2. Then the greedy algorithm finds $O\\left(k \\ln \\left(\\frac{1}{\\varepsilon}\\right)\\right)$ sets covering $(1-2 \\varepsilon)|X|$ elements of $X$.\n\nThe proof is based on the classic proof of the approximation ratio for set cover Johnson (1973).\nProof. Let $S_{1}, S_{2}, \\ldots$ be the sets chosen by the greedy algorithm, let $U_{i}=\\bigcup_{i^{\\prime}=1}^{i} S_{i^{\\prime}}$, and let $x_{1}, x_{2}, \\ldots$ be the covered elements in order. Define the cost of an element $x_{j}$ as follows: Let $S_{i}$ be the first set where $x_{j}$ appears, and set\n\n$$\nc_{j}=\\frac{1}{\\left|S_{i} \\backslash U_{i-1}\\right|}\n$$\n\nNow suppose that $U_{i-1}=\\left\\{x_{1}, \\ldots, x_{j-1}\\right\\}$, and consider the cost of $x_{j}$. We claim that\n\n$$\nc_{j} \\leq \\frac{k}{(1-\\varepsilon) n-(j-1)}\n$$\n\nTo see this, let $A_{1}, \\ldots, A_{k}$ be an optimal cover of $(1-\\varepsilon)|X|$ elements of $X$. Then they must cover $(1-\\varepsilon)|X|-(j-1)$ elements of $X \\backslash U_{i-1}$, so\n\n$$\n\\sum_{\\ell=1}^{k}\\left|A_{\\ell} \\cap\\left(X \\backslash U_{i-1}\\right)\\right| \\geq(1-\\varepsilon) n-(j-1)\n$$\n\nBy optimality of $S_{i},\\left|S_{i} \\cap\\left(X \\backslash U_{i-1}\\right)\\right| \\geq \\frac{1}{k}((1-\\varepsilon) n-(j-1))$, which shows the claim. Then the number of sets required to cover the first $(1-2 \\varepsilon)|X|$ elements is given by the sum of costs of those elements. It is the ceiling of\n\n$$\n\\sum_{j=1}^{\\lceil(1-2 \\varepsilon) n\\rceil} c_{j} \\leq \\sum_{j=1}^{\\lceil(1-2 \\varepsilon) n\\rceil} \\frac{k}{(1-\\varepsilon) n-j+1} \\leq k \\sum_{j=0}^{(1-2 \\varepsilon) n} \\frac{1}{\\varepsilon n+j}=O\\left(k \\ln \\left(\\frac{1}{\\varepsilon}\\right)\\right)\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 24,
      "text": "# 5.5 Proof of Theorem 1.2 \\& Corollary 1.3 \n\nCombining all the pieces, we prove Theorem 1.2 and Corollary 1.3.\nProof of Theorem 1.2. Here we show that Algorithm 1.1 successfully samples from $P_{0}=Q_{0} *$ $\\mathcal{N}\\left(0, \\sigma_{0}^{2}\\right)$ with the claimed time and sample complexity. Let $M_{2}=\\mathbb{E}_{x \\sim P_{0}}|x|^{2}$. Let $\\widehat{P}_{t_{1}}$ be the output of Algorithm 1.1. By the triangle and Pinsker's inequality,\n\n$$\n\\mathrm{TV}\\left(P_{0}, \\widehat{P}_{t_{1}}\\right) \\leq \\mathrm{TV}\\left(P_{0}, P_{t_{1}}\\right)+\\mathrm{TV}\\left(P_{t_{1}}, \\widehat{P}_{t_{1}}\\right)=\\mathrm{TV}\\left(P_{0}, P_{t_{1}}\\right)+\\sqrt{\\frac{1}{2} \\mathrm{KL}\\left(P_{t_{1}} \\| \\widehat{P}_{t_{1}}\\right)}\n$$\n\nChoose the starting time $t_{1}=\\frac{\\varepsilon^{2} \\sigma_{0}^{2}}{2 \\sqrt{n}}$, so that by Lemma C. 1 we have $\\mathrm{TV}\\left(P_{0}, P_{t_{1}}\\right) \\leq \\frac{\\varepsilon^{2}}{2}$. Hence it is sufficient to prove $\\mathrm{KL}\\left(P_{t_{1}} \\| \\widehat{P}_{t_{1}}\\right) \\leq \\frac{\\varepsilon^{2}}{2}$; actually it suffices to find parameters to make $\\mathrm{KL}\\left(P_{t_{1}} \\| \\widehat{P}_{t_{1}}\\right)=$ $O\\left(\\varepsilon^{2}\\right)$ as the exact constant can be adjusted by rescaling $\\varepsilon$ by a constant. We implement Algorithm 1.1 with the step size schedule obtained recursively by the equality version of condition 3 in Theorem 3.1, i.e. $t_{k}+1=\\left(t_{k+1}+1\\right) \\max \\left\\{e^{-2 \\kappa},\\left(t_{k+1}+1\\right)^{-\\kappa}\\right\\}$ for $\\kappa=\\frac{\\varepsilon^{2}}{M_{2}+n \\ln (T+1)}$, with ending time $t_{N_{\\text {step }}}=T=\\frac{M_{2}+d}{\\varepsilon^{2}}$, and number of iterations $N_{\\text {step }}=O\\left(\\frac{1}{\\kappa} \\ln \\left(\\frac{T+1}{t_{1}}\\right)\\right)$. Then, given that we pick $\\varepsilon_{\\ell}^{2}=\\frac{\\varepsilon^{2}}{\\ln (T+1)}$, Theorem 3.1 tells us that $\\mathrm{KL}\\left(P_{t_{1}} \\| \\widehat{P}_{t_{1}}\\right)=O\\left(\\varepsilon^{2}\\right)$, as needed.\n\nHence the problem reduces to giving sufficiently accurate estimates of the score function, to guarantee an accuracy of $\\varepsilon_{\\ell}^{2}=\\frac{\\varepsilon^{2}\\left(\\sigma_{\\ell}^{2}+1\\right)}{\\ln (T+1)}$ for all time steps $t_{1}, \\ldots, t_{N_{\\text {step }}}$. Obtaining such accurate scores is our main contribution and the proof consists of showing by backwards induction on $\\ell=$ $N_{\\text {step }}, \\ldots, 1$ that the following hold:\n\n1. The set $\\mathcal{C}_{\\ell}$ is a complete set of $R_{\\ell}$-warm starts for $Q_{0}$ (see Definition 5.1), where\n\n$$\nR_{\\ell}:=C\\left(R_{0}+2 \\sigma_{\\ell} \\sqrt{\\ln \\left(\\left(\\frac{R_{0}}{\\sigma_{\\ell}}+1\\right) \\frac{k}{\\alpha_{\\min }}\\right)}\\right)\n$$\n\nfor a large enough universal constant $C$. Moreover, $\\left|\\mathcal{C}_{\\ell}\\right| \\leq k^{\\prime}=O\\left(k \\ln \\left(\\frac{1}{\\alpha_{\\min }}\\right)\\right)$.\n2. The score estimate $s_{t_{\\ell}}$ is $\\varepsilon_{\\ell}$-accurate in $L^{2}\\left(P_{t_{\\ell}}\\right)$, for $\\varepsilon_{\\ell}^{2}=\\frac{\\varepsilon^{2}\\left(\\sigma_{\\ell}^{2}+1\\right)}{\\ln (T+1)}$.\n\nMore precisely, we will show that 1 holds for $\\ell=N_{\\text {step }}, \\ldots, N_{\\text {step }}-\\ell^{\\prime}$ and 2 holds for $\\ell=$ $N_{\\text {step }}, \\ldots, N_{\\text {step }}-\\ell^{\\prime}+1$ with probability at least $1-\\frac{\\ell^{\\prime} k}{N_{\\text {step }}}$.\n\nThe base case of the induction $\\left(\\ell=N_{\\text {step }}, \\ell^{\\prime}=0\\right)$ is to show that $\\mathcal{C}_{N_{\\text {step }}}=\\{0\\}$ is a complete set of $R_{N_{\\text {step }}}$-warm-starts. But note that the assumption $\\varepsilon^{2} \\leq \\frac{M_{2}+n}{D^{2}}$ implies the variance at step $N_{\\text {step }}$ satisfies $\\sigma_{N_{\\text {step }}}^{2}=T+\\sigma_{0}^{2} \\geq T=\\frac{M_{2}+n}{\\varepsilon^{2}} \\geq D^{2}$, which given that we pick $C \\geq 1$ means $R_{N_{\\text {step }}} \\geq \\sigma_{N_{\\text {step }}} \\geq D$. Therefore, from the definition of $D$, we have that $Q_{0}$ is supported on $B_{D} \\subseteq B_{R_{N_{\\text {step }}}}$.\n\nNext, we show the induction step; here, the hypothesis of induction for step $\\ell$ is that the set $\\mathcal{C}_{\\ell}$ is a complete set of $R_{\\ell}$-warm starts for $Q_{0}$. Then, we show that 2 for $\\ell$ and 1 for $\\ell-1$ are satisfied, after excluding an event of probability at most $\\frac{\\ell}{N_{\\text {step }}}$. First we handle 2. Using Lemma 5.6 with\n\n$$\n\\tilde{\\varepsilon}^{2}:=\\frac{\\varepsilon^{2}}{\\left(\\left(\\frac{R_{\\ell}}{\\sigma_{\\ell}}\\right)^{2}+\\ln \\left(\\frac{k^{\\prime} R_{\\ell}\\left(M_{2}+n\\right)}{\\sigma_{\\ell} \\varepsilon}\\right)\\right) \\ln (T+1)}\n$$\n\nthere exists a piece-wise polynomial $\\widetilde{g}_{\\ell}$ on the Voronoi partition of $\\mathcal{C}_{\\ell}$ that approximates the score function with the desired accuracy,\n\n$$\n\\left\\|\\widetilde{g}_{\\ell}-f_{\\sigma_{\\ell}^{2}}\\right\\|_{P_{\\ell_{i}}}^{2} \\lesssim \\widetilde{\\varepsilon}^{2}\\left(R_{\\ell}^{2}+\\sigma_{\\ell}^{2} \\ln \\left(\\frac{k^{\\prime}}{\\widetilde{\\varepsilon}}\\right)\\right) \\lesssim \\frac{\\varepsilon^{2}\\left(\\sigma_{\\ell}^{2}+1\\right)}{\\ln (T+1)}\n$$\n\nWe note\n\n$$\n\\ln \\left(\\frac{1}{\\widetilde{\\varepsilon}}\\right)=O\\left(\\ln \\frac{\\left(\\frac{R_{i}}{\\sigma_{\\ell}}\\right)\\left(M_{2}+n\\right)}{\\varepsilon}\\right)=O\\left(\\ln \\left(\\frac{1}{\\varepsilon}\\right)\\right)\n$$\n\nassuming that $\\varepsilon \\leq \\min \\left\\{\\frac{1}{2}, \\frac{\\sigma_{0}}{R_{0}}, \\frac{1}{D}, \\frac{1}{n}, \\alpha_{\\min }\\right\\}$ and noting $\\sigma_{0} \\leq \\sigma_{\\ell}, \\alpha_{\\min } \\leq \\frac{1}{k}$. The degree of $\\widetilde{g}_{\\ell}$ is at most\n\n$$\n\\begin{aligned}\nd_{\\ell} & =O\\left(\\left(\\frac{R_{\\ell}}{\\sigma_{\\ell}}+\\sqrt{\\ln \\left(\\frac{k^{\\prime}}{\\widetilde{\\varepsilon}}\\right)}\\right)^{6} \\ln \\left(\\frac{1}{\\widetilde{\\varepsilon}}\\right)^{4}\\right) \\\\\n& =O\\left(\\left(\\frac{R_{0}}{\\sigma_{\\ell}}+\\sqrt{\\ln \\left(\\left(1+\\frac{R_{0}}{\\sigma_{\\ell}}\\right) \\frac{k}{\\alpha_{\\min }}\\right)}+\\sqrt{\\ln \\left(\\frac{1}{\\varepsilon}\\right)}\\right)^{6} \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4}\\right) \\\\\n& =O\\left(\\left(\\ln \\left(\\frac{1}{\\varepsilon}\\right)^{3}+\\left(\\frac{R_{0}}{\\sigma_{0}}\\right)^{6}\\right) \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4}\\right)\n\\end{aligned}\n$$\n\nNow for an arbitrary warm start point $\\widehat{\\mu}_{i}^{(\\ell)} \\in \\mathcal{C}_{\\ell}$, by Lemma 5.6, there exists a polynomial $\\widetilde{g}_{\\ell}=$ $\\sum_{|\\mathbf{k}| \\leq d_{\\ell}} \\widehat{h}_{\\mathbf{k}}^{(\\ell)} h_{\\mathbf{k}}\\left(y-\\widehat{\\mu}_{i}^{(\\ell)}\\right)$ where $h_{\\mathbf{k}}=h_{\\mathbf{k}, \\sigma_{\\ell}^{2}}$ are the Hermite polynomials with variance $\\sigma^{2}$, and the coefficients satisfy\n\n$$\n\\left|\\left(\\widehat{h}_{\\mathbf{k}}^{(\\ell)}\\right)_{\\mid \\mathbf{k} \\mid \\leq d_{\\ell}}\\right|=\\left\\|\\widetilde{g}_{\\ell}\\right\\|_{\\gamma_{\\widehat{\\mu}_{i}, \\sigma_{\\ell}^{2}}} \\leq D\n$$\n\nBut letting $d_{\\ell}$ be the RHS of (48) with appropriate constants, the condition $d_{\\ell} \\geq\\left(R_{\\ell} / \\sigma_{\\ell}\\right)^{2}$ is satisfied, and hence we can use Lemma 5.8 with parameters $D, M$ both set to $D$; the implication is that given\n\n$$\nN=\\varepsilon^{\\prime-4} n\\left(k(D+1)^{2} \\ln \\left(\\frac{1}{\\alpha_{\\min }}\\right)\\right)^{2}\\left(n \\ln \\left(\\frac{k^{\\prime} N_{\\text {step }}}{\\delta}\\right)\\right)^{c d_{\\ell}}\n$$\n\nsamples for $\\varepsilon^{\\prime 2}=\\frac{\\varepsilon^{2}\\left(\\sigma_{\\ell}+1\\right)}{\\ln (T+1)}$ and some universal constant $c$, we have the guarantee\n\n$$\n\\left\\|\\widetilde{g}_{\\ell}-f\\right\\|_{L^{2}\\left(P_{\\ell}\\right)}^{2}-\\left\\|\\widetilde{g}_{\\ell}-f\\right\\|_{L^{2}\\left(P_{\\ell}\\right)}^{2} \\leq \\varepsilon^{\\prime 2}\n$$\n\nafter excluding an event of probability at most $\\frac{\\delta}{2 N_{\\text {step }}}$. Combining this with Equation (47), we obtain\n\n$$\n\\left\\|\\widetilde{g}_{\\ell}-f\\right\\|_{L^{2}\\left(P_{\\ell}\\right)}^{2} \\leq O\\left(\\varepsilon^{\\prime 2}\\right)\n$$\n\nas desired.\nNext, we handle 1, i.e. show that $\\mathcal{C}_{\\ell-1}$ is a complete set of $R_{\\ell-1}$-warm starts for $Q_{0}$. Note that when $\\sigma_{\\ell}$ changes by a constant factor, 1 is still satisfied with a modified constant, so we only have to update $\\mathcal{C}_{\\ell}$ each time $t_{\\ell}+1$ halves (as done in Section 1.1). According to Lemma 5.13, it is sufficient to find an estimate $g$ for the score which satisfies $\\|f-g\\|_{L^{2}\\left(P_{t_{\\ell}}\\right)}^{2} \\leq\\left(R_{0}+\\sigma_{\\ell}\\right)^{2} \\alpha_{\\min }$. But from our assumption we have $\\varepsilon^{\\prime 2}=\\frac{\\varepsilon^{2}\\left(\\sigma_{\\ell}+1\\right)}{\\ln (T+1)} \\leq\\left(R_{0}+\\sigma_{\\ell}\\right)^{2} \\alpha_{\\min }$, which means the polynomial $\\widetilde{g}_{\\ell}$ that we obtained for proving part 2 in the induction already satisfies the required accuracy for updating the warm starts as well; hence we can use the same degree of polynomial for our regression here. Again we exclude an event of probability at most $\\frac{\\delta}{2 N_{\\text {step }}}$. Note that the sample size required by Algorithm 2 is $O\\left(\\frac{\\ln \\left(N_{\\text {step }} / \\delta\\right) k}{\\alpha_{\\min }}\\right)$, which is negligible compared to the number of samples needed for regression. This completes the induction step.\n\nMultiplying $N$ by the number of steps $N_{\\text {step }}$ and dropping lower-order terms (recalling the assumption on $\\varepsilon$ ), we get that the total sample complexity is\n\n$$\nN^{\\text {total }}=\\left(n \\ln \\left(\\frac{1}{\\delta}\\right)\\right)^{O\\left(d_{\\ell}\\right)}=\\left(n \\ln \\left(\\frac{1}{\\delta}\\right)\\right)^{O\\left(\\left(\\ln \\left(\\frac{1}{\\varepsilon}\\right)^{3}+\\left(\\frac{R_{0}}{\\sigma_{0}}\\right)^{6}\\right) \\ln \\left(\\frac{1}{\\varepsilon}\\right)^{4}\\right)}\n$$\n\nNote that the regression problems that we solve to obtain score estimates in Algorithm 1.1 can still use the same batch of samples, because we apply a union bound. However, we do need to use fresh samples each time for updating the warm starts, separate from the ones that we use for score estimates. This is because our uniform convergence bounds in Lemma 5.8 require the condition that the samples are independent from the randomness used in the construction of the Voronoi partition.\n\nProof of Corollary 1.3. This follows from Theorem 1.2 with $\\alpha_{\\min }=\\frac{\\varepsilon}{C^{1}}$ and substituting $\\min \\left\\{\\frac{\\varepsilon}{C^{1}}, \\frac{1}{2}, \\frac{1}{R_{0}}, \\frac{1}{n}, \\frac{1}{D}\\right\\}$ for $\\varepsilon$ in Theorem 1.2, noting $\\ln \\left(\\frac{C^{1}}{\\varepsilon}\\right)=O\\left(l+\\ln \\left(\\frac{1}{\\varepsilon}\\right)\\right)$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 25,
      "text": "# Acknowledgements \n\nThe authors would like to thank Sitan Chen and Allen Liu for helpful discussions. Jonathan Kelner's work on this project was partially supported by NSF Medium CCF-1955217 and NSF TRIPODS 1740751 .",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 26,
      "text": "## References\n\nAcharya, J., Diakonikolas, I., Li, J., and Schmidt, L. (2017). Sample-optimal density estimation in nearly-linear time. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1278-1289. SIAM.\n\nAnderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313-326.\n\nArora, S. and Kannan, R. (2005). Learning mixtures of separated nonspherical gaussians. Annals of Applied Probability, pages 69-92.\n\nAshtiani, H., Ben-David, S., Harvey, N., Liaw, C., Mehrabian, A., and Plan, Y. (2018). Nearly tight sample complexity bounds for learning mixtures of gaussians via sample compression schemes. Advances in Neural Information Processing Systems, 31.\n\nBakshi, A., Diakonikolas, I., Jia, H., Kane, D. M., Kothari, P. K., and Vempala, S. S. (2022). Robustly learning mixtures of k arbitrary gaussians. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1234-1247.\n\nBenjamini, I., Kalai, G., and Schramm, O. (1999). Noise sensitivity of boolean functions and applications to percolation. Publications Math\u00e9matiques de l'Institut des Hautes \u00c9tudes Scientifiques, $90: 5-43$.\n\nBenton, J., De Bortoli, V., Doucet, A., and Deligiannidis, G. (2023). Linear convergence bounds for diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686.\n\nBietti, A., Bruna, J., and Pillaud-Vivien, L. (2023). On learning gaussian multi-index models with gradient flow. arXiv preprint arXiv:2310.19793.\n\nBiroli, G. and M\u00e9zard, M. (2023). Generative diffusion in very large dimensions. Journal of Statistical Mechanics: Theory and Experiment, 2023(9):093402.\n\nChen, H., Lee, H., and Lu, J. (2023a). Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735-4763. PMLR.\n\nChen, M., Huang, K., Zhao, T., and Wang, M. (2023b). Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. arXiv preprint arXiv:2302.07194.\n\nChen, S., Kontonis, V., and Shah, K. (2024). Learning general gaussian mixtures with efficient score matching.\n\nChen, Y. and Eldan, R. (2022). Localization schemes: A framework for proving mixing bounds for markov chains. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 110-122. IEEE.\n\nCole, F. and Lu, Y. (2024). Score-based generative models break the curse of dimensionality in learning a family of sub-gaussian probability distributions. arXiv preprint arXiv:2402.08082.\n\nCui, H., Krzakala, F., Vanden-Eijnden, E., and Zdeborov\u00e1, L. (2023). Analysis of learning a flow-based generative model from limited sample complexity. arXiv preprint arXiv:2310.03575.\n\nDasgupta, S. and Schulman, L. J. (2000). A two-round variant of em for gaussian mixtures. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 152-159.\n\nDe Bortoli, V. (2022). Convergence of denoising diffusion models under the manifold hypothesis. arXiv preprint arXiv:2208.05314.\n\nDiakonikolas, I. and Kane, D. M. (2020). Small covers for near-zero sets of polynomials and learning latent variable models. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 184-195. IEEE.\n\nDiakonikolas, I., Kane, D. M., and Stewart, A. (2017). Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 73-84. IEEE.\n\nDoss, N., Wu, Y., Yang, P., and Zhou, H. H. (2020). Optimal estimation of high-dimensional location gaussian mixtures. arXiv preprint arXiv:2002.05818.\n\nEl Alaoui, A., Montanari, A., and Sellke, M. (2022). Sampling from the sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 323-334. IEEE.\n\nEldan, R. (2013). Thin shell implies spectral gap up to polylog via a stochastic localization scheme. Geometric and Functional Analysis, 23(2):532-569.\n\nGupte, A., Vafa, N., and Vaikuntanathan, V. (2022). Continuous lwe is as hard as lwe \\& applications to learning gaussian mixtures. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 1162-1173. IEEE.\n\nHopkins, S. B. and Li, J. (2018). Mixture models, robustness, and sum of squares proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 10211034 .\n\nIbragimov, R. and Sharakhmetov, S. (2001). The best constant in the Rosenthal inequality for nonnegative random variables. Statistics $\\mathcal{\\&}$ probability letters, 55(4):367-376.\n\nJohnson, D. S. (1973). Approximation algorithms for combinatorial problems. In Proceedings of the fifth annual ACM symposium on Theory of computing, pages 38-49.\n\nKalai, A. T., Klivans, A. R., Mansour, Y., and Servedio, R. A. (2008). Agnostically learning halfspaces. SIAM Journal on Computing, 37(6):1777-1805.\n\nKim, A. K. and Guntuboyina, A. (2022). Minimax bounds for estimating multivariate gaussian location mixtures. Electronic Journal of Statistics, 16(1):1461-1484.\n\nKlivans, A. R., O\u2019Donnell, R., and Servedio, R. A. (2004). Learning intersections and thresholds of halfspaces. Journal of Computer and System Sciences, 68(4):808-840.\n\nKlivans, A. R., O\u2019Donnell, R., and Servedio, R. A. (2008). Learning geometric concepts via gaussian surface area. In 2008 49th Annual IEEE Symposium on Foundations of Computer Science, pages 541-550. IEEE.\n\nKoehler, F., Heckett, A., and Risteski, A. (2022). Statistical efficiency of score matching: The view from isoperimetry. arXiv preprint arXiv:2210.00726.\n\nKothari, P. K., Steinhardt, J., and Steurer, D. (2018). Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1035-1046.\n\nLee, H., Lu, J., and Tan, Y. (2023). Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946-985. PMLR.\n\nLi, M. and Chen, S. (2024). Critical windows: non-asymptotic theory for feature emergence in diffusion models. arXiv preprint arXiv:2403.01633.\n\nLiu, A. and Li, J. (2022). Clustering mixtures with almost optimal separation in polynomial time. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages $1248-1261$.\n\nLiu, A. and Moitra, A. (2021). Settling the robust learnability of mixtures of gaussians. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 518-531.\n\nMei, S. and Wu, Y. (2023). Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models. arXiv preprint arXiv:2309.11420.\n\nMoitra, A. and Valiant, G. (2010). Settling the polynomial learnability of mixtures of gaussians. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 93-102. IEEE.\n\nMontanari, A. (2023). Sampling, diffusions, and stochastic localization. arXiv preprint arXiv:2305.10690.\n\nNagaev, S. and Pinelis, I. (1978). Some inequalities for the distribution of sums of independent random variables. Theory of Probability $\\mathcal{O}$ Its Applications, 22(2):248-256.\n\nNelson, E. (1967). Dynamical theories of Brownian motion, volume 101. Princeton university press.\nOko, K., Akiyama, S., and Suzuki, T. (2023). Diffusion models are minimax optimal distribution estimators. arXiv preprint arXiv:2303.01861.\n\nPabbaraju, C., Rohatgi, D., Sevekari, A. P., Lee, H., Moitra, A., and Risteski, A. (2024). Provable benefits of score matching. Advances in Neural Information Processing Systems, 36.\n\nQin, Y. and Risteski, A. (2023). Fit like you sample: Sample-efficient generalized score matching from fast mixing markov chains. arXiv preprint arXiv:2306.09332.\n\nRegev, O. and Vijayaraghavan, A. (2017). On learning mixtures of well-separated gaussians. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 85-96. IEEE.\n\nRobbins, H. E. (1992). An empirical bayes approach to statistics. In Breakthroughs in Statistics: Foundations and basic theory, pages 388-394. Springer.\n\nSaha, S. and Guntuboyina, A. (2020). On the nonparametric maximum likelihood estimator for gaussian location mixture densities with application to gaussian denoising. The Annals of Statistics, $48(2): 738-762$.\n\nSchwab, C. and Zech, J. (2023). Deep learning in high dimension: Neural network expression rates for analytic functions in $L^{2}\\left(\\mathbb{R}^{d}, \\gamma_{d}\\right)$. SIAM/ASA Journal on Uncertainty Quantification, $11(1): 199-234$.\n\nShah, K., Chen, S., and Klivans, A. (2023). Learning mixtures of gaussians using the ddpm objective. arXiv preprint arXiv:2307.01178.\n\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256-2265. PMLR.\n\nSong, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. In Proceedings of the 33rd Annual Conference on Neural Information Processing Systems.\n\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2020). Scorebased generative modeling through stochastic differential equations. In International Conference on Learning Representations.\n\nTalagrand, M. (2010). Mean field models for spin glasses: Volume I: Basic examples, volume 54. Springer Science \\& Business Media.\n\nTang, W. and Zhao, H. (2024). Score-based diffusion models via stochastic differential equations-a technical tutorial. arXiv preprint arXiv:2402.07487.\n\nTitterington, D. M., Smith, A. F., and Makov, U. E. (1985). Statistical analysis of finite mixture distributions. Chichester-New York: J. Willey $\\mathcal{E}$ Sons, 646.\n\nVempala, S. and Wang, G. (2004). A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841-860.\n\nVershynin, R. (2020). High-dimensional probability. University of California, Irvine, 10:11.\nVincent, P. (2011). A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661-1674.\n\nWibisono, A., Wu, Y., and Yang, K. Y. (2024). Optimal score estimation via empirical bayes smoothing. arXiv preprint arXiv:2402.07747.\n\nWu, Y., Chen, M., Li, Z., Wang, M., and Wei, Y. (2024). Theoretical insights for diffusion guidance: A case study for gaussian mixture models. arXiv preprint arXiv:2403.01639.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 27,
      "text": "# A Proof of Theorem 3.1 \n\nWe conclude the proof of Theorem 3.1 from the analogous guarantee for the VP process.\nTheorem A. 1 (Reverse KL guarantee for variance-preserving diffusion models (Benton et al., 2023, Theorem 2)). Let $0<t_{1}<t_{2}<\\ldots<t_{N_{\\text {app }}}=T$ and $\\zeta_{k}=t_{k+1}-t_{k}$. Suppose $T \\geq 1$. Let $P_{t}^{\\mathrm{OU}}$ denote the distribution of the Ornstein-Uhlenbeck process run for time $t$ initialized from $P_{0}$, that is, the distribution of $e^{-t} X+\\sqrt{1-e^{-2 t}} \\cdot \\xi$ where $X \\sim P_{0}$ and $\\xi \\sim \\mathcal{N}\\left(0, I_{d}\\right)$. Assume the following.\n\n1. We have a score function estimate $s_{t}(x)$ for each $t=t_{k}$ such that\n\n$$\n\\mathbb{E}_{P_{t_{k}}^{\\mathrm{OU}}}\\left|\\nabla \\ln p_{t_{k}}^{\\mathrm{OU}}(x)-s_{t_{k}}(x)\\right|^{2} \\leq \\varepsilon_{k}^{2}\n$$\n\n2. The data distribution has bounded second moment $M_{2}=\\mathbb{E}_{P_{0}}|x|^{2}$.\n\n3. The step size schedule satisfies $\\zeta_{k} \\leq \\kappa \\min \\left\\{1, t_{k}\\right\\}$ (i.e. $\\zeta_{k} \\leq \\min \\left\\{\\kappa, \\frac{\\kappa}{\\kappa+1} t_{k+1}\\right\\}$ )\n\nLet $\\widehat{p}_{t}$ denote the distribution of the exponential integrator discretization of the reverse process when initialized at $\\widehat{P}_{T}=\\mathcal{N}\\left(0, I_{n}\\right)$, given by\n\n$$\n\\widehat{x}_{t_{k}}=e^{\\zeta_{k}} \\widehat{x}_{t_{k+1}}+2\\left(e^{\\zeta_{k}}-1\\right) s_{t_{k+1}}^{x}\\left(\\widehat{x}_{t_{k+1}}\\right)+\\sqrt{e^{2 \\zeta_{k}}-1} \\cdot \\xi_{t_{k}}, \\quad \\xi_{t_{k}} \\sim \\mathcal{N}\\left(0, I_{n}\\right)\n$$\n\nThen\n\n$$\n\\mathrm{KL}\\left(p_{t_{1}} \\|\\widehat{p}_{t_{1}}\\right) \\lesssim\\left(n+M_{2}\\right) e^{-2 T}+\\sum_{k=1}^{N_{\\text {step }}} \\zeta_{k} \\varepsilon_{k}^{2}+\\kappa n T+\\kappa^{2} n N_{\\text {step }}+\\kappa M_{2}\n$$\n\nNote that as opposed to Benton et al. (2023), we index time in the forward direction.\nProof of Theorem 3.1. Consider the following two sets of continuous processes,\n\n$$\nd x_{t}=-x_{t} d t+\\sqrt{2} d W_{t} \\quad d y_{t}=d W_{t}\n$$\n\nwith same initial distribution $x_{0}, y_{0} \\sim P$. Let $P_{t}^{x}$ and $P_{t}^{y}$ be the distribution of the two processes at time $t$. We claim that\n\n$$\nx_{t}=e^{-t} y_{e^{2 t}-1}\n$$\n\nTo see this, we check that $e^{-t} y_{e^{2 t}-1}$ satisfies the equation for $x_{t}$ :\n\n$$\n\\begin{aligned}\nd\\left(e^{-t} y_{e^{2 t}-1}\\right) & =-e^{-t} y_{e^{2 t}-1} d t+e^{-t} d y_{e^{2 t}-1} \\\\\n& =-\\left(e^{-t} y_{e^{2 t}-1}\\right) d t+e^{-t} \\sqrt{2 e^{2 t}} d W_{t} \\\\\n& =-\\left(e^{-t} y_{e^{2 t}-1}\\right) d t+\\sqrt{2} d W_{t}\n\\end{aligned}\n$$\n\nBy the change-of-variables formula,\n\n$$\n\\begin{aligned}\np_{t}^{x}(x) & =e^{t n} p_{e^{2 t}-1}\\left(e^{t} x\\right) \\\\\n\\Longrightarrow \\nabla \\ln p_{t}^{x}(x) & =e^{t} \\nabla \\ln p_{e^{2 t}-1}\\left(e^{t} x\\right)\n\\end{aligned}\n$$\n\nWrite $s_{t}^{y}=s_{t}$ for clarity. Defining $s_{t}^{x}$ to satisfy\n\n$$\ns_{t}^{x}(x)=e^{t} s_{e^{2 t}-1}^{y}\\left(e^{t} x\\right)\n$$\n\nwe have\n\n$$\n\\mathbb{E}_{P_{t}^{x}}\\left|\\nabla \\ln p_{t}^{x}(x)-s_{t}^{x}(x)\\right|^{2}=e^{2 t} \\mathbb{E}_{P_{t}^{y}}\\left|\\nabla \\ln p_{e^{2 t}}^{y}(y)-s_{e^{2 t}-1}^{y}(y)\\right|^{2}\n$$\n\nNow define\n\n$$\nt_{k}^{x}=\\frac{\\ln \\left(t_{k}+1\\right)}{2}, \\quad \\zeta_{k}^{x}=t_{k+1}^{x}-t_{k}^{x}\n$$\n\nand consider the discrete processes defined backwards in time,\n\n$$\n\\begin{array}{ll}\n\\widehat{x}_{t}=e^{\\zeta} \\widehat{x}_{t+h}+2\\left(e^{\\zeta}-1\\right) s_{t+\\zeta}^{x}\\left(\\widehat{x}_{t+\\zeta}\\right)+\\sqrt{e^{2 \\zeta}-1} \\cdot \\xi_{t}^{x} & (t, \\zeta)=\\left(t_{k}^{x}, h_{k}^{x}\\right) \\\\\n\\widehat{y}_{t}=\\widehat{y}_{t+\\zeta}+2\\left[(t+\\zeta-1)-\\sqrt{(t-1)(t+\\zeta-1)}\\right] s_{t+\\zeta}^{y}\\left(\\widehat{y}_{t+\\zeta}\\right)+\\sqrt{\\zeta} \\cdot \\xi_{t} & (t, \\zeta)=\\left(t_{k}, \\zeta_{k}\\right)\n\\end{array}\n$$\n\nwhere $x_{t_{N_{\\text {step }}}^{x}} \\sim P_{t_{N_{\\text {step }}}^{x}}^{x}$ and $y_{t_{N_{\\text {step }}}}=e^{t_{N_{\\text {step }}}^{x}} x_{t_{N_{\\text {step }}}^{x}}\\left(\\right.$ so that $y_{t_{N_{\\text {step }}}} \\sim P_{t_{N_{\\text {step }}}^{y}}^{y}$ ), and we couple $\\xi_{t_{k}^{x}}^{x}=\\xi_{t_{k}}$. We can inductively check that\n\n$$\n\\widehat{x}_{t_{k}}=e^{-t_{k}^{x}} \\widehat{y}_{t_{k}}\n$$\n\nIndeed, if this holds for $k+1$, then\n\n$$\n\\begin{aligned}\n\\widehat{x}_{t_{k}^{x}} & =e^{\\zeta_{k}^{x}} \\widehat{x}_{t_{k+1}^{x}}+2\\left(e^{\\zeta_{k}^{x}}-1\\right) s_{t_{k+1}^{x}}\\left(\\widehat{x}_{t_{k+1}^{x}}\\right)+\\sqrt{e^{2 \\zeta_{k}^{x}}-1} \\cdot \\xi_{t}^{x} \\\\\n& =e^{\\zeta_{k}^{x}}\\left(e^{-t_{k+1}^{x}} \\widehat{y}_{t_{k+1}}\\right)+2\\left(e^{\\zeta_{k}^{x}}-1\\right) e^{t_{k+1}^{x}} s_{t_{k+1}}^{y}\\left(\\widehat{y}_{t_{k+1}}\\right)+\\sqrt{e^{2 \\zeta_{k}^{x}}-1} \\cdot \\xi_{t}^{x} \\\\\n& =e^{-t_{k}^{x}}\\left(\\widehat{y}_{t_{k+1}}+2\\left(e^{\\zeta_{k}^{x}}-1\\right) e^{t_{k+1}^{x}+t_{k}^{x}} s_{t_{k+1}}^{y}\\left(\\widehat{y}_{t_{k+1}}\\right)+\\sqrt{t_{k+1}-t_{k}} \\cdot \\xi_{t}\\right) \\\\\n& =e^{-t_{k}^{x}}\\left(\\widehat{y}_{t_{k+1}}+2\\left[\\left(t_{k+1}-1\\right)-\\sqrt{\\left(t_{k}-1\\right)\\left(t_{k+1}-1\\right)}\\right] s_{t_{k+1}}^{y}\\left(\\widehat{y}_{t_{k+1}}\\right)+\\sqrt{t_{k+1}-t_{k}} \\cdot \\xi_{t}\\right) \\\\\n& =e^{-t_{k}^{x}} \\widehat{y}_{t_{k}}\n\\end{aligned}\n$$\n\nHence, by Theorem A.1,\n\n$$\n\\begin{aligned}\n\\mathrm{KL}\\left(p_{t_{1}}^{y} \\| \\widehat{p}_{t_{1}}^{y}\\right)=\\mathrm{KL}\\left(p_{t_{1}}^{x} \\| \\widehat{p}_{t_{1}}^{x}\\right) & \\lesssim\\left(n+M_{2}\\right) e^{-2 t_{N_{\\text {step }}}^{x}}+\\sum_{k=1}^{N_{\\text {step }}} \\zeta_{k}^{x} \\cdot e^{-2 t_{k}^{x}} \\cdot \\varepsilon_{k}^{2}+\\kappa n t_{N_{\\text {step }}}^{x}+\\kappa^{2} n N_{\\text {step }}+\\kappa M_{2} \\\\\n& \\lesssim \\frac{n+M_{2}}{T+1}+\\sum_{k=1}^{N_{\\text {step }}} \\ln \\left(\\frac{t_{k+1}^{y}+1}{t_{k}^{y}+1}\\right) \\cdot \\frac{1}{t_{k}^{y}+1} \\cdot \\varepsilon_{k}^{2}+\\kappa n \\ln (T+1)+\\kappa^{2} n N_{\\text {step }}+\\kappa M_{2}\n\\end{aligned}\n$$\n\nBecause $y_{T}=\\sqrt{T+1} \\cdot x_{\\frac{\\ln (T+1)}{2}}$, the initialization $x_{t_{N}^{x}} \\sim \\mathcal{N}(0, I)$ corresponds to $y_{T} \\sim \\mathcal{N}(0,(T+1) \\cdot I)$. The requirement on step sizes is\n\n$$\n\\begin{aligned}\n\\frac{\\ln \\left(t_{k+1}+1\\right)-\\ln \\left(t_{k}+1\\right)}{2} & \\leq \\min \\left\\{\\kappa, \\frac{\\kappa}{\\kappa+1} \\cdot \\frac{\\ln \\left(t_{k+1}+1\\right)}{2}\\right\\} \\\\\n& \\Longleftrightarrow t_{k+1}+1 \\geq\\left(t_{k+1}+1\\right) \\max \\left\\{e^{-2 \\kappa},\\left(t_{k+1}+1\\right)^{-\\frac{\\kappa}{\\kappa+1}}\\right\\}\n\\end{aligned}\n$$\n\nNote that\n\n$$\n\\max \\left\\{e^{-2 \\kappa},(t+1)^{-\\frac{\\kappa}{\\kappa+1}}\\right\\}= \\begin{cases}e^{-2 \\kappa}, & t \\geq e^{2(\\kappa+1)}-1 \\\\ (t+1)^{-\\kappa}, & t \\leq e^{2(\\kappa+1)}-1\\end{cases}\n$$\n\nHence for $\\kappa<1$, the number of steps required is $O\\left(\\frac{\\ln (T+1)}{\\kappa}\\right)$ to get to a constant and $\\frac{1}{\\kappa} \\ln \\left(\\frac{1}{t_{1}}\\right)$ to get down to $t_{1}$ (as we need $s$ steps where $\\left(1-\\frac{\\kappa}{\\kappa+1}\\right)^{s} \\lesssim \\ln \\left(1+t_{1}\\right) \\sim t_{1}$ ), for a total of $O\\left(\\frac{1}{\\kappa} \\ln \\left(\\frac{T+1}{t_{1}}\\right)\\right)$. The last part follows from bounding every term and noting that $\\sum_{k=1}^{N_{\\text {step }}-1} \\ln \\left(\\frac{t_{k+1}+1}{t_{k}+1}\\right) \\frac{1}{t_{k}+1} \\varepsilon_{k+1}^{2} \\leq$ $\\sum_{k=1}^{N_{\\text {step }}-1} \\ln \\left(\\frac{t_{k+1}+1}{t_{k}+1}\\right) \\max _{1 \\leq k \\leq N_{\\text {step }}-1} \\frac{1}{t_{k}+1} \\varepsilon_{k+1}^{2} \\lesssim \\ln \\left(\\frac{T+1}{t_{1}+1}\\right) \\frac{\\varepsilon^{2}}{\\ln (T+1)} \\leq \\varepsilon^{2}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 28,
      "text": "# B Inequalities \n\nTheorem B. 1 (Gaussian hypercontractivity, Nelson (1967)). Let $q>p>1$ and $t$ be such that $e^{-t}=\\frac{p-1}{q-1}$. Let $g: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ be a function such that $\\|g\\|_{L^{p}(\\gamma)}<\\infty$. Then\n\n$$\n\\left\\|\\mathscr{P}_{t} g\\right\\|_{L^{q}(\\gamma)} \\leq\\|g\\|_{L^{p}(\\gamma)}\n$$\n\nLemma B.2. Let $q>2$ and let $f$ be a polynomial of degree at most $d$. Then\n\n$$\n\\|f\\|_{L^{q}(\\gamma)} \\leq(q-1)^{d}\\|f\\|_{L^{2}(\\gamma)}\n$$\n\nProof. Write $f=\\sum_{|\\mathbf{k}| \\leq d} a_{\\mathbf{k}} h_{\\mathbf{k}}$ in the Hermite basis. Then $f=\\mathscr{P}_{t} g$ where $g=\\sum_{|\\mathbf{k}| \\leq d} e^{|\\mathbf{k}| t} a_{\\mathbf{k}} h_{\\mathbf{k}}$. By Gaussian hypercontractivity (Theorem B.1), choosing $t$ such that $e^{-t}=\\frac{1}{q-1}$,\n\n$$\n\\|f\\|_{L^{q}(\\gamma)}=\\left\\|\\mathscr{P}_{t} g\\right\\|_{L^{q}(\\gamma)} \\leq\\|g\\|_{L^{2}(\\gamma)} \\leq e^{t d}\\|f\\|_{L^{2}(\\gamma)}=(q-1)^{d}\\|f\\|_{L^{2}(\\gamma)}\n$$\n\nThe following tells us how the $L^{2}$ norm changes when we change from one Gaussian to another, for a bounded degree polynomial.\n\nLemma B.3. Let $f$ be a polynomial of degree at most $d$ and let $\\nu$ be a measure such that for all $a \\geq 0,\\left\\|\\frac{d \\nu}{d \\gamma}\\right\\|_{L^{1+a}(\\gamma)} \\leq e^{\\frac{a R^{2}}{2}}$ (e.g. from Lemma B.4, $Q * \\mathcal{N}\\left(0, I_{n}\\right)$ where $Q$ is supported on $B_{R}(0)$ ). Then\n\n$$\n\\|f\\|_{\\nu}^{2} \\leq\\|f\\|_{\\gamma}^{2} e^{2 \\sqrt{d} R}\n$$\n\nNote this works for $\\mathbb{R}^{n}$-valued polynomials as well, since in this case $\\|f\\|_{\\nu}^{2}=\\sum_{i=1}^{n}\\left\\|f_{i}\\right\\|_{\\nu}^{2}$.\nProof. By H\u00f6lder's inequality and the given assumption,\n\n$$\n\\int_{\\mathbb{R}^{n}}|f|^{2} d \\nu=\\int_{\\mathbb{R}^{n}}|f|^{2} \\frac{d \\nu}{d \\gamma} d \\gamma \\leq\\|f\\|_{L^{2}\\left(1+\\frac{1}{p}\\right)_{(\\gamma)}}^{2}\\left\\|\\frac{d \\nu}{d \\gamma}\\right\\|_{L^{p+1}(\\gamma)} \\leq\\|f\\|_{L^{2}\\left(1+\\frac{1}{p}\\right)_{(\\gamma)}}^{2} e^{\\frac{p R^{2}}{2}}\n$$\n\nBy Lemma B. 2 with $q=2\\left(1+\\frac{1}{p}\\right)$,\n\n$$\n\\|f\\|_{L^{2}\\left(1+\\frac{1}{p}\\right)_{(\\gamma)}} \\leq\\left(1+\\frac{2}{p}\\right)^{d}\\|f\\|_{L^{2}(\\gamma)} \\leq e^{\\frac{2 d}{p}}\\|f\\|_{L^{2}(\\gamma)}\n$$\n\nNow take $p=\\frac{2 \\sqrt{d}}{R}$ to get\n\n$$\n\\|f\\|_{\\nu}^{2} \\leq e^{\\frac{2 d}{p}+\\frac{p R^{2}}{2}}\\|f\\|_{\\gamma}^{2}=e^{2 \\sqrt{d} R}\\|f\\|_{\\gamma}^{2}\n$$\n\nLemma B.4. Suppose $P=Q * \\mathcal{N}\\left(0, I_{n}\\right)$, where $Q$ is supported on $B_{R}(0)$. Then for all $a \\geq 0$,\n\n$$\n\\left\\|\\frac{d P}{d \\gamma}\\right\\|_{L^{1+a}(\\gamma)}=\\left(\\int\\left(\\frac{d P}{d \\gamma}\\right)^{1+a} d \\gamma\\right)^{\\frac{1}{1+a}} \\leq \\exp \\left(\\frac{a R^{2}}{2}\\right) \\quad \\text { and } \\quad\\left\\|\\frac{d \\gamma}{d P}\\right\\|_{L^{1+a}(P)} \\leq \\exp \\left(\\frac{a R^{2}}{2}\\right)\n$$\n\nProof. By convexity, it suffices to consider when $P=\\mathcal{N}\\left(\\mu, I_{n}\\right)$ with $\\|\\mu\\| \\leq R$. Let $Z=(2 \\pi)^{n / 2}$ be the normalizing constant of the standard Gaussian. Then\n\n$$\n\\begin{aligned}\n\\left(\\int\\left(\\frac{d P}{d \\gamma}\\right)^{1+a} d \\gamma\\right)^{\\frac{1}{1+a}} & =\\frac{1}{Z} \\int_{\\mathbb{R}^{n}}\\left(\\frac{e^{-\\frac{\\|\\gamma\\|^{2}}{2}}}{e^{-\\frac{\\|\\gamma-\\mu\\|^{2}}{2}}}\\right)^{a} e^{-\\frac{\\|\\gamma-\\mu\\|^{2}}{2}} d x \\\\\n& =\\frac{1}{Z} \\int_{\\mathbb{R}^{n}} e^{-\\frac{\\|\\gamma\\|^{2}}{2}-(a+1)\\langle x, \\mu\\rangle+(a+1) \\frac{\\|\\mu\\|^{2}}{2}} \\\\\n& =\\frac{1}{Z} \\int_{\\mathbb{R}^{n}} e^{-\\frac{\\|x+(a+1)\\mu\\|^{2}}{2}+\\frac{a(a+1)\\|\\mu\\|^{2}}{2}} d x=e^{\\frac{a(a+1)\\|\\mu\\|^{2}}{2}} \\leq e^{\\frac{a(a+1) \\theta^{2}}{2}}\n\\end{aligned}\n$$\n\nRaising to the power $\\frac{1}{a+1}$ then gives the first result in this case. For $P=\\mathcal{N}\\left(\\mu, I_{n}\\right)$ the same inequality follows from symmetry.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 29,
      "text": "# C Bounding $T V\\left(P_{0}, P_{\\sigma^{2}}\\right)$ \n\nNote that Algorithm 1.1 ultimately samples from $P_{t_{1}}$ for some positive time $t_{1}$ close to zero. Hence, to prove a TV guarantee on the distribution of the sample with respect to the target mixture $P_{0}$, we need to bound $T V\\left(P_{t_{1}}, P_{0}\\right)$. Fortunately, this task is convenient using the Gaussian-convolution structure hidden in $P_{0}$ and $P_{t_{1}}$, by upper bounding their KL divergence.\n\nLemma C.1. We have\n\n$$\n\\operatorname{TV}\\left(P_{0}, P_{\\sigma^{2}}\\right) \\leq \\frac{1}{\\sqrt{2}} \\frac{\\sigma^{2} \\sqrt{n}}{\\sigma_{0}^{2}}\n$$\n\nProof. Note\n\n$$\n\\begin{aligned}\n\\mathrm{KL}\\left(\\mathcal{N}\\left(0, \\sigma_{0}^{2} I_{n}\\right) \\| \\mathcal{N}\\left(0,\\left(\\sigma_{0}^{2}+\\sigma^{2}\\right) I_{n}\\right)\\right. & \\leq \\frac{1}{2}\\left[-n \\ln \\frac{\\sigma_{0}^{2}+\\sigma^{2}}{\\sigma_{0}^{2}}-n+n \\frac{\\sigma_{0}^{2}+\\sigma^{2}}{\\sigma_{0}^{2}}\\right] \\\\\n& \\leq \\frac{n}{2}\\left[-\\frac{\\sigma^{2}}{\\sigma_{0}^{2}}+\\frac{1}{2} \\frac{\\sigma^{4}}{\\sigma_{0}^{4}}+\\frac{\\sigma^{2}}{\\sigma_{0}^{2}}\\right] \\leq \\frac{n}{4} \\frac{\\sigma^{4}}{\\sigma_{0}^{4}}\n\\end{aligned}\n$$\n\nBy Pinsker's inequality,\n\n$$\n\\operatorname{TV}\\left(\\mathcal{N}\\left(0, \\sigma_{0}^{2} I_{n}\\right), \\mathcal{N}\\left(0,\\left(\\sigma_{0}^{2}+\\sigma^{2}\\right) I_{n}\\right)\\right) \\leq \\sqrt{2 \\mathrm{KL}\\left(\\mathcal{N}\\left(0, \\sigma_{0}^{2} I_{n}\\right) \\| \\mathcal{N}\\left(0,\\left(\\sigma_{0}^{2}+\\sigma^{2}\\right) I_{n}\\right)} \\leq \\frac{1}{\\sqrt{2}} \\frac{\\sigma^{2} \\sqrt{n}}{\\sigma_{0}^{2}}\n$$\n\nThe result follows from joint convexity of TV distance.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 30,
      "text": "## D Sub-exponential random variables\n\nDefinition D.1. We say centered random variable $X$ is $\\left(v^{2}, \\alpha\\right)$ sub-exponential for $v, \\alpha>0$ if\n\n$$\n\\mathbb{E} e^{\\lambda X} \\leq e^{\\frac{\\lambda^{2} v^{2}}{2}}, \\forall \\lambda: \\quad|\\lambda| \\leq \\alpha^{-1}\n$$\n\nWe call any random variable sub-exponential if its centered version is sub-exponential.\n\nSub-exponential random variables obey a desirable concentration of measure due to their fast decaying tails. This further implies their moments have a slow growth rate, a property that is of interest.\n\nProposition D. 2 (Sub-exponential properties Vershynin (2020)): The following properties are equivalent:\n\n- $X$ is $\\left(\\left(c_{0} v\\right)^{2}, c_{0} v\\right)$-sub-exponential for some universal constant $c_{0}$.\n- $X$ satisfies the tail bound $\\mathbb{P}(X \\geq t) \\leq 2 \\exp \\left(-c_{1} t / v\\right)$ for universal constant $c_{1}$.\n- For all $p \\geq 1$, the $p$ th moment of $X$ is bounded as $\\|X\\|_{p} \\leq c_{2} v p$ for universal constant $c_{2}$.\n\nBy equivalent, we mean that given one of the properties, each of the other properties holds for some constant depending on the original constant. In particular, the second property implies that the square of a $v$-sub-Gaussian variable is $\\left(O\\left(v^{2}\\right), O(v)\\right)$-sub-exponential. Another important property of sub-exponential variables is that they behaves nicely under summation.\n\nProposition D. 3 (Adding sub-exponential variables): Given independent $\\left(v_{i}^{2}, \\alpha_{i}\\right)$-sub-exponential random variables $X_{i}$ for $1 \\leq i \\leq n, \\sum_{i=1}^{n} X_{i}$ is $\\left(\\sum_{i=1}^{n} v_{i}^{2}, \\max _{1 \\leq i \\leq n} \\alpha_{i}\\right)$-sub-exponential.\n\nProof. By induction, it suffices to prove this for the case $n=2$. Our assumptions imply that $\\mathbb{E} e^{\\lambda X_{1}} \\leq e^{\\frac{\\lambda^{2} v_{1}^{2}}{2}}$ and $\\mathbb{E} e^{\\lambda X_{2}} \\leq e^{\\frac{\\lambda^{2} v_{2}^{2}}{2}}$ for all $\\lambda$ such that $|\\lambda| \\leq \\min \\left\\{\\alpha_{1}^{-1}, \\alpha_{2}^{-1}\\right\\}=\\max \\left\\{\\alpha_{1}, \\alpha_{2}\\right\\}^{-1}$. By the independence of $X_{1}$ and $X_{2}$, we therefore have\n\n$$\n\\mathbb{E} e^{\\lambda\\left(X_{1}+X_{2}\\right)}=\\mathbb{E} e^{\\lambda X_{1}} \\mathbb{E} e^{\\lambda X_{2}} \\leq e^{\\frac{\\lambda^{2} v_{1}^{2}}{2}} e^{\\frac{\\lambda^{2} v_{2}^{2}}{2}}=e^{\\frac{\\lambda^{2}\\left(v_{1}^{2}+v_{2}^{2}\\right)}{2}}\n$$\n\nfor all such $\\lambda$, so $X_{1}+X_{2}$ is $\\left(v_{1}^{2}+v_{2}^{2}, \\max \\left\\{\\alpha_{1}, \\alpha_{2}\\right\\}\\right)$, as claimed.\nWhen the random variables are not independent, we accrue an extra factor of the number of terms.\n\nProposition D.4: Given $\\left(v_{i}^{2}, \\alpha_{i}\\right)$-sub-exponential random variables $X_{i}$ for $1 \\leq i \\leq n$ (not necessarily independent), $\\sum_{i=1}^{n} X_{i}$ is $\\left(n \\cdot \\sum_{i=1}^{n} v_{i}^{2}, n \\cdot \\max _{1 \\leq i \\leq n} \\alpha_{i}\\right)$-sub-exponential.\n\nProof. For $|\\lambda| \\leq \\frac{1}{n} \\min _{1 \\leq i \\leq n} \\alpha_{i}^{-1}=\\left(n \\cdot \\max _{1 \\leq i \\leq n} \\alpha_{i}\\right)^{-1}$, the defining inequality for the subexponential random variables are satisfied for $\\lambda n$. By H\u00f6lder's inequality,\n\n$$\n\\mathbb{E} e^{\\lambda \\sum_{i=1}^{n} X_{i}} \\leq \\prod_{i=1}^{n}\\left(\\mathbb{E} e^{\\lambda n X_{i}}\\right)^{1 / n} \\leq \\prod_{i=1}^{n} e^{\\frac{n^{2} \\lambda^{2} v_{i}^{2}}{2}} \\stackrel{!}{=} e^{\\frac{\\lambda^{2} n \\sum_{i=1}^{n} v_{i}^{2}}{2}}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 31,
      "text": "# E Derivation of Tweedie's formula \n\nWe derive (3). Consider $\\mu \\sim Q, \\xi \\sim \\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)$ drawn independently, and let $Y=\\mu+\\xi$. Let $Q_{\\sigma^{2}}=Q * \\mathcal{N}\\left(0, \\sigma^{2} I_{n}\\right)$, and suppose it has density $q_{\\sigma^{2}}$. By Bayes's Rule, letting $q(y \\mid \\mu)=\\gamma_{\\mu, \\sigma^{2}}(y)$\n\ndenote the density of $Y$ given $\\mu$,\n\n$$\n\\begin{aligned}\n\\nabla \\ln q_{\\sigma^{2}}(y) & =\\nabla \\log \\left(q * \\gamma_{\\sigma^{2}}(y)\\right)=\\nabla_{y} \\log \\int_{\\mathbb{R}^{d}} e^{-\\frac{\\|y-y\\|^{2}}{2 \\sigma^{2}}} d Q_{0}(\\mu) \\\\\n& =-\\frac{\\int_{\\mathbb{R}^{d}} \\frac{y-\\mu}{\\sigma^{2}} e^{-\\frac{\\|y-\\mu\\|^{2}}{2 \\sigma^{2}}} d Q_{0}(\\mu)}{\\int_{\\mathbb{R}^{d}} e^{-\\frac{\\|y-\\mu\\|^{2}}{2 \\sigma^{2}}} d Q_{0}(\\mu)}=\\frac{\\int_{\\mathbb{R}^{d}} \\frac{\\mu-y}{\\sigma^{2}} q(y \\mid \\mu) d Q_{0}(\\mu)}{\\int_{\\mathbb{R}^{d}} q(y \\mid \\mu) d Q_{0}(\\mu)} \\\\\n& =\\frac{1}{\\sigma^{2}} \\mathbb{E}[\\mu-y \\mid Y=y]\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2404.18869v2",
  "authors": [
    "Khashayar Gatmiry",
    "Jonathan Kelner",
    "Holden Lee"
  ],
  "categories": [
    "cs.LG",
    "cs.DS",
    "math.PR",
    "math.ST",
    "stat.ML",
    "stat.TH"
  ],
  "abstract": "We give a new algorithm for learning mixtures of $k$ Gaussians (with identity\ncovariance in $\\mathbb{R}^n$) to TV error $\\varepsilon$, with quasi-polynomial\n($O(n^{\\text{poly\\,log}\\left(\\frac{n+k}{\\varepsilon}\\right)})$) time and sample\ncomplexity, under a minimum weight assumption. Our results extend to continuous\nmixtures of Gaussians where the mixing distribution is supported on a union of\n$k$ balls of constant radius. In particular, this applies to the case of\nGaussian convolutions of distributions on low-dimensional manifolds, or more\ngenerally sets with small covering number, for which no sub-exponential\nalgorithm was previously known. Unlike previous approaches, most of which are\nalgebraic in nature, our approach is analytic and relies on the framework of\ndiffusion models. Diffusion models are a modern paradigm for generative\nmodeling, which typically rely on learning the score function (gradient\nlog-pdf) along a process transforming a pure noise distribution, in our case a\nGaussian, to the data distribution. Despite their dazzling performance in tasks\nsuch as image generation, there are few end-to-end theoretical guarantees that\nthey can efficiently learn nontrivial families of distributions; we give some\nof the first such guarantees. We proceed by deriving higher-order Gaussian\nnoise sensitivity bounds for the score functions for a Gaussian mixture to show\nthat that they can be inductively learned using piecewise polynomial regression\n(up to poly-logarithmic degree), and combine this with known convergence\nresults for diffusion models.",
  "updated": "2025-03-04T15:36:34Z",
  "published": "2024-04-29T17:00:20Z"
}