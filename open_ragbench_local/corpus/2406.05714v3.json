{
  "title": "Contextual Continuum Bandits: Static Versus Dynamic Regret",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nWe study the contextual continuum bandits problem, where the learner sequentially receives a side information vector and has to choose an action in a convex set, minimizing a function associated with the context. The goal is to minimize all the underlying functions for the received contexts, leading to a dynamic (contextual) notion of regret, which is stronger than the standard static regret. Assuming that the objective functions are H\u00f6lder with respect to the contexts, we demonstrate that any algorithm achieving a sub-linear static regret can be extended to achieve a sub-linear dynamic regret. We further study the case of strongly convex and smooth functions when the observations are noisy. Inspired by the interior point method and employing self-concordant barriers, we propose an algorithm achieving a sub-linear dynamic regret. Lastly, we present a minimax lower bound, implying two key facts. First, no algorithm can achieve sub-linear dynamic regret over functions that are not continuous with respect to the context. Second, for strongly convex and smooth functions, the algorithm that we propose achieves, up to a logarithmic factor, the minimax optimal rate of dynamic regret as a function of the number of queries.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "## 1 Introduction\n\nThe continuum bandit problem presents a sequential decision-making challenge where, at each round, an action is taken as a continuous variable followed by suffering a corresponding loss. Various variants of this problem have been explored in the literature. The most extensively studied framework involves the learner receiving a function $f_{t}: \\Theta \\rightarrow \\mathbb{R}$ (random or non-random), at each round $t$, where $\\Theta \\subseteq \\mathbb{R}^{d}$ is a convex body (compact and convex set with a non-empty interior). Based on $\\left\\{\\left(y_{k}, \\boldsymbol{z}_{k}\\right)\\right\\}_{k=1}^{t-1}$, the learner selects $\\boldsymbol{z}_{t} \\in \\Theta$, suffers loss $f_{t}\\left(\\boldsymbol{z}_{t}\\right)$, and receives the noisy feedback\n\n$$\ny_{t}=f_{t}\\left(\\boldsymbol{z}_{t}\\right)+\\xi_{t}\n$$\n\nwhere $\\xi_{t}$ denotes the noise variable. The standard goal of the learner is to minimize the static regret\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T} f_{t}\\left(\\boldsymbol{z}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} \\sum_{t=1}^{T} f_{t}(\\boldsymbol{z})\\right]\n$$\n\nHowever, in this paper we consider a more ambitious task, which is to minimize the dynamic regret\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T} f_{t}\\left(\\boldsymbol{z}_{t}\\right)-\\sum_{t=1}^{T} \\min _{\\boldsymbol{z} \\in \\Theta} f_{t}(\\boldsymbol{z})\\right]\n$$\n\nNote that (2) presents a stronger notion of regret; achieving sub-linear dynamic regret implies sublinear static regret. The difference between static and dynamic regret is where the minimum is placed. If static regret is small, it means the algorithm is at least as good as a single constant action that does well on the average of the functions seen during learning. In contrast, proving that dynamic regret is small implies that the algorithm is good for any function individually.\n\nThere is a sharp difference between static and dynamic regret. In general, obtaining a sub-linear static regret does not imply convergence properties for the sequence $\\left\\{f_{t}\\left(\\boldsymbol{z}_{t}\\right)\\right\\}_{t=1}^{T}$. The static regret is not necessarily positive. Conversely, achieving sub-linear dynamic regret (2) implies that for any $\\epsilon>0$, only for a finite number of rounds we have\n\n$$\n\\mathbf{E}\\left[f_{t}\\left(\\boldsymbol{z}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f_{t}(\\boldsymbol{z})\\right] \\geq \\epsilon\n$$\n\nIn other words, if an algorithm achieves sub-linear dynamic regret, it asymptotically approaches the minimum values of the received functions. Therefore a key question arises: When it is possible to achieve a sub-linear dynamic regret?\n\nIn the setting of non-bandit online zero- and first-order optimization, several authors have extensively explored the minimization of dynamic regret (2), see e.g., [9, 19, 24, 34, 52, 54]. To achieve sub-linear dynamic regret, these works introduce complexity assumptions on the received functions. Such assumptions characterize similarity between two consecutive functions helping the learner to leverage previously received information for subsequent inference. In the present paper, we consider the problem of contextual continuum bandits under the similarity notion described by Assumption 1 below.\n\nContextual continuum bandits. Let $\\Theta \\subseteq \\mathbb{R}^{d}$ be a convex body. Let $f: \\mathbb{R}^{d} \\times[0,1]^{p} \\rightarrow \\mathbb{R}$ be an unknown function. At each round $t$ :\n\n- A context $\\boldsymbol{c}_{t} \\in[0,1]^{p}$ is revealed by the adversary.\n- The learner chooses a query point $\\boldsymbol{z}_{t} \\in \\Theta$ and gets a noisy evaluation of $f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)$ :\n\n$$\ny_{t}=f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)+\\xi_{t}\n$$\n\nwhere $\\xi_{t}$ is a scalar noise variable.\n\nThe learner's objective in this framework is to minimize the contextual regret, defined as:\n\n$$\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right]\n$$\n\nBy letting $f_{t}(\\cdot)=f\\left(\\cdot, \\boldsymbol{c}_{t}\\right)$ we see that (4) is equivalent to the dynamic regret (2). Here and in the sequel, unless other assumption is explicitly stated, we assume that $\\boldsymbol{c}_{t}$ is drawn by an adversary who may be aware of all previous actions of the learner and past information received by the learner. This formalization is an extension of the contextual multi-armed bandit problem [5] to the continuous action setup. One can consider the continuous dosage drug design problem as an example. At each round $t$, the learner encounters a patient with a specific medical profile encoded as $\\boldsymbol{c}_{t}$. The learner's goal is to craft a drug containing $d$ substances that closely matches the optimal drug design for a patient with the profile $\\boldsymbol{c}_{t}$. Informally, achieving a sub-linear dynamic regret implies that, as $t$ increases, the designed drug approaches the best possible drug for the medical profile $\\boldsymbol{c}_{t}$. However, in Section 5 we demonstrate that, regardless of the behavior of the function $f(\\cdot, \\boldsymbol{c})$, if the function $f(\\boldsymbol{x}, \\cdot)$ lacks continuity, obtaining a sub-linear dynamic regret may be impossible. Hence, we are lead to assume that $f(\\boldsymbol{x}, \\cdot)$ is a continuous function for any $\\boldsymbol{x} \\in \\mathbb{R}^{d}$. This is aligned with the example of continuous dosage drug design. Indeed, it is natural to consider that the effect of a fixed drug is similar for two patients with similar medical profiles. This intuition leads us to the following somewhat stronger assumption that we adopt in the sequel.\nAssumption 1. For $(L, \\gamma) \\in[0, \\infty) \\times(0,1]$ we assume that $f \\in \\mathcal{F}_{\\gamma}(L)$, where $\\mathcal{F}_{\\gamma}(L)$ denotes the class of all functions $f: \\mathbb{R}^{d} \\times[0,1]^{p} \\rightarrow \\mathbb{R}$ such that\n\n$$\n\\left|f(\\boldsymbol{x}, \\boldsymbol{c})-f\\left(\\boldsymbol{x}, \\boldsymbol{c}^{\\prime}\\right)\\right| \\leq L\\left\\|\\boldsymbol{c}-\\boldsymbol{c}^{\\prime}\\right\\|^{\\gamma}, \\quad \\text { for all } \\quad \\boldsymbol{x} \\in \\Theta, \\boldsymbol{c}, \\boldsymbol{c}^{\\prime} \\in[0,1]^{p}\n$$\n\nwhere $\\|\\cdot\\|$ is the Euclidean norm.\nIn Section 3, we provide a static-to-dynamic regret conversion theorem that can be summarized as follows. Consider functions $f_{t}$ that can be represented as $f_{t}(\\boldsymbol{x})=f\\left(\\boldsymbol{x}, \\boldsymbol{c}_{t}\\right)$, where $f: \\mathbb{R}^{d} \\times$ $[0,1]^{p} \\rightarrow \\mathbb{R}$. We show that if there exists an algorithm attaining a sub-linear static regret (1) when all $f_{t}$ 's belong to a class of functions $\\mathcal{F}$, then the same algorithm can be extended to achieve sublinear dynamic regret for functions within $\\mathcal{F} \\cap \\mathcal{F}_{\\gamma}(L)$.\n\nIn Section 4, we focus on a specific class $\\mathcal{F}$ that consists of strongly convex and smooth functions. In order to apply the approach developed in Section 3, we first construct an algorithm that achieves sub-linear static regret, see Algorithm 2. It is inspired by the Bandit Convex Optimization (BCO) algorithm proposed in [20], which is valid for non-noisy observations, that is, when $\\xi_{t} \\equiv 0$ in (3). In contrast, Algorithm 2 works for noisy observations. Next, for functions $f$ that additionally belong to the class $\\mathcal{F}_{\\gamma}(L)$, we extend the algorithm to attain a sub-linear dynamic regret using the conversion theorem of Section 3. Finally, in Section 5 we demonstrate that, if $f(\\boldsymbol{x}, \\cdot)$ lacks continuity, obtaining a sub-linear dynamic regret may be impossible. Moreover, the minimax lower bound established in Section 5 implies near optimality (up to a logarithmic factor) of the algorithm proposed in Section 4.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 2 Related work \n\nThe material presented below can be approached from three interconnected points of view: (i) the setting of contextual bandits, which is the main focus of this paper; (ii) the study of dynamic regret and the conditions under which sub-linear dynamic regret is achievable; and (iii) the convex bandits setting with strongly convex objective functions (see Sections 4 and 5).\n\nContextual bandits. Contextual bandit problem is a variant of bandit problem, where at each round the learner is equipped with an additional side information on the subsequent objective function. The problem has been introduced by [46] in the context of multi-armed bandits. Improvements and different setups, but all within the framework of multi-armed bandits, have been studied by many authors, see, e.g., $[6,7,18,25,28,38-40,45,48]$. In the framework of continuum bandits, [42] studied the problem when Assumption 1 holds with $\\gamma=1$, along with an additional assumption that the objective functions are Lipschitz with respect to the action variable, and proposed a method achieving an exponentially growing with $p$ dynamic regret. In [14], the authors studied the problem when Assumption 1 is satisfied, the objective functions are Lipschitz and convex with respect to the action variable, and the objective functions attain their global minimum in the constraint set $\\Theta$. They proposed an algorithm that achieves a sub-linear dynamic regret and proved its optimality by deriving a matching minimax lower bound.\n\nIn a framework close to the one that we consider in Sections 4 and 5, [33] addressed the problem of contextual bandits when the functions are strongly convex and smooth. To establish an upper bound for the regret, [33] assumed that, for each $\\boldsymbol{c} \\in[0,1]^{p}$, the global minimizer $\\arg \\min _{\\boldsymbol{x} \\in \\mathbb{R}^{d}} f(\\boldsymbol{x}, \\boldsymbol{c})$ is an interior point of the constraint set $\\Theta$. While this assumption might be hard to grant in practice, it does simplify the analysis considerably. In the present paper, we drop this assumption and obtain in Section 4 an upper bound that improves upon the result of [33]. Furthermore, in Section 5 we present a matching lower bound demonstrating that the algorithm that we propose is optimal to within a logarithmic factor.\n\nDynamic regret. In the context of online convex optimization, numerous authors have obtained bounds on the dynamic regret. In the first-order optimization setup, where the learner can access a noisy gradient of the function at any given point, dynamic regret was explored by $[9,24,34,44,47$, $49,52,53]$. For zero-order optimization, the dynamic regret has been studied by $[15,47,50,51,54]$. Among these papers, only $[15,47,51]$ have addressed the classical bandit framework, in which the learner is not allowed to query function evaluations outside of the constraint set $\\Theta$. We may also note that all the papers on contextual bandits cited above are dealing with the contextual regret, which is equivalent to the dynamic regret if the contexts are adversarial as discussed after (4). However, it is quite common in the literature to assume that the contexts are i.i.d. rather than adversarial.\n\nConvex bandits. Convex bandits represent a family of constrained convex online optimization problems where, at each round, the learner can only observe a single function evaluation, and the query point must belong to the constraint set $\\Theta$. Static regret in convex bandits problems has been extensively studied in the literature, see e.g., [30, 32, 37] and the references therein. A particular instance dealing with strongly convex and smooth functions has been investigated in [20] and [23]\n\nin a non-stochastic setting. In [20], the authors proposed an algorithm leveraging an interior point method and a self-concordant function. The achieved regret is nearly optimal, ranging from $d \\sqrt{T}$ to $d^{3 / 2} \\sqrt{T}$ depending on the geometry of the constraint set. Assuming that all the minimizers of the objective functions are in the interior of the constraint set, [23] introduced an algorithm with a regret of the order $d \\sqrt{T}$. Both [20] and [23] studied the problem with no noise ( $\\xi_{t} \\equiv 0$ ). In Section 4 , we extend the results of [20] to noisy function evaluation as in (3).\n\nRegarding simply convex functions, the first studies date back to [16, 26], where the authors addressed the setting with convex and Lipschitz objectives and proposed algorithms that achieve a regret of the order $T^{3 / 4}$ up to a factor depending on $d$. These works assume that the observed function values are noise-free. Successive improvements [8, 12, 21, 29] showed that a regret of the order $\\sqrt{T}$ (up to a factor depending on $d$ ) can be attained in the noise-free setting, the sharpest known rate being $d^{2.5} \\sqrt{T}$ [29]. Noisy variant of the problem was studied only when the objective function does not vary over time and the noise is sub-Gaussian. This led to algorithms with a regret of the order $d^{16} \\sqrt{T}$ [2] and $d^{4.5} \\sqrt{T}$ [31]. For a detailed survey on convex bandits, we refer the reader to $[30$, Sec. 2].",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 3 Static-to-dynamic regret conversion \n\nWe consider strategies for choosing the query points such that $\\boldsymbol{z}_{0} \\in \\mathbb{R}^{d}$ is a random variable, and $\\boldsymbol{z}_{t}=\\pi_{t}\\left(\\left\\{\\boldsymbol{c}_{k}, \\boldsymbol{z}_{k}, y_{k}\\right\\}_{k=1}^{t-1}, \\boldsymbol{c}_{t}, \\boldsymbol{\\zeta}_{t}\\right)$ for $t \\geq 1$, where $\\pi_{t}$ is a measurable function with values in $\\mathbb{R}^{d}$, and $\\left(\\zeta_{t}\\right)_{t \\geq 1}$ is a sequence of random variables with values in a measurable space $(\\mathcal{Z}, \\mathcal{U})$ such that $\\zeta_{t}$ is independent of $\\left\\{\\boldsymbol{c}_{k}, \\boldsymbol{z}_{k}, y_{k}\\right\\}_{k=1}^{t-1}, \\boldsymbol{c}_{t}$. We call the process $\\pi=\\left(\\pi_{t}\\right)_{t \\geq 1}$ a randomized procedure and we denote the set of all randomized procedures by $\\Pi$.\n\nLet $\\mathcal{F}^{\\prime}$ be a class of real-valued functions on $\\mathbb{R}^{d}$. We define $\\mathcal{F}=\\left\\{f: f(\\cdot, \\boldsymbol{c}) \\in \\mathcal{F}^{\\prime}\\right.$ for all $\\boldsymbol{c} \\in$ $[0,1]^{p}$ \\}. Assume that there exists a randomized procedure $\\pi$, for which we can control its static regret over $T$ runs for the sequence of functions $\\left\\{f(\\cdot, \\boldsymbol{c}_{t})\\right\\}_{t=1}^{T}$ where $f \\in \\mathcal{F}$. Namely, if $\\left\\{\\boldsymbol{z}_{t}\\right\\}_{t=1}^{T}$ are outputs of $\\pi$, then there exist $F: \\mathbb{R}^{+} \\cup\\{0\\} \\rightarrow \\mathbb{R}^{+} \\cup\\{0\\}, F_{1}: \\mathbb{R}^{+} \\cup\\{0\\} \\rightarrow \\mathbb{R}^{+} \\cup\\{0\\}$ such that $F$ is a concave function, $F_{1}$ is a non-decreasing function, and\n\n$$\n\\sup _{f \\in \\mathcal{F}} \\mathbf{E}\\left[\\sum_{t=1}^{T} f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} \\sum_{t=1}^{T} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right] \\leq F(T) F_{1}(T)\n$$\n\nIn particular, we will be interested in the case where there exists $\\left(\\tau_{1}, \\tau_{2}, \\tau_{3}, \\mathrm{~T}_{0}\\right) \\in[0, \\infty) \\times[0,1) \\times$ $[0, \\infty) \\times[1, \\infty)$ such that, for $T \\geq \\mathrm{~T}_{0}$,\n\n$$\nF(T) \\leq C d^{\\tau_{1}} T^{\\tau_{2}} \\quad \\text { and } \\quad F_{1}(T) \\leq \\log ^{\\tau_{3}}(T+1)\n$$\n\nwhere $C>0$ is a constant that does not depend on $T$ and $d$.\nDefinition 1. We call a randomized procedure that satisfies (6) and (7) a $\\left(\\mathcal{F}, \\tau_{1}, \\tau_{2}, \\tau_{3}, \\mathrm{~T}_{0}\\right)$-consistent randomized procedure and we denote the set of all such procedures by $\\Pi\\left(\\mathcal{F}, \\tau_{1}, \\tau_{2}, \\tau_{3}, \\mathrm{~T}_{0}\\right)$.\n\nAlgorithm 1:\nInput: Randomized procedure $\\pi=\\left(\\pi_{t}\\right)_{t=1}^{\\infty}$, parameter $K \\in \\mathbb{N}$ and a partition $\\left\\{B_{i}\\right\\}_{i=1}^{K^{d}}$ of $[0,1]^{p}$\nInitialization: $N_{1}(0), \\ldots, N_{K^{p}}(0)=0, H(1), \\ldots, H\\left(K^{p}\\right)=\\{0\\}$, and $\\boldsymbol{z}_{0}=\\mathbf{0}$\nfor $t=1, \\ldots, T$ do\nif $\\boldsymbol{c}_{\\boldsymbol{t}} \\in B_{i}$ then\nLet $N_{i}(t)=N_{i}(t-1)+1 \\quad / /$ Increment\nBased on $\\boldsymbol{c}_{t}$ and $\\left\\{y_{k}, \\boldsymbol{z}_{k}, \\boldsymbol{c}_{k}\\right\\}_{k \\in H(i)}$, use $\\pi_{N_{i}(t)}$ to choose $\\boldsymbol{z}_{t} \\in \\Theta / /$ Choosing\nthe query point\n$y_{t}=f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)+\\xi_{t} \\quad / /$ Query\n$H(i)=H(i) \\cup\\{t\\} \\quad / /$ Update the set of indices\nend\nend\n\nAlgorithm 1 is a meta-algorithm for static-to-dynamic regret conversion. It uses a partition $\\left\\{B_{i}\\right\\}_{i=1}^{K^{p}}$ of the hypercube $[0,1]^{p}$, where all the cells $B_{i}$ are hypercubes of equal volume with edges of length $1 / K$. Fix a consistent randomized procedure $\\pi$ for the class of functions $\\mathcal{F}$. At each round $t$, if the context $\\boldsymbol{c}_{t}$ falls into the cell $B_{i}$, Algorithm 1 chooses the query point $\\boldsymbol{z}_{t}$ by the procedure $\\pi$ based only on the past contexts from the same cell $B_{i}$ and the corresponding past query points.\n\nLet $\\left\\{\\boldsymbol{c}_{i_{j}}\\right\\}_{j=1}^{N_{i}(T)}$ be all the contexts belonging to $B_{i}$ revealed up to round $T$, where $N_{i}(T)$ is the total number of such contexts, and let $\\left\\{\\boldsymbol{z}_{i_{j}}\\right\\}_{j=1}^{N_{i}(T)}$ be the outputs of $\\pi$ restricted to the contexts that belong to $B_{i}$ (the inner loop of Algorithm 1). This segment of the algorithm involves saving the index of the rounds associated with the received contexts in cell $B_{i}$. The set $H(i)$ is the collection of such indices. Denote by $\\boldsymbol{b}_{i}$ the barycenter of $B_{i}$, and let $\\boldsymbol{x}_{i}^{*} \\in \\arg \\min _{\\boldsymbol{x} \\in \\Theta} f\\left(\\boldsymbol{x}, \\boldsymbol{b}_{i}\\right)$. Then,\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right]=\\mathbf{E}\\left[\\sum_{i=1}^{K^{p}} \\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)+\\Delta_{i_{j}}\\right)\\right] \\\\\n& \\leq \\mathbf{E}\\left[\\sum_{i=1}^{K^{p}}\\left(\\sum_{j=1}^{N_{i}(T)} f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} \\sum_{j=1}^{N_{i}(T)} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{i_{j}}\\right)+\\sum_{j=1}^{N_{i}(T)} \\Delta_{i_{j}}\\right)\\right]\n\\end{aligned}\n$$\n\nwhere $\\Delta_{i_{j}}=f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{i_{j}}\\right)$. Notice that $\\Delta_{i_{j}} \\leq 2 \\max _{\\boldsymbol{z} \\in \\Theta}\\left|f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{z}, \\boldsymbol{b}_{i}\\right)\\right|$, so that under Assumption 1 we have $\\Delta_{i_{j}} \\leq 2 L(\\sqrt{p} / K)^{\\gamma}$. Thus, we can control the dynamic regret by the sum of the static regret, which grows with $K$, and the bias term comprised of $\\Delta_{i_{j}}$ 's, which decreases with $K$. There exists an optimal $K$ exhibiting a trade-off between the static regret and\n\nthe bias. In Theorem 1, we show that if $K$ is tuned optimally,\n\n$$\nK=K\\left(\\tau_{1}, \\tau_{2}, \\tau_{3}, \\gamma\\right) \\triangleq \\max \\left(1,\\left\\lfloor\\left(L p^{\\frac{\\gamma}{2}} d^{-\\tau_{1}} T^{\\left(1-\\tau_{2}\\right)} \\log ^{-\\tau_{3}}(T+1)\\right)^{\\frac{1}{p\\left(1-\\tau_{2}\\right)+\\gamma}}\\right\\rfloor\\right)\n$$\n\nthen Algorithm 1 achieves a sub-linear dynamic regret.\nTheorem 1. Let $(L, \\gamma) \\in[0, \\infty) \\times(0,1]$. Let $\\pi$ be a randomized procedure such that (6) holds with a concave function $F$, a non-decreasing function $F_{1}$, and let $\\boldsymbol{z}_{t}$ 's be the updates of Algorithm 1 with a positive integer $K$. Then,\n\n$$\n\\sup _{f \\in \\mathcal{F} \\cap \\mathcal{F}_{\\gamma}(L)} \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq K^{p} F\\left(\\frac{T}{K^{p}}\\right) F_{1}(T)+2 L T\\left(\\frac{\\sqrt{p}}{K}\\right)^{\\gamma}\n$$\n\nIf, in addition, $\\pi \\in \\Pi\\left(\\mathcal{F}, \\tau_{1}, \\tau_{2}, \\tau_{3}, \\mathrm{~T}_{0}\\right)$ for a triplet $\\left(\\tau_{1}, \\tau_{2}, \\mathrm{~T}_{0}\\right) \\in[0, \\infty) \\times[0,1) \\times[1, \\infty)$, and $K=K\\left(\\tau_{1}, \\tau_{2}, \\tau_{3}, \\gamma\\right)$ (see (8)), then for any $T \\geq K^{p} \\mathrm{~T}_{0}$, we have\n\n$$\n\\begin{aligned}\n& \\sup _{f \\in \\mathcal{F} \\cap \\mathcal{F}_{\\gamma}(L)} \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\\\\n& \\quad \\leq C^{\\prime} \\max \\left(d^{\\tau_{1}} T^{\\tau_{2}} L_{1},\\left(L p^{\\frac{\\gamma}{2}}\\right)^{\\frac{p\\left(1-\\tau_{2}\\right)}{p\\left(1-\\tau_{2}\\right)+\\gamma}}\\left(d^{\\tau_{1}} L_{1}\\right)^{\\frac{\\gamma}{p\\left(1-\\tau_{2}\\right)+\\gamma}} T^{\\frac{(p-\\gamma)\\left(1-\\tau_{2}\\right)+\\gamma}{p\\left(1-\\tau_{2}\\right)+\\gamma}}\\right) .\n\\end{aligned}\n$$\n\nwhere $L_{1}=\\log ^{\\tau_{3}}(T+1)$ and $C^{\\prime}>0$ is a constant independent of $T, d$, and $p$.\nNote that the Lipschitz constant $L$ measures how sensitive the function $f$ is with respect to variations of the contexts. If $L=0$ the static and dynamic regrets are equivalent, and it is not surprising that (7) and (10) have the same rates. We also note that a bound that depends in the same way as (10) on the parameters $p, d$, and $T$ can be achieved with a choice of $K$ independent of $L$. In Theorem 1 we assume that the learner knows the value of $L$ but this is only done to highlight the gap between the notions of static and dynamic regret.\n\nThe approach to contextual bandits based on partitioning the space of contexts and running some algorithms $\\pi$ separately within each cell is not new, see [13, 27, 42]. These papers, as well as the follow-up works developing the same approach, focused on a specific algorithm $\\pi$, a specific class $\\mathcal{F}$ and provided a proof using their particular properties. They did not invoke conversion from static to dynamic regret. In contrast, Theorem 1 highlights a general static-to-dynamic conversion phenomenon and provides a ready-to-use tool for exploring particular examples. In the next section, we demonstrate its application to the important case that $\\mathcal{F}^{\\prime}$ is the class of strongly convex and smooth functions.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 4 Strongly convex and smooth functions \n\nIn this section, we apply the static-to-dynamic regret conversion developed in Section 3 to the contextual bandits problem with the objective functions $f$ such that $f(\\cdot, \\boldsymbol{c})$ is $\\beta$-smooth and $\\alpha$ strongly convex for any $\\boldsymbol{c} \\in[0,1]^{p}$.\n\nDefinition 2. Let $\\beta>0$. Function $F: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ is called $\\beta$-smooth, if it is continuously differentiable and for any $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{d}$ it satisfies\n\n$$\n\\|\\nabla F(\\boldsymbol{x})-\\nabla F(\\boldsymbol{y})\\| \\leq \\beta\\|\\boldsymbol{x}-\\boldsymbol{y}\\|\n$$\n\nDefinition 3. Let $\\alpha>0$. A differentiable function $F: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ is called $\\alpha$-strongly convex, if for any $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{d}$ it satisfies\n\n$$\nF(\\boldsymbol{x}) \\geq F(\\boldsymbol{y})+\\langle\\nabla F(\\boldsymbol{y}), \\boldsymbol{x}-\\boldsymbol{y}\\rangle+\\frac{\\alpha}{2}\\|\\boldsymbol{x}-\\boldsymbol{y}\\|^{2}\n$$\n\nWe impose the following assumption on the function $f$.\nAssumption 2. For $(\\alpha, \\beta, M) \\in(0, \\infty) \\times[\\alpha, \\infty) \\times(0, \\infty)$, we assume that $f \\in \\mathcal{F}_{\\alpha, \\beta}(M)$, where $\\mathcal{F}_{\\alpha, \\beta}(M)$ denotes the class of all functions $f: \\mathbb{R}^{d} \\times[0,1]^{p} \\rightarrow \\mathbb{R}$ such that for all $\\boldsymbol{c} \\in[0,1]^{p}$ the following holds.\ni) $f(\\cdot, \\boldsymbol{c})$ is $\\alpha$-strongly convex.\nii) $f(\\cdot, \\boldsymbol{c})$ is $\\beta$-smooth.\niii) $\\max _{x \\in \\Theta}|f(\\boldsymbol{x}, \\boldsymbol{c})| \\leq M$.\n\nFollowing the approach of Section 3, we first construct a consistent randomized procedure, that is, a procedure with sub-linear static regret, cf. (6). To this end, our starting point is the BCO algorithm introduced in [20] to solve strongly convex bandit problems. The BCO algorithm makes use of the interior point method, which is a vital tool in convex optimization, see, e.g, [36]. A pioneering role of introducing this method to the field of online learning is credited to [1]. We cannot directly apply the BCO algorithm as defined in [20] since [20] deals with the setting, where the observations $y_{t}$ are noise-free. Therefore, we propose a modification of the BCO algorithm adapted to noisy observations (3) with sub-Gaussian noise $\\xi_{t}$. It is defined in Algorithm 2. In what follows, $[T]$ denotes the set of all positive integers less than or equal to $T$. We denote the unit Euclidean ball in $\\mathbb{R}^{d}$ by $B^{d}$ and the unit Euclidean sphere by $\\partial B^{d}$.\n\nAssumption 3. There is $\\sigma>0$ such that for all $t \\in[T]$ the following holds: (i) $\\xi_{t}$ is $\\sigma$-subGaussian, i.e., for any $\\lambda>0$ we have $\\mathbf{E}\\left[\\exp \\left(\\lambda \\xi_{t}\\right)\\right] \\leq \\exp \\left(\\sigma^{2} \\lambda^{2} / 2\\right)$; (ii) $\\xi_{t}$ is independent of $\\zeta_{t}$.\n\nWe emphasize that the mutual independence of the noise variables $\\xi_{t}$ is not assumed.\nThe main ingredient of the interior point method is a self-concordant barrier defined as follows.\nDefinition 4. Let $\\operatorname{int}(\\Theta)$ be the interior of the set $\\Theta$. For $\\mu>0$, a function $\\mathcal{R}: \\operatorname{int}(\\Theta) \\rightarrow \\mathbb{R}$ is a self-concordant function if the following holds.\n\n- $\\mathcal{R}$ is three times differentiable and converges to infinity for any trajectory that converges to the boundary of $\\Theta$, i.e., $\\mathcal{R}(\\boldsymbol{y}) \\rightarrow \\infty$ as $\\boldsymbol{y} \\rightarrow \\partial \\Theta$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# Algorithm 2: \n\nInput: $\\mu$-self-concordant barrier function $\\mathcal{R}$ on $\\Theta$, and $q_{T}=M+2 \\sigma \\sqrt{\\log (T+1)}$\nInitialization: $\\boldsymbol{x}_{0}=\\arg \\min _{x \\in \\operatorname{int}(\\Theta)} \\mathcal{R}(\\boldsymbol{x})$\nfor $\\underline{t}=1, \\ldots, T$ do\n$\\eta_{t}=\\left(4 d q_{T}\\right)^{-1} \\min \\left(1, \\sqrt{\\frac{16(\\mu+\\beta / \\alpha) \\log (t+1)}{t}}\\right) \\quad / /$ Update the step size\n$P_{t}=\\left(\\nabla^{2} \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)+\\eta_{t} \\alpha t \\mathbf{I}_{d}\\right)^{-\\frac{1}{2}} \\quad / /$ Update the perturbation parameter\nGenerate $\\boldsymbol{\\zeta}_{t}$ from $\\partial B^{d}$\n// Randomization\n$\\boldsymbol{z}_{t}=\\boldsymbol{x}_{t-1}+P_{t} \\boldsymbol{\\zeta}_{t}$\n// Output\n$y_{t}=f\\left(z_{t}, \\boldsymbol{c}_{t}\\right)+\\xi_{t}$\n// Query\n$\\boldsymbol{g}_{t}=d y_{t} P_{t}^{-1} \\boldsymbol{\\zeta}_{t}$\n// Estimate the gradient\n$\\boldsymbol{x}_{t}=\\arg \\min _{x \\in \\Theta} \\sum_{k=1}^{t} \\eta_{k}\\left(\\left\\langle\\boldsymbol{g}_{k}, \\boldsymbol{x}\\right\\rangle+\\frac{\\alpha}{2}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{k-1}\\right\\|^{2}\\right)+\\mathcal{R}(\\boldsymbol{x}) \\quad / /$ Iterate\nend\n\n- For any $\\boldsymbol{x} \\in \\mathbb{R}^{d}$ and $\\boldsymbol{y} \\in \\operatorname{int}(\\Theta)$ we have\n\n$$\n\\left|\\nabla^{3} \\mathcal{R}(\\boldsymbol{y})[\\boldsymbol{x}, \\boldsymbol{x}, \\boldsymbol{x}]\\right| \\leq 2\\left(\\nabla^{2} \\mathcal{R}(\\boldsymbol{y})[\\boldsymbol{x}, \\boldsymbol{x}]\\right)^{\\frac{3}{2}}\n$$\n\nwhere for $m \\in[3]$, and $\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{m} \\in \\mathbb{R}^{d}$\n\n$$\n\\nabla^{m} \\mathcal{R}(\\boldsymbol{y})\\left[\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{m}\\right]=\\left.\\frac{\\partial^{m}}{\\partial t_{1} \\ldots \\partial t_{m}} \\mathcal{R}\\left(\\boldsymbol{y}+t_{1} \\boldsymbol{x}_{1}+\\cdots+t_{m} \\boldsymbol{x}_{m}\\right)\\right|_{t_{1}=0, \\ldots, t_{m}=0}\n$$\n\nIf, in addition, for some $\\mu>0$, and any $\\boldsymbol{x} \\in \\mathbb{R}^{d}$ and $\\boldsymbol{y} \\in \\operatorname{int}(\\Theta)$ we have\n\n$$\n|\\nabla \\mathcal{R}(\\boldsymbol{y})[\\boldsymbol{x}]| \\leq \\mu^{\\frac{1}{2}}\\left(\\nabla^{2} \\mathcal{R}(\\boldsymbol{y})[\\boldsymbol{x}, \\boldsymbol{x}]\\right)^{\\frac{1}{2}}\n$$\n\nthen the function $\\mathcal{R}$ is called a $\\mu$-self-concordant barrier.\nSince $\\Theta$ is bounded, $\\nabla^{2} \\mathcal{R}(\\boldsymbol{x})$ is positive definite and the minimizer of $\\mathcal{R}$ in $\\operatorname{int}(\\Theta)$ is unique, see [35, Chapter 3, Section V]. Moreover, without loss of generality, we assume that $\\min _{\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)} \\mathcal{R}(\\boldsymbol{x})=$ 0 . Self-concordant barriers can be chosen depending on the geometry of $\\Theta$. For the examples of $\\mathcal{R}$ and the corresponding values of $\\mu$, we refer the reader to Appendix D.\n\nThe discrepancy between Algorithm 2 and the BCO algorithm of [20] is in the assigned step size. In [20] the step size is constant while it varies with $t$ in Algorithm 2. Next, in Algorithm 2, the step size is downscaled by $q_{T}=M+2 \\sigma \\sqrt{\\log (T+1)}$ whereas in [20] it is downscaled by $M$ without any additional logarithmic term. This difference is due to the fact that the observations in (3) are noisy whereas in [20] they are assumed to be noise-free. In our analysis, we rely on the\n\nfact that $\\eta_{t}\\left\\|P_{t} \\boldsymbol{g}_{t}\\right\\|$ is uniformly bounded with high probability. To ensure this, it is enough to scale down the step size by adding the above logarithmic term to $M$. Indeed, due to Assumption 3, we have that $\\max _{t \\in[T]}\\left|y_{t}\\right| \\leq q_{T}$ with a suitably high probability.\n\nAlgorithm 2 algorithm selects $\\boldsymbol{z}_{t} \\in\\left\\{\\boldsymbol{x}_{t-1}+P_{t} \\boldsymbol{\\zeta} \\mid \\boldsymbol{\\zeta} \\in \\partial B^{d}\\right\\} \\subseteq D \\triangleq\\left\\{\\boldsymbol{z} \\mid\\left(\\boldsymbol{z}-\\boldsymbol{x}_{t-1}\\right)^{\\top} \\nabla^{2} \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)\\left(\\boldsymbol{z}-\\right.\\right.$ $\\left.\\left.\\boldsymbol{x}_{t-1}\\right) \\leq 1\\right\\}$, in which $D$ is the Dikin ellipsoid. Consequently, $D \\subseteq \\Theta$; see [35, Proposition 14.2.2] for a proof. Therefore, $\\boldsymbol{z}_{t} \\in \\Theta$ for all $t \\in[T]$.\n\nTheorem 2. Let Assumptions 2 and 3 hold. Then, the static regret of Algorithm 2 satisfies\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T} f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} \\sum_{t=1}^{T} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right] \\leq F(T) F_{1}(T)\n$$\n\nwhere $F(\\cdot)$ is a concave function defined for $x \\geq 0$ by\n\n$$\nF(x)=11 \\sqrt{\\nu x \\log (x+1)}+\\nu(\\log (x+1)+2 \\log (\\nu+1))\n$$\n\nand $F_{1}(\\cdot)$ is a non-decreasing function such that for $x \\geq 0$\n\n$$\nF_{1}(x)=\\frac{d}{4}(5.3 M+2 \\sigma \\sqrt{\\log (x+1)})\n$$\n\nwith $\\nu=16(\\mu+\\beta / \\alpha)$. Moreover, for $T \\geq \\nu^{2} / 16$, we have\n\n$$\nF(T) \\leq 17 \\sqrt{\\nu T \\log (T+1)}\n$$\n\nand subsequently\n\n$$\nF(T) F_{1}(T) \\leq 17 d\\left(5.3 M+2 \\sigma \\sqrt{\\log (T+1)}\\right)\\left(\\left(\\mu+\\frac{\\beta}{\\alpha}\\right) T \\log (T+1)\\right)\n$$\n\nFor the case of noise-free observations in (3) we have $\\sigma=0$, and (11) has the same order as the bound on the regret provided in [20]. Note that for $\\sigma=0$, unlike [20], our Algorithm 2 is anytime and does not require any knowledge of $T$. We also note that, for the case of bounded noise $\\left(\\left|\\zeta_{t}\\right| \\leq \\sigma\\right), q_{T}$ can be set as $q_{T}=M+\\sigma$ and a refined regret in (11) can be derived, where the multiplicative term $M+2 \\sigma \\sqrt{\\log (T+1)}$ is replaced by $M+\\sigma$.\n\nIt is a known fact (see Appendix D) that, for any convex body $\\Theta$, one can construct a $\\mu$-selfconcordant barrier such that for $\\rho \\in[0,1], \\mu \\leq \\mathbb{A} d^{\\rho}$, where $\\mathbb{A}>0$ is a constant depending only on $\\Theta$ and independent of $d$. This fact and Theorem 2 imply that $\\Pi\\left(\\mathcal{F}_{\\alpha, \\beta}(M), 1+\\rho / 2,1 / 2,1,16(\\mu+\\beta / \\alpha)^{2}\\right)$ is non-empty. In [3, 4, 41], the authors established a lower bound of the order of $d / \\sqrt{T}$ for the optimization error when the noise is Gaussian [41] or satisfies more general assumptions [3, 4]. By convexity, this lower bound, when multiplied by $T$, provides a lower bound for the static regret in the case $L=0$, that is, $f\\left(\\boldsymbol{x}, \\boldsymbol{c}_{1}\\right)=f\\left(\\boldsymbol{x}, \\boldsymbol{c}_{t}\\right)$ for all $\\boldsymbol{x} \\in \\mathbb{R}^{d}$ and $t \\in[T]$. Thus, the lower bound in $[3,4,41]$ results in a valid lower bound of the order of $d \\sqrt{T}$ for the static regret in the noisy",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "# Algorithm 3: \n\nInput: Parameter $K \\in \\mathbb{N}$ and a partition of $[0,1]^{p}$ namely $\\left(B_{i}\\right)_{i=1}^{K^{p}}, \\mu$-self-concordant barrier $\\mathcal{R}$\nInitialization: $N_{1}(0), \\ldots, N_{K^{p}}(0)=0, H(1), \\ldots, H\\left(K^{p}\\right)=\\{0\\}$,\n\n$$\n\\boldsymbol{x}_{0}=\\arg \\min _{x \\in \\operatorname{int}(\\Theta)} \\mathcal{R}(\\boldsymbol{x})\n$$\n\nfor $\\underline{t=1, \\ldots, T}$ do\nif $\\underline{c_{t} \\in B_{i}}$ then\n$N_{i}(t)=N_{i}(t-1)+1 \\quad / /$ Increment\nLet $q=\\max H(i) \\quad / /$ The index of the last event in $B_{i}$\n$\\eta_{t}=\\left(4 d q_{T}\\right)^{-1} \\min \\left(1, \\sqrt{\\frac{16(\\mu+\\beta / \\alpha) \\log \\left(N_{i}(t)+1\\right)}{N_{i}(t)}}\\right) \\quad / /$ Update the step\nsize\n$P_{t}=\\left(\\nabla^{2} \\mathcal{R}\\left(\\boldsymbol{x}_{q}\\right)+\\eta_{t} \\alpha N_{i}(t) \\mathbf{I}_{d}\\right)^{-\\frac{1}{2}} \\quad / /$ Update the perturbation parameter\nGenerate $\\zeta_{t}$ from $\\partial B^{d} \\quad / /$ Randomization\n$\\boldsymbol{z}_{t}=\\boldsymbol{x}_{q}+P_{t} \\boldsymbol{\\zeta}_{t} \\quad / /$ Output\n$y_{t}=f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)+\\xi_{t} \\quad / /$ Query\n$\\boldsymbol{g}_{t}=d y_{t} P_{t}^{-1} \\boldsymbol{\\zeta}_{t} \\quad / /$ Estimate the gradient\n$\\boldsymbol{x}_{t}=\\arg \\min _{\\boldsymbol{x} \\in \\Theta} \\sum_{k \\in H(i)} \\eta_{k}\\left(\\left\\langle\\boldsymbol{g}_{k}, \\boldsymbol{x}\\right\\rangle+\\frac{\\alpha}{2}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{k-1}\\right\\|^{2}\\right)+\\mathcal{R}(\\boldsymbol{x})$\n// Iterate\n$H(i)=H(i) \\cup t \\quad / /$ Save the index of last event in $B_{i}$\nend\nend\nsetting. Therefore, if $\\rho=0$, which is the case when $\\Theta$ is a Euclidean ball, (11) is optimal up to a logarithmic factor. In the worst case scenario $(\\rho=1)$, for instance, when $\\Theta$ is the $\\ell_{\\infty}$-ball or the simplex (see Appendix D), akin to [20], our algorithm exhibits an optimality gap of up to $d^{1 / 2}$ and logarithmic factors.\n\nIn order to enable the static-to-dynamic regret conversion via Theorem 1, we need Assumption 1 , that is, the premise that $f$ belongs to $\\mathcal{F}_{\\gamma}(L)$. Since it always holds that $\\mu \\leq \\mathbb{A} d^{\\rho}$, where $\\rho \\in[0,1]$ and $\\mathrm{A}>0$ is a numerical constant, we obtain the following corollary of Theorems 1 and 2.\n\nCorollary 1. Let Assumptions 1, 2, and 3 hold. Let $\\rho \\in[0,1]$ be such that $\\mu \\leq \\mathbb{A} d^{\\rho}$, where $\\mathbb{A}>0$ is a numerical constant. Then, the dynamic regret of Algorithm 3 with $K=K(1+\\rho / 2,1 / 2,1, \\gamma)$\n\n(see (8)), satisfies\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq C \\max \\left\\{L_{1} d^{1+\\rho}+L_{2} \\sqrt{d^{2+\\rho} T}\\right. \\\\\n& \\left.L_{1}\\left(L p^{\\frac{\\gamma}{2}} L_{2}^{-1}\\right)^{\\frac{2 p}{p+2 \\gamma}} d^{\\frac{2 \\gamma(1+\\rho)-p}{p+2 \\gamma}} T^{\\frac{p}{p+2 \\gamma}}, L_{2}^{\\frac{2 \\gamma}{p+2 \\gamma}}\\left(L p^{\\frac{\\gamma}{2}}\\right)^{\\frac{p}{p+2 \\gamma}} d^{\\frac{(2+\\rho) \\gamma}{p+2 \\gamma}} T^{\\frac{p+\\gamma}{p+2 \\gamma}}\\right\\}\n\\end{aligned}\n$$\n\nwhere $L_{1}=\\sqrt{\\log (T+1)} \\log (T d+1), L_{2}=\\log (T+1)$, and $C>0$ is a constant depending only on $M, \\sigma, \\beta / \\alpha$.\n\nThe main term in the bound of Corollary 1 scales as $T^{\\frac{p+\\gamma}{p+2 \\gamma}}$. In [33], this rate was obtained for a different algorithm, under the additional assumption that for each $\\boldsymbol{c} \\in[0,1]^{p}$ the global minimizer $\\arg \\min _{\\boldsymbol{x} \\in \\mathbb{R}^{d}} f(\\boldsymbol{x}, \\boldsymbol{c})$ is an interior point of the constraint set $\\Theta$. Note also that the multiplicative factors in Corollary 1 exhibit a very mild dependence on both dimensions $p$ and $d$, never exceeding $p^{1 / 2} d$ in the main term.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 5 Lower bound \n\nIn this section, along with the classes $\\mathcal{F}_{\\gamma}(L), \\gamma \\in(0,1]$, we will consider the class $\\mathcal{F}_{0}(L)$, which includes discontinuous functions.\n\nDefinition 5. For $L>0$, let $\\mathcal{F}_{0}(L)$ be the class of functions such that $f \\in \\mathcal{F}_{0}(L)$ if\n\n$$\n\\left|f(\\boldsymbol{x}, \\boldsymbol{c})-f\\left(\\boldsymbol{x}, \\boldsymbol{c}^{\\prime}\\right)\\right| \\leq L, \\quad \\text { for all } \\quad \\boldsymbol{x} \\in \\Theta, \\boldsymbol{c}, \\boldsymbol{c}^{\\prime} \\in[0,1]^{p}\n$$\n\nWe will assume that the noises $\\left\\{\\xi_{t}\\right\\}_{t=1}^{T}$ are independent with cumulative distribution function $F$ satisfying the condition\n\n$$\n\\int \\log (\\mathrm{d} F(u) / d F(u+v)) \\mathrm{d} F(u) \\leq I_{0} v^{2}, \\quad|v|<v_{0}\n$$\n\nfor some $0<I_{0}<\\infty, 0<v_{0} \\leq \\infty$. It can be shown that condition (14) is satisfied for $F$ with a sufficiently smooth density and finite Fisher information. If $F$ follows a Gaussian distribution, (14) holds with $v_{0}=\\infty$. Note that Gaussian noise satisfies Assumption 3 used to prove the upper bound.\n\nNext, we introduce the distribution of the contexts used in deriving the lower bound. Similar to previous sections, for any $K \\in \\mathbb{N}$, consider the partition of $[0,1]^{p}$, denoted by $\\left\\{B_{i}\\right\\}_{i=1}^{K^{p}}$, for which we assume that $B_{i}$ 's have equal volumes. Moreover, for $i \\in\\left[K^{p}\\right]$, let $\\mathbf{P}_{i}$ denote the uniform distribution on $G_{i}=(1 / 2)\\left(B_{i}+\\boldsymbol{b}_{i}\\right)$, where $\\boldsymbol{b}_{i}$ is the barycenter of $B_{i}$. Then, we assume that $\\left\\{\\boldsymbol{c}_{t}\\right\\}_{t=1}^{T}$ are independently distributed according to $\\mathbb{P}_{K}$, where\n\n$$\n\\mathbb{P}_{K}(\\Upsilon)=K^{-p} \\sum_{i=1}^{K^{p}} \\mathbf{P}_{i}\\left(\\Upsilon \\cap G_{i}\\right)\n$$\n\nfor $\\Upsilon \\subseteq[0,1]^{p}$. Distribution $\\mathbb{P}_{K}$ represents a challenging scenario for the learning process. Indeed, the contexts are uniformly spread, preventing concentration in any specific region of $[0,1]^{p}$. Moreover, they are not clustered near the boundaries of each cell, thus preventing the learner from exploiting proximity between the cells. See Appendix C for further details.\n\nTheorem 3. For $(\\alpha, \\beta, M, \\gamma, L) \\in(0, \\infty) \\times[3 \\alpha, \\infty) \\times[\\alpha+1, \\infty) \\times[0,1] \\times[0, \\infty)$ consider the class of functions $\\overline{\\mathcal{F}}=\\mathcal{F}_{\\alpha, \\beta}(M) \\cap \\mathcal{F}_{\\gamma}(L)$. Let $\\Theta=B^{d}$, and let $\\left\\{\\boldsymbol{c}_{t}\\right\\}_{t=1}^{T}$ be independently distributed according to $\\mathbb{P}_{K}$, where $K=\\max \\left(1,\\left\\lfloor\\left(\\min \\left(1, L^{2}\\right) T\\right)^{\\frac{1}{p+2 \\gamma}}\\right\\rfloor\\right)$. Let $\\pi \\in \\Pi$ be any randomized procedure and assume that $\\left\\{\\boldsymbol{z}_{t}\\right\\}_{t=1}^{T}$ are outputs of $\\pi$. Then,\n\n$$\n\\sup _{f \\in \\overline{\\mathcal{F}}} \\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right] \\geq \\mathrm{A}\\left(\\min \\left(1, L^{\\frac{2(p+\\gamma)}{p+2 \\gamma}}\\right) T^{\\frac{p+\\gamma}{p+2 \\gamma}}+\\min (T, d \\sqrt{T})\\right)\n$$\n\nwhere $\\mathrm{A}>0$ is a constant that does not depend on $d, p$, and $T$.\nApplying (16) with $L>0$ and $\\gamma=0$ shows that no randomized procedure can achieve sublinear dynamic regret on the corresponding class $\\overline{\\mathcal{F}}$. Thus, controlling the increments of $f$ with any $L>0$, in the absence of continuity with respect to $\\boldsymbol{c}$, is not sufficient to get a sub-linear dynamic regret.\n\nFor $\\gamma \\in(0,1]$, the bound (16) combined with Corollary 1 implies that Algorithm 3 with $K$ chosen as in Corollary 1 achieves the optimal rate of regret in $T$ up to logarithmic factors. An additional (mild) gap between the upper and lower bounds is due to the discrepancy in the factors depending on $d, p$, and $L$.\n\nIf $L=0$ the bound in (16) scales as $\\min (T, d \\sqrt{T})$. In this case, the objective functions are constant with respect to the context variable, $f\\left(\\boldsymbol{x}, \\boldsymbol{c}_{1}\\right)=f\\left(\\boldsymbol{x}, \\boldsymbol{c}_{t}\\right)$, for $\\boldsymbol{x} \\in \\mathbb{R}^{d}$ and $t \\in[T]$, so that the dynamic regret in (16) is equal to the static regret. Moreover, by convexity, both regrets are bounded from below by the optimization error corresponding to $f\\left(\\cdot, \\boldsymbol{c}_{1}\\right)$ multiplied by $T$. Using this remark we notice that, for $L=0$, the bound (16) agrees with the lower bound on the optimization error for strongly convex and smooth functions, known to be of the order $d / \\sqrt{T}[3,4,41]$.\n\nTheorem 3 can be stated in a more general manner. In the definition of $\\Pi$, the randomized procedures are restricted to query only within the constraint set $\\Theta$. However, in the proof of the lower bound, we did not impose such a restriction, allowing $\\pi$ to query anywhere within $\\mathbb{R}^{d}$. Thus, Theorem 3 remains valid for this broader set of algorithms. Furthermore, the proof of Theorem 3 can be extended to hold for any convex body $\\Theta$ and not only for $\\Theta=B^{d}$. Indeed, by shifting and scaling the unit Euclidean ball, we can embed it in $\\Theta$. Subsequently, we can apply the same shifting and scaling to the action argument of the set of functions $f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}$ in the proof of Theorem 3.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "# References \n\n[1] Jacob D Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In COLT, pages 263-274. Citeseer, 2008.\n\n[2] Alekh Agarwal, Dean P Foster, Daniel Hsu, Sham M Kakade, and Alexander Rakhlin. Stochastic convex optimization with bandit feedback. SIAM Journal on Optimization, 23(1):213-240, 2013.\n[3] Arya Akhavan, Evgenii Chzhen, Massimiliano Pontil, and Alexandre B Tsybakov. Gradientfree optimization of highly smooth functions: improved analysis and a new algorithm. arXiv preprint arXiv:2306.02159, 2023.\n[4] Arya Akhavan, Massimiliano Pontil, and Alexandre Tsybakov. Exploiting higher order smoothness in derivative-free optimization and continuous bandits. Advances in Neural Information Processing Systems, 33:9017-9027, 2020.\n[5] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397-422, 2002.\n[6] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235-256, 2002.\n[7] Hamsa Bastani and Mohsen Bayati. Online decision making with high-dimensional covariates. Operations Research, 68(1):276-294, 2020.\n[8] Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. In Conference on Learning Theory, pages 240-265. PMLR, 2015.\n[9] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations Research, 63(5):1227-1244, 2015.\n[10] S\u00e9bastien Bubeck, Michael B Cohen, Yin Tat Lee, and Yuanzhi Li. An homotopy method for lp regression provably beyond self-concordance and in input-sparsity time. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1130-1137, 2018.\n[11] S\u00e9bastien Bubeck and Ronen Eldan. The entropic barrier: a simple and optimal universal self-concordant barrier. arXiv preprint arXiv:1412.1587, 2014.\n[12] S\u00e9bastien Bubeck, Yin Tat Lee, and Ronen Eldan. Kernel-based methods for bandit convex optimization. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 72-85, 2017.\n[13] S\u00e9bastien Bubeck, Gilles Stoltz, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Online optimization in x-armed bandits. Advances in Neural Information Processing Systems, 21, 2008.\n[14] Ningyuan Chen and Guillermo Gallego. Nonparametric learning and optimization with covariates. arXiv preprint arXiv:1805.01136, 2018.\n\n[15] Tianyi Chen and Georgios B Giannakis. Bandit convex optimization for scalable and dynamic IoT management. IEEE Internet of Things Journal, 6(1):1276-1286, 2018.\n[16] Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. arXiv preprint cs/040800?, 2004.\n[17] Daniel JF Fox. A Schwarz lemma for K\u00e4hler affine metrics and the canonical potential of a proper convex cone. Annali di Matematica Pura ed Applicata, 194(1):1-42, 2015.\n[18] Alexander Goldenshluger and Assaf Zeevi. Woodroofe's one-armed bandit problem revisited. The Annals of Applied Probability, 19(4):1603-1633, 2009.\n[19] Eric C Hall and Rebecca M Willett. Online convex optimization in dynamic environments. IEEE Journal of Selected Topics in Signal Processing, 9(4):647-662, 2015.\n[20] Elad Hazan and Kfir Levy. Bandit convex optimization: Towards tight bounds. Advances in Neural Information Processing Systems, 27, 2014.\n[21] Elad Hazan and Yuanzhi Li. An optimal algorithm for bandit convex optimization. arXiv preprint arXiv:1603.04350, 2016.\n[22] Roland Hildebrand. Canonical barriers on convex cones. Mathematics of operations research, 39(3):841-850, 2014.\n[23] Shinji Ito. An optimal algorithm for bandit convex optimization with strongly-convex and smooth loss. In International Conference on Artificial Intelligence and Statistics, pages 22292239. PMLR, 2020.\n[24] Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online optimization: Competing with dynamic comparators. In Artificial Intelligence and Statistics, pages 398-406. PMLR, 2015.\n[25] Sham M Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Efficient bandit algorithms for online multiclass prediction. In Proceedings of the 25th international conference on Machine learning, pages 440-447, 2008.\n[26] Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in Neural Information Processing Systems, 17, 2004.\n[27] Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In Proceedings of the fortieth annual ACM symposium on Theory of computing, pages 681690, 2008.\n[28] John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. Advances in neural information processing systems, 20, 2007.\n\n[29] Tor Lattimore. Improved regret for zeroth-order adversarial bandit convex optimisation. Mathematical Statistics and Learning, 2(3):311-334, 2020.\n[30] Tor Lattimore. Bandit convex optimisation. arXiv preprint arXiv:2402.06535, 2024.\n[31] Tor Lattimore and Andr\u00e1s Gy\u00f6rgy. Improved regret for zeroth-order stochastic convex bandits. In Conference on Learning Theory, pages 2938-2964. PMLR, 2021.\n[32] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.\n[33] Wenhao Li, Ningyuan Chen, and L Jeff Hong. A dimension-free algorithm for contextual continuum-armed bandits. arXiv preprint arXiv:1907.06550, 2019.\n[34] Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimization in dynamic environments: Improved regret rates for strongly convex problems. In 2016 IEEE 55th Conference on Decision and Control (CDC), pages 7195-7201. IEEE, 2016.\n[35] Arkadi Nemirovski. Interior point polynomial time methods in convex programming. Lecture notes, 42(16):3215-3224, 2004.\n[36] Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex programming. SIAM, 1994.\n[37] Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213, 2019.\n[38] Sandeep Pandey, Deepak Agarwal, Deepayan Chakrabarti, and Vanja Josifovski. Bandits for taxonomies: A model-based approach. In Proceedings of the 2007 SIAM international conference on data mining, pages 216-227. SIAM, 2007.\n[39] Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. The Annals of Statistics, 41(2):693-721, 2013.\n[40] Philippe Rigollet and Assaf Zeevi. Nonparametric bandits with covariates. arXiv preprint arXiv:1003.1630, 2010.\n[41] Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In Conference on Learning Theory, pages 3-24. PMLR, 2013.\n[42] Aleksandrs Slivkins. Contextual bandits with similarity information. In Proceedings of the 24th annual Conference On Learning Theory, pages 679-702. JMLR Workshop and Conference Proceedings, 2011.\n[43] Alexandre B. Tsybakov. Introduction to nonparametric estimation. Springer Series in Statistics. Springer, New York, 2009.\n\n[44] Yuanyu Wan, Lijun Zhang, and Mingli Song. Improved dynamic regret for online FrankWolfe. In The Thirty Sixth Annual Conference on Learning Theory, pages 3304-3327. PMLR, 2023.\n[45] Chih-Chun Wang, Sanjeev R Kulkarni, and H Vincent Poor. Bandit problems with side observations. IEEE Transactions on Automatic Control, 50(3):338-355, 2005.\n[46] Michael Woodroofe. A one-armed bandit problem with a concomitant variable. Journal of the American Statistical Association, 74(368):799-806, 1979.\n[47] Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: Optimal dynamic regret of online learning with true and noisy gradient. In International Conference on Machine Learning, pages 449-457. PMLR, 2016.\n[48] Yuhong Yang and Dan Zhu. Randomized allocation with nonparametric estimation for a multi-armed bandit problem with covariates. The Annals of Statistics, 30(1):100-121, 2002.\n[49] Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou. Improved dynamic regret for non-degenerate functions. Advances in Neural Information Processing Systems, 30, 2017.\n[50] Lijun Zhang, Tianbao Yang, Zhi-Hua Zhou, et al. Dynamic regret of strongly adaptive methods. In International conference on machine learning, pages 5882-5891. PMLR, 2018.\n[51] Peng Zhao, Guanghui Wang, Lijun Zhang, and Zhi-Hua Zhou. Bandit convex optimization in non-stationary environments. Journal of Machine Learning Research, 22(125):1-45, 2021.\n[52] Peng Zhao and Lijun Zhang. Improved analysis for dynamic regret of strongly convex and smooth functions. In Learning for Dynamics and Control, pages 48-59. PMLR, 2021.\n[53] Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Dynamic regret of convex and smooth functions. Advances in Neural Information Processing Systems, 33:12510-12520, 2020.\n[54] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning, pages 928-936, 2003.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# Supplementary material \n\nIn Appendix A, we present the proof of Theorem 1. Appendices B and C begin with a set of auxiliary and preliminary lemmas, followed by the proofs of the main results: Theorem 2, Corollary 1, and Theorem 3. In Appendix D, we collect some properties of self-concordant barrier functions.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "## A Proof of Theorem 1\n\nProof of Theorem 1. For $i \\in\\left[K^{p}\\right]$, let $N_{i}(T)$ the number of $\\boldsymbol{c}_{t}$ 's belonging to $B_{i}$. Denote $i_{1}, \\ldots, i_{N_{i}(T)}$ as the set of all indices such that $\\boldsymbol{c}_{i_{j}} \\in B_{i}$ for all $j \\in\\left[N_{i}(T)\\right]$. We have\n\n$$\n\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)=\\sum_{i=1}^{K^{p}} \\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{i_{j}}\\right)\\right)\n$$\n\nFor $i \\in\\left[K^{p}\\right]$, let $\\boldsymbol{b}_{i} \\in B_{i}$ be the barycenter of the cube $B_{i}$, and $\\boldsymbol{x}_{i}^{*} \\in \\arg \\min _{\\boldsymbol{x} \\in \\Theta} f\\left(\\boldsymbol{x}, \\boldsymbol{b}_{i}\\right)$. Since (6) holds it follows that\n\n$$\n\\mathbf{E}\\left[\\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)\\right) \\mid N_{i}(T)\\right] \\leq F\\left(N_{i}(T)\\right) F_{1}\\left(N_{i}(T)\\right) \\leq F\\left(N_{i}(T)\\right) F_{1}(T)\n$$\n\nwhere $F$ is a concave function and $F_{1}$ is a non-decreasing function. Note that $\\mathbf{E}\\left[N_{i}(T)\\right]=p_{i} T$, for some $p_{i} \\in[0,1]$, such that $\\sum_{i=1}^{K^{p}} p_{i}=1$. By taking expectations from both sides and applying Jensen's inequality we get\n\n$$\n\\mathbf{E}\\left[\\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)\\right)\\right] \\leq F\\left(p_{i} T\\right) F_{1}(T)\n$$\n\nSumming both sides over $i \\in\\left[K^{p}\\right]$ and using Jensen's inequality again we obtain\n\n$$\n\\begin{aligned}\n\\mathbf{E}\\left[\\sum_{i=1}^{K^{p}} \\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)\\right)\\right] & \\leq \\sum_{i=1}^{K^{p}} F\\left(p_{i} T\\right) F_{1}(T) \\\\\n& \\leq K^{p} F\\left(\\sum_{i=1}^{K^{p}} \\frac{p_{i} T}{K^{p}}\\right) F_{1}(T) \\\\\n& =K^{p} F\\left(\\frac{T}{K^{p}}\\right) F_{1}(T)\n\\end{aligned}\n$$\n\nIf, in addition, $\\pi \\in \\Pi\\left(\\mathcal{F}, \\tau_{1}, \\tau_{2}, \\tau_{3}, \\mathrm{~T}_{0}\\right)$, then for $T \\geq K^{p} \\mathrm{~T}_{0}$ we get that\n\n$$\n\\mathbf{E}\\left[\\sum_{i=1}^{K^{p}} \\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)\\right)\\right] \\leq C d^{\\tau_{1}} T^{\\tau_{2}} \\log ^{\\tau_{3}}(T+1) K^{p\\left(1-\\tau_{2}\\right)}\n$$\n\nLet $\\boldsymbol{z}_{i_{j}}^{*} \\in \\arg \\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{i_{j}}\\right)$. We can write\n\n$$\n\\begin{aligned}\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right]= & \\mathbf{E}\\left[\\sum_{i=1}^{K^{p}} \\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)+f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{z}_{i_{j}}^{*}, \\boldsymbol{c}_{i_{j}}\\right)\\right)\\right] \\\\\n= & \\mathbf{E}\\left[\\sum_{i=1}^{K^{p}} \\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)+f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{b}_{i}\\right)\\right.\\right. \\\\\n& \\left.\\left.+f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{b}_{i}\\right)-f\\left(\\boldsymbol{z}_{i_{j}}^{*}, \\boldsymbol{c}_{i_{j}}\\right)\\right)\\right]\n\\end{aligned}\n$$\n\nSince $f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{b}_{i}\\right) \\leq f\\left(\\boldsymbol{z}_{i_{j}}^{*}, \\boldsymbol{b}_{i}\\right)$, we have\n\n$$\n\\begin{aligned}\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq & \\mathbf{E}\\left[\\sum_{i=1}^{K^{p}} \\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)+f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{b}_{i}\\right)\\right.\\right. \\\\\n& \\left.\\left.+f\\left(\\boldsymbol{z}_{i_{j}}^{*}, \\boldsymbol{b}_{i}\\right)-f\\left(\\boldsymbol{z}_{i_{j}}^{*}, \\boldsymbol{c}_{i_{j}}\\right)\\right)\\right] \\\\\n\\leq & \\mathbf{E}\\left[\\sum_{i=1}^{K^{p}} \\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)+2 L\\left\\|\\boldsymbol{c}_{i_{j}}-\\boldsymbol{b}_{i}\\right\\|^{\\gamma}\\right)\\right]\n\\end{aligned}\n$$\n\nwhere the last inequality is due to Assumption 1. Let $\\delta=\\max _{i \\in[K^{p}]} \\max _{\\boldsymbol{x}, \\boldsymbol{y} \\in B_{i}}\\|\\boldsymbol{x}-\\boldsymbol{y}\\|$. Since all $B_{i}$ 's are hypercubes with edges of length $1 / K$ we have $\\delta=\\sqrt{p} / K$, so that\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq \\mathbf{E}\\left[\\sum_{i=1}^{K^{p}} \\sum_{j=1}^{N_{i}(T)}\\left(f\\left(\\boldsymbol{z}_{i_{j}}, \\boldsymbol{c}_{i_{j}}\\right)-f\\left(\\boldsymbol{x}_{i}^{*}, \\boldsymbol{c}_{i_{j}}\\right)\\right)\\right]+2 T L\\left(\\frac{\\sqrt{p}}{K}\\right)^{\\gamma}\n$$\n\nCombining (17) and (19), gives\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq K^{p} F\\left(\\frac{T}{K^{p}}\\right)+2 T L\\left(\\frac{\\sqrt{p}}{K}\\right)^{\\gamma}\n$$\n\nwhich concludes the proof of (9). In order to prove the bound (10), we note that, by (18) and (19),\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq C d^{\\tau_{1}} T^{\\tau_{2}} \\log ^{\\tau_{3}}(T+1) K^{p\\left(1-\\tau_{2}\\right)}+2 T L\\left(\\frac{\\sqrt{p}}{K}\\right)^{\\gamma}\n$$\n\nThe two terms on the right hand side are of the same order of magnitude if $K=K\\left(\\tau_{1}, \\tau_{2}, \\tau_{3}, \\gamma\\right)$. Under this choice of $K$ we get\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq C^{\\prime} \\max \\left(d^{\\tau_{1}} T^{\\tau_{2}} \\log ^{\\tau_{3}}(T+1)\\right. \\\\\n& \\left.\\quad\\left(L p^{\\frac{3}{2}}\\right)^{\\frac{p\\left(1-\\tau_{2}\\right)}{p\\left(1-\\tau_{2}\\right)+1}}\\left(d^{\\tau_{1}} \\log ^{\\tau_{3}}(T+1)\\right)^{\\frac{\\gamma}{p\\left(1-\\tau_{2}\\right)+1}} T^{\\frac{\\left(p-\\gamma\\right)\\left(1-\\tau_{2}\\right)+\\gamma}{p\\left(1-\\tau_{2}\\right)+1}}\\right),\n\\end{aligned}\n$$\n\nwhere $C^{\\prime}>0$ is a constant that does not depend on $T, p$ and $d$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "# B Proof of Theorem 2 and Corollary 1 \n\nLemma 1. Let $\\left\\{\\eta_{t}\\right\\}_{t=1}^{T}$ be as in Algorithm 2. Then, for all $t \\geq 2$ we have\n\n$$\n1-\\frac{\\eta_{t}}{\\eta_{t-1}} \\leq \\mathbf{1}(t \\geq 16 \\mu) \\sqrt{\\frac{1}{t}}\n$$\n\nProof. Note that $t / \\log (t+1)$ is an increasing function for all $t \\geq 1$. Let $t_{0}$ be the smallest positive integer such that $t_{0} / \\log \\left(t_{0}+1\\right) \\geq 16(\\mu+\\beta / \\alpha)$, and note that\n\n$$\n\\eta_{t}=\\left(4 d q_{T}\\right)^{-1}, \\quad \\text { for } t<t_{0}\n$$\n\nand\n\n$$\n\\eta_{t}=\\left(4 d q_{T}\\right)^{-1}\\left(\\frac{16(\\mu+\\beta / \\alpha) \\log (t+1)}{t}\\right)^{\\frac{1}{2}}, \\quad \\text { for } t \\geq t_{0}\n$$\n\nwhere $q_{T}=M+2 \\sigma \\sqrt{\\log (T+1)}$. If $t<t_{0}$ then $1-\\eta_{t} / \\eta_{t-1}=0$. For the case $t=t_{0}$, we can write\n\n$$\n1-\\frac{\\eta_{t}}{\\eta_{t-1}}=\\frac{\\sqrt{t_{0}}-\\sqrt{16(\\mu+\\beta / \\alpha) \\log \\left(t_{0}+1\\right)}}{\\sqrt{t_{0}}}\n$$\n\nNote that $t_{0} \\leq 16(\\mu+\\beta / \\alpha) \\log \\left(t_{0}+1\\right)+1$. Thus,\n\n$$\n1-\\frac{\\eta_{t}}{\\eta_{t-1}} \\leq \\frac{\\sqrt{t_{0}}-\\sqrt{t_{0}-1}}{\\sqrt{t_{0}}} \\leq \\sqrt{\\frac{1}{t_{0}}}=\\sqrt{\\frac{1}{t}}\n$$\n\nFor $t>t_{0}$ we have\n\n$$\n1-\\frac{\\eta_{t}}{\\eta_{t-1}}=1-\\sqrt{\\frac{t-1}{t}} \\sqrt{\\frac{\\log (t+1)}{\\log (t)}} \\leq \\frac{\\sqrt{t}-\\sqrt{t-1}}{\\sqrt{t}} \\leq \\sqrt{\\frac{1}{t}}\n$$\n\nBy combing the above inequalities we get\n\n$$\n1-\\frac{\\eta_{t}}{\\eta_{t-1}} \\leq \\mathbf{1}\\left(t \\geq t_{0}\\right) \\sqrt{\\frac{1}{t}}\n$$\n\nSince $16(\\mu+\\beta / \\alpha) \\geq 16$, we have $t_{0} \\geq 2$, and consequently $\\log \\left(t_{0}+1\\right) \\geq 1$. Thus,\n\n$$\nt_{0} \\geq 16\\left(\\mu+\\frac{\\beta}{\\alpha}\\right) \\log \\left(t_{0}+1\\right) \\geq 16 \\mu\n$$\n\nTherefore, for all $t \\in[T]$ we have\n\n$$\n\\mathbf{1}\\left(t \\geq t_{0}\\right) \\leq \\mathbf{1}(t \\geq 16 \\mu)\n$$\n\nwhich concludes the proof.\nBefore proceeding with the proofs, let us recall the concept of the Newton decrement of a selfconcordant function. Let $\\Psi: \\operatorname{int}(\\Theta) \\rightarrow \\mathbb{R}$ be a self-concordant function. The Newton decrement of $\\Psi$ at $\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)$ is defined as\n\n$$\n\\lambda(\\Psi, \\boldsymbol{x})=\\max _{\\boldsymbol{h} \\in \\mathbb{R}^{d}}\\left\\{\\nabla \\Psi(\\boldsymbol{x})[\\boldsymbol{h}]\\left\\lvert\\, \\nabla^{2} \\Psi(\\boldsymbol{x})[\\boldsymbol{h}, \\boldsymbol{h}] \\leq 1\\right.\\right\\}\n$$\n\nIn the case that $\\nabla^{2} \\Psi(\\boldsymbol{x})$ is positive definite, for all $\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)$ (21) can be written as\n\n$$\n\\lambda(\\Psi, \\boldsymbol{x})=\\sqrt{(\\nabla \\Psi(\\boldsymbol{x}))^{\\top}\\left(\\nabla^{2} \\Psi(\\boldsymbol{x})\\right)^{-1} \\nabla \\Psi(\\boldsymbol{x})}\n$$\n\nThe proof of the equivalency of (21) and (22) can be found in [35, Chapter 2, Section IVa]. Moreover, if $\\Psi$ is a $\\mu$-self-concordant barrier then $\\lambda(\\Psi, \\boldsymbol{x}) \\leq \\mu^{1 / 2}$ for all $\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)$.\n\nThe next lemma plays a key role in our analysis.\nLemma 2. Let $\\Psi: \\operatorname{int}(\\Theta) \\rightarrow \\mathbb{R}$ be a self-concordant function. Then $\\Psi$ attains its minimum on $\\operatorname{int}(\\Theta)$ if and only if there exists $\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)$ with $\\lambda(\\Psi, \\boldsymbol{x})<1$. Moreover, if $\\lambda(\\Psi, \\boldsymbol{x})<1$, then\n\n$$\n\\left(\\left(\\boldsymbol{x}-\\boldsymbol{x}^{*}\\right)^{\\top} \\nabla^{2} \\Psi(\\boldsymbol{x})\\left(\\boldsymbol{x}-\\boldsymbol{x}^{*}\\right)\\right)^{\\frac{1}{2}} \\leq \\frac{\\lambda(\\Psi, \\boldsymbol{x})}{1-\\lambda(\\Psi, \\boldsymbol{x})}\n$$\n\nwhere $\\boldsymbol{x}^{*}=\\arg \\min _{\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)} \\Psi(\\boldsymbol{x})$.\nFor a proof of Lemma 2, we refer the reader to [35, Chapter 2, Section VIII].\nLemma 3. Define the event\n\n$$\n\\Lambda_{T}=\\left\\{\\left|\\xi_{t}\\right| \\leq 2 \\sigma \\sqrt{\\log (T+1)} \\text { for all } t \\in[T]\\right\\}\n$$\n\nLet $\\left\\{\\boldsymbol{x}_{t}\\right\\}_{t=0}^{T}$ be the iterates defined in Algorithm 2. If $\\Lambda_{T}$ holds, for all $t \\in[T]$ we have\n\n$$\n\\left\\|\\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\|_{t} \\leq 2\\left(\\eta_{t}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}+\\sqrt{\\frac{\\mu}{t}}\\right)\n$$\n\nwhere we use the notation $\\|\\boldsymbol{u}\\|_{t}:=\\sqrt{\\boldsymbol{u}^{\\top} P_{t}^{-2} \\boldsymbol{u}}$ and $\\|\\boldsymbol{u}\\|_{t}^{*}:=\\sqrt{\\boldsymbol{u}^{\\top} P_{t}^{2} \\boldsymbol{u}}$ for all $\\boldsymbol{u} \\in \\mathbb{R}^{d}$.\n\nProof. Let $\\tilde{\\Phi}_{t}: \\operatorname{int}(\\Theta) \\rightarrow \\mathbb{R}$ be such that\n\n$$\n\\tilde{\\Phi}_{t}(\\boldsymbol{u})=\\eta_{t}\\left(\\sum_{k=1}^{t}\\left\\langle\\boldsymbol{g}_{k}, \\boldsymbol{u}\\right\\rangle+\\frac{\\alpha}{2} \\sum_{k=1}^{t}\\left\\|\\boldsymbol{u}-\\boldsymbol{x}_{k-1}\\right\\|^{2}\\right)+\\mathcal{R}(\\boldsymbol{u})\n$$\n\nNote that $\\tilde{\\Phi}_{t}$ is a self-concordant function with $\\boldsymbol{x}_{t} \\in \\arg \\min _{\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)} \\tilde{\\Phi}_{t}(\\boldsymbol{x})$, and $\\nabla^{2} \\tilde{\\Phi}_{t}\\left(\\boldsymbol{x}_{t-1}\\right)=$ $P_{t}^{-2}$. Consider the Newton decrement $\\lambda\\left(\\tilde{\\Phi}_{t}, \\boldsymbol{x}_{t-1}\\right)$, cf. (22). The first part of the proof is dedicated to providing an upper bound for $\\lambda\\left(\\tilde{\\Phi}_{t}, \\boldsymbol{x}_{t-1}\\right)$. Then we use Lemma 2 to prove (23). For $t \\in[T]$ we have\n\n$$\n\\lambda\\left(\\tilde{\\Phi}_{t}, \\boldsymbol{x}_{t-1}\\right)=\\sqrt{\\left(\\nabla \\tilde{\\Phi}_{t}\\left(\\boldsymbol{x}_{t-1}\\right)\\right)^{\\top}\\left(\\nabla^{2} \\tilde{\\Phi}_{t}\\left(\\boldsymbol{x}_{t-1}\\right)\\right)^{-1} \\nabla \\tilde{\\Phi}_{t}\\left(\\boldsymbol{x}_{t-1}\\right)}=\\left\\|\\nabla \\tilde{\\Phi}_{t}\\left(\\boldsymbol{x}_{t-1}\\right)\\right\\|_{t}^{*}\n$$\n\nFor $t=1$, we can write\n\n$$\n\\left\\|\\nabla \\tilde{\\Phi}_{1}\\left(\\boldsymbol{x}_{0}\\right)\\right\\|_{1}^{*}=\\left\\|\\eta_{1}\\left\\langle\\boldsymbol{g}_{1}, \\boldsymbol{x}_{0}\\right\\rangle+\\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{0}\\right)\\right\\| \\leq \\eta_{1}\\|\\boldsymbol{g}\\|_{1}^{*}\n$$\n\nwhere the last inequality follows from the facts that $\\boldsymbol{x}_{0}=\\arg \\min _{\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)} \\mathcal{R}(\\boldsymbol{x})$ and $\\mathcal{R}\\left(\\boldsymbol{x}_{0}\\right)=0$.\nMoreover, for $t \\geq 2$ we have\n\n$$\n\\begin{aligned}\n\\nabla \\tilde{\\Phi}_{t}\\left(\\boldsymbol{x}_{t-1}\\right)= & \\frac{\\eta_{t}}{\\eta_{t-1}}\\left(\\eta_{t-1} \\sum_{k=1}^{t-1} \\boldsymbol{g}_{k}+\\eta_{t-1} \\alpha \\sum_{k=1}^{t-1}\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k-1}\\right)+\\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)\\right)+\\eta_{t} \\boldsymbol{g}_{t} \\\\\n& +\\left(1-\\frac{\\eta_{t}}{\\eta_{t-1}}\\right) \\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right) \\\\\n= & \\frac{\\eta_{t}}{\\eta_{t-1}} \\nabla \\tilde{\\Phi}_{t-1}\\left(\\boldsymbol{x}_{t-1}\\right)+\\eta_{t} \\boldsymbol{g}_{t}+\\left(1-\\frac{\\eta_{t}}{\\eta_{t-1}}\\right) \\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right) \\\\\n= & \\eta_{t} \\boldsymbol{g}_{t}+\\left(1-\\frac{\\eta_{t}}{\\eta_{t-1}}\\right) \\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)\n\\end{aligned}\n$$\n\nUsing (24) - (25) we find that, for all $t \\in[T]$,\n\n$$\n\\lambda\\left(\\tilde{\\Phi}_{t}, \\boldsymbol{x}_{t-1}\\right)=\\left\\|\\nabla \\tilde{\\Phi}_{t}\\left(\\boldsymbol{x}_{t-1}\\right)\\right\\|_{t}^{*} \\leq \\eta_{t}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}+\\left(1-\\frac{\\eta_{t}}{\\eta_{t-1}}\\right)\\left\\|\\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)\\right\\|_{t}^{*}\n$$\n\nThanks to Lemma 1 we can bound the term $1-\\eta_{t} / \\eta_{t-1}$ to obtain\n\n$$\n\\lambda\\left(\\tilde{\\Phi}_{t}, \\boldsymbol{x}_{t-1}\\right) \\leq \\eta_{t}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}+\\left\\|\\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)\\right\\|_{t}^{*} \\mathbf{1}(t \\geq 16 \\mu) \\sqrt{\\frac{1}{t}}\n$$\n\nOn the other hand, since $\\mathcal{R}$ is a $\\mu$-self-concordant function we have\n\n$$\n\\left(\\left\\|\\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)\\right\\|_{t}^{*}\\right)^{2} \\leq \\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)^{\\top}\\left(\\nabla^{2} \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)\\right)^{-1} \\nabla \\mathcal{R}\\left(\\boldsymbol{x}_{t-1}\\right)=\\lambda^{2}\\left(\\mathcal{R}, \\boldsymbol{x}_{t-1}\\right) \\leq \\mu\n$$\n\nso that\n\n$$\n\\lambda\\left(\\tilde{\\Phi}_{t}, \\boldsymbol{x}_{t-1}\\right) \\leq \\eta_{t}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}+\\mathbf{1}(t \\geq 16 \\mu) \\sqrt{\\frac{\\mu}{t}} \\leq \\eta_{t}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}+\\frac{1}{4}\n$$\n\nSince we assumed that $\\Lambda_{T}$ holds, we can write\n\n$$\n\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}=\\left\\|d\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)+\\xi_{t}\\right) P_{t}^{-1} \\boldsymbol{\\zeta}_{t}\\right\\|_{t}^{*} \\leq d\\left(\\left|f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)\\right|+\\left|\\xi_{t}\\right|\\right) \\leq d(M+2 \\sigma \\sqrt{\\log (T+1)})\n$$\n\nBy the definition of $\\eta_{t}$, for all $t \\in[T]$ we have that $\\eta_{t} \\leq(4 d(M+2 \\sigma \\sqrt{\\log (T+1)}))^{-1}$ which implies that $\\eta_{t}\\|\\boldsymbol{g}\\|_{t}^{*} \\leq 1 / 4$. Using this inequality in (27) gives $\\lambda\\left(\\tilde{\\Phi}_{t}, \\boldsymbol{x}_{t-1}\\right) \\leq 1 / 2$. Therefore, by Lemma 2 we can write\n\n$$\n\\left\\|\\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\|_{t}=\\left\\|\\boldsymbol{x}_{t-1}-\\underset{\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)}{\\arg \\min } \\tilde{\\Phi}_{t}(\\boldsymbol{x})\\right\\|_{t} \\leq 2 \\lambda\\left(\\tilde{\\Phi}_{t}, \\boldsymbol{x}_{t-1}\\right)\n$$\n\nThe last display together with (27) yields\n\n$$\n\\left\\|\\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\| \\leq 2\\left(\\eta_{t}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}+\\mathbf{1}(t \\geq 16 \\mu) \\sqrt{\\frac{\\mu}{t}}\\right) \\leq 2\\left(\\eta_{t}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}+\\sqrt{\\frac{\\mu}{t}}\\right)\n$$\n\nLemma 4. Consider a function $h: \\mathbb{R}^{d} \\times[0,1]^{p} \\rightarrow \\mathbb{R}$, and set $h_{t}(\\boldsymbol{x})=h\\left(\\boldsymbol{x}, \\boldsymbol{c}_{t}\\right)$ for $t \\geq 1$. Assume that $\\left\\{h_{t}\\right\\}_{t=1}^{T}$ is a sequence of $\\alpha$-strongly convex functions for some $\\alpha>0$. Let $\\left\\{\\boldsymbol{x}_{t}\\right\\}_{t=0}^{T}$ be the iterates defined in Algorithm 2. Let $\\boldsymbol{g}_{t}$ 's be random vectors in $\\mathbb{R}^{d}$ such that $\\mathbf{E}\\left[\\boldsymbol{g}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}, \\Lambda_{T}\\right]=$ $\\nabla h_{t}\\left(\\boldsymbol{x}_{t-1}\\right)$ for $t \\geq 1$. Then, for any $\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)$ we have\n\n$$\n\\sum_{t=1}^{T} \\mathbf{E}\\left[h_{t}\\left(\\boldsymbol{x}_{t-1}\\right)-h_{t}(\\boldsymbol{x}) \\mid \\Lambda_{T}\\right] \\leq 2 d q_{T}\\left(d q_{T} \\sum_{t=1}^{T} \\eta_{t}+2 \\sqrt{\\mu T}\\right)+\\eta_{T}^{-1} \\mathcal{R}(\\boldsymbol{x})\n$$\n\nwhere $q_{T}=M+2 \\sigma \\sqrt{\\log (T+1)}$.\nProof. Fix $\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)$. Since $h_{t}$ is a strongly convex function, for any $\\boldsymbol{x} \\in \\Theta$ we can write\n\n$$\n\\sum_{t=1}^{T}\\left(h_{t}\\left(\\boldsymbol{x}_{t-1}\\right)-h_{t}(\\boldsymbol{x})\\right) \\leq \\sum_{t=1}^{T}\\left\\langle\\mathbf{E}\\left[\\boldsymbol{g}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}, \\Lambda_{T}\\right], \\boldsymbol{x}_{t-1}-\\boldsymbol{x}\\right\\rangle-\\frac{\\alpha}{2} \\sum_{t=1}^{T}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{t-1}\\right\\|^{2}\n$$\n\nTo simplify the notation, we rewrite $\\boldsymbol{x}_{t}$ as follows\n\n$$\n\\begin{aligned}\n\\boldsymbol{x}_{t} & =\\underset{\\boldsymbol{u} \\in \\Theta}{\\arg \\min } \\sum_{k=1}^{t}\\left\\langle\\boldsymbol{g}_{k}, \\boldsymbol{u}\\right\\rangle+\\underbrace{\\frac{\\alpha}{2} \\sum_{k=1}^{t}\\left\\|\\boldsymbol{u}-\\boldsymbol{x}_{k-1}\\right\\|^{2}+\\eta_{t}^{-1} \\mathcal{R}(\\boldsymbol{u})}_{=: R_{t}(\\boldsymbol{u})} \\\\\n& =\\underset{\\boldsymbol{u} \\in \\Theta}{\\arg \\min } \\underbrace{\\sum_{k=1}^{t}\\left\\langle\\boldsymbol{g}_{k}, \\boldsymbol{u}\\right\\rangle+R_{t}(\\boldsymbol{u})}_{=: \\Phi_{t}(\\boldsymbol{u})}=\\underset{\\boldsymbol{u} \\in \\Theta}{\\arg \\min } \\Phi_{t}(\\boldsymbol{u})\n\\end{aligned}\n$$\n\nMoreover, set $\\Phi_{0}(\\boldsymbol{u})=\\mathcal{R}(\\boldsymbol{u})$ for all $\\boldsymbol{u} \\in \\operatorname{int}(\\Theta)$. Since $\\boldsymbol{x}_{0}=\\arg \\min _{\\boldsymbol{u} \\in \\operatorname{int}(\\Theta)} \\Phi_{0}(\\boldsymbol{u})$ and we assume that $\\mathcal{R}\\left(\\boldsymbol{x}_{0}\\right)=0$, we have\n\n$$\n\\begin{aligned}\n-\\sum_{t=1}^{T}\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}\\right\\rangle & =-\\Phi_{T}(\\boldsymbol{x})+\\Phi_{T}\\left(\\boldsymbol{x}_{T}\\right)-\\Phi_{T}\\left(\\boldsymbol{x}_{T}\\right)+R_{T}(\\boldsymbol{x}) \\\\\n& \\leq \\sum_{t=1}^{T}\\left(\\Phi_{t-1}\\left(\\boldsymbol{x}_{t-1}\\right)-\\Phi_{t}\\left(\\boldsymbol{x}_{t}\\right)\\right)+R_{T}(\\boldsymbol{x})\n\\end{aligned}\n$$\n\nwhere the last display is due to the fact that $\\Phi_{T}\\left(\\boldsymbol{x}_{T}\\right) \\leq \\Phi_{T}(\\boldsymbol{x})$. Subsequently,\n\n$$\n\\sum_{t=1}^{T}\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t-1}-\\boldsymbol{x}\\right\\rangle \\leq \\sum_{t=1}^{T}\\left(\\underbrace{\\Phi_{t-1}\\left(\\boldsymbol{x}_{t-1}\\right)-\\Phi_{t}\\left(\\boldsymbol{x}_{t}\\right)+\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t-1}\\right\\rangle}_{\\text {term I }}\\right)+R_{T}(\\boldsymbol{x})\n$$\n\nIn order to control term I, we first derive an alternative expression for $\\Phi_{t}$. By the definition of $\\Phi_{t}$,\n\n$$\n\\begin{aligned}\n\\Phi_{t}\\left(\\boldsymbol{x}_{t}\\right)=\\sum_{k=1}^{t}\\left\\langle\\boldsymbol{g}_{k}, \\boldsymbol{x}_{t}\\right\\rangle+R_{t}\\left(\\boldsymbol{x}_{t}\\right) & =\\sum_{k=1}^{t-1}\\left\\langle\\boldsymbol{g}_{k}, \\boldsymbol{x}_{t}\\right\\rangle+\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t}\\right\\rangle+R_{t-1}\\left(\\boldsymbol{x}_{t}\\right)+\\left(R_{t}\\left(\\boldsymbol{x}_{t}\\right)-R_{t-1}\\left(\\boldsymbol{x}_{t}\\right)\\right) \\\\\n& =\\Phi_{t-1}\\left(\\boldsymbol{x}_{t}\\right)+\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t}\\right\\rangle+\\left(R_{t}\\left(\\boldsymbol{x}_{t}\\right)-R_{t-1}\\left(\\boldsymbol{x}_{t}\\right)\\right)\n\\end{aligned}\n$$\n\nso that\n\n$$\n\\begin{aligned}\n\\text { term I } & =\\left(\\Phi_{t-1}\\left(\\boldsymbol{x}_{t-1}\\right)+\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t-1}\\right\\rangle\\right)-\\left(\\Phi_{t-1}\\left(\\boldsymbol{x}_{t}\\right)+\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t}\\right\\rangle\\right)+R_{t-1}\\left(\\boldsymbol{x}_{t}\\right)-R_{t}\\left(\\boldsymbol{x}_{t}\\right) \\\\\n& \\leq\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\rangle\n\\end{aligned}\n$$\n\nwhere the last inequality is due to the fact that $R_{t-1}\\left(\\boldsymbol{x}_{t}\\right) \\leq R_{t}\\left(\\boldsymbol{x}_{t}\\right)$, and $\\Phi_{t-1}\\left(\\boldsymbol{x}_{t-1}\\right)=\\min _{\\boldsymbol{z} \\in \\Theta} \\Phi_{t-1}(\\boldsymbol{z}) \\leq$ $\\Phi_{t-1}\\left(\\boldsymbol{x}_{t}\\right)$. From (30) and (31) we obtain\n\n$$\n\\sum_{t=1}^{T}\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t-1}-\\boldsymbol{x}\\right\\rangle \\leq \\sum_{t=1}^{T}\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\rangle+\\frac{\\alpha}{2} \\sum_{t=1}^{T}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{t-1}\\right\\|^{2}+\\eta_{T}^{-1} \\mathcal{R}(\\boldsymbol{x})\n$$\n\nTaking expectations conditioned on the event $\\Lambda_{T}$ gives\n\n$$\n\\begin{aligned}\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t-1}-\\boldsymbol{x}\\right\\rangle \\mid \\Lambda_{T}\\right] \\leq & \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left\\langle\\boldsymbol{g}_{t}, \\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\rangle+\\frac{\\alpha}{2} \\sum_{t=1}^{T}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{t-1}\\right\\|^{2} \\mid \\Lambda_{T}\\right]+\\eta_{T}^{-1} \\mathcal{R}(\\boldsymbol{x}) \\\\\n& \\leq \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}\\left\\|\\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\|_{t}+\\frac{\\alpha}{2} \\sum_{t=1}^{T}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{t-1}\\right\\|^{2} \\mid \\Lambda_{T}\\right]+\\eta_{T}^{-1} \\mathcal{R}(\\boldsymbol{x})\n\\end{aligned}\n$$\n\nFurthermore, by the tower rule,\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left\\langle\\mathbf{E}\\left[\\boldsymbol{g}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}, \\Lambda_{T}\\right], \\boldsymbol{x}_{t-1}-\\boldsymbol{x}\\right\\rangle \\mid \\Lambda_{T}\\right] \\leq \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}\\left\\|\\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\|_{t}+\\frac{\\alpha}{2} \\sum_{t=1}^{T}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{t-1}\\right\\|^{2} \\mid \\Lambda_{T}\\right] \\\\\n& +\\eta_{T}^{-1} \\mathcal{R}(\\boldsymbol{x})\n\\end{aligned}\n$$\n\nUsing the assumption that $\\mathbf{E}\\left[\\boldsymbol{g}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}, \\Lambda_{T}\\right]=\\nabla h_{t}\\left(\\boldsymbol{x}_{t-1}\\right)$, the fact that $\\left\\{h_{t}\\right\\}_{t=1}^{T}$ are $\\alpha$-strongly convex functions, and (29), yields\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(h_{t}\\left(\\boldsymbol{x}_{t-1}\\right)-h_{t}(\\boldsymbol{x})\\right) \\mid \\Lambda_{T}\\right] \\leq \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*} \\mid \\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\|_{t} \\mid \\Lambda_{T}\\right]+\\eta_{T}^{-1} \\mathcal{R}(\\boldsymbol{x})\n$$\n\nWe now apply Lemma 3 to bound the value $\\left\\|\\boldsymbol{x}_{t-1}-\\boldsymbol{x}_{t}\\right\\|_{t}$ on the event $\\Lambda_{T}$. It follows that\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(h_{t}\\left(\\boldsymbol{x}_{t-1}\\right)-h_{t}(\\boldsymbol{x})\\right) \\mid \\Lambda_{T}\\right] \\leq 2\\left(\\sum_{t=1}^{T} \\eta_{t} \\mathbf{E}\\left[\\left(\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*}\\right)^{2} \\mid \\Lambda_{T}\\right]+\\sqrt{\\frac{\\mu}{t}} \\mathbf{E}\\left[\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*} \\mid \\Lambda_{T}\\right]\\right)+\\eta_{T}^{-1} \\mathcal{R}(\\boldsymbol{x})\n$$\n\nRecall that $q_{T}=M+2 \\sigma \\sqrt{\\log (T+1)}$, and under $\\Lambda_{T}$, the uniform bound (28) gives $\\left\\|\\boldsymbol{g}_{t}\\right\\|_{t}^{*} \\leq d q_{T}$. Hence,\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{T} \\mathbf{E}\\left[h_{t}\\left(\\boldsymbol{x}_{t-1}\\right)-h_{t}(\\boldsymbol{x}) \\mid \\Lambda_{T}\\right] & \\leq 2\\left(d^{2} q_{T}^{2} \\sum_{t=1}^{T} \\eta_{t}+d q_{T} \\sum_{t=1}^{T} \\sqrt{\\frac{\\mu}{t}}\\right)+\\eta_{T}^{-1} \\mathcal{R}(\\boldsymbol{x}) \\\\\n& \\leq 2 d q_{T}\\left(d q_{T} \\sum_{t=1}^{T} \\eta_{t}+2 \\sqrt{\\mu T}\\right)+\\eta_{T}^{-1} \\mathcal{R}(\\boldsymbol{x})\n\\end{aligned}\n$$\n\nLemma 5. [20, Corollary 6] Let $A \\in \\mathbb{R}^{d \\times d}$ be an invertible matrix. Let $\\boldsymbol{U}$ be distributed uniformly on $B^{d}$. Let $\\mathrm{f}_{A}(\\boldsymbol{x}, \\boldsymbol{c}) \\triangleq \\mathbf{E}[f(\\boldsymbol{x}+A \\boldsymbol{U}, \\boldsymbol{c})]$ for $\\boldsymbol{x} \\in \\mathbb{R}^{d}$ and $\\boldsymbol{c} \\in[0,1]^{p}$. Then,\n\n$$\n\\nabla_{\\boldsymbol{x}} \\mathrm{f}_{A}(\\boldsymbol{x}, \\boldsymbol{c})=\\mathbf{E}\\left[d f(\\boldsymbol{x}+A \\boldsymbol{\\zeta}, \\boldsymbol{c}) A^{-1} \\boldsymbol{\\zeta}\\right]\n$$\n\nwhere $\\boldsymbol{\\zeta}$ is uniformly distributed on $\\partial B^{d}$. If Assumption 2(i) holds, then $\\mathrm{f}_{A}(\\cdot, \\boldsymbol{c})$ is $\\alpha$-strongly convex on $\\mathbb{R}^{d}$ for all $\\boldsymbol{c} \\in[0,1]^{p}$.\n\nLemma 6. Let Assumption 3(ii) hold. Let $\\boldsymbol{U}$ be distributed uniformly on $B^{d}$. For $t \\geq 1$, let $\\mathrm{f}_{t, P}: \\mathbb{R}^{d} \\times[0,1]^{p} \\rightarrow \\mathbb{R}$ be a surrogate function defined by the relation\n\n$$\n\\mathrm{f}_{t, P}(\\boldsymbol{x}, \\boldsymbol{c})=\\mathbf{E}\\left[f\\left(\\boldsymbol{x}+P_{t} \\boldsymbol{U}, \\boldsymbol{c}\\right) \\mid \\boldsymbol{x}_{t-1}\\right]\n$$\n\nLet $\\left\\{\\boldsymbol{g}_{t}\\right\\}_{t=1}^{T}$ and $\\left\\{\\boldsymbol{x}_{t}\\right\\}_{t=0}^{T-1}$ be defined as in Algorithm 2, and $\\Lambda$ be any event that only depends on $\\left\\{\\xi_{t}\\right\\}_{t=1}^{T}$. Then for all $t \\in[T]$ we have\n\n$$\n\\mathbf{E}\\left[\\boldsymbol{g}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}, \\Lambda\\right]=\\nabla_{\\boldsymbol{x}} \\mathrm{f}_{t, P}\\left(\\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right)\n$$\n\nProof. By the definition of $\\boldsymbol{g}_{t}$ we have\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left[\\boldsymbol{g}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}, \\Lambda\\right]=d \\mathbf{E}\\left[\\left(f\\left(\\boldsymbol{x}_{t-1}+P_{t} \\boldsymbol{\\zeta}_{t}, \\boldsymbol{c}_{t}\\right)+\\xi_{t}\\right) P_{t}^{-1} \\boldsymbol{\\zeta}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}, \\Lambda\\right] \\\\\n& \\quad=d \\mathbf{E}\\left[f\\left(\\boldsymbol{x}_{t-1}+P_{t} \\boldsymbol{\\zeta}_{t}, \\boldsymbol{c}_{t}\\right) P_{t}^{-1} \\boldsymbol{\\zeta}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right]+\\underbrace{d \\mathbf{E}\\left[\\xi_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}, \\Lambda\\right] P_{t}^{-1} \\mathbf{E}\\left[\\boldsymbol{\\zeta}_{t}\\right]}_{=0}\n\\end{aligned}\n$$\n\nwhere the last equality is due to Assumption 3(ii). Now, by Lemma 5 we have\n\n$$\nd \\mathbf{E}\\left[f\\left(\\boldsymbol{x}_{t-1}+P_{t} \\boldsymbol{\\zeta}_{t}, \\boldsymbol{c}_{t}\\right) P_{t}^{-1} \\boldsymbol{\\zeta}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right]=\\nabla_{\\boldsymbol{x}} \\mathrm{f}_{t, P}\\left(\\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right)\n$$\n\nwhich concludes the proof.\nLemma 7. [20, Lemma 4] For $\\mu>0$ let $\\mathcal{R}: \\operatorname{int}(\\Theta) \\rightarrow \\mathcal{R}$ be a $\\mu$-self-concordant barrier with $\\min _{\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)} \\mathcal{R}(\\boldsymbol{x})=0$. Then, for all $\\boldsymbol{x}, \\boldsymbol{y} \\in \\operatorname{int}(\\Theta)$ we have\n\n$$\n\\mathcal{R}(\\boldsymbol{x})-\\mathcal{R}(\\boldsymbol{y}) \\leq \\mu \\log \\left(\\frac{1}{1-\\pi_{x}(\\boldsymbol{y})}\\right)\n$$\n\nwhere $\\pi_{x}(\\boldsymbol{y})=\\inf \\left\\{t \\geq 0: \\boldsymbol{x}+t^{-1}(\\boldsymbol{y}-\\boldsymbol{x})\\right\\}$ is the Minkowsky function. Particularly, for $\\boldsymbol{x} \\in \\operatorname{int}(\\Theta)$ if $\\boldsymbol{x}_{0} \\in \\arg \\min _{\\boldsymbol{u} \\in \\operatorname{int}(\\Theta)} \\mathcal{R}(\\boldsymbol{u})$, and $\\boldsymbol{x}^{\\prime}=(1-1 / T) \\boldsymbol{x}+\\boldsymbol{x}_{0}$ then\n\n$$\n\\mathcal{R}\\left(\\boldsymbol{x}^{\\prime}\\right) \\leq \\mu \\log (T)\n$$\n\nProof of Theorem 2. Let $\\boldsymbol{z}^{\\prime}=(1-1 / T) \\boldsymbol{z}+(1 / T) \\boldsymbol{x}_{0}$. Note that\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right] & =\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right)\\right]+\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right] \\\\\n& \\leq \\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right)\\right]+\\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{x}_{0}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right] \\\\\n& \\leq \\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right)\\right]+2 M\n\\end{aligned}\n$$\n\nwhere (36) is due to convexity of $f(\\cdot, \\boldsymbol{c})$. Invoking the event $\\Lambda_{T}$ we can write\n\n$$\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right)\\right]=\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}^{c}\\right] \\mathbf{P}\\left[\\Lambda_{T}^{c}\\right]+\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right] \\mathbf{P}\\left[\\Lambda_{T}\\right]\n$$\n\nSince $\\left\\{\\xi_{t}\\right\\}_{t=1}^{T}$ are $\\sigma$-sub-Gaussian we have\n\n$$\n\\mathbf{P}\\left[\\Lambda_{T}^{c}\\right] \\leq \\sum_{t=1}^{T} \\mathbf{P}\\left[\\left|\\xi_{t}\\right|>2 \\sigma \\sqrt{\\log (T+1)}\\right] \\leq 2 \\sum_{t=2}^{T+1} T^{-2}=2 T^{-1}\n$$\n\nThus,\n\n$$\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right)\\right] \\leq 4 M+\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right]\n$$\n\nNext, introducing the surrogate function $\\mathrm{f}_{t, P}(\\boldsymbol{x}, \\boldsymbol{c})=\\mathbf{E}\\left[f\\left(\\boldsymbol{x}+P_{t} \\boldsymbol{U}, \\boldsymbol{c}\\right) \\mid \\boldsymbol{x}_{t-1}\\right]$ we consider the decomposition\n\n$$\n\\begin{aligned}\n& \\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right] \\\\\n& =\\underbrace{\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right]}_{\\text {term I }}+\\underbrace{\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right)-\\mathrm{f}_{t, P}\\left(\\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right]}_{\\text {term II }}} \\\\\n& +\\underbrace{\\sum_{t=1}^{T} \\mathbf{E}\\left[\\mathrm{f}_{t, P}\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right]}_{\\text {term III }}+\\underbrace{\\sum_{t=1}^{T} \\mathbf{E}\\left[\\mathrm{f}_{t, P}\\left(\\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right)-\\mathrm{f}_{t, P}\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right]}_{\\text {term IV }} .\n\\end{aligned}\n$$\n\nIn what follows, we bound each term in this decomposition separately. For term I, by Assumption 2(ii) we have\n\n$$\n\\begin{aligned}\n\\operatorname{term} \\mathrm{I}=\\mathbf{E}\\left[\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right) \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right] \\mid \\Lambda_{T}\\right] & \\leq \\frac{\\beta}{2} \\sum_{t=1}^{T} \\mathbf{E}\\left[\\left\\|P_{t} \\boldsymbol{\\zeta}_{t}\\right\\|^{2}\\right] \\\\\n& \\leq \\frac{\\beta}{2} \\sum_{t=1}^{T} \\mathbf{E}\\left[\\left\\|P_{t}\\right\\|_{\\mathrm{op}}^{2}\\right] \\leq \\frac{\\beta}{2 \\alpha} \\sum_{t=1}^{T} \\frac{1}{\\eta_{t} t}\n\\end{aligned}\n$$\n\nwhere $\\|\\cdot\\|_{\\text {op }}$ denotes the operator norm. By the fact that $f\\left(\\cdot, \\boldsymbol{c}_{t}\\right)$ is a convex function and by Jensen's inequality, we deduce that term II is negative. For term III, by Assumption 2(ii) we have\n\n$$\n\\text { term III } \\leq \\frac{\\beta}{2} \\sum_{i=1}^{T} \\mathbf{E}\\left[\\left\\|P_{t} \\boldsymbol{U}\\right\\|^{2}\\right] \\leq \\frac{\\beta}{2} \\sum_{t=1}^{T} \\mathbf{E}\\left[\\left\\|P_{t}\\right\\|_{\\mathrm{op}}^{2}\\right] \\leq \\frac{\\beta}{2 \\alpha} \\sum_{t=1}^{T} \\frac{1}{\\eta_{t} t}\n$$\n\nTo control term IV, first note that by Lemma $5\\left\\{\\mathrm{f}_{t, P}\\left(\\cdot, \\boldsymbol{c}_{t}\\right)\\right\\}_{t=1}^{T}$ are $\\alpha$-strongly convex functions. Furthermore, by Lemma 6 we have that $\\mathbf{E}\\left[\\boldsymbol{g}_{t} \\mid \\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}, \\Lambda_{T}\\right]=\\nabla \\mathrm{f}_{t, P}\\left(\\boldsymbol{x}_{t-1}, \\boldsymbol{c}_{t}\\right)$. Therefore, by Lemma 4 we obtain\n\n$$\n\\text { term IV } \\leq 2 d^{2} q_{T}^{2} \\sum_{t=1}^{T} \\eta_{t}+4 d q_{T} \\sqrt{\\mu T}+\\eta_{T}^{-1} \\mathcal{R}\\left(\\boldsymbol{z}^{\\prime}\\right)\n$$\n\nwhere $q_{T}=M+2 \\sigma \\sqrt{\\log (T+1)}$ and $\\boldsymbol{z}^{\\prime}=(1-1 / T) \\boldsymbol{z}+(1 / T) \\boldsymbol{x}_{0}$. By Lemma 7 we have $\\mathcal{R}\\left(\\boldsymbol{z}^{\\prime}\\right) \\leq \\mu \\log (T)$, so that\n\n$$\n\\text { term IV } \\leq 2 d^{2} q_{T}^{2} \\sum_{t=1}^{T} \\eta_{t}+4 d q_{T} \\sqrt{\\mu T}+\\eta_{T}^{-1} \\mu \\log (T)\n$$\n\nThus,\n\n$$\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right] \\leq 2 d^{2} q_{T}^{2} \\sum_{t=1}^{T} \\eta_{t}+4 d q_{T} \\sqrt{\\mu T}+\\eta_{T}^{-1} \\mu \\log (T)+\\frac{\\beta}{\\alpha} \\sum_{t=1}^{T} \\frac{1}{\\eta_{t} t}\n$$\n\nSince $\\eta_{t}=\\left(4 d q_{T}\\right)^{-1} \\min \\left(1, \\frac{\\nu \\log (t+1)}{t}\\right)^{\\frac{1}{2}}$ with $\\nu=16(\\mu+\\beta / \\alpha)$ we get\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right] & \\leq d q_{T} \\sqrt{\\nu \\log (T+1) T}+4 d q_{T} \\sqrt{\\mu T}+ \\\\\n& +4 d q_{T} \\mu \\max \\left(\\log ^{2}(T+1), \\frac{T \\log (T+1)}{\\nu}\\right)^{\\frac{1}{2}}+\\frac{\\beta}{\\alpha} \\sum_{t=1}^{T} \\frac{1}{\\eta_{t} t}\n\\end{aligned}\n$$\n\nUsing the fact that $16 \\mu \\leq \\nu$, and $\\max \\{a, b\\} \\leq a+b$ for $a, b>0$, we further simplify the above bound to obtain\n\n$$\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right] \\leq \\frac{9}{4} d q_{T} \\sqrt{\\nu \\log (T+1) T}+4 d q_{T} \\mu \\log (T+1)+\\frac{\\beta}{\\alpha} \\sum_{t=1}^{T} \\frac{1}{\\eta_{t} t}\n$$\n\nSince $t / \\log (t+1) \\geq \\sqrt{t}$ for all $t \\in[T]$, then if $t \\geq \\nu^{2}$ we have $t \\geq \\nu \\log (t+1)$. Thus,\n\n$$\n\\begin{aligned}\n\\frac{\\beta}{\\alpha} \\sum_{t=1}^{T} \\frac{1}{\\eta_{t} t} & \\leq \\frac{4 \\beta}{\\alpha} d q_{T}\\left(\\sum_{t=1}^{\\nu^{2}} \\frac{1}{t}+\\sum_{t=2}^{T} \\sqrt{\\frac{1}{\\nu t \\log (t+1)}}\\right) \\\\\n& \\leq 4 d q_{T}\\left(\\frac{\\beta}{\\alpha} \\log \\left(\\nu^{2}+1\\right)+\\frac{2 \\beta}{\\alpha} \\sqrt{\\frac{T}{\\nu}}\\right) \\leq 4 d q_{T}\\left(\\frac{\\beta}{\\alpha} \\log \\left(\\nu^{2}+1\\right)+\\frac{1}{8} \\sqrt{\\nu T}\\right)\n\\end{aligned}\n$$\n\nwhere the last inequality is due to the fact that $16 \\beta / \\alpha \\leq \\nu$. Thus,\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right] & \\leq d q_{T}\\left(\\frac{11}{4} \\sqrt{\\nu \\log (T+1) T}+4\\left(\\mu \\log (T+1)+\\frac{\\beta}{\\alpha} \\log \\left(\\nu^{2}+1\\right)\\right)\\right) \\\\\n& \\leq \\frac{d q_{T}}{4}\\left(11 \\sqrt{\\nu \\log (T+1) T}+\\nu\\left(\\log (T+1)+\\log \\left(\\nu^{2}+1\\right)\\right)\\right)\n\\end{aligned}\n$$\n\nSince $q_{T}=M+2 \\sigma \\sqrt{\\log (T+1)}$ we have\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}^{\\prime}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right] \\leq & \\frac{d}{4}(M+2 \\sigma \\sqrt{\\log (T+1)})\\left(11 \\sqrt{\\nu T \\log (T+1)}\\right. \\\\\n& \\left.+\\nu(\\log (T+1)+2 \\log (\\nu+1))\\right)\n\\end{aligned}\n$$\n\nCombining (37), (38), and (39) implies\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right] \\leq & \\frac{d}{4}(M+2 \\sigma \\sqrt{\\log (T+1)})(11 \\sqrt{\\nu T \\log (T+1)} \\\\\n& \\left.+\\nu(\\log (T+1)+2 \\log (\\nu+1))\\right)+6 M\n\\end{aligned}\n$$\n\nNote that since $\\nu \\geq 16$ and $2 \\log (\\nu+1) \\geq 5.5$, we have\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right) \\mid \\Lambda_{T}\\right] \\leq & \\frac{d}{4}(5.3 M+2 \\sigma \\sqrt{\\log (T+1)})(11 \\sqrt{\\nu T \\log (T+1)} \\\\\n& \\left.+\\nu(\\log (T+1)+2 \\log (\\nu+1))\\right)\n\\end{aligned}\n$$\n\nThus, we can write\n\n$$\n\\sum_{t=1}^{T} \\mathbf{E}\\left[f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right] \\leq F(T) F_{1}(T)\n$$\n\nwhere\n\n$$\nF(T)=11 \\sqrt{\\nu T \\log (T+1)}+\\nu(\\log (T+1)+2 \\log (\\nu+1))\n$$\n\nand\n\n$$\nF_{1}(T)=\\frac{d}{4}(5.3 M+2 \\sigma \\sqrt{\\log (T+1)})\n$$\n\nNoticing that $F$ is a concave and $F_{1}$ is a non-decreasing function we complete the first part of the proof. For the second part of the proof, assume that $T \\geq 16(\\mu+\\beta / \\alpha)^{2}$. In this case we have $T \\geq \\nu$ since $\\beta / \\alpha \\geq 1$. Therefore,\n\n$$\nF(T) \\leq 11 \\sqrt{\\nu T \\log (T+1)}+3 \\nu \\log (T+1)\n$$\n\nOn the other hand, the condition $T \\geq 16(\\mu+\\beta / \\alpha)^{2}$ implies that\n\n$$\n\\sqrt{\\nu T \\log (T+1)} \\geq \\frac{\\nu}{2} T^{\\frac{1}{4}} \\sqrt{\\log (T+1)} \\geq \\frac{\\nu}{2} \\log (T+1)\n$$\n\nand therefore,\n\n$$\nF(T) \\leq 17 \\sqrt{\\nu T \\log (T+1)}=68 \\sqrt{\\left(\\mu+\\frac{\\beta}{\\alpha}\\right) T \\log (T+1)}\n$$\n\nProof of Corollary 1. Since $F$ in (12) is a concave function we can apply (9) to obtain\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq K^{p} F\\left(\\frac{T}{K^{p}}\\right) F_{1}(T)+2 L T\\left(\\frac{\\sqrt{p}}{K}\\right)^{\\gamma}\n$$\n\nUsing the definition of $F$ and the inequality $\\mu \\leq \\lambda d^{\\rho}$, we get\n\n$$\n\\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq C_{*}\\left(L_{1} d^{1+\\rho} K^{p}+L_{2} \\sqrt{K^{p} d^{2+\\rho} T}+L T\\left(\\frac{\\sqrt{p}}{K}\\right)^{\\gamma}\\right)\n$$\n\nwhere $L_{1}=\\sqrt{\\log (T+1)} \\log (T d+1), L_{2}=\\log (T+1)$, and $C_{*}>0$ is a constant depending only on $M, \\sigma, \\beta / \\alpha$.\n\nConsider now two cases. First, let $L p^{\\frac{\\alpha}{2}} d^{-(1+\\rho / 2)} T^{1 / 2} \\log ^{-1}(T+1)>1$. In this case, since $x / 2 \\leq\\lfloor x\\rfloor \\leq x$ for $x>1$, we obtain that\n\n$$\nK=K(1+\\rho / 2,1 / 2,1, \\gamma) \\asymp\\left(L p^{\\frac{\\alpha}{2}} d^{-(1+\\rho / 2)} T^{1 / 2} \\log ^{-1}(T+1)\\right)^{\\frac{\\alpha}{p+2 \\gamma}}\n$$\n\nso that, to within a constant depending only on $M, \\sigma, \\beta / \\alpha$, the right hand side of (40) is of the order\n\n$$\nL_{1} d^{1+\\rho}\\left(L p^{\\frac{\\alpha}{2}} d^{-(1+\\rho / 2)} L_{2}^{-1}\\right)^{\\frac{2 p}{p+2 \\gamma}} T^{\\frac{p}{p+2 \\gamma}}+L_{2}^{\\frac{2 \\gamma}{p+2 \\gamma}}\\left(L p^{\\frac{\\alpha}{2}}\\right)^{\\frac{p}{p+2 \\gamma}} d^{\\frac{(2+\\rho) \\gamma}{p+2 \\gamma}} T^{\\frac{p+\\gamma}{p+2 \\gamma}}\n$$\n\nIn the second case, $L p^{\\frac{\\alpha}{2}} d^{-(1+\\rho / 2)} T^{1 / 2} \\log ^{-1}(T+1) \\leq 1$. Then $K(1+\\rho / 2,1 / 2,1, \\gamma)=1$, so that the right hand side of (40) does not exceed\n\n$$\nC_{*}\\left(L_{1} d^{1+\\rho}+L_{2} \\sqrt{d^{2+\\rho} T}+\\left(L p^{\\frac{\\alpha}{2}} T^{1 / 2}\\right) \\sqrt{T}\\right) \\leq C_{*}\\left(L_{1} d^{1+\\rho}+\\left(L_{2}+1\\right) \\sqrt{d^{2+\\rho} T}\\right)\n$$\n\nPutting these remarks together we get\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left[\\sum_{t=1}^{T}\\left(f\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{z} \\in \\Theta} f\\left(\\boldsymbol{z}, \\boldsymbol{c}_{t}\\right)\\right)\\right] \\leq C \\max \\left\\{L_{1} d^{1+\\rho}+L_{2} \\sqrt{d^{2+\\rho} T}\\right. \\\\\n& \\left.L_{1}\\left(L p^{\\frac{\\alpha}{2}} L_{2}^{-1}\\right)^{\\frac{2 p}{p+2 \\gamma}} d^{\\frac{2 \\gamma(1+\\rho)-p}{p+2 \\gamma}} T^{\\frac{p}{p+2 \\gamma}}, L_{2}^{\\frac{2 \\gamma}{p+2 \\gamma}}\\left(L p^{\\frac{\\alpha}{2}}\\right)^{\\frac{p}{p+2 \\gamma}} d^{\\frac{(2+\\rho) \\gamma}{p+2 \\gamma}} T^{\\frac{p+\\gamma}{p+2 \\gamma}}\\right\\}\n\\end{aligned}\n$$\n\nwhere $C>0$ is a constant depending only on $M, \\sigma, \\beta / \\alpha$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# C Proof of Theorem 3 \n\nLemma 8. Suppose that $\\mathbf{P}_{1}$ and $\\mathbf{P}_{2}$ are two probability measures that are defined on a probability space $(X, \\mathcal{M})$, such that $\\mathbf{P}_{1}$ is absolutely continuous with respect to $\\mathbf{P}_{2}$. Let $\\Upsilon \\in \\mathcal{M}$ be such that $\\mathbf{P}_{1}(\\Upsilon)=\\mathbf{P}_{2}(\\Upsilon) \\neq 0$. Let $\\tilde{\\mathbf{P}}_{1}$ and $\\tilde{\\mathbf{P}}_{2}$ be the conditional probability measures of $\\mathbf{P}_{1}$ and $\\mathbf{P}_{2}$ given the event $\\Upsilon$, respectively, i.e., for $i=1,2$ and $\\Gamma \\in \\mathcal{M}$\n\n$$\n\\tilde{\\mathbf{P}}_{i}(\\Gamma)=\\mathbf{P}_{i}(\\Gamma \\cap \\Upsilon) \\mathbf{P}_{i}^{-1}(\\Upsilon)\n$$\n\nThen,\n\n1. For $i=1,2$\n\n$$\n\\frac{\\mathrm{d} \\tilde{\\mathbf{P}}_{i}(\\boldsymbol{x})}{\\mathrm{d} \\mathbf{P}_{i}(\\boldsymbol{x})}=\\mathbb{1}(\\boldsymbol{x} \\in \\Upsilon) \\mathbf{P}_{i}^{-1}(\\Upsilon)\n$$\n\n2. $K L\\left(\\tilde{\\mathbf{P}}_{1}, \\tilde{\\mathbf{P}}_{2}\\right)=\\int \\log \\left(\\frac{\\mathrm{d} \\mathbf{P}_{1}(\\boldsymbol{x})}{\\mathrm{d} \\mathbf{P}_{2}(\\boldsymbol{x})}\\right) \\mathbb{1}(\\boldsymbol{x} \\in \\Upsilon) \\mathbf{P}_{1}^{-1}(\\Upsilon) \\mathrm{d} \\mathbf{P}_{1}(\\boldsymbol{x})$.\n\nProof of Theorem 3. We choose a finite set of functions in a way that its members cannot be distinguished from each other with positive probability but are separated enough from each other to guarantee that the maximal regret for this family is not smaller than the desired lower bound.\n\nSet $K=\\max \\left(1,\\left\\lfloor\\left(\\min \\left(1, L^{2}\\right) T\\right)^{\\frac{1}{p+2 \\gamma}}\\right\\rfloor\\right)$. Let $\\left(B_{i}\\right)_{i=1}^{K^{p}}$ be a partition of $[0,1]^{p}$, where $B_{i}$ 's are disjoint cubes with edges of length $1 / K$. Then, for any $\\boldsymbol{c} \\in[0,1]^{p}$ there exists $i \\in\\left\\{1, \\ldots, K^{p}\\right\\}$ that satisfies $d\\left(\\boldsymbol{c}, \\partial B_{i}\\right) \\leq \\delta$, where $\\partial B_{i}$ is the boundary of $B_{i}, d\\left(\\boldsymbol{c}, \\partial B_{i}\\right)=\\min _{\\boldsymbol{u} \\in \\partial B_{i}}\\|\\boldsymbol{c}-\\boldsymbol{u}\\|$ and $\\delta=\\frac{1}{2 K}$, cf. (45).\n\nLet $\\eta_{0}: \\mathbb{R} \\rightarrow \\mathbb{R}$ be an infinitely many times differentiable function such that\n\n$$\n\\eta_{0}(x)= \\begin{cases}=1 & \\text { if }|x| \\leq 1 / 4 \\\\ \\in(0,1) & \\text { if } 1 / 4<|x|<1 \\\\ =0 & \\text { if }|x| \\geq 1\\end{cases}\n$$\n\nSet $\\eta(x)=\\int_{-\\infty}^{x} \\eta_{0}(u) \\mathrm{d} u$. Let $\\Omega=\\{-1,1\\}^{K^{p}}$ and $\\{-1,1\\}^{d-1}$ be the sets of binary sequences of length $K^{p}$ and $d-1$, respectively. Consider the finite set of functions $f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}: \\mathbb{R}^{d} \\times[0,1]^{p} \\rightarrow \\mathbb{R}$, $\\boldsymbol{\\omega} \\in \\Omega, \\boldsymbol{\\tau} \\in\\{-1,1\\}^{d-1}$ such that:\n\n$$\n\\begin{aligned}\nf_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}(\\boldsymbol{x}, \\boldsymbol{c})=\\alpha\\|\\boldsymbol{x}\\|^{2}+r_{1} L \\eta\\left(x_{1} \\delta^{-\\frac{\\gamma}{2}}\\right) & \\left(\\sum_{j=1}^{K^{p}} \\omega_{j} d^{\\gamma}\\left(\\boldsymbol{c}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\boldsymbol{c} \\in B_{j}\\right)\\right) \\\\\n& +r_{2} h^{2}\\left(\\sum_{i=2}^{d} \\tau_{i} \\eta\\left(\\frac{x_{i}}{h}\\right)\\right), \\quad \\boldsymbol{x}=\\left(x_{1}, \\ldots, x_{d}\\right)\n\\end{aligned}\n$$\n\nwhere $\\omega_{j}, \\tau_{i} \\in\\{-1,1\\}, h=\\min \\left(d^{-\\frac{1}{2}}, T^{-\\frac{1}{4}}\\right)$ and $r_{1}, r_{2}$ are fixed positive numbers that will be chosen small enough. By Lemma 10 we have that $f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}} \\in \\mathcal{F}_{\\alpha, \\beta}(M) \\cap \\mathcal{F}_{\\gamma}(L)$ provided $r_{1} \\leq$ $\\min \\left(1,1 / L^{2}\\right) \\min \\left(1 / 2 \\eta(1), \\alpha / L^{\\prime}\\right), r_{2} \\leq \\min \\left(1 / 2 \\eta(1), \\alpha / L^{\\prime}\\right)$, where $L^{\\prime}=\\max _{x \\in \\mathbb{R}}\\left|\\eta^{\\prime \\prime}(x)\\right|$. Furthermore, by choosing $r_{1} \\leq \\alpha /(2 \\max (1, L))$ and $r_{2} \\leq \\alpha / 2$ we have\n\n$$\n\\alpha^{-1} \\delta^{-\\gamma} r_{1} L \\sum_{j=1}^{K^{p}} \\omega_{j} d^{\\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\boldsymbol{c} \\in B_{j}\\right) \\leq \\frac{1}{2}, \\quad \\alpha^{-1} r_{2} \\leq \\frac{1}{2}\n$$\n\nand that under this condition the equation $\\nabla_{\\boldsymbol{x}} f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}(\\boldsymbol{x}, \\boldsymbol{c})=0$ has the solution\n\n$$\n\\boldsymbol{x}^{*}(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c})=\\left(x_{1}^{*}(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}), \\ldots, x_{d}^{*}(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c})\\right)\n$$\n\nwhere $x_{1}^{*}(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c})=-(1 / 2) \\alpha^{-1} \\delta^{-\\frac{\\gamma}{2}} r_{1} L \\sum_{j=1}^{K^{p}} \\omega_{j} d^{\\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\boldsymbol{c} \\in B_{j}\\right)$ and $x_{i}^{*}(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c})=-(1 / 2) \\tau_{i} r_{2} \\alpha^{-1} h$, for $2 \\leq i \\leq d$. Furthermore,\n\n$$\n\\left\\|\\boldsymbol{x}^{*}(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c})\\right\\|^{2} \\leq \\frac{1}{4 \\alpha^{2}}\\left(r_{1}^{2} L^{2} \\delta^{\\gamma}+d r_{2}^{2} h^{2}\\right) \\leq \\frac{1}{8}\n$$\n\nwhich implies $\\boldsymbol{x}^{*}(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}) \\in \\Theta$.\nFor any fixed $\\boldsymbol{\\omega} \\in \\Omega, \\boldsymbol{\\tau} \\in\\{-1,1\\}^{d-1}$, and $1 \\leq t \\leq T$, we denote by $\\mathbf{P}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}$ the probability measure corresponding to the joint distribution of $\\left(\\boldsymbol{z}_{1}, y_{1}^{t}, \\boldsymbol{c}_{1}^{t}\\right)$ where $y_{k}=f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{z}_{k}, \\boldsymbol{c}_{k}\\right)+\\xi_{k}$ with independent identically distributed $\\xi_{k}$ 's such that (14) holds and $\\boldsymbol{z}_{k}$ 's chosen by the randomized procedure $\\pi$. We have\n\n$$\n\\mathrm{d} \\mathbf{P}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left(\\boldsymbol{z}_{1}, y_{1}^{t}, \\boldsymbol{c}_{1}^{t}\\right)=\\mathrm{d} F\\left(y_{1}-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{z}_{1}, \\boldsymbol{c}_{1}\\right)\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{1}\\right) \\prod_{i=2}^{t} \\mathrm{~d} F\\left(y_{i}-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\Phi_{i}\\left(\\boldsymbol{z}_{1}, y_{1}^{i-1}, \\boldsymbol{c}_{1}^{i}\\right), \\boldsymbol{c}_{i}\\right)\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{i}\\right)\n$$\n\nWithout loss of generality, we omit here the dependence of $\\Phi_{i}$ on $\\boldsymbol{z}_{2}, \\ldots, \\boldsymbol{z}_{i-1}$ since $\\boldsymbol{z}_{i}, i \\geq 2$, is a Borel function of $\\boldsymbol{z}_{1}, y_{1}, \\ldots, y_{i-1}, \\boldsymbol{c}_{1}, \\ldots, \\boldsymbol{c}_{i}$. Let $\\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}$ denote the expectation w.r.t. $\\mathbf{P}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}$.\n\nNote that\n\n$$\n\\sum_{t=1}^{T} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{z}_{t}, \\boldsymbol{c}_{t}\\right)-\\min _{\\boldsymbol{x} \\in \\Theta} f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{t}\\right)\\right] \\geq \\frac{\\alpha}{2} \\sum_{t=1}^{T} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}_{t}\\right)\\right\\|^{2}\\right]\n$$\n\ndue to the uniform strong convexity of functions $\\boldsymbol{x} \\mapsto f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}(\\boldsymbol{x}, \\boldsymbol{c})$.\nConsider the statistics\n\n$$\n\\left(\\hat{\\boldsymbol{\\omega}}_{t}, \\hat{\\boldsymbol{\\tau}}_{t}\\right) \\in \\underset{\\boldsymbol{\\omega} \\in \\Omega, \\boldsymbol{\\tau} \\in\\{-1,1\\}^{d-1}}{\\arg \\min }\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}_{t}\\right)\\right\\|\n$$\n\nSince $\\left\\|\\boldsymbol{x}^{*}\\left(\\hat{\\boldsymbol{\\omega}}_{t}, \\hat{\\boldsymbol{\\tau}}_{t}, \\boldsymbol{c}_{t}\\right)-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}_{t}\\right)\\right\\| \\leq\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}^{*}\\left(\\hat{\\boldsymbol{\\omega}}_{t}, \\hat{\\boldsymbol{\\tau}}_{t}, \\boldsymbol{c}_{t}\\right)\\right\\|+\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}_{t}\\right)\\right\\| \\leq 2\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}_{t}\\right)\\right\\|$ for all $\\boldsymbol{\\omega} \\in \\Omega, \\boldsymbol{\\tau} \\in\\{-1,1\\}^{d-1}$ we obtain\n\n$$\n\\begin{aligned}\n& \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}_{t}\\right)\\right\\|^{2}\\right] \\geq \\frac{1}{4} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\left\\|\\boldsymbol{x}^{*}\\left(\\hat{\\boldsymbol{\\omega}}_{t}, \\hat{\\boldsymbol{\\tau}}_{t}, \\boldsymbol{c}_{t}\\right)-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}_{t}\\right)\\right\\|^{2}\\right] \\\\\n& =\\frac{1}{4} \\alpha^{-2} L^{2} \\delta^{-\\gamma} r_{1}^{2} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\sum_{j=1}^{K^{p}} d^{2 \\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq \\omega_{j}\\right) \\mathbb{1}\\left(\\boldsymbol{c}_{t} \\in B_{j}\\right)\\right] \\\\\n& +\\frac{1}{4} \\alpha^{-2} h^{2} r_{2}^{2} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\rho\\left(\\boldsymbol{\\tau}, \\hat{\\boldsymbol{\\tau}}_{t}\\right)\\right]\n\\end{aligned}\n$$\n\nwhere $\\rho\\left(\\boldsymbol{\\tau}, \\hat{\\boldsymbol{\\tau}}_{t}\\right)=\\sum_{i=2}^{d} \\mathbb{1}\\left(\\boldsymbol{\\tau}_{i} \\neq \\hat{\\boldsymbol{\\tau}}_{t, i}\\right)$ is the Hamming distance between $\\boldsymbol{\\tau}$ and $\\hat{\\boldsymbol{\\tau}}_{t}$, with $\\hat{\\boldsymbol{\\omega}}_{t}=$ $\\left(\\hat{\\omega}_{t, 1}, \\ldots, \\hat{\\omega}_{t, K^{p}}\\right)$ and $\\hat{\\boldsymbol{\\tau}}_{t}=\\left(\\hat{\\tau}_{t, 2}, \\ldots, \\hat{\\tau}_{t, d}\\right)$. Note that, due to (46), for $\\varepsilon=1 / 2$ we have\n\n$$\n\\mathbb{P}_{K}\\left[d\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\geq \\frac{1}{4 K}\\left|\\boldsymbol{c}_{t} \\in B_{j}\\right| \\geq \\mathbb{P}_{K}\\left[\\left\\|\\boldsymbol{c}_{t}-\\boldsymbol{b}_{j}\\right\\|_{\\infty} \\leq \\frac{1}{4 K}\\left|\\boldsymbol{c}_{t} \\in B_{j}\\right]=1\\right.\\right.\n$$\n\nwhich yields\n\n$$\n\\mathbb{P}_{K}\\left[d^{2 \\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\geq\\left(\\frac{\\delta^{2}}{4 p^{2}}\\right)^{\\gamma} \\mid \\boldsymbol{c}_{t} \\in B_{j}\\right]=1\n$$\n\nThus,\n\n$$\n\\begin{aligned}\n& \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{c}_{t}\\right)\\right\\|^{2}\\right] \\geq \\frac{1}{4} \\alpha^{-2} \\delta^{-\\gamma} r_{1}^{2} L^{2} \\sum_{j=1}^{K^{p}} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[d^{2 \\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq \\omega_{j}\\right) \\mid \\boldsymbol{c}_{t} \\in B_{j}\\right] \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{t} \\in B_{j}\\right) \\\\\n& +\\frac{1}{4} \\alpha^{-2} r_{2}^{2} h^{2} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\rho\\left(\\boldsymbol{\\tau}, \\hat{\\boldsymbol{\\tau}}_{t}\\right)\\right] \\\\\n& =\\frac{1}{4} \\alpha^{-2} r_{1}^{2} L^{2} \\delta^{-\\gamma} \\sum_{j=1}^{K^{p}}\\left(\\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[d^{2 \\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq \\omega_{j}\\right) \\mid \\boldsymbol{c}_{t} \\in B_{j}, d^{2 \\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\geq\\left(\\frac{\\delta^{2}}{4 p^{2}}\\right)^{\\gamma}\\right]\\right. \\\\\n& \\left.\\mathbb{P}_{K}\\left(d^{2 \\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\geq\\left(\\frac{\\delta^{2}}{4 p^{2}}\\right)^{\\gamma} \\mid \\boldsymbol{c}_{t} \\in B_{j}\\right) \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{t} \\in B_{j}\\right)\\right)+\\alpha^{-2} r_{2}^{2} h^{2} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\rho\\left(\\boldsymbol{\\tau}, \\hat{\\boldsymbol{\\tau}}_{t}\\right)\\right] \\\\\n& \\geq \\frac{\\alpha^{-2}}{4^{\\gamma+1}} r_{1}^{2} L^{2} \\delta^{\\gamma} \\sum_{j=1}^{K^{p}} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq \\omega_{j}\\right) \\mid \\boldsymbol{c}_{t} \\in B_{j}, d^{2 \\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\geq\\left(\\frac{\\delta^{2}}{4 p^{2}}\\right)^{\\gamma}\\right] \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{t} \\in B_{j}\\right) \\\\\n& +\\frac{1}{4} \\alpha^{-2} r_{2}^{2} h^{2} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\rho\\left(\\boldsymbol{\\tau}, \\hat{\\boldsymbol{\\tau}}_{t}\\right)\\right]\n\\end{aligned}\n$$\n\nIntroduce the random event $\\Upsilon_{t, j}=\\left\\{\\boldsymbol{c}_{t} \\in B_{j}, d\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right) \\geq \\frac{\\delta}{2 p}\\right\\}$. Summing both sides from 1 to $T$, taking the maximum over $\\Omega$ and $\\{-1,1\\}^{d-1}$ and then taking the minimum over all statistics $\\hat{\\omega}_{t}$ with values in $\\Omega$ and $\\hat{\\boldsymbol{\\tau}}_{t}$ with values in $\\{-1,1\\}^{d-1}$ we obtain\n\n$$\n\\begin{aligned}\n\\min _{\\hat{\\omega}_{1}, \\ldots, \\hat{\\omega}_{T}} \\max _{\\omega \\in \\Omega} & \\sum_{t=1}^{T} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{c}_{t}\\right)\\right\\|^{2}\\right] \\geq \\frac{\\alpha^{-2}}{4^{\\gamma+1}} r_{1}^{2} L^{2} \\delta^{\\gamma} K^{-p} \\underbrace{\\min _{\\hat{\\omega}_{1}, \\ldots, \\hat{\\omega}_{T}} \\max _{\\omega \\in \\Omega} \\sum_{t=1}^{T} \\sum_{j=1}^{K^{p}} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq \\omega_{j}\\right) \\mid \\Upsilon_{t, j}\\right]}_{\\text {term I }} \\\\\n& +\\frac{1}{4} \\alpha^{-2} r_{2}^{2} h^{2} \\underbrace{\\min _{\\hat{\\boldsymbol{\\tau}}_{1}, \\ldots, \\hat{\\boldsymbol{\\tau}}_{T}} \\max _{\\boldsymbol{\\omega} \\in \\Omega, \\boldsymbol{\\tau} \\in\\{-1,1\\}^{d-1}} \\sum_{t=1}^{T} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\rho\\left(\\boldsymbol{\\tau}, \\hat{\\boldsymbol{\\tau}}_{t}\\right)\\right]}_{\\text {term II }} .\n\\end{aligned}\n$$\n\nWe will treat terms I and II separately. For term I we can write\n\n$$\n\\begin{aligned}\n\\operatorname{term} \\mathrm{I} & \\geq 2^{-K^{p}-d+1} \\min _{\\hat{\\omega}_{1}, \\ldots, \\hat{\\omega}_{T}} \\sum_{t=1}^{T} \\sum_{j=1}^{K^{p}} \\sum_{\\boldsymbol{\\tau} \\in\\{-1,1\\}^{d-1}} \\sum_{\\boldsymbol{\\omega} \\in \\Omega} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq \\omega_{j}\\right) \\mid \\Upsilon_{t, j}\\right] \\\\\n& \\geq 2^{-K^{p}-d+1} \\sum_{t=1}^{T} \\sum_{j=1}^{K^{p}} \\sum_{\\boldsymbol{\\tau} \\in\\{-1,1\\}^{d-1}} \\sum_{\\boldsymbol{\\omega} \\in \\Omega} \\min _{\\boldsymbol{\\omega}_{t, j}} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}\\left[\\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq \\omega_{j}\\right) \\mid \\Upsilon_{t, j}\\right]\n\\end{aligned}\n$$\n\nLet $\\Omega_{j}=\\left\\{\\omega \\in \\Omega: \\omega_{j}=1\\right\\}$, and for any $\\omega \\in \\Omega_{j}$, denote $\\bar{\\omega}$ such that $\\omega_{i}=\\bar{\\omega}_{i}$, for any $i \\neq j$, and $\\bar{\\omega}_{j}=-1$. Thus,\n\n$$\n\\begin{aligned}\n\\text { term } \\mathrm{I} & \\geq 2^{-K^{p}-d+1} \\sum_{t=1}^{T} \\sum_{j=1}^{K^{p}} \\sum_{\\tau \\in\\{-1,1\\}^{d-1}} \\sum_{\\omega \\in \\Omega_{j}} \\min _{\\omega_{t, j}}\\left(\\mathbf{E}_{\\omega, \\tau, t}\\left[\\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq 1\\right) \\mid \\Upsilon_{t, j}\\right]+\\mathbf{E}_{\\bar{\\omega}, \\tau, t}\\left[\\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq-1\\right) \\mid \\Upsilon_{t, j}\\right]\\right) \\\\\n& \\geq \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{j=1}^{K^{p}} \\min _{\\tau \\in\\{-1,1\\}^{d-1}, \\omega \\in \\Omega_{j}} \\min _{\\omega_{t, j}}\\left(\\mathbf{E}_{\\omega, \\tau, t}\\left[\\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq 1\\right) \\mid \\Upsilon_{t, j}\\right]+\\mathbf{E}_{\\bar{\\omega}, \\tau, t}\\left[\\mathbb{1}\\left(\\hat{\\omega}_{t, j} \\neq-1\\right) \\mid \\Upsilon_{t, j}\\right]\\right)\n\\end{aligned}\n$$\n\nFor any $\\omega \\in \\Omega_{j}$ and $\\tau \\in\\{-1,1\\}^{d-1}$, let $\\overline{\\mathbf{P}}_{\\omega, \\tau, t}$ and $\\overline{\\mathbf{P}}_{\\bar{\\omega}, \\tau, t}$ be the conditional probability distributions of $\\mathbf{P}_{\\omega, \\tau, t}$ and $\\mathbf{P}_{\\bar{\\omega}, \\tau, t}$ on the event $\\Upsilon_{t, j}$, respectively. Then, by Lemma 9 we have\n\n$$\n\\begin{aligned}\nK L\\left(\\overline{\\mathbf{P}}_{\\omega, \\tau, t}, \\overline{\\mathbf{P}}_{\\bar{\\omega}, \\tau, t}\\right) \\leq & I_{0} \\sum_{i=1}^{t-1} \\int \\max _{\\boldsymbol{x} \\in \\Theta}\\left|f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)-f_{\\bar{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)\\right|^{2} \\mathrm{~d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{i}\\right) \\\\\n& +I_{0} \\int \\max _{\\boldsymbol{x} \\in \\Theta}\\left|f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{t}\\right)-f_{\\bar{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{t}\\right)\\right|^{2} \\mathbb{1}\\left(\\boldsymbol{c}_{t} \\in \\Upsilon_{t, j}\\right) \\mathbb{P}_{K}^{-1}\\left(\\Upsilon_{t, j}\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{t}\\right) \\\\\n\\leq & I_{0} \\sum_{i=1}^{t-1} \\int\\left|2 r_{1} L \\eta(1) d^{\\gamma}\\left(\\boldsymbol{c}_{i}, \\partial B_{j}\\right)\\right|^{2} \\mathbb{1}\\left(\\boldsymbol{c}_{i} \\in B_{j}\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{i}\\right) \\\\\n& +I_{0} \\int\\left|2 r_{1} \\eta(1) d^{\\gamma}\\left(\\boldsymbol{c}_{t}, \\partial B_{j}\\right)\\right|^{2} \\mathbb{1}\\left(\\boldsymbol{c}_{t} \\in \\Upsilon_{t, j}\\right) \\mathbb{P}_{K}^{-1}\\left(\\Upsilon_{t, j}\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{t}\\right) \\\\\n\\leq & 4 I_{0} r_{1}^{2} L^{2} \\eta^{2}(1) \\delta^{2 \\gamma}\\left(\\sum_{i=1}^{t-1} \\int \\mathbb{1}\\left(\\boldsymbol{c}_{i} \\in B_{j}\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{i}\\right)+1\\right) \\leq 4 I_{0} r_{1}^{2} L^{2} \\eta^{2}(1) \\delta^{2 \\gamma}\\left(t K^{-p}+1\\right)\n\\end{aligned}\n$$\n\nSince $\\delta=1 /(2 K), K=\\max \\left(1,\\left\\lfloor\\left(\\min \\left(1, L^{2}\\right) T\\right)^{\\frac{1}{p+2 \\gamma}}\\right\\rfloor\\right)$ we obtain that\n\n$$\nK L\\left(\\overline{\\mathbf{P}}_{\\omega, \\tau, t}, \\overline{\\mathbf{P}}_{\\bar{\\omega}, \\tau, t}\\right) \\leq \\log (2)\n$$\n\nif we choose $r_{1} \\leq\\left(\\log (2) /\\left(8 I_{0} \\eta^{2}(1) \\max \\left(1, L^{2}\\right)\\right)\\right)^{1 / 2}$. Therefore, using Theorem 2.12 from [43] we get\n\n$$\n\\operatorname{term} \\mathrm{I} \\geq \\frac{T K^{p}}{4} \\exp (-\\log (2))=\\frac{T K^{p}}{8}\n$$\n\nMoreover, for any $\\omega \\in \\Omega$ and $\\boldsymbol{\\tau}, \\boldsymbol{\\tau}^{\\prime} \\in\\{-1,1\\}^{d-1}$ such that $\\rho\\left(\\boldsymbol{\\tau}, \\boldsymbol{\\tau}^{\\prime}\\right)=1$ we have\n\n$$\n\\begin{aligned}\nK L\\left(\\mathbf{P}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}, \\mathbf{P}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}^{\\prime}, t}\\right)= & \\int \\log \\left(\\frac{\\mathrm{d} \\mathbf{P}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t}}{\\mathrm{d} \\mathbf{P}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}^{\\prime}, t}}\\right) \\mathrm{d} \\mathbf{P}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t} \\\\\n= & \\int\\left[\\log \\left(\\frac{\\mathrm{~d} F\\left(y_{1}-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{z}_{1}, \\boldsymbol{c}_{1}\\right)\\right)}{\\mathrm{d} F\\left(y_{1}-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}^{\\prime}}\\left(\\boldsymbol{z}_{1}, \\boldsymbol{c}_{1}\\right)\\right)}\\right)+\\right. \\\\\n& \\left.+\\sum_{i=2}^{t} \\log \\left(\\frac{\\mathrm{~d} F\\left(y_{i}-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\Phi_{i}\\left(\\boldsymbol{z}_{1}^{i-1}, y_{1}^{i-1}, \\boldsymbol{c}_{1}^{i}\\right), \\boldsymbol{c}_{i}\\right)\\right)}{\\mathrm{d} F\\left(y_{i}-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}^{\\prime}}\\left(\\Phi_{i}\\left(\\boldsymbol{z}_{1}^{i-1}, y_{1}^{i-1}, \\boldsymbol{c}_{1}^{i}\\right), \\boldsymbol{c}_{i}\\right)\\right)}\\right)\\right] \\\\\n& \\mathrm{d} F\\left(y_{i}-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{z}_{1}^{i-1}, \\boldsymbol{c}_{1}\\right)\\right) \\prod_{i=2}^{t} \\mathrm{~d} F\\left(y_{i}-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\Phi_{i}\\left(\\boldsymbol{z}_{1}^{i-1}, y_{1}^{i-1}, \\boldsymbol{c}_{1}^{i}\\right), \\boldsymbol{c}_{i}\\right)\\right) \\\\\n\\leq & I_{0} \\sum_{i=1}^{t} \\max _{\\boldsymbol{x} \\in \\Theta}\\left|f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}^{\\prime}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)\\right|^{2}=4 t I_{0} r_{2}^{2} h^{4} \\eta^{2}(1)\n\\end{aligned}\n$$\n\nUsing the fact that $h \\leq T^{-\\frac{1}{4}}$, choosing $r_{2} \\leq\\left(\\log (2) /\\left(4 I_{0} \\eta^{2}(1)\\right)\\right)^{1 / 2}$ and applying again Theorem 2.12 from [43] implies\n\n$$\n\\text { term II } \\geq \\frac{T(d-1)}{4} \\exp (-\\log (2)) \\geq \\frac{T d}{16}\n$$\n\nPutting together the bounds for terms I and II we get\n\n$$\n\\begin{aligned}\n\\min _{\\tilde{\\boldsymbol{\\omega}}_{1}, \\tilde{\\boldsymbol{\\tau}}_{1}, \\ldots, \\tilde{\\boldsymbol{\\omega}}_{T}, \\tilde{\\boldsymbol{\\tau}}_{T} \\boldsymbol{\\omega} \\in \\Omega, \\tau \\in\\{-1,1\\}^{d-1}} \\sum_{t=1}^{T} \\mathbf{E}_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, t} & {\\left[\\left\\|\\boldsymbol{z}_{t}-\\boldsymbol{x}^{*}\\left(\\boldsymbol{\\omega}, \\boldsymbol{\\tau}, \\boldsymbol{c}_{t}\\right)\\right\\|^{2}\\right] \\geq \\frac{\\alpha^{-2}}{256} r_{1}^{2} T \\delta^{\\gamma}+\\frac{\\alpha^{-2}}{64} r_{2}^{2} d T h^{2} } \\\\\n& \\geq \\frac{\\alpha^{-2}}{2048} r_{3}^{2} \\min \\left(1, L^{\\frac{2(d+1)}{d+2 \\gamma}}\\right) T^{\\frac{d+\\gamma}{d+2 \\gamma}}+\\frac{\\alpha^{-2}}{64} r_{2}^{2} \\min \\left(T, d T^{\\frac{1}{2}}\\right)\n\\end{aligned}\n$$\n\nwhere $r_{3}=r_{2} / \\min (1, L)$. Considering the fact that $r_{2}$ and $r_{3}$ are independent of $d, p, L$, and $T$, (44) combined with (43) implies the theorem.\n\nRecall the definition of $\\mathbb{P}_{K}$ in (15). The following figure illustrates the support of $\\mathbb{P}_{K}$ for $K=3$ and $p=2$. By definition, $\\mathbb{P}_{K}$ yields a uniform distribution for each black square in the figure.\n\nThe next lemma provides an upper bound on the Kullback-Leibler divergence of two probability measures, which plays a key role in our analysis of the lower bound.\n\nLemma 9. With the same notation as in the proof of Theorem 3, for $\\boldsymbol{\\omega}, \\boldsymbol{\\omega}^{\\prime} \\in \\Omega$ and $t \\geq 1$ denote $\\mathbf{P}_{\\boldsymbol{\\omega}, t}$ and $\\mathbf{P}_{\\boldsymbol{\\omega}^{\\prime}, t}$ as they are defined in (42). Let $\\Upsilon \\subseteq[0,1]^{p}$ such that $\\mathbb{P}_{K}(\\Upsilon) \\neq 0$. Let $\\tilde{\\mathbf{P}}_{\\boldsymbol{\\omega}, t} \\tilde{\\mathbf{P}}_{\\boldsymbol{\\omega}^{\\prime}, t}$\n\n![img-0.jpeg](img-0.jpeg)\n\nSupport of $\\mathbb{P}_{K}$ for $K=3$ and $p=2$\nbe the conditional probability measures of $\\mathbf{P}_{\\boldsymbol{\\omega}, t} \\mathbf{P}_{\\boldsymbol{\\omega}^{\\prime}, t}$ given the event $\\boldsymbol{c}_{t} \\in \\Upsilon$, respectively. Then, if (14) holds, we have\n\n$$\n\\begin{aligned}\n& K L\\left(\\tilde{\\mathbf{P}}_{\\boldsymbol{\\omega}, t}, \\tilde{\\mathbf{P}}_{\\boldsymbol{\\omega}^{\\prime}, t}\\right) \\leq I_{0} \\sum_{i=1}^{t-1} \\max _{\\boldsymbol{x} \\in \\Theta}\\left|f_{\\boldsymbol{\\omega}, t}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)-f_{\\boldsymbol{\\omega}^{\\prime}, t}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)\\right| \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{i}\\right) \\\\\n& +I_{o} \\int \\max _{\\boldsymbol{x} \\in \\Theta}\\left|f_{\\boldsymbol{\\omega}, t}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)-f_{\\boldsymbol{\\omega}^{\\prime}, t}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)\\right| \\mathbb{1}\\left(\\boldsymbol{c}_{t} \\in \\Upsilon\\right) \\mathbb{P}_{K}^{-1}(\\Upsilon) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{t}\\right)\n\\end{aligned}\n$$\n\nProof. By Lemma 8(i), for any $\\boldsymbol{\\omega} \\in \\Omega$ we have\n\n$$\n\\begin{aligned}\n& \\mathrm{d} \\tilde{\\mathbf{P}}_{\\boldsymbol{\\omega}, t}\\left(\\boldsymbol{z}_{1}, y_{1}^{t}, \\boldsymbol{c}_{1}^{t}\\right)=\\mathrm{d} F\\left(y_{1}-f_{\\boldsymbol{\\omega}}\\left(\\boldsymbol{z}_{1}, \\boldsymbol{c}_{1}\\right)\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{1}\\right)\\left(\\prod_{i=2}^{t-1} \\mathrm{~d} F\\left(y_{i}-f_{\\boldsymbol{\\omega}}\\left(\\pi_{i}\\left(\\boldsymbol{z}_{1}, y_{1}^{i-1}, \\boldsymbol{c}_{1}^{i-1}\\right)\\right)\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{i}\\right)\\right) \\\\\n& \\mathrm{d} F\\left(y_{t}-f_{\\boldsymbol{\\omega}}\\left(\\pi_{i}\\left(\\boldsymbol{z}_{1}, y_{1}^{i-1}, \\boldsymbol{c}_{1}^{t}\\right)\\right)\\right) \\mathbb{1}\\left(\\boldsymbol{c}_{t} \\in \\Upsilon\\right) \\mathbb{P}_{K}^{-1}(\\Upsilon) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{t}\\right)\n\\end{aligned}\n$$\n\nTherefore, for any $\\boldsymbol{\\omega}, \\boldsymbol{\\omega}^{\\prime} \\in \\Omega$, by Lemma 8(ii) we can write\n\n$$\n\\begin{aligned}\n& K L\\left(\\tilde{\\mathbf{P}}_{\\boldsymbol{\\omega}, t}, \\tilde{\\mathbf{P}}_{\\boldsymbol{\\omega}^{\\prime}, t}\\right)=\\int \\log \\left(\\frac{\\mathrm{d} \\mathbf{P}_{\\boldsymbol{\\omega}, t}}{\\mathrm{d} \\mathbf{P}_{\\boldsymbol{\\omega}^{\\prime}, t}}\\right) \\mathrm{d} \\tilde{\\mathbf{P}}_{\\boldsymbol{\\omega}, t} \\\\\n& =\\int\\left[\\log \\left(\\frac{\\mathrm{d} F\\left(y_{1}-f_{\\boldsymbol{\\omega}}\\left(\\boldsymbol{z}_{1}, \\boldsymbol{c}_{1}\\right)\\right)}{\\mathrm{d} F\\left(y_{1}-f_{\\boldsymbol{\\omega}^{\\prime}}\\left(\\boldsymbol{z}_{1}, \\boldsymbol{c}_{1}\\right)\\right)}\\right)+\\right. \\\\\n& \\left.+\\sum_{i=2}^{t} \\log \\left(\\frac{\\mathrm{~d} F\\left(y_{i}-f_{\\boldsymbol{\\omega}}\\left(\\pi_{i}\\left(\\boldsymbol{z}_{1}, y_{1}^{i-1}, \\boldsymbol{c}_{1}^{i}\\right), \\boldsymbol{c}_{i}\\right)\\right)}{\\mathrm{d} F\\left(y_{i}-f_{\\boldsymbol{\\omega}^{\\prime}}\\left(\\pi_{i}\\left(\\boldsymbol{z}_{1}, y_{1}^{i-1}, \\boldsymbol{c}_{1}^{i}\\right), \\boldsymbol{c}_{i}\\right)\\right)}\\right)\\right] \\\\\n& \\mathrm{d} F\\left(y_{1}-f_{\\boldsymbol{\\omega}}\\left(\\boldsymbol{z}_{1}, \\boldsymbol{c}_{1}\\right)\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{1}\\right)\\left(\\prod_{i=2}^{t-1} \\mathrm{~d} F\\left(y_{i}-f_{\\boldsymbol{\\omega}}\\left(\\pi_{i}\\left(\\boldsymbol{z}_{1}, y_{1}^{i-1}, \\boldsymbol{c}_{1}^{i}\\right), \\boldsymbol{c}_{i}\\right)\\right) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{i}\\right)\\right) \\\\\n& \\mathrm{d} F\\left(y_{t}-f_{\\boldsymbol{\\omega}}\\left(\\pi_{t}\\left(\\boldsymbol{z}_{1}, y_{1}^{t-1}, \\boldsymbol{c}_{1}^{t}\\right), \\boldsymbol{c}_{t}\\right)\\right) \\mathbb{1}\\left(\\boldsymbol{c}_{t} \\in \\Upsilon\\right) \\mathbb{P}_{K}^{-1}(\\Upsilon) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{t}\\right) \\\\\n& \\leq I_{0} \\sum_{i=1}^{t-1} \\int \\max _{\\boldsymbol{x} \\in \\Theta}\\left|f_{\\boldsymbol{\\omega}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)-f_{\\boldsymbol{\\omega}^{\\prime}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{i}\\right)\\right|^{2} \\mathrm{~d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{i}\\right) \\\\\n& +I_{0} \\int \\max _{\\boldsymbol{x} \\in \\Theta}\\left|f_{\\boldsymbol{\\omega}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{t}\\right)-f_{\\boldsymbol{\\omega}^{\\prime}}\\left(\\boldsymbol{x}, \\boldsymbol{c}_{t}\\right)\\right|^{2} \\mathbb{1}\\left(\\boldsymbol{c}_{t} \\in \\Upsilon\\right) \\mathbb{P}_{K}^{-1}(\\Upsilon) \\mathrm{d} \\mathbb{P}_{K}\\left(\\boldsymbol{c}_{t}\\right)\n\\end{aligned}\n$$\n\nLemma 10. With the same notation as in the proof Theorem 3, let $f_{\\omega, \\tau}: \\mathbb{R}^{d} \\times[0,1]^{p} \\rightarrow \\mathbb{R}$ denoted the function that it is defined in (41). Then, for all $\\omega \\in \\Omega$ and $\\tau \\in\\{-1,1\\}^{d-1}, f_{\\omega, \\tau} \\in$ $\\mathcal{F}_{\\alpha, \\beta}(M) \\cap \\mathcal{F}_{\\gamma}(L)$.\nProof. Fix $\\omega \\in \\Omega, \\tau \\in\\{-1,1\\}^{d-1}$. First, let us prove that for all $\\boldsymbol{x} \\in \\mathbb{R}^{d}, f_{\\omega, \\tau}(\\boldsymbol{x}, \\cdot)$ is a 1 Lipschitz function. For any $\\boldsymbol{c}, \\boldsymbol{c}^{\\prime} \\in[0,1]^{p}$, we can write\n\n$$\n\\begin{aligned}\n\\left|f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}(\\boldsymbol{x}, \\boldsymbol{c})-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{x}, \\boldsymbol{c}^{\\prime}\\right)\\right| & =r_{1} L \\eta\\left(x_{1} \\delta^{-\\frac{\\gamma}{2}}\\right)\\left|\\sum_{j=1}^{K^{p}} \\omega_{j}\\left(d^{\\gamma}\\left(\\boldsymbol{c}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\boldsymbol{c} \\in B_{j}\\right)-d^{\\gamma}\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\boldsymbol{c}^{\\prime} \\in B_{j}\\right)\\right)\\right| \\\\\n& \\leq r_{1} L \\eta(1)\\left|\\sum_{j=1}^{K^{p}} \\omega_{j}\\left(d\\left(\\boldsymbol{c}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\boldsymbol{c} \\in B_{j}\\right)-d^{\\gamma}\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\boldsymbol{c}^{\\prime} \\in B_{j}\\right)\\right)\\right|\n\\end{aligned}\n$$\n\nFor $i, j \\in K^{p}$, let us assume that $\\boldsymbol{c} \\in B_{i}$ and $\\boldsymbol{c}^{\\prime} \\in B_{j}$, and let $\\boldsymbol{y}_{i} \\in \\partial B_{i}, \\boldsymbol{y}_{j}^{\\prime} \\in \\partial B_{j}$, such that $d\\left(\\boldsymbol{c}, \\partial B_{i}\\right)=\\left\\|\\boldsymbol{c}-\\boldsymbol{y}_{i}\\right\\|$ and $d\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{j}\\right)=\\left\\|\\boldsymbol{c}^{\\prime}-\\boldsymbol{y}_{j}^{\\prime}\\right\\|$. If $i=j$, then\n\n$$\n\\begin{aligned}\n\\left|f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}(\\boldsymbol{x}, \\boldsymbol{c})-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{x}, \\boldsymbol{c}^{\\prime}\\right)\\right| \\leq & r_{1} L \\eta(1)\\left|d^{\\gamma}\\left(\\boldsymbol{c}, \\partial B_{i}\\right)-d^{\\gamma}\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{i}\\right)\\right| \\\\\n= & r_{1} L \\eta(1)\\left(\\mathbb{1}\\left(d\\left(\\boldsymbol{c}, \\partial B_{i}\\right) \\geq d\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{i}\\right)\\right)\\left(\\left\\|\\boldsymbol{c}-\\boldsymbol{y}_{i}\\right\\|-\\left\\|\\boldsymbol{c}^{\\prime}-\\boldsymbol{y}_{i}^{\\prime}\\right\\|\\right)\\right. \\\\\n& \\left.+\\mathbb{1}\\left(d\\left(\\boldsymbol{c}, \\partial B_{i}\\right)<d\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{i}\\right)\\right)\\left(\\left\\|\\boldsymbol{c}^{\\prime}-\\boldsymbol{y}_{i}^{\\prime}\\right\\|^{\\gamma}-\\left\\|\\boldsymbol{c}-\\boldsymbol{y}_{i}\\right\\|^{\\gamma}\\right)\\right) \\\\\n\\leq & r_{1} L \\eta(1)\\left(\\mathbb{1}\\left(d\\left(\\boldsymbol{c}, \\partial B_{i}\\right) \\geq d\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{i}\\right)\\right)\\left(\\left\\|\\boldsymbol{c}-\\boldsymbol{y}_{i}^{\\prime}\\right\\|^{\\gamma}-\\left\\|\\boldsymbol{c}^{\\prime}-\\boldsymbol{y}_{i}^{\\prime}\\right\\|^{\\gamma}\\right)\\right. \\\\\n& \\left.+\\mathbb{1}\\left(d\\left(\\boldsymbol{c}, \\partial B_{i}\\right)<d\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{i}\\right)\\right)\\left(\\left\\|\\boldsymbol{c}^{\\prime}-\\boldsymbol{y}_{i}\\right\\|^{\\gamma}-\\left\\|\\boldsymbol{c}-\\boldsymbol{y}_{i}\\right\\|^{\\gamma}\\right)\\right) \\leq L\\left\\|\\boldsymbol{c}-\\boldsymbol{c}^{\\prime}\\right\\|^{\\gamma}\n\\end{aligned}\n$$\n\nwhere the last inequality is obtained by the fact that $r_{1} \\leq 1 /(2 \\eta(1) \\max (1, L))$. If $i \\neq j$, it means that $\\boldsymbol{c}$ and $\\boldsymbol{c}^{\\prime}$ does not belong to the same grid and we have\n\n$$\nd\\left(\\boldsymbol{c}, \\partial B_{i}\\right) \\leq\\left\\|\\boldsymbol{c}-\\boldsymbol{c}^{\\prime}\\right\\|, \\quad \\text { and } \\quad d\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{j}\\right) \\leq\\left\\|\\boldsymbol{c}-\\boldsymbol{c}^{\\prime}\\right\\|\n$$\n\nConsequently, we can write\n\n$$\n\\left|f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}(\\boldsymbol{x}, \\boldsymbol{c})-f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}\\left(\\boldsymbol{x}, \\boldsymbol{c}^{\\prime}\\right)\\right| \\leq r_{1} L \\eta(1)\\left(d^{\\gamma}\\left(\\boldsymbol{c}, \\partial B_{i}\\right)+d^{\\gamma}\\left(\\boldsymbol{c}^{\\prime}, \\partial B_{i}\\right)\\right) \\leq L\\left\\|\\boldsymbol{c}-\\boldsymbol{c}^{\\prime}\\right\\|^{\\gamma}\n$$\n\nwhere the last inequality is due to $r_{1} \\leq 1 /(2 \\eta(1) \\max (1, L))$.\nFixing $\\boldsymbol{c} \\in[0,1]^{p}$, we calculate $\\nabla_{\\#}^{2} f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}(\\cdot, \\boldsymbol{c})$. Let $\\left(\\nabla_{\\#}^{2} f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}(\\boldsymbol{x}, \\boldsymbol{c})\\right)_{i, j}$ indicate the $(i, j)$ entry of the Hessian matrix. For $1 \\leq i \\neq j \\leq K^{p}$ it is straightforward to check that\n\n$$\n\\left(\\nabla_{\\#}^{2} f_{\\boldsymbol{\\omega}, \\boldsymbol{\\tau}}(\\boldsymbol{x}, \\boldsymbol{c})\\right)_{i, j}=0\n$$\n\nNow, for $i=j=1$, we have\n\n$$\n\\left|\\left(\\nabla_{\\boldsymbol{x}}^{2} f_{\\boldsymbol{\\omega}, \\boldsymbol{r}}(\\boldsymbol{x}, \\boldsymbol{c})\\right)_{1,1}-2 \\alpha\\right| \\leq r_{1} L \\delta^{-\\gamma} \\max _{x \\in \\mathbb{R}}\\left|\\nabla_{x}^{2} \\eta(x)\\right| \\sum_{j=1}^{K^{p}} d^{\\gamma}\\left(\\boldsymbol{c}, \\partial B_{j}\\right) \\mathbb{1}\\left(\\boldsymbol{c} \\in B_{j}\\right) \\leq \\alpha\n$$\n\nfor $r_{1} \\leq \\alpha /\\left(L^{\\prime} \\max (1, L)\\right)$, where we introduced $L^{\\prime}=\\max _{x \\in \\mathbb{R}}\\left|\\eta^{\\prime \\prime}(x)\\right|$. Furthermore, for $i=j>1$, we can write\n\n$$\n\\left|\\left(\\nabla_{\\boldsymbol{x}}^{2} f_{\\boldsymbol{\\omega}, \\boldsymbol{r}}(\\boldsymbol{x}, \\boldsymbol{c})\\right)_{i, i}-2 \\alpha\\right| \\leq r_{2} \\max _{x \\in \\mathbb{R}}\\left|\\nabla_{x}^{2} \\eta(x)\\right| \\leq \\alpha\n$$\n\nfor $r_{2} \\leq \\alpha / L^{\\prime}$. Finally, to show that $|f(\\boldsymbol{x}, \\boldsymbol{c})| \\leq M$ for all $\\boldsymbol{x} \\in \\Theta$ and $\\boldsymbol{c} \\in[0,1]^{p}$, we have\n\n$$\n|f(\\boldsymbol{x}, \\boldsymbol{c})| \\leq \\alpha+r_{1} L \\eta(1)+r_{2} \\eta(1) \\leq M\n$$\n\nLemma 11. For any $\\boldsymbol{c} \\in B_{j}$ we have\n\n$$\nd\\left(\\boldsymbol{c}, \\partial B_{j}\\right) \\leq \\frac{1}{2 K}\n$$\n\nand, for any $\\varepsilon \\in(0,1)$,\n\n$$\n\\left\\|\\boldsymbol{c}-\\boldsymbol{b}_{j}\\right\\|_{\\infty} \\leq \\frac{\\varepsilon}{2 K} \\quad \\Longrightarrow \\quad d\\left(\\boldsymbol{c}, \\partial B_{j}\\right) \\geq \\frac{1-\\varepsilon}{2 K}\n$$\n\nwhere $\\|\\cdot\\|_{\\infty}$ is the $\\ell_{\\infty}$ norm.\nProof. Without loss of generality, we assume throughout the proof that $K=1$ and $\\boldsymbol{b}_{j}=0$, so that the closure of $B_{j}$ is $[-1 / 2,1 / 2]^{d}$ and $\\partial B_{j}=\\left\\{\\boldsymbol{u}:\\|\\boldsymbol{u}\\|_{\\infty}=1 / 2\\right\\}$. Inequality (45) follows from the fact that $d\\left(\\boldsymbol{c}, \\partial B_{j}\\right) \\leq d\\left(\\boldsymbol{c}, \\boldsymbol{c}^{\\prime}\\right) \\leq 1 / 2$, where $\\boldsymbol{c}^{\\prime} \\in \\partial B_{j}$ is a vector such that all its coordinates except the first one coincide with those of $\\boldsymbol{c}$, and the first coordinate $c_{1}^{\\prime}$ of $\\boldsymbol{c}^{\\prime}$ is equal to the projection of the first coordinate $c_{1} \\in[-1 / 2,1 / 2]$ of $\\boldsymbol{c}$ onto the set $\\{-1 / 2,1 / 2\\}$.\n\nIn order to prove (46), we denote by $\\boldsymbol{u}^{*}$ a solution of the problem\n\n$$\n\\min _{\\boldsymbol{u}:\\left\\|\\boldsymbol{u}\\right\\|_{\\infty}=1 / 2}\\|\\boldsymbol{c}-\\boldsymbol{u}\\|\n$$\n\nThere exists $i^{*}$ such that $\\left|u_{i^{*}}^{*}\\right|=1 / 2$, where $u_{i^{*}}^{*}$ is the $i^{*}$ th coordinate of $\\boldsymbol{u}^{*}$. Under the assumption that $\\|\\boldsymbol{c}\\|_{\\infty} \\leq \\varepsilon / 2$ we have\n\n$$\nd^{2}\\left(\\boldsymbol{c}, \\partial B_{j}\\right)=\\left\\|\\boldsymbol{c}-\\boldsymbol{u}^{*}\\right\\|^{2} \\geq\\left|c_{i^{*}}-u_{i^{*}}^{*}\\right|^{2} \\geq \\min \\left(\\left|c_{i^{*}}-1 / 2\\right|^{2},\\left|c_{i^{*}}+1 / 2\\right|^{2}\\right) \\geq\\left(\\frac{1-\\varepsilon}{2}\\right)^{2}\n$$\n\nThis proves (46).",
      "tables": {},
      "images": {
        "img-0.jpeg": "data:image/jpeg;base64,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"
      }
    },
    {
      "section_id": 13,
      "text": "# D General comments on self-concordant barriers \n\nIn [36, Proposition 2.3.6], the authors demonstrated the existence of a convex body $\\Theta$ such that any $\\mu$-self-concordant barrier over $\\Theta$ should have $\\mu \\geq d$. The proposition is stated as follows:\n\nStatement of [36, Proposition 2.3.6]: Let $\\Theta \\in \\mathbb{R}^{d}$ be a convex polytope, such that certain boundary points of $\\Theta$ belong to $m$ of $(d-1)$-dimensional facets of $\\Theta$, where $m \\in[d]$. Moreover, assume that the normal vectors to these facets are linearly independent. Then the value of $\\mu$ for any $\\mu$-self-concordant barrier on $\\Theta$ cannot be less than $m$. In particular, the $d$-dimensional non-negative orthant, simplex, and hyper-cube do not admit barriers with the parameter less than $\\mu$.\n\nFor the case when $\\Theta$ is a polytope, one can construct the following self-concordant barrier [35, Corollary 3.1.1]\n\n- $\\mathcal{R}(\\boldsymbol{u})=-\\sum_{j=1}^{m} \\log \\left(\\left\\langle\\boldsymbol{a}_{j}, \\boldsymbol{u}\\right\\rangle-b_{j}\\right)$ is an $m$-self-concordant barrier for the polytope\n\n$$\n\\Theta=\\left\\{\\boldsymbol{u} \\in \\mathbb{R}^{d}:\\left\\langle\\boldsymbol{a}_{j}, \\boldsymbol{u}\\right\\rangle \\geq b_{j}, \\quad \\text { for } j \\in[m] \\quad \\text { and } \\quad \\boldsymbol{a}_{j} \\in \\mathbb{R}^{d}, b_{j} \\in \\mathbb{R}\\right\\}\n$$\n\nNote that in the above, if $\\Theta$ is the $d$-dimensional hyper-cube then $m=2 d$, and in this case [36, Proposition 2.3.6] indicates that the above $\\mathcal{R}$ has an optimal $\\mu$ as a function of $d$. Moreover, if $\\Theta$ is the $d$-dimensional simplex then $m=d+1$, which again by [36, Proposition 2.3.6] we can see that the above example of $\\mathcal{R}$ has an optimal $\\mu$ as a function of $d$.\n\nFor the case when $\\Theta$ is scaled version of the unit Euclidean ball with ratio $r>0$, one can consider the following self-concordant barrier [35, Example 9.3.2]\n\n- $\\mathcal{R}(\\boldsymbol{u})=-\\log \\left(r^{2}-\\|\\boldsymbol{u}\\|^{2}\\right)$ is a 2-self-concordant barrier for $\\Theta$.\n\nConsidering the result in [36, Corollary 2.3.3], we have always that $\\mu \\geq 1$ which shows the optimality of the above $\\mathcal{R}$ for the unit Euclidean ball.\n\nWe refer the reader to [35, Chapter 9] for further examples of self-concordant barrier functions.\nIn the general case of a convex body $\\Theta,[11,17,22]$, proposed $\\mu$-self-concordant barriers with $\\mu \\leq \\mathbb{A} d$, where $\\mathbb{A}>0$ is a non-increasing function of $d$. Particularly, [11, Theorem 1] shows that $\\mathbb{A} \\leq 1+100 \\sqrt{\\log (d) / d}$, for all $d \\geq 80$. All of these constructions by [11, 17, 22] are computationally expensive (see [11, Section 2.2] for further details on the computational aspects of these methods). Now, let us consider the special case where $\\Theta$ is the unit $\\ell_{q}$-ball for $q \\in(2, \\infty]$, i.e.,\n\n$$\n\\Theta=\\left\\{\\boldsymbol{u} \\in \\mathbb{R}^{d}:\\left.\\sum_{j=1}^{d}\\left|\\boldsymbol{u}_{j}\\right|^{q} \\leq 1\\right\\}\n$$\n\nfor $q \\in(2, \\infty)$, and\n\n$$\n\\Theta=\\left\\{\\boldsymbol{u} \\in \\mathbb{R}^{d}: \\max _{j \\in[d]}\\left|\\boldsymbol{u}_{j}\\right| \\leq 1\\right\\}\n$$\n\nfor $q=\\infty$. In [10, Lemma 11 and Theorem 5], the authors showed that any $\\mu$-self-concordant barrier on $\\Theta$ should satisfy\n\n$$\n\\mu \\geq \\mathbb{A} \\max \\left(d^{\\frac{1}{q}} \\log (d)^{-\\frac{q}{q-2}}, d^{1-\\frac{2}{q}}\\right)\n$$\n\nwhere $\\mathrm{A}>0$ only depends on $q$. The lower bound indicates that for the case of $q=\\infty$, the $(d+1)$-self-concordant barrier proposed by $[11,17,22]$ is optimal with respect to $\\mu$ as a function of $d$. However, constructing an optimal $\\mu$-self-concordant barrier for any $\\ell_{q}$-ball for $q \\in(2, \\infty)$ remains an open problem.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2406.05714v3",
  "authors": [
    "Arya Akhavan",
    "Karim Lounici",
    "Massimiliano Pontil",
    "Alexandre B. Tsybakov"
  ],
  "categories": [
    "stat.ML",
    "cs.LG",
    "math.ST",
    "stat.TH"
  ],
  "abstract": "We study the contextual continuum bandits problem, where the learner\nsequentially receives a side information vector and has to choose an action in\na convex set, minimizing a function associated to the context. The goal is to\nminimize all the underlying functions for the received contexts, leading to a\ndynamic (contextual) notion of regret, which is stronger than the standard\nstatic regret. Assuming that the objective functions are H\\\"older with respect\nto the contexts, we demonstrate that any algorithm achieving a sub-linear\nstatic regret can be extended to achieve a sub-linear dynamic regret. We\nfurther study the case of strongly convex and smooth functions when the\nobservations are noisy. Inspired by the interior point method and employing\nself-concordant barriers, we propose an algorithm achieving a sub-linear\ndynamic regret. Lastly, we present a minimax lower bound, implying two key\nfacts. First, no algorithm can achieve sub-linear dynamic regret over functions\nthat are not continuous with respect to the context. Second, for strongly\nconvex and smooth functions, the algorithm that we propose achieves, up to a\nlogarithmic factor, the minimax optimal rate of dynamic regret as a function of\nthe number of queries.",
  "updated": "2025-03-03T23:11:43Z",
  "published": "2024-06-09T10:12:08Z"
}