{
  "title": "Dense networks of integrate-and-fire neurons: Spatially-extended\n  mean-field limit of the empirical measure",
  "sections": [
    {
      "section_id": 0,
      "text": "## CONTENTS\n\n1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.1 General motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Model: Non-exchangeable systems of interacting neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n3.6 Effect of the random resets of the membrane potentials ..... 28\n3.7 Concluding the proof ..... 33\nA Appendix A ..... 40\nA. 1 Proof of Proposition 1 (Propagation of moments) ..... 40\nA. 2 Proof of Proposition 2 (Weak-* convergence) ..... 41\nA. 3 Proof of Proposition 3 (Convergence of the initial data) ..... 44\nAcknowledgments ..... 45\nFunding ..... 45\nReferences ..... 46",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "# 1. Introduction. \n\n1.1. General motivation. Spatially-structured population equations were among the first mathematical models proposed to describe the macroscopic dynamics of large networks of neurons in the brain [7, 48]. Fifty years ago, Wilson and Cowan derived a famous integrodifferential equation approximating the collective dynamics of spatially-structured networks of integrate-and-fire-type neurons [97]. Motivated by early neuronal recordings in sensory cortices [76, 50], their model was based on the idea that the cortex is functionally organized as a sheet of radially redundant neurons called cortical columns [77]. While this view is biologically simplistic from today's standpoint, spatially-structured population equationsor neural field models-have established themselves as classical models in neuroscience [78, $2,5,36,12,11]$.\n\nIntegro-differential equations such as that of Wilson and Cowan [97] are not exact meanfield limits of spatially-structured networks of integrate-and-fire-type neurons because their derivation relied on a form of coarse-graining in time. The first exact mean-field limit equation that does not neglect the fine time structure of integrate-and-fire-type neurons appears to have been derived by Gerstner [42] and it takes the form of a spatially-structured partial differential equation (PDE). It is therefore known in theoretical neuroscience that spatiallystructured networks can be described, as the number of neurons $N$ tends to infinity and if synaptic weights scale as $\\mathcal{O}(1 / N)$, by a spatially-structured PDE. A much less trivial question is following: In the absence of any prescribed spatial structure in the synaptic weights, does $\\mathcal{O}(1 / N)$ weight scaling suffice, by itself, to give rise to spatially-structured population dynamics in the mean-field limit? If the answer is positive, the spatial structure in the mean-field limit has to be interpreted as an emergent property that does not need to be related to the location of the neurons in putative cortical columns. Recent applications of the theory of dense graph limits [65, 64], or graphons, to the study of mean-field limits of nonexchangeable systems $[57,53]$ suggest that the answer to the question is indeed positive.\n\nThe goal of this work is to present a transparent proof for the convergence of nonexchangeable systems of interacting integrate-and-fire-type neurons to a spatially-extended PDE in the mean-field limit. In order to do so, we will consider the mean-field limit of dense networks only (instead of the networks with intermediate sparsity considered previously in [54]). While the spatial extension of the mean-field PDE we obtain resembles that of classical models such as neural field models, its interpretation is radically different: The spatial extension does not come from any prescribed spatial structure, but it reflects the organization of the emergent redundancies of individual neuron trajectories induced by $\\mathcal{O}(1 / N)$ synaptic weight scaling in the mean-field limit.\n1.2. Model: Non-exchangeable systems of interacting neurons. We consider networks of biological neurons where neurons are modeled as integrate-and-fire neurons with escape noise [43], a network model which has already been studied in several mathematical works, see, e.g., $[31,39,27,28,91]$.\n\nFor any network size $N$, each neuron $i \\in\\{1, \\ldots, N\\}$ has a variable $\\boldsymbol{X}^{i ; N}(t) \\in \\mathbb{R}$ that represents its membrane potential. The membrane potential of a neuron $\\boldsymbol{X}^{i ; N}(t)$ determines its instantaneous probability of emitting a spike, that is, its conditional intensity $f\\left(\\boldsymbol{X}^{i ; N}(t-\\right)$ ), through the non-negative and non-decreasing intensity function $f: \\mathbb{R} \\rightarrow \\mathbb{R}_{+}$. Each time neuron $i$ emits a spike, its membrane potential is reset to 0 and the membrane potentials of all other neurons $j \\neq i$ make a jump of $\\frac{1}{N} w_{j, i}^{N}$, respectively, where $w_{j, i}^{N}$ denotes the (rescaled) synaptic weight from neuron $i$ to neuron $j$ (by convention, we put $w_{i, i}^{N}=0$ for all $i$ ). Between spikes, the membrane potential $\\boldsymbol{X}^{i ; N}(t)$ drifts according to the velocity field $b: \\mathbb{R} \\rightarrow \\mathbb{R}$.\n\nMore formally, given the initial conditions $\\left\\{\\boldsymbol{X}^{i ; N}(0)=\\boldsymbol{X}_{0}^{i ; N}\\right\\}_{i=1}^{N}$, the stochastic dynamics of the network model is characterized by the following system of stochastic differential equations (SDEs): For $i \\in\\{1, \\ldots, N\\}$,\n\n$$\n\\begin{aligned}\n\\mathrm{d} \\boldsymbol{X}^{i ; N}(t) & =b\\left(\\boldsymbol{X}^{i ; N}(t)\\right) \\mathrm{d} t+\\frac{1}{N} \\sum_{j=1}^{N} w_{i, j}^{N} \\mathrm{~d} \\boldsymbol{Z}^{j ; N}(t)-\\boldsymbol{X}^{i ; N}(t-) \\mathrm{d} \\boldsymbol{Z}^{i ; N}(t) \\\\\n\\boldsymbol{Z}^{i ; N}(t) & :=\\int_{[0, t] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{i ; N}(s-\\right)\\right)\\right\\}} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z) \\\\\n\\boldsymbol{X}^{i ; N}(0) & =\\boldsymbol{X}_{0}^{i ; N}\n\\end{aligned}\n$$\n\nwhere the $\\left\\{\\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)\\right\\}_{i=1}^{N}$ are independent Poisson random measures on $\\mathbb{R}_{+} \\times \\mathbb{R}_{+}$with intensity $\\mathrm{d} s \\mathrm{~d} z^{1}$ and the initial data $\\left\\{\\boldsymbol{X}_{0}^{i ; N}\\right\\}_{i=1}^{N}$ are $N$ real-valued random variables.\n\nWe consider the pathwise unique $\\left(\\mathscr{F}_{t}\\right)_{t \\in \\mathbb{R}_{+}}$-adapted c\u00e0dl\u00e0g strong solution to (1) in the filtered probability space $\\left(\\Omega, \\mathscr{F},\\left(\\mathscr{F}_{t}\\right)_{t \\in \\mathbb{R}_{+}}, \\mathbb{P}\\right)$, where the Poisson random measures $\\boldsymbol{\\Pi}^{i ; N}$ are independent of $\\mathscr{F}_{0}=\\sigma\\left(\\left\\{\\boldsymbol{X}_{0}^{i ; N}\\right\\}_{i=1}^{N}\\right)$ and where $\\mathscr{F}_{t}=\\mathscr{F}_{0} \\cup \\sigma\\left(\\left\\{\\boldsymbol{\\Pi}^{t}\\right\\}_{i \\in \\mathbb{N}^{*}, B \\in \\mathscr{B}\\left(\\mathbb{R}_{+}\\right)}\\right)\\left(\\mathscr{B}\\left(\\mathbb{R}_{+}\\right)\\right.$ denotes the Borel $\\sigma$-algebra on $\\mathbb{R}_{+}$). Furthermore, we work with the following assumptions on the intensity function $f$ and the velocity field $b$.\n\nAssumptions 1. The intensity function $f: \\mathbb{R} \\rightarrow \\mathbb{R}_{+}$is bounded and continuously differentiable; $f \\in C_{b}^{1}\\left(\\mathbb{R}, \\mathbb{R}_{+}\\right)$. The velocity field $b: \\mathbb{R} \\rightarrow \\mathbb{R}$ is bounded and continuously differentiable; $b \\in C_{b}^{1}(\\mathbb{R})$.\n\nUnder Assumptions 1, the system of SDEs (1) defines a Piecewise-Deterministic Markov Process with bounded variation: The membrane potentials follow the deterministic dynamics $\\frac{\\mathrm{d}}{\\mathrm{d} t} \\boldsymbol{X}^{i ; N}(t)=b\\left(\\boldsymbol{X}^{i ; N}(t)\\right)$ between spikes and the probability of spike emission is bounded by $\\|f\\|_{\\infty}<+\\infty$.\n\nThis work focuses on the behavior of the system (1) as $N \\rightarrow \\infty$, when synaptic weights scale as $\\mathcal{O}(1 / N)$. For clarity, in (1), the $\\mathcal{O}(1 / N)$ scaling of synaptic weights is explicitly indicated by the factor $1 / N$. Consistently, the rescaled synaptic weights $w_{i, j}^{N}$ will be assumed to be of order $\\mathcal{O}(1)$ as $N \\rightarrow \\infty$. The goal of this work is to describe the mean-field limits of the system (1) when no particular assumptions are made on the rescaled synaptic weights $w_{i, j}^{N}$. As explained in [53], because of the $\\mathcal{O}(1 / N)$ scaling of interactions between neurons, the stochastic multi-neuron system (1) is expected to effectively behave as a deterministic system based on the neurons' law (or density function) as $N \\rightarrow \\infty$; this intuition can be seen as a generalization of standard propagation of chaos arguments, which we recall in Section 2.1. The general non-exchangeable system (1) can be reduced to an exchangeable system by assuming that the initial data $\\left\\{\\boldsymbol{X}_{0}^{i ; N}\\right\\}_{i=1}^{N}$ are i.i.d. and rescaled synaptic weights\n\n[^0]\n[^0]:    ${ }^{1}$ See $[58,63]$ for introductions on Poisson random measures.\n\n$w_{i, j}^{N}$ are also i.i.d.; in this case, the mean-field limit of the system can be described by a mean-field PDE with no spatial extension, as proved in [31, 39].\n\nTo give an intuition for the type of mean-field limit we can expect in the case of general non-exchangeable systems, let us consider the example of growing uniform attachment graphs (see [64]) as an example of sequences of random synaptic weights giving rise to nonexchangeable systems. In this example, the synaptic weights $w_{i, j}^{N}$ are generated via an iterative procedure. The procedure starts at $N=1$ with a single neuron. At the $N+1$-th iteration step, a neuron is added to the network of the previous step and is randomly and independently connected to each of the $N$ previous neurons with probability $1 /(N+1)$. Two neurons are said to be connected if $w_{i, j}=w_{j, i}=1$ and not connected if $w_{i, j}=w_{j, i}=0$ (synaptic weights are symmetric). This construction leads to a sequence of non-exchangeable systems since neurons added at the beginning of the procedure tend to have more connections than neurons added at the end. Although the algorithm for generating growing uniform attachment graphs does not rely on an explicit spatial structure, the order in which neurons are added actually defines an implicit spatial structure which makes these graphs equal in law to $W$-random graphs. Indeed, as explained in [64, Example 11.39], for any network size $N$, if the indices $i$ of neurons correspond to the iteration step at which they were added to the network, then each neuron $i$ can be assigned to a location $\\xi_{i}=(i-1) / N$ on the interval $[0,1]$. Then, one can verify that the connections between pairs of distinct neurons are independent $\\{0,1\\}$-valued random variables with expectation $\\mathbb{E}\\left[w_{i, j}^{N}\\right]=1-\\max \\left(\\xi_{i}, \\xi_{j}\\right)$, for all $i \\neq j$.\n\nThe theory of graphons [64], which will be briefly reviewed in Sec. 2.2, tells us that growing uniform attachment graphs converge, as $N \\rightarrow \\infty$, to the limit graphon $(\\xi, \\zeta) \\mapsto$ $1-\\max (\\xi, \\zeta) \\in L^{\\infty}\\left([0,1]^{2}\\right)$, in the topology induced by the cut norm (see [64, Proposition 11.40]). This suggests that, in this example, the dynamics of the system (1) could converge, in the mean-field limit, to the solution to a deterministic equation with a spatial extension given by the interval $[0,1]$. In this example, neurons can be ordered on the interval $[0,1]$ in a way that reveals the implicit spatial structure of the model. However, one can imagine sequences of dense graphs with no apparent spatial structure. In the latter case, the theory of graphons tells us that any sequence of dense graphs converges, up to the extraction of a subsequence, to a limit graphon $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$, in the topology of the cut distance, which is the infimum of the cut norm over all possible orderings of the nodes (neurons) on the interval $[0,1]$ (see Sec. 2.2 for precise statements). This highly nontrivial result from the theory of graphons suggests that, in some appropriate topology, the dynamics of any sequence of dense networks (1) has to converge-up to neuron re-orderings and up to the extraction of a subsequence-to the solution to a deterministic equation with a spatial extension on the interval $[0,1]$ and a limit kernel $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$. Therefore, to fully exploit the theory of graphons and obtain such a general result for non-exchangeable systems, we will consider mean-field limits up to neuron re-orderings and up to the extraction of a subsequence.\n\nFrom a technical point of view, our task will be to identify a topology in which the convergence of the synaptic weights to the limit kernel $w$, as $N \\rightarrow \\infty$, can be well propagated to the dynamics of the system as a whole.\n1.3. Main result: Mean-field limits of non-exchangeable systems. As mentioned in the previous section, to leverage the theory of graphons and obtain a result for general nonexchangeable systems, we need to study mean-field limits up to neuron re-orderings. To allow neurons to be arbitrarily re-ordered, we assign to each neuron $i \\in\\{1, \\ldots, N\\}$ a measurable set of Lebesgue measure $1 / N$ on the interval $[0,1]$ such that the $N$ sets form a partition of $[0,1]$. More specifically, we will use the following notion of almost everywhere partition.\n\nDefinition. A set of measurable sets $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N} \\subset[0,1]^{N}$ is an almost everywhere partition of $\\{1, \\ldots, N\\}$ if, for all $1 \\leq i \\leq N, E^{i ; N}$ has Lebesgue measure $1 / N$ and the\n\nidentity $\\mathbb{1}_{[0,1]}=\\sum_{i=1}^{N} \\mathbb{1}_{E^{i, N}}$ holds almost everywhere on $[0,1]$.\nConveniently, almost everywhere partitions will allow us to deal with neuron re-orderings in an implicit manner. Given an almost everywhere partition $\\left\\{E^{i, N}\\right\\}_{i=1}^{N}$, we define the extended empirical measure of $\\left\\{\\boldsymbol{X}^{i, N}(t)\\right\\}_{i=1}^{N}$ as\n\n$$\n\\boldsymbol{\\mu}^{N}(t, \\xi, \\mathrm{~d} x):=\\sum_{i=1}^{N} \\delta_{\\boldsymbol{X}^{i, N}(t)}(\\mathrm{d} x) \\mathbb{1}_{E^{i, N}}(\\xi), \\quad \\forall t \\geq 0, \\forall \\xi \\in[0,1]\n$$\n\nwhere $\\delta_{X}$ denotes the Dirac measure located in $X$. The auxiliary variable $\\xi \\in[0,1]$ in this definition will play the same role as the auxiliary variable $\\xi$ in the theory of graphons (see Sec. 2.2).\n\nInformally, our main result, Theorem 1, states that, up to neuron re-orderings (which are taken care of by the almost everywhere partitions), the extended empirical measures $\\boldsymbol{\\mu}^{N}$ of the non-exchangeable system (1) converges in the mean-field limit $N \\rightarrow \\infty$, in a weak-* sense and up to the extraction of a subsequence, to the solution $\\mu$ to the spatially-extended mean-field PDE\n\n$$\n\\partial_{t} \\mu(t, \\xi, \\mathrm{~d} x)+\\partial_{x}[(b(x)+h(t, \\xi)) \\mu(t, \\xi, \\mathrm{~d} x)]+f(x) \\mu(t, \\xi, \\mathrm{~d} x)-r(t, \\xi) \\delta_{0}(\\mathrm{~d} x)=0\n$$\n\n(3b)\n\n$$\n\\begin{aligned}\n& r(t, \\xi)=\\int_{\\mathbb{R}} f(x) \\mu(t, \\xi, \\mathrm{~d} x), \\quad h(t, \\xi)=\\int_{[0,1]} w(\\xi, \\zeta) r(t, \\zeta) \\mathrm{d} \\zeta \\\\\n& \\mu(0, \\xi, \\mathrm{~d} x)=\\mu_{0}(\\xi, \\mathrm{~d} x)\n\\end{aligned}\n$$\n\nwhere $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$ is a limit kernel. The PDE (3) can be seen a spatially-extended version of the mean-field PDE rigorously derived by De Masi, Galves, L\u00f6cherbach, and Presutti [31] in the case of exchangeable systems of integrate-and-fire neurons with escape noise. In (3b), $r(t, \\xi)$ represents the firing rate of neurons at \"location\" $\\xi$, and $h(t, \\xi)$ represents the postsynaptic input received by neurons at location $\\xi$. Importantly, the result does not require the networks (1) to have a prescribed spatial structure as it applies to general non-exchangeable systems with $\\mathcal{O}(1 / N)$ synaptic weight scaling. This generality implies that, in a the meanfield limit, the dynamics of large networks of neurons with $\\mathcal{O}(1 / N)$ synaptic weight scaling can always be described by a deterministic PDE with spatial extension over the interval $[0,1]$.\n\nGeneral notations for function spaces. The function space $C_{b}(\\mathbb{R})$ denotes the space of continuous bounded functions in $\\mathbb{R}$, equipped with the uniform norm, and $C_{b}^{k}(\\mathbb{R})$ denotes the subspace of $C_{b}(\\mathbb{R})$ with continuous and bounded derivatives up to the $k$-th order. The function space $C([0, t])$ denotes the space of continuous functions on the finite time interval $[0, t]$. Lebesgue and Sobolev spaces are denoted $L^{p}$ and $W^{s, p}$ respectively.\n\nWe denote by $\\mathcal{M}(\\mathbb{R})$ the space of signed Borel measures with bounded total variation norm on $\\mathbb{R}, \\mathcal{M}_{+}(\\mathbb{R})$ the subspace of non-negative measures, and $\\mathcal{P}(\\mathbb{R})$ the subspace probability measures. As for the choice of the topology on spaces of measures, we will use the standard notion of weak-* topology and use $\\xrightarrow{*}$ to denote the weak-* convergence.\n\nTHEOREM 1. Grant Assumptions 1. Let $\\left\\{\\boldsymbol{X}^{i, N}\\right\\}_{i=1}^{N}, N \\rightarrow \\infty$, be a sequence of solutions to (1) with synaptic weight matrices $\\left\\{w_{i, j}^{N}\\right\\}_{i, j=1}^{N}$ satisfying the uniform boundedness condition\n\n$$\n\\sup _{N} \\max _{1 \\leq i, j \\leq N}\\left|w_{i, j}^{N}\\right|<\\infty\n$$\n\nMoreover, assume that the initial data $\\left\\{\\boldsymbol{X}_{0}^{i ; N}\\right\\}_{i=1}^{N}$ are independent random variables. Finally, assume the moment bound for the initial data\n\n$$\n\\sup _{N} \\sup _{1 \\leq i \\leq N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{0}^{i ; N}\\right|^{2}\\right]<\\infty\n$$\n\nThen, there exist a limit kernel $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$, a solution $\\mu \\in L^{\\infty}\\left(\\left[0, t_{*}\\right] \\times[0,1] ; \\mathcal{M}(\\mathbb{R})\\right)$ to the PDE (3) in the sense of characteristics for all $t_{*}>0$, and almost everywhere partitions $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}$ such that the sequence $\\left\\{\\boldsymbol{\\mu}^{N}\\right\\}$ as defined in (2) converges, up to the extraction of a subsequence, to $\\mu$ in the following sense: For any $t \\in\\left[0, t^{*}\\right]$,\n\n$$\n\\boldsymbol{\\mu}^{N}(t, \\cdot, \\cdot) \\stackrel{*}{\\rightarrow} \\mu(t, \\cdot, \\cdot) \\text { in } \\mathcal{M}([0,1] \\times \\mathbb{R}) \\quad \\text { a.s., } \\quad \\text { as } N \\rightarrow \\infty\n$$\n\nThe exact definition of the weak-* convergence above is presented in Section 2.3, where we also construct the topology of weak-* convergence through quantitative norms. (Using such a constructed weak norm, the convergence in (6) can actually be re-stated as a uniform-in-time convergence on the finite interval $\\left[0, t_{*}\\right]$; see Theorem 2 in Sec. 2.3.)\n\nNote that the weak-* convergence $\\boldsymbol{\\mu}^{N} \\xrightarrow{*} \\mu$ also includes the weak-* convergence of the extended empirical measure at time 0 , which is nontrivial because the sequences of partitions $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}$ we use. Intuitively, the sequence of partitions $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}$ re-order the \"locations\" of the neurons on $[0,1]$ for each network size $N$, as $N \\rightarrow \\infty$. Therefore, we need to work with a notion of convergence that is weak enough in the $\\xi$-dimension so that the extended empirical measure at time $t=0$ converges despite the re-orderings caused by the sequences of partitions $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}$.\n\nImportantly, the convergence of the extended empirical measure $\\boldsymbol{\\mu}^{N}$ in the mean-field limit, established in Theorem 1, keeps track of individual neuron trajectories $\\boldsymbol{X}^{i ; N}(t)$, as stated in the following corollary.\n\nCOROLLARY 1. Grant all the assumptions and results of Theorem 1. For all $N \\geq$ 1 , let $\\left\\{\\widetilde{\\boldsymbol{X}}^{i ; N}(t)\\right\\}_{i=1}^{N}$ be the auxiliary processes where the postsynaptic input terms $\\frac{1}{N} \\sum_{j=1}^{N} w_{i, j}^{N} \\mathrm{~d} \\boldsymbol{Z}^{j ; N}(t)$ are replaced by the respective mean-field inputs, namely\n\n$$\n\\begin{aligned}\n\\mathrm{d} \\widetilde{\\boldsymbol{X}}^{i ; N}(t)= & b\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(t)\\right) \\mathrm{d} t+\\int_{[0,1]^{2}} \\mathbb{1}_{E^{i ; N}}(\\xi) w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(y) \\mu(t, \\zeta, y) \\mathrm{d} y \\mathrm{~d} \\zeta \\mathrm{~d} \\xi \\\\\n& -\\widetilde{\\boldsymbol{X}}^{i ; N}(t-) \\mathrm{d} \\widetilde{\\boldsymbol{Z}}^{i ; N}(t)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n\\widetilde{\\boldsymbol{Z}}^{i ; N}(t) & :=\\int_{[0, t] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\{z \\leq J\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\}} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z) \\\\\n\\widetilde{\\boldsymbol{X}}^{i ; N}(0) & =\\boldsymbol{X}_{0}^{i ; N}\n\\end{aligned}\n$$\n\nThen, for all $t_{*} \\geq 0$,\n\n$$\n\\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\sup _{t \\in\\left[0, t_{*}\\right]} \\min \\left(\\left|\\boldsymbol{X}^{i ; N}(t)-\\widetilde{\\boldsymbol{X}}^{i ; N}(t)\\right|, 1\\right)\\right] \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$\n\nThe fine tracking of individual neuron trajectories in the mean-field limit is one of the main advantages of the proof method presented in this work, compared to method of macroscopic \"observables\" used in [54] (see Sec. 1.4). Note that the auxiliary processes $\\widetilde{\\boldsymbol{X}}^{i ; N}$ in Corollary 1 do not form a closed system since (7) involves the limit empirical measure $\\mu$.\n\n1.4. Previous works. Mean-field models have played a major role in shaping our understanding of emergent behaviors in large networks of interacting neurons. We refer the interested reader to [44, 93, 94, 49] for recent and comprehensive textbooks covering different types of mean-field theories in neuroscience. Although mean-field theory in neuroscience is a vast topic, in the following, we restrict our survey of previous works to exact mean-field models for networks of integrate-and-fire-type neurons, with an emphasis on the mathematical literature most relevant to the present work. For recent results on the replica-mean-field limit of networks of neurons, a distinct notion of limit which will not be covered here, we refer the reader to $[30,3]$ and references therein.\n\nExact mean-field models for networks of integrate-and-fire-type neurons can be broadly classified into two classes, depending on how the intrinsic noise of neurons is modeled: Neuronal noise can be modeled with \"escape noise\", i.e., a soft, probabilistic firing threshold, or it can be modeled with diffusive noise in the membrane potential dynamics. Mean-field models with escape noise were introduced in theoretical neuroscience by [45, 42, 43] (see also [44, Chaps. 9 \\& 14], [40] and [90, Chap. 1.2] for introductions). The convergence of homogeneous networks, i.e., exchangeable systems, in the mean-field limit to a nonlinear population equation was proved in [31, 39], and the dynamics of the limit equation has been analyzed in [88, 27, 28, 26]. Integrate-and-fire neurons with escape noise are, from a modeling point of view, very similar to age-dependent Hawkes processes (see [90, p. 10]). The convergence of exchangeable systems of interacting age-dependent Hawkes processes to a mean-field limit was proved in [22] and this convergence proof has since been generalized to the case of multiple interacting populations of neurons [87] and to the case of neurons with additional short-term memory variables [89]. The dynamics of the limit mean-field equation, known as the Time Elapsed Neuron Network Model, has been analyzed in [83, 84, 74, 75, 17] (see also $[85,95,38]$ for works on the dynamics of generalized versions of the original limit equation). Concurrently, mean-field models with diffusive noise were introduced in theoretical neuroscience by Brunel and collaborators [14, 13, 82], the convergence of exchangeable systems to a mean-field limit was proved in [32, 51], and the dynamics of the limit mean-field equationknown as the Nonlinear Noisy Leaky Integrate and Fire (NNLIF) model-has been analyzed in $[15,18,16,19,33]$.\n\nThe mean-field limit of spatially-structured networks of integrate-and-fire neurons, which are examples of non-exchangeable systems, has received much less attention in mathematical neuroscience (but see [54]). However, many works have studied the mean-field limit of spatially-structured networks of neurons that are not of integrate-and-fire type: The case of Kuramoto oscillators was treated in [72, 71, 57, 80], the cases of FitzHugh-Nagumo or Hodgkin-Huxley neurons in [96, 69, 29, 73, 68], and the case of nonlinear Hawkes processes in [23, 1]. Our general non-exchangeable system model, presented in Sec. 1.2, differs from the models considered in the aforementioned works in two respects. First, we do not assume any prescribed spatial structure; and second, we consider integrate-and-fire-type neurons, which have discontinuous dynamics that poses specific mathematical difficulties.\n\nOur result and proof method mainly build on three recent works. (I) Our result is a generalization of the mean-field convergence proof for exchangeable systems of interacting integrate-and-fire neurons with escape noise [39] to the case of non-exchangeable systems. Notably, we use the same probabilistic modeling framework as in [39], which involves Poisson random measures, and our neuron model is almost identical to theirs. Also, our treatment of individual neuron trajectories in the limit (Corollary 1) uses some of their methods. (II) Our result is complementary to another work by two of the authors [54], where a recent method for proving the convergence in the mean-field limit of non-exchangeable systems [53] is applied to networks of integrate-and-fire neurons. In [54], it is proven that non-exchangeable systems of interacting integrate-and-fire neurons with both diffusive noise and escape noise\n\n(soft threshold) converge to a spatially-extended mean-field limit. While the result in [54] is closely related to the result presented here, there is an important difference that makes the two results complementary: In [54], the mean-field convergence is not proven through the empirical measure of the networks but through a tree-indexed hierarchy of macroscopic \"observables\" which can be seen as a generalization of the classical BBGKY hierarchy for exchangeable systems. The advantage of the approach in [54] is that it enables the study the mean-field limit of networks with synaptic weight scalings that are more general than $\\mathcal{O}(1 / N)$ synaptic weight scaling. However, the disadvantage of the \"observables\" of [54] is that, by their very definition, they destroy all information about the dynamics of individual neurons or subsets of neurons. In contrast, the spatially-extended empirical measure we consider here keeps track of the dynamics of individual neurons and subsets of neurons, offering a much more detailed and transparent view of the system as we take the limit. The cost of working directly on the empirical measure is that we have to require synaptic weights to scale as $\\mathcal{O}(1 / N)$, which means that our result only applies to the limit of dense networks. Interestingly, diffusive noise is necessary for the proof method in [54], whereas the proof presented here relies on the absence of diffusive noise, which adds to the complementary nature of the two works. (III) The weak-* convergence of the empirical measure of exchangeable systems of interacting integrate-and-fire neurons was studied in [37] (using a slightly different neuron model). We will adapt the notion of weak-* convergence of the empirical measure to the case of non-exchangeable systems; in particular, we will show that in the case of non-exchangeable systems, a spatially-extended empirical measure is the natural object to consider.\n\nWe end this section by briefly outlining the general mathematical context of the use of graph limits for the study of limit nonlinear dynamics in large systems. The two extremes of the theory of graph limits are, on the one hand, the theory of graphons for dense graphs $[65,64]$, i.e., graphs with degrees growing linearly with the number of nodes $N$ as $N \\rightarrow \\infty$, and, on the other hand, the Benjamini-Schramm local convergence theory for sparse graphs [6], i.e., graphs with finite degrees as $N \\rightarrow \\infty$. In between, there are various graph limit theories for graphs with intermediate sparsity, e.g., $L^{p}$ graphons [8, 9], extended graphons [53], and graphops [4] (note that graphops also apply to the limit of some specific sparse graphs). These different graph limits can then be applied to the study of the limit nonlinear dynamics of systems as the number of agents or particles tends to infinity. The theory of graphons for dense graphs was first applied to the mean-field limit of deterministic systems, notably in [72, 71, 57, 24, 86], and then to the mean-field limit of interacting diffusions, e.g., in [68], and of interacting point processes [1]. At the other end of the sparsity spectrum, the BenjaminiSchramm local convergence for sparse graphs was used to characterize the limit nonlinear dynamics of interacting diffusions; see, for instance, [70, 81, 59, 60, 61]. For systems interacting on graphs of intermediate sparsity, the mean-field limit of deterministic systems has been studied in [56, 53, 46], of interacting diffusions in [80, 25], and, as mentioned above, of networks of integrate-and-fire-type neurons in [54].",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 2. Preliminaries. \n\n2.1. Kinetic theory formalism for the mean-field limit of the empirical measure. Most methods currently used in mathematical neuroscience to study mean-field limits of networks of neurons have their origin in mathematical kinetic theory, a field initiated by Kac who first introduced the notion of propagation of chaos in a seminal paper [55]. Propagation of chaos is a pivotal notion that allows us to link the microscopic dynamics of interacting particles to mean-field PDEs describing the evolution of the density of particles, as the number of particles $N$ tends to infinity. In very simple terms, a system of interacting particles (or agents,\n\nneurons) satisfy propagation of chaos if, as $N \\rightarrow \\infty$, particles behave like independent processes with the same law (see [20, 21] for a recent and comprehensive two-part review). In the case of exchangeable systems-systems where the joint law of particles is invariant with respect to permutations of particles-mean-field limits and propagation of chaos, are practically synonymous. However, in the case of non-exchangeable systems, e.g., spatiallystructured networks, there can be mean-field limits without propagation of chaos (see references in the previous sections). Here, we say that a system converges to a mean-field limit if, as $N \\rightarrow \\infty$, its dynamics can be described by a deterministic equation where each particle interacts with an infinite number of other particles.\n\nSince deterministic methods will play an important role in this work, we briefly recall below a formalism used in kinetic theory to study the convergence of the empirical measure of deterministic systems [10,35]. This formalism will guide our proof strategy. The following exposition is adapted from [52, 47], where deterministic methods for the study of mean-field limits are reviewed.\n\nLet us consider the toy model described by the following system of $N$ ordinary differential equations: For $i \\in\\{1, \\ldots, N\\}$,\n\n$$\n\\frac{\\mathrm{d}}{\\mathrm{~d} t} X^{i ; N}(t)=b\\left(X^{i ; N}(t)\\right)+\\frac{w_{0}}{N} \\sum_{j=1}^{N} f\\left(X^{j ; N}(t)\\right), \\quad X^{i ; N}(0)=X_{0}^{i ; N}\n$$\n\nThe toy model (8) is deterministic and, in the context of neural networks, can be interpreted as a network of $N$ rate-units with uniform synaptic weights $w_{0} / N$. If the initial data $\\left\\{\\boldsymbol{X}_{0}^{i ; N}\\right\\}_{i=1}^{N}$ are assumed to be i.i.d. with law $\\rho_{0}(\\mathrm{~d} x)$ (in this work, random variables and random functions are in bold), the system (8) becomes an exchangeable system, and it is a well-known result that the empirical measure\n\n$$\n\\boldsymbol{\\rho}^{N}(t, \\mathrm{~d} x)=\\frac{1}{N} \\sum_{i=1}^{N} \\delta_{\\boldsymbol{X}^{i ; N}(t)}(\\mathrm{d} x), \\quad \\forall t>0\n$$\n\nconverges the the solution $\\rho(t, \\mathrm{~d} x)$ to the mean-field PDE\n\n$$\n\\partial_{t} \\rho(t, \\mathrm{~d} x)+\\partial_{x}\\left[\\left(b(x)+w_{0} \\int_{\\mathbb{R}} f(y) \\rho(t, \\mathrm{~d} y)\\right) \\rho(t, \\mathrm{~d} x)\\right]=0, \\quad \\rho(0, \\mathrm{~d} x)=\\rho_{0}(\\mathrm{~d} x)\n$$\n\nDeterministic methods offer an elegant way to quantify the convergence of the random empirical measure $\\boldsymbol{\\rho}^{N}$ to the deterministic solution $\\rho$ to the mean-field PDE (9). First, going back to deterministic initial data $\\left\\{X_{0}^{i ; N}\\right\\}_{i=1}^{N}$ in the system (8), let us notice that the deterministic empirical measure $\\rho^{N}$ given by the system (8) for initial data $\\left\\{X_{0}^{i ; N}\\right\\}_{i=1}^{N}$ is already a solution to the mean-field PDE (9), in the sense of distributions, if the initial datum $\\rho_{0}(\\mathrm{~d} x)$ is replaced by the empirical measure $\\rho_{0}^{N}(\\mathrm{~d} x):=\\frac{1}{N} \\sum_{i=1}^{N} \\delta_{\\boldsymbol{X}_{0}^{i ; N}}(\\mathrm{~d} x)$. This observation suggests that proving the convergence of the empirical measure to the mean-field limit can be reduced to a deterministic problem: Can we find an appropriate weak metric on the space of probability measure $\\mathcal{P}(\\mathbb{R})$ such that the distance between the empirical measure $\\rho^{N}(t, \\mathrm{~d} x)$ and the limit $\\rho(t, \\mathrm{~d} x)$ at time $t>0$ can be controlled by the distance between $\\rho^{N}(0, \\mathrm{~d} x)$ and $\\rho_{0}(\\mathrm{~d} x)$ ?\n\nAn example of such weak metric is the negative Sobolev norm $W^{-1,1}(\\mathbb{R})$, defined as\n$\\left\\|\\rho_{1}-\\rho_{2}\\right\\|_{W^{-1,1}(\\mathbb{R})}:=\\sup _{\\|\\phi\\|_{W^{1, \\infty}(\\mathbb{R})} \\leq 1} \\int_{\\mathbb{R}} \\phi(x)\\left(\\rho_{1}(\\mathrm{~d} x)-\\rho_{2}(\\mathrm{~d} x)\\right), \\forall\\left(\\rho_{1}, \\rho_{2}\\right) \\in \\mathcal{M}(\\mathbb{R}) \\times \\mathcal{M}(\\mathbb{R})$,\n\nwhere $W^{1, \\infty}(\\mathbb{R})$ denotes the Sobolev space with $k=1$ and $p=\\infty$ and $\\mathcal{M}(\\mathbb{R})$ denotes the space of signed Borel measures with finite total variation norm on $\\mathbb{R} .{ }^{1}$ The weak norm $W^{-1,1}(\\mathbb{R})$ is convenient for two reasons. First, it metrizes the weak-* convergence of probability measures on $\\mathbb{R}$, in the sense that, given a sequence of probability measures $\\left\\{\\varrho^{N}\\right\\}$, $\\left\\|\\varrho^{N}-\\varrho\\right\\|_{W^{-1,1}(\\mathbb{R})} \\rightarrow 0$ as $N \\rightarrow \\infty$ if and only if $\\varrho^{N} \\xrightarrow{*} \\varrho$ ( $\\stackrel{*}{\\sim}$ denotes weak-* convergence) and if the sequence $\\left\\{\\varrho^{N}\\right\\}$ is tight (which is typically guaranteed via some uniform moment bound). ${ }^{2}$ Second, the weak norm $W^{-1,1}(\\mathbb{R})$ can be used to obtain a stability estimate for the mean-field PDE (9),\n\n$$\n\\left\\|\\rho^{N}(t, \\cdot)-\\rho(t, \\cdot)\\right\\|_{W^{-1,1}(\\mathbb{R})} \\leq C(t)\\left\\|\\rho_{0}^{N}-\\rho_{0}\\right\\|_{W^{-1,1}(\\mathbb{R})}\n$$\n\nwhere $C(t)$ is a function depending on $\\|b\\|_{W^{1, \\infty}}$ and $\\|f\\|_{W^{1, \\infty}}$. This stability estimate indeed implies that, taking i.i.d initial data $\\left\\{\\boldsymbol{X}_{0}^{t / N}\\right\\}_{t=1}^{N}$ with law $\\rho_{0}$, the convergence of the random empirical measure $\\boldsymbol{\\rho}^{N}(t, \\cdot)$ to the deterministic $\\rho(t, \\cdot)$ at time $t>0$ is directly controlled by the convergence of the initial datum $\\boldsymbol{\\rho}_{0}^{N}$ to the law $\\rho_{0}$ (which only require some moment bound).\n\nThe main arguments for the proof of the stability estimate (10) can be found, e.g., in [52, Theorem 2.2]. Crucially, the proof involves the propagation of the regularity of the test function $\\tilde{\\phi} \\in W^{1, \\infty}(\\mathbb{R})$ along the dual-backward equation\n\n$$\n\\partial_{t} \\phi(t, x)+\\left(b(x)+w_{0} \\int_{\\mathbb{R}} f(y) \\rho(t, \\mathrm{~d} y)\\right) \\partial_{x} \\phi(t, x)=0, \\quad \\phi(t, x)=\\tilde{\\phi}(x)\n$$\n\nTo some extent, the proof of our main result is inspired by this formalism. Notably, the proof will also require a propagation of regularity along the dual-backward equation of a mean-field PDE. Of course, the toy model (8) is much simpler than the network model (1), which is non-exchangeable and involves stochastic jumps. Therefore, the non-exchangeable system of interacting neurons model (1) will require a much more general framework, as well as a combination of deterministic and probabilistic methods.\n2.2. Limits of dense synaptic weight matrices. The auxiliary variable $\\xi \\in[0,1]$ we use in the definition of the extended empirical measure (2) is motivated by the theory of graphons [64]. This theory provides a topological framework for studying the limits of large dense graphs and comes with a standard representation for graphs and their large size limits.\n\nThe theory of graphons is primarily concerned with the study of the large size limits of simple graphs that are undirected and unweighted. A measurable function $w: I \\times I \\rightarrow \\mathbb{R}$, where $I=[0,1]$, is a graphon if it is symmetric, non-negative and bounded by 1 . Let $I^{1 ; N}, \\ldots, I^{N ; N}$ be intervals of equal length that form a partition of $I=[0,1]$. A simple graph $G$ with $N$ nodes can be associated with a graphon $W_{G}$ defined as\n\n$$\nW_{G}(\\xi, \\zeta):= \\begin{cases}1 & \\text { if }(\\xi, \\zeta) \\in I^{i ; N} \\times I^{j ; N} \\text { and }(i, j) \\text { is an edge of } G \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nThis definition can be extended to graphs that are directed and weighted. Dropping the requirement for symmetry and non-negativity, a measurable function $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$ is referred to as digraphon in the graphon theory literature. A weight matrix $\\left\\{w_{i, j}^{N}\\right\\}_{i, j=1}^{N} \\in \\mathbb{R}^{N \\times N}$\n\n[^0]\n[^0]:    ${ }^{1}$ Note that this negative Sobolev norm is similar, but not equivalent, to the Wasserstein distance $W_{1}(\\mathbb{R})$\n\n    $$\n    \\left\\|\\rho_{1}-\\rho_{2}\\right\\|_{W_{1}(\\mathbb{R})}:=\\sup _{\\|\\phi\\|_{\\operatorname{Lip}(\\mathbb{R})} \\leq 1} \\int_{\\mathbb{R}} \\phi(x)\\left(\\rho_{1}(\\mathrm{~d} x)-\\rho_{2}(\\mathrm{~d} x)\\right), \\quad \\forall\\left(\\rho_{1}, \\rho_{2}\\right) \\in \\mathcal{P}(\\mathbb{R}) \\times \\mathcal{P}(\\mathbb{R})\n    $$\n\n    which more commonly used in the study of mean-field limits; cf. [92, 52, 47].\n    ${ }^{2}$ For more detailed statements, we refer the reader to [52, Sec. 3.3].\n\nis associated with the digraphon $w^{I ; N} \\in L^{\\infty}\\left([0,1]^{2}\\right)$, defined as\n\n$$\nw^{I ; N}(\\xi, \\zeta):=\\sum_{i, j=1}^{N} w_{i, j}^{N} \\mathbb{1}_{I^{i, N}}(\\xi) \\mathbb{1}_{I^{j, N}}(\\zeta)\n$$\n\nGraphons and digraphons can be considered as $L^{p}([0,1]) \\rightarrow L^{q}([0,1])$ operators for any $1 \\leq p, q \\leq \\infty$. One of the essential findings of graphon theory is that the most convenient norm for studying dense graph limits is, arguably, the operator norm $\\|\\cdot\\|_{L^{\\infty}([0,1]) \\rightarrow L^{1}([0,1])}$, which is weaker that than the $L^{p}$ norms $\\|\\cdot\\|_{L^{p}([0,1]^{2})}$. It is important to mention that the operator norm $\\|\\cdot\\|_{L^{\\infty}([0,1]) \\rightarrow L^{1}([0,1])}$ is equivalent to the cut norm $\\|\\cdot\\|_{\\square}$ more commonly used in graphon theory, as explained in [64, Chapter 8].\n\nWhen considering graphs with unlabeled nodes, the order of the nodes is arbitrary. Similarly, considering our non-exchangeable system (1), for any permutation $\\sigma$ of the neuron indices $\\{1, \\ldots, N\\},\\left\\{\\boldsymbol{X}^{\\sigma(i) ; N}\\right\\}_{i=1}^{N}$ is a solution of (1) with weight matrix $\\left\\{w_{\\sigma(i), \\sigma(j)}^{N}\\right\\}_{i, j=1}^{N}$ if and only if $\\left\\{\\boldsymbol{X}^{i ; N}\\right\\}_{i=1}^{N}$ is a solution of (1) with weight matrix $\\left\\{w_{i, j}^{N}\\right\\}_{i, j=1}^{N}$. This motivates the definition of a distance between graphs (or systems) up to node (or neuron) re-orderings. Such a distance is given by the cut distance $\\delta_{\\square}$, which is the infimum of the operator norm $\\|\\cdot\\|_{L^{\\infty}([0,1]) \\rightarrow L^{1}([0,1])}$ over all possible re-orderings:\n\n$$\n\\delta_{\\square}\\left(W_{1}, W_{2}\\right):=\\inf _{\\psi}\\left\\|W_{1}^{\\psi}-W_{2}\\right\\|_{L^{\\infty} \\rightarrow L^{1}}, \\quad \\forall\\left(W_{1}, W_{2}\\right) \\in L^{\\infty}\\left([0,1]^{2}\\right)^{2}\n$$\n\nwhere $\\psi$ ranges over all measure-preserving maps from $I$ to $I$, and, for any measurepreserving map $\\psi, W^{\\psi}(\\xi, \\zeta)=W(\\psi(\\xi), \\psi(\\zeta))$. Actually, the cut distance $\\delta_{\\square}$ is only a pseudo-distance since any $W \\in L^{\\infty}\\left([0,1]^{2}\\right)$ has distance 0 with its \"re-ordered\" version $W^{\\psi}$.\n\nThe cut distance $\\delta_{\\square}$ is remarkable because is leads to a compactness result on the space of equivalent classes it induces.\n\nLEMMA 1. For any sequence $W^{N}, N \\rightarrow \\infty$, of elements in $L^{\\infty}\\left([0,1]^{2}\\right)$ satisfying the uniform boundedness condition $\\sup _{N}\\left\\|W^{N}\\right\\|_{L^{\\infty}}<\\infty$, we can extract a subsquence $\\left\\{W^{n(N)}\\right\\}_{N=1}^{\\infty}$ such that there exists a digraphon $W \\in L^{\\infty}\\left([0,1]^{2}\\right)$ such that $\\delta_{\\square}\\left(W^{n(N)}, W\\right) \\rightarrow 0$ as $N \\rightarrow \\infty$.\n\nThe proof of the lemma, in the case of standard graphons, can be found, for example, in [64, Chapter 9]; the proof in the more general case of digraphons and their extensions can be found in $[66,67]$.\n\nFor a sequence of digraphons corresponding to weight matrices, the following lemma is a straightforward consequence of Lemma 1.\n\nLemma 2. For any sequence $\\left\\{w_{i, j}^{N}\\right\\}_{1 \\leq i, j \\leq N}, N \\rightarrow \\infty$, of weight matrices with increasing sizes $N \\times N$ satisfying the uniform boundedness condition\n\n$$\n\\sup _{N} \\max _{1 \\leq i, j \\leq N}\\left|w_{i, j}^{N}\\right|<\\infty\n$$\n\nwe can extract a subsequence $\\left\\{w_{i, j}^{n(N)}\\right\\}_{1 \\leq i, j \\leq n(N)}$ such that there exist a digraphon $w \\in$ $L^{\\infty}\\left([0,1]^{2}\\right)$ and almost everywhere partitions $\\left\\{\\left\\{E^{i ; n(N)}\\right\\}_{i=1}^{n(N)}\\right\\}_{N=1}^{\\infty}$ such that the digraphons\n\n$$\nw^{n(N)}(\\xi, \\zeta):=w^{E ; n(N)}(\\xi, \\zeta)=\\sum_{i, j=1}^{n(N)} w_{i, j}^{n(N)} \\mathbb{1}_{E^{i ; n(N)}}(\\xi) \\mathbb{1}_{E^{j ; n(N)}}(\\zeta)\n$$\n\nsatisfy $\\left\\|w^{n(N)}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}} \\rightarrow 0$ as $N \\rightarrow \\infty$.\n\n2.3. Defining a kernel-dependent weak metric for the empirical measure. Passing to the limit in the weights $w_{i, j}^{N}$ to obtain a limit kernel $w$ as $N \\rightarrow \\infty$ is only the first step towards passing to the limit in the empirical measure, $\\boldsymbol{\\mu}^{N} \\xrightarrow{*} \\mu$. Our proof strategy relies on identifying a proper metric for measures allowing us to pass to the limit from the non-exchangeable system (1) with weights $w^{N}$ to the limit mean-field PDE (3) with the kernel $w$. To this end, the metric should correspond to the weak-* convergence discussed in Section 2.1 and the form of the limit equation (3). Note that the naive choice of the $W^{-1,1}([0,1] \\times \\mathbb{R})$-norm does not appear to work as proving the propagation of this norm seems to require the kernel $w$ to be Lipschitz, while we only assume $w$ to be in $L^{\\infty}\\left([0,1]^{2}\\right)$.\n\nWe introduce a metric that is adapted to the limit kernel $w$. For any $\\mu_{1}, \\mu_{2} \\in$ $L^{\\infty}([0,1] ; \\mathcal{M}(\\mathbb{R}))$, we define the $\\Phi_{w}^{-1}$ metric between them through the duality formula\n\n$$\n\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{\\Phi_{w}^{-1}}:=\\sup _{\\|\\phi\\|_{\\Phi_{w}} \\leq 1} \\int_{[0,1] \\times \\mathbb{R}} \\phi(\\xi, x)\\left(\\mu_{1}(\\xi, x)-\\mu_{2}(\\xi, x)\\right) \\mathrm{d} \\xi \\mathrm{~d} x\n$$\n\n(11b)\n\n$$\n\\begin{aligned}\n\\|\\phi\\|_{\\Phi_{w}}:= & \\max \\left(\\|\\phi\\|_{L_{\\xi, x}^{\\infty}},\\left\\|\\partial_{x} \\phi\\right\\|_{L_{\\xi}^{\\infty} L_{x}^{1}},\\left\\|\\partial_{x} \\phi\\right\\|_{L_{\\xi, x}^{\\infty}}\\right. \\\\\n& \\left.\\inf \\left\\{C>0: \\forall h \\in(-1,1), \\int_{[0,1]} \\sup _{x \\in \\mathbb{R}} \\mid \\phi(\\xi-h, x)-\\phi(\\xi, x) \\mid \\mathrm{d} \\xi \\leq C \\epsilon_{w}(|h|)\\right\\}\\right)\n\\end{aligned}\n$$\n\nwhere $\\epsilon_{w}:[0,1] \\rightarrow[0, \\infty)$ is a non-decreasing function providing a modulus of continuity of $w$ in $L^{1}$ :\n\n$$\n\\begin{aligned}\n& \\lim _{r \\rightarrow 0^{+}} \\epsilon_{w}(r)=\\epsilon_{w}(0)=0 \\\\\n& \\int_{[0,1]^{2}}|w(\\xi-h, \\zeta)-w(\\xi, \\zeta)| \\mathrm{d} \\zeta \\mathrm{~d} \\xi \\leq \\epsilon_{w}(|h|), \\quad \\forall h \\in \\mathbb{R} \\\\\n& \\int_{[0,1]^{2}}|w(\\xi, \\zeta-h)-w(\\xi, \\zeta)| \\mathrm{d} \\zeta \\mathrm{~d} \\xi \\leq \\epsilon_{w}(|h|), \\quad \\forall h \\in \\mathbb{R}\n\\end{aligned}\n$$\n\nIn (11a), we make a small abuse of notation, writing $\\left(\\mu_{1}(\\xi, x)-\\mu_{2}(\\xi, x)\\right) \\mathrm{d} \\xi \\mathrm{d} x$ instead of $\\left(\\mu_{1}(\\mathrm{~d} \\xi, \\mathrm{~d} x)-\\mu_{2}(\\mathrm{~d} \\xi, \\mathrm{~d} x)\\right)$. This abuse of notation will be useful to indicate the order of integration.\n\nIn (11c), $\\xi-h$ may exit the domain $[0,1]$. To address this, we adopt the following convention: Any $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$ is identified with its natural periodic extension to $L^{\\infty}\\left(\\mathbb{T}^{2}\\right)$, where $\\mathbb{T}^{2}$ denotes the torus in dimension 2 . Similarly, any $\\mu \\in L^{\\infty}\\left(\\left[0, t_{*}\\right] \\times[0,1] ; \\mathcal{M}(\\mathbb{R})\\right)$ is extended to $L^{\\infty}\\left(\\left[0, t_{*}\\right] \\times \\mathbb{T} ; \\mathcal{M}(\\mathbb{R})\\right)$, where $\\mathbb{T}$ denotes the torus in dimension 1 (the circle). This convention is to be applied to all subsequent functions involving the variable $\\xi \\in[0,1]$.\n\nThe theory of $L^{p}$ spaces tells us that for any $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$, there always exists a modulus of continuity $\\epsilon_{w}$ satisfying (11c). ${ }^{3}$ This can be interpreted as a weak form of regularity, adapted to the particular choice of $w$, which is nevertheless stronger than mere $L^{\\infty}$ regularity. Exploiting this slightly stronger regularity, we will be able to obtain a stability analysis with respect to the $\\Phi_{w}^{-1}$ metric between the system (1) and the limit equation (3).\n\nTHEOREM 2. Grant Assumptions 1. Let $\\left\\{\\boldsymbol{X}^{i ; N}\\right\\}_{i=1}^{N}, N \\rightarrow \\infty$, be a sequence of solutions to (1) with weight matrices $\\left\\{w_{i, j}^{N}\\right\\}_{i, j=1}^{N}$ satisfying the uniform boundedness condition (4) and\n\n[^0]\n[^0]:    ${ }^{3}$ Indeed, $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$ implies $w \\in L^{1}\\left([0,1]^{2}\\right)$, and the space continuous functions $C\\left([0,1]^{2}\\right)$ is dense in $L^{1}\\left([0,1]^{2}\\right)$.\n\nthe uniform a priori moment bound\n\n$$\n\\sup _{N} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\eta\\left(\\boldsymbol{X}_{t}^{i ; N}\\right)\\right]=\\sup _{N} \\mathbb{E}\\left[\\int_{[0,1]} \\int_{\\mathbb{R}} \\eta(x) \\boldsymbol{\\mu}^{N}(t, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi\\right] \\leq C(t)<\\infty\n$$\n\nfor some moment function $\\eta: \\mathbb{R} \\rightarrow \\mathbb{R}_{+}$satisfying $\\left\\|\\eta^{-1}\\right\\|_{L^{1}}<\\infty$. Assume that $w$ is a digraphon and $w^{N}$ are digraphons corresponding to $\\left\\{w_{i, j}^{N}\\right\\}_{i, j=1}^{N}$ and some almost everywhere partition $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}$ such that $\\| w^{N}-w\\|_{L^{\\infty} \\rightarrow L^{1}} \\rightarrow 0$. Moreover, assume that $\\mu \\in$ $L^{\\infty}\\left(\\left[0, t_{*}\\right] \\times[0,1] ; \\mathcal{M}(\\mathbb{R})\\right)$ is a solution to the mean-field PDE (3) with kernel $w$, and $\\boldsymbol{\\mu}^{N}$ are extended empirical measures corresponding to $\\left\\{\\boldsymbol{X}^{i ; N}\\right\\}_{i=1}^{N}$ and $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}$ such that\n\n$$\n\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(0, \\cdot, \\cdot)-\\mu(0, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\mathbb{C}}^{1}}\\right] \\rightarrow 0, \\quad \\text { as } N \\rightarrow 0\n$$\n\nThen, for all $t_{*}>0$,\n\n$$\n\\sup _{t \\in\\left[0, t_{*}\\right]} \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(t, \\cdot, \\cdot)-\\mu(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\mathbb{C}}^{1}}\\right] \\rightarrow 0, \\quad \\text { as } N \\rightarrow 0\n$$\n\nTheorem 2 can be seen as the core of Theorem 1. Below, we show that the assumptions of Theorem 1 imply the assumptions of Theorem 2 and, conversely, the result of Theorem 2 implies the result of Theorem 1. The relationship between Theorems 1 and 2 can be decomposed in three propositions. Since their proofs follow standard arguments, they are postponed to Appendix A.\n\nProof of Theorem 1 via Theorem 2. To start with, by Lemma 2, we can extract a subsequence and choose almost everywhere partitions $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}$ such that $\\| w^{N}-$ $w \\|_{L^{\\infty} \\rightarrow L^{1}} \\rightarrow 0$. The first proposition states that the initial moment bound (5) in Theorem 1 can be propagated through time to obtain the uniform moment bound (12) in Theorem 2 (choosing $\\eta(x)=1+x^{2}$ ).\n\nProposition 1. Let $\\left\\{\\boldsymbol{X}^{i ; N}\\right\\}_{i=1}^{N}$ be a solution to (1) with weight matrix $\\left(w_{i, j}^{N}\\right)_{i, j=1}^{N}$ and with initial moment bound\n\n$$\n\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{0}^{i ; N}\\right|^{2}\\right]=E_{0}<\\infty\n$$\n\nThen, for all $t \\in[0, \\infty)$,\n\n$$\n\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{t}^{i ; N}\\right|^{2}\\right]=C(t)<\\infty\n$$\n\nwhere $C(t)$ may depend on $E_{0},\\|b\\|_{L^{\\infty}},\\|f\\|_{L^{\\infty}}$, and $\\max _{1 \\leq i, j \\leq N}\\left|w_{i, j}^{N}\\right|$.\nThe second proposition establishes the equivalence between the expected $\\Phi_{w}^{-1}$ convergence in Theorem 2 and the weak-* convergence in Theorem 1.\n\nProposition 2. Let $\\mu^{N}, N \\rightarrow \\infty$, be a sequence of measure on $[0,1] \\times \\mathbb{R}$, and $\\mu$ be a measure on $[0,1] \\times \\mathbb{R}$. Assume that $\\mu^{N}(\\xi, \\cdot) \\in \\mathcal{P}(\\mathbb{R})$ for a.e. $\\xi \\in[0,1]$ and $\\mu(\\xi, \\cdot) \\in \\mathcal{P}(\\mathbb{R})$ for a.e. $\\xi \\in[0,1]$. Further, let $\\epsilon_{w}:(0,1) \\rightarrow(0, \\infty)$ be a non-decreasing function satisfying $\\lim _{r \\rightarrow 0+} \\epsilon_{w}(r)=0$. The following statements are equivalent:\n(i) $\\left\\|\\mu^{N}-\\mu\\right\\|_{\\Phi_{\\mathbb{C}}^{-1}} \\rightarrow 0$ as $N \\rightarrow \\infty$, where the metric $\\Phi_{w}^{-1}$ is defined in (11a)-(11b).\n\n(ii) For all $\\varphi \\in C_{c}([0,1] \\times \\mathbb{R})$,\n\n$$\n\\int_{[0,1] \\times \\mathbb{R}} \\varphi\\left(\\mu^{N}-\\mu\\right) \\mathrm{d} \\xi \\mathrm{~d} x \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$\n\nStatement (ii) in Proposition 2 defines the weak-* convergence in $\\mathcal{M}([0,1] \\times \\mathbb{R})$ we use in Theorem 1. The third proposition states that the assumptions on the initial data in Theorem 1 imply the expected $\\Phi_{w}^{-1}$-convergence of the empirical measure at time 0 , Eq. (13), in Theorem 2.\n\nProposition 3. Assume that the sequence of initial data $\\left\\{\\boldsymbol{X}_{0}^{i ; N}\\right\\}_{i=1}^{N}, N \\rightarrow \\infty$, are made of independent random variables with $\\operatorname{Law}\\left(\\boldsymbol{X}_{0}^{i ; N}\\right)=\\mu_{0}^{i ; N} \\in \\mathcal{P}(\\mathbb{R})$ and moments are bounded by\n\n$$\n\\sup _{N \\in \\mathbb{N}} \\sup _{1 \\leq i \\leq N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{0}^{i ; N}\\right|^{2}\\right]<\\infty\n$$\n\nMoreover, assume that $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}, N \\rightarrow \\infty$, are almost everywhere partitions.\nThen, up to extraction of a subsequence, there exists $\\mu_{0} \\in L^{\\infty}([0,1] ; \\mathcal{M}(\\mathbb{R}))$ such that $\\mu_{0}(\\xi, \\cdot) \\in \\mathcal{P}(\\mathbb{R})$ for a.e. $\\xi \\in[0,1]$ and\n\n$$\n\\left\\|\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right)-\\mu_{0}\\right\\|_{\\Phi_{w}^{-1}} \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$\n\nFurther, for this subsequence, the extended empirical measure $\\boldsymbol{\\mu}_{0}^{N}=\\sum_{i=1}^{N} \\delta_{\\boldsymbol{X}_{0}^{i ; N}} \\mathbb{1}_{E^{i ; N}}$ satisfies\n\n$$\n\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\mu_{0}\\right\\|_{\\Phi_{w}^{-1}}\\right] \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$\n\nIn summary, Propositions 1 and 3 guarantee that all the assumptions of Theorem 2 are satisfied under the assumptions of Theorem 1. Theorem 2 establishes the convergence in terms of the expectation of the $\\Phi_{w}^{-1}$ metric. Finally, Proposition 2 shows that the convergence stated in Theorem 2 implies the weak-* convergence in Theorem 1. Hence, to prove Theorem 1, it suffices to prove Theorem 2.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 3. Proof of the main result. \n\n3.1. Well-posedness. Before starting the proof of Theorem 2, we first describe the solutions to the system of SDEs (1) and to the mean-field PDE (3). The following proposition is about the mean-field PDE (3). In addition to its solution $\\mu$, we also introduce the measure $\\mu_{\\#}$, which is the pushforward measure of $\\mu$ by the measurable function $(t, \\xi, x) \\mapsto$ $(t, \\xi, x-H(t, \\xi))$. Formally, the measure $\\mu_{\\#}$ can be seen as the result of a change of variable that is rough with respect to the $\\xi$ variable. The motivation for introducing this change of variable will be explained in Section 3.2.\n\nProposition 4. Grant Assumptions 1. In addition, assume that $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$ and $\\mu_{0} \\in L^{\\infty}([0,1] ; \\mathcal{M}(\\mathbb{R}))$. Then, for any $t_{*}>0$, there exists a unique $\\mu \\in L^{\\infty}\\left(\\left[0, t_{*}\\right] \\times\\right.$ $[0,1] ; \\mathcal{M}(\\mathbb{R}))$ such that $\\mu$ is a solution to the mean-field PDE (3), restated here\n(3a)\n\n$$\n\\partial_{t} \\mu(t, \\xi, \\mathrm{~d} x)+\\partial_{x}\\left[(b(x)+h(t, \\xi)) \\mu(t, \\xi, \\mathrm{~d} x)\\right]+f(x) \\mu(t, \\xi, \\mathrm{~d} x)-r(t, \\xi) \\delta_{0}(\\mathrm{~d} x)=0\n$$\n\n(3b)\n\n$$\n\\begin{aligned}\n& r(t, \\xi)=\\int_{\\mathbb{R}} f(x) \\mu(t, \\xi, \\mathrm{~d} x), \\quad h(t, \\xi)=\\int_{[0,1]} w(\\xi, \\zeta) r(t, \\zeta) \\mathrm{d} \\zeta \\\\\n& \\mu(0, \\xi, \\mathrm{~d} x)=\\mu_{0}(\\xi, \\mathrm{~d} x)\n\\end{aligned}\n$$\n\nin the sense of characteristics. Moreover, define\n\n$$\n\\begin{aligned}\n& H(t, \\xi):=\\int_{0}^{t} h(s, \\xi) \\mathrm{d} s=\\int_{0}^{t} \\int_{[0,1]} w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\mu(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} s \\\\\n& \\mu_{\\#}(t, \\xi, \\mathrm{~d} x):=\\mu(t, \\xi, \\mathrm{~d} x+H(t, \\xi))\n\\end{aligned}\n$$\n\ntogether with\n\n$$\n\\left\\{\\begin{array}{l}\nb_{\\#}(t, \\xi, x):=b(x+H(t, \\xi)) \\\\\nf_{\\#}(t, \\xi, x):=f(x+H(t, \\xi)) \\\\\n\\delta_{\\#}(t, \\xi, \\mathrm{~d} x):=\\delta_{0}(\\mathrm{~d} x+H(t, \\xi))\n\\end{array}\\right.\n$$\n\nThen, $H \\in L^{\\infty}\\left([0,1] ; C\\left(\\left[0, t_{*}\\right]\\right)\\right)$ and the equation\n\n$$\n\\begin{aligned}\n\\partial_{t} \\mu_{\\#}(t, \\xi, \\mathrm{~d} x)+\\partial_{x}\\left[b_{\\#}(t, \\xi, x)\\right. & \\mu_{\\#}(t, \\xi, \\mathrm{~d} x)] \\\\\n& +f_{\\#}(t, \\xi, x) \\mu_{\\#}(t, \\xi, \\mathrm{~d} x)-r(t, \\xi) \\delta_{\\#}(t, \\xi, \\mathrm{~d} x)=0\n\\end{aligned}\n$$\n\nholds in the following distributional sense: $\\forall \\varphi \\in L^{\\infty}\\left([0,1] ; C_{b}^{1}\\left(\\left[0, t_{*}\\right] \\times \\mathbb{R}\\right)\\right), t \\in\\left[0, t_{*}\\right]$,\n\n$$\n\\begin{aligned}\n& \\int_{[0,1] \\times \\mathbb{R}} \\varphi(t, \\xi, x) \\mu_{\\#}(t, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi-\\int_{[0,1] \\times \\mathbb{R}} \\varphi(0, \\xi, x) \\mu_{\\#}(0, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\\\\n& =\\int_{0}^{t} \\int_{[0,1] \\times \\mathbb{R}}\\left[\\partial_{s} \\varphi(s, \\xi, x)+b_{\\#}(s, \\xi, x) \\partial_{x} \\varphi(s, \\xi, x)\\right] \\mu_{\\#}(s, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\mathrm{~d} s \\\\\n& \\quad+\\int_{0}^{t} \\int_{[0,1] \\times \\mathbb{R}} \\varphi(s, \\xi, x)\\left[-f_{\\#}(s, \\xi, x) \\mu_{\\#}(s, \\xi, \\mathrm{~d} x)+r(s, \\xi) \\delta_{\\#}(s, \\xi, \\mathrm{~d} x)\\right] \\mathrm{d} \\xi \\mathrm{~d} x \\mathrm{~d} s\n\\end{aligned}\n$$\n\nFor clarity, let us explicitly state what we mean, above, by \"solution in the sense of characteristics.\" Given $\\mu \\in L^{\\infty}\\left(\\left[0, t_{*}\\right] \\times[0,1] ; \\mathcal{M}(\\mathbb{R})\\right)$, consider the unique flow solving the system of characteristic equations\n\n$$\n\\left\\{\\begin{aligned}\n\\frac{\\mathrm{d} X_{\\mu}}{\\mathrm{d} t}(t, s, \\xi, x) & =b\\left(X_{\\mu}(t, s, \\xi, x)\\right)+h(t, \\xi) \\\\\nX_{\\mu}(s, s, \\xi, x) & =x\n\\end{aligned}\\right.\n$$\n\nfor a.e. $\\xi \\in[0,1]$. (Note that $h(t, \\xi)$, defined in (3a), depends on $\\mu$.) The measure $\\mu$ is a solution to the mean-field PDE (3) in the sense of characteristics, if for any $t \\in\\left[0, t_{*}\\right]$ and a.e. $\\xi \\in[0,1]$,\n\n$$\n\\mu(t, \\xi, \\mathrm{~d} x)=\\mu_{0}\\left(\\xi, X_{\\mu}(0, t, \\xi, \\mathrm{~d} x)\\right) \\partial_{x} X_{\\mu}(0, t, \\xi, x)\n$$\n\nProof. The well-posedness of the nonlinear mean-fiel PDE (3) can be obtained by adapting a standard contraction argument for population equations without spatial extension [22, 27, 89, 38] to the spatially-extended case; the spatial extension $\\xi$ in (3) does not cause any difficulty under the assumption that $w \\in L^{\\infty}\\left([0,1]^{2}\\right)$.\n\nEqs. (16) and (17) simply stem from the change of variable $\\mu_{\\#}(t, \\xi, \\mathrm{~d} x):=\\mu(t, \\xi, \\mathrm{~d} x+$ $H(t, \\xi))$.\n\nThe next proposition establishes the well-posedness of the system of SDEs (1), together with a change of variable similar to the one we used for $\\mu_{\\#}$ in the previous proposition.\n\nProposition 5. Grant Assumptions 1. For all $N \\geq 1$, let $\\left\\{\\boldsymbol{X}_{0}^{i ; N}\\right\\}_{i=1}^{N}$ be $\\mathscr{F}_{0^{-}}$ measurable random variables.\n(i) There exists a pathwise unique c\u00e0dl\u00e0g strong solution $\\left\\{\\boldsymbol{X}^{i ; N}\\right\\}_{i=1}^{N}$ to the system of SDEs (1), in the sense that $\\left\\{\\boldsymbol{X}^{i ; N}\\right\\}_{i=1}^{N}$ is a c\u00e0dl\u00e0g $\\left(\\mathscr{F}_{t}\\right)_{t \\in \\mathbb{R}_{+}}$-adapted process solving, a.s.,\n\n$$\n\\begin{aligned}\n\\boldsymbol{X}^{i ; N}(t)=\\boldsymbol{X}_{0}^{i ; N}+ & \\int_{0}^{t} b\\left(\\boldsymbol{X}^{i ; N}(s)\\right) \\mathrm{d} s \\\\\n+ & \\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right)\\right\\}} \\boldsymbol{\\Pi}^{j ; N}(\\mathrm{~d} s, \\mathrm{~d} z) \\\\\n& -\\int_{[0, t] \\times \\mathbb{R}_{+}} \\boldsymbol{X}^{i ; N}(s-) \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right)\\right\\}} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z), \\quad \\forall t \\geq 0\n\\end{aligned}\n$$\n\n(ii) Moreover, define the random processes $\\left\\{\\boldsymbol{H}^{i ; N}\\right\\}_{i=1}^{N},\\left\\{\\boldsymbol{Y}^{i ; N}\\right\\}_{i=1}^{N}$ as\n\n$$\n\\begin{aligned}\n& \\boldsymbol{Y}^{i ; N}(t):=\\boldsymbol{X}^{i ; N}(t)-\\boldsymbol{H}^{i ; N}(t) \\\\\n& \\boldsymbol{H}^{i ; N}(t):=\\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right)\\right\\}} \\boldsymbol{\\Pi}^{j ; N}(\\mathrm{~d} s, \\mathrm{~d} z)\n\\end{aligned}\n$$\n\nThen, a.s., for all $\\varphi^{i ; N} \\in C_{b}^{1}\\left([0, t_{*}] \\times \\mathbb{R}\\right), i \\in\\{1, \\ldots, N\\}$,\n\n$$\n\\begin{aligned}\n& \\sum_{i=1}^{N} \\varphi^{i ; N}\\left(t, \\boldsymbol{Y}^{i ; N}(t)\\right)-\\sum_{i=1}^{N} \\varphi^{i ; N}\\left(0, \\boldsymbol{Y}^{i ; N}(0)\\right) \\\\\n& =\\sum_{i=1}^{N} \\int_{0}^{t} \\partial_{s} \\varphi^{i ; N}\\left(s, \\boldsymbol{Y}^{i ; N}(s)\\right)+b\\left(\\boldsymbol{Y}^{i ; N}(s)+\\boldsymbol{H}^{i ; N}(s)\\right) \\partial_{x} \\varphi^{i ; N}\\left(s, \\boldsymbol{Y}^{i ; N}(s)\\right) \\mathrm{d} s \\\\\n& \\quad+\\sum_{i=1}^{N} \\int_{[0,1] \\times \\mathbb{R}}\\left[\\varphi^{i ; N}\\left(s,-\\boldsymbol{H}^{i ; N}(s-\\right)\\right)-\\varphi^{i ; N}\\left(s, \\boldsymbol{Y}^{i ; N}(s-\\right)\\right)] \\\\\n& \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{Y}^{i ; N}(s-)+\\boldsymbol{H}^{i ; N}(s-)\\right)\\right\\}} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)\n\\end{aligned}\n$$\n\nProof. The well-posedness (i) has been established in [39] and, as discussed in [41, 89], a.s., a solution can be constructed via a standard thinning procedure [62, 79, 34]. For (ii), since the solution $\\left\\{\\boldsymbol{X}^{i ; N}\\right\\}_{i=1}^{N}$ has bounded variation a.s., $\\left\\{\\boldsymbol{Y}^{i ; N}\\right\\}_{i=1}^{N}$ also has bounded variation a.s. Hence, for any realization of $\\left\\{\\boldsymbol{Y}^{i ; N}\\right\\}_{i=1}^{N}$, a.s., (18) holds for all $\\varphi^{i ; N} \\in C_{b}^{1}\\left([0, t_{*}] \\times \\mathbb{R}\\right)$ by simple differentiation of $\\varphi^{i ; N}\\left(s, \\boldsymbol{Y}^{i ; N}(s)\\right)$ in the distributional sense.\n\nWe would like to emphasize that Proposition 5 states the identity (18) in a stronger sense than It\u00f4's lemma usually does. The most widely known version of It\u00f4's lemma deals with semi-martingales and only asserts that for any test function $\\varphi$, the It\u00f4 formula (which is (18) in our case) is satisfied a.s. In contrast, Proposition 5 states that for a.e. realization $\\omega \\in \\Omega,(18)$ holds for all $\\varphi^{i ; N} \\in C_{b}^{1}\\left([0, t_{*}] \\times \\mathbb{R}\\right)$. This is possible because $\\left(\\boldsymbol{Y}^{i ; N}\\right)_{i=1}^{N}$ and $\\left(\\boldsymbol{H}^{i ; N}\\right)_{i=1}^{N}$ are bounded variation processes, and the integral in (18) can be understood in the\n\nStieltjes sense. To see that these two types of statements are not equivalent, consider (18) where $\\varphi^{i ; N}$ is replaced by any random $\\varphi^{i ; N}$ such that $\\varphi^{i ; N} \\in C_{b}^{1}\\left(\\left[0, t_{*}\\right] \\times \\mathbb{R}\\right), i \\in\\{1, \\ldots, N\\}$ a.s. Proposition 5 guarantees that the identity remains true a.s., while a standard It\u00f4 lemmatype statement would be insufficient to guarantee the identity in this case.\n\nThe identity (18) can be reformulated as an expression similar to (17) by the notion of extended empirical measure. Given any almost everywhere partition $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}$, we define\n\n$$\n\\begin{aligned}\n\\boldsymbol{H}^{N}(t, \\xi) & :=\\sum_{i=1}^{N} \\boldsymbol{H}^{i ; N}(t) \\mathbb{1}_{E^{i ; N}}(\\xi) \\\\\n\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x) & :=\\sum_{i=1}^{N} \\delta_{\\boldsymbol{Y}^{i ; N}(t)}(\\mathrm{d} x) \\mathbb{1}_{E^{i ; N}}(\\xi)\n\\end{aligned}\n$$\n\nin accordance with the previous definition of $\\boldsymbol{\\mu}^{N}$, restated here\n\n$$\n\\boldsymbol{\\mu}^{N}(t, \\xi, \\mathrm{~d} x):=\\sum_{i=1}^{N} \\delta_{\\boldsymbol{X}^{i ; N}(t)}(\\mathrm{d} x) \\mathbb{1}_{E^{i ; N}}(\\xi)\n$$\n\nThen, a.s., for all $\\varphi \\in L^{\\infty}\\left([0,1] ; C_{b}^{1}\\left(\\left[0, t_{*}\\right] \\times \\mathbb{R}\\right), t \\in\\left[0, t_{*}\\right]\\right.$,\n\n$$\n\\begin{aligned}\n& \\int_{[0,1] \\times \\mathbb{R}} \\varphi(t, \\xi, x) \\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi-\\int_{[0,1] \\times \\mathbb{R}} \\varphi(0, \\xi, x) \\boldsymbol{\\mu}_{\\#}^{N}(0, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\\\\n& \\quad=\\int_{0}^{t} \\int_{[0,1] \\times \\mathbb{R}}\\left[\\partial_{s} \\varphi(s, \\xi, x)+\\boldsymbol{b}_{\\#}^{N}(s, \\xi, x) \\partial_{x} \\varphi(s, \\xi, x)\\right] \\boldsymbol{\\mu}_{\\#}^{N}(s, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\mathrm{~d} s \\\\\n& \\quad+\\int_{[0,1] \\times \\mathbb{R}} \\int_{[0, t] \\times \\mathbb{R}_{+}} \\varphi(s, \\xi, x)\\left[-\\boldsymbol{\\mu}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)+\\boldsymbol{\\delta}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)\\right] \\\\\n& \\quad \\sum_{i=1}^{N} \\mathbb{1}_{E^{i ; N}}(\\xi) \\mathbb{1}_{\\{z \\leq \\boldsymbol{r}^{N}(s-, \\xi)\\}} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z) \\mathrm{d} \\xi\n\\end{aligned}\n$$\n\nwhere the random fields and measures are defined as\n\n$$\n\\left\\{\\begin{array}{l}\n\\boldsymbol{b}_{\\#}^{N}(t, \\xi, x):=b\\left(x+\\boldsymbol{H}^{N}(t, \\xi)\\right) \\\\\n\\boldsymbol{f}_{\\#}^{N}(t, \\xi, x):=f\\left(x+\\boldsymbol{H}^{N}(t, \\xi)\\right) \\\\\n\\boldsymbol{\\delta}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x):=\\delta_{0}\\left(\\mathrm{~d} x+\\boldsymbol{H}^{N}(t, \\xi)\\right)\n\\end{array}\\right.\n$$\n\nand\n$\\boldsymbol{r}^{N}(s, \\xi):=\\sum_{i=1}^{N} f\\left(\\boldsymbol{X}^{i ; N}(s)\\right) \\mathbb{1}_{E^{i ; N}}(\\xi)=\\int_{\\mathbb{R}} f(x) \\boldsymbol{\\mu}^{N}(s, \\xi, \\mathrm{~d} x)=\\int_{\\mathbb{R}} \\boldsymbol{f}_{\\#}^{N}(s, \\xi, x) \\boldsymbol{\\mu}_{\\#}^{N}(s, \\xi, \\mathrm{~d} x)$.\nTo see that (19) is a reformulation of (18), it suffices to notice that $\\varphi^{i ; N}(t, x)=$ $\\int_{E^{i ; N}} \\varphi(t, \\xi, x) \\mathrm{d} \\xi$.\n3.2. On the change of variable involving the intergrated postsynaptic input. In this subsection, we explain the usefulness of the aforementioned change of variable. In the original system (1), the SDEs governing $\\boldsymbol{X}^{i ; N}$ with distinct indices $i \\in\\{1, \\ldots, N\\}$ are coupled via the term\n\n$$\n\\frac{1}{N} \\sum_{j=1}^{N} w_{i, j}^{N} \\mathrm{~d} \\boldsymbol{Z}^{j ; N}(t)\n$$\n\nThis coupling term is interpreted as the postsynaptic input to the $i$-th neuron and is singular in time. Intuitively, as $N \\rightarrow \\infty$, we expect this term to converge to the drift term in the meanfield PDE (3)\n\n$$\nh(t, \\xi)=\\int_{[0,1]} w(\\xi, \\zeta) r(t, \\zeta) \\mathrm{d} \\zeta\n$$\n\nwhich is continuous in time. However, the singularity in time poses serious challenges if we try to directly pass to the limit from the system (1) to the mean-field PDE (3). To address this difficulty, we separate the coupling term from the rest of the dynamics. In Proposition 4, we have defined the \"integrated drift term\" $H(t, \\xi)$ as\n\n$$\nH(t, \\xi):=\\int_{0}^{t} h(s, \\xi) \\mathrm{d} s=\\int_{0}^{t} \\int_{[0,1]} w(\\xi, \\zeta) r(s, \\zeta) \\mathrm{d} \\zeta \\mathrm{~d} s\n$$\n\nand adopted the change of variable $\\mu_{\\#}(t, \\xi, \\mathrm{~d} x):=\\mu(t, \\xi, \\mathrm{~d} x+H(t, \\xi))$. Analogously, in Proposition 5 we have defined the \"integrated postsynaptic input\" $\\boldsymbol{H}^{i ; N}(t)$ as\n$\\boldsymbol{H}^{i ; N}(t):=\\frac{1}{N} \\sum_{j=1}^{N} \\int_{0}^{t} w_{i, j}^{N} \\mathrm{~d} \\boldsymbol{Z}^{j ; N}(s)=\\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right)\\right\\}} \\boldsymbol{\\Pi}^{j ; N}(\\mathrm{~d} s, \\mathrm{~d} z)$,\nand applied a subsequent change of variable $\\boldsymbol{Y}^{i ; N}(t):=\\boldsymbol{X}^{i ; N}(t)-\\boldsymbol{H}^{i ; N}(t)$. The change of variable prevents the direct appearance of the singular-in-time coupling terms in the dynamics, as we can see in (18), or equivalently (19). The cost of this change of variable is that it makes field $\\boldsymbol{b}_{\\#}^{N}$ in (19) time-dependent and random.\n\nThe following proposition shows that distances after the change of variable control distances before the change of variable.\n\nProposition 6. For any $t_{*}>0$, let $\\left\\{\\boldsymbol{X}^{i ; N}\\right\\}_{i=1}^{N}$ and $\\left\\{\\boldsymbol{H}^{i ; N}\\right\\}_{i=1}^{N}$ be $\\mathbb{R}^{N}$-valued random processes on $\\left[0, t_{*}\\right]$. Given an almost everywhere partition $\\left\\{E^{i ; N}\\right\\}_{i=1}^{N}$ and taking the definition $\\boldsymbol{Y}^{i ; N}:=\\boldsymbol{X}^{i ; N}-\\boldsymbol{H}^{i ; N}$, define, for all $t \\in\\left[0, t^{*}\\right]$,\n\n$$\n\\begin{aligned}\n\\boldsymbol{H}^{N}(t, \\xi) & :=\\sum_{i=1}^{N} \\boldsymbol{H}^{i ; N}(t) \\mathbb{1}_{E^{i ; N}}(\\xi) \\\\\n\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x) & :=\\sum_{i=1}^{N} \\delta_{\\boldsymbol{X}^{i ; N}(t)}(\\mathrm{d} x) \\mathbb{1}_{E^{i ; N}}(\\xi) \\\\\n\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x) & :=\\sum_{i=1}^{N} \\delta_{\\boldsymbol{Y}^{i ; N}(t)}(\\mathrm{d} x) \\mathbb{1}_{E^{i ; N}}(\\xi)\n\\end{aligned}\n$$\n\nFurther, let $\\mu \\in L^{\\infty}\\left(\\left[0, t_{*}\\right] \\times[0,1] ; \\mathcal{M}(\\mathbb{R})\\right)$ and $H \\in L^{\\infty}\\left(\\left[0, t_{*}\\right] \\times[0,1]\\right)$. Then, for all $t \\in$ $\\left[0, t^{*}\\right]$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x)=\\boldsymbol{\\mu}^{N}(t, \\xi, \\mathrm{~d} x+\\boldsymbol{H}^{N}(t, \\xi)), \\quad \\forall t \\geq 0, \\text { a.e. } \\xi \\in[0,1] \\\\\n& \\left\\|\\boldsymbol{\\mu}^{N}(t, \\cdot, \\cdot)-\\mu(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\omega}^{-1}} \\leq\\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(t, \\cdot, \\cdot)-\\mu_{\\#}(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\omega}^{-1}}+\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-H(t, \\cdot)\\right\\|_{L^{1}}\n\\end{aligned}\n$$\n\nProof. For a.e. $\\xi \\in[0,1]$ there exists a unique index $i(\\xi) \\in\\{1, \\ldots, N\\}$ such that $\\xi \\in$ $E^{i(\\xi) ; N}$ and $\\xi \\notin E^{j ; N}$ for all $j$ distinct from $i(\\xi)$. Given this uniqueness,\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x) & =\\delta_{\\boldsymbol{Y}^{i(\\xi) ; N}(t)}(\\mathrm{d} x)=\\delta_{\\boldsymbol{X}^{i(\\xi) ; N}(t)-\\boldsymbol{H}^{i(\\xi) ; N}(t)}(\\mathrm{d} x) \\\\\n& =\\delta_{\\boldsymbol{X}^{i(\\xi) ; N}(t)}\\left(\\mathrm{d} x+\\boldsymbol{H}^{i(\\xi) ; N}(t)\\right)=\\boldsymbol{\\mu}^{N}(t, \\xi, \\mathrm{~d} x+\\boldsymbol{H}^{N}(t, \\xi))\n\\end{aligned}\n$$\n\nwhich proves the first identity a.e.\nNext, using the definition of the metric $\\Phi_{\\omega}^{-1}$ and incorporating the identity just proved yields\n\n$$\n\\begin{aligned}\n& \\left\\|\\boldsymbol{\\mu}^{N}(t, \\cdot, \\cdot)-\\mu(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\omega}^{-1}}=\\sup _{\\|\\phi\\|_{\\Phi_{\\omega}} \\leq 1} \\int_{[0,1] \\times \\mathbb{R}} \\phi(\\xi, x)\\left(\\boldsymbol{\\mu}^{N}(t, \\xi, \\mathrm{~d} x)-\\mu(t, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi \\\\\n& \\quad=\\sup _{\\|\\phi\\|_{\\Phi_{\\omega}} \\leq 1} \\int_{[0,1] \\times \\mathbb{R}} \\phi(\\xi, x)\\left(\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x-\\boldsymbol{H}^{N}(t, \\xi))-\\mu_{\\#}(t, \\xi, \\mathrm{~d} x-H(t, \\xi))\\right) \\mathrm{d} \\xi\n\\end{aligned}\n$$\n\nNote that $\\boldsymbol{\\mu}^{N}, \\boldsymbol{\\mu}_{\\#}^{N}$, and $\\boldsymbol{H}^{N}$ are random. Hence, the function $\\phi$ used in the supremum and the integration is also random.\n\nApply the change of variable, $x \\mapsto x+H(t, \\xi)$, we get\n\n$$\n\\begin{aligned}\n& \\left\\|\\boldsymbol{\\mu}^{N}(t, \\cdot, \\cdot)-\\mu(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\omega}^{-1}} \\\\\n& =\\sup _{\\|\\phi\\|_{\\Phi_{\\omega}} \\leq 1} \\int_{[0,1] \\times \\mathbb{R}} \\phi(\\xi, x+H(t, \\xi)) \\\\\n& \\left(\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x-\\boldsymbol{H}^{N}(t, \\xi)+H(t, \\xi))-\\mu_{\\#}(t, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi \\\\\n& \\leq \\sup _{\\|\\phi\\|_{\\Phi_{\\omega}} \\leq 1} \\int_{[0,1] \\times \\mathbb{R}} \\phi(\\xi, x+H(t, \\xi))\\left(\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x)-\\mu_{\\#}(t, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi \\\\\n& \\quad+\\sup _{\\|\\phi\\|_{\\Phi_{\\omega}} \\leq 1} \\int_{[0,1] \\times \\mathbb{R}} \\phi(\\xi, x+H(t, \\xi)) \\\\\n& \\left(\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x-\\boldsymbol{H}^{N}(t, \\xi)+H(t, \\xi))-\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi \\\\\n& =: L_{\\#}+L_{H} .\n\\end{aligned}\n$$\n\nBy definition, $L_{\\#}=\\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(t, \\cdot, \\cdot)-\\mu_{\\#}(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\omega}^{-1}}$. We can further simplify $L_{H}$ with another change of variable,\n\n$$\n\\begin{aligned}\nL_{H} & =\\sup _{\\|\\phi\\|_{\\Phi_{\\omega}} \\leq 1} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\phi\\left(\\xi, x+\\boldsymbol{H}^{N}(t, \\xi)\\right)-\\phi(\\xi, x+H(t, \\xi))\\right) \\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\\\\n& \\leq \\int_{[0,1] \\times \\mathbb{R}}\\left|\\boldsymbol{H}^{N}(t, \\xi)-H(t, \\xi)\\right| \\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\\\\n& =\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-H(t, \\cdot)\\right\\|_{L^{1}}\n\\end{aligned}\n$$\n\nwhere, in the inequality we used the fact that $\\|\\phi\\|_{\\Phi_{\\omega}} \\leq 1$ implies that $\\phi$ is 1-Lipschitz in the $x$-direction, and in the final equality, we integrated with respect to $x$ then $\\xi$.\n\nProposition 6 tells us that to control $\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(t, \\cdot, \\cdot)-\\mu(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\omega}^{-1}}\\right]$ (and prove Theorem 2), it suffices to control $\\mathbb{E}\\left[\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-H(t, \\cdot)\\right\\|_{L^{1}}\\right]$ and $\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(t, \\cdot, \\cdot)-\\mu_{\\#}(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\omega}^{-1}}\\right]$, which are more tractable. In Section 3.3, we derive the bound for $\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-H(t, \\cdot)\\right\\|_{L^{1}}$, which illustrates the main ideas of our proof; the estimation of $\\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(t, \\cdot, \\cdot)-\\mu_{\\#}(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\omega}^{-1}}$, which is more intricate, is treated in Sections 3.4-3.6. Finally, these estimates are combined in Section 3.7 to conclude the proof.\n3.3. Quantifying the difference between the integrated postsynaptic inputs. This subsection is devoted to the bound of the term $\\mathbb{E}\\left[\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-H(t, \\cdot)\\right\\|_{L^{1}}\\right]$. Let us restate here that\n\n$$\nH(t, \\xi):=\\int_{0}^{t} \\int_{[0,1]} w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\mu(t, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} s\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\boldsymbol{H}^{N}(t, \\xi) & :=\\sum_{i=1}^{N} \\boldsymbol{H}^{i ; N}(t) \\mathbb{1}_{E^{i ; N}}(\\xi) \\\\\n\\boldsymbol{H}^{i ; N}(t) & :=\\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\{z \\leq f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right)\\}} \\boldsymbol{\\Pi}^{j ; N}(\\mathrm{~d} s, \\mathrm{~d} z)\n\\end{aligned}\n$$\n\nwhere $\\left\\{E^{i, N}\\right\\}_{i=1}^{N}$ is an almost everywhere partition.\nFor the forthcoming steps, we also introduce the auxiliary functions\n\n$$\n\\begin{aligned}\n\\tilde{\\boldsymbol{H}}^{N}(t, \\xi) & =\\int_{0}^{t} \\int_{[0,1]} w^{N}(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\boldsymbol{\\mu}^{N}(t, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} s \\\\\n\\widetilde{\\boldsymbol{H}}^{N}(t, \\xi) & =\\int_{0}^{t} \\int_{[0,1]} w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\boldsymbol{\\mu}^{N}(t, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} s\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\nw^{N}(\\xi, \\zeta) & :=\\sum_{i, j=1}^{N} w_{i, j}^{N} \\mathbb{1}_{E^{i, N}}(\\xi) \\mathbb{1}_{E^{j, N}}(\\zeta) \\\\\n\\boldsymbol{\\mu}^{N}(t, \\xi, \\mathrm{~d} x) & :=\\sum_{i=1}^{N} \\delta_{\\boldsymbol{X}^{i, N}(t)}(\\mathrm{d} x) \\mathbb{1}_{E^{i, N}}(\\xi)\n\\end{aligned}\n$$\n\nThe definition of these auxiliary functions is closely related to that of $H$, the only difference being that $\\boldsymbol{\\mu}^{N}$ substitutes $\\mu$ and that $w^{N}$ substitutes $w$.\n\nLEMMA 3. The following estimate holds:\n\n$$\n\\begin{aligned}\n& \\left\\|\\tilde{\\boldsymbol{H}}^{N}(t, \\cdot)-H(t, \\cdot)\\right\\|_{L_{t}^{1}} \\leq \\kappa_{1} \\int_{0}^{t}\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot)-\\mu(s, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{\\omega}}^{1}} \\mathrm{~d} s+\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\|f\\|_{L^{\\infty}} t \\\\\n& \\kappa_{1}:=\\left(\\max \\left(\\|f\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{1}}\\right)\\|w\\|_{L^{\\infty} L^{1}}+\\|f\\|_{L^{\\infty}}\\right)\n\\end{aligned}\n$$\n\nProof. We first examine the distance between $\\widetilde{\\boldsymbol{H}}^{N}$ and $H$, which differ only through the measures $\\boldsymbol{\\mu}^{N}$ and $\\mu$.\n\n$$\n\\begin{aligned}\n& \\int_{0}^{1}\\left|\\widetilde{\\boldsymbol{H}}^{N}(t, \\xi)-H(t, \\xi)\\right| \\mathrm{d} \\xi \\\\\n& =\\int_{0}^{1}\\left|\\int_{0}^{t} \\int_{[0,1]} w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x)\\left[\\boldsymbol{\\mu}^{N}(s, \\zeta, \\mathrm{~d} x)-\\mu(s, \\zeta, \\mathrm{~d} x)\\right] \\mathrm{d} \\zeta \\mathrm{~d} s\\right| \\mathrm{d} \\xi \\\\\n& =\\sup _{\\psi:\\|\\psi\\|_{L_{t}^{\\infty}} \\leq 1} \\int_{0}^{1} \\int_{0}^{t} \\int_{[0,1]} \\psi(\\xi) w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x)\\left[\\boldsymbol{\\mu}^{N}(s, \\zeta, \\mathrm{~d} x)-\\mu(s, \\zeta, \\mathrm{~d} x)\\right] \\mathrm{d} \\zeta \\mathrm{~d} s \\mathrm{~d} \\xi \\\\\n& =\\sup _{\\psi:\\|\\psi\\|_{L_{t}^{\\infty}} \\leq 1} \\int_{0}^{t} \\int_{[0,1]} \\int_{0}^{1} \\psi(\\xi) w(\\xi, \\zeta) \\mathrm{d} \\xi \\int_{\\mathbb{R}} f(x)\\left[\\boldsymbol{\\mu}^{N}(s, \\zeta, \\mathrm{~d} x)-\\mu(s, \\zeta, \\mathrm{~d} x)\\right] \\mathrm{d} \\zeta \\mathrm{~d} s \\\\\n& \\leq \\sup _{\\psi:\\|\\psi\\|_{L_{t}^{\\infty}} \\leq 1} \\int_{0}^{t}\\left\\|\\int_{0}^{1} \\psi(\\xi) w(\\xi, \\cdot) f(\\cdot) \\mathrm{d} \\xi\\right\\|_{\\Phi_{w}}\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot)-\\mu(s, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{\\omega}}^{1}} \\mathrm{~d} s\n\\end{aligned}\n$$\n\nwhere in the last inequality, we used the definition of the metric $\\Phi_{w}^{-1}$. Moreover,\n\n$$\n\\begin{aligned}\n& \\sup _{\\psi: \\|\\psi\\|_{L_{\\xi}^{\\infty}} \\leq 1}\\left\\|\\int_{0}^{1} \\psi(\\zeta) w(\\zeta, \\cdot) f(\\cdot) \\mathrm{d} \\zeta\\right\\|_{\\Phi_{w}} \\\\\n& \\leq \\sup _{\\psi:\\|\\psi\\|_{L_{\\xi}^{\\infty}} \\leq 1} \\max \\left(\\left\\|\\int_{0}^{1} \\psi(\\zeta) w(\\zeta, \\cdot) f(\\cdot) \\mathrm{d} \\zeta\\right\\|_{L_{\\xi, \\infty}^{\\infty}},\\left\\|\\int_{0}^{1} \\psi(\\zeta) w(\\zeta, \\cdot) \\partial_{x} f(\\cdot) \\mathrm{d} \\zeta\\right\\|_{L_{\\xi, \\infty}^{\\infty}}\\right. \\\\\n& \\left.\\quad\\left\\|\\int_{0}^{1} \\psi(\\zeta) w(\\zeta, \\cdot) \\partial_{x} f(\\cdot) \\mathrm{d} \\zeta\\right\\|_{L_{\\xi}^{\\infty} L_{\\zeta}^{\\infty}}\\right. \\\\\n& \\sup _{h}\\left\\{\\epsilon_{w}(|h|)^{-1} \\int_{[0,1]} \\sup _{x}\\left|\\int_{[0,1]} \\psi(\\zeta) w(\\zeta, \\xi) f(x)-\\psi(\\zeta) w(\\zeta, \\xi-h) f(x) \\mathrm{d} \\zeta\\right| \\mathrm{d} \\xi\\right\\}\\right) \\\\\n& \\leq \\max \\left(\\max \\left(\\|f\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{1}}\\right) \\sup _{\\psi:\\|\\psi\\|_{L_{\\xi}^{\\infty}} \\leq 1}\\left\\|\\int_{0}^{1} \\psi(\\zeta) w(\\zeta, \\cdot) \\mathrm{d} \\zeta\\right\\|_{L_{\\xi}^{\\infty}}, \\\\\n& \\quad\\|f\\|_{L^{\\infty}} \\sup _{h}\\left\\{\\epsilon_{w}(|h|)^{-1} \\int_{[0,1]^{2}}|w(\\zeta, \\xi)-w(\\zeta, \\xi-h)| \\mathrm{d} \\zeta \\mathrm{~d} \\xi\\right\\}\\right) \\\\\n& \\leq \\max \\left(\\|f\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{1}}\\right)\\|w\\|_{L^{\\infty} L^{1}}+\\|f\\|_{L^{\\infty}} .\n\\end{aligned}\n$$\n\nTherefore, we have that\n\n$$\n\\begin{aligned}\n\\int_{0}^{1}\\left|\\widetilde{\\boldsymbol{H}}^{N}(t, \\xi)-H(t, \\xi)\\right| \\mathrm{d} \\xi \\leq & \\left(\\max \\left(\\|f\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{1}}\\right)\\|w\\|_{L^{\\infty} L^{1}}+\\|f\\|_{L^{\\infty}}\\right) \\\\\n& \\int_{0}^{t}\\left\\|\\mu^{N}(s, \\cdot, \\cdot)-\\mu(s, \\cdot, \\cdot)\\right\\|_{\\Phi_{w}^{\\cdot}} \\mathrm{d} s\n\\end{aligned}\n$$\n\nNow, we turn to the distance between $\\widetilde{\\boldsymbol{H}}^{N}$ and $\\bar{H}^{N}$, which differ only through the kernels $w^{N}$ and $w$. By considering $w^{N}$ and $w$ as $L^{\\infty} \\rightarrow L^{1}$ operators (from the $\\zeta$ domain to the $\\xi$ domain), we directly have that\n\n$$\n\\begin{aligned}\n& \\int_{0}^{1}\\left|\\bar{\\boldsymbol{H}}^{N}(t, \\xi)-\\widetilde{\\boldsymbol{H}}^{N}(t, \\xi)\\right| \\mathrm{d} \\xi \\\\\n& \\quad=\\int_{[0,1]}\\left|\\int_{0}^{t} \\int_{[0,1]}\\left[w^{N}(\\xi, \\zeta)-w(\\xi, \\zeta)\\right] \\int_{\\mathbb{R}} f(x) \\boldsymbol{\\mu}^{N}(t, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} s\\right| \\mathrm{d} \\xi \\\\\n& \\quad \\leq\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\left\\|\\int_{0}^{t} \\int_{\\mathbb{R}} f(x) \\boldsymbol{\\mu}^{N}(t, \\cdot, \\mathrm{~d} x) \\mathrm{d} s\\right\\|_{L^{\\infty}} \\\\\n& \\quad \\leq\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\|f\\|_{L^{\\infty}} t\n\\end{aligned}\n$$\n\nA triangular inequality concludes the proof.\nNext, we compare $\\bar{H}^{N}$ and $\\boldsymbol{H}^{N}$; this is where the Poisson random measures $\\left\\{\\Pi^{t, N}(\\mathrm{~d} t, \\mathrm{~d} z)\\right\\}_{z=1}^{N}$ come into play. Establishing a deterministic or a.s. bound on the distance would be difficult and inefficient. However, because of the $1 / N$ scaling in the definition of $\\boldsymbol{H}^{N}$, the expectation of the distance should be controllable by some sort of law of large numbers argument. This is what is shown in the following lemma, which uses It\u00f4 isometry.\n\nLEMMA 4. The following estimate holds:\n\n$$\n\\mathbb{E}\\left[\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-\\bar{\\boldsymbol{H}}^{N}(t, \\cdot)\\right\\|_{L_{\\xi}^{1}}\\right] \\leq \\frac{1}{\\sqrt{N}}\\left(\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}\\|f\\|_{L^{\\infty}} t\\right)^{1 / 2}\n$$\n\nProof. Let us fully expand the definition of $\\boldsymbol{H}^{N}$\n\n$$\n\\boldsymbol{H}^{N}(t, \\xi)=\\sum_{i=1}^{N}\\left(\\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j, N}(s-)\\right)\\right\\}} \\boldsymbol{\\Pi}^{j, N}(\\mathrm{~d} s, \\mathrm{~d} z)\\right) \\mathbb{1}_{E^{i, N}}(\\xi)\n$$\n\nSimilarly, $\\bar{H}^{N}$ can be written as\n\n$$\n\\bar{H}^{N}(t, \\xi)=\\sum_{i=1}^{N}\\left(\\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j, N}(s-)\\right)\\right\\}} \\mathrm{d} s \\mathrm{~d} z\\right) \\mathbb{1}_{E^{i, N}}(\\xi)\n$$\n\nHence, the $L_{\\xi}^{1}$ distance between these two functions can be reduced to a discrete $\\ell^{1}$ distance on $1 \\leq i \\leq N$ :\n\n$$\n\\begin{aligned}\n& \\int_{0}^{1}\\left|\\boldsymbol{H}^{N}(t, \\xi)-\\bar{H}^{N}(t, \\xi)\\right| \\mathrm{d} \\xi \\\\\n& \\quad=\\frac{1}{N} \\sum_{i=1}^{N}\\left|\\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j, N}(s-)\\right)\\right\\}}\\left[\\boldsymbol{\\Pi}^{j, N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right]\\right| .\n\\end{aligned}\n$$\n\nThe expectation can then be bounded by Jensen's inequality:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-\\bar{H}^{N}(t, \\cdot)\\right\\|_{L_{\\xi}^{1}}\\right] \\\\\n& \\quad=\\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N}\\left\\lfloor\\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j, N}(s-)\\right)\\right\\}}\\left[\\boldsymbol{\\Pi}^{j, N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right]\\right|\\right] \\\\\n& \\quad \\leq \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left\\lvert\\, \\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j, N}(s-)\\right)\\right\\}}\\left[\\boldsymbol{\\Pi}^{j, N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right]\\right|^{2}\\right]^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nSince $\\left\\{\\boldsymbol{X}^{i, N}\\right\\}_{i=1}^{N}$ is an adapted process, we can apply the It\u00f4 isometry for Poisson random measures to simplify the expectations above, and get\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-\\bar{H}^{N}(t, \\cdot)\\right\\|_{L_{\\xi}^{1}}\\right] \\\\\n& \\quad \\leq \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left\\lvert\\, \\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j, N}(s-)\\right)\\right\\}}\\left[\\boldsymbol{\\Pi}^{j, N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right]\\right|^{2}\\right]^{\\frac{1}{2}} \\\\\n& \\quad=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}}\\left(\\frac{1}{N} w_{i, j}^{N} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j, N}(s-)\\right)\\right\\}}\\right)^{2} \\mathrm{~d} s \\mathrm{~d} z\\right]^{\\frac{1}{2}} \\\\\n& \\quad \\leq \\frac{1}{N} \\sum_{i=1}^{N}\\left(\\sum_{j=1}^{N}\\left(\\frac{1}{N} w_{i, j}^{N}\\right)^{2}\\|f\\|_{L^{\\infty}} t\\right)^{\\frac{1}{2}} \\\\\n& \\quad \\leq \\frac{1}{\\sqrt{N}}\\left(\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}\\|f\\|_{L^{\\infty}} t\\right)^{1 / 2}\n\\end{aligned}\n$$\n\nwhich concludes the proof.\n\nCombining the two lemmas, we have that\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-H(t, \\cdot)\\right\\|_{L^{1}}\\right] \\\\\n& \\quad \\leq \\kappa_{1} \\int_{0}^{t} \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot)-\\mu(s, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{\\omega}}^{1}}\\right] \\mathrm{d} s+\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\|f\\|_{L^{\\infty}} t \\\\\n& \\quad+\\frac{1}{\\sqrt{N}}\\left(\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}\\|f\\|_{L^{\\infty}} t\\right)^{1 / 2}\n\\end{aligned}\n$$\n\nThis concludes the first part of the main stability estimate.\n3.4. Stability via duality. We now consider the term $\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(t, \\cdot, \\cdot)-\\mu_{\\#}(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{\\omega}}^{1}}\\right]$. Let $\\left\\{\\boldsymbol{X}^{i ; N}\\right\\}_{i=1}^{N}$ be a strong solution to the system (1) with $\\left\\{w_{i, j}^{N}\\right\\}_{i, j=1}^{N} \\in \\mathbb{R}^{N \\times N}$, as provided by Proposition 5, and let $\\mu$ be a solution to the mean-field $\\operatorname{PDE}$ (3) in the sense of characteristics with $w \\in L^{\\infty}([0,1])$, as provided by Proposition 4. Then, $\\boldsymbol{\\mu}_{\\#}^{N}$ and $\\mu_{\\#}$, the measures after the change of variable, satisfy (19) and (17) respectively. By subtracting (17) from (19), we immediately derive the following lemma.\n\nLemma 5. Grant all the assumptions and take all the definitions from Propositions 4 and 5. Define the linear operator $D^{H}$ for $\\varphi \\in L^{\\infty}\\left([0,1] ; C_{b}^{1}\\left(\\left[0, t_{*}\\right] \\times \\mathbb{R}\\right)\\right)$ as\n\n$$\nD^{H} \\varphi(s, \\xi, x):=b_{\\#}(s, \\xi, x) \\partial_{x} \\varphi(s, \\xi, x)+f_{\\#}(s, \\xi, x)(\\varphi(s, \\xi,-H(s, \\xi))-\\varphi(s, \\xi, x))\n$$\n\nThen, a.s., for all $\\varphi \\in L^{\\infty}\\left([0,1] ; C_{b}^{1}\\left(\\left[0, t_{*}\\right] \\times \\mathbb{R}\\right)\\right)$ and $t \\in\\left[0, t_{*}\\right]$,\n\n$$\n\\begin{aligned}\n& \\int_{[0,1] \\times \\mathbb{R}} \\varphi(t, \\xi, x)\\left(\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x)-\\mu_{\\#}(t, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi \\\\\n& \\quad=\\int_{[0,1] \\times \\mathbb{R}} \\varphi(0, \\xi, x)\\left(\\boldsymbol{\\mu}_{\\#}^{N}(0, \\xi, \\mathrm{~d} x)-\\mu_{\\#}(0, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi+\\boldsymbol{J}_{\\boldsymbol{D}}(\\varphi)+\\boldsymbol{J}_{\\boldsymbol{H}}(\\varphi)+\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& \\boldsymbol{J}_{\\boldsymbol{D}}(\\varphi):=\\int_{0}^{t} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\partial_{s}+D^{H}\\right) \\varphi(s, \\xi, x)\\left(\\boldsymbol{\\mu}_{\\#}^{N}(s, \\xi, \\mathrm{~d} x)-\\mu_{\\#}(s, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi \\mathrm{~d} s \\\\\n& \\boldsymbol{J}_{\\boldsymbol{H}}(\\varphi):=\\int_{0}^{t} \\int_{[0,1] \\times \\mathbb{R}}\\left[\\left(\\boldsymbol{b}_{\\#}^{N}(s, \\xi, x)-b_{\\#}(s, \\xi, x)\\right) \\partial_{x} \\varphi(s, \\xi, x)\\right. \\\\\n& \\left.\\quad-\\left(\\boldsymbol{f}_{\\#}^{N}(s, \\xi, x)-f_{\\#}(s, \\xi, x)\\right) \\varphi(s, \\xi, x)+\\boldsymbol{f}_{\\#}^{N}(s, \\xi, x) \\varphi(s, \\xi,-\\boldsymbol{H}^{N}(s, \\xi))\\right) \\\\\n& \\quad-f_{\\#}(s, \\xi, x) \\varphi(s, \\xi,-H(s, \\xi))] \\boldsymbol{\\mu}_{\\#}^{N}(s, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\mathrm{~d} s \\\\\n& \\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi):=\\int_{(\\xi, x) \\in[0,1] \\times \\mathbb{R}} \\int_{(s, z) \\in[0, t] \\times \\mathbb{R}_{+}} \\varphi(s, \\xi, x)\\left[-\\boldsymbol{\\mu}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)+\\boldsymbol{\\delta}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)\\right] \\\\\n& \\sum_{i=1}^{N} \\mathbb{1}_{E^{i, N}}(\\xi) \\mathbb{1}_{\\left\\{z \\leq \\boldsymbol{r}^{N}(s-, \\xi)\\right\\}}\\left[\\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right] \\mathrm{d} \\xi\n\\end{aligned}\n$$\n\nThrough Lemma 5 and the definition of the metric $\\Phi_{w}^{-1}$, we can readily obtain that, a.s.,\n\n$$\n\\begin{aligned}\n& \\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(t, \\cdot, \\cdot)-\\mu_{\\#}(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{w}^{-1}} \\\\\n& =\\sup _{\\|\\phi\\|_{\\Phi_{w}} \\leq 1, \\phi \\in L_{\\xi}^{\\infty} C_{x}^{1}} \\int_{[0,1] \\times \\mathbb{R}} \\phi(\\xi, x)\\left(\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x)-\\mu_{\\#}(t, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi \\\\\n& =\\sup _{\\|\\phi\\|_{\\Phi_{w}} \\leq 1, \\phi \\in L_{\\xi}^{\\infty} C_{x}^{1}} \\operatorname{inf}_{(t, \\cdot, \\cdot)=\\phi, \\varphi \\in L_{\\xi}^{\\infty} C_{t, x}^{1}}\\left[\\int_{[0,1] \\times \\mathbb{R}} \\varphi(0, \\xi, x)\\left(\\boldsymbol{\\mu}_{\\#}^{N}(0, \\xi, \\mathrm{~d} x)-\\mu_{\\#}(0, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi\\right. \\\\\n& \\left.+\\boldsymbol{J}_{\\boldsymbol{D}}(\\varphi)+\\boldsymbol{J}_{\\boldsymbol{H}}(\\varphi)+\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)\\right]\n\\end{aligned}\n$$\n\nNote that in the original definition of the norm $\\|\\cdot\\|_{\\Phi_{w}^{-1}}$, we do not impose that $\\phi \\in L_{\\xi}^{\\infty} C_{x}^{1}$. Of course, there exist $\\phi$ that are not $C_{x}^{1}$-differentiable but still satisfy $\\|\\phi\\|_{\\Phi_{w}} \\leq 1$. Nonetheless, it is easy to verify that the subset of $C_{x}^{1}$ functions is dense in $\\Phi_{w}$. Therefore, it is sufficient to take the supremum over this dense subset.\n\nThe decomposition in (21) and (22) can be interpreted as follows: $\\boldsymbol{J}_{\\boldsymbol{D}}(\\varphi)$ corresponds to the definition of the distributional solution presented in (17), and vanishes if $\\varphi$ is a $C_{t, x}^{1}$ solution of the adjoint equation $\\left(\\partial_{t}+D^{H}\\right) \\varphi(t, \\xi, x)=0$ (which we will later call the dualbackward equation). On the other hand, $\\boldsymbol{\\mu}_{\\#}^{N}$ does not exactly solve (17), leading to the additional terms $\\boldsymbol{J}_{\\boldsymbol{H}}(\\varphi)$ and $\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)$. Like in Section 3.3, we put in $\\boldsymbol{J}_{\\boldsymbol{H}}(\\varphi)$ the parts that can be bounded a.s., and leave in $\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)$ the effect of the random resets of the membrane potentials due to the Poisson random measures.\n3.5. Propagation of regularity along the dual-backward equation. In this subsection, we show the propagation of the $\\Phi_{w}$ norm along the adjoint equation backward in time. The $\\Phi_{w}^{-1}$ stability of measures follows from a primal-dual argument on the right-hand side of (22).\n\nLemma 6. For any $H \\in L^{\\infty}\\left([0,1] ; C\\left(\\left[0, t_{*}\\right]\\right)\\right), \\phi \\in L^{\\infty}\\left([0,1] ; C_{h}^{1}(\\mathbb{R})\\right)$, and any $t \\in$ $\\left[0, t_{*}\\right]$, there exists a unique solution to the dual-backward equation in the sense of characteristics,\n\n$$\n\\begin{aligned}\n\\left(\\partial_{s}+D^{H}\\right) \\varphi(s, \\xi, x) & =0, \\quad s \\in[0, t] \\\\\n\\varphi(t, \\xi, x) & =\\phi(\\xi, x)\n\\end{aligned}\n$$\n\nsuch that $\\varphi \\in L^{\\infty}\\left([0,1] ; C_{h}^{1}([0, t] \\times \\mathbb{R})\\right)$. Assume that $\\|\\phi\\|_{\\Phi_{w}} \\leq 1$. Then, by choosing\n\n$$\n\\begin{aligned}\n& \\kappa_{2}(t):=\\exp \\left[\\left(2\\|f\\|_{L^{\\infty}}+2\\left\\|\\partial_{x} f\\right\\|_{L^{1}}+2\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}}+\\left\\|\\partial_{x} b\\right\\|_{L^{\\infty}}\\right) t\\right] \\\\\n& \\kappa_{3}(t):=\\exp \\left(2\\|f\\|_{L^{\\infty}} t\\right)\\left\\{1+\\frac{t^{2}}{2}\\|f\\|_{L^{\\infty}}\\left(\\left\\|\\partial_{x} b\\right\\|_{L^{\\infty}}+2\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}}+\\|f\\|_{L^{\\infty}}\\right) \\kappa_{2}(t)\\right\\}\n\\end{aligned}\n$$\n\nwe have\n\n$$\n\\begin{gathered}\n\\sup _{s \\in[0, t]} \\max \\left(\\|\\varphi(s, \\cdot, \\cdot)\\|_{L_{\\xi, x}^{\\infty}},\\left\\|\\partial_{x} \\varphi(s, \\cdot, \\cdot)\\right\\|_{L_{\\xi}^{\\infty} L_{x}^{1}},\\left\\|\\partial_{x} \\varphi(s, \\cdot, \\cdot)\\right\\|_{L_{\\xi, x}^{\\infty}}\\right) \\leq \\kappa_{2}(t) \\\\\n\\int_{[0,1]} \\sup _{s \\in[0, t]} \\sup _{x \\in \\mathbb{R}} \\mid \\varphi(s, \\xi-h, x)-\\varphi(s, \\xi, x) \\mid \\mathrm{d} \\xi \\leq \\epsilon_{w}(|h|) \\kappa_{3}(t)\n\\end{gathered}\n$$\n\nAs a consequence,\n\n$$\n\\sup _{s \\in[0, t]}\\|\\varphi(s, \\cdot, \\cdot)\\|_{\\Phi_{w}} \\leq \\kappa(t):=\\max \\left(\\kappa_{2}(t), \\kappa_{3}(t)\\right)\n$$\n\nand\n\n$$\n\\left\\|\\partial_{t} \\varphi\\right\\|_{L_{L_{k, s}^{\\infty}}^{+\\infty}} \\leq \\kappa_{4}(t):=\\left(\\|b\\|_{L^{\\infty}}+2\\|f\\|_{L^{\\infty}}\\right) \\kappa_{2}(t)\n$$\n\nProof. For a.e. $\\xi \\in[0,1]$, we can associate a unique and well-defined flow solving the system of characteristic equations\n\n$$\n\\left\\{\\begin{array}{l}\n\\frac{\\mathrm{d}}{\\mathrm{~d} s} Y_{H}(t, s, \\xi, x)=b_{\\#}\\left(s, \\xi, Y_{H}(t, s, \\xi, x)\\right) \\\\\nY_{H}(t, t, \\xi, x)=x\n\\end{array}\\right.\n$$\n\nBy the method of characteristics, we can derive the identity\n\n$$\n\\begin{aligned}\n\\varphi(\\tau, \\xi, x)= & \\phi\\left(\\xi, Y_{H}(\\tau, t, \\xi, x)\\right) \\exp \\left(-\\int_{\\tau}^{t} f_{\\#}\\left(s, \\xi, Y_{H}(\\tau, s, \\xi, x)\\right) \\mathrm{d} s\\right) \\\\\n& +\\int_{\\tau}^{t} \\varphi(r, \\xi,-H(r, \\xi)) \\exp \\left(-\\int_{\\tau}^{r} f_{\\#}\\left(s, \\xi, Y_{H}(\\tau, s, \\xi, x)\\right) \\mathrm{d} s\\right) \\mathrm{d} r\n\\end{aligned}\n$$\n\nfrom which we can prove the uniqueness of a solution $\\varphi \\in L^{\\infty}\\left([0,1] ; C_{b}^{1}\\left(\\left[0, t_{*}\\right] \\times \\mathbb{R}\\right)\\right)$ through a contraction argument, using the iteration\n\n$$\n\\begin{aligned}\n\\varphi^{(k)}(\\tau, \\xi, x)= & \\phi\\left(\\xi, Y_{H}(\\tau, t, \\xi, x)\\right) \\exp \\left(-\\int_{\\tau}^{t} f_{\\#}\\left(s, \\xi, Y_{H}(\\tau, s, \\xi, x)\\right) \\mathrm{d} s\\right) \\\\\n& +\\int_{\\tau}^{t} \\varphi^{(k-1)}(r, \\xi,-H(r, \\xi)) \\exp \\left(-\\int_{\\tau}^{r} f_{\\#}\\left(s, \\xi, Y_{H}(\\tau, s, \\xi, x)\\right) \\mathrm{d} s\\right) \\mathrm{d} r\n\\end{aligned}\n$$\n\nWhile the detailed regularity estimate in the $x$-direction can also be deduced via the method of characteristics, we only present here a formal calculation due to the length a rigorous argument would take; turning the following calculation into a rigorous argument would not pose any major difficulty, since $\\varphi \\in L^{\\infty}\\left([0,1] ; C_{b}^{1}\\left(\\left[0, t_{*}\\right] \\times \\mathbb{R}\\right)\\right)$.\n\nWe omit the $t, \\xi$ variables for brevity. The test function $\\varphi$ satisfies the adjoint equation,\n\n$$\n\\partial_{t} \\varphi(x)+b_{\\#}(x) \\partial_{x} \\varphi(x)-f_{\\#}(x) \\varphi(x)+f_{\\#}(x) \\varphi(-H)=0\n$$\n\nand its $x$-derivative satisfies\n\n$$\n\\partial_{t} \\varphi_{x}(x)+\\partial_{x}\\left[b_{\\#}(x) \\varphi_{x}(x)\\right]-\\partial_{x} f_{\\#}(x)\\left[\\varphi(x)-\\varphi(-H)\\right]-f_{\\#}(x) \\varphi_{x}(x)=0\n$$\n\nIt is easy to verify that\n\n$$\n\\begin{aligned}\n\\left|\\frac{\\mathrm{d}}{\\mathrm{~d} t}\\left\\|\\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{L, s}^{\\infty}}\\right| \\leq & 2\\|f\\|_{L^{\\infty}}\\|\\varphi(t, \\cdot, \\cdot)\\|_{L_{L, s}^{\\infty}} \\\\\n\\left|\\frac{\\mathrm{d}}{\\mathrm{~d} t}\\left\\|\\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{L}^{\\infty} L_{x}^{\\perp}}\\right| \\leq & 2\\left\\|\\partial_{x} f\\left\\|_{L^{1}}\\right\\| \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{L, s}^{\\infty}}+\\|f\\|_{L^{\\infty}}\\left\\|\\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{L}^{\\infty} L_{x}^{\\perp}} \\\\\n\\left|\\frac{\\mathrm{d}}{\\mathrm{~d} t}\\left\\|\\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{L, s}^{\\infty}}\\right| \\leq & \\left\\|\\partial_{x} b\\left\\|_{L^{\\infty}}\\right\\| \\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{L, s}^{\\infty}}+2\\left\\|\\partial_{x} f\\left\\|_{L^{\\infty}}\\right\\| \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{L, s}^{\\infty}} \\\\\n& +\\|f\\|_{L^{\\infty}}\\left\\|\\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{L, s}^{\\infty}}\n\\end{aligned}\n$$\n\nwhich yields the regularity (24a) in the $x$-direction.\n\nNext, we consider the regularity in the $\\xi$-direction. For a.e. $\\xi \\in[0,1]$,\n\n$$\n\\begin{aligned}\n& \\left|\\frac{\\mathrm{d}}{\\mathrm{~d} t} \\sup _{x \\in \\mathbb{R}}\\right| \\varphi(t, \\xi-h, x)-\\varphi(t, \\xi, x)\\left|\\left|\\begin{array}{c}\n\\square \\\\\n\\leq \\sup _{x \\in \\mathbb{R}}\\left|\\left[b_{\\#}(t, \\xi-h, x)-b_{\\#}(t, \\xi, x)\\right] \\partial_{x} \\varphi(t, \\xi, x)\\right| \\\\\n+\\sup _{x \\in \\mathbb{R}}\\left|f_{\\#}(t, \\xi-h, x) \\varphi(t, \\xi-h, x)-f_{\\#}(t, \\xi, x) \\varphi(t, \\xi, x)\\right| \\\\\n& +\\sup _{x \\in \\mathbb{R}}\\left|f_{\\#}(t, \\xi-h, x) \\varphi(t, \\xi-h,-H(t, \\xi-h))-f_{\\#}(t, \\xi, x) \\varphi(t, \\xi,-H(t, \\xi))\\right| \\\\\n& \\leq 2\\|f\\|_{L^{\\infty}} \\sup _{x \\in \\mathbb{R}}|\\varphi(t, \\xi-h, x)-\\varphi(t, \\xi, x)| \\\\\n& +\\left(\\left\\|\\partial_{x} b\\right\\|_{L^{\\infty}}\\left\\|\\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{\\xi, x}^{\\infty}}+2\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}}\\left\\|\\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{\\xi, x}^{\\infty}}+\\|f\\|_{L^{\\infty}}\\left\\|\\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{\\xi, x}^{\\infty}}\\right) \\\\\n& |H(t, \\xi-h)-H(t, \\xi)|\n\\end{aligned}\n$$\n\nTaking the supremum over the interval $[0, t]$, integrating over $\\xi \\in[0,1]$, and then applying Gronwall's lemma, we obtain\n\n$$\n\\begin{aligned}\n& \\int_{[0,1]} \\sup _{s \\in[0, t]} \\sup _{x \\in \\mathbb{R}}|\\varphi(s, \\xi-h, x)-\\varphi(s, \\xi, x)| \\mathrm{d} \\xi \\\\\n& \\leq\\left\\{\\int_{[0,1]} \\sup _{x \\in \\mathbb{R}}|\\varphi(t, \\xi-h, x)-\\varphi(t, \\xi, x)| \\mathrm{d} \\xi+\\left(\\left\\|\\partial_{x} b\\right\\|_{L^{\\infty}}\\left\\|\\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{\\xi, x}^{\\infty}}\\right.\\right. \\\\\n& \\left.\\quad+2\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}}\\left\\|\\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{\\xi, x}^{\\infty}}+\\|f\\|_{L^{\\infty}}\\left\\|\\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{\\xi, x}^{\\infty}}\\right) \\\\\n& \\quad \\times \\int_{0}^{t} \\int_{[0,1]}|H(s, \\xi-h)-H(s, \\xi)| \\mathrm{d} \\xi \\mathrm{~d} s\\} \\exp \\left(2\\|f\\|_{L^{\\infty}} t\\right)\n\\end{aligned}\n$$\n\nThe difference $|H(t, \\xi-h)-H(t, \\xi)|$ can be bounded by the inherent regularity of $w$ characterized by the modulus of continuity $\\epsilon_{w}:[0,1] \\rightarrow[0, \\infty)$. To establish this, we first observe that\n\n$$\n\\begin{aligned}\nH(t, \\xi-h)-H(t, \\xi) & =\\int_{0}^{t} \\int_{[0,1]}[w(\\xi-h, \\zeta)-w(\\xi, \\zeta)] \\int_{\\mathbb{R}} f(x) \\mu(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} s \\\\\n& \\leq t\\|f\\|_{L^{\\infty}} \\int_{[0,1]}|w(\\xi-h, \\zeta)-w(\\xi, \\zeta)| \\mathrm{d} \\zeta\n\\end{aligned}\n$$\n\nIntegrating over the domain $[0, t] \\times[0,1]$, we encounter a double integral spanning $[0,1]^{2}$, which allows us to use the definition of $\\epsilon_{w}$, yielding\n\n$$\n\\begin{aligned}\n& \\int_{0}^{t} \\int_{[0,1]}|H(s, \\xi-h)-H(s, \\xi)| \\mathrm{d} \\xi \\mathrm{~d} s \\\\\n& \\quad \\leq \\int_{0}^{t} \\int_{[0,1]} s\\|f\\|_{L^{\\infty}} \\int_{[0,1]}|w(\\xi-h, \\zeta)-w(\\xi, \\zeta)| \\mathrm{d} \\zeta \\mathrm{~d} \\xi \\mathrm{~d} s \\\\\n& \\quad=\\frac{t^{2}}{2}\\|f\\|_{L^{\\infty}} \\int_{[0,1]^{2}}|w(\\xi-h, \\zeta)-w(\\xi, \\zeta)| \\mathrm{d} \\zeta \\mathrm{~d} \\xi \\\\\n& \\quad \\leq \\frac{t^{2}}{2}\\|f\\|_{L^{\\infty}} \\epsilon_{w}(|h|)\n\\end{aligned}\n$$\n\nPlugging this bound into the Gronwall inequality (25), we obtain the regularity in the $\\xi$ direction, as expressed in (24b):\n\n$$\n\\begin{aligned}\n& \\int_{[0,1]} \\sup _{s \\in[0, t]} \\sup _{x \\in \\mathbb{R}} \\mid \\varphi(s, \\xi-h, x)-\\varphi(s, \\xi, x) \\mid \\mathrm{d} \\xi \\\\\n& \\leq\\left\\{\\int_{[0,1]} \\sup _{x \\in \\mathbb{R}} \\mid \\varphi(t, \\xi-h, x)-\\varphi(t, \\xi, x) \\mid \\mathrm{d} \\xi+\\left(\\left\\|\\partial_{x} b\\right\\|_{L^{\\infty}}+2\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}}+\\|f\\|_{L^{\\infty}}\\right) \\times\\right. \\\\\n& \\left.\\max \\left(\\|\\varphi(t, \\cdot, \\cdot)\\|_{L_{L \\infty}^{\\infty}},\\left\\|\\partial_{x} \\varphi(t, \\cdot, \\cdot)\\right\\|_{L_{L \\infty}^{\\infty}}\\right) \\frac{t^{2}}{2}\\|f\\|_{L^{\\infty}} \\epsilon_{w}(|h|)\\right\\} \\exp \\left(2\\|f\\|_{L^{\\infty}} t\\right) \\\\\n& \\leq \\epsilon_{w}(|h|) \\kappa_{3}(t)\\|\\varphi(t, \\cdot, \\cdot)\\|_{\\Phi_{w}}\n\\end{aligned}\n$$\n\nFinally, combining the two regularity propagation estimates, we conclude that\n\n$$\n\\sup _{s \\in[0, t]}\\|\\varphi(s, \\cdot, \\cdot)\\|_{\\Phi_{w}} \\leq \\max \\left(\\kappa_{2}(t), \\kappa_{3}(t)\\right)\\|\\varphi(t, \\cdot, \\cdot)\\|_{\\Phi_{w}}\n$$\n\nThe last estimate,\n\n$$\n\\left\\|\\partial_{t} \\varphi\\right\\|_{L_{L, s}^{\\infty}} \\leq\\left(\\|b\\|_{L^{\\infty}}+2\\|f\\|_{L^{\\infty}}\\right) \\kappa_{2}(t)\\|\\varphi(t, \\cdot, \\cdot)\\|_{\\Phi_{w}}\n$$\n\ncan also easily be derived from the dual-backward equation (23).\nTo estimate the sup-inf expression on the right-hand side of (22), given any $\\phi \\in$ $L^{\\infty}\\left([0,1] ; C_{L}^{1}(\\mathbb{R})\\right)$ such that $\\|\\phi\\|_{\\Phi_{w}} \\leq 1$, we choose $\\varphi$ as the solution to the dual-backward equation (23). With this choice, we ensure that $\\boldsymbol{J}_{\\boldsymbol{D}}(\\varphi)=0$ and have an a priori estimates provided by Lemma 6 , which gives\n\n$$\n\\begin{aligned}\n& \\sup _{\\|\\phi\\|_{\\Phi_{w}} \\leq 1, \\phi \\in L_{L}^{\\infty} C_{L}^{1}, \\varphi \\text { solves (23) }}\\left[\\int_{[0,1] \\times \\mathbb{R}} \\varphi(0, \\xi, x)\\left(\\boldsymbol{\\mu}_{\\#}^{N}(0, \\xi, \\mathrm{~d} x)-\\mu_{\\#}(0, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi\\right] \\\\\n& \\quad \\leq \\kappa(t)\\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(0, \\cdot, \\cdot)-\\mu_{\\#}(0, \\cdot, \\cdot)\\right\\|_{\\Phi_{w}^{-1}}\n\\end{aligned}\n$$\n\nIn addition, we can bound $\\boldsymbol{J}_{\\boldsymbol{H}}(\\varphi)$ as\n\n$$\n\\begin{aligned}\n& \\left|\\boldsymbol{J}_{\\boldsymbol{H}}(\\varphi)\\right| \\\\\n& \\leq \\int_{0}^{t} \\int_{[0,1] \\times \\mathbb{R}}\\left[\\left|\\boldsymbol{b}_{\\#}^{N}(s, \\xi, x)-b_{\\#}(s, \\xi, x)\\right| \\mid \\partial_{x} \\varphi(s, \\xi, x)\\right] \\\\\n& \\quad-\\left|\\boldsymbol{f}_{\\#}^{N}(s, \\xi, x)-f_{\\#}(s, \\xi, x)\\right||\\varphi(s, \\xi, x)| \\\\\n& \\quad+\\left|\\boldsymbol{f}_{\\#}^{N}(s, \\xi, x) \\varphi(s, \\xi,-\\boldsymbol{H}^{N}(s, \\xi))-f_{\\#}(s, \\xi, x) \\varphi(s, \\xi,-H(s, \\xi))\\right|\\right] \\boldsymbol{\\mu}_{\\#}^{N}(s, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\mathrm{~d} s \\\\\n& \\leq \\int_{0}^{t} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\left\\|\\partial_{x} b\\right\\|_{L^{\\infty}}+\\|f\\|_{L^{\\infty}}+2\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}}\\right) \\kappa_{2}(t)\\left|\\boldsymbol{H}^{N}(s, \\xi)-H(s, \\xi)\\right| \\\\\n& \\times \\boldsymbol{\\mu}_{\\#}^{N}(s, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\mathrm{~d} s\n\\end{aligned}\n$$\n\nIntegrate first over $x$ and then over $\\xi$, we conclude that\n\n$$\n\\begin{aligned}\n& \\sup _{\\|\\phi\\|_{\\Phi_{w}} \\leq 1, \\phi \\in L_{L}^{\\infty} C_{L}^{1}, \\varphi \\text { solves (23) }}\\left|\\boldsymbol{J}_{\\boldsymbol{H}}(\\varphi)\\right| \\leq \\kappa_{5}(t) \\int_{0}^{t}\\left\\|\\boldsymbol{H}^{N}(s, \\cdot)-H(s, \\cdot)\\right\\|_{L_{L}^{1}} \\mathrm{~d} s \\\\\n& \\kappa_{5}(t):=\\left(\\left\\|\\partial_{x} b\\right\\|_{L^{\\infty}}+\\|f\\|_{L^{\\infty}}+2\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}}\\right) \\kappa_{2}(t)\n\\end{aligned}\n$$\n\n(Recall that a bound for $\\mathbb{E}\\left[\\left\\|\\boldsymbol{H}^{N}(s, \\cdot)-H(s, \\cdot)\\right\\|_{L_{L}^{1}}\\right]$ has already been derived in Lemmas 3 and 4.)\n\n3.6. Effect of the random resets of the membrane potentials. It remains to estimate $\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)$ in (22), which summarizes the effect of the random resets of the membrane potentials due to the randomness of the Poisson random measures. Recall that for all $\\varphi \\in$ $L^{\\infty}\\left([0,1] ; C_{b}^{1}\\left(\\left[0, t_{*}\\right] \\times \\mathbb{R}\\right)\\right)$,\n\n$$\n\\begin{gathered}\n\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi):=\\int_{(\\xi, x) \\in[0,1] \\times \\mathbb{R}} \\int_{(s, z) \\in[0, t] \\times \\mathbb{R}_{+}} \\varphi(s, \\xi, x)\\left[-\\boldsymbol{\\mu}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)+\\boldsymbol{\\delta}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)\\right] \\\\\n\\sum_{i=1}^{N} \\mathbb{1}_{E^{i, N}}(\\xi) \\mathbb{1}_{\\left\\{z \\leq \\boldsymbol{r}^{N}(s-, \\xi)\\right\\}}\\left[\\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right] \\mathrm{d} \\xi\n\\end{gathered}\n$$\n\nwhere\n\n$$\n\\varphi^{i ; N}(s, x)=\\int_{E^{i, N}} \\varphi(s, \\xi, x) \\mathrm{d} \\xi\n$$\n\nWe introduce the compensated jump process,\n\n$$\n\\bar{Z}_{t}^{i ; N}(\\xi)=\\int_{[0, t] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{z \\leq \\boldsymbol{r}^{N}(s-, \\xi)\\right\\}}\\left[\\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right]\n$$\n\nso that we can write\n\n$$\n\\begin{aligned}\n& \\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi) \\\\\n& =\\int_{[0, t] \\times[0,1] \\times \\mathbb{R}} \\varphi(s, \\xi, x)\\left[-\\boldsymbol{\\mu}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)+\\boldsymbol{\\delta}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)\\right] \\sum_{i=1}^{N} \\mathbb{1}_{E^{i, N}}(\\xi) \\mathrm{d} \\bar{Z}_{s}^{i ; N}(\\xi) \\mathrm{d} \\xi\n\\end{aligned}\n$$\n\nNote that, for all $\\xi \\in[0,1]$ and $t \\geq 0, \\mathbb{E}\\left[\\bar{Z}_{t}^{i ; N}(\\xi)\\right]=0$, which implies that $\\mathbb{E}\\left[\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)\\right]=0$. In analogy to Lemma 4, we can expect that $\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)$ vanishes as $N \\rightarrow \\infty$ because of some sort of law of large numbers. In the following, we prove this intuition, using a mollification procedure and It\u00f4 isometry. For notational convenience, we will write\n\n$$\n\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)=\\int_{[0, t] \\times[0,1] \\times \\mathbb{R}} \\varphi(s, \\xi, x) \\boldsymbol{F}^{N}(\\mathrm{~d} s, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi\n$$\n\nwhere\n\n$$\n\\boldsymbol{F}^{N}(\\mathrm{~d} s, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi=\\left[-\\boldsymbol{\\mu}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)+\\boldsymbol{\\delta}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)\\right] \\sum_{i=1}^{N} \\mathbb{1}_{E^{i, N}}(\\xi) \\mathrm{d} \\bar{Z}_{s}^{i ; N}(\\xi) \\mathrm{d} \\xi\n$$\n\nIn order to interpolate between the \"oscillation\" of $\\varphi$ and the randomness of $\\boldsymbol{F}^{N}$, we introduce the mollification kernel $\\chi$, which is a normalized indicator function with support size $[0, \\Delta t] \\times[-\\Delta \\xi, \\Delta \\xi] \\times[-\\Delta x, \\Delta x]$ :\n\n$$\n\\begin{aligned}\n\\chi(s, \\xi, x) & :=\\chi_{1}(s) \\chi_{2}(\\xi) \\chi_{3}(x):=\\frac{1}{\\Delta t} \\mathbb{1}_{[0, \\Delta t]}(s) \\frac{1}{2 \\Delta \\xi} \\mathbb{1}_{[-\\Delta \\xi, \\Delta \\xi]}(\\xi) \\frac{1}{2 \\Delta x} \\mathbb{1}_{[-\\Delta x, \\Delta x]}(x) \\\\\n\\chi^{\\dagger}(s, \\xi, x) & :=\\chi(-s,-\\xi,-x)\n\\end{aligned}\n$$\n\nNotably, $\\chi \\star \\varphi$ involves a convolution along both the $\\xi$ - and $t$-directions, thereby requiring our embedding of $[0,1]$ in the torus (see Sec. 2.3) and an extended definition for $\\varphi$ when $t<0$. As the definition of the dual operator $D^{H}$ only requires $H \\in L^{\\infty}\\left([0,1] ; C\\left(\\left[0, t_{*}\\right]\\right)\\right)$, we can take the formal extension $H(s, \\xi)=0$ for negative time $s \\in[-\\Delta t, 0]$ and solve (23) over $[-\\Delta t, t]$. With this extension, all a priori estimates for $\\varphi$ in Lemma 6 apply, with the sole\n\nmodification that $\\kappa(t)$ has to be replaced by $\\kappa(t+\\Delta t)$. We can further decompose $\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)$ into two parts by using $\\varphi=[\\varphi-(\\chi \\star \\varphi)]+(\\chi \\star \\varphi)$. The first part is bounded by\n\n$$\n\\begin{aligned}\n\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi-(\\chi \\star \\varphi))= & \\int_{[0, t] \\times[0,1] \\times \\mathbb{R}}[\\varphi(s, \\xi, x)-(\\chi \\star \\varphi)(s, \\xi, x)] \\boldsymbol{F}^{N}(\\mathrm{~d} s, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\\\\n\\leq & \\left\\{\\int_{[0,1]}\\left(\\sup _{s \\in[0, t], x \\in \\mathbb{R}}|\\varphi(s, \\xi, x)-(\\chi \\star \\varphi)(s, \\xi, x)|\\right)^{2} \\mathrm{~d} \\xi\\right\\}^{\\frac{1}{2}} \\\\\n& \\left\\{\\int_{[0,1]}\\left(\\int_{[0, t] \\times \\mathbb{R}} \\boldsymbol{F}^{N}(\\mathrm{~d} s, \\xi, \\mathrm{~d} x)\\right)^{2} \\mathrm{~d} \\xi\\right\\}^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nFor any moment function $\\eta: \\mathbb{R} \\rightarrow \\mathbb{R}_{+}$such that $\\left\\|\\eta^{-1}\\right\\|_{L^{1}}<\\infty$, the second part is bounded by\n\n$$\n\\begin{aligned}\n& \\boldsymbol{J}_{\\boldsymbol{F}}(\\chi \\star \\varphi)=\\int_{[0, t] \\times[0,1] \\times \\mathbb{R}}(\\chi \\star \\varphi)(s, \\xi, x) \\boldsymbol{F}^{N}(\\mathrm{~d} s, \\xi, \\mathrm{~d} x) \\mathrm{d} \\xi \\\\\n& \\quad=\\int_{-\\infty}^{+\\infty} \\int_{[0,1] \\times \\mathbb{R}} \\varphi(s, \\xi, x) \\mathbb{1}_{[-\\Delta t, t]}(s)\\left[\\chi^{\\dagger} \\star\\left(\\boldsymbol{F}^{N} \\mathbb{1}_{[0, t] \\times[0,1] \\times \\mathbb{R}}\\right)\\right](s, \\xi, x) \\mathrm{d} \\xi \\mathrm{~d} x \\mathrm{~d} s \\\\\n& \\quad \\leq\\left\\{\\int_{-\\infty}^{+\\infty} \\int_{[0,1] \\times \\mathbb{R}}(\\varphi(s, \\xi, x))^{2} \\mathbb{1}_{[-\\Delta t, t]}(s) \\eta^{-1}(x) \\mathrm{d} \\xi \\mathrm{~d} x \\mathrm{~d} s\\right\\}^{\\frac{1}{2}} \\\\\n& \\quad\\left\\{\\int_{-\\infty}^{+\\infty} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\left[\\chi^{\\dagger} \\star\\left(\\boldsymbol{F}^{N} \\mathbb{1}_{[0, t] \\times[0,1] \\times \\mathbb{R}}\\right)\\right](s, \\xi, x)\\right)^{2} \\eta(x) \\mathrm{d} \\xi \\mathrm{~d} x \\mathrm{~d} s\\right\\}^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nThe two factors involving $\\varphi$ above can be respectively bounded by\n\n$$\n\\begin{aligned}\n& \\sup _{\\|\\phi\\|_{\\Phi_{w}} \\leq 1, \\phi \\in L_{1}^{\\infty} C_{+}^{1}, \\varphi \\text { solves }(\\text { (23) }}\\left\\{\\int_{[0,1]}\\left(\\sup _{s \\in[0, t], x \\in \\mathbb{R}}|\\varphi(s, \\xi, x)-(\\chi \\star \\varphi)(s, \\xi, x)|\\right)^{2} \\mathrm{~d} \\xi\\right\\}^{\\frac{1}{2}} \\\\\n& \\leq \\sup _{\\ldots}\\left[\\int_{[0,1]} \\sup _{s, x}|\\varphi(s, \\xi, x)-(\\chi \\star \\varphi)(s, \\xi, x)| \\mathrm{d} \\xi\\right]^{\\frac{1}{2}} \\sup _{s, \\xi, x}|\\varphi(s, \\xi, x)-(\\chi \\star \\varphi)(s, \\xi, x)|^{\\frac{1}{2}} \\\\\n& \\leq \\sup _{\\ldots}\\left[\\left\\|\\partial_{t} \\varphi\\right\\|_{L^{\\infty}} \\Delta t+\\left\\|\\partial_{x} \\varphi\\right\\|_{L^{\\infty}} \\Delta x+\\|\\varphi\\|_{\\Phi_{w}} \\epsilon_{w}(\\Delta \\xi)\\right]^{\\frac{1}{2}}\\left(2\\|\\varphi\\|_{L^{\\infty}}\\right)^{\\frac{1}{2}} \\\\\n& \\leq \\max (\\kappa(t+\\Delta t), \\kappa_{4}(t+\\Delta t)) \\sqrt{2\\left(\\Delta t+\\Delta x+\\epsilon_{w}(\\Delta \\xi)\\right)}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\sup _{\\|\\phi\\|_{\\Phi_{w}} \\leq 1, \\phi \\in L_{1}^{\\infty} C_{+}^{1}, \\varphi \\text { solves (23) }}\\left\\{\\int_{-\\infty}^{+\\infty} \\int_{[0,1] \\times \\mathbb{R}}(\\varphi(s, \\xi, x))^{2} \\mathbb{1}_{[-\\Delta t, t]}(s) \\eta^{-1}(x) \\mathrm{d} \\xi \\mathrm{~d} x \\mathrm{~d} s\\right\\}^{\\frac{1}{2}} \\\\\n& \\leq \\sup _{\\ldots}\\left\\{\\|\\varphi\\|_{L^{\\infty}}^{2}\\left\\|\\eta^{-1}\\right\\|_{L^{1}}(t+\\Delta t)\\right\\}^{\\frac{1}{2}} \\\\\n& \\leq \\kappa(t+\\Delta t)\\left(\\left\\|\\eta^{-1}\\right\\|_{L^{1}}(t+\\Delta t)\\right)^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nThe mollification procedure perfomed above reduces the proof of the convergence of $\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)$ to zero as $N \\rightarrow \\infty$ to the following lemma, which relies on It\u00f4 isometry.\n\nLEMMA 7. The following estimate holds:\n\n$$\n\\mathbb{E}\\left[\\left\\{\\int_{[0,1]}\\left(\\int_{[0, t] \\times \\mathbb{R}} \\boldsymbol{F}^{N}(\\mathrm{~d} s, \\xi, \\mathrm{~d} x)\\right)^{2} \\mathrm{~d} \\xi\\right\\}^{\\frac{1}{2}}\\right] \\leq\\left(16\\|f\\|_{L^{\\infty}}^{2} t^{2}+4\\|f\\|_{L^{\\infty}} t\\right)^{\\frac{1}{2}}\n$$\n\nMoreover, given the a priori moment bound\n$(28 b)$\n\n$$\n\\sup _{s \\in[0, t]} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\eta\\left(\\left|\\boldsymbol{X}^{i ; N}(s-)-\\boldsymbol{H}^{i ; N}(s-) \\mid+\\Delta x\\right)+\\eta\\left(\\left|\\boldsymbol{H}^{i ; N}(s-) \\mid+\\Delta x\\right)\\right] \\leq M_{\\eta}(t)\\right.\n$$\n\nthe following estimate holds:\n\n$$\n\\begin{aligned}\n& (\\text { 28c) } \\mathbb{E}\\left[\\left\\{\\int_{\\mathbb{R} \\times[0,1] \\times \\mathbb{R}}\\left(\\left[\\chi^{\\dagger} \\star\\left(\\boldsymbol{F}^{N} \\mathbb{1}_{[0, t] \\times[0,1] \\times \\mathbb{R}}\\right)\\right](s, \\xi, x)\\right)^{2} \\eta(x) \\mathrm{d} \\xi \\mathrm{~d} x \\mathrm{~d} s\\right\\}^{\\frac{1}{2}}\\right] \\\\\n& \\leq \\frac{1}{\\sqrt{N}}\\left(\\frac{\\|f\\|_{L^{\\infty}} t}{2 \\Delta t \\Delta \\xi \\Delta x} M_{\\eta}(t)\\right)^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nProof. Proof of (28a): It is easy to verify that\n\n$$\n\\begin{aligned}\nE_{1} & :=\\mathbb{E}\\left[\\left\\{\\int_{[0,1]}\\left(\\int_{[0, t] \\times \\mathbb{R}} \\boldsymbol{F}^{N}(\\mathrm{~d} s, \\xi, \\mathrm{~d} x)\\right)^{2} \\mathrm{~d} \\xi\\right\\}^{\\frac{1}{2}}\\right] \\\\\n& \\leq \\mathbb{E}\\left[\\int_{[0,1]}\\left(\\int_{[0, t] \\times \\mathbb{R}}\\left|\\boldsymbol{F}^{N}(\\mathrm{~d} s, \\xi, \\mathrm{~d} x)\\right|\\right)^{2} \\mathrm{~d} \\xi\\right]^{\\frac{1}{2}} \\\\\n& \\leq \\mathbb{E}\\left[\\int_{[0,1]}\\left(\\int_{[0, t] \\times \\mathbb{R}_{+}} \\int_{\\mathbb{R}}\\left[\\boldsymbol{\\mu}_{\\mathscr{G}}^{N}(s-, \\xi, \\mathrm{~d} x)+\\boldsymbol{\\delta}_{\\mathscr{G}}^{N}(s-, \\xi, \\mathrm{~d} x)\\right]\\right.\\right. \\\\\n& \\left.\\left.\\sum_{i=1}^{N} \\mathbb{1}_{E^{i, N}}(\\xi) \\mathbb{1}_{\\left\\{z \\leq \\boldsymbol{r}^{N}(s-, \\xi)\\right\\}}\\left[\\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)+\\mathrm{d} s \\mathrm{~d} z\\right]\\right)^{2} \\mathrm{~d} \\xi\\right]^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nSince $\\left\\|\\boldsymbol{r}^{N}\\right\\|_{L^{\\infty}} \\leq\\|f\\|_{L^{\\infty}}$ holds a.s., we can consider the integration $\\mathrm{d} z$ over $z \\in\\left[0,\\|f\\|_{L^{\\infty}}\\right] \\subset$ $\\mathbb{R}_{+}$. Integrating first over $\\mathrm{d} x$ and then over $\\mathrm{d} z$, we obtain the following:\n\n$$\n\\begin{aligned}\nE_{1} & \\leq \\mathbb{E}\\left[\\int_{[0,1]}\\left(\\int_{[0, t] \\times[0,\\|f\\|_{L^{\\infty}}]} 2 \\sum_{i=1}^{N} \\mathbb{1}_{E^{i, N}}(\\xi)\\left[\\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)+\\mathrm{d} s \\mathrm{~d} z\\right]\\right)^{2} \\mathrm{~d} \\xi\\right]^{\\frac{1}{2}} \\\\\n& =\\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N}\\left(2 \\int_{[0, t] \\times[0,\\|f\\|_{L^{\\infty}}]}\\left[\\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)+\\mathrm{d} s \\mathrm{~d} z\\right]\\right)^{2}\\right]^{\\frac{1}{2}} \\\\\n& =\\left(16\\|f\\|_{L^{\\infty}}^{2} t^{2}+4\\|f\\|_{L^{\\infty}} t\\right)^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nwhere the last line is a straightforward computation of the moment of a Poisson random measure.\n\nProof of (28c): Replace the variables $(s, \\xi, x)$ in the integration of (28c) by $(\\tau, \\theta, y)$, we get\n\n$$\n\\begin{aligned}\nE_{2} & :=\\mathbb{E}\\left[\\left\\{\\int_{-\\infty}^{+\\infty} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\left[\\chi^{\\dagger} \\star\\left(\\boldsymbol{F}^{N} \\mathbb{1}_{[0, t] \\times[0,1] \\times \\mathbb{R}}\\right)\\right](\\tau, \\theta, y)\\right)^{2} \\eta(y) \\mathrm{d} \\theta \\mathrm{~d} y \\mathrm{~d} \\tau\\right)^{\\frac{1}{2}}\\right] \\\\\n& \\leq \\mathbb{E}\\left[\\int_{-\\infty}^{+\\infty} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\left[\\chi^{\\dagger} \\star\\left(\\boldsymbol{F}^{N} \\mathbb{1}_{[0, t] \\times[0,1] \\times \\mathbb{R}}\\right)\\right](\\tau, \\theta, y)\\right)^{2} \\eta(y) \\mathrm{d} \\theta \\mathrm{~d} y \\mathrm{~d} \\tau\\right]^{\\frac{1}{2}} \\\\\n& =\\mathbb{E}\\left[\\int_{(-\\infty, \\infty) \\times[0,1] \\times \\mathbb{R}}\\left(\\int_{[0,1]} \\int_{(-\\infty, \\infty) \\times \\mathbb{R}_{+}} \\int_{\\mathbb{R}} \\chi(s-\\tau, \\xi-\\theta, x-y) \\mathbb{1}_{[0, t]}(s)\\right.\\right. \\\\\n& \\left.\\left.\\left[\\boldsymbol{\\mu}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)-\\boldsymbol{\\delta}_{\\#}^{N}(s-, \\xi, \\mathrm{~d} x)\\right]\\right.\\right. \\\\\n& \\left.\\sum_{i=1}^{N} \\mathbb{1}_{E^{i, N}}(\\xi) \\mathbb{1}_{\\{z \\leq \\boldsymbol{r}^{N}(s-, \\xi)\\}}\\left[\\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right] \\mathrm{d} \\xi\\right)^{2} \\eta(y) \\mathrm{d} \\tau \\mathrm{~d} \\theta \\mathrm{~d} y]^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nHere, the random measures have finite total variation with probability one. Hencey, Fubini's theorem for measures applies a.s., leading to\n\n$$\n\\begin{aligned}\n& E_{2} \\leq \\mathbb{E}\\left[\\int_{(-\\infty, \\infty) \\times[0,1] \\times \\mathbb{R}}\\left(\\sum_{i=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}}\\right.\\right. \\\\\n& \\left(\\int_{E^{i, N}} \\chi(s-\\tau, \\xi-\\theta, \\boldsymbol{Y}^{i ; N}(s-)-y)-\\chi(s-\\tau, \\xi-\\theta,-\\boldsymbol{H}^{i ; N}(s-)-y) \\mathrm{d} \\xi\\right)^{2}\\right. \\\\\n& \\left.\\left.\\mathbb{1}_{\\{z \\leq f\\left(\\boldsymbol{X}^{i ; N}(s-) \\}}\\right\\}\\left[\\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right]\\right)^{2} \\eta(y) \\mathrm{d} \\tau \\mathrm{~d} \\theta \\mathrm{~d} y\\right]^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nBy It\u00f4 isometry, we have\n\n$$\n\\begin{aligned}\n& E_{2} \\leq \\mathbb{E}\\left[\\int_{(-\\infty, \\infty) \\times[0,1] \\times \\mathbb{R}} \\sum_{i=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}}\\right. \\\\\n& \\left(\\int_{E^{i, N}} \\chi(s-\\tau, \\xi-\\theta, \\boldsymbol{Y}^{i ; N}(s-)-y)-\\chi(s-\\tau, \\xi-\\theta,-\\boldsymbol{H}^{i ; N}(s-)-y) \\mathrm{d} \\xi)^{2}\\right. \\\\\n& \\left.\\mathbb{1}_{\\{z \\leq f\\left(\\boldsymbol{X}^{i ; N}(s-) \\}}\\right) \\mathrm{d} s \\mathrm{~d} z \\eta(y) \\mathrm{d} \\tau \\mathrm{~d} \\theta \\mathrm{~d} y\\right]^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nAgain, the integration in $z$ can be restricted over $z \\in\\left[0,\\|f\\|_{L^{\\infty}}\\right] \\subset \\mathbb{R}_{+}$so that\n\n$$\n\\begin{aligned}\n& E_{2} \\leq \\mathbb{E}\\left[\\int_{(-\\infty, \\infty) \\times[0,1] \\times \\mathbb{R}} \\sum_{i=1}^{N} \\int_{[0, t]}\\right. \\\\\n& \\left.\\quad\\left(\\chi_{1}(s-\\tau)\\right)^{2}\\left(\\int_{E^{i, N}} \\chi_{2}(\\xi-\\theta) \\mathrm{d} \\xi\\right)^{2}\\left(\\chi_{3}\\left(\\boldsymbol{Y}^{i ; N}(s-)-y\\right)-\\chi_{3}\\left(-\\boldsymbol{H}^{i ; N}(s-)-y\\right)\\right)^{2}\\right. \\\\\n& \\quad\\|f\\|_{L^{\\infty}} \\mathrm{d} s \\eta(y) \\mathrm{d} \\tau \\mathrm{~d} \\theta \\mathrm{~d} y]^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nTaking the integral over $s$ last and separating the variables $\\tau, \\theta$ and $y$ in the integration, we see that\n\n$$\n\\begin{aligned}\n& E_{2} \\leq \\mathbb{E}\\left[\\|f\\|_{L^{\\infty}} \\int_{[0, t]}\\left(\\int_{(-\\infty, \\infty)}\\left(\\chi_{1}(s-\\tau)\\right)^{2} \\mathrm{~d} \\tau\\right)\\right. \\\\\n& \\sum_{i=1}^{N}\\left(\\int_{[0,1]}\\left(\\int_{E^{i, N}} \\chi_{2}(\\xi-\\theta) \\mathrm{d} \\xi\\right)^{2} \\mathrm{~d} \\theta\\right) \\\\\n&\\left.\\left(\\int_{\\mathbb{R}}\\left(\\chi_{3}\\left(\\boldsymbol{Y}^{i ; N}(s-)-y\\right)-\\chi_{3}\\left(-\\boldsymbol{H}^{i ; N}(s-)-y\\right)\\right)^{2} \\eta(y) \\mathrm{d} y\\right) \\mathrm{d} s\\right]^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nBy the definition of $\\chi_{1}$, we have\n\n$$\n\\int_{(-\\infty, \\infty)}\\left(\\chi_{1}(s-\\tau)\\right)^{2} \\mathrm{~d} \\tau=\\frac{1}{\\Delta t}\n$$\n\nNoticing that $E^{i ; N}=1 / N, 1 \\leq i \\leq N$, we also have\n\n$$\n\\begin{aligned}\n\\int_{[0,1]}\\left(\\int_{E^{i ; N}} \\chi_{2}(\\xi-\\theta) \\mathrm{d} \\xi\\right)^{2} \\mathrm{~d} \\theta & \\leq \\int_{[0,1]} \\frac{1}{N} \\int_{E^{i ; N}}\\left(\\chi_{2}(\\xi-\\theta)\\right)^{2} \\mathrm{~d} \\xi \\mathrm{~d} \\theta \\\\\n& =\\frac{1}{N^{2}(2 \\Delta \\xi)}\n\\end{aligned}\n$$\n\nwhere the final bound is independent of the index $i \\in\\{1, \\ldots, N\\}$.\nFinally, by the fact that $\\chi_{3}$ has compact support $[-\\Delta x, \\Delta x]$, we obtain that\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\int_{[0, t]} \\sum_{i=1}^{N} \\int_{\\mathbb{R}}\\left(\\chi_{3}\\left(\\boldsymbol{Y}^{i ; N}(s-)-y\\right)-\\chi_{3}\\left(-\\boldsymbol{H}^{i ; N}(s-)-y\\right)\\right)^{2} \\eta(y) \\mathrm{d} y \\mathrm{~d} s\\right] \\\\\n& \\leq 2 \\sum_{i=1}^{N} \\mathbb{E}\\left[\\int_{[0, t]} \\int_{\\mathbb{R}}\\left(\\chi_{3}\\left(\\boldsymbol{Y}^{i ; N}(s-)-y\\right)\\right)^{2} \\eta(y) \\mathrm{d} y \\mathrm{~d} s\\right. \\\\\n& \\left.\\quad+\\int_{[0, t]} \\int_{\\mathbb{R}}\\left(\\chi_{3}\\left(-\\boldsymbol{H}^{i ; N}(s-)-y\\right)\\right)^{2} \\eta(y) \\mathrm{d} y \\mathrm{~d} s\\right] \\\\\n& \\leq 2 \\sum_{i=1}^{N} \\mathbb{E}\\left[\\int_{[0, t]} \\eta\\left(\\left|\\boldsymbol{Y}^{i ; N}(s-)\\right|+\\Delta x\\right) \\int_{\\mathbb{R}}\\left(\\chi_{3}\\left(\\boldsymbol{Y}^{i ; N}(s-)-y\\right)\\right)^{2} \\mathrm{~d} y \\mathrm{~d} s\\right. \\\\\n& \\left.\\quad+\\int_{[0, t]} \\eta\\left(\\left|\\boldsymbol{H}^{i ; N}(s-)\\right|+\\Delta x\\right) \\int_{\\mathbb{R}}\\left(\\chi_{3}\\left(-\\boldsymbol{H}^{i ; N}(s-)-y\\right)\\right)^{2} \\mathrm{~d} y \\mathrm{~d} s\\right] \\\\\n& =\\frac{2}{(2 \\Delta x)} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\int_{[0, t]} \\eta\\left(\\left|\\boldsymbol{Y}^{i ; N}(s-)\\right|+\\Delta x\\right)+\\eta\\left(\\left|\\boldsymbol{H}^{i ; N}(s-)\\right|+\\Delta x\\right) d s\\right]\n\\end{aligned}\n$$\n\nApplying the three inequalities to our previous calculation, we conclude that\n\n$$\n\\begin{aligned}\n& E_{2}:=\\mathbb{E}\\left[\\left\\{\\int_{-\\infty}^{+\\infty} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\left[\\chi^{\\dagger} \\star\\left(\\boldsymbol{F}^{N} \\mathbb{1}_{[0, t] \\times[0,1] \\times \\mathbb{R}}\\right)\\right](\\tau, \\theta, y)\\right)^{2} \\eta(y) \\mathrm{d} \\theta \\mathrm{~d} y \\mathrm{~d} \\tau\\right\\}^{\\frac{1}{2}}\\right] \\\\\n& \\leq \\frac{1}{\\sqrt{N}}\\left(\\frac{\\|f\\|_{L^{\\infty}} t}{2 \\Delta t \\Delta \\xi \\Delta x} \\sup _{s \\in[0, t]} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\eta\\left(\\left|\\boldsymbol{Y}^{i ; N}(s-)\\right|+\\Delta x\\right)+\\eta\\left(\\left|\\boldsymbol{H}^{i ; N}(s-)\\right|+\\Delta x\\right)\\right]\\right)^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nwhich is equivalent to (28c) by substituting $\\boldsymbol{Y}^{i ; N}(s-)=\\boldsymbol{X}^{i ; N}(s-)-\\boldsymbol{H}^{i ; N}(s-)$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 3.7. Concluding the proof. \n\nProof of Theorem 2. Let $\\left(\\boldsymbol{X}^{1: N}\\right)_{i=1}^{N}$ be a strong solution to the system (1) with $\\left(w_{i, j}^{N}\\right)_{i, j=1}^{N} \\in \\mathbb{R}^{N \\times N}$, as provided by Proposition 5, and let $\\mu$ be a solution to the mean-field PDE (3) in the sense of characteristics with $w \\in L^{\\infty}([0,1])$, as provided by Proposition 4. We recall the inequality established in Proposition 6,\n\n$$\n\\left\\|\\boldsymbol{\\mu}^{N}(t, \\cdot, \\cdot)-\\mu(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{w}}^{-1}} \\leq\\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(t, \\cdot, \\cdot)-\\mu_{\\#}(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{w}}^{-1}}+\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-H(t, \\cdot)\\right\\|_{L_{b}^{1}}\n$$\n\nThe first term on the right hand side a.s. satisfies (22), restated here,\n\n$$\n\\begin{aligned}\n& \\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(t, \\cdot, \\cdot)-\\mu_{\\#}(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{w}}} \\\\\n& =\\sup _{\\|\\phi\\|_{\\Phi_{\\tilde{w}} \\leq 1, \\phi \\in L_{b}^{\\infty} C_{l, s}^{1}}} \\int_{[0,1] \\times \\mathbb{R}} \\phi(\\xi, x)\\left(\\boldsymbol{\\mu}_{\\#}^{N}(t, \\xi, \\mathrm{~d} x)-\\mu_{\\#}(t, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi \\\\\n& =\\sup _{\\|\\phi\\|_{\\Phi_{\\tilde{w}} \\leq 1, \\phi \\in L_{b}^{\\infty} C_{s}^{1}, \\varphi \\text { solves (23) }}} \\int_{[0,1] \\times \\mathbb{R}} \\varphi(0, \\xi, x)\\left(\\boldsymbol{\\mu}_{\\#}^{N}(0, \\xi, \\mathrm{~d} x)-\\mu_{\\#}(0, \\xi, \\mathrm{~d} x)\\right) \\mathrm{d} \\xi \\\\\n& \\quad+\\boldsymbol{J}_{\\boldsymbol{D}}(\\varphi)+\\boldsymbol{J}_{\\boldsymbol{H}}(\\varphi)+\\boldsymbol{J}_{\\boldsymbol{F}}(\\varphi)\n\\end{aligned}\n$$\n\nBy Lemma 6, we have that, a.s.,\n\n$$\n\\begin{aligned}\n\\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(t, \\cdot, \\cdot)-\\mu_{\\#}(t, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{w}}^{-1}} \\leq & \\kappa(t)\\left\\|\\boldsymbol{\\mu}_{\\#}^{N}(0, \\cdot, \\cdot)-\\mu_{\\#}(0, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{w}}} \\\\\n& +\\kappa_{5}(t) \\int_{0}^{t}\\left\\|\\boldsymbol{H}^{N}(s, \\cdot)-H(s, \\cdot)\\right\\|_{L_{b}^{1}} \\mathrm{~d} s+L\\left(\\boldsymbol{F}^{N} ; \\chi\\right)\n\\end{aligned}\n$$\n\nwhere the choice of $\\chi$ depends on $\\Delta t, \\Delta \\xi, \\Delta x$ and\n\n$$\n\\begin{aligned}\nL\\left(\\boldsymbol{F}^{N} ; \\chi\\right):= & \\max (\\kappa(t+\\Delta t), \\kappa_{4}(t+\\Delta t)) \\sqrt{2(\\Delta t+\\Delta x+\\epsilon_{w}(\\Delta \\xi))} \\\\\n\\times\\{ & \\int_{[0,1]}\\left(\\int_{0}^{t} \\int_{\\mathbb{R}} \\boldsymbol{F}^{N}(\\mathrm{~d} s, \\xi, \\mathrm{~d} x))^{2} \\mathrm{~d} \\xi\\right)^{\\frac{1}{2}}+\\kappa(t+\\Delta t)\\left(\\left\\|\\eta^{-1}\\right\\|_{L^{1}}(t+\\Delta t)\\right)^{\\frac{1}{2}} \\\\\n& \\quad \\times\\left\\{\\int_{-\\infty}^{+\\infty} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\left[\\chi^{1} \\star\\left(\\boldsymbol{F}^{N} \\mathbb{1}_{[0, t] \\times[0,1] \\times \\mathbb{R}}\\right)\\right](s, \\xi, x)\\right)^{2} \\eta(x) \\mathrm{d} \\xi \\mathrm{~d} x \\mathrm{~d} s\\right\\}^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\n(see Sec. 3.6).\nThe expectation of the norm of $\\boldsymbol{H}^{N}(t, \\cdot)-H(t, \\cdot)$ is controlled by Lemmas 3 and 4:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|\\boldsymbol{H}^{N}(t, \\cdot)-H(t, \\cdot)\\right\\|_{L_{b}^{1}}\\right] \\\\\n& \\leq \\kappa_{1} \\int_{0}^{t} \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot)-\\mu(s, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\tilde{w}}^{-1}}\\right] \\mathrm{d} s+\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\|f\\|_{L^{\\infty}} t \\\\\n& \\quad+\\frac{1}{\\sqrt{N}}\\left(\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}\\|f\\|_{L^{\\infty}} t\\right)^{1 / 2}\n\\end{aligned}\n$$\n\nLastly, choosing $\\Delta t=\\Delta \\xi=\\Delta x=N^{-\\frac{1}{4}}$ and applying Lemma 7, we have that\n\n$$\n\\mathbb{E}\\left[L\\left(\\boldsymbol{F}^{N} ; \\chi\\right)\\right] \\leq \\kappa_{6}(t) \\sqrt{\\max \\left(\\epsilon\\left(N^{-\\frac{1}{2}}\\right), N^{-\\frac{1}{4}}\\right)}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\kappa_{6}(t):= & \\left(16\\|f\\|_{L^{\\infty}}^{2} t^{2}+2\\|f\\|_{L^{\\infty}} t\\right)^{\\frac{1}{2}} \\max (\\kappa(t+1), \\kappa_{4}(t+1)) \\sqrt{6} \\\\\n& +\\kappa(t+1)\\left(\\left\\|\\eta^{-1}\\right\\|_{L^{1}}(t+1)\\right)^{\\frac{1}{2}}\\left(\\frac{\\|f\\|_{L^{\\infty} t}}{2} M_{\\eta}(t)\\right)^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nCombining all estimates, we obtain that\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(t, \\cdot, \\cdot)-\\mu(t, \\cdot, \\cdot)\\|_{\\Phi_{\\infty}^{-1}}\\right]\\right. \\\\\n& \\leq \\kappa(t) \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(0, \\cdot, \\cdot)-\\mu(0, \\cdot, \\cdot)\\|_{\\Phi_{\\infty}^{-1}}\\right]+\\kappa_{6}(t) \\sqrt{\\max \\left(\\epsilon\\left(N^{-\\frac{1}{4}}\\right), N^{-\\frac{1}{4}}\\right)}\\right. \\\\\n& \\quad+\\kappa_{5}(t) \\int_{0}^{t}\\left(\\frac{1}{\\sqrt{N}}\\left(\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}\\|f\\|_{L^{\\infty}} \\tau\\right)^{1 / 2}+\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\|f\\|_{L^{\\infty}} \\tau\\right. \\\\\n& \\left.\\quad+\\kappa_{1} \\int_{0}^{\\tau} \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot), \\mu(s, \\cdot, \\cdot)\\|_{\\Phi_{\\infty}^{-1}}\\right] \\mathrm{d} s\\right) \\mathrm{d} \\tau\\right. \\\\\n& \\frac{1}{\\sqrt{N}}\\left(\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}\\|f\\|_{L^{\\infty}} t\\right)^{1 / 2}+\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\|f\\|_{L^{\\infty}} t \\\\\n& \\left.+\\kappa_{1} \\int_{0}^{t} \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot), \\mu(s, \\cdot, \\cdot)\\|_{\\Phi_{\\infty}^{-1}}\\right] \\mathrm{d} s\\right.\\right.\n\\end{aligned}\n$$\n\nBy another Gronwall estimate, we conclude that\n\n$$\n\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(t, \\cdot, \\cdot)-\\mu(t, \\cdot, \\cdot)\\|_{\\Phi_{\\infty}^{-1}}\\right] \\rightarrow 0, \\quad \\forall t \\geq 0\\right.\n$$\n\nprovided that\n\n$$\nN \\rightarrow \\infty, \\quad\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}} \\rightarrow 0, \\quad \\text { and } \\quad \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(0, \\cdot, \\cdot)-\\mu(0, \\cdot, \\cdot)\\|_{\\Phi_{\\infty}^{-1}}\\right] \\rightarrow 0\\right.\n$$\n\nIt only remains to prove Corollary 1 on the trajectories.\nProof of Corollary 1. Let us first restate the definitions of $H$ and $\\boldsymbol{H}^{N}$, and introduce further notations simplifying our calculations. Let $\\left\\{E^{i, N}\\right\\}_{i=1}^{N}$ be an almost everywhere partition. The integrated postsynaptic input $\\boldsymbol{H}^{i, N}$ and the extension $\\boldsymbol{H}^{N}$ to $\\xi \\in[0,1]$ are also defined as\n\n$$\n\\begin{aligned}\n\\boldsymbol{H}^{N}(t, \\xi) & :=\\sum_{i=1}^{N} \\boldsymbol{H}^{i, N}(t) \\mathbb{1}_{E^{i, N}}(\\xi) \\\\\n\\boldsymbol{H}^{i, N}(t) & :=\\frac{1}{N} \\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} w_{i, j}^{N} \\mathbb{1}_{\\{z \\leq f\\left(\\boldsymbol{X}^{j, N}(s-)\\right)\\}} \\boldsymbol{\\Pi}^{j, N}(\\mathrm{~d} s, \\mathrm{~d} z)\n\\end{aligned}\n$$\n\nThe limit input $H$ and its formal restriction $H^{i, N}$ to $i \\in\\{1, \\ldots, N\\}$ can be defined as\n\n$$\n\\begin{aligned}\n& H(t, \\xi):=\\int_{0}^{t} \\int_{[0,1]} w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\mu(t, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} s \\\\\n& H^{i, N}(t):=\\int_{[0,1]} H(t, \\xi) N \\mathbb{1}_{E^{i, N}}(\\xi) \\mathrm{d} \\xi\n\\end{aligned}\n$$\n\nWith the above notations, the equation for $\\boldsymbol{X}^{i ; N}$ and $\\widetilde{\\boldsymbol{X}}^{i ; N}$ can be rewritten respectively as\n\n$$\n\\begin{aligned}\n\\boldsymbol{X}^{i ; N}(t)= & \\boldsymbol{X}_{0}^{i ; N}+\\int_{0}^{t} b\\left(\\boldsymbol{X}^{i ; N}(s)\\right) \\mathrm{d} s+\\boldsymbol{H}^{i ; N}(t) \\\\\n& -\\int_{[0, t] \\times \\mathbb{R}_{+}} \\boldsymbol{X}^{i ; N}(s-) \\mathbb{1}_{\\{z \\leq f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right)\\}} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z), \\quad \\text { a.s. } \\forall t \\geq 0 \\\\\n\\widetilde{\\boldsymbol{X}}^{i ; N}(t)= & \\boldsymbol{X}_{0}^{i ; N}+\\int_{0}^{t} b\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right) \\mathrm{d} s+H^{i ; N}(t) \\\\\n& -\\int_{[0, t] \\times \\mathbb{R}_{+}} \\widetilde{\\boldsymbol{X}}^{i ; N}(s-) \\mathbb{1}_{\\{z \\leq f\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\}} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z), \\quad \\text { a.s. } \\forall t \\geq 0\n\\end{aligned}\n$$\n\nSubtracting the two equations, we have that, a.s., $\\forall t \\geq 0$,\n\n$$\n\\begin{aligned}\n& \\boldsymbol{X}^{i ; N}(t)-\\widetilde{\\boldsymbol{X}}^{i ; N}(t) \\\\\n& =\\int_{0}^{t}\\left(b\\left(\\boldsymbol{X}^{i ; N}(s)\\right)-b\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right)\\right) \\mathrm{d} s+\\boldsymbol{H}^{i ; N}(t)-H^{i ; N}(t) \\\\\n& \\quad-\\int_{[0, t] \\times \\mathbb{R}_{+}}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right) \\\\\n& \\quad \\mathbb{1}\\left\\{z \\leq \\min \\left(f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right), f\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right)\\right\\} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z) \\\\\n& \\quad-\\int_{[0, t] \\times \\mathbb{R}_{+}} \\boldsymbol{X}^{i ; N}(s-) \\mathbb{1}\\left\\{f\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)<z \\leq f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right)\\right\\} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z) \\\\\n& \\quad-\\int_{[0, t] \\times \\mathbb{R}_{+}}-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-) \\mathbb{1}\\left\\{f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right)<z \\leq f\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right\\} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)\n\\end{aligned}\n$$\n\nIn the following, we will use an approximation of the absolute value function that is better suited for dealing with stochastic jumps. Its definition and properties are presented in the following lemma.\n\nLemma 8. For any $\\epsilon>0$, define the regularized absolute value function\n\n$$\n|x|_{\\epsilon}:=\\left(x^{2}+\\epsilon^{2}\\right)^{1 / 2}\n$$\n\nIt satisfies\n\n$$\n|x| \\leq|x|_{\\epsilon} \\leq|x|+\\epsilon, \\quad|x|_{\\epsilon}^{\\prime}=\\frac{x}{\\left(x^{2}+\\epsilon^{2}\\right)^{1 / 2}}, \\quad|x|_{\\epsilon}^{\\prime \\prime}=\\frac{\\epsilon^{2}}{\\left(x^{2}+\\epsilon^{2}\\right)^{3 / 2}}\n$$\n\nIn addition, define the regularized saturating absolute value function\n\n$$\n\\lambda_{\\epsilon}(x):=\\frac{\\left(|x|_{\\epsilon}+1\\right)-\\sqrt{\\left(|x|_{\\epsilon}-1\\right)^{2}+\\epsilon^{2}}}{2}+\\epsilon\n$$\n\nIt satisfies\n\n$$\n\\begin{aligned}\n\\min (|x|, 1) & \\leq \\lambda_{\\epsilon}(x) \\leq \\min (|x|, 1)+3 \\epsilon \\\\\n\\lambda_{\\epsilon}^{\\prime}(x) & =\\frac{1}{2}\\left(1-\\frac{|x|_{\\epsilon}-1}{\\sqrt{\\left(|x|_{\\epsilon}-1\\right)^{2}+\\epsilon^{2}}}\\right)|x|_{\\epsilon}^{\\prime} \\leq|x|_{\\epsilon}^{\\prime} \\leq 1 \\\\\n\\lambda_{\\epsilon}^{\\prime \\prime}(x) & =\\frac{1}{2}\\left(1-\\frac{|x|_{\\epsilon}-1}{\\sqrt{\\left(|x|_{\\epsilon}-1\\right)^{2}+\\epsilon^{2}}}\\right)|x|_{\\epsilon}^{\\prime \\prime}-\\frac{1}{2} \\frac{\\epsilon^{2}}{\\left(\\left(|x|_{\\epsilon}-1\\right)^{2}+\\epsilon^{2}\\right)^{3 / 2}}\\left(|x|_{\\epsilon}^{\\prime}\\right)^{2} \\\\\n& \\leq \\frac{1}{2}\\left(1-\\frac{|x|_{\\epsilon}-1}{\\sqrt{\\left(|x|_{\\epsilon}-1\\right)^{2}+\\epsilon^{2}}}\\right)|x|_{\\epsilon}^{\\prime \\prime} \\leq|x|_{\\epsilon}^{\\prime \\prime}\n\\end{aligned}\n$$\n\nMoreover, the finite difference is bounded from above by\n\n$$\n\\begin{aligned}\n\\lambda_{\\epsilon}(x+u) & =\\lambda_{\\epsilon}(x)+\\lambda_{\\epsilon}^{\\prime}(x) u+\\int_{0}^{u} \\lambda_{\\epsilon}^{\\prime \\prime}(x+r) r \\mathrm{~d} r \\\\\n& \\leq \\lambda_{\\epsilon}(x)+\\lambda_{\\epsilon}^{\\prime}(x) u+\\frac{u^{2}}{2 \\epsilon}\n\\end{aligned}\n$$\n\nWe now come back to our proof but, instead of trying to directly bound $\\left|\\boldsymbol{X}^{i ; N}(t)-\\right.$ $\\widetilde{\\boldsymbol{X}}^{i ; N}(t) \\mid$, we work with $\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(t)-\\widetilde{\\boldsymbol{X}}^{i ; N}(t)\\right)$. Applying It\u00f4's lemma, we have that\n\n$$\n\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(t)-\\widetilde{\\boldsymbol{X}}^{i ; N}(t)\\right)=\\lambda_{\\epsilon}(0)+\\boldsymbol{B}^{i ; N}(t)+\\boldsymbol{I}^{i ; N}(t)+\\boldsymbol{R}_{0}^{i ; N}(t)+\\boldsymbol{R}_{1}^{i ; N}(t)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& \\boldsymbol{B}^{i ; N}(t)=\\int_{0}^{t}\\left(b\\left(\\boldsymbol{X}^{i ; N}(s)\\right)-b\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right)\\right) \\lambda_{\\epsilon}^{\\prime}\\left(\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right) \\mathrm{d} s \\\\\n& \\boldsymbol{I}^{i ; N}(t)=\\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right)\\right\\}}\\left[\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)+w_{i, j}^{N} / N\\right)\\right. \\\\\n& \\left.-\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right] \\boldsymbol{\\Pi}^{j ; N}(\\mathrm{~d} s, \\mathrm{~d} z) \\\\\n& +\\int_{0}^{t}-\\left(\\int_{[0,1]} \\int_{[0,1]} N \\mathbb{1}_{E^{i ; N}}(\\xi) w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(y) \\mu(s, \\zeta, y) \\mathrm{d} y \\mathrm{~d} \\zeta \\mathrm{~d} \\xi\\right) \\\\\n& \\lambda_{\\epsilon}^{\\prime}\\left(\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right) \\mathrm{d} s\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\boldsymbol{R}_{0}^{i ; N}(t)=\\int_{[0, t] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{z \\leq \\min \\left(f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right), f\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right)\\right\\}} \\\\\n& \\left[\\lambda_{\\epsilon}(0)-\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right] \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z) \\\\\n& \\boldsymbol{R}_{1}^{i ; N}(t)=\\int_{[0, t] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{f\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)<z \\leq f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right)\\right\\}} \\\\\n& \\left[\\lambda_{\\epsilon}\\left(-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)-\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right] \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z) \\\\\n& +\\int_{[0, t] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\{ f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right)<z \\leq f\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\}} \\\\\n& \\left[\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)\\right)-\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right] \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)\n\\end{aligned}\n$$\n\nThe estimates for $\\boldsymbol{B}^{i ; N}(t), \\boldsymbol{R}_{0}^{i ; N}(t)$, and $\\boldsymbol{R}_{1}^{i ; N}(t)$ are easy to derive and are given by\n\n$$\n\\begin{aligned}\n& \\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{B}^{i ; N}(t) \\leq\\|b\\|_{W^{1, \\infty}} \\int_{0}^{t_{*}} \\min \\left(\\left|\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right|, 1\\right) \\mathrm{d} s \\\\\n& \\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{R}_{0}^{i ; N}(t) \\leq 0 \\\\\n& \\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{R}_{1}^{i ; N}(t) \\leq(1+3 \\epsilon) \\int_{\\left[0, t_{*}\\right] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{f\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)<z \\leq f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right)\\right\\}} \\\\\n& +\\mathbb{1}_{\\left\\{f\\left(\\boldsymbol{X}^{i ; N}(s-)\\right)<z \\leq f\\left(\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right\\}} \\boldsymbol{\\Pi}^{i ; N}(\\mathrm{~d} s, \\mathrm{~d} z)\n\\end{aligned}\n$$\n\nTaking expectations, we have (for sufficient small $\\epsilon>0$ )\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{B}^{i ; N}(t)\\right] \\leq\\|b\\|_{W^{1, \\infty}} \\int_{0}^{t_{*}} \\mathbb{E}\\left[\\min \\left(\\left|\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right|, 1\\right)\\right] \\mathrm{d} s \\\\\n& \\mathbb{E}\\left[\\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{R}_{0}^{i ; N}(t)\\right] \\leq 0 \\\\\n& \\mathbb{E}\\left[\\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{R}_{1}^{i ; N}(t)\\right] \\leq 2\\|f\\|_{W^{1, \\infty}} \\int_{0}^{t_{*}} \\mathbb{E}\\left[\\min \\left(\\left|\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right|, 1\\right)\\right] \\mathrm{d} s\n\\end{aligned}\n$$\n\nThe term $\\boldsymbol{I}^{i ; N}(t)$ quantifies the influence of $\\boldsymbol{H}^{i ; N}(t)-H^{i ; N}(t)$ on the dynamics. Following the strategy in Section 3.3, we consider the martingale\n\n$$\n\\begin{aligned}\n& \\boldsymbol{I}_{M}^{i ; N}(t)=\\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right)\\right\\}}\\left[\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)+w_{i, j}^{N} / N\\right)\\right. \\\\\n& \\left.\\quad-\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right]\\left[\\boldsymbol{\\Pi}^{j ; N}(\\mathrm{~d} s, \\mathrm{~d} z)-\\mathrm{d} s \\mathrm{~d} z\\right]\n\\end{aligned}\n$$\n\ntogether with the difference due to the finite jump sizes\n\n$$\n\\begin{aligned}\n& \\boldsymbol{I}_{1}^{i ; N}(t)=\\sum_{j=1}^{N} \\int_{[0, t] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right)\\right\\}}\\left[\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)+w_{i, j}^{N} / N\\right)\\right. \\\\\n& \\left.-\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right] \\mathrm{d} s \\mathrm{~d} z \\\\\n& -\\int_{0}^{t}\\left(\\int_{[0,1]} \\int_{[0,1]} N \\mathbb{1}_{E^{i ; N}}(\\xi) w^{N}(\\xi, \\zeta) \\int_{\\mathbb{R}} f(y) \\boldsymbol{\\mu}^{N}(s, \\zeta, \\mathrm{~d} y) \\mathrm{d} \\zeta \\mathrm{~d} \\xi\\right) \\\\\n& \\lambda_{\\epsilon}^{\\prime}\\left(\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right) \\mathrm{d} s\n\\end{aligned}\n$$\n\nand the error created by advection\n\n$$\n\\begin{aligned}\n& \\boldsymbol{I}_{2}^{i ; N}(t)=\\int_{0}^{t}\\left(\\int_{[0,1]} \\int_{[0,1]} N \\mathbb{1}_{E^{i ; N}}(\\xi) w^{N}(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\boldsymbol{\\mu}^{N}(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} \\xi\\right) \\\\\n& \\lambda_{\\epsilon}^{\\prime}\\left(\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right) \\mathrm{d} s \\\\\n& -\\int_{0}^{t}\\left(\\int_{[0,1]} \\int_{[0,1]} N \\mathbb{1}_{E^{i ; N}}(\\xi) w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\mu(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} \\xi\\right) \\\\\n& \\lambda_{\\epsilon}^{\\prime}\\left(\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right) \\mathrm{d} s\n\\end{aligned}\n$$\n\nBy Doob's martingale inequality and It\u00f4 isometry,\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\sup _{t \\in\\left[0, t_{*}\\right]}\\left|\\boldsymbol{I}_{M}^{i ; N}(t)\\right|^{2}\\right] \\\\\n& \\quad \\leq 4 \\mathbb{E}\\left[\\left|\\boldsymbol{I}_{M}^{i ; N}\\left(t_{*}\\right)\\right|^{2}\\right] \\\\\n& \\leq \\mathbb{E}\\left[\\sum_{j=1}^{N} \\int_{\\left[0, t_{*}\\right] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{z \\leq f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right)\\right\\}}\\left[\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)+w_{i, j}^{N} / N\\right)\\right.\\right. \\\\\n& \\left.\\left.-\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right]^{2} \\mathrm{~d} s \\mathrm{~d} z\\right] \\\\\n& \\leq \\sum_{j=1}^{N} \\int_{\\left[0, t_{*}\\right] \\times \\mathbb{R}_{+}} \\mathbb{1}_{\\left\\{z \\leq\\|f\\|_{L^{\\infty}}\\right\\}} \\frac{\\left(\\max _{1 \\leq i, j \\leq N}\\left|w_{i, j}^{N}\\right|\\right)^{2}}{N^{2}} \\mathrm{~d} s \\mathrm{~d} z \\\\\n& \\quad \\leq \\frac{\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}}{N}\\|f\\|_{L^{\\infty}} t_{*} \\text {. }\n\\end{aligned}\n$$\n\nRewriting the extended variable $\\xi \\in[0,1]$ as the index $i, \\ldots, N$, we have\n\n$$\n\\begin{aligned}\n& \\boldsymbol{I}_{1}^{i ; N}(t)=\\sum_{j=1}^{N} \\int_{0}^{t} f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right)\\left[\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)+w_{i, j}^{N} / N\\right)\\right. \\\\\n& \\left.-\\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(s-)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s-)\\right)\\right] \\mathrm{d} s \\\\\n& -\\sum_{j=1}^{N} \\int_{0}^{t} f\\left(\\boldsymbol{X}^{j ; N}(s-)\\right) \\lambda_{\\epsilon}^{\\prime}\\left(\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right) w_{i, j}^{N} / N \\mathrm{~d} s\n\\end{aligned}\n$$\n\nThe supremum over $t \\in\\left[0, t_{*}\\right]$ is hence bounded by\n\n$$\n\\begin{aligned}\n\\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{I}_{1}^{i ; N}(t) & \\leq \\sum_{j=1}^{N} \\int_{0}^{t_{*}}\\|f\\|_{L^{\\infty}} \\frac{\\left(\\max _{1 \\leq i, j \\leq N}\\left|w_{i, j}^{N}\\right|\\right)^{2}}{2 \\epsilon N^{2}} \\mathrm{~d} s \\\\\n& \\leq \\frac{\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}}{2 \\epsilon N}\\|f\\|_{L^{\\infty}} t_{*}\n\\end{aligned}\n$$\n\nwhere, in the first inequality, we used Lemma 8.\nTaking the maximum over $t \\in\\left[0, t_{*}\\right]$ of $\\boldsymbol{I}_{2}^{i ; N}$,\n\n$$\n\\begin{aligned}\n\\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{I}_{2}^{i ; N}(t)=\\int_{0}^{t_{*}} & \\mid \\int_{[0,1]} \\int_{[0,1]} N \\mathbb{1}_{E^{i ; N}}(\\xi) w^{N}(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\boldsymbol{\\mu}^{N}(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} \\xi \\\\\n& \\left.-\\int_{[0,1]} \\int_{[0,1]} N \\mathbb{1}_{E^{i ; N}}(\\xi) w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\mu(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\mathrm{~d} \\xi \\right\\rvert\\, \\mathrm{d} s\n\\end{aligned}\n$$\n\nNow, by averaging over the indices $i$, we find that\n\n$$\n\\begin{aligned}\n\\frac{1}{N} \\sum_{i=1}^{N} \\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{I}_{2}^{i ; N}(t)=\\int_{0}^{t_{*}} \\int_{[0,1]} & \\int_{[0,1]} w^{N}(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\boldsymbol{\\mu}^{N}(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\\\\n& \\left.-\\int_{[0,1]} w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\mu(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta \\right\\rvert\\, \\mathrm{d} \\xi \\mathrm{~d} s\n\\end{aligned}\n$$\n\nFollowing the argument in the proof of Lemma 3, we deduce that\n\n$$\n\\begin{aligned}\n& \\int_{[0,1]}\\left|\\int_{[0,1]} w^{N}(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\boldsymbol{\\mu}^{N}(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta-\\int_{[0,1]} w(\\xi, \\zeta) \\int_{\\mathbb{R}} f(x) \\mu(s, \\zeta, \\mathrm{~d} x) \\mathrm{d} \\zeta\\right| \\mathrm{d} \\xi \\\\\n& \\quad \\leq \\kappa_{1}\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot)-\\mu(s, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\mathbb{Z}}^{+}}+\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\|f\\|_{L^{\\infty}}\n\\end{aligned}\n$$\n\nwhere $\\kappa_{1}:=\\left(\\max \\left(\\|f\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{\\infty}},\\left\\|\\partial_{x} f\\right\\|_{L^{1}}\\right)\\|w\\|_{L^{\\infty} L^{1}}+\\|f\\|_{L^{\\infty}}\\right)$.\nThus, we have the bound\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\sup _{t \\in\\left[0, t_{*}\\right]} \\boldsymbol{I}_{2}^{i ; N}(t)\\right] \\\\\n& \\quad \\leq \\kappa_{1} \\int_{0}^{t_{*}} \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot)-\\mu(s, \\cdot, \\cdot)\\right\\|_{\\Phi_{\\mathbb{Z}}^{+}}\\right] \\mathrm{d} s+\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\|f\\|_{L^{\\infty}} t_{*}\n\\end{aligned}\n$$\n\nFinally,\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\sup _{t \\in\\left[0, t_{*}\\right]} \\min \\left(\\left|\\boldsymbol{X}^{i ; N}(t)-\\widetilde{\\boldsymbol{X}}^{i ; N}(t)\\right|, 1\\right)\\right] \\\\\n& \\quad \\leq \\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\sup _{t \\in\\left[0, t_{*}\\right]} \\lambda_{\\epsilon}\\left(\\boldsymbol{X}^{i ; N}(t)-\\widetilde{\\boldsymbol{X}}^{i ; N}(t)\\right)\\right] \\\\\n& \\quad=\\lambda_{\\epsilon}(0)+\\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\sup _{t \\in\\left[0, t_{*}\\right]}\\left(\\boldsymbol{B}^{i ; N}(t)+\\boldsymbol{I}^{i ; N}(t)+\\boldsymbol{R}_{0}^{i ; N}(t)+\\boldsymbol{R}_{1}^{i ; N}(t)\\right)\\right] \\\\\n& \\quad \\leq 3 \\epsilon+\\left(\\|b\\|_{W^{1, \\infty}}+\\|f\\|_{W^{1, \\infty}}\\right) \\int_{0}^{t_{*}} \\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\min \\left(\\left|\\boldsymbol{X}^{i ; N}(s)-\\widetilde{\\boldsymbol{X}}^{i ; N}(s)\\right|, 1\\right)\\right] \\mathrm{d} s \\\\\n& \\quad+\\left(\\frac{\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}}{N}\\|f\\|_{L^{\\infty}} t_{*}\\right)^{\\frac{1}{2}}+\\frac{\\left\\|w^{N}\\right\\|_{L^{\\infty}}^{2}}{2 \\epsilon N}\\|f\\|_{L^{\\infty}} t_{*} \\\\\n& \\quad+\\kappa_{1} \\int_{0}^{t_{*}} \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot)-\\mu(s, \\cdot, \\cdot)\\right\\|_{\\boldsymbol{\\phi}_{S}^{-1}}\\right] \\mathrm{d} s+\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}}\\|f\\|_{L^{\\infty}} t_{*} .\n\\end{aligned}\n$$\n\nWe know that $\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}^{N}(s, \\cdot, \\cdot)-\\mu(s, \\cdot, \\cdot)\\right\\|_{\\boldsymbol{\\phi}_{S}^{-1}}\\right] \\rightarrow 0$ and $\\left\\|w^{N}-w\\right\\|_{L^{\\infty} \\rightarrow L^{1}} \\rightarrow 0$ as $N \\rightarrow 0$. Taking the scaling $\\epsilon \\sim N^{-1 / 2}$ and applying Gronwall's lemma, we have that, for all $t_{*} \\geq 0$,\n\n$$\n\\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\sup _{t \\in\\left[0, t_{*}\\right]} \\min \\left(\\left|\\boldsymbol{X}^{i ; N}(t)-\\widetilde{\\boldsymbol{X}}^{i ; N}(t)\\right|, 1\\right)\\right] \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# APPENDIX A \n\nA.1. Proof of Proposition 1 (Propagation of moments). By choosing $\\eta(x)=x^{2}$ and applying It\u00f4's lemma, we have\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{t}^{i}\\right|^{2}\\right]=\\mathbb{E}\\left[\\left|\\boldsymbol{X}_{0}^{i}\\right|^{2}+\\int_{0}^{t} 2 b\\left(\\boldsymbol{X}_{s^{-}}^{i}\\right) \\boldsymbol{X}_{s^{-}}^{i} \\mathrm{~d} s\\right. \\\\\n& +\\sum_{j \\neq i} \\int_{0}^{t} f\\left(\\boldsymbol{X}_{s^{-}}^{j}\\right)\\left(\\left|\\boldsymbol{X}_{s^{-}}^{i}+w_{i, j}^{N} / N\\right|^{2}-\\left|\\boldsymbol{X}_{s^{-}}^{i}\\right|^{2}\\right) \\mathrm{d} s \\\\\n& \\left.+\\int_{0}^{t} f\\left(\\boldsymbol{X}_{s^{-}}^{i}\\right)\\left(-\\left|\\boldsymbol{X}_{s^{-}}^{i}\\right|^{2}\\right) \\mathrm{d} s\\right]\n\\end{aligned}\n$$\n\nNotice that the last term is non-positive, and $|x+w|^{2}-|x|^{2} \\leq 2|w||x|+|w|^{2}$. Hence, we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left|\\boldsymbol{X}_{t}^{i}\\right|^{2}\\right] \\leq & \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{0}^{i}\\right|^{2}\\right]+\\int_{0}^{t} 2\\|b\\|_{L^{\\infty}} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{s^{-}}^{i}\\right|\\right] \\mathrm{d} s \\\\\n& +\\int_{0}^{t}\\|f\\|_{L^{\\infty}} \\sum_{j \\neq i}\\left(2\\left|w_{i, j}^{N} / N\\right| \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{s^{-}}^{i}\\right|\\right]+\\left|w_{i, j}^{N} / N\\right|^{2}\\right) \\mathrm{d} s \\\\\n\\leq & \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{0}^{i}\\right|^{2}\\right]+\\int_{0}^{t}\\left(\\|b\\|_{L^{\\infty}}^{2}+\\mathbb{E}\\left[\\left|\\boldsymbol{X}_{s^{-}}^{i}\\right|^{2}\\right]\\right) \\mathrm{d} s \\\\\n& +\\int_{0}^{t}\\|f\\|_{L^{\\infty}} \\sum_{j \\neq i}\\left(2\\left|w_{i, j}^{N} / N\\right| \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{s^{-}}^{i}\\right|\\right]+\\left|w_{i, j}^{N} / N\\right|^{2}\\right) \\mathrm{d} s\n\\end{aligned}\n$$\n\nTaking the average over all indices $i$, we obtain\n\n$$\n\\begin{aligned}\n& \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{t}^{i}\\right|^{2}\\right] \\\\\n& \\leq \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{0}^{i}\\right|^{2}\\right]+\\int_{0}^{t}\\left(\\|b\\|_{L^{\\infty}}^{2}+\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{s^{-}}^{i}\\right|^{2}\\right]\\right) \\mathrm{d} s \\\\\n& \\quad+\\int_{0}^{t}\\|f\\|_{L^{\\infty}}\\left(\\max _{1 \\leq i, j \\leq N}\\left|w_{i, j}^{N}\\right| \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{s^{-}}^{i}\\right|^{2}\\right]+\\left(\\max _{1 \\leq i, j \\leq N}\\left|w_{i, j}^{N}\\right|\\right)^{2} / N\\right) \\mathrm{d} s\n\\end{aligned}\n$$\n\nWe can conclude by Gronwall's lemma, which yields a constant $C(t)$ that only depends on $E_{0}=\\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{0}^{i}\\right|^{2}\\right],\\|b\\|_{L^{\\infty}},\\|f\\|_{L^{\\infty}}, \\max _{1 \\leq i, j \\leq N}\\left|w_{i, j}^{N}\\right|$, and $t$.\nA.2. Proof of Proposition 2 (Weak-* convergence). In this subsection, we prove Proposition 2, which establishes the equivalence between the weak-* convergence in $\\mathcal{M}([0,1] \\times \\mathbb{R})$ we use in Theorem 1 and the convergence in $\\Phi_{w}^{-1}$ defined in Sec. 2.3 and used in Theorem 2. Also, in order to treat the convergence of the initial data in Appendix A.3, we introduce another metric on $\\mathcal{P}([0,1] \\times \\mathbb{R})$ \u2014a negative Sobolev norm\u2014and show that the three topologies are all equivalent in our setting.\n\nTo define the negative Sobolev norm, let us introduce the kernel $\\Lambda: \\mathbb{R} \\rightarrow \\mathbb{R}_{+}$,\n\n$$\n\\Lambda(x):=\\frac{1}{2} \\exp (-|x|), \\quad \\forall x \\in \\mathbb{R}\n$$\n\nThen, the negative Sobolev norm, $\\|\\cdot\\|_{H^{-1}(\\mathbb{R})}$, is defined by convolution:\n\n$$\n\\|f\\|_{H^{-1}(\\mathbb{R})}^{2}:=\\int_{\\mathbb{R} \\times \\mathbb{R}} f(x) \\Lambda(x-y) f(y) \\mathrm{d} x \\mathrm{~d} y\n$$\n\nSince the Fourier transform of $\\Lambda$ is\n\n$$\n\\mathcal{F}(\\Lambda)(v)=\\int_{\\mathbb{R}} \\Lambda(x) e^{-2 \\pi i x v} \\mathrm{~d} x=\\frac{1}{1+4 \\pi^{2} v^{2}}, \\quad \\forall v \\in \\mathbb{R}\n$$\n\nwe see that our definition of $H^{-1}(\\mathbb{R})$ is equivalent to the usual definition of $H^{s}(\\mathbb{R})$ in the Fourier sense, i.e.,\n\n$$\n\\|f\\|_{H^{s}(\\mathbb{R})}^{2}:=\\int_{\\mathbb{R}}\\left(1+4 \\pi^{2} v^{2}\\right)^{s}|\\mathcal{F}(f)(v)|^{2} \\mathrm{~d} v\n$$\n\nNote that the convolution by $\\Lambda$ can also be seen as the inverse operator of $\\left(1-\\partial_{x}^{2}\\right)$, since, in the sense of distributions,\n\n$$\n\\left(1-\\partial_{x}^{2}\\right) \\Lambda=\\delta, \\quad\\left(1-\\partial_{x}^{2}\\right)(\\Lambda \\star f)=f, \\quad \\forall f \\in \\mathscr{D}^{\\prime}(\\mathbb{R})\n$$\n\nwhere $\\delta$ denotes the Dirac delta function and $\\mathscr{D}^{\\prime}(\\mathbb{R})$ denotes the distributions on $\\mathbb{R}$.\nAll of the above extend to Sobolev spaces on the circle, which we denote $H^{-1}([0,1])$ and $H^{1}([0,1])$. Replacing the kernel $\\Lambda$ by $\\tilde{\\Lambda}:[0,1] \\rightarrow \\mathbb{R}_{+}$,\n\n$$\n\\tilde{\\Lambda}(\\xi):=\\frac{1}{2} \\sum_{n=-\\infty}^{+\\infty} \\exp (-|\\xi+n|), \\quad \\forall \\xi \\in[0,1]\n$$\n\nwhich has a natural periodic extension on $\\mathbb{R}$, we define the negative Sobolev norm, $H^{-1}([0,1])$, by convolution:\n\n$$\n\\|f\\|_{H^{-1}([0,1])}^{2}:=\\int_{[0,1] \\times[0,1]} f(\\xi) \\tilde{\\Lambda}(\\xi-\\zeta) f(\\zeta) \\mathrm{d} \\xi \\mathrm{~d} \\zeta\n$$\n\nWe recall that, in this work, the interval $[0,1]$ is identified with the circle $\\mathbb{T}$. Hence, the Sobolev spaces on the circle, $H^{-1}([0,1])$ and $H^{1}([0,1])$, should not be confused with Sobolev spaces defined on bounded domains (despite our slight abuse of notation).\n\nFinally, we define the tensorized negative Sobolev structure, $H^{-1}([0,1]) \\otimes H^{-1}(\\mathbb{R})$, for functions on the product domain $[0,1] \\times \\mathbb{R}$, through the norm\n\n$$\n\\begin{aligned}\n\\|f\\|_{H^{-1}([0,1]) \\otimes H^{-1}(\\mathbb{R})}^{2} & :=\\int_{[0,1] \\times \\mathbb{R}} f[(\\tilde{\\Lambda} \\otimes \\Lambda) \\star f] \\mathrm{d} \\xi \\mathrm{~d} x \\\\\n& =\\int_{[0,1] \\times \\mathbb{R}} \\int_{[0,1] \\times \\mathbb{R}} f(\\xi, x) \\tilde{\\Lambda}(\\xi-\\zeta) \\Lambda(x-y) f(\\zeta, y) \\mathrm{d} \\xi \\mathrm{~d} x \\mathrm{~d} \\zeta \\mathrm{~d} y \\\\\n& =\\sum_{k=-\\infty}^{+\\infty} \\int_{\\mathbb{R}} \\frac{1}{\\left(1+4 \\pi^{2} k^{2}\\right)\\left(1+4 \\pi^{2} v^{2}\\right)}|\\mathcal{F}(f)(k, v)|^{2} \\mathrm{~d} v\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\mathcal{F}(f)(k, v):=\\int_{[0,1] \\times \\mathbb{R}} f(\\xi, x) e^{-2 \\pi i(\\xi k+x v)} \\mathrm{d} \\xi \\mathrm{~d} x\n$$\n\nIn Appendix A.3, we will also need the norm\n\n$$\n\\begin{aligned}\n\\|f\\|_{H^{1}([0,1]) \\otimes H^{1}(\\mathbb{R})}^{2} & :=\\int_{[0,1] \\times \\mathbb{R}}\\left[\\left(1-\\partial_{x}^{2}\\right)\\left(1-\\partial_{\\xi}^{2}\\right) f\\right] f \\mathrm{~d} \\xi \\mathrm{~d} x \\\\\n& =\\sum_{k=-\\infty}^{+\\infty} \\int_{\\mathbb{R}}\\left(1+4 \\pi^{2} k^{2}\\right)\\left(1+4 \\pi^{2} v^{2}\\right)|\\mathcal{F}(f)(k, v)|^{2} \\mathrm{~d} v\n\\end{aligned}\n$$\n\nThe norm $H^{-1}([0,1]) \\otimes H^{-1}(\\mathbb{R})$ will serve as the third notion of convergence in $\\mathcal{P}([0,1] \\times$ $\\mathbb{R})$. The following proposition, which encompasses Proposition 2, states that the three types of convergence in $\\mathcal{P}([0,1] \\times \\mathbb{R})$ considered in this work, namely, the weak-* convergence, the $\\Phi_{w}^{-1}$-convergence, and the convergence in the negative Sobolev norm $H^{-1}([0,1]) \\otimes H^{-1}(\\mathbb{R})$ are all equivalent.\n\nProposition 7. Let $\\mu^{N}, N \\rightarrow \\infty$, be a sequence of measures on $[0,1] \\times \\mathbb{R}$, and $\\mu$ be a measure on $[0,1] \\times \\mathbb{R}$. Assume that $\\mu^{N}(\\xi, \\cdot) \\in \\mathcal{P}(\\mathbb{R})$ for a.e. $\\xi \\in[0,1]$ and $\\mu(\\xi, \\cdot) \\in \\mathcal{P}(\\mathbb{R})$ for a.e. $\\xi \\in[0,1]$. Further, let $\\epsilon_{w}:(0,1) \\rightarrow(0, \\infty)$ be a non-decreasing function satisfying $\\lim _{r \\rightarrow 0+} \\epsilon_{w}(r)=0$. The following statements are equivalent:\n\n(i) For all $\\varphi \\in C_{c}([0,1] \\times \\mathbb{R})$,\n\n$$\n\\int_{[0,1] \\times \\mathbb{R}} \\varphi\\left(\\mu^{N}-\\mu\\right) \\mathrm{d} \\xi \\mathrm{~d} x \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$\n\n(ii) $\\left\\|\\mu^{N}-\\mu\\right\\|_{\\Phi_{w}^{-1}} \\rightarrow 0$ as $N \\rightarrow \\infty$, where the metric $\\Phi_{w}^{-1}$ is defined in (11a)-(11b).\n(iii) $\\left\\|\\mu^{N}-\\mu\\right\\|_{H^{-1}([0,1]) \\otimes H^{-1}(\\mathbb{R})} \\rightarrow 0$ as $N \\rightarrow \\infty$.\n\nThe proposition can be deduced from a general lemma:\nLemmA 9. Assume $\\mu^{N}, N \\rightarrow \\infty$, and $\\mu$ as in Proposition 7. Let $E$ be a Banach space such that $C_{c}^{\\infty}([0,1] \\times \\mathbb{R}) \\subset E \\subset L^{\\infty}([0,1] ; C(\\mathbb{R}))$ and such that $E$ is compactly embedded in $L^{1}([0,1] ; C(K))$ for any compact set $K$. Then, (i) is equivalent to the following:\n(i') As $N \\rightarrow \\infty$,\n\n$$\n\\left\\|\\mu^{N}-\\mu\\right\\|_{E^{-1}}:=\\sup \\left\\{\\int_{[0,1] \\times \\mathbb{R}} \\phi(\\xi, x)\\left(\\mu^{N}(\\xi, x)-\\mu(\\xi, x)\\right) \\mathrm{d} \\xi \\mathrm{~d} x \\mid\\|\\phi\\|_{E} \\leq 1\\right\\} \\rightarrow 0\n$$\n\nLemma 9 directly implies Proposition 7. Indeed, the equivalence between (i) and (iii) follows immediately from an extension of Morrey's inequality, which implies that $H^{1}([0,1]) \\otimes$ $H^{1}(\\mathbb{R})$ is embedded in $C^{\\frac{1}{2}}([0,1] \\times \\mathbb{R})$. The equivalence with (ii) is proved by a Fr\u00e9chetKolmogorov argument once one observes that a bounded set in $\\Phi_{w}$ is uniformly bounded, Lipschitz in $x$, and also equicontinuous in $\\xi$ by the definition of $\\epsilon_{w}$.\n\nProof of Lemma 9. (i') $\\Longrightarrow$ (i): The proof is immediate since for any $\\varphi \\in C_{c}([0,1] \\times$ $\\mathbb{R}) \\subset E$,\n\n$$\n\\left|\\int_{[0,1] \\times \\mathbb{R}} \\varphi\\left(\\mu^{N}-\\mu\\right) \\mathrm{d} \\xi \\mathrm{~d} x\\right| \\leq\\|\\varphi\\|_{E}\\left\\|\\mu^{N}-\\mu\\right\\|_{E^{-1}}, \\text { as } N \\rightarrow \\infty\n$$\n\n(i) $\\Longrightarrow$ (i'): Up to the extraction of a subsequence (still indexed by $N$ ), there exists $\\left\\{\\phi^{N}\\right\\}_{N=1}^{\\infty}$ such that $\\left\\|\\phi^{N}\\right\\|_{E} \\leq 1$ and\n\n$$\n\\lim _{N \\rightarrow \\infty} \\int_{[0,1] \\times \\mathbb{R}} \\phi^{N}\\left(\\mu^{N}-\\mu\\right) \\mathrm{d} \\xi \\mathrm{~d} x=\\limsup _{N \\rightarrow \\infty}\\left\\|\\mu^{N}-\\mu\\right\\|_{E^{-1}}\n$$\n\nSince $E$ is compactly embedded in $L^{1}([0,1] ; C(K))$ for any compact $K$, we can extract a further subsequence (still indexed by $N$ for readability) and there a $\\phi$ such that, for any $R>0$, $\\phi^{N} \\rightarrow \\phi$ in $L^{1}([0,1] ; C([-R, R]))$. Since $\\mu^{N}$ and $\\mu$ are probability measures for a.e. $\\xi$, we have that\n\n$$\n\\begin{aligned}\n& \\int_{[0,1] \\times \\mathbb{R}}\\left(\\phi^{N}-\\phi\\right)\\left(\\mu^{N}-\\mu\\right) \\mathrm{d} \\xi \\mathrm{~d} x \\\\\n& \\quad \\leq 2\\left\\|\\phi^{N}-\\phi\\right\\|_{L^{1}([0,1] ; C([-R, R]))}+\\left\\|\\phi^{N}-\\phi\\right\\|_{L^{\\infty}([0,1] ; C(\\mathbb{R}))} \\int_{[0,1] \\times(\\mathbb{R} \\backslash[-R, R])}\\left(\\mu^{N}+\\mu\\right)\n\\end{aligned}\n$$\n\nPassing to the limit $N \\rightarrow \\infty$, the first term vanishes by the convergence $\\phi^{N} \\rightarrow \\phi$. By Prokhorov's theorem, if (i) is true, then the sequence $\\left\\{\\mu^{N}\\right\\} \\subset \\mathcal{P}([0,1] \\times \\mathbb{R})$ is tight; hence, the second term vanishes by letting $R \\rightarrow \\infty$. Hence,\n\n$$\n\\begin{aligned}\n& \\limsup _{N \\rightarrow \\infty}\\left\\|\\mu^{N}-\\mu\\right\\|_{E^{-1}}=\\lim _{N \\rightarrow \\infty} \\int_{[0,1] \\times \\mathbb{R}} \\phi^{N}\\left(\\mu^{N}-\\mu\\right) \\mathrm{d} \\xi \\mathrm{~d} x \\\\\n& =\\lim _{N \\rightarrow \\infty} \\int_{[0,1] \\times \\mathbb{R}} \\phi\\left(\\mu^{N}-\\mu\\right) \\mathrm{d} \\xi \\mathrm{~d} x=0\n\\end{aligned}\n$$\n\nfrom (i). (As a remark, (i) itself only implies the last equality for $\\phi \\in C_{c}([0,1] \\times \\mathbb{R})$. A density argument involving the tightness of $\\mu^{N}$ is needed to prove (i) for general $\\phi \\in$ $L^{1}([0,1] ; C(\\mathbb{R}))$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "# A.3. Proof of Proposition 3 (Convergence of the initial data). \n\nProof of Proposition 3. To prove the first part of the proposition, notice that\n\n$$\n\\int_{[0,1] \\times \\mathbb{R}}\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N}(x) \\mathbb{1}_{E^{i ; N}}(\\xi)\\right) x^{2} \\mathrm{~d} \\xi \\mathrm{~d} x \\leq \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[\\left|\\boldsymbol{X}_{0}^{i ; N}\\right|^{2}\\right]\n$$\n\nHence, the sequence of measures is uniformly tight. By Prokhorov's theorem, up to the extraction of a subsequence, there exists $\\mu_{0} \\in L^{\\infty}([0,1] ; \\mathcal{M}(\\mathbb{R}))$ such that\n\n$$\n\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right) \\xrightarrow{\\sim} \\mu_{0}, \\text { as } N \\rightarrow \\infty\n$$\n\nIt is then straightforward to check that $\\mu_{0}(\\xi, \\cdot) \\in \\mathcal{P}(\\mathbb{R})$ for a.e. $\\xi \\in[0,1]$. By Proposition 7, the weak-* convergence is equivalent to the $\\Phi_{w}^{-1}$-convergence,\n\n$$\n\\left\\|\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right)-\\mu_{0}\\right\\|_{\\Phi_{w}^{-1}} \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$\n\nor the convergence in the negative Sobolev norm $H^{-1}([0,1]) \\otimes H^{-1}(\\mathbb{R})$,\n\n$$\n\\left\\|\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right)-\\mu_{0}\\right\\|_{H^{-1}([0,1]) \\otimes H^{-1}(\\mathbb{R})} \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$\n\nThis concludes the proof of the first part.\nNext, we prove the convergence of the extended empirical measure in the $H^{-1}([0,1]) \\otimes$ $H^{-1}(\\mathbb{R})$ norm (which we denote $H^{-1}$ to lighten the notation). Our strategy starts with applying the triangle inequality\n\n$$\n\\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\mu_{0}\\right\\|_{H^{-1}} \\leq\\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right)\\right\\|_{H^{-1}}+\\left\\|\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right)-\\mu_{0}\\right\\|_{H^{-1}}\n$$\n\nThe second term converges to 0 by our previous discussion. For the first term, we have that,\n\n$$\n\\begin{aligned}\n& \\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right)\\right\\|_{H^{-1}}^{2} \\\\\n& =\\sum_{i=1}^{N} \\sum_{j=1}^{N}\\left\\langle\\left(\\delta_{\\boldsymbol{X}_{0}^{i ; N}}-\\mu_{0}^{i ; N}\\right) \\mathbb{1}_{E^{i ; N}},\\left(\\delta_{\\boldsymbol{X}_{0}^{j ; N}}-\\mu_{0}^{j ; N}\\right) \\mathbb{1}_{E^{i ; N}}\\right\\rangle_{H^{-1}} \\\\\n& =\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\delta_{\\boldsymbol{X}_{0}^{j ; N}}-\\mu_{0}^{i ; N}\\right)(x) \\mathbb{1}_{E^{i ; N}}(\\xi) \\Lambda \\star\\left(\\delta_{\\boldsymbol{X}_{0}^{j ; N}}-\\mu_{0}^{j ; N}\\right)(x) \\tilde{\\Lambda} \\star \\mathbb{1}_{E^{i ; N}}(\\xi) \\mathrm{d} \\xi \\mathrm{~d} x\n\\end{aligned}\n$$\n\nSince the $\\left\\{\\boldsymbol{X}_{0}^{i ; N}\\right\\}_{i=1}^{N}$ are independent, the expectation can be computed:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right)\\right\\|_{H^{-1}}^{2}\\right] \\\\\n& =\\mathbb{E} \\sum_{i, j} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\delta_{\\boldsymbol{X}_{0}^{i ; N}}-\\mu_{0}^{i ; N}\\right)(x) \\mathbb{1}_{E^{i ; N}}(\\xi) \\Lambda \\star\\left(\\delta_{\\boldsymbol{X}_{0}^{i ; N}}-\\mu_{0}^{j ; N}\\right)(x) \\tilde{\\Lambda} \\star \\mathbb{1}_{E^{j ; N}}(\\xi) \\mathrm{d} \\xi \\mathrm{~d} x \\\\\n& =\\mathbb{E} \\sum_{i=1}^{N} \\int_{[0,1] \\times \\mathbb{R}}\\left(\\delta_{\\boldsymbol{X}_{0}^{i ; N}}-\\mu_{0}^{i ; N}\\right)(x) \\mathbb{1}_{E^{i ; N}}(\\xi) \\Lambda \\star\\left(\\delta_{\\boldsymbol{X}_{0}^{i ; N}}-\\mu_{0}^{i ; N}\\right)(x) \\tilde{\\Lambda} \\star \\mathbb{1}_{E^{i ; N}}(\\xi) \\mathrm{d} \\xi \\mathrm{~d} x\n\\end{aligned}\n$$\n\nwhere, for the second equality, we used the fact that the terms with $i \\neq j$ have expectation 0 . It is easy to verify that\n\n$$\n\\begin{aligned}\n& \\left\\|\\Lambda \\star\\left(\\delta_{\\boldsymbol{X}_{0}^{i ; N}}-\\mu_{0}^{i ; N}\\right)\\right\\|_{L^{\\infty}} \\leq\\|\\Lambda\\|_{L^{\\infty}}\\left\\|\\delta_{\\boldsymbol{X}_{0}^{i ; N}}-\\mu_{0}^{i ; N}\\right\\|_{\\mathrm{TV}} \\leq 1, \\quad \\text { a.s. } \\\\\n& \\left\\|\\tilde{\\Lambda} \\star \\mathbb{1}_{E^{i ; N}}\\right\\|_{L^{\\infty}} \\leq\\|\\tilde{\\Lambda}\\|_{L^{\\infty}}\\left\\|\\mathbb{1}_{E^{i ; N}}\\right\\|_{\\mathrm{TV}} \\leq \\frac{2}{N}\n\\end{aligned}\n$$\n\nwhere $\\|\\cdot\\|_{\\mathrm{TV}}$ denotes the total variation norm. Therefore,\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right)\\right\\|_{H^{-1}}^{2}\\right] \\\\\n& \\quad \\leq \\mathbb{E} \\sum_{i=1}^{N}\\left\\|\\delta_{\\boldsymbol{X}_{0}^{i ; N}}-\\mu_{0}^{i ; N}\\right\\|_{\\mathrm{TV}}\\left\\|\\mathbb{1}_{E^{i ; N}}\\right\\|_{\\mathrm{TV}}\\left\\|\\Lambda \\star\\left(\\delta_{\\boldsymbol{X}_{0}^{i ; N}}-\\mu_{0}^{i ; N}\\right)\\right\\|_{L^{\\infty}}\\left\\|\\tilde{\\Lambda} \\star \\mathbb{1}_{E^{i ; N}}\\right\\|_{L^{\\infty}} \\\\\n& \\quad=\\frac{4}{N}\n\\end{aligned}\n$$\n\nThus,\n\n$$\n\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\mu_{0}\\right\\|_{H^{-1}}\\right] \\leq \\frac{2}{\\sqrt{N}}+\\left\\|\\left(\\sum_{i=1}^{N} \\mu_{0}^{i ; N} \\mathbb{1}_{E^{i ; N}}\\right)-\\mu_{0}\\right\\|_{H^{-1}} \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$\n\nwhich implies that\n\n$$\n\\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\mu_{0}\\right\\|_{H^{-1}} \\rightarrow 0, \\text { as } N \\rightarrow \\infty, \\quad \\text { a.s. }\n$$\n\nBy Proposition 7, this convergence is equivalent to\n\n$$\n\\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\mu_{0}\\right\\|_{\\Phi_{w}^{-1}} \\rightarrow 0, \\text { as } N \\rightarrow \\infty, \\quad \\text { a.s. }\n$$\n\nSince the metric $\\Phi_{w}^{-1}$ is bounded a.s. by definition, we conclude that\n\n$$\n\\mathbb{E}\\left[\\left\\|\\boldsymbol{\\mu}_{0}^{N}-\\mu_{0}\\right\\|_{\\Phi_{w}^{-1}}\\right] \\rightarrow 0, \\text { as } N \\rightarrow \\infty\n$$\n\nAcknowledgments. P.-E. J. is also affiliated with the Huck Institutes of the Life Sciences at Pennsylvania State University. D. Z. is also affiliated with the Laboratoire Jacques-Louis Lions at Sorbonne Universit\u00e9.\n\nFunding. P.-E. J. and D. Z. were partially supported by NSF DMS Grants 2205694, 2219297. D. Z. was supported by European Union's Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No. 101034255. V. S. was supported by the Swiss National Science Foundation (no 200020_207426) and a Royal Society Newton International Fellowship (NIF\\R1\\231927).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# REFERENCES \n\n[1] Z. Agathe-Nerine, Multivariate Hawkes processes on inhomogeneous random graphs, Stochastic Processes and their Applications, 152 (2022), pp. 86-148.\n[2] S.-I. AMARI, Dynamics of pattern formation in lateral-inhibition type neural fields, Biological cybernetics, 27 (1977), pp. 77-87.\n[3] D. Avitabile and M. DaVydov, Poisson hypothesis and large-population limit for networks of spiking neurons, arXiv preprint arXiv:2502.03379, (2025).\n[4] \u00c1. BACKHAUSZ AND B. SzEGEDY, Action convergence of operators and graphs, Canadian Journal of Mathematics, 74 (2022), pp. 72-121.\n[5] R. Ben-Yishal, R. L. Bar-Or, and H. Sompolinsky, Theory of orientation tuning in visual cortex., Proceedings of the National Academy of Sciences, 92 (1995), pp. 3844-3848.\n[6] I. Benjamini and O. Schramm, Recurrence of Distributional Limits of Finite Planar Graphs, Electronic Journal of Probability, 6 (2001), pp. 1 - 13.\n[7] R. L. Beurle, Properties of a mass of cells capable of regenerating pulses, Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences, (1956), pp. 55-94.\n[8] C. Borgs, J. Chayes, H. Cohn, and Y. Zhao, An $L^{p}$ theory of sparse graph convergence I: Limits, sparse random graph models, and power law distributions, Transactions of the American Mathematical Society, 372 (2019), pp. 3019-3062.\n[9] C. Borgs, J. T. Chayes, H. Cohn, and Y. Zhao, An Lp theory of sparse graph convergence II: LD convergence, quotients and right convergence, Annals of Probability, 46 (2018), pp. 337-396.\n[10] W. Braun and K. Hepp, The Vlasov dynamics and its fluctuations in the $1 / N$ limit of interacting classical particles, Communications in Mathematical Physics, 56 (1977), pp. 101-113.\n[11] M. BREAKSPEAR, Dynamic models of large-scale brain activity, Nature Neuroscience, 20 (2017), pp. 340352.\n[12] P. C. Bressloff, Spatiotemporal dynamics of continuum neural fields, Journal of Physics A: Mathematical and Theoretical, 45 (2011), p. 033001.\n[13] N. Brunel, Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons, Journal of Computational Neuroscience, 8 (2000), pp. 183-208.\n[14] N. Brunel and V. Hakim, Fast global oscillations in networks of integrate-and-fire neurons with low firing rates, Neural Computation, 11 (1999), pp. 1621-1671.\n[15] M. J. C\u00c1ceres, J. A. Carrillo, and B. Perthame, Analysis of nonlinear noisy integrate \\& fire neuron models: blow-up and steady states, Journal of Mathematical Neuroscience, 1 (2011), pp. 1-33.\n[16] M. J. Caceres and B. Perthame, Beyond blow-up in excitatory integrate and fire neuronal networks: refractory period and spontaneous activity, Journal of Theoretical Biology, 350 (2014), pp. 81-89.\n[17] J. A. Ca\u00f1izo and H. Yolda\u015e, Asymptotic behaviour of neuron population models structured by elapsedtime, Nonlinearity, 32 (2019), p. 464.\n[18] J. A. Carrillo, M. D. M. Gonz\u00e1lez, M. P. Gualdani, and M. E. Schonbek, Classical solutions for a nonlinear Fokker-Planck equation arising in computational neuroscience, Communications in Partial Differential Equations, 38 (2013), pp. 385-409.\n[19] J. A. Carrillo, B. Perthame, D. Salort, and D. Smets, Qualitative properties of solutions for the noisy integrate and fire model in computational neuroscience, Nonlinearity, 28 (2015), p. 3365.\n[20] L.-P. Chaintron and A. Diez, Propagation of chaos: A review of models, methods and applications. I. models and methods., Kinetic and Related Models, 15 (2022).\n[21] , Propagation of chaos: A review of models, methods and applications. II. applications., Kinetic and Related Models, 15 (2022).\n[22] J. Chevallier, Mean-field limit of generalized Hawkes processes, Stochastic Processes and their Applications, 127 (2017), pp. 3870-3912.\n[23] J. Chevallier, A. Duarte, E. L\u00f6cherbach, and G. Ost, Mean field limits for nonlinear spatially extended Hawkes processes with exponential memory kernels, Stochastic Processes and their Applications, 129 (2019), pp. 1-27.\n[24] H. Chiba and G. S Medvedev, The mean field analysis of the Kuramoto model on graphs I. the mean field equation and transition point formulas, Discrete \\& Continuous Dynamical Systems-A, 39 (2019).\n[25] F. CopPini, H. Dietert, and G. Giacomin, A law of large numbers and large deviations for interacting diffusions on Erd\u00f6s-R\u00e9nyi graphs, Stochastics and Dynamics, 20 (2020), p. 2050010.\n[26] Q. CORMIER, A mean-field model of Integrate-and-Fire neurons: non-linear stability of the stationary solutions, Mathematical Neuroscience and Applications, 4 (2024).\n[27] Q. CORMIER, E. TAHRE, AND R. VELTZ, Long time behavior of a mean-field model of interacting neurons, Stochastic Processes and their Applications, 130 (2020), pp. 2553-2595.\n\n[28] Q. Cormier, E. Tanr\u00e9, and R. Veltz, Hopf bifurcation in a mean-field model of spiking neurons, Electronic Journal of Probability, 26 (2021), pp. 1-40.\n[29] J. Crevat, Mean-field limit of a spatially-extended FitzHugh-Nagumo neural network, Kinetic and Related Models, 12 (2019), p. 1329.\n[30] M. DAVYDOV, Replica-mean-field limit of continuous-time fragmentation-interaction-aggregation processes, Arxiv preprint arXiv:2305.03464, (2023).\n[31] A. De Masi, A. Galves, E. L\u00f6cherbach, and E. Presutti, Hydrodynamic limit for interacting neurons, Journal of Statistical Physics, 158 (2015), pp. 866-902.\n[32] F. Delarue, J. Inglis, S. Rubenthaler, and E. Tanr\u00e9, Global solvability of a networked integrate-and-fire model of McKean-Vlasov type, Annals of Applied Probability, 25 (2015), pp. 2096-2133.\n[33] , Particle systems with a singular mean-field self-excitation. application to neuronal networks, Stochastic Processes and their Applications, 125 (2015), pp. 2451-2492.\n[34] S. Delattre, N. Fournier, and M. Hoffmann, Hawkes processes on large networks, Annals of Applied Probability, 26 (2016), pp. 216-261.\n[35] R. L. Dobrushin, Vlasov equations, Functional Analysis and Its Applications, 13 (1979), pp. 115-123.\n[36] B. Ermentrout, Neural networks as spatio-temporal pattern-forming systems, Reports on Progress in Physics, 61 (1998), p. 353.\n[37] F. Flandoli, E. Priola, and G. Zanco, A mean-field model with discontinuous coefficients for neurons with spatial interaction, Discrete and Continuous Dynamical Systems, 39 (2019), pp. 3037-3067.\n[38] C. Fonte and V. Schmutz, Long time behavior of an age-and leaky memory-structured neuronal population equation, SIAM Journal on Mathematical Analysis, 54 (2022), pp. 4721-4756.\n[39] N. Fournier and E. L\u00f6cherbach, On a toy model of interacting neurons, Annales de l'Institut Henri Poincar\u00e9 Section (B) Probability and Statistics, 52 (2016), pp. 1844-1876.\n[40] A. Galves and E. L\u00f6cherbach, Modeling networks of spiking neurons as interacting processes with memory of variable length, Journal de la Soci\u00e9t\u00e9 Fran\u00e7aise de Statistique, 157 (2016), pp. 17-32.\n[41] A. Galves, E. L\u00f6cherbach, C. Pouzat, and E. Presutti, A system of interacting neurons with short term synaptic facilitation, Journal of Statistical Physics, 178 (2020), pp. 869-892.\n[42] W. Gerstner, Time structure of the activity in neural network models, Physical Review E, 51 (1995), p. 738 .\n[43] , Population dynamics of spiking neurons: fast transients, asynchronous states, and locking, Neural Computation, 12 (2000), pp. 43-89.\n[44] W. Gerstner, W. M. Kistler, R. Naud, and L. Paninski, Neuronal dynamics: From single neurons to networks and models of cognition, Cambridge University Press, 2014.\n[45] W. Gerstner and J. L. van Hemmen, Associative memory in a network of 'spiking' neurons, Network: Computation in Neural Systems, 3 (1992), pp. 139-164.\n[46] M. A. Gkogkas and C. Kuehn, Graphop mean-field limits for Kuramoto-type models, SIAM Journal on Applied Dynamical Systems, 21 (2022), pp. 248-283.\n[47] F. Golse, On the dynamics of large particle systems in the mean field limit, Macroscopic and large scale phenomena: coarse graining, mean field limits and ergodicity, (2016), pp. 1-144.\n[48] J. S. Griffith, A field theory of neural nets: I: Derivation of field equations, Bulletin of Mathematical Biophysics, 25 (1963), pp. 111-120.\n[49] M. HeliAs and D. Dahmen, Statistical Field Theory for Neural Networks, Springer, 2020.\n[50] D. H. Hubel and T. N. Wiesel, Receptive fields, binocular interaction and functional architecture in the cat's visual cortex, Journal of Physiology, 160 (1962), p. 106.\n[51] J. Inglis and D. Talay, Mean-field limit of a stochastic particle system smoothly interacting through threshold hitting-times and applications to neural networks with dendritic component, SIAM Journal on Mathematical Analysis, 47 (2015), pp. 3884-3916.\n[52] P.-E. Jabin, A review of the mean field limits for Vlasov equations, Kinetic and Related models, 7 (2014), pp. 661-711.\n[53] P.-E. Jabin, D. Poyato, and J. Soler, Mean-field limit of non-exchangeable systems, Communications on Pure and Applied Mathematics, (2024).\n[54] P.-E. Jabin and D. Zhou, The mean-field limit of sparse networks of integrate and fire neurons, arXiv preprint arXiv:2309.04046, (2023).\n[55] M. Kac, Foundations of kinetic theory, in Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, vol. 3, 1956, pp. 171-197.\n[56] D. Kaliuzhnyi-Verbovetskyi and G. S. Medvedev, The semilinear heat equation on sparse random graphs, SIAM Journal on Mathematical Analysis, 49 (2017), pp. 1333-1355.\n[57] , The mean field equation for the Kuramoto model on graph sequences with non-lipschitz limit, SIAM Journal on Mathematical Analysis, 50 (2018), pp. 2441-2465.\n\n[58] J. F. C. Kingman, Poisson processes, vol. 3, Clarendon Press, 1992.\n[59] D. Lacker, K. Ramanan, and R. Wu, Locally interacting diffusions as Markov random fields on path space, Stochastic Processes and their Applications, 140 (2021), pp. 81-114.\n[60] , Local weak convergence for sparse networks of interacting processes, Annals of Applied Probability, 33 (2023), pp. 843-888.\n[61] , Marginal dynamics of interacting diffusions on unimodular Galton-Watson trees, Probability Theory and Related Fields, (2023), pp. 1-68.\n[62] P. W. Lewis and G. S. Shedler, Simulation of nonhomogeneous Poisson processes by thinning, Naval Research Logistics Quarterly, 26 (1979), pp. 403-413.\n[63] E. L\u00d6CHERBACH, Spiking neurons: interacting Hawkes processes, mean field limits and oscillations, ESAIM: Proceedings and Surveys, 60 (2017), pp. 90-103.\n[64] L. Lov\u00c1sz, Large networks and graph limits, vol. 60, American Mathematical Soc., 2012.\n[65] L. Lov\u00c1sz and B. Szegedy, Limits of dense graph sequences, Journal of Combinatorial Theory, Series B, 96 (2006), pp. 933-957.\n[66] , Limits of compact decorated graphs, arXiv preprint arXiv:1010.5155, (2010).\n[67] L. Lov\u00c1sz AND K. VESZTERGOMBI, Non-deterministic graph property testing, Combinatorics, Probability and Computing, 22 (2013), pp. 749-762.\n[68] E. Lu\u00e7on, Quenched asymptotics for interacting diffusions on inhomogeneous random graphs, Stochastic Processes and their Applications, 130 (2020), pp. 6783-6842.\n[69] E. Lu\u00e7on and W. Stannat, Mean field limit for disordered diffusions with singular interactions, Annals of Applied Probability, (2014), pp. 1946-1993.\n[70] J. MacLaurin, Large deviations of a network of interacting particles with sparse random connections, arXiv preprint arXiv:1607.05471, (2018).\n[71] G. S. Medvedev, The nonlinear heat equation on dense graphs and graph limits, SIAM Journal on Mathematical Analysis, 46 (2014), pp. 2743-2766.\n[72] , The nonlinear heat equation on w-random graphs, Archive for Rational Mechanics and Analysis, 212 (2014), pp. 781-803.\n[73] S. Mehri, M. Scheutzow, W. Stannat, and B. Z. Zangenef, Propagation of chaos for stochastic spatially structured neuronal networks with delay driven by jump diffusions, Annals of Applied Probability, 30 (2020), pp. 175-207.\n[74] S. Mischler, C. Qu\u00ed\u00f1uaO, and Q. Weng, Weak and strong connectivity regimes for a general time elapsed neuron network model, Journal of Statistical Physics, 173 (2018), pp. 77-98.\n[75] S. Mischler and Q. Weng, Relaxation in time elapsed neuron network models in the weak connectivity regime, Acta Applicandae Mathematicae, 157 (2018), pp. 45-74.\n[76] V. B. MountCastle, Modality and topographic properties of single neurons of cat's somatic sensory cortex, Journal of neurophysiology, 20 (1957), pp. 408-434.\n[77] , The columnar organization of the neocortex., Brain: a journal of neurology, 120 (1997), pp. 701-722.\n[78] P. L. Nunez, The brain wave equation: a model for the EEG, Mathematical Biosciences, 21 (1974), pp. 279-297.\n[79] Y. Ogata, On lewis' simulation method for point processes, IEEE transactions on information theory, 27 (1981), pp. 23-31.\n[80] R. I. Oliveira and G. H. Reis, Interacting diffusions on random graphs with diverging average degrees: Hydrodynamics and large deviations, Journal of Statistical Physics, 176 (2019), pp. 1057-1087.\n[81] R. I. Oliveira, G. H. Reis, and L. M. Stolerman, Interacting diffusions on sparse graphs: hydrodynamics from local weak limits, Electronic Journal of Probability, 25 (2020), pp. 1 - 35.\n[82] S. Ostojic, N. Brunel, and V. Hakim, Synchronization properties of networks of electrically coupled neurons in the presence of noise and heterogeneities, Journal of computational neuroscience, 26 (2009), pp. 369-392.\n[83] K. Pakdaman, B. Perthame, and D. Salort, Dynamics of a structured neuron population, Nonlinearity, 23 (2010), pp. 55-75.\n[84] , Relaxation and self-sustained oscillations in the time elapsed neuron network model, SIAM J. Appl. Math., 73 (2013), pp. 1260-1279.\n[85] , Adaptation and fatigue model for neuron networks and large time asymptotics in a nonlinear fragmentation equation, Journal of Mathematical Neuroscience, 4 (2014), pp. Art. 14, 26.\n[86] T. Paul and E. Tr\u00e9lat, From microscopic to macroscopic scale equations: mean field, hydrodynamic and graph limits, arXiv preprint arXiv:2209.08832, (2022).\n[87] M. B. RaAd, S. Ditlevsen, and E. L\u00f6cherbach, Stability and mean-field limits of age dependent Hawkes processes, Ann. Inst. Henri Poincar\u00e9 Probab. Stat., 56 (2020), pp. 1958-1990.\n[88] P. Robert and J. Touboul, On the dynamics of random neuronal networks, Journal of Statistical Physics, 165 (2016), pp. 545-584.\n\n[89] V. Schmutz, Mean-field limit of age and leaky memory dependent hawkes processes, Stochastic Processes and their Applications, 149 (2022), pp. 39-59.\n[90] , Taming neuronal noise with large networks, thesis, EPFL, 2022.\n[91] V. Schmutz, E. L\u00f6cherbach, and T. Schwalger, On a finite-size neuronal population equation, SIAM Journal on Applied Dynamical Systems, 22 (2023), pp. 996-1029.\n[92] A.-S. Sznitman, Topics in propagation of chaos, in \u00c9cole d'\u00c9t\u00e9 de Probabilit\u00e9s de Saint-Flour XIX1989, vol. 1464 of Lecture Notes in Math., Springer, Berlin, 1991, pp. 165-251.\n[93] M. Talagrand, Mean field models for spin glasses: Volume I: Basic examples, vol. 54, Springer Science \\& Business Media, 2010.\n[94] , Mean Field Models for Spin Glasses: Volume II: Advanced Replica-Symmetry and Low Temperature, vol. 55, Springer Science \\& Business Media, 2011.\n[95] N. Torres, B. Perthame, and D. Salort, A multiple time renewal equation for neural assemblies with elapsed time model, Nonlinearity, 35 (2022), p. 5051.\n[96] J. Touboul, Propagation of chaos in neural fields, Annals of Applied Probability, 24 (2014), pp. 12981327 .\n[97] H. R. Wilson and J. D. Cowan, A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue, Kybernetik, 13 (1973), pp. 55-80.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2409.06325v3",
  "authors": [
    "Pierre-Emmanuel Jabin",
    "Valentin Schmutz",
    "Datong Zhou"
  ],
  "categories": [
    "math.PR",
    "math.AP",
    "q-bio.NC"
  ],
  "abstract": "The dynamics of spatially-structured networks of $N$ interacting stochastic\nneurons can be described by deterministic population equations in the\nmean-field limit. While this is known, a general question has remained\nunanswered: does synaptic weight scaling suffice, by itself, to guarantee the\nconvergence of network dynamics to a deterministic population equation, even\nwhen networks are not assumed to be homogeneous or spatially structured? In\nthis work, we consider networks of stochastic integrate-and-fire neurons with\narbitrary synaptic weights satisfying a $O(1/N)$ scaling condition. Borrowing\nresults from the theory of dense graph limits, or graphons, we prove that, as\n$N\\to\\infty$, and up to the extraction of a subsequence, the empirical measure\nof the neurons' membrane potentials converges to the solution of a\nspatially-extended mean-field partial differential equation (PDE). Our proof\nrequires analytical techniques that go beyond standard propagation of chaos\nmethods. In particular, we introduce a weak metric that depends on the dense\ngraph limit kernel and we show how the weak convergence of the initial data can\nbe obtained by propagating the regularity of the limit kernel along the\ndual-backward equation associated with the spatially-extended mean-field PDE.\nOverall, this result invites us to reinterpret spatially-extended population\nequations as universal mean-field limits of networks of neurons with $O(1/N)$\nsynaptic weight scaling.",
  "updated": "2025-04-06T20:51:41Z",
  "published": "2024-09-10T08:29:49Z"
}