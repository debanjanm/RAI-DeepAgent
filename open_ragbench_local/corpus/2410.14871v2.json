{"title": "Learning the Effect of Persuasion via Difference-In-Differences", "sections": [{"section_id": 0, "text": "## DIFFERENCE-IN-DIFFERENCES*\n\nSUNG JaE Jun ${ }^{\\dagger}$<br>SOKBAE LeE ${ }^{\\ddagger}$<br>Pennsylvania State University Columbia University\n\nDecember 5, 2024", "tables": {}, "images": {}}, {"section_id": 1, "text": "#### Abstract\n\nThe persuasion rate is a key parameter for measuring the causal effect of a directional message on influencing the recipient's behavior. Its identification has relied on exogenous treatment or the availability of credible instruments, but the requirements are not always satisfied in observational studies. Therefore, we develop a novel econometric framework for the average persuasion rate on the treated and other related parameters by using the difference-in-differences approach. The average treatment effect on the treated is a standard parameter in difference-in-differences, but we show that it is an overly conservative measure in the context of persuasion. For estimation and inference, we propose regression-based approaches as well as semiparametrically efficient estimators. Beginning with the two-period case, we extend the framework to staggered treatment settings, where we show how to conduct richer analyses like the event-study design. We investigate the British election and the Chinese curriculum reform as empirical examples.\n\n\nKey Words: Persuasion Rate, Treatment Effects, Media, Voting Behavior, Event Study Design.\n\nJEL Classification Codes: C22, C23, D72, L82\n\n[^0]\n[^0]:    *We would like to thank Peng Ding, Stefano DellaVigna, Vitor Possebom, Jonathan Roth, Pedro Sant'Anna, Zeyang (Arthur) Yu, and the seminar participants at McMaster University, Munich Econometrics Seminar, Seoul National University, Sungkyunkwan University, the University of Kentucky, and the University of Toronto for encouraging and helpful comments.\n    ${ }^{\\dagger}$ Department of Economics, suj14@psu.edu.\n    $\\ddagger$ Department of Economics, sl3841@columbia.edu.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 1. INTRODUCTION \n\nSince the dawn of civilization, the meaning of persuasion has evolved from Aristotle's Rhetoric to Jane Austen's Persuasion and to curriculum reform in China (e.g., Cantoni et al., 2017). Persuasion is integral to both democracy and the market economy. Economists have empirically measured how persuasive efforts influence the actions of consumers, voters, donors, and investors (see DellaVigna and Gentzkow, 2010, for a survey of the early literature).\n\nOne of the popular measures for the effectiveness of persuasive efforts is the persuasion rate that was first used by DellaVigna and Kaplan (2007) in the context of a biased news media and its influence on voters' behaviors. It has been regarded as a standardized measure to compare the magnitudes of treatment effects across different settings. For example, Table 1 in DellaVigna and Gentzkow (2010) and Figure 7 in Bursztyn and Yang $(2022)^{1}$ show the estimates of persuasion rates across many papers. Jun and Lee (2023) then reformulate the persuasion rate as a causal parameter to measure the effect of a persuasive message on influencing the recipient's behavior. They conduct a formal identification analysis within the potential outcome framework, for which they largely rely on exogenous treatment or the availability of credible instruments. However, those requirements are not always satisfied in observational studies. Therefore, we propose the average persuasion rate on the treated (APRT) and its reverse version (R-APRT) as causal parameters of interest, and we develop a framework to identify, estimate, and conduct inference for them via difference-in-differences (DID). The average treatment effect on the treated (ATT) is a popular causal parameter in this context, but we show that it is an overly conservative measure compared to APRT and R-APRT.\n\nTo convey the idea, consider the following example. Let $D$ be a binary indicator for an exposure to a certain media platform that delivers a directional message favoring a particular political party. For $d \\in\\{0,1\\}$, let $Y(d)$ be a binary potential outcome that indicates\n\n[^0]\n[^0]:    ${ }^{1}$ Specifically, Bursztyn and Yang (2022) provide a meta-analysis of the literature that shows that experimental treatments to recalibrate misperceptions can lead to changes in behaviors. Figure 7 in Bursztyn and Yang (2022) displays the persuasion rates based on DellaVigna and Kaplan (2007).\n\nwhether the agent votes for the favored party or not. Since the media platform delivers a directional message, suppose that $Y(1) \\geq Y(0)$ with probability one: we refer to this assumption as the assumption of no backlash. Then, ATT can be written as\n\n$$\n\\mathbb{P}\\{Y(1)=1 \\mid D=1\\}-\\mathbb{P}\\{Y(0)=1 \\mid D=1\\}=\\mathbb{P}\\{Y(1)=1, Y(0)=0 \\mid D=1\\} .^{2}\n$$\n\nHowever, ATT is a different measure from the proportion of those who voted for the favored party after the media exposure among the people in the treatment group who would have voted differently without the media exposure: i.e.,\n\n$$\n\\mathbb{P}\\{Y(1)=1, Y(0)=0 \\mid D=1\\} \\leq \\mathbb{P}\\{Y(1)=1 \\mid Y(0)=0, D=1\\}\n$$\n\nwhere the inequality can be severely strict when $\\mathbb{P}\\{Y(0)=0 \\mid D=1\\}$ is small. If the size of the target audience to persuade among the treated is small, then ATT will also be small even if the message is quite successful to change their behaviors.\n\nThe conditional probability on the right-hand side of (1) is what we call APRT, which focuses only on the subpopulation that is relevant to measure the persuasive effect of a directional message. However, this target audience is a counterfactual group that is not directly observable. Therefore, as a supplementary parameter, we also consider the reverse version of APRT (R-APRT), i.e., $\\mathbb{P}\\{Y(0)=0 \\mid Y(1)=1, D=1\\}$, which shows what proportion of those who were exposed to a certain media platform, and who actually voted for the party the media endorsed would have voted differently if it were not for the media exposure. In other words, it measures the degree of necessity of the media exposure for the voting behavior desired by the media. Indeed, R-APRT is exactly what Pearl (1999) refers to as the probability of necessity (PN). ${ }^{3}$ However, we remark that the forward version of APRT in equation (1) is distinct from Pearl (1999)'s probability of sufficiency (PS), which would correspond to the (average) persuasion rate on the untreated, i.e., $\\mathbb{P}\\{Y(1)=0 \\mid$\n\n[^0]\n[^0]:    ${ }^{2}$ The equality follows from the fact that $\\mathbb{P}\\{Y(1)=1 \\mid D=1\\}=\\mathbb{P}\\{Y(1)=1, Y(0)=1 \\mid D=1\\}+\\mathbb{P}\\{Y(1)=$ $1, Y(0)=0 \\mid D=1\\}$ and $\\mathbb{P}\\{Y(0)=1 \\mid D=1\\}=\\mathbb{P}\\{Y(0)=1, Y(1)=1 \\mid D=1\\}$ under no backlash.\n    ${ }^{3}$ The same parameter is called the probability of causal attribution in the political science literature (Yamamoto, 2012).\n\n$Y(0)=1, D=0\\}$ in our context. We do not consider PS because it will not be pointidentified via DID, just like the average treatment effect on the untreated is not identifiable via DID. See section 2 for more discussion.\n\nThe idea of \"rescaling\" to focus on a particular event or subpopulation has been around for a long time: e.g., Bayes (1763) rescales joint probability to obtain conditional probability; Imbens and Angrist (1994) rescale the effect of the intent-to-treat (ITT) to obtain the local average treatment effect (LATE); Jun and Lee (2023) use rescaling the average treatment effect (ATE) and LATE to obtain the persuasion rate and the local persuasion rate, respectively. Here, we follow the same idea, and we rescale ATT to obtain APRT and RAPRT in the absence of backlash. Furthermore, even if the assumption of no backlash is violated, scaling up ATT leads to lower bounds on APRT and R-APRT. As it is the case for joint versus conditional probabilities, ATT, APRT, and R-APRT are all related, but they deliver different information in general.\n\nWe not only articulate identification issues for APRT and R-APRT in panel-like setups with no instruments, but we also investigate estimation and inference problems in depth. Specifically, the main contributions of the paper are threefold. First, we clarify the relationships between APRT, R-APRT, and ATT with and without backlash: for instance, in the absence of backlash, identification of APRT as well as that of R-APRT is reduced to that of ATT. For the latter, we follow Abadie (2005) and Callaway and Sant'Anna (2021), and we focus on the setting where the assumption of parallel trends holds after conditioning on observed covariates. We consider the canonical two-period case as well as the popular case of staggered treatment adoption. In this way, we contribute to the recent fast-growing literature on the DID framework and the panel event-study design: DID and heterogeneous treatment effects (e.g. De Chaisemartin and d'Haultfoeuille, 2020; Xu et al., 2024), estimation of causal effects with panel data (e.g. Ding and Li, 2019; Freyaldenhoven et al., 2019; Arkhangelsky et al., 2021), and heterogeneous treatment timings (e.g. Goodman-Bacon, 2021; Sun and Abraham, 2021; Callaway and Sant'Anna, 2021; Athey and Imbens, 2022).\n\nSee e.g., de Chaisemartin and D'Haultf\u0153uille (2022); Freyaldenhoven et al. (2021); Roth et al. (2023); Sun and Shapiro (2022) for the introductory articles and the latest surveys.\n\nSecond, we propose two alternative estimation methods. When covariates are not needed, we show that one can use simple regression-based approaches for estimation, and we explain how they are connected to the standard two-way fixed effect regression and eventstudy design models. When covariates are needed, \"partialing them out\" from the regression analysis could be an easy option, but we do not advocate it because of potential contamination bias problems in estimating treatment effects as Blandhol et al. (2022) and Goldsmith-Pinkham et al. (2022) pointed out. Instead, we propose a battery of asymptotically equivalent efficient estimators of APRT and R-APRT, including doubly (and locally) robust estimators based on our derivation of the efficient influence functions of APRT and R-APRT. Therefore, we contribute to the literature on efficient and doubly robust estimation of treatment effects for which ATE and ATT are typical parameters of interest (see, e.g., Hahn, 1998; Hirano et al., 2003; Sant'Anna and Zhao, 2020; Chernozhukov et al., 2022, among others).\n\nThird, we offer two options for inference in addition to the straightforward method based on the regression-based approaches. One relies on our efficient estimators, while the other involves back-of-the-envelope inference based on ATT. The former, though efficient and asymptotically exact, necessitates access to the entire dataset, whereas the latter is potentially conservative but can be conducted by combining the confidence interval for ATT with information on a certain summary statistic, eliminating the need for the entire dataset. In other words, the latter provides a convenient way to obtain the confidence intervals for APRT and R-APRT when the confidence interval for ATT is already provided.\n\nWe are not aware of any existing work that studies the persuasion rate within the DID framework. However, there are related papers on the persuasion rate in general. In the same setting as Jun and Lee (2023), Yu (2023) explores identification of the statistical characteristics of persuasion types. Possebom and Riva (2024) study identification of the average persuasion rate with sample selection. The concept of the causal persuasion rate is\n\nrelated to probabilities of causation, which have been studied for decades in the literature outside economics: e.g., Pearl (1999); Yamamoto (2012); Dawid et al. (2014); Dawid and Musio (2022); Zhang et al. (2024). However, APRT does not appear to match any of the probabilities of causation, and furthermore, the existing work on them is mostly limited to an analysis of population bounds without relating to DID or investigating inferential problems. Furthermore, economists were not cognizant of this literature; one recent exception is Possebom and Riva (2024). More broadly speaking, our paper is related to a branch of the literature on treatment effects that depend on the joint distribution of potential outcomes. For example, see Heckman et al. (1997) as early pioneering work as well as Ji et al. (2023) and Kaji and Cao (2023) for recent working papers.\n\nThe remainder of the paper is organized as follows. Sections 2 to 7 focus on the canonical case of two periods, whereas section 8 shows extensions to the case of staggered treatments in detail. In section 7, we present an empirical example of the two-period case by reexamining the impact of news media on the 1997 British election (Ladd and Lenz, 2009; Hainmueller, 2012). In exploring staggered treatments in section 8, we emphasize how to do an event-study-design-like analysis. In section 9, as an empirical example of staggered treatments, we re-evaluate the effects of curriculum reforms on political attitudes in China (Cantoni et al., 2017). Finally, the appendices, including the online ones, include all the proofs as well as additional results and discussions.", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 2. The Parameters \n\nLet $Y_{t}$ and $D_{t}$ be binary random variables that are observed at time $t$, where $t \\in\\{0,1\\}$ : $t=0$ represents a pre-treatment period. We will denote the potential outcomes at time $t$ by $Y_{t}(d)$, and therefore we observe $Y_{t}=D_{t} Y_{t}(1)+\\left(1-D_{t}\\right) Y_{t}(0)$. We also observe a vector of exogenous covariates, which will be denoted by $X$. It will be helpful to have an example in mind: e.g., $Y_{1}(1)$ indicates whether an agent votes for a certain party or not, after getting exposed to a certain news platform in period 1, whereas $Y_{1}(0)$ is whether she votes for the party without having an exposure to the platform in period 1.\n\nOur goal is to study identification and estimation of the causal persuasion rate on the treated in a setup of panel data: we will formally define and discuss the parameter of interest in the following subsections. We first make the following assumption to describe the setup formally. Let $\\mathcal{X}$ be the support of $X$.\n\nAssumption A (No Anticipation and Non-Degenerate Probabilities). At time $t=0$, no one is treated so that $Y_{0}=Y_{0}(0)$ and $D_{0}=0$ with probability one. At time $t=1$, there is a constant $\\epsilon>0$ such that for (almost) all $x \\in \\mathcal{X}, \\epsilon \\leq \\min \\left[\\mathbb{P}\\left\\{Y_{1}(0)=0, D_{1}=1 \\mid X=x\\right\\}, \\mathbb{P}\\left\\{Y_{1}(1)=\\right.\\right.$ $\\left.1, D_{1}=1 \\mid X=x\\}\\right]$ and $\\mathbb{P}\\left(D_{1}=1 \\mid X=x\\right) \\leq 1-\\epsilon$.\n\nAssumption A clarifies that $t=0$ is a pre-treatment period, and that the treatment occurs at $t=1$. The second part of assumption A is to ensure that, for instance, there is a non-trivial group of people who are exposed to a certain news platform and who would note have voted for the party that the news platform supports if they had not been exposed to it. Otherwise, there would be nobody to \"persuade\" among the treated.\n\nIn the following subsections, we will define two versions of the individualized persuasion rate on the treated, and we will discuss their aggregation.\n2.1. The Conditional Persuasion Rate on the Treated. We define the conditional persuasion rate on the treated (CPRT) and its reverse version (R-CPRT) in the post-treatment period as follows: for $x \\in \\mathcal{X}$,\n\n$$\n\\begin{aligned}\n\\theta_{c}(x) & :=\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid Y_{1}(0)=0, D_{1}=1, X=x\\right\\} \\\\\n\\theta_{c}^{(r)}(x) & :=\\mathbb{P}\\left\\{Y_{1}(0)=0 \\mid Y_{1}(1)=1, D_{1}=1, X=x\\right\\}\n\\end{aligned}\n$$\n\nwhich are all well-defined under assumption A.\nAlthough the two parameters are closely related, they have different merits. Recall the voting example we mentioned earlier, where $D_{1}$ indicates an exposure to a certain media platform that endorses a particular political party. Both $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x)$ focus on those who are characterized by the exogenous covariates and who are exposed to the media platform. Within this subpopulation, $\\theta_{c}(x)$ measures the proportion of those who have\n\nvoted for the party the news media endorsed relative to those who would not have done so without the media exposure. Therefore, it measures the direct persuasive effect of the media on the exposed in that it shows how many of those who have received the persuasive message have actually changed their behavior as the message wanted them to. This is the on-the-treated version of the persuasion rate of DellaVigna and Kaplan (2007) and Jun and Lee (2023).\n\nAlthough $\\theta_{c}(x)$ provides a straightforward interpretation, it has a drawback: i.e., it conditions on a counterfactual group that is not directly observed by the researcher. The reverse rate $\\theta_{c}^{(r)}(x)$ addresses this issue. We now consider those who have been exposed to the media and voted for the party the media endorse. We then ask how many of them would have behaved differently if it were not for the media. This type of approach is common in the context of lawsuits and legal debates.\n\nIn fact, $\\theta_{c}^{(r)}(x)$ is exactly Pearl (1999)'s probability of necessity conditional on $X=x$, which was proposed to measure the degree of necessity of a causal treatment for a \"successful\" outcome. In the context of the voting example, it shows how necessary the media influence is for the desired voting behavior. For additional examples that frequently arise in legal contexts, see Pearl (1999) and Dawid et al. (2014, and the accompanying comments and author responses). In contrast, $\\theta_{c}(x)$ is distinct from a conditional version of Pearl's probability of sufficiency, i.e., $\\operatorname{PS}(x):=\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid Y_{1}(0)=0, D_{1}=0, X=x\\right\\}$, which we may call a conditional persuasion rate on the untreated. Unlike $\\theta_{c}(x), \\operatorname{PS}(x)$ is conditioned on a subpopulation that is directly identifiable. However, $\\operatorname{PS}(x)$ is not identifiable in the standard natural experiment setup with a parallel trend assumption.\n\nTo further understand CPRT and R-CPRT, let $p_{s t}(s):=\\mathbb{P}\\left\\{Y_{1}(0)=s, Y_{1}(1)=t \\mid D_{1}=\\right.$ $1, X=x\\}$ for $(s, t) \\in\\{0,1\\}^{2}$, and we can write\n\n$$\n\\theta_{c}(x)=\\frac{p_{01}(x)}{p_{01}(x)+p_{00}(x)} \\text { and } \\theta_{c}^{(r)}(x)=\\frac{p_{01}(x)}{p_{01}(x)+p_{11}(x)}\n$$\n\nHere, $p_{00}(x)$ regards the \"never-persuadable (NP) among the treated with $X=x$ \" and $p_{11}(x)$ does the \"already-persuaded (AP) among the treated with $X=x$ \" while $p_{01}(x)$\n\nconsiders the \"treatment-persuadable (TP) among the treated with $X=x$.\" Therefore, CPRT $\\theta_{c}(x)$ is the relative share of TP in the subpopulation of NP and TP among the treated with $X=x$, whereas R-CPRT $\\theta_{c}^{(r)}(x)$ is the relative size of TP in the subgroup of AP and TP among the treated with $X=x$.\n\nSince the potential outcomes $Y_{1}(0)$ and $Y_{1}(1)$ cannot be observed simultaneously, we cannot generally point-identify CPRT or R-CPRT without making additional assumptions. Consider the following parameters:\n\n$$\n\\theta_{c L}(x):=\\frac{p_{01}(x)-p_{10}(x)}{p_{01}(x)+p_{00}(x)} \\quad \\text { and } \\quad \\theta_{c L}^{(r)}(x):=\\frac{p_{01}(x)-p_{10}(x)}{p_{01}(x)+p_{11}(x)}\n$$\n\nUnlike $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x), \\theta_{c L}(x)$ and $\\theta_{c L}^{(r)}(x)$ depend only on the marginal probabilities of the potential outcomes given $D_{1}=1$ and $X=x$ because $p_{01}(x)-p_{10}(x)$ is equal to\n\n$$\n\\operatorname{CATT}(x):=\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=1, X=x\\right\\}\n$$\n\nSince $\\theta_{c L}(x) \\leq \\theta_{c}(x)$ and $\\theta_{c L}^{(r)}(x) \\leq \\theta_{c}^{(r)}(x)$ in general, we can view $\\theta_{c L}(x)$ and $\\theta_{c L}^{(r)}(x)$ as general conservative measures of $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x)$, respectively. However, they are less conservative than ATT in general. Indeed, they are Fr\u00e9chet-Hoeffding lower bounds on $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x)$, and they are sharp based on the marginals of the potential outcomes given $D_{1}=1$ and $X=x$. See appendix $S-1$ for more discussion.\n\nOf course, if $p_{10}(x)=0$, then $\\theta_{c L}(x)$ and $\\theta_{c L}^{(r)}(x)$ coincide with $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x)$. Below we discuss this condition in detail.\n\nAssumption B (No Backlash). $\\mathbb{P}\\left\\{Y_{1}(1) \\geq Y_{1}(0) \\mid D_{1}=1, X\\right\\}=1$ almost surely.\nUnder assumption B, we have $p_{10}(x)=0$, and hence $p_{00}(x)+p_{01}(x)+p_{11}(x)=1$ for (almost) all $x \\in \\mathcal{X}$. In other words, we rule out the presence of \"contrarians\" (among the treated) i.e., those who always do the opposite of what the persuasive message directs; hence, the treated (with or without conditioning on $X$ ) can be decomposed into three persuasion types: i.e., NP, AP, and TP.\n\nIn the context of the voting example, assumption B says that if the voters (among the treated) would vote for the party the news media support without listening to them, then exposing them to the media would not change their mind. Put differently, assumption B assumes that a persuasive message is directional or biased, and that there is no backlash at least among the treated. A sufficient condition for assumption $B$ is the monotone treatment response (MTR) assumption (Manski, 1997), which states that $Y_{1}(1) \\geq Y_{1}(0)$ almost surely in our setting. The MTR assumption with binary potential outcomes was also adopted in Pearl (1999) and Jun and Lee (2023).\n\nLemma 1. If assumptions $A$ and $B$ holds, then $\\theta_{c}(x)=\\theta_{c L}(x)$ and $\\theta_{c}^{(r)}(x)=\\theta_{c L}^{(r)}(x)$ for all $x \\in \\mathcal{X}$. Even if assumption $B$ can be violated, we have $\\theta_{c}(x) \\geq \\theta_{c L}(x)$ and $\\theta_{c}^{(r)}(x) \\geq \\theta_{c L}^{(r)}(x)$ for all $x \\in \\mathcal{X}$ in general.\n\nLemma 1 is not an identification result yet. However, it shows that ruling out a backlash effect enables us to express the conditional probabilities $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x)$ by using the marginal probabilities of the potential outcomes. Moreover, even if the backlash is of concern, $\\theta_{c L}(x)$ and $\\theta_{c L}^{(r)}(x)$ will continue to serve as conservative measures of $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x)$, respectively. Our notation using the subscript $L$ is to emphasize that they are robust lower bounds even if assumption B is violated. We provide further discussion on bounds, their sharpness, and the backlash in appendix S-1.\n\nTherefore, under assumptions A and B, the point-identification of CPRT and R-CPRT will hinge on that of CATT. Since assumption B ensures that $\\operatorname{CATT}(x) \\geq 0$, both CPRT and R-CPRT are guaranteed to be no smaller than CATT: i.e., $\\min \\left\\{\\theta_{c L}(x), \\theta_{c L}^{(r)}(x)\\right\\} \\geq \\operatorname{CATT}(x) \\geq$ 0 , where the first inequality is strict if $\\operatorname{CATT}(x)>0$ and $\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=1, X=x\\right\\}>$ 0 : e.g., if there are people who would vote for the party a news platform endorses without even listening to them, then CPRT is strictly larger than CATT. This is because CATT does not take into account the case of \"preaching to the converted\" in measuring the persuasive effect, whereas CPRT and R-CPRT address this issue by conditioning.\n\nSince $Y_{1}(1)=Y_{1}$ when $D_{1}=1$ and $Y_{0}(0)=Y_{0}$, there is only one unidentified object in $\\theta_{c L}(x)$ and $\\theta_{c L}^{(r)}(x)$ : i.e.,\n\n$$\n\\tau_{c}(x):=\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=1, X=x\\right\\}\n$$\n\nBefore we proceed for identification of $\\tau_{c}(x)$, we first discuss aggregation in the following subsection.\n2.2. Aggregation. The persuasion rates on the treated as aggregated versions of $\\theta_{c}(\\cdot)$ and $\\theta_{c}^{(r)}(\\cdot)$, which we call the average persuasion rate on the treated (APRT) and reverse APRT (R-APRT), respectively, are defined by\n\n$$\n\\begin{aligned}\n\\theta & :=\\mathbb{E}\\left\\{\\theta_{c}(X) \\mid Y_{1}(0)=0, D_{1}=1\\right\\}=\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid Y_{1}(0)=0, D_{1}=1\\right\\} \\\\\n\\theta^{(r)} & :=\\mathbb{E}\\left\\{\\theta_{c}^{(r)}(X) \\mid Y_{1}(1)=1, D_{1}=1\\right\\}=\\mathbb{P}\\left\\{Y_{1}(0)=0 \\mid Y_{1}(1)=1, D_{1}=1\\right\\}\n\\end{aligned}\n$$\n\nThe forward version $\\theta$ can be compared with the average persuasion rate (APR) that Jun and Lee (2023) studied, i.e., $\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid Y_{1}(0)=0\\right\\}$. The relationship between APR and APRT is similar to that of ATE and ATT.\n\nBy aggregating the numerators and denominators in equation (3), we obtain the following parameters:\n\n$$\n\\begin{aligned}\n\\theta_{L} & :=\\frac{\\mathbb{E}\\left\\{\\operatorname{CATT}(X) \\mid D_{1}=1\\right\\}}{\\mathbb{E}\\left\\{1-\\tau_{c}(X) \\mid D_{1}=1\\right\\}}=\\frac{\\text { ATT }}{\\text { ATT }+\\mathbb{P}\\left(Y_{1}=0 \\mid D_{1}=1\\right)} \\\\\n\\theta_{L}^{(r)} & :=\\frac{\\mathbb{E}\\left\\{\\operatorname{CATT}(X) \\mid D_{1}=1\\right\\}}{\\mathbb{E}\\left\\{\\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=1, X\\right) \\mid D_{1}=1\\right\\}}=\\frac{\\text { ATT }}{\\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=1\\right)}\n\\end{aligned}\n$$\n\nwhere $\\operatorname{ATT}:=\\mathbb{E}\\left\\{\\operatorname{CATT}(X) \\mid D_{1}=1\\right\\}$. If there is no backlash for all $x \\in \\mathcal{X}$, then there is no difference between $\\left(\\theta, \\theta^{(r)}\\right)$ and $\\left(\\theta_{L}, \\theta_{L}^{(r)}\\right)$.\n\nLemma 2. If assumptions $A$ and $B$ hold, then we have $\\theta=\\theta_{L}$ and $\\theta^{(r)}=\\theta_{L}^{(r)}$. Even if assumption B can be violated, we have $\\theta \\geq \\theta_{L}$ and $\\theta^{(r)} \\geq \\theta_{L}^{(r)}$ in general.\n\nIt is a consequence of the Bayes rule that averaging the numerators and the denominators of $\\theta_{c L}(\\cdot)$ and $\\theta_{c L}^{(r)}(\\cdot)$ separately is the right way of aggregation. Again, identification of $\\tau_{c}(\\cdot)$ is sufficient for that of $\\theta_{L}$ and $\\theta_{L}^{(r)}$.\n\nAs in the case of CPRT and R-CPRT, the subscript $L$ emphasizes that $\\theta_{L}$ and $\\theta_{L}^{(r)}$ provide valid lower bounds on $\\theta$ and $\\theta^{(r)}$ even without assumption B. However, if assumption B does not hold, then $\\theta_{c L}(x)$ and $\\theta_{c L}^{(r)}(x)$ can be negative for some $x \\in \\mathcal{X}$, whereas $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x)$ can never be. Therefore, the aggregated parameters $\\theta_{L}$ and $\\theta_{L}^{(r)}$ may not be the best bounds we can obtain from the marginals of the potential outcomes. We discuss the issues of sharp bounds and robust interpretation without assumption B in detail in Online Appendix S-1.\n\nFor the sake of intuition, suppose that $Y_{1}(1) \\geq Y_{1}(0)$ with probability one so that assumption B is satisfied, and focus on APRT. Then, APRT is obtained by rescaling ATT, where the rescaling factor is determined by how many people in the treatment group are not pre-converted and have potential to be persuaded: ${ }^{4}$ the more people in the treatment group were going to take the action of interest without listening to a persuasive message, the more important it is to address the issue of \"preaching to the converted\" in measuring the pure persuasive effect of the treatment. For instance, if no one in the treatment group is pre-converted and everybody is a real target to persuade, then there will be no difference between ATT and APRT. Otherwise, the two causal parameters are distinct, and the latter is usually larger.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 3. IDENTIFICATION \n\n3.1. Identification via Parallel Trends. We continue to focus on the simple case of two time periods and take a difference-in-differences (DID) approach for identification of $\\tau_{c}(\\cdot)$. The case of staggered treatment will be discussed in section 8. The key assumption for identification is that of parallel trends.\n\nAssumption C (Parallel Trends). $\\mathbb{P}\\left\\{Y_{t}(0)=1 \\mid D_{1}=d, X=x\\right\\}$ is separable into the sum of a time component and a treatment component: i.e., there exist functions $G$ and $H$ such that $\\mathbb{P}\\left\\{Y_{t}(0)=1 \\mid D_{1}=d, X=x\\right\\}=G(t, x)+H(d, x)$.\n\n[^0]\n[^0]:    ${ }^{4}$ In terms of the decomposition of the treatment group we discussed earlier, APRT is equal to $\\# \\mathrm{TP} /(\\# \\mathrm{TP}+$ $\\# \\mathrm{NP})=\\# \\mathrm{TP} / \\# \\mathrm{AP}^{c}$, where $\\# A$ represents the size of group $A$.\n\nBy Proposition 3.2 and Example 1 of Roth and Sant'Anna (2023), assumption C is an equivalent form of parallel trends such that\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=1, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{0}(0)=1 \\mid D_{1}=1, X=x\\right\\} \\\\\n& \\quad=\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=0, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{0}(0)=1 \\mid D_{1}=0, X=x\\right\\}\n\\end{aligned}\n$$\n\nThe class of generalized linear models such as logit or probit is popular in parametric approaches, but assumption C does not allow it. For example, a parametric model such as $\\mathbb{P}\\left\\{Y_{t}(0)=1 \\mid D_{1}=d, X=x\\right\\}=\\Lambda^{-1}\\left(\\beta_{0}+\\beta_{1} t+\\beta_{2} d+\\beta_{3}^{\\top} x\\right)$ with $\\Lambda^{-1}(s):=$ $\\exp (s) /\\{1+\\exp (s)\\}$ is not allowed. However, it is straightforward to modify assumption C to introduce a nonlinear link function $\\Lambda$ on $[0,1]$, as long as the link function is pre-specified. ${ }^{5}$ We discuss this modification in Online Appendix S-2. ${ }^{6}$\n\nDefine\n\n$$\n\\Psi(X):=\\Pi_{0}(1, X)+\\Pi_{1}(0, X)-\\Pi_{0}(0, X)\n$$\n\nwhere $\\Pi_{t}(d, x):=\\mathbb{P}\\left(Y_{t}=1 \\mid D_{1}=d, X=x\\right)$ for $t=\\{0,1\\}$ and $\\left(d, x^{\\top}\\right)^{\\top} \\in\\{0,1\\} \\times \\mathcal{X}$, which are all directly identified from the distribution of $\\left(Y_{0}, Y_{1}, D_{1}, X^{\\top}\\right)^{\\top}$.\n\nTheorem 1. Suppose that assumptions $A$ and $C$ hold. Then, for all $x \\in \\mathcal{X}, \\tau_{c}(x)$ is point-identified by $\\Psi(x)$.\n\nTherefore, under assumptions A to $\\mathrm{C}, \\theta_{c L}(\\cdot), \\theta_{c L}^{(r)}(\\cdot), \\theta_{L}$, and $\\theta_{L}^{(r)}$ are all point-identified: e.g., $\\theta_{c L}(x)$ is identified by rescaling the usual DID parameter in that for all $x \\in \\mathcal{X}$,\n\n$$\n\\theta_{c L}(x)=\\frac{\\Pi_{1}(1, x)-\\Psi(x)}{1-\\Psi(x)}=\\frac{\\operatorname{CATT}(x)}{\\operatorname{CATT}(x)+1-\\Pi_{1}(1, x)}\n$$\n\n[^0]\n[^0]:    ${ }^{5}$ The change-in-changes (CIC) approach of Athey and Imbens (2006) is popular to handle nonlinearity under alternative assumptions. However, it is less convenient when the outcomes are only binary, and therefore we do not purpose this possibility in this paper.\n    ${ }^{6}$ Specifically, we consider a generalized version of parallel trends with the transformation $\\Lambda$ :\n\n    $$\n    \\begin{aligned}\n    \\Lambda\\left[\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=1, X=x\\right\\}\\right]-\\Lambda\\left[\\mathbb{P}\\left\\{Y_{0}(0)=1 \\mid D_{1}=1, X=x\\right\\}\\right] \\\\\n    \\quad=\\Lambda\\left[\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=0, X=x\\right\\}\\right]-\\Lambda\\left[\\mathbb{P}\\left\\{Y_{0}(0)=1 \\mid D_{1}=0, X=x\\right\\}\\right] \\text {. }\n    \\end{aligned}\n    $$\n\nwhere $\\operatorname{CATT}(x)=\\Pi_{1}(1, x)-\\Psi(x)$ is the usual DID estimand: i.e., $\\operatorname{CATT}(x)=\\Delta(1, x)-$ $\\Delta(0, x)$ with $\\Delta(d, x):=\\Pi_{1}(d, x)-\\Pi_{0}(d, x)$.\n\nSince the aggregated parameters are of particular interest when we have many covariates, we make a formal statement about them as a corollary. Define\n\n$$\n\\tilde{\\theta}_{L}:=\\frac{\\mathbb{E}\\left\\{\\Pi_{1}(1, X)-\\Psi(X) \\mid D_{1}=1\\right\\}}{\\mathbb{E}\\left\\{1-\\Psi(X) \\mid D_{1}=1\\right\\}}, \\quad \\tilde{\\theta}_{L}^{(r)}:=\\frac{\\mathbb{E}\\left\\{\\Pi_{1}(1, X)-\\Psi(X) \\mid D_{1}=1\\right\\}}{\\mathbb{E}\\left\\{\\Pi_{1}(1, X) \\mid D_{1}=1\\right\\}}\n$$\n\nCorollary 1. Suppose that assumptions $A$ to $C$ hold. Then, $\\theta=\\theta_{L}=\\tilde{\\theta}_{L}$ and $\\theta^{(r)}=\\theta_{L}^{(r)}=\\tilde{\\theta}_{L}^{(r)}$. If assumption B can be violated, we have $\\theta \\geq \\theta_{L}=\\tilde{\\theta}_{L}$ and $\\theta^{(r)} \\geq \\theta_{L}^{(r)}=\\tilde{\\theta}_{L}^{(r)}$ in general.\n\nTherefore, if there is no backlash in the sense of assumption B, then APRT and R-APRT are point-identified by rescaling the usual DID estimand. Even if backlash effects are concerning, rescaling the DID parameter provides conservative measures of APRT and RAPRT. However, $\\tilde{\\theta}_{L}$ and $\\tilde{\\theta}_{L}^{(r)}$ are not generally sharp identified lower bounds. See Online Appendix S-1 for more details.\n\nRecall that assumption B decomposes the group of the treated (with or without conditioning on $X$ ) into three persuasion types. If assumption C holds in addition, then ATT is identified. Therefore, the shares of the three types among the treated are all identified as well: the share of NP among the treated is given by $\\mathbb{E}\\left\\{p_{00}(X) \\mid D_{1}=1\\right\\}=\\mathbb{P}\\left(Y_{1}=0 \\mid\\right.$ $\\left.D_{1}=1\\right)$, and the share of AP among the treated is $\\mathbb{E}\\left\\{p_{11}(X) \\mid D_{1}=1\\right\\}=\\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=\\right.$ 1) - ATT, while that of TP among the treated is just ATT.\n\nFor estimation, we can simply replace $\\Pi_{t}(d, x)$ with their parametric or nonparametric estimates. However, when it comes to APRT and R-APRT, directly plugging in $\\Pi_{t}(d, x)$ 's and aggregating them is not the only possibility. We will discuss various approaches for estimation in section 4.\n3.2. Controlling for the Pre-Treatment Outcome and Unconfoundedness. As an alternative to the parallel trend assumption, it is popular to assume unconfoundedness after controlling for enough covariates. Specifically, when a pre-treatment outcome $Y_{0}$ is observed,\n\nit is a natural idea to assume unconfoundedness after controlling for $Y_{0}$ in addition to $X$ to achieve identification.\n\nIn this context, it is worth noting that if we use $Z:=\\left[Y_{0}, X\\right]$ in lieu of $X$ in defining $\\tilde{\\theta}_{L}$ and $\\tilde{\\theta}_{L}^{(r)}$, then we are led to estimands that identify $\\theta_{L}$ and $\\theta_{L}^{(r)}$, respectively, under the independence assumption of $Y_{1}(0)$ and $D_{1}$ given $Z$. Therefore, adding $Y_{0}$ to $X$ and using the DID formulas can be thought of as an implementation of identifying APRT and R-APRT via unconfoundedness given $Z$.\n\nThere has been some debate about the desirability of conditioning on $Y_{0}$ when estimating ATT via DID: see, e.g., Roth et al. (2023, pp. 2232) and the references therein. However, it seems less well-known that there is a testable condition under which it becomes moot to distinguish the two approaches. In fact, it can be shown that if $Y_{0}$ is independent of $D_{1}$ given $X$, then the two alternative identification assumptions lead to the same estimands for $\\theta_{L}$ and $\\theta_{L}^{(r)}$. Therefore, under the aforementioned independence condition, it does not matter which stance the researcher takes between the two alternative identification assumptions. We provide a more detailed discussion on this issue in appendix S-3.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 4. ESTIMATION \n\n4.1. Regression-Based Approaches. It is a popular practice to estimate ATT by using a two-way fixed effect regression model. There is a similar approach to estimate the persuasion rate. In order to clarify the idea, we focus on the case where there are no covariates for now. If assumptions A to C hold without the covariates $X$, then APRT and R-APRT are given by\n\n$$\n\\begin{aligned}\n\\tilde{\\theta}_{L} & =\\frac{\\Delta(1)-\\Delta(0)}{\\Delta(1)-\\Delta(0)+1-\\Pi_{1}(1)}=\\frac{\\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=1\\right)-\\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0\\right)}{\\mathbb{E}\\left(1-Y_{0} \\mid D_{1}=1\\right)-\\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0\\right)} \\\\\n\\tilde{\\theta}_{L}^{(r)} & =\\frac{\\Delta(1)-\\Delta(0)}{\\Pi_{1}(1)}=\\frac{\\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=1\\right)-\\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0\\right)}{\\mathbb{E}\\left(Y_{1} \\mid D_{1}=1\\right)}\n\\end{aligned}\n$$\n\nwhere $\\Delta(d):=\\Pi_{1}(d)-\\Pi_{0}(d)$ and $\\Pi_{t}(d):=\\mathbb{P}\\left(Y_{t}=1 \\mid D_{1}=d\\right)$ for $(t, d)=\\{0,1\\}^{2}$. Below we treat $\\tilde{\\theta}_{L}$ and $\\tilde{\\theta}_{L}^{(r)}$ in (6) and (7) as the estimands of interest, for which we directly assume\n\nthat $\\mathbb{E}\\left(Y_{1} \\mid D_{1}=1\\right)>0$ and $\\Psi:=\\mathbb{E}\\left(Y_{0} \\mid D_{1}=1\\right)+\\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0\\right)<1$, and we show that they can be obtained from a simple linear regression.\n\nConsider the following two-way fixed effect regression model:\n\n$$\nY_{i t}=\\gamma_{0}+G_{i} \\gamma_{1}+t \\gamma_{2}+t G_{i} \\gamma+\\epsilon_{i t}\n$$\n\nwhere $G_{i}$ is a group indicator, taking a value of 1 if individual $i$ is in the treatment group, and 0 otherwise, and $\\gamma_{1}$ and $\\gamma_{2}$ capture group and time fixed effects, respectively. We make the following assumption.\n\nAssumption D (Two-Way Fixed Effects). (i) All those (and only those) in the treatment group are treated in period 1 so that $D_{i t}=t \\cdot G_{i}$.\n(ii) The time and group assignment is exogenous in that $\\mathbb{E}\\left(\\epsilon_{i t} \\mid G_{i}, t=0\\right)=0$ and $\\mathbb{E}\\left(\\epsilon_{i t} \\mid\\right.$ $\\left.G_{i}, t=1\\right)=0$.\n\nUnder assumption D, $\\gamma_{0}, \\gamma_{1}, \\gamma_{2}$, and $\\gamma$ can be estimated by the ordinary least squares (OLS). In equation (8), it is usually the parameter $\\gamma$ that is of interest because it corresponds to the DID estimand, i.e., the numerator of $\\bar{\\theta}_{L}$ in (6), and that of $\\bar{\\theta}_{L}^{(r)}$ in (7). The following theorem shows that the persuasion rates can also be obtained from the regression in (8).\n\nTheorem 2. Suppose that $\\mathbb{E}\\left(Y_{i 1} \\mid D_{i 1}\\right)>0$ and $\\Psi<1$ so that both $\\bar{\\theta}_{L}$ and $\\bar{\\theta}_{L}^{(r)}$ are well-defined. If assumption D holds, then\n\n$$\n\\bar{\\theta}_{L}=\\frac{\\gamma}{1-\\gamma_{0}-\\gamma_{1}-\\gamma_{2}} \\quad \\text { and } \\quad \\bar{\\theta}_{L}^{(r)}=\\frac{\\gamma}{\\gamma_{0}+\\gamma_{1}+\\gamma_{2}+\\gamma}\n$$\n\nThe denominators in (9) reflect the fact that we need to adjust the DID estimand to estimate APRT and R-APRT. If we assume that we have a random sample $\\left\\{\\left(Y_{i 0}, Y_{i 1}, G_{i}\\right)\\right.$ : $i=1,2, \\cdots, n\\}$, inference based on theorem 2 can be done by combining the standard theory of OLS and the delta method. ${ }^{7}$\n\nTo gain further insights, we now present a seemingly different but actually equivalent approach. First, we define an auxiliary outcome variable for period 1 by $\\bar{Y}_{i 1}:=D_{i 1}+$\n\n[^0]\n[^0]:    ${ }^{7}$ If the denominators are close to zero, then the delta method may provide a poor approximation in a finite sample. Using an Anderson-Rubin type statistic is an alternative robust method for inference.\n\n$Y_{i 1}\\left(1-D_{i 1}\\right)$. That is, the auxiliary outcome $\\tilde{Y}_{i 1}$ is the same as the original outcome $Y_{i 1}$ if $D_{i 1}=0$, but $\\tilde{Y}_{i 1}=1$ if $D_{i 1}=1$. In words, it represents 'pseudo voters' who behave like the original voters when not exposed to the persuasive message, but will definitely vote for the party of interest when they do receive the persuasive message.\n\nFor either $A_{i}:=\\tilde{Y}_{i 1}-Y_{i 0}$ or $A_{i}:=Y_{i 1} D_{i 1}$, consider the following moment conditions:\n\n$$\n\\mathbb{E}\\left\\{\\left(Y_{i 1}-Y_{i 0}\\right)-\\beta_{0}-\\beta_{1} A_{i}\\right\\}=0 \\quad \\text { and } \\quad \\mathbb{E}\\left[D_{i 1}\\left\\{\\left(Y_{i 1}-Y_{i 0}\\right)-\\beta_{0}-\\beta_{1} A_{i}\\right\\}\\right]=0\n$$\n\nIn other words, $\\beta_{1}$ is a two-stage least squares (2SLS) regression coefficient of $Y_{i 1}-Y_{i 0}$ on $A_{i}$ with using $D_{i 1}$ as an instrumental variable.\n\nTheorem 3. Suppose that $\\mathbb{E}\\left(Y_{i 1} \\mid D_{i 1}=1\\right)>0$ and $\\Psi<1$ so that both $\\bar{\\theta}_{L}$ and $\\bar{\\theta}_{L}^{(r)}$ are welldefined. If $A_{i}=\\tilde{Y}_{i 1}-Y_{i 0}$ is used, then $\\beta_{1}=\\bar{\\theta}_{L}$, while using $A_{i}=Y_{i 1} D_{i 1}$ leads to $\\beta_{1}=\\bar{\\theta}_{L}^{(r)}$.\n\nTherefore, both $\\bar{\\theta}_{L}$ and $\\bar{\\theta}_{L}^{(r)}$ can be obtained by 2SLS regression. ${ }^{8}$ In order to understand the idea behind theorem 3, first focus on $\\bar{\\theta}_{L}$, i.e., APRT, where both the numerator and the denominator have a form of a DID estimand. Recall that a DID estimand can be obtained by running an OLS regression of first-differenced outcomes on $D_{i 1}$ in general. Specifically, the numerator of $\\bar{\\theta}_{L}$ in (6) can be estimated by running an OLS of $Y_{i 1}-Y_{i 0}$ on $D_{i 1}$ in general. Furthermore, the denominator of $\\bar{\\theta}_{L}$ also has a DID form by using $\\tilde{Y}_{i 1}$ in lieu of $Y_{i 1}$. Therefore, we know from those facts that\n\n$$\n\\bar{\\theta}_{L}=\\frac{\\operatorname{Cov}\\left(Y_{i 1}-Y_{i 0}, D_{i 1}\\right) / \\mathbb{V}\\left(D_{i 1}\\right)}{\\operatorname{Cov}\\left(\\tilde{Y}_{i 1}-Y_{i 0}, D_{i 1}\\right) / \\mathbb{V}\\left(D_{i 1}\\right)}=\\frac{\\operatorname{Cov}\\left(Y_{i 1}-Y_{i 0}, D_{i 1}\\right)}{\\operatorname{Cov}\\left(\\tilde{Y}_{i 1}-Y_{i 0}, D_{i 1}\\right)}\n$$\n\nThe case of R-APRT is similar. That is, the numerator of $\\bar{\\theta}_{L}^{(r)}$ is the same as that of $\\bar{\\theta}_{L}$, and its denominator is $\\mathbb{E}\\left(Y_{i 1} \\mid D_{i 1}=1\\right)=\\operatorname{Cov}\\left(Y_{i 1} D_{i 1}, D_{i 1}\\right) / \\mathbb{V}\\left(D_{i 1}\\right)$, where we use the fact that $D_{i 1}$ is binary, and hence $D_{i 1}^{2}=D_{i 1}$.\n\nInference based on theorem 3 can be done within the framework of the generalized method of moments (GMM): we can even stack all the moment equations to obtain both $\\overline{\\overline{8}}$ If the denominators in equation (9) are close to zero, we will have a weak instrument problem in this formulation. We do not explore this direction though.\n\nAPRT and R-APRT by a single GMM procedure. We remark that the two-way fixed effect and GMM estimators are all algebraically equivalent.\n\nWhen we have covariates, it is an easy option to simply \"partial them out\" by adding the covariates, potentially including their powers and interactions, to the regression. However, the algebraic equivalence between the fixed effect and GMM estimators does not hold in that case. Furthermore, it can induce contamination bias, resulting in misleading conclusions in severe cases, and therefore, it requires great caution: see e.g., Blandhol et al. (2022) and Goldsmith-Pinkham et al. (2022) among others. We can address the covariate issue more properly by e.g., estimating equation (8) locally around $X_{i}=x$ first, after which we deal with averaging over $X_{i}$ to obtain APRT or R-APRT. It is then moving toward semiparametric estimation that uses first-step estimators conditional on $X_{i}$. We will discuss several options for semiparametric estimators in the following subsection.\n4.2. Semiparametric Approaches. We will now be explicit about covariates to focus on semiparametric options. For brevity and clarity, we first provide a detailed description for the more complicated parameter APRT, and then we present a short summary for R-APRT at the end of this subsection. We can construct several semiparametric estimators of $\\hat{\\theta}_{L}$ by using the first-step estimators of $\\Pi_{t}(d, x)$ or those of $P(x):=\\mathbb{P}\\left(D_{1}=1 \\mid X=x\\right) .{ }^{9}$\n\nFirst, by using the definition of $\\Psi(X)$, we rewrite $\\hat{\\theta}_{L}$ as\n\n$$\n\\hat{\\theta}_{L}=\\frac{\\mathbb{E}\\left[\\{\\Delta(1, X)-\\Delta(0, X)\\} D_{1}\\right]}{\\mathbb{E}\\left[\\{\\Delta(1, X)-\\Delta(0, X)\\} D_{1}\\right]+\\mathbb{E}\\left[\\left\\{1-\\Pi_{1}(1, X)\\right\\} D_{1}\\right]}\n$$\n\nwhere $\\Delta(d, x):=\\Pi_{1}(d, x)-\\Pi_{0}(d, x)$. Therefore, if a random sample $\\left\\{\\left(Y_{i 0}, Y_{i 1}, D_{i 1}, X_{i}\\right)\\right.$ : $i=1, \\cdots n\\}$ is given, then equation (11) suggests the following DID-based estimator\n\n$$\n\\hat{\\theta}_{L, D I D}:=\\frac{\\sum_{i=1}^{n}\\left\\{\\widehat{\\Delta}\\left(1, X_{i}\\right)-\\widehat{\\Delta}\\left(0, X_{i}\\right)\\right\\} D_{i 1}}{\\sum_{i=1}^{n}\\left\\{\\widehat{\\Delta}\\left(1, X_{i}\\right)-\\widehat{\\Delta}\\left(0, X_{i}\\right)\\right\\} D_{i 1}}+\\sum_{i=1}^{n}\\left\\{1-\\widehat{\\Pi}_{1}\\left(1, X_{i}\\right)\\right\\} D_{i 1}}\n$$\n\n[^0]\n[^0]:    ${ }^{9}$ To minimize misspecification, the first-step estimators are typically nonparametric, though they may also be parametric. We do not limit ourselves to a specific type. Strictly speaking, the parametric case should be referred to as two-step parametric estimation, but for simplicity, we use the term semiparametric estimation throughout our discussion, though it may be a misnomer.\n\nwhere $\\bar{\\Delta}\\left(d, X_{i}\\right):=\\widehat{\\Pi}_{1}\\left(d, X_{i}\\right)-\\widehat{\\Pi}_{0}\\left(d, X_{i}\\right)$ with $\\widehat{\\Pi}_{t}(d, x)$ being the first-step estimator of $\\Pi_{t}(d, x)$ of the researcher's choice.\n\nThere are alternative estimators. For instance, we can equivalently express $\\bar{\\theta}_{L}$ as\n\n$$\n\\bar{\\theta}_{L}=\\frac{\\mathbb{E}\\left\\{D_{1}\\left(Y_{1}-Y_{0}\\right)\\right\\}-\\mathbb{E}\\left\\{D_{1} \\Delta(0, X)\\right\\}}{\\mathbb{E}\\left\\{D_{1}\\left(1-Y_{0}\\right)\\right\\}-\\mathbb{E}\\left\\{D_{1} \\Delta(0, X)\\right\\}}\n$$\n\nand therefore, a direct plug-in estimator based on (12) can be obtained by\n\n$$\n\\hat{\\theta}_{L, P I}:=\\frac{\\sum_{i=1}^{n}\\left(Y_{i 1}-Y_{i 0}\\right) D_{i 1}-\\sum_{i=1}^{n} \\bar{\\Delta}\\left(0, X_{i}\\right) D_{i 1}}{\\sum_{i=1}^{n}\\left(1-Y_{i 0}\\right) D_{i 1}-\\sum_{i=1}^{n} \\bar{\\Delta}\\left(0, X_{i}\\right) D_{i 1}}\n$$\n\nwhich will numerically coincide with $\\hat{\\theta}_{L, D I D}$ if there are no covariates $X_{i}{ }^{10}$ Further, applying the law of iterated expectations and Bayes' rule to equation (12), $\\bar{\\theta}_{L}$ can be expressed in another form as follows:\n\n$$\n\\bar{\\theta}_{L}=\\frac{\\mathcal{N}}{\\mathcal{N}+\\mathbb{E}\\left\\{D_{1}\\left(1-Y_{1}\\right)\\right\\}}\n$$\n\nwhere\n\n$$\n\\mathcal{N}:=\\mathbb{E}\\left\\{D_{1}\\left(Y_{1}-Y_{0}\\right)-\\left(1-D_{1}\\right)\\left(Y_{1}-Y_{0}\\right) \\frac{P(X)}{1-P(X)}\\right\\}\n$$\n\nTherefore, we can obtain yet another estimator from equation (14), i.e.,\n\n$$\n\\hat{\\theta}_{L, P O W}:=\\frac{\\widehat{\\mathcal{N}}}{\\widehat{\\mathcal{N}}+n^{-1} \\sum_{i=1}^{n}\\left(1-Y_{i 1}\\right) D_{i 1}}\n$$\n\nwhere\n\n$$\n\\widehat{\\mathcal{N}}:=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i 1}-Y_{i 0}\\right) D_{i 1}-\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-D_{i 1}\\right)\\left(Y_{i 1}-Y_{i 0}\\right) \\frac{\\widehat{P}\\left(X_{i}\\right)}{1-\\widehat{P}\\left(X_{i}\\right)}\n$$\n\nwith $\\widehat{P}(X)$ being the first-step estimator of the propensity score $P(X):=\\mathbb{P}\\left(D_{1}=1 \\mid X\\right)$ of the researcher's choice. The estimator $\\hat{\\theta}_{L, P O W}$, which we will call a propensity-oddsweighted (POW) estimator, requires only one first-step estimator. Indeed, it is worth noting that $\\hat{\\theta}_{L, P O W}$ requires estimating only $P(X)$ in the first step, while $\\hat{\\theta}_{L, P I}$ needs to use 10 In fact, if there are no covariates, an IV estimator based on equation (10) will be identical as well.\n\n$\\Delta(0, X)=\\Pi_{1}(0, X)-\\Pi_{0}(0, X)$. This difference will be discussed again in the following section, where we will propose an additional estimator that is doubly (and locally) robust.\n\nAnalogously, we obtain the following estimators for R-APRT:\n\n$$\n\\begin{aligned}\n\\hat{\\theta}_{L, D I D}^{(r)} & :=\\frac{\\sum_{i=1}^{n}\\left\\{\\widehat{\\Delta}\\left(1, X_{i}\\right)-\\widehat{\\Delta}\\left(0, X_{i}\\right)\\right\\} D_{i 1}}{\\sum_{i=1}^{n} \\widehat{\\Pi}_{1}\\left(1, X_{i}\\right) D_{i 1}} \\\\\n\\hat{\\theta}_{L, P I}^{(r)} & :=\\frac{\\sum_{i=1}^{n}\\left(Y_{i 1}-Y_{i 0}\\right) D_{i 1}-\\sum_{i=1}^{n} \\widehat{\\Delta}\\left(0, X_{i}\\right) D_{i 1}}{\\sum_{i=1}^{n} Y_{i 1} D_{i 1}} \\\\\n\\hat{\\theta}_{L, P O W}^{(r)} & :=\\frac{\\widehat{\\mathcal{N}}}{n^{-1} \\sum_{i=1}^{n} Y_{i 1} D_{i 1}}\n\\end{aligned}\n$$\n\nThe suggested estimators use equivalent expressions of the same moment conditions, and therefore, they will have the same efficient influence function for each parameter: i.e., they are all asymptotically equivalent under suitable regularity conditions. In the following section, we will explicitly calculate the efficient influence function for two purposes. First, it will show us the asymptotic variance of an asymptotically linear and regular semiparametric estimator of $\\hat{\\theta}_{L}$ and that of $\\hat{\\theta}_{L}^{(r)}$ without specifying all regularity conditions. Second, our derivation of the efficient influence function will be useful to find locally and doubly robust estimators.", "tables": {}, "images": {}}, {"section_id": 6, "text": "# 5. The Efficient Influence Function \n\nAs it is more complicated to derive the efficient influence function for APRT, we first focus on $\\hat{\\theta}_{L}$, and we will briefly discuss the case of R-APRT at the end of this section. All the aforementioned semiparametric estimators will be asymptotically linear, regular, and normal under suitable regularity conditions: the specific conditions will depend on the choice of the first-step estimators. Since the theory of semiparametric estimation is well established (e.g., Ackerberg et al., 2014), we will not elaborate all regularity conditions to obtain asymptotic normality. Instead, we follow the approach of Newey (1994): i.e., we calculate the semiparametrically efficient influence function for $\\hat{\\theta}_{L}$, of which the variance will be the asymptotic variance of a regular and asymptotically linear estimator of $\\hat{\\theta}_{L}$. For this purpose, we work under the assumption of random sampling of $\\left(Y_{1}, Y_{0}, D_{1}, X^{\\top}\\right)^{\\top}$.\n\nWe start with making a regularity assumption.\nAssumption E. There exists a constant $\\epsilon>0$ such that for all $d, y_{0}, y_{1} \\in\\{0,1\\}$ and for all $x \\in \\mathcal{X}, \\epsilon \\leq \\mathbb{P}\\left(Y_{0}=y_{0}, Y_{1}=y_{1}, D_{1}=d \\mid X=x\\right) \\leq 1-\\epsilon$.\n\nAssumption E is to ensure that the likelihood and scores are all well-behaved. Below we calculate the efficient influence function for $\\tilde{\\theta}_{L}$, which we present in a couple of equivalent forms. The two forms reflect whether we estimate $P(X)$ or $\\Pi_{t}(d, X)$ 's in the first step, and they will lead us to a doubly robust estimator.\n\nLet $\\mathcal{D}:=\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$, and let $\\theta_{L, d e n}:=\\mathbb{E}\\left[\\{1-\\Psi(X)\\} D_{1}\\right]$. For EST $\\in\\{P O W, P I\\}$, define\n\n$$\n\\begin{aligned}\nF_{E S T, \\text { main }}(\\mathcal{D}) & :=\\frac{1}{\\theta_{L, \\text { den }}}\\left\\{H_{E S T, n u m}(\\mathcal{D})-\\theta_{L} H_{E S T, \\text { den }}(\\mathcal{D})\\right\\} \\\\\nF_{E S T, a d j}(\\mathcal{D}) & :=\\frac{\\left(1-\\theta_{L}\\right)}{\\theta_{L, d e n}} H_{E S T, a d j}(\\mathcal{D})\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\nH_{P O W, n u m}(\\mathcal{D}) & :=D_{1}\\left(Y_{1}-Y_{0}\\right)-\\frac{P(X)}{1-P(X)}\\left(1-D_{1}\\right)\\left(Y_{1}-Y_{0}\\right) \\\\\nH_{P O W, d e n}(\\mathcal{D}) & :=D_{1}\\left(1-Y_{0}\\right)-\\frac{P(X)}{1-P(X)}\\left(1-D_{1}\\right)\\left(Y_{1}-Y_{0}\\right) \\\\\nH_{P O W, a d j}(\\mathcal{D}) & :=-\\left\\{D_{1}-\\frac{P(X)}{1-P(X)}\\left(1-D_{1}\\right)\\right\\} \\Delta(0, X) \\\\\nH_{P I, n u m}(\\mathcal{D}) & :=D_{1}\\left\\{\\left(Y_{1}-Y_{0}\\right)-\\Delta(0, X)\\right\\} \\\\\nH_{P I, d e n}(\\mathcal{D}) & :=D_{1}\\left\\{\\left(1-Y_{0}\\right)-\\Delta(0, X)\\right\\} \\\\\nH_{P I, a d j}(\\mathcal{D}) & :=-\\frac{P(X)}{1-P(X)}\\left(1-D_{1}\\right)\\left\\{\\left(Y_{1}-Y_{0}\\right)-\\Delta(0, X)\\right\\}\n\\end{aligned}\n$$\n\nWe are now ready to state the main theorem of this section.\n\nTheorem 4. Suppose that assumption $E$ is satisfied. Then, the semiparametrically efficient influence function for $\\tilde{\\theta}_{L}$ under the random sampling of $\\left(Y_{1}, Y_{0}, D_{1}, X^{\\top}\\right)^{\\top}$ is given by\n\n$$\nF_{D I D}(\\mathcal{D}):=F_{P O W, \\text { main }}(\\mathcal{D})+F_{P O W, a d j}(\\mathcal{D})\n$$\n\nwhich can be equivalently written as\n\n$$\nF_{D I D}(\\mathcal{D})=F_{P I, \\text { main }}(\\mathcal{D})+F_{P I, \\text { adj }}(\\mathcal{D})\n$$\n\nIn particular, if a semiparametric estimator $\\hat{\\theta}_{L}$ of $\\bar{\\theta}_{L}$ that uses a random sample of size $n$ is regular and asymptotically linear, then we must have\n\n$$\n\\sqrt{n}\\left(\\hat{\\theta}_{L}-\\bar{\\theta}_{L}\\right) \\xrightarrow{d} N\\left(0, \\mathbb{E}\\left\\{F_{D I D}^{2}(\\mathcal{D})\\right\\}\\right)\n$$\n\nTheorem 4 presents the efficient influence function $F_{D I D}$ in two forms. Although equivalence of the two expressions can be easily verified by simple algebra, it is worth discussing the ideas behind them. Recall that $\\hat{\\theta}_{L, P O W}$ and $\\hat{\\theta}_{L, P I}$ are alternative estimators of $\\bar{\\theta}_{L}$, where the former uses $P(X)$, while the latter does $\\Delta(0, X)=\\Pi_{1}(0, X)-\\Pi_{0}(0, X)$. In fact, $\\hat{\\theta}_{L, P O W}$ is based on the moment condition\n\n$$\n\\mathbb{E}\\left\\{H_{P O W, \\text { num }}(\\mathcal{D})-\\hat{\\theta}_{L} H_{P O W, \\text { den }}(\\mathcal{D})\\right\\}=0\n$$\n\nwhich corresponds to the leading term $F_{P O W, \\text { main }}(\\mathcal{D})$ in the expression in (17). Then, $F_{P O W, a d j}(\\mathcal{D})$ accounts for the effect of nonparametric estimation of $P(X)$. Similarly, the moment condition $\\mathbb{E}\\left\\{F_{P I, \\text { main }}(\\mathcal{D})\\right\\}=0$ leads to the estimator $\\hat{\\theta}_{L, P I}$, and the adjustment term $F_{P I, a d j}(\\mathcal{D})$ reflects that $\\Delta(0, X)=\\Pi_{1}(0, X)-\\Pi_{0}(0, X)$ is estimated in the first step.\n\nThe asymptotic variance in (19) is a consequence of Theorem 2.1 in Newey (1994): because the set of scores is sufficiently rich, all regular and asymptotically linear estimators of $\\hat{\\theta}_{L}$ must have the same efficient influence function $F_{D I D}$. An implication is that $\\hat{\\theta}_{L, D I D}$, $\\hat{\\theta}_{L, P I}$, and $\\hat{\\theta}_{L, P O W}$ will be asymptotically equivalent, as long as they are regular and asymptotically linear.\n\nThe efficient influence function formula is also useful to find a doubly robust estimator. Instead of using only the leading term in either (17) or (18), we may use the entire $F_{D I D}(\\mathcal{D})$ to find an estimator of $\\bar{\\theta}_{L}$. Indeed, since we trivially have\n\n$$\nH_{P O W, R}(\\mathcal{D})+H_{P O W, a d j}(\\mathcal{D})=H_{P I, R}(\\mathcal{D})+H_{P I, a d j}(\\mathcal{D}) \\text { for } R \\in\\{\\text { num, den }\\}\n$$\n\nusing the moment condition $\\mathbb{E}\\left\\{F_{D I D}(\\mathcal{D}\\right\\}=0$, regardless of whether we use (17) or (18), leads to the same estimator, i.e.,\n\n$$\n\\hat{\\theta}_{L, D R}:=\\frac{\\sum_{i=1}^{n}\\left(Y_{i 1}-Y_{i 0}\\right) D_{i 1}-\\sum_{i=1}^{n} \\widehat{\\Delta}\\left(0, X_{i}\\right) D_{i 1}+\\sum_{i=1}^{n} \\widehat{H}_{P I, a d j}\\left(\\mathcal{D}_{i}\\right)}{\\sum_{i=1}^{n}\\left(1-Y_{i 0}\\right) D_{i 1}-\\sum_{i=1}^{n} \\widehat{\\Delta}\\left(0, X_{i}\\right) D_{i 1}+\\sum_{i=1}^{n} \\widehat{H}_{P I, a d j}\\left(\\mathcal{D}_{i}\\right)}\n$$\n\nwhere\n\n$$\n\\widehat{H}_{P I, a d j}\\left(\\mathcal{D}_{i}\\right):=-\\frac{\\widehat{P}\\left(X_{i}\\right)}{1-\\widehat{P}\\left(X_{i}\\right)}\\left(1-D_{i 1}\\right)\\left\\{\\left(Y_{i 1}-Y_{i 0}\\right)-\\widehat{\\Delta}\\left(0, X_{i}\\right)\\right\\}\n$$\n\nBy construction, the estimator $\\hat{\\theta}_{L, D R}$ is both locally and doubly robust. Specifically, since it is using the entire efficient influence function, the effect of first-step nonparametric estimation is already reflected in the form of the estimator: the asymptotic variance of $\\hat{\\theta}_{L, D R}$ will be the same as its oracle form that uses the true $\\Delta(0, X)$ and $P(X)$. This local robustness suggests that we can develop a double/debiased machine learning (DML) estimator in a similar way to $\\hat{\\theta}_{L, D R}$ but by using machine learning estimators in the first step and cross-fitting in the second step. Further, in view of equation (20), we can see that $\\hat{\\theta}_{L, D R}$ is doubly robust in that it will be consistent as long as either $\\widehat{\\Delta}(0, \\cdot)$ or $\\widehat{P}(\\cdot)$, but not necessarily both, is correctly specified: i.e., $\\mathbb{E}\\left\\{H_{P I, a d j}(\\mathcal{D})\\right\\}=0$ holds even if $P(X)$ is replaced with an arbitrary function of $X$, while $\\mathbb{E}\\left\\{H_{P O W, a d j}(\\mathcal{D})\\right\\}=0$ stays true whatever function of $X$ is used in lieu of $\\Delta(0, X)$.\n\nWe end this section by discussing the case of R-APRT. The only difference between APRT and R-APRT is in their denominators. Indeed, R-APRT has a simpler denominator, i.e., $\\theta_{L, \\text { den }}^{(r)}:=\\mathbb{E}\\left(Y_{1} D_{1}\\right)$, and it can be shown that under the same condition as theorem 4 , the efficient influence function for $\\theta_{L}^{(r)}$ is given by\n\n$$\nF_{D I D}^{(r)}(\\mathcal{D}):=\\frac{1}{\\theta_{L, \\text { den }}^{(r)}}\\left\\{H_{E S T, n u m}(\\mathcal{D})-\\theta_{L}^{(r)} Y_{1} D_{1}+H_{E S T, a d j}(\\mathcal{D})\\right\\}\n$$\n\nwhere $E S T \\in\\{P O W, P I\\}$. Therefore, the same reasoning as before shows that a (locally and) doubly robust estimator of $\\theta_{L}^{(r)}$ can be obtained by\n\n$$\n\\hat{\\theta}_{L, D R}^{(r)}:=\\frac{\\sum_{i=1}^{n}\\left(Y_{i 1}-Y_{i 0}\\right) D_{i 1}-\\sum_{i=1}^{n} \\widehat{\\Delta}\\left(0, X_{i}\\right) D_{i 1}+\\sum_{i=1}^{n} \\widehat{H}_{P I, a d j}\\left(\\mathcal{D}_{i}\\right)}{\\sum_{i=1}^{n} Y_{i 1} D_{i 1}}\n$$", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 6. DISCUSSION \n\nIn this section, we discuss how to conduct back-of-the-envelope inference on APRT and R-APRT when we do not have access to the full data, but we have information for ATT. For the sake of simple discussion, we again focus on the two time-period case and assume that ATT is weakly positive (as implied by assumption B). APRT is linked to ATT via $\\operatorname{APRT}(q)=\\operatorname{ATT} /(\\operatorname{ATT}+q)$, where $q:=\\mathbb{P}\\left(Y_{1}=0 \\mid D_{1}=1\\right)$. Because $\\operatorname{ATT}+q=$ $\\mathbb{P}\\left\\{Y_{1}(0)=0 \\mid D_{1}=1\\right\\}$, APRT is strictly larger than ATT unless $\\operatorname{ATT}=0$ or $\\mathbb{P}\\left\\{Y_{1}(0)=0 \\mid\\right.$ $\\left.D_{1}=1\\right\\}=1$, which means that every individual in the treated group is a target audience to persuade. In other words, APRT will be strictly larger than ATT in many interesting cases.\n\nSuppose that there is a known interval $[\\underline{q}, \\bar{q}] \\subseteq[0,1]$ such that it contains $q$ with probability tending to $1-\\alpha_{0}$ for some constant $0 \\leq \\alpha_{0}<1$. As $q \\mapsto \\operatorname{APRT}(q)$ is nonincreasing in $q$ (under the assumption that $\\operatorname{ATT} \\geq 0$ ), the resulting bounds on APRT conditional on $q \\in[\\underline{q}, \\bar{q}]$ are\n\n$$\n[L(\\text { APRT }), U(\\text { APRT })]:=\\left[\\frac{\\text { ATT }}{\\text { ATT }+\\bar{q}}, \\frac{\\text { ATT }}{\\text { ATT }+\\underline{q}}\\right]\n$$\n\nwhich is our basis for the back-of-the-envelope inference on APRT from ATT.\nTo be more specific, suppose that an estimate $\\widehat{\\text { ATT}}$ of ATT, and its standard error, say $s e(\\widehat{\\text { ATT}})$, are available so that the asymptotic $(1-\\alpha)$ confidence interval for ATT is obtained by $\\left[\\widehat{\\operatorname{ATT}}-z_{1-\\alpha / 2} \\cdot s e(\\widehat{\\operatorname{ATT}}), \\widehat{\\operatorname{ATT}}+z_{1-\\alpha / 2} \\cdot s e(\\widehat{\\operatorname{ATT}})\\right]$, where $z_{\\tau}$ is the $\\tau$ quantile of the standard normal distribution. Then, by the delta method, the pointwise standard error of $\\widehat{\\operatorname{APRT}}(q):=\\widehat{\\operatorname{ATT}} /(\\widehat{\\operatorname{ATT}}+q)$ is $s e(\\widehat{\\operatorname{ATT}}) q /\\left(\\widehat{\\operatorname{ATT}}+q\\right)^{2}$. Then, in view of the interval identification in (22), a $(1-\\alpha)$ Bonferroni confidence interval for APRT can be obtained by $[\\underline{\\text { APRT }}, \\overline{\\text { APRT }}]$, where\n\n$$\n\\begin{aligned}\n& \\widehat{\\operatorname{APRT}}:=\\widehat{\\operatorname{APRT}}(\\bar{q})-z_{1-\\left(\\alpha-\\alpha_{0}\\right) / 2} \\cdot \\frac{s e(\\widehat{\\operatorname{ATT}}) \\bar{q}}{(\\widehat{\\operatorname{ATT}}+\\bar{q})^{2}} \\\\\n& \\widehat{\\operatorname{APRT}}:=\\widehat{\\operatorname{APRT}}(\\underline{q})+z_{1-\\left(\\alpha-\\alpha_{0}\\right) / 2} \\cdot \\frac{s e(\\widehat{\\operatorname{ATT}}) q}{(\\widehat{\\operatorname{ATT}}+\\underline{q})^{2}}\n\\end{aligned}\n$$\n\nbecause\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\{\\operatorname{APRT} \\in[\\underline{\\text { APRT }}, \\overrightarrow{\\operatorname{APRT}}]\\} \\geq \\mathbb{P}\\{q \\in[\\underline{q}, \\bar{q}] \\text {, APRT } \\in[\\underline{\\text { APRT }}, \\overrightarrow{\\operatorname{APRT}}]\\} \\\\\n& \\geq 1-\\mathbb{P}(q \\notin[\\underline{q}, \\bar{q}])-\\mathbb{P}(\\operatorname{APRT} \\notin[\\underline{\\text { APRT }}, \\overrightarrow{\\operatorname{APRT}}])=1-\\alpha_{0}-\\frac{\\alpha-\\alpha_{0}}{2}-\\frac{\\alpha-\\alpha_{0}}{2}=1-\\alpha .\n\\end{aligned}\n$$\n\nFor example, we may set $\\alpha_{0}=\\alpha / 2$, resulting in $\\left(\\alpha-\\alpha_{0}\\right) / 2=\\alpha / 4$. In short, when we have access to the estimate of ATT with its standard error, we can conduct back-of-the-envelope inference on APRT, provided that we have probabilistic bounds on $\\mathbb{P}\\left(Y_{1}=0 \\mid D_{1}=1\\right)$.\n\nWe now move to R-APRT. Using the fact that R-APRT $=\\mathrm{ATT} /(1-q)$, we propose the following back-of-the-envelope confidence interval for R-APRT:\n\n$$\n\\widehat{\\operatorname{R}-\\operatorname{APRT}}(\\underline{q})-z_{1-\\left(\\alpha-\\alpha_{0}\\right) / 2} \\cdot \\frac{\\operatorname{se}(\\widehat{\\mathrm{ATT}})}{(1-\\underline{q})}, \\widehat{\\mathrm{R}-\\operatorname{APRT}}(\\bar{q})-z_{1-\\left(\\alpha-\\alpha_{0}\\right) / 2} \\cdot \\frac{\\operatorname{se}(\\widehat{\\mathrm{ATT}})}{(1-\\bar{q})}\n$$", "tables": {}, "images": {}}, {"section_id": 8, "text": "# 7. Example I: News Media Persuasion \n\nLadd and Lenz (2009) exploited abrupt shifts in British newspaper endorsements from the Conservative party to the Labour party before the 1997 general election. Using data from the British Election Panel Study, they compared readers of newspapers that switched endorsements (treated group) with those who did not read these newspapers (control group). The binary outcome variable is whether a respondent voted Labour in the 1997 election. The binary treatment variable is whether a respondent read the switching newspapers. This is an example of the two-period case with no treatment in the first period. In addition to the lagged outcome (prior Labour vote in 1992), there are a large number of pretreatment variables $(X)$ all measured in 1992. The dataset is on the public domain as part of replication materials for Hainmueller (2012), which are available in the Political Analysis Dataverse at http://dvn.iq.harvard.edu/dvn/dv/pan. We use the same set of covariates as in Hainmueller (2012) but exclude the three prior voting variables (prior Labour vote, prior Conservative vote, prior Liberal vote), resulting in 36 predetermined covariates. They include prior party identification, prior party support, prior ideology,\n\nparents vote, political knowledge, television viewership, daily newspaper readership, authoritarianism, trade union membership, mortgage payment status, education, income, age, race, socioeconomic status, gender, region, and occupation.\n\nTable 1 reports a variety of estimates of the average persuasion rates on the treated. Panel A reports the forward version, i.e., APRT and panel B the reverse version, i.e., RAPRT. In the columns of FE and GMM, the two-way fixed effect (FE) and GMM estimators are given without controlling for the covariates. They are algebraically equivalent but the t-statistics are slightly different because they rely on different asymptotic approximations. The covariates are separately partialled out in the columns of FE-X and GMM-X. Each of the two-step estimates, which are given in the columns of DID, PI, POW, and DR, involves the first step estimation of (some but not all of) five conditional probabilities: $\\mathbb{P}\\left(Y_{t}=1\\right.$ $\\left.D_{1}=d, X=x\\right)$, where $t=0,1$ and $d=0,1$, as well as $\\mathbb{P}\\left(D_{1}=1 \\mid X=x\\right)$. They are all estimated via logistic regression that is linear in $X$.\n\nTable 1. Average Persuasion Rates on the Treated: Media Persuasion\n\n![table_0](table_0)\n\nTo compare our estimates with other measures reported in the literature, we note that our ATT estimate using the doubly robust method is just 0.089 , which is very similar to the unconditional DID estimate of 0.086 (Ladd and Lenz, 2009, see the first column of their Table 2). The previous ATT estimates using matching (Ladd and Lenz, 2009) or entropy balancing (Hainmueller, 2012) are slightly larger, ranging from 0.096 to 0.140 . Our\n\nestimates of APRT and R-APRT are larger than any of the ATT estimates, indicating that media persuasion was even more substantial than the ATT estimates suggest. Using the decomposition method described in our discussion below equation (2) and in the one that follows assumption B, we can divide the population of the treated by three groups: the share of the treatment-persuadable is 0.089 as it is just ATT; that of the never-persuadable is 0.417 , and that of the already-persuaded is 0.494 . The shares of the latter two groups are much larger, explaining why ATT is substantially different from APRT and R-APRT.\n\nWe end this section by illustrating back-of-the-envelope inference. Strictly speaking, we do not have access to the replication files of the original analysis in Ladd and Lenz (2009) because we rely on the replication materials from Hainmueller (2012). Table 2 in Ladd and Lenz (2009) provides a variety of treatment effect estimates. For example, the third column shows the DID estimate using only exactly matched treated/control groups, where exact matching is implemented on selected variables. The resulting estimate is 0.109 with its standard error of 0.041 . We can interpret this as the ATT estimate. In order to conduct inference on APRT using their ATT estimate, we need bounds on $q=\\mathbb{P}\\left(Y_{1}=0 \\mid D_{1}=1\\right)$. Its estimated value, 0.583 , and the size, 211 , of the treated group are given on page 399 in their paper. To be conservative, we use the $97.5 \\%$ confidence interval on $q$, that is $[q, \\bar{q}]=$ $[0.507,0.659]$. Then, the $95 \\%$ confidence interval on APRT is $[0.039,0.300]$, while the point estimate is 0.158 using $q=0.583$. If we carry out the same exercise for R-APRT, we obtain the back-of-the-envelope confidence interval of $[0.035,0.589]$ with the point estimate of R-APRT being 0.261 .", "tables": {"table_0": "| Panel A: APRT |  |  |  |  |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n|  |  | Regression-Based |  |  |  | Two-Step |  |  |\n| method | FE | GMM | FE-X | GMM-X | DID | PI | POW | DR |\n| equation | (9) | (10) | (9) | (10) | (11) | (12) | (14) | (17) |\n| covariates | none |  | partial out |  | two-step |  | estimation |  |\n| estimate | 0.172 | 0.172 | 0.170 | 0.169 | 0.177 | 0.172 | 0.184 | 0.176 |\n| t-statistic | 2.798 | 2.800 | 2.384 | 2.777 | 2.687 | 2.609 | 2.792 | 2.667 |\n| Panel B: R-APRT |  |  |  |  |  |  |  |  |\n| estimate | 0.148 | 0.148 | 0.150 | 0.149 | 0.149 | 0.149 | 0.162 | 0.153 |\n| t-statistic | 2.737 | 2.739 | 2.522 | 2.761 | 2.651 | 2.645 | 2.872 | 2.716 |"}, "images": {}}, {"section_id": 9, "text": "# 8. Staggered Treatment \n\nAs we extend our setup to allow for multiple time periods, we focus on the leading case of staggered treatments, where the observational units receive treatment at different times and the treatment stays on once it is adopted. Furthermore, to avoid repetition, we limit our attention to the more complicated parameter, i.e., APRT, noting that the modifications required for R-APRT are easily adaptable.\n\n8.1. The Setup and the Parameters of Interest. Let $t \\in\\{0,1,2, \\cdots, T\\}$ denote the time period, and let $D_{t}$ denote the treatment status at time $t$. In our setup, no one is treated at time 0 , and if the agent receives a treatment at time $s \\in\\{1,2, \\cdots, T\\}$, then we observe $D_{t}=0$ for all $0 \\leq t<s$ and $D_{t}=1$ for all $s \\leq t \\leq T$. If the agents are never treated so that $D_{t}=0$ for all $t \\in\\{0,1,2, \\cdots, T\\}$, then they belong to the control group. Exogenous covariates are denoted by $X$ as before.\n\nLet $\\mathcal{S}:=\\left\\{t \\in\\{1, \\cdots, T\\}: D_{t}=1\\right\\}$, and define the new random variable $S$ by $S:=\\min \\mathcal{S}$ if $\\mathcal{S}$ is non-empty, and $S:=\\infty$ otherwise. Therefore, $S$ has the support $\\{1,2, \\cdots, T, \\infty\\}$, indicating the time period at which the treatment is given: i.e., $S=\\infty$ means that the individuals are never treated.\n\nNow, in order to capture treatment effect heterogeneity, we write $Y_{t}(s)$ for the potential outcome at time $t$ for the case where the treatment is given at time $s$. We now make the following assumption to formally describe our setting.\n\nAssumption F. At time 0 , no one receives or anticipates receiving a treatment so that $Y_{0}:=$ $Y_{0}(\\infty)$ is observed. At time $t=1, \\cdots, T, Y_{t}:=Y_{t}(S)$ is observed, and there is no anticipation for a treatment in that we have $\\mathbb{P}\\left\\{Y_{t}(s)=1 \\mid S=s, X\\right\\}=\\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=s, X\\right\\}$ whenever $t<s$ with probability one. Further, for all $s, t \\in\\{1, \\cdots, T\\}$ and for (almost) all $x \\in \\mathcal{X}$, there is a constant $\\epsilon>0$ such that $\\epsilon \\leq \\mathbb{P}\\left\\{Y_{t}(\\infty)=1, S=s \\mid X=x\\right\\}$ and $\\mathbb{P}(S=s \\mid X=x) \\leq 1-\\epsilon$.\n\nThe most straightforward parameters we can start with are the persuasion rate at time $t$ on those who are treated at time $s$ conditional on $X=x$, and its aggregation across $X$ :\n\n$$\n\\begin{aligned}\n\\theta_{c}(s, t \\mid x) & :=\\mathbb{P}\\left\\{Y_{t}(s)=1 \\mid Y_{t}(\\infty)=0, S=s, X=x\\right\\} \\\\\n\\theta(s, t) & :=\\mathbb{P}\\left\\{Y_{t}(s)=1 \\mid Y_{t}(\\infty)=0, S=s\\right\\}\n\\end{aligned}\n$$\n\nInterpretation is not any different from that of the persuasion rate on the treated in the case of two periods. For example, $\\theta(s, t)$ measures the proportion of those who take the action of interest at time $t$ (e.g. voting for a certain party) among those who received a persuasive message at time $s$ and who would not have taken the action without the\n\npersuasive message at all. Again, the point is that we focus only on those who switch their action because of the persuasive message.\n\nAs a measure of the cumulative effect of persuasion, we consider the $j$-period-forward persuasion rate on the treated, i.e. the persuasion rate in $j$ periods after the time of the treatment on those who are ever treated (by $T-j$ ). Following the literature on the panel event-study design, we call it the event-study persuasion rate (ESPR), and it can be formally expressed by\n\n$$\n\\theta_{E S P R}(j):=\\mathbb{P}\\left\\{Y_{S+j}(S)=1 \\mid Y_{S+j}(\\infty)=0, S \\leq T-j\\right\\} \\text { for } j=0,1, \\cdots, T-1\n$$\n\nThis is probably the most comprehensive summary parameter, and its identification, estimation, and inference will be discussed in detail in the following subsections. ${ }^{11}$\n8.2. Identification. All the parameters defined above depend on the joint probabilities of the potential outcomes. As in the case of two periods, a monotonicity assumption is useful to handle the situation.\n\nAssumption G. For all $s, t \\in\\{1, \\cdots, T\\}, \\mathbb{P}\\left\\{Y_{t}(s) \\geq Y_{t}(\\infty) \\mid S=s, X\\right\\}=1$ almost surely.\nAssumption G is an extension of assumption B to the case of staggered treatment: i.e., the persuasive message is directional, and there is no backlash regardless of the timing of the exposure, and therefore the monotonicity condition is satisfied.\n\nAs before, all the parameters will be identified if\n\n$$\n\\tau_{\\text {stagger }}(s, t \\mid x):=\\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=s, X=x\\right\\} \\quad \\text { with } t \\geq s\n$$\n\nis identified for all $x \\in \\mathcal{X}$. Identification of $\\tau_{\\text {stagger }}(s, t \\mid x)$ can be achieved via the DID approach. For this purpose, we modify assumption C as follows.\n\n[^0]\n[^0]:    ${ }^{11}$ Alternatively, $\\theta(s, t)$ could be aggregated in different ways; for instance, a new summary parameter could be constructed as a function of $t$.\n\nAssumption H. For all $s, t \\in\\{0,1,2, \\cdots, T\\}$ and for (almost) all $x \\in \\mathcal{X}, \\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=\\right.$ $s, X=x\\}$ is separable into the sum of a time component and a treatment component: i.e., there exist functions $G^{*}$ and $H^{*}$ such that $\\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=s, X=x\\right\\}=G^{*}(t, x)+H^{*}(s, x)$.\n\nAssumption H is an extended version of the parallel trend assumption. Specifically, assumption H ensures that for any $s, t \\in\\{1,2, \\cdots, T\\}$ with $t \\geq s$, we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=s, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{s-1}(\\infty)=1 \\mid S=s, X=x\\right\\} \\\\\n& \\quad=\\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=\\infty, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{s-1}(\\infty)=1 \\mid S=\\infty, X=x\\right\\}\n\\end{aligned}\n$$\n\nLet\n\n$$\n\\begin{aligned}\n\\Psi_{\\text {stagger }}(s, t \\mid x):=\\mathbb{P}\\left(Y_{s-1}\\right. & =1 \\mid S=s, X=x) \\\\\n& +\\mathbb{P}\\left(Y_{t}=1 \\mid S=\\infty, X=x\\right)-\\mathbb{P}\\left(Y_{s-1}=1 \\mid S=\\infty, X=x\\right)\n\\end{aligned}\n$$\n\nwhich is directly identified from panel data. We have the following identification results.\nTheorem 5. Suppose that assumptions F to H are satisfied. Then, $\\theta_{c}(s, t \\mid x), \\theta(s, t)$, and $\\theta_{E S P R}(j)$ are point-identified by\n\n$$\n\\begin{aligned}\n& \\theta_{c}(s, t \\mid x)=\\frac{\\mathbb{P}\\left(Y_{t}=1 \\mid S=s, X=x\\right)-\\Psi_{\\text {stagger }}(s, t \\mid x)}{1-\\Psi_{\\text {stagger }}(s, t \\mid x)} \\\\\n& \\theta(s, t)=\\frac{\\mathbb{P}\\left(Y_{t}=1 \\mid S=s\\right)-\\mathbb{E}\\left\\{\\Psi_{\\text {stagger }}(s, t \\mid X) \\mid S=s\\right\\}}{1-\\mathbb{E}\\left\\{\\Psi_{\\text {stagger }}(s, t \\mid X) \\mid S=s\\right\\}} \\\\\n& \\theta_{E S P R}(j)=\\frac{\\sum_{s=1}^{T-j} \\mathbb{P}(S=s)\\left[\\mathbb{P}\\left(Y_{s+j}=1 \\mid S=s\\right)-\\mathbb{E}\\left\\{\\Psi_{\\text {stagger }}(s, s+j \\mid X) \\mid S=s\\right\\}\\right]}{\\sum_{s=1}^{T-j} \\mathbb{P}(S=s)\\left[1-\\mathbb{E}\\left\\{\\Psi_{\\text {stagger }}(s, s+j \\mid x) \\mid S=s\\right\\}\\right]}\n\\end{aligned}\n$$\n\nTheorem 5 follows from the intermediate identification result that under assumption H , $\\tau_{\\text {stagger }}(s, t \\mid x)=\\Psi_{\\text {stagger }}(s, t \\mid x)$. The fact that the numerator and the denominator of equation (24) are separately aggregated to obtain equation (25) is a consequence of the Bayes rule. Further, it is worth noting that ESPR is obtained by aggregating over $s$ on the numerator and on the denominator separately, which is reminiscent of the aggregation procedure for $X$. Since the persuasion rate is a conditional probability, it takes the form of\n\na ratio, but when it comes to aggregation (over $X$ or $s$ ), it is always collapsed to separate aggregations of the numerator and denominator.\n8.3. Estimation. All the estimators we discussed in the case of two periods can be extended to the current setup of staggered treatment. Below we discuss both regressionbased and semiparametric approaches, focusing on $\\theta_{E S P R}(j)$ as a comprehensive summary parameter.\n8.3.1. Regression-Based Approaches. We assume that we have no covariates for now. Theorem 5 implies that\n\n$$\n\\theta(s, s+j)=\\frac{\\theta_{\\text {num }}(s, s+j)}{\\theta_{\\text {den }}(s, s+j)} \\quad \\text { and } \\quad \\theta_{\\text {ESPR }}(j)=\\frac{\\sum_{s=1}^{T-j} \\theta_{\\text {num }}(s, s+j) \\mathbb{P}(S=s)}{\\sum_{s=1}^{T-j} \\theta_{\\text {den }}(s, s+j) \\mathbb{P}(S=s)}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\theta_{\\text {num }}(s, s+j) & :=\\mathbb{E}\\left(Y_{s+j} \\mid S=s\\right)-\\mathbb{E}\\left(Y_{s-1} \\mid S=s\\right)-\\mathbb{E}\\left(Y_{s+j} \\mid S=\\infty\\right)+\\mathbb{E}\\left(Y_{s-1} \\mid S=\\infty\\right) \\\\\n\\theta_{\\text {den }}(s, s+j) & :=1-\\mathbb{E}\\left(Y_{s-1} \\mid S=s\\right)-\\mathbb{E}\\left(Y_{s+j} \\mid S=\\infty\\right)+\\mathbb{E}\\left(Y_{s-1} \\mid S=\\infty\\right)\n\\end{aligned}\n$$\n\nBelow we use linear regression to estimate $\\theta(s, s+j)$, which has a similar structure to the persuasion rate in the two-period case. For instance, the numerator $\\theta_{\\text {num }}(s, s+j)$ is a DID estimand, where $S=s$ represents the treatment group, and $S=\\infty$ the control group. Therefore, we can extend the ideas in section 4.1 to the current setup in a pairwise manner.\n\nTo be more specific, suppose that we have a random sample $\\left\\{\\left(Y_{i 0}, Y_{i 1}, \\cdots, Y_{i T}, S_{i}\\right)\\right.$ : $i=1, \\cdots, n\\}$. For $\\theta(s, s+j)$, we use only the subset of the data $\\left\\{\\left(Y_{i s-1}, Y_{s+j}, S_{i}\\right): S_{i}=\\right.$ $s$ or $\\left.S_{i}=\\infty\\right\\}$, and we estimate the following regression by OLS:\n\n$$\nY_{i t}=\\gamma_{0}+\\mathbb{1}\\left(S_{i}=s\\right) \\gamma_{1}+\\mathbb{1}(t=s+j) \\gamma_{2}+\\mathbb{1}\\left(S_{i}=s\\right) \\mathbb{1}(t=s+j) \\gamma+\\epsilon_{i t}\n$$\n\nwhere we assume that $\\mathbb{E}\\left(\\epsilon_{i t} \\mid S_{i}=d, t=r\\right)=0$ for $d \\in\\{s, \\infty\\}$ and $r \\in\\{s-1, s+j\\}$. The extra conditioning of $S_{i} \\in\\{s, \\infty\\}$ is to ensure that $S_{i} \\neq s$ corresponds to $S_{i}=\\infty$ and vice versa. ${ }^{12}$ It is in the same spirit to restrict ourselves to $t \\in\\{s-1, s+j\\}$.\n\nEquation (27) can be interpreted like the usual two-way fixed effect regression: $\\gamma_{1}$ captures the group effect, $\\gamma_{2}$ does the time effect, and $\\gamma$ is the DID estimand that corresponds to the numerator $\\theta_{\\text {num }}(s, s+j)$. The denominator $\\theta_{\\text {den }}(s, s+j)$ can be obtained from the rest of the coefficients as we did in theorem 2.\n\nTheorem 6. Consider the regression of (27). If we restrict ourselves to the data with $S_{i} \\in\\{s, \\infty\\}$ and $t \\in\\{s-1, s+j\\}$, then we have\n\n$$\n\\theta_{\\text {num }}(s, s+j)=\\gamma \\quad \\text { and } \\quad \\theta_{\\text {den }}(s, s+j)=1-\\gamma_{0}-\\gamma_{1}-\\gamma_{2}\n$$\n\nTheorem 6 suggests the following sample analog estimators:\n\n$$\n\\hat{\\theta}(s, s+j):=\\frac{\\hat{\\theta}_{\\text {num }}(s, s+j)}{\\hat{\\theta}_{\\text {den }}(s, s+j)} \\quad \\text { and } \\quad \\hat{\\theta}_{E S P R}(j):=\\frac{\\sum_{s=1}^{T-j} \\hat{\\theta}_{\\text {num }}(s, s+j) \\widehat{\\mathbb{P}}(S=s)}{\\sum_{s=1}^{T-j} \\hat{\\theta}_{\\text {den }}(s, s+j) \\widehat{\\mathbb{P}}(S=s)}\n$$\n\nwhere $\\hat{\\theta}_{\\text {num }}(s, s+j):=\\hat{\\gamma}, \\hat{\\theta}_{\\text {den }}(s, s+j):=1-\\hat{\\gamma}_{0}-\\hat{\\gamma}_{1}-\\hat{\\gamma}_{2},\\left(\\hat{\\gamma}_{0}, \\hat{\\gamma}_{1}, \\hat{\\gamma}_{2}, \\hat{\\gamma}\\right)$ are the OLS estimators from (27), and $\\widehat{\\mathbb{P}}(S=s)$ is the sample analog of $\\mathbb{P}(S=s){ }^{13}$\n\nInstead of running the regression of (27) for each pair of $(s-1, s+j)$, one may run the following regression model that resembles the panel event-study design regression more closely: for each $s \\in S$, using only the subset of the data $\\left\\{\\left(Y_{i t}, S_{i}\\right): S_{i}=s, t=0, \\ldots, T\\right\\}$, we run the following regression:\n\n$$\ny_{i t}=\\mu_{-1}^{(s)}+\\sum_{j \\neq-1} \\alpha_{j}^{(s)} \\mathbb{1}(t=s+j)+\\epsilon_{i t}\n$$\n\n[^0]\n[^0]:    $\\overline{12}$ It is useful to recall that for any events $E_{1}, E_{2}, E_{3}, \\mathbb{P}\\left(E_{1} \\mid E_{2}\\right)=\\mathbb{P}\\left(E_{1} \\mid E_{2}, E_{2} \\cup E_{3}\\right)$, provided that $\\mathbb{P}\\left(E_{2}\\right)>$ 0 .\n    ${ }^{13}$ The OLS estimators $\\left(\\hat{\\gamma}_{0}, \\hat{\\gamma}_{1}, \\hat{\\gamma}_{2}, \\hat{\\gamma}\\right)$ should be indexed by $(s, s+j)$, but its dependence is suppressed to minimize the notional burden.\n\nThen, the estimators of $\\theta(s, s+j)$ and $\\hat{\\theta}_{E S P R}(j)$ are now given by\n\n$$\n\\hat{\\theta}(s, s+j)=\\frac{\\hat{\\alpha}_{j}^{(s)}-\\hat{\\alpha}_{j}^{(\\infty)}}{1-\\hat{\\mu}_{-1}^{(s)}-\\hat{\\alpha}_{j}^{(\\infty)}} \\quad \\text { and } \\quad \\hat{\\theta}_{E S P R}(j)=\\frac{\\sum_{s=1}^{T-j}\\left\\{\\hat{\\alpha}_{j}^{(s)}-\\hat{\\alpha}_{j}^{(\\infty)}\\right\\} \\widehat{\\mathbb{P}}(S=s)}{\\sum_{s=1}^{T-j}\\left\\{1-\\hat{\\mu}_{-1}^{(s)}-\\hat{\\alpha}_{j}^{(\\infty)}\\right\\} \\widehat{\\mathbb{P}}(S=s)}\n$$\n\nwhere a hat on the right-hand side of the equations above denotes the OLS estimator. The estimators defined in equations (28) and (30) look seemingly different; however, they are identical because we run fully saturated regressions in two different ways. Finally, we comment that statistical inference is straightforward. Since the OLS-based estimators are all asymptotically linear, $\\hat{\\theta}(s, s+j)$ and $\\hat{\\theta}_{E S P R}(j)$ are asymptotically linear and normal by the delta method.\n8.3.2. Semiparametric Approaches. We now consider the covariates and describe how to implement semiparametric estimation. First, let $\\tilde{S}_{s}=\\mathbb{1}(S=s)+\\mathbb{1}(S=\\infty)$ for each $s \\in\\{1,2, \\cdots, T-j\\}$. Using the fact in footnote 12, we can then equivalently express $\\theta(s, s+j)$ as follows:\n\n$$\n\\theta(s, s+j)=\\frac{\\mathcal{N}_{\\text {stagger }}\\left(s, s+j \\mid \\tilde{S}_{s}=1\\right)}{\\mathcal{N}_{\\text {stagger }}\\left(s, s+j \\mid \\tilde{S}_{s}=1\\right)+\\mathbb{P}\\left(Y_{s+j}=0, S=s \\mid \\tilde{S}_{s}=1\\right)}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\mathcal{N}_{\\text {stagger }}\\left(s, s+j \\mid \\tilde{S}_{s}=1\\right): & =\\mathbb{P}\\left(Y_{s+j}=1, S=s \\mid \\tilde{S}_{s}=1\\right)-\\mathbb{P}\\left(Y_{s-1}=1, S=s \\mid \\tilde{S}_{s}=1\\right) \\\\\n& -\\mathbb{E}\\left\\{\\left(Y_{s+j}-Y_{s-1}\\right) \\mathbb{1}(S=\\infty) \\frac{\\mathbb{P}\\left(S=s \\mid X, \\tilde{S}_{s}=1\\right)}{1-\\mathbb{P}\\left(S=s \\mid X, \\tilde{S}_{s}=1\\right)} \\mid \\tilde{S}_{s}=1\\right\\}\n\\end{aligned}\n$$\n\nThese expressions conveniently show that our discussions on estimation and inference in the case of two periods can be applied to the current case of multiple periods.\n\nFor example, we can obtain a doubly robust estimator of $\\theta(s, s+j)$ based on a random sample $\\left\\{\\left(Y_{i 0}, Y_{i 1}, \\cdots, Y_{i T}, S_{i}, X_{i}^{\\top}\\right)^{\\top}: i=1,2, \\cdots, n\\right\\}$ by using the same formula for the two-period DR estimator $\\hat{\\theta}_{L, D R}$ defined in equation (21) but using a subsample of $\\left(Y_{i s-1}, Y_{i s+j}, \\mathbb{1}\\left(S_{i}=s\\right), X_{i}^{\\top}\\right)^{\\top}$ that satisfies $\\mathbb{1}\\left(S_{i}=s\\right.$ or $\\left.S_{i}=\\infty\\right)=1$. Since $\\mathbb{1}\\left(S_{i}=s\\right) \\mathbb{1}\\left(\\tilde{S}_{s, i}=\\right.$\n\n1) $=\\mathbb{1}\\left(S_{i}=s\\right)$, we obtain the following formula:\n\n$$\n\\hat{\\theta}_{D R}(s, s+j):=\\frac{\\sum_{i=1}^{n}\\left\\{\\left(Y_{i s+j}-Y_{i s-1}-\\widehat{\\Delta}_{s, s+j}\\left(\\infty, X_{i}\\right)\\right\\} \\mathbb{1}\\left(S_{i}=s\\right)-\\widehat{C}(s, s+j)}{\\sum_{i=1}^{n}\\left\\{\\left(1-Y_{i s-1}-\\widehat{\\Delta}_{s, s+j}\\left(\\infty, X_{i}\\right)\\right\\} \\mathbb{1}\\left(S_{i}=s\\right)-\\widehat{C}(s, s+j)}\\right.\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\widehat{\\Delta}_{s, s+j}\\left(\\infty, X_{i}\\right) & :=\\widehat{\\mathbb{P}}\\left(Y_{i s+j}=1 \\mid S_{i}=\\infty, X_{i}\\right)-\\widehat{\\mathbb{P}}\\left(Y_{i s-1}=1 \\mid S_{i}=\\infty, X_{i}\\right) \\\\\n\\widehat{C}(s, s+j) & :=\\sum_{i=1}^{n} \\frac{\\widehat{\\mathbb{P}}\\left(S_{i}=s \\mid X_{i}, \\widehat{S}_{s, i}=1\\right) \\mathbb{1}\\left(S_{i}=\\infty\\right)}{1-\\widehat{\\mathbb{P}}\\left(S_{i}=s \\mid X_{i}, \\widehat{S}_{s, i}=1\\right)}\\left\\{\\left(Y_{i s+j}-Y_{i s-1}\\right)-\\widehat{\\Delta}_{s, s+j}\\left(\\infty, X_{i}\\right)\\right\\}\n\\end{aligned}\n$$\n\nAggregating over $s$ in $\\hat{\\theta}_{D R}(s, s+j)$ can be similarly done. For example, a DR estimator of $\\theta_{E S P R}(j)$ can be obtained by\n$\\hat{\\theta}_{E S P R, D R}(j):=\\frac{\\sum_{s=1}^{T-j} \\widehat{\\mathbb{P}}(S=s)\\left[\\sum_{i=1}^{n}\\left\\{\\left(Y_{i s+j}-Y_{i s-1}-\\widehat{\\Delta}_{s, s+j}\\left(\\infty, X_{i}\\right)\\right\\} \\mathbb{1}\\left(S_{i}=s\\right)-\\widehat{C}(s, s+j)\\right]}{\\sum_{s=1}^{T-j} \\widehat{\\mathbb{P}}(S=s)\\left[\\sum_{i=1}^{n}\\left\\{\\left(1-Y_{i s-1}-\\widehat{\\Delta}_{s, s+j}\\left(\\infty, X_{i}\\right)\\right\\} \\mathbb{1}\\left(S_{i}=s\\right)-\\widehat{C}(s, s+j)\\right]}$.\nWe discuss inference for $\\hat{\\theta}_{E S P R, D R}(j)$ in Online Appendix S-6.", "tables": {}, "images": {}}, {"section_id": 10, "text": "# 9. Empirical Example II: Curriculum Persuasion \n\nCantoni et al. (2017) investigated the impact of a textbook reform in China by exploiting a staggered introduction of the new curriculum across provinces. Specifically, they conducted a survey of the political attitudes and beliefs with Peking University undergraduate students of four cohorts of students, who entered high school between 2006 and 2009. They adopted a difference-in-differences framework, and their baseline regression model (Cantoni et al., 2017, see equation (1) on p.361) is of the following form: using their notation,\n\n$$\ny_{i c p}=\\sum_{c} \\gamma_{c}+\\sum_{p} \\delta_{p}+\\beta \\text { New Curriculum }_{c p}+\\varepsilon_{i c p}\n$$\n\nwhere $i$ denotes the individual, $c$ the high school entry cohort, and $p$ the province of high school attendance; $y_{i c p}$ is an outcome variable, New Curriculum ${ }_{c p}$ is the indicator variable\n\nthat has value 1 if cohort $c$ in province $p$ studied under the new curriculum and 0 otherwise, and $\\gamma_{c}$ and $\\delta_{p}$ are cohort and province fixed effects; and $\\beta$ is the parameter of interest, aiming to uncover the effect of the new curriculum.\n\nBased on the regression estimates of (32), they also reported the persuasion rates (Cantoni et al., 2017, see column (7) of Table 3). To describe their estimates of the persuasion rates, let $\\widehat{\\beta}, \\widehat{\\gamma}_{c}$ and $\\widehat{\\delta}_{p}$, respectively, denote the estimates of $\\beta, \\gamma_{c}$, and $\\delta_{p}$ in the two-way fixed effect regression of (32), where if $y_{i c p}$ is not binary, the outcome variable is transformed to a binary variable that equals one if the outcome is greater than or equal to the median answer. Then, the persuasion rate reported in Cantoni et al. (2017) can be written as\n\n$$\n\\widehat{\\theta}_{\\mathrm{CCYYZ}}:=\\frac{\\widehat{\\beta}}{1-\\left(\\sum_{c} \\widehat{\\gamma}_{c}+\\sum_{p} \\widehat{\\delta}_{p}\\right)}\n$$\n\nwhere the numerator $\\widehat{\\beta}$ aims to measure the impacts of the curriculum reform, while the denominator is a proxy of \"the share of students without the desired attitude among individuals who studied under the old curriculum\" using the words of Cantoni et al. (2017, p. 382).\n\nIf there were only two cohorts and two provinces, $\\widehat{\\theta}_{\\mathrm{CCYYZ}}$ would be the same as our twoperiod regression-based estimator of APRT developed in section 4.1. In light of this, we do not consider R-APRT but focus on APRT. However, their setting involves a staggered introduction of the new curriculum with more than two cohorts and in more than two provinces. In addition, in view of recent advances in the literature on staggered treatment (e.g. De Chaisemartin and d'Haultfoeuille, 2020; Callaway and Sant'Anna, 2021), which postdated Cantoni et al. (2017), note that $\\widehat{\\beta}$ (the numerator of $\\widehat{\\theta}_{\\mathrm{CCYYZ}}$ ) might be a biased estimate of ATT. Hence, we revisit their analysis. Their sample is not from panel data but from a survey with multiple cohorts. However, a simple modification of the regressionbased approach in section 8.3.1 provides a method for estimating the persuasion rates in this example. In their sample, there are four high school entry cohorts: 2006, 2007, 2008, and 2009. As we need one comparison year before treatment, the earliest treatment group\n\n$(S=1$ using our notation) is a subset of cohort 2007 who attended high school in the provinces that introduced the new curriculum in 2007. The control group ( $S=\\infty$ using our notation) consists of those who attended high school in the provinces that introduced the new curriculum in 2010, which is the last year when the textbook reform was complete across China. Table 2 summarizes the staggered adoption of treatment in this example.\n\nTable 2. Staggered Introduction of the New Curriculum\n\n![table_1](table_1)\n\n${ }^{a}$ The provinces that introduced the new curriculum in 2007 are Beijing, Heilongjiang, Hunan, Jilin, and Shaanxi.\n${ }^{\\text {b }}$ The provinces that introduced the new curriculum in 2008 are Henan, Jiangxi, Shanxi, and Xinjiang.\n${ }^{c}$ The provinces that introduced the new curriculum in 2009 are Hebei, Hubei, Inner Mongolia, and Yunnan.\n${ }^{d}$ The provinces that introduced the new curriculum in 2010 are Chongqing, Gansu, Guangxi, Guizhou, Qinghai, Sichuan, and Tibet.\n${ }^{e}$ An entry in the table refers to the relevant high school entry year (i.e., $s+j$ ) for each $S=s$ group.\n${ }^{f}$ The provinces that adopted the new curriculum before 2007 are excluded in our analysis.\n\nWe now describe how to estimate $\\theta(s, s+j)$. As in (29), for each $s$, we run the following regression:\n\n$$\ny_{i c p}=\\mu_{-1}^{(s)}+\\sum_{j \\neq-1} \\alpha_{j}^{(s)} \\mathbb{1}(c=s+j)+e_{i c p}, p \\in P_{s}\n$$\n\nwhere $P_{s}$ is the subset of provinces that introduced the new curriculum in year $s$. Then, as in (30), the estimator of $\\theta(s, s+j)$, where $s \\in\\{2007,2008,2009\\}$, is given by\n\n$$\n\\widehat{\\theta}(s, s+j)=\\frac{\\widehat{\\alpha}_{j}^{(s)}-\\widehat{\\alpha}_{j}^{(2010)}}{1-\\widehat{\\mu}_{-1}^{(s)}-\\widehat{\\alpha}_{j}^{(2010)}}\n$$\n\nwhere a hat denotes an estimator of the corresponding parameter. When $j \\geq 0, \\widehat{\\theta}(s, s+j)$ measures the average persuasion rate on the treated; whereas $j<-1$, it can be used for checking pre-treatment difference-in-differences.\n\nFigure 1. Average Persuasion Rates on the Treated by Year of Treatment Adoption\n![img-0.jpeg](img-0.jpeg)\n\nNotes: The left and right panels of the figure show estimates of both the average treatment effect on the treated (ATT) and the average persuasion rates on the treated (APRT) for $S=2007$ and $S=2008$ groups, respectively. The vertical lines represent $95 \\%$ pointwise confidence intervals.\n\nAs an illustration, we consider the first outcome variable in Cantoni et al. (2017, the first row of Table 3), namely, \"Trust: central government\". The left panel of Figure 1 shows the estimates of both ATT and APRT for the $S=2007$ group. The $X$-axis shows different values of $s+j$, and by definition, 2006 corresponds to the null effect. The vertical lines represent $95 \\%$ pointwise confidence intervals using the robust standard errors. They are obtained via the delta method, while combining the $S=s$ and $S=2010$ regression models as a seemingly unrelated regression model with clustered dependence at the province level for each $S=s$ regression in (33). Not surprisingly, the APRT estimates are larger than the ATT estimates, as the former focuses on a more relevant subpopulation. It is remarkable to note that the 2007 ATT estimate is insignificant while the 2007 APRT estimate is significant, indicating that it is important to look at APRT as the key parameter. The right panel of Figure 1 shows the same estimates for the $S=2008$ group. In this group, the 2006 estimates correspond to the pre-treatment period and are close to zero and statistically insignificant. Overall, both panels indicate that the impacts of the textbook reform were substantial on students' trust on central government.\n\nWe do not report the estimates for the $S=2009$ group separately as they turn out to be imprecisely estimated and include only one post-treatment period. Instead, we now report the estimator of the event-study persuasion rates (ESPR), i.e., $\\theta_{E S P R}(j)$. If $j \\geq 0$, it\n\nFigure 2. Event-Study Persuasion Rates (ESPR)\n![img-1.jpeg](img-1.jpeg)\n\nNotes: Each of the figures shows estimates of the event-study persuasion rates (ESPR) for each horizon level. The vertical lines represent $95 \\%$ pointwise confidence intervals.\ncan be obtained by\n\n$$\n\\widehat{\\theta}_{E S P R}(j)=\\frac{\\sum_{s=2007}^{2009-j} \\widehat{\\mathbb{P}}(S=s)\\left\\{\\widehat{\\alpha}_{j}^{(s)}-\\widehat{\\alpha}_{j}^{(2010)}\\right\\}}{\\sum_{s=2007}^{2009-j} \\widehat{\\mathbb{P}}(S=s)\\left\\{1-\\widehat{\\mu}_{-1}^{(s)}-\\widehat{\\alpha}_{j}^{(2010)}\\right\\}}\n$$\n\nIn addition, we also report the following estimate as a check for the pre-treatment period:\n\n$$\n\\widehat{\\theta}_{E S P R}(-2)=\\frac{\\sum_{s=2008}^{2009} \\widehat{\\mathbb{P}}(S=s)\\left\\{\\widehat{\\alpha}_{-2}^{(s)}-\\widehat{\\alpha}_{-2}^{(2010)}\\right\\}}{\\sum_{s=2008}^{2009} \\widehat{\\mathbb{P}}(S=s)\\left\\{1-\\widehat{\\mu}_{-1}^{(s)}-\\widehat{\\alpha}_{-2}^{(2010)}\\right\\}}\n$$\n\nFigure 2 shows the ESPR estimates at each horizon level for the first 6 outcome variables from Cantoni et al. (2017, see Panel A in Table 3). The blue dashed line corresponds to the persuasion rate reported in Cantoni et al. (2017). It can be seen that our estimates are slightly larger, possibly and partly due to the fact that we allow for heterogeneous treatment effects, while carefully constructing control groups and dropping provinces that introduced the new curriculum before 2007. As before, pointwise $95 \\%$ confidence intervals are plotted based on the robust standard errors, clustered at the province level for each $S=$ $s$ regression model. On one hand, there is no significant effect in the pre-treatment period\n\nexcept for the outcome \"Trust: courts\". On the other hand, most of post-treatment period effects are significantly large. Overall, we observe that the main findings in Cantoni et al. (2017) are re-confirmed with our analysis, providing further evidence on their conclusion that \"studying the new curriculum led to more positive views of China's governance\".", "tables": {"table_1": "| Group $(S)$ by year | Event Horizon $(j)$ |  |  |  |  |  |  |  |\n| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| of a new curriculum | -4 | -3 | -2 | -1 | 0 | 1 | 2 |  |\n| 2007 |  |  |  | 2006 | 2007 | 2008 | 2009 |  |\n| 2008 |  |  | 2006 | 2007 | 2008 | 2009 |  |  |\n| 2009 |  | 2006 | 2007 | 2008 | 2009 |  |  |  |\n| $2010(S=\\infty)$ | 2006 | 2007 | 2008 | 2009 |  |  |  |  |"}, "images": {"img-0.jpeg": "data:image/jpeg;base64,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", "img-1.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 11, "text": "# 10. CONCLUSIONS \n\nWe have developed an econometric framework for learning the average persuasion rates on the treated using difference-in-differences, providing detailed discussions on identification, estimation, and inference. While our framework is designed for panel data, it can be extended to repeated cross-sectional data. However, the specific modifications required for such an extension have not been fully explored in this paper. Investigating these extensions is a promising direction for future research.", "tables": {}, "images": {}}, {"section_id": 12, "text": "## Appendix A. Proofs of the Results in the Main Text.\n\nProof of Lemma 1: We generally have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=1, X=x\\right\\} \\\\\n& \\quad=\\mathbb{P}\\left\\{Y_{1}(0)=0, Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{1}(0)=1, Y_{1}(1)=0 \\mid D_{1}=1, X=x\\right\\}\n\\end{aligned}\n$$\n\nHowever, $\\mathbb{P}\\left\\{Y_{1}(0)=1, Y_{1}(1)=0 \\mid D_{1}=1, X=x\\right\\}=0$ under assumption B. If assumption $B$ is violated, then the claim follows from the lower bound of Fr\u00e9chet-Hoeffding inequalities: see lemma 3 in Online Appendix S-1 for more detail.\n\nProofs of Lemma 2: Suppose that assumption B holds for all $x \\in \\mathcal{X}$. Then, $\\theta_{c}(x)=\\theta_{c L}(x)$ and $\\theta_{c}^{(r)}(x)=\\theta_{c L}^{(r)}(x)$ for all $x \\in \\mathcal{X}$ by lemma 1. Therefore, multiplying the density of $X$ at $x$ given $D_{1}=1, Y_{1}(0)=0$ to both sides of $\\theta_{c}(x)=\\theta_{c L}(x)$ and integrating with respect to $x$ shows $\\theta=\\theta_{L}$. The other case of $\\theta^{(r)}=\\theta_{L}^{(r)}$ is similar.\n\nProof of Theorem 1: It follows from assumption A and equation (5) and the fact that $Y_{1}(d)=Y_{1}$ when we condition on $D_{1}=d$.\n\nProof of Corollary 1: It follows from lemma 1 and theorem 1, and the Bayes rule.\n\nCorollary 2 in Online Appendix S-1 contains extended results on sharp identified bounds when assumption B can be violated.\n\nProof of Theorem 2: Under assumption D, the coefficients in equation (8) are identified by OLS, where $D_{i 1}=G_{i}$. Therefore, the denominator of $\\tilde{\\theta}_{L}$ is\n\n$$\n\\begin{aligned}\n& \\left\\{1-\\mathbb{E}\\left(Y_{i 0} \\mid D_{i 1}=1\\right)\\right\\}-\\left\\{\\mathbb{E}\\left(Y_{i 1} \\mid D_{i 1}=0\\right)-\\mathbb{E}\\left(Y_{i 0} \\mid D_{i 1}=0\\right)\\right\\} \\\\\n& =\\left\\{1-\\mathbb{E}\\left(Y_{i t} \\mid G_{i}=1, t=0\\right)\\right\\}-\\left\\{\\mathbb{E}\\left(Y_{i t} \\mid G_{i}=0, t=1\\right)-\\mathbb{E}\\left(Y_{i t} \\mid G_{i}=0, t=0\\right)\\right\\} \\\\\n& =1-\\left(\\gamma_{0}+\\gamma_{1}\\right)-\\left(\\gamma_{0}+\\gamma_{2}\\right)+\\gamma_{0}=1-\\gamma_{0}-\\gamma_{1}-\\gamma_{2}\n\\end{aligned}\n$$\n\nand that of $\\tilde{\\theta}_{L}^{(r)}$ is $\\mathbb{E}\\left(Y_{i 1} \\mid D_{i 1}=1\\right)=\\mathbb{E}\\left(Y_{i t} \\mid G_{i}=1, t=1\\right)=\\gamma_{0}+\\gamma_{1}+\\gamma_{2}+\\gamma$. The numerators of $\\theta_{L}$ and $\\theta_{L}^{(r)}$ are similar.\n\nProof of Theorem 3: Let $\\mathbb{E}\\left(D_{i 1}\\right)=q$, and consider the denominator of the middle expression in equation (10): i.e.,\n\n$$\n\\frac{\\operatorname{Cov}\\left(\\tilde{Y}_{i 1}-Y_{i 0}, D_{i 1}\\right)}{\\mathbb{V}\\left(D_{i 1}\\right)}=\\frac{\\mathbb{E}\\left(\\tilde{Y}_{i 1} D_{i 1}\\right)-\\mathbb{E}\\left(Y_{i 0} D_{i 1}\\right)-q\\left\\{\\mathbb{E}\\left(\\tilde{Y}_{i 1}\\right)-\\mathbb{E}\\left(Y_{i 0}\\right)\\right\\}}{q(1-q)}\n$$\n\nwhich is equal to\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left(\\tilde{Y}_{i 1} D_{i 1}\\right)-\\mathbb{E}\\left(Y_{i 0} D_{i 1}\\right)-q\\left[\\mathbb{E}\\left(\\tilde{Y}_{i 1} D_{i 1}\\right)+\\mathbb{E}\\left\\{\\tilde{Y}_{i 1}\\left(1-D_{i 1}\\right)\\right\\}-\\mathbb{E}\\left(Y_{i 0} D_{i 1}\\right)-\\mathbb{E}\\left\\{Y_{i 0}\\left(1-D_{i 1}\\right)\\right\\}\\right] \\\\\n& \\quad \\frac{q(1-q)}{q(1-q)} \\\\\n& =\\frac{(1-q)\\left\\{\\mathbb{E}\\left(\\tilde{Y}_{i 1} D_{i 1}\\right)-\\mathbb{E}\\left(Y_{i 0} D_{i 1}\\right)\\right\\}-q\\left[\\mathbb{E}\\left\\{\\tilde{Y}_{i 1}\\left(1-D_{i 1}\\right)\\right\\}-\\mathbb{E}\\left\\{Y_{i 0}\\left(1-D_{i 1}\\right)\\right\\}\\right]}{q(1-q)} \\\\\n& =\\left\\{\\mathbb{E}\\left(\\tilde{Y}_{i 1} \\mid D_{i 1}=1\\right)-\\mathbb{E}\\left(Y_{i 0} \\mid D_{i 1}=1\\right)\\right\\}-\\left\\{\\mathbb{E}\\left(\\tilde{Y}_{i 1} \\mid D_{i 1}=0\\right)-\\mathbb{E}\\left(Y_{i 0} \\mid D_{i 1}=0\\right)\\right\\} \\\\\n& =1-\\mathbb{E}\\left(Y_{i 0} \\mid D_{i 1}=1\\right)-\\mathbb{E}\\left(Y_{i 1} \\mid D_{i 1}=0\\right)+\\mathbb{E}\\left(Y_{i 0} \\mid D_{i 1}=0\\right)\n\\end{aligned}\n$$\n\nwhich is the denominator of $\\tilde{\\theta}_{L}$. The numerator of $\\tilde{\\theta}_{L}$ is similar. Finally, for the denominator of $\\tilde{\\theta}_{L}^{(r)}$, just note that $\\operatorname{Cov}\\left(Y_{i 1} D_{i 1}, D_{i 1}\\right)=\\mathbb{E}\\left(Y_{i 1} \\mid D_{i 1}=1\\right) \\mathbb{V}\\left(D_{i 1}\\right)$.\n\nProof of Theorem 4: Equivalence between the right-hand side of equation (17) and that of equation (18) is a simple algebraic result. The fact that $F_{D I D}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ is the semiparametrically efficient influence function follows from lemma 9 in Online Appendix S-5\n\nand the fact that $F_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ and $F_{\\text {den }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ are in the tangent space $\\mathcal{T}$ described in lemma 5 in Online Appendix S-4: see the expressions of $F_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ and $F_{\\text {den }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ given before the proofs of lemmas 6 and 7 in Online Appendix S-5. Finally, asymptotic normality in (19) follows from Theorem 2.1 of Newey (1994). Specifically, the scores are given in equation (42) in Online Appendix S-4, and they form a linear space. Also, any mean-zero function $s\\left(Y_{0}, Y_{1}, D_{1}, X\\right)=s_{000}(X)\\left(1-Y_{0}\\right)\\left(1-Y_{1}\\right)\\left(1-D_{1}\\right)+\\cdots+$ $s_{111}(X) Y_{0} Y_{1} D_{1}$ can be exactly matched with a score in the form of equation (42) in Online Appendix S-4. Therefore, an asymptotically linear estimator of $\\hat{\\theta}_{L}$ must have a unique influence function, which will coincide with $F_{D I D}$ by Theorem 2.1 of Newey (1994).\n\nProof of Theorem 5: It is useful to define a few immediate objects of interest. Let\n\n$$\n\\begin{aligned}\n\\theta_{c L}(s, t \\mid x) & :=\\frac{\\mathbb{P}\\left\\{Y_{t}(s)=1 \\mid S=s, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=s, X=x\\right\\}}{1-\\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=s, X=x\\right\\}} \\\\\n\\theta_{L}(s, t) & :=\\frac{\\mathbb{E}\\left[\\mathbb{P}\\left\\{Y_{t}(s)=1 \\mid S=s, X\\right\\}-\\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=s, X\\right\\} \\mid S=s\\right]}{1-\\mathbb{E}\\left[\\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=s, X\\right\\} \\mid S=s\\right]} \\\\\n\\theta_{E S P R, L}(j) & :=\\frac{\\sum_{s=1}^{T-j} \\theta_{L}(s, s+j) \\mathbb{P}(S=s)[1-\\mathbb{E}\\left\\{\\tau_{\\text {stagger }}(s, s+j \\mid X) \\mid S=s\\right\\}]}{\\sum_{s=1}^{T-j} \\mathbb{P}(S=s)[1-\\mathbb{E}\\left\\{\\tau_{\\text {stagger }}(s, s+j \\mid x) \\mid S=s\\right\\}]}\n\\end{aligned}\n$$\n\nAs in the proof of Lemma 1, we generally have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left\\{Y_{t}(s)=1 \\mid S=s, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{t}(\\infty)=1 \\mid S=s, X=x\\right\\} \\\\\n& \\quad=\\mathbb{P}\\left\\{Y_{t}(s)=1, Y_{t}(\\infty)=0 \\mid S=s, X=x\\right\\}-\\mathbb{P}\\left\\{Y_{t}(s)=0, Y_{t}(\\infty)=1 \\mid S=s, X=x\\right\\}\n\\end{aligned}\n$$\n\nHowever, under assumption $\\mathrm{G}, \\mathbb{P}\\left\\{Y_{t}(s)=0, Y_{t}(\\infty)=1 \\mid S=s, X=x\\right\\}=0$. Therefore, $\\theta_{c}(s, t \\mid x)=\\theta_{c L}(s, t \\mid x)$. Furthermore, we can verify that $\\theta(s, t)=\\theta_{L}(s, t)$ and $\\theta_{E S P R}(j)=\\theta_{E S P R, L}(j)$ using the fact that for generic random variables $A, B$, and an event $E$, we generally have $\\mathbb{E}(A \\mid B \\in E)=\\mathbb{E}\\{\\mathbb{E}(A \\mid B) \\mathbb{1}(B \\in E)\\} / \\mathbb{P}(B \\in E)$.\n\nNow the first identification result in equation (24) follows immediately from equation (23), because $Y_{t}(\\infty)=Y_{t}$ and $Y_{s-1}(\\infty)=Y_{s-1}$ when $S=\\infty$, and $\\mathbb{P}\\left\\{Y_{s-1}(\\infty)=1 \\mid S=s, X\\right\\}=$ $\\mathbb{P}\\left\\{Y_{s-1}(s)=1 \\mid S=s, X\\right\\}$ by assumption F that rules out anticipation. The second result in equation (25) follows from an aggregation of $\\theta_{c L}(s, s+j \\mid X)$ by using the conditional\n\ndistribution of $X$ given $Y_{s+j}(\\infty)=0$ and $S=s$. Finally, to obtain the third result, note that $\\theta_{E S P R, L}(j)$ is identified by\n\n$$\n\\theta_{E S P R, L}(j)=\\frac{\\sum_{s=1}^{T-j} \\theta_{L}(s, s+j) \\mathbb{P}(S=s)[1-\\mathbb{E}\\left\\{\\Psi_{\\text {stagger }}(s, s+j \\mid X) \\mid S=s\\right\\}]}{\\sum_{s=1}^{T-j} \\mathbb{P}(S=s)[1-\\mathbb{E}\\left\\{\\Psi_{\\text {stagger }}(s, s+j \\mid x) \\mid S=s\\right\\}]}\n$$\n\nCombining (34) with (25) yields equation (26).\nProof of Theorem 6: We are conditioning on $S_{i}=s$ or $S_{i}=\\infty$ in running this regression. However, since $\\mathbb{E}\\left(\\cdot \\mid S_{i}=r, S_{i}=s\\right.$ or $\\left.S_{i}=\\infty\\right)=\\mathbb{E}\\left(\\cdot \\mid S_{i}=r\\right)$ for $r \\in\\{s, \\infty\\}$ in general, the claim follows by the same reasoning as in the proof of theorem 2.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# REFERENCES \n\nAbadie, Alberto (2005): \"Semiparametric Difference-in-Differences Estimators,\" Review of Economic Studies, 72 (1), 1-19.\n\nAckerberg, D., X. Chen, J. Hahn, and Z. Liao (2014): \"Asymptotic efficiency of semiparametric two-step GMM,\" Review of Economic Studies, 81 (3), 919-943.\n\nArkhangelsky, Dmitry, Susan Athey, David A Hirshberg, Guido W Imbens, and Stefan WAGER (2021): \"Synthetic difference-in-differences,\" American Economic Review, 111 (12), 40884118 .\n\nAthey, Susan and Guido W Imbens (2006): \"Identification and inference in nonlinear difference-in-differences models,\" Econometrica, 74 (2), 431-497.\n-_ (2022): \"Design-based analysis in difference-in-differences settings with staggered adoption,\" Journal of Econometrics, 226 (1), 62-79.\n\nBAYES, THOMAS (1763): \"LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F.R.S. communicated by Mr. Price, in a letter to John Canton, A.M.F.R.S,\" Philosophical Transactions of the Royal Society, 53, 370-418.\n\nBlandhol, Christine, John Bonney, Magne Mogstad, and Alexander Torgovitsky (2022): \"When is TSLS Actually LATE?\" Working Paper 29709, National Bureau of Economic Research.\n\nBURSzTyn, LEONARDO AND DAVID Y. YanG (2022): \"Misperceptions About Others,\" Annual Review of Economics, 14, 425-452.\n\nCallaway, BrAntLY AND Pedro H.C. Sant'Anna (2021): \"Difference-in-Differences with Multiple Time Periods,\" Journal of Econometrics, 225 (2), 200-230.\n\nCantoni, Davide, Yuyu Chen, David Y Yang, Noam Yuchtman, and Y Jane Zhang (2017): \"Curriculum and ideology,\" Journal of Political Economy, 125 (2), 338-392.\n\nChernozhukov, Victor, Juan Carlos Escanciano, Hidehiko Ichimura, Whitney K. NeWEY, AND JAMES M. ROBINS (2022): \"Locally Robust Semiparametric Estimation,\" Econometrica, 90 (4), 1501-1535.\n\nDAWID, A. PHILIP, DAVID L. FAIGMAN, AND STEPHEN E. FienBERG (2014): \"Fitting Science Into Legal Contexts: Assessing Effects of Causes or Causes of Effects?\" Sociological Methods \\& Research, 43 (3), 359-390.\n\nDAWID, A. PHILIP AND MONICA MUSIO (2022): \"Effects of Causes and Causes of Effects,\" Annual Review of Statistics and Its Application, 9 (Volume 9, 2022), 261-287.\n\nDe ChaISEMARTin, Cl\u00e9ment and XAVIER D'HAUltFoeuille (2020): \"Two-way fixed effects estimators with heterogeneous treatment effects,\" American Economic Review, 110 (9), 2964-2996.\n\nDellaVigna, Stefano and Matthew Gentzkow (2010): \"Persuasion: empirical evidence,\" Annual Review of Economics, 2, 643-669.\n\nDellaVigna, Stefano and Ethan Kaplan (2007): \"The Fox News effect: Media bias and voting,\" Quarterly Journal of Economics, 122 (3), 1187-1234.\n\nDE CHAISEMARTIN, Cl\u00c9MENT AND XAVIER D'HAUltFGEUILLE (2022): \"Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey,\" The Econometrics Journal, 26 (3), C1-C30.\n\nDing, Peng and Fan Li (2019): \"A Bracketing Relationship between Difference-in-Differences and Lagged-Dependent-Variable Adjustment,\" Political Analysis, 27 (4), 605-615.\n\nFreyaldenhoven, Simon, Christian Hansen, Jorge P\u00e9rez P\u00e9rez, and Jesse M Shapiro (2021): \"Visualization, Identification, and Estimation in the Linear Panel Event-Study Design,\" Working Paper 29170, National Bureau of Economic Research.\n\nFreyaldenhoven, Simon, Christian Hansen, and Jesse M. Shapiro (2019): \"Pre-event Trends in the Panel Event-Study Design,\" American Economic Review, 109 (9), 3307-38.\n\nGoldSMith-Pinkham, Paul, Peter Hull, and Michal KoleS\u00c1r (2022): \"Contamination Bias in Linear Regressions,\" Working Paper 30108, National Bureau of Economic Research.\n\nGOODMAN-BACON, ANDREW (2021): \"Difference-in-differences with variation in treatment timing,\" Journal of Econometrics, 225 (2), 254-277.\n\nHAHN, JINYONG (1998): \"On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects,\" Econometrica, 66 (2), 315-331.\n\nHAINMUELLER, JENS (2012): \"Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies,\" Political Analysis, 20 (1), 25-46.\n\nHeckman, James J, Jeffrey Smith, and Nancy Clements (1997): \"Making the most out of programme evaluations and social experiments: Accounting for heterogeneity in programme impacts,\" Review of Economic Studies, 64 (4), 487-535.\n\nHirano, Keisuke, Guido W. Imbens, and Geert Ridder (2003): \"Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score,\" Econometrica, 71 (4), 1161-1189.\n\nImbens, Guido W and JoshuA D. Angrist (1994): \"Identification and estimation of local average treatment effects,\" Econometrica, 62 (2), 467-475.\n\nJi, Wenlong, Lihua Lei, and Asher Spector (2023): \"Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects,\" ArXiv:2310.08115 [econ.EM], available at https: //arxiv.org/abs/2310.08115.\n\nJUN, SUNG JaE AND SOKBAE LeE (2023): \"Identifying the effect of persuasion,\" Journal of Political Economy, 131 (8), 2032-2058.\n\nKaJi, Tetsuya and Jianfei CaO (2023): \"Assessing Heterogeneity of Treatment Effects,\" ArXiv:2306.15048 [econ.EM], available at https://arxiv.org/abs/2306.15048.\n\nLADD, JONATHAN McDONALD AND GABRIEL S. LENZ (2009): \"Exploiting a Rare Communication Shift to Document the Persuasive Power of the News Media,\" American Journal of Political Science, 53 (2), 394-410.\n\nMANSKI, CHARLES F. (1997): \"Monotone treatment response,\" Econometrica, 1311-1334.\nNeWEY, WhitNEY K (1994): \"The asymptotic variance of semiparametric estimators,\" Econometricay, 1349-1382.\n\nPEARL, JUDEA (1999): \"Probabilities of causation: three counterfactual interpretations and their identification,\" Synthese, 121 (1-2), 93-149.\n\nPOSSEbOM, VITOR AND Flavio Riva (2024): \"Probability of Causation with Sample Selection: A Reanalysis of the Impacts of J\u00f3venes en Acci\u00f3n on Formality,\" Journal of Business \\& Economic\n\nStatistics, in press, eprint available at https://arxiv.org/abs/2210.01938.\nRoth, Jonathan and Pedro HC Sant'Anna (2023): \"When is parallel trends sensitive to functional form?\" Econometrica, 91 (2), 737-747.\n\nRoth, Jonathan, Pedro HC Sant'Anna, Alyssa Bilinski, and John Poe (2023): \"What's trending in difference-in-differences? A synthesis of the recent econometrics literature,\" Journal of Econometrics, 235 (2), 2218-2244.\n\nSANT'ANNA, Pedro HC AND JUN ZhaO (2020): \"Doubly robust difference-in-differences estimators,\" Journal of Econometrics, 219 (1), 101-122.\n\nSUN, LIYANG AND SARAH AbRAHAM (2021): \"Estimating dynamic treatment effects in event studies with heterogeneous treatment effects,\" Journal of Econometrics, 225 (2), 175-199.\n\nSUN, LIYANG AND JESSE M. SHAPIRO (2022): \"A Linear Panel Model with Heterogeneous Coefficients and Variation in Exposure,\" Journal of Economic Perspectives, 36 (4), 193-204.\n\nWOOLDRIDGE, JEFFREY M (2023): \"Simple approaches to nonlinear difference-in-differences with panel data,\" The Econometrics Journal, 26 (3), C31-C66.\n\nXu, Yiqing, AnQi Zhao, and Peng Ding (2024): \"Factorial Difference-in-Differences,\" ArXiv:2407.11937 [stat.ME], available at https://arxiv.org/abs/2407.11937.\n\nYamamoto, Teppei (2012): \"Understanding the Past: Statistical Analysis of Causal Attribution,\" American Journal of Political Science, 56 (1), 237-256.\n\nYu, Arthur Zeyang (2023): \"A Binary IV Model for Persuasion: Profiling Persuasion Types among Compliers,\" Working Paper, available at https://arthurzeyangyu.github.io/jmp/yu_ 2023local.pdf.\n\nZhang, Chao, Zhi Geng, Wei Li, and Peng Ding (2024): \"Identifying and bounding the probability of necessity for causes of effects with ordinal outcomes,\" ArXiv:2411.01234 [math.ST], available at https://arxiv.org/abs/2411.01234.", "tables": {}, "images": {}}, {"section_id": 14, "text": "# Online Appendices to \"Learning the Effect of Persuasion via Difference-in-Differences\" \n\nSung Jae Jun<br>Sokbae Lee<br>Penn State University<br>Columbia University", "tables": {}, "images": {}}, {"section_id": 15, "text": "## APPENDix S-1. The Case of the Backlash\n\nWithout making assumption B, the Fr\u00e9chet-Hoeffding inequality yields bounds on $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x)$ in terms of the marginal probabilities of the potential outcomes conditional on $X=x$. Further, $\\theta_{c}(x)$ and $\\theta_{c}^{(r)}(x)$ are linearly dependent in that for all $x \\in \\mathcal{X}$, we have\n\n$$\n\\theta_{c}^{(r)}(x)=\\frac{\\left\\{1-\\tau_{c}(x)\\right\\} \\theta_{c}(x)}{\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\}}\n$$\n\nTherefore, assuming that $\\tau_{c}(x)$ is known, it suffices to have bounds on $\\theta_{c}(x)$ to have bounds on $\\theta_{c}^{(r)}(x)$.\n\nBelow we first derive bounds on $\\theta_{c}(x)$ without using assumption $B$ that depend only on the marginal probabilities of the potential outcomes given $D_{1}=1$ and $X=x$. Then, bounds on $\\theta_{c}^{(r)}(x)$ will follow from the bounds on $\\theta_{c}(x)$ and equation (35), or vice versa.\n\nDefine\n\n$$\n\\begin{aligned}\n\\mathcal{B}(x) & :=\\left\\{(p, q) \\in \\mathbb{R}^{2}: \\max \\left\\{0, \\theta_{c L}(x)\\right\\} \\leq p \\leq \\min \\left\\{\\theta_{c U}(x), 1\\right\\}, q=\\alpha(x) p\\right\\} \\\\\n& =\\left\\{(p, q) \\in \\mathbb{R}^{2}: p=q / \\alpha(x), \\max \\left\\{0, \\theta_{c L}^{(r)}(x)\\right\\} \\leq q \\leq \\min \\left\\{\\theta_{c U}^{(r)}(x), 1\\right\\}\\right\\}\n\\end{aligned}\n$$\n\nwhere $\\alpha(x):=\\left\\{1-\\tau_{c}(x)\\right\\} / \\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\}$, and\n\n$$\n\\theta_{c U}(x):=\\frac{\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\}}{1-\\tau_{c}(x)}, \\quad \\theta_{c U}^{(r)}(x):=\\frac{1-\\tau_{c}(x)}{\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\}}\n$$\n\nWe then have the following lemma.\n\nLemma 3. Suppose that assumption A holds. For all $x \\in \\mathcal{X}$, we have $\\left(\\theta_{c}(x), \\theta_{c}^{(r)}(x)\\right) \\in \\mathcal{B}$, which is sharp based on the information of $\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\}$ and $\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=\\right.$ $1, X=x\\}$.\n\nProof. It follows from equation (35) and an application of Lemma I1 in Appendix I of Jun and Lee (2023) by setting $\\mathbb{P}^{*}(A \\cap B)$ in the lemma to be $\\mathbb{P}\\left\\{Y_{1}(0) \\in A, Y_{1}(1) \\in B \\mid D_{1}=\\right.$ $1, X=x\\}$.\n\nThe set $\\mathcal{B}$ is a line that describes the bounds on $\\left(\\theta_{c}(x), \\theta_{c}^{(r)}(x)\\right)$ we can obtain from the marginal probabilities of the potential outcomes (given $D_{1}=1, X=x$ ) without using assumption B. Therefore, lemmas 1 and 3 show that rescaling CATT as in the end point $\\left(\\theta_{c L}(x), \\theta_{c L}^{(r)}(x)\\right)$ provides useful causal parameters to consider. Conditioning on $X=x$, $\\left(\\theta_{c L}(x), \\theta_{c L}^{(r)}(x)\\right)$ exactly corresponds to the persuasion rates on the treated that we are interested in if there is no backlash, while they serve as conservative measures in general, even if the backlash effect is a concern.\n\nAggregating $X$ with appropriate conditioning shows that $\\theta_{L}$ and $\\theta_{L}^{(r)}$ are valid lower bounds on $\\theta$ and $\\theta^{(r)}$, respectively: i.e., even if there is a concern about the backlash so that assumption B may be violated, $\\theta_{L}$ and $\\theta_{L}^{(r)}$ still offer conservative measures of $\\theta$ and $\\theta^{(r)}$, respectively, although it may not be sharp in general. In order to be more precise on this issue, let\n\n$$\n\\begin{aligned}\n\\mathbb{L} & :=\\mathbb{E}\\left\\{\\mathbb{1}_{\\mathcal{X}_{L}}(X) \\operatorname{CATT}(X) \\mid D_{1}=1\\right\\} \\\\\n\\mathbb{U} & :=\\mathbb{E}\\left[\\mathbb{1}_{\\mathcal{X}_{U}}(X) \\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=1, X\\right)+\\left\\{1-\\mathbb{1}_{\\mathcal{X}_{U}}(X)\\right\\}\\left\\{1-\\tau_{c}(X)\\right\\} \\mid D_{1}=1\\right]\n\\end{aligned}\n$$\n\nwhere $\\mathcal{X}_{L}$ and $\\mathcal{X}_{U}$ are defined by\n\n$$\n\\begin{aligned}\n\\mathcal{X}_{L} & :=\\left\\{x \\in \\mathcal{X}: \\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=1, X=x\\right)-\\tau_{c}(x) \\geq 0\\right\\} \\\\\n\\mathcal{X}_{U} & :=\\left\\{x \\in \\mathcal{X}: \\tau_{c}(x)+\\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=1, X=x\\right) \\leq 1\\right\\}\n\\end{aligned}\n$$\n\nDefine\n\n$$\n\\mathcal{B}:=\\left\\{(p, q) \\in \\mathbb{R}^{2}: \\theta_{L}^{*} \\leq p \\leq \\theta_{U}^{*}, q=\\alpha p\\right\\}=\\left\\{(p, q) \\in \\mathbb{R}^{2}: p=q / \\alpha, \\theta_{L}^{(r) *} \\leq q \\leq \\theta_{U}^{(r) *}\\right\\}\n$$\n\nwhere $\\alpha=\\mathbb{E}\\left\\{1-\\tau_{c}(X) \\mid D_{1}=1\\right\\} / \\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=1\\right)$, and\n\n$$\n\\begin{aligned}\n\\theta_{L}^{*}:=\\mathbb{L} / \\mathbb{E}\\left\\{1-\\tau_{c}(X) \\mid D_{1}=1\\right\\} & \\text { and } \\quad \\theta_{U}^{*}:=\\mathbb{U} / \\mathbb{E}\\left\\{1-\\tau_{c}(X) \\mid D_{1}=1\\right\\} \\\\\n\\theta_{L}^{(r) *}:=\\mathbb{L} / \\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=1\\right) & \\text { and } \\quad \\theta_{U}^{(r) *}:=\\mathbb{U} / \\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=1\\right)\n\\end{aligned}\n$$\n\nLemma 4. Suppose that assumption $A$ holds for all $x \\in \\mathcal{X}$. We then have $\\left(\\theta, \\theta^{(r)}\\right) \\in \\mathcal{B}$, which is sharp based on the information of $\\mathbb{P}\\left\\{Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\}$ and $\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=\\right.$ $1, X=x\\}$ for all $x \\in \\mathcal{X}$, and the distribution of $X$ given $D_{1}=1$.\n\nProof. We will only show the bounds on $\\theta$ : the set $\\mathcal{B}$ will follow from them and the linear relationship between $\\theta$ and $\\theta^{(r)}$, i.e., $\\theta^{(r)}=\\theta \\mathbb{E}\\left\\{1-\\tau_{c}(X) \\mid D_{1}=1\\right\\} / \\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=1\\right)$. First, we note that\n\n$$\n\\mathcal{X}_{L}=\\left\\{x \\in \\mathcal{X}: \\theta_{c L}(x) \\geq 0\\right\\} \\quad \\text { and } \\quad \\mathcal{X}_{U}=\\left\\{x \\in \\mathcal{X}: \\theta_{c U}(x) \\leq 1\\right\\}\n$$\n\nby definition. Therefore, we can equivalently write the sharp bounds in lemma 3 as\n\n$$\n\\mathbb{1}_{\\mathcal{X}_{L}}(x) \\theta_{c L}(x) \\leq \\theta_{c}(x) \\leq \\mathbb{1}_{\\mathcal{X}_{U}}(x) \\theta_{c U}(x)+1-\\mathbb{1}_{\\mathcal{X}_{U}}(x)\n$$\n\nWe then use the fact\n\n$$\nf\\left\\{x \\mid Y_{1}(0)=0, D_{1}=1\\right\\}=\\frac{\\mathbb{P}\\left\\{Y_{1}(0)=0 \\mid D_{1}=1, X=x\\right\\} f\\left(x \\mid D_{1}=1\\right)}{\\mathbb{P}\\left\\{Y_{1}(0)=0 \\mid D_{1}=1\\right\\}}\n$$\n\nby Bayes' rule. Specifically, multiplying the conditional density in equation (37) to both sides of the inequalities in equation (36) and integrating shows the claim.\n\nHere, $\\tau_{c}(\\cdot)$ is the only unidentified object so that $\\theta_{L}^{*}$ and $\\theta_{U}^{*}$ will be identified if $\\tau_{c}(\\cdot)$ is identified. The set $\\mathcal{X}_{L}$ represents the values $x$ of $X$ such that $\\operatorname{CATT}(x) \\geq 0$ : i.e., it is the set of $x$ such that the lower bound on $\\theta_{c}(x)$ in lemma 3 is nontrivial. Similarly, $\\mathcal{X}_{U}$ is the set of the values $x \\in \\mathcal{X}$ such that the upper bound on $\\theta_{c}(x)$ in lemma 3 is not trivial: or,\n\nequivalently, it is the set of values $x \\in \\mathcal{X}$ such that the upper bound on $\\theta_{c}^{(r)}(x)$ is trivial. Indeed, the condition that defines $\\mathcal{X}_{U}$ can be equivalently expressed as\n\n$$\n\\mathbb{P}\\left\\{Y_{1}(0)=1, Y_{1}(1)=1 \\mid D_{1}=1, X=x\\right\\} \\leq \\mathbb{P}\\left\\{Y_{1}(0)=0, Y_{1}(1)=0 \\mid D_{1}=1, X=x\\right\\}\n$$\n\nIf this inequality is not satisfied so that there are too many 'voters' who are characterized by $X=x$ and who would vote for the party the media publicly endorses no matter what, then the upper bound on $\\theta_{c}(x)$ based on the marginals of the potential outcomes will be just trivial, i.e., $\\theta_{c U}(x)=1$.\n\nIf assumption B holds, then we have $\\operatorname{CATT}(X) \\geq 0$ almost surely, and therefore $\\theta_{L}^{*}=\\theta_{L}$ as well as $\\theta_{L}^{(r) *}=\\theta_{L}^{(r)}$ will follow. Therefore, lemmas 2 and 4 show that $\\theta_{L}^{*}$ and $\\theta^{(r) *}$ will be interesting parameters to consider: they are sharp lower bounds on $\\theta$ and $\\theta^{(r)}$, respectively, in general, while they are exactly equal to $\\theta$ and $\\theta^{(r)}$ under monotonicity.\n\nHowever, $\\left(\\theta_{L}^{*}, \\theta_{L}^{(r) *}\\right)$ is a more difficult parameter than $\\left(\\theta_{L}, \\theta_{L}^{(r)}\\right)$ because the former contains $\\mathbb{1}_{\\mathcal{X}_{L}}(X)$ that depends on unknown objects in a nonsmooth way. In contrast, $\\left(\\theta_{L}, \\theta_{L}^{(r)}\\right)$ can be estimated in a more straightforward manner, as long as $\\tau_{c}(\\cdot)$ is identified. Therefore, we consider $\\theta_{L}$ and $\\theta_{L}^{(r)}$ the aggregate parameters of interest. Since $\\max \\left(0, \\theta_{L}\\right) \\leq \\theta_{L}^{*}$ in general, $\\theta_{L}$ is always a robust lower bound on $\\theta$; the same comment applies to $\\theta_{L}^{(r)}$ as well. If $\\operatorname{CATT}(x)$ is nonnegative for almost all $x \\in \\mathcal{X}$, then $\\theta_{L}$ and $\\theta_{L}^{(r)}$ are the sharp lower bounds $\\theta_{L}^{*}$ and $\\theta_{L}^{(r) *}$, respectively. Further, if assumption B holds, then $\\theta_{L}=\\theta_{L}^{*}=\\theta$, and $\\theta_{L}^{(r)}=\\theta_{L}^{(r) *}=\\theta^{(r)}$.\n\nPartial identification under assumption C without using assumption B is largely uneventful. For example, lemma 4 and theorem 1 show the joint sharp identified set of CPRT and R-CPRT when assumption B is violated. For the aggregated parameters, the joint sharp identified set requires aggregation over an unknown subset of the support of $X$ in general. Below we clarify this issue, and we make a formal statement about identification of the aggregated parameters.\n\nDefine\n\n$$\n\\begin{aligned}\n\\tilde{\\theta}_{L}^{*} & :=\\frac{\\mathbb{\\mathbb { L }}}{\\mathbb{E}\\left\\{1-\\Psi(X) \\mid D_{1}=1\\right\\}}, \\quad \\tilde{\\theta}_{U}^{*}:=\\frac{\\mathbb{\\mathbb { L }}}{\\mathbb{E}\\left\\{1-\\Psi(X) \\mid D_{1}=1\\right\\}} \\\\\n\\tilde{\\theta}_{L}^{(r) *} & :=\\frac{\\mathbb{\\mathbb { L }}}{\\mathbb{E}\\left\\{\\Pi_{1}(1, X) \\mid D_{1}=1\\right\\}}, \\quad \\tilde{\\theta}_{U}^{(r) *}:=\\frac{\\mathbb{\\mathbb { L }}}{\\mathbb{E}\\left\\{\\Pi_{1}(1, X) \\mid D_{1}=1\\right\\}}\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& \\mathbb{\\mathbb { L }}:=\\mathbb{E}\\left[\\mathbb{1}_{\\mathcal{X}_{L}^{*}}(X)\\left\\{\\Pi_{1}(1, X)-\\Psi(X)\\right\\} \\mid D_{1}=1\\right] \\\\\n& \\mathbb{\\mathbb { L }}:=\\mathbb{E}\\left[\\mathbb{1}_{\\mathcal{X}_{U}^{*}}(X) \\Pi_{1}(1, X)+\\left\\{1-\\mathbb{1}_{\\mathcal{X}_{U}^{*}}(X)\\right\\}\\left\\{1-\\Psi(X)\\right\\} \\mid D_{1}=1\\right]\n\\end{aligned}\n$$\n\nwith\n\n$$\n\\mathcal{X}_{L}^{*}:=\\left\\{x \\in \\mathcal{X}: \\Psi(x) \\leq \\Pi_{1}(1, x)\\right\\} \\quad \\text { and } \\quad \\mathcal{X}_{U}^{*}:=\\left\\{x \\in \\mathcal{X}: \\Psi(x) \\leq 1-\\Pi_{1}(1, x)\\right\\}\n$$\n\nHere, $\\tilde{\\theta}_{L}, \\tilde{\\theta}_{L}^{(r)}, \\tilde{\\theta}_{L}^{*}, \\tilde{\\theta}_{L}^{(r)}, \\tilde{\\theta}_{U}^{*}$, and $\\tilde{\\theta}_{U}^{(r)}$ are all directly identified from the data.\n\nCorollary 2. Suppose that assumptions $A$ and $C$ are satisfied for all $x \\in \\mathcal{X}$. Then, $\\left(\\theta_{L}, \\theta_{L}^{*}, \\theta_{U}^{*}\\right)$ and $\\left(\\theta_{L}^{(r)}, \\theta_{L}^{(r) *}, \\theta_{U}^{(r) *}\\right)$ are identified by $\\left(\\tilde{\\theta}_{L}, \\tilde{\\theta}_{L}^{*}, \\tilde{\\theta}_{U}^{*}\\right)$ and $\\left(\\tilde{\\theta}_{L}^{(r)}, \\tilde{\\theta}_{L}^{(r) *}, \\tilde{\\theta}_{U}^{(r) *}\\right)$, respectively. Therefore,\n(i) if assumptions $A$ to $C$ hold for all $x \\in \\mathcal{X}$, then $\\theta=\\theta_{L}$ and $\\theta^{(r)}=\\theta_{L}^{(r)}$ are point-identified by $\\tilde{\\theta}_{L}$ and $\\tilde{\\theta}_{L}^{(r)}$, resepctively;\n(ii) if assumptions $A$ and $C$ hold for all $x \\in \\mathcal{X}$, then the joint sharp identifiable set of $\\left(\\theta, \\theta^{(r)}\\right)$ is given by the line connecting $\\left[\\tilde{\\theta}_{L}^{*}, \\tilde{\\theta}_{L}^{(r) *}\\right]$, and $\\left[\\tilde{\\theta}_{U}^{*}, \\tilde{\\theta}_{U}^{(r) *}\\right]$.\n\nProof. Noting that $\\mathcal{X}_{L}^{*}=\\mathcal{X}_{L}$ and $\\mathcal{X}_{U}^{*}=\\mathcal{X}_{U}$ under assumptions A and C by theorem 1, the claim immediately follows from lemma 4 and theorem 1.\n\nHere, $\\tilde{\\theta}_{L}$ and $\\tilde{\\theta}_{L}^{(r)}$ are natural estimands to focus on. They are conservative measures of APRT and R-APRT in general, while they are exactly equal to APRT and R-APRT under monotonicity. Also, they are easier parameters to estimate than $\\tilde{\\theta}_{L}^{*}$ or $\\tilde{\\theta}_{L}^{(r)}$, which depends on unknown objects in a nonsmooth way.", "tables": {}, "images": {}}, {"section_id": 16, "text": "# Appendix S-2. Using a Known Link Function \n\nAs we commented in section 3, assumption C does not allow for popular parametric models such as logit or probit. However, we can modify assumption C to introduce a link function, as long as the link function is pre-specified. Below is a modification of assumption C.\n\nAssumption I (Parallel Trends). For some known link function $\\Lambda$ on $[0,1]$ that is strictly increasing and differentiable, and for all $x \\in \\mathcal{X}, \\Lambda\\left[\\mathbb{P}\\left\\{Y_{t}(0)=1 \\mid D_{1}=d, X=x\\right\\}\\right]$ is separable into the sum of a time component and a treatment component: i.e., there exist functions $G$ (of $t, x$ ) and $H$ (of $d, x$ ) such that\n\n$$\n\\mathbb{P}\\left\\{Y_{t}(0)=1 \\mid D_{1}=d, X=x\\right\\}=\\Lambda^{-1}\\{G(t, x)+H(d, x)\\}\n$$\n\nDifferentiability of $\\Lambda$ will be useful for calculating the efficient influence function, not strictly necessary for identification. The choice of the link function $\\Lambda$ is a specification issue for the researcher: e.g., $\\Lambda(s)=s$ is an obvious choice that we used throughout the main text. More generally, assumption I allows for the class of generalized linear models. For example, the logistic model with $\\Lambda^{-1}(s)=\\exp (s) /\\{1+\\exp (s)\\}$ and\n\n$$\n\\mathbb{P}\\left\\{Y_{t}(0)=1 \\mid D_{1}=d, X=x\\right\\}=\\Lambda^{-1}\\left(\\beta_{0}+\\beta_{1} t+\\beta_{2} d+\\beta_{3} x+\\beta_{4}^{\\top} t x+\\beta_{5}^{\\top} d x\\right)\n$$\n\ndoes not satisfy assumption C, but it does satisfy assumption I.\nIn addition to the linear or logistic choice of $\\Lambda$, it is worth considering $\\Lambda^{-1}(s)=1-$ $\\exp (-s)$ with $s \\geq 0$, i.e., the distribution function of the standard exponential distribution. This choice of the link function is for the case where the time and treatment component are multiplicatively separable, and therefore there is common growth as in Wooldridge (2023). Specifically, suppose that $\\mathbb{P}\\left\\{Y_{t}(0)=0 \\mid D_{1}=d, X=x\\right\\}=\\tilde{G}(t, x) \\tilde{H}(d, x)$ so that for all $x \\in \\mathcal{X}$,\n\n$$\n\\frac{\\mathbb{P}\\left\\{Y_{1}(0)=0 \\mid D_{1}=1, X=x\\right\\}}{\\mathbb{P}\\left\\{Y_{0}(0)=0 \\mid D_{1}=1, X=x\\right\\}}=\\frac{\\mathbb{P}\\left\\{Y_{1}(0)=0 \\mid D_{1}=0, X=x\\right\\}}{\\mathbb{P}\\left\\{Y_{0}(0)=0 \\mid D_{1}=0, X=x\\right\\}}\n$$\n\nIn this case, the choice of $\\Lambda^{-1}(s)=1-\\exp (-s)$ leads to\n\n$$\n\\Lambda\\left[\\mathbb{P}\\left\\{Y_{t}(0)=1 \\mid D_{1}=d, X=x\\right\\}\\right]=-\\log \\tilde{G}(t, x)-\\log \\tilde{H}(d, x)\n$$\n\nUnder assumption I, we have parallel trends with the transformation $\\Lambda$ : i.e.,\n\n$$\n\\begin{aligned}\n\\Lambda\\left[\\mathbb{P}\\left\\{Y_{1}(0)\\right.\\right. & \\left.=1 \\mid D_{1}=1, X=x\\right\\}\\right]-\\Lambda\\left[\\mathbb{P}\\left\\{Y_{0}(0)=1 \\mid D_{1}=1, X=x\\right\\}\\right] \\\\\n& =\\Lambda\\left[\\mathbb{P}\\left\\{Y_{1}(0)=1 \\mid D_{1}=0, X=x\\right\\}\\right]-\\Lambda\\left[\\mathbb{P}\\left\\{Y_{0}(0)=1 \\mid D_{1}=0, X=x\\right\\}\\right]\n\\end{aligned}\n$$\n\nTherefore, theorem 1 and corollaries 1 and 2 continue to hold with the modification of\n\n$$\n\\Psi(X):=\\Lambda^{-1}\\left[\\Lambda\\left\\{\\Pi_{0}(1, X)\\right\\}+\\Lambda\\left\\{\\Pi_{1}(0, X)\\right\\}-\\Lambda\\left\\{\\Pi_{0}(0, X)\\right\\}\\right]\n$$\n\nIndeed, most of our results in the main text can be extended by using assumption I instead of assumption C with some exceptions: e.g., the plug-in approach that uses $\\Psi$ is valid for any choice of $\\Lambda$, while the propensity-odss-weighting approach in equation (14) relies on the specific choice of $\\Lambda(s)=s$.", "tables": {}, "images": {}}, {"section_id": 17, "text": "# APPENDix S-3. Further DisCussion on Controlling for \n\nTHE Pre-TREATMENT OUTCOME AND UnCONFOUNDEDNESS\nIn this part of the appendix, we return to the issues discussed in section 3.2 and provide a more detailed discussion. Specifically, the unconfoundedness condition is an alternative assumption that has been used in the literature especially with cross-sectional data. When the pre-treatment outcome variable is available, it is natural to include it in the conditioning variables to define the unconfoundedness assumption. Below we first compare the unconfoundedness assumption with the parallel trend assumption. Let $Z:=\\left[Y_{0}, X^{\\top}\\right]^{\\top}$, and consider the following assumptions.\n\nAssumption J. At time $t=0$, no one is treated. At time $t=1$, there is a constant $\\epsilon>0$ such that $\\epsilon \\leq \\min \\left[\\mathbb{P}\\left\\{Y_{1}(0)=0, D_{1}=1 \\mid Z\\right\\}, \\mathbb{P}\\left\\{Y_{1}(1)=1, D_{1}=1 \\mid Z\\right\\}\\right]$ and $\\mathbb{P}\\left(D_{1}=1 \\mid Z\\right) \\leq 1-\\epsilon$ with probability one.\n\nAssumption K. $Y_{1}(0)$ is independent of $D_{1}$ conditional on $Z$.\n\nAssumption J is simply setting up the same environment as in the main text, but with Z in lieu of $X$ (see assumption A). Assumption K imposes unconfoundedness given $Z$. We remark that assumption $C$ holds with $Z$ in lieu of $X$ if and only if assumption $K$ holds: this can be verified by using the fact that $Z$ contains $Y_{0}$.\n\nUnder assumptions J and K , both $\\theta_{L}$ and $\\theta_{L}^{(r)}$ are well-defined, and they are identified by\n\n$$\n\\begin{aligned}\n& \\bar{\\theta}_{L, Z}:=\\frac{\\mathbb{E}\\left\\{D_{1}\\left(Y_{1}-Y_{0}\\right)\\right\\}-\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0, Z\\right)\\right\\}}{\\mathbb{E}\\left\\{D_{1}\\left(1-Y_{0}\\right)\\right\\}-\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0, Z\\right)\\right\\}} \\\\\n& \\bar{\\theta}_{L, Z}^{(r)}:=\\frac{\\mathbb{E}\\left\\{D_{1}\\left(Y_{1}-Y_{0}\\right)\\right\\}-\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0, Z\\right)\\right\\}}{\\mathbb{E}\\left(D_{1} Y_{1}\\right)}\n\\end{aligned}\n$$\n\nwhere we use the fact that $Y_{0}$ is included in $Z$. The expressions in equation (40) are reminiscent of $\\bar{\\theta}_{L}$ and $\\bar{\\theta}_{L}^{(r)}$ that identify $\\theta_{L}$ and $\\theta_{L}^{(r)}$ under assumption C: see corollary 1. Indeed,\n\n$$\n\\begin{aligned}\n\\bar{\\theta}_{L} & =\\frac{\\mathbb{E}\\left\\{D_{1}\\left(Y_{1}-Y_{0}\\right)\\right\\}-\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0, X\\right)\\right\\}}{\\mathbb{E}\\left\\{D_{1}\\left(1-Y_{0}\\right)\\right\\}-\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0, X\\right)\\right\\}} \\\\\n\\bar{\\theta}_{L}^{(r)} & =\\frac{\\mathbb{E}\\left\\{D_{1}\\left(Y_{1}-Y_{0}\\right)\\right\\}-\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1}-Y_{0} \\mid D_{1}=0, X\\right)\\right\\}}{\\mathbb{E}\\left(D_{1} Y_{1}\\right)}\n\\end{aligned}\n$$\n\nhave the same forms as $\\bar{\\theta}_{L, Z}$ and $\\bar{\\theta}_{L, Z}^{(r)}$ except that they do not include $Y_{0}$ in the controls. Put differently, adding $Y_{0}$ to $X$ and using the DID formula is an implementation of identifying ATT via assumption K instead of assumption C.\n\nOne might prefer the unconfoundedness condition after controlling for both $X$ and $Y_{0}$ if it is important to avoid the functional form restriction of the parallel trend assumption. Alternatively, the parallel trend assumption might be favored since it would not require fully conditioning on $Y_{0}$. Generally speaking, the required $X$ under the parallel trend assumption could be different from $X$ under the unconfoundedness assumption. If both are the same, which identification assumption to adopt is just reduced to the matter of whether to include $Y_{0}$ in the covariates or not.\n\nFurther, there is a simple testable condition under which it becomes moot to distinguish the two identification strategies. If $Y_{0}$ is independent of $D_{1}$ conditional on $X$, then it can\n\nbe shown that $\\tilde{\\theta}_{L, Z}=\\tilde{\\theta}_{L}$ and $\\tilde{\\theta}_{L, Z}^{(r)}=\\tilde{\\theta}_{L}^{(r)}$. Below we provide a more detailed discussion on this point. We first consider the following assumptions.\n\nAssumption L. $Y_{1}(0)$ is independent of $D_{1}$ conditional on $X$.\nAssumption M. $Y_{0}$ is independent of $D_{1}$ conditional on $X$.\nUnlike Assumption K, Assumption L imposes unconfoundedness given $X$ only. Assumptions K and L are not testable, but Assumption M is: $Y_{0}$ is observed for all observational units, but $Y_{1}(0)$ is not.\n\nLet $\\tilde{\\theta}_{L, X}$ and $\\tilde{\\theta}_{L, X}^{(r)}$ be estimands that identify $\\theta_{L}$ and $\\theta_{L}^{(r)}$, respectively, under unconfoundedness given $X$ (i.e., assumption L). That is, define\n\n$$\n\\tilde{\\theta}_{L, X}:=\\frac{\\mathbb{E}\\left(D_{1} Y_{1}\\right)-\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1} \\mid D_{1}=0, X\\right)\\right\\}}{\\mathbb{E}\\left(D_{1}\\right)-\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1} \\mid D_{1}=0, X\\right)\\right\\}}, \\tilde{\\theta}_{L, X}^{(r)}:=\\frac{\\mathbb{E}\\left(D_{1} Y_{1}\\right)-\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1} \\mid D_{1}=0, X\\right)\\right\\}}{\\mathbb{E}\\left(D_{1} Y_{1}\\right)}\n$$\n\nNow, in view of (40), note that $\\tilde{\\theta}_{L, Z}$ and $\\tilde{\\theta}_{L, Z}^{(r)}$ can be equivalently written as $\\tilde{\\mathcal{N}} /[\\tilde{\\mathcal{N}}+$ $\\left.\\mathbb{E}\\left\\{D_{1}\\left(1-Y_{1}\\right)\\right\\}\\right]$ and $\\tilde{\\mathcal{N}} / \\mathbb{E}\\left(D_{1} Y_{1}\\right)$, respectively, where\n\n$$\n\\tilde{\\mathcal{N}}:=\\mathbb{E}\\left\\{D_{1}\\left(Y_{1}-Y_{0}\\right)-\\left(1-D_{1}\\right)\\left(Y_{1}-Y_{0}\\right) \\frac{\\mathbb{P}\\left(D_{1}=1 \\mid Z\\right)}{\\mathbb{P}\\left(D_{1}=0 \\mid Z\\right)}\\right\\}\n$$\n\nRecall that $\\tilde{\\theta}_{L}=\\mathcal{N} /\\left[\\mathcal{N}+\\mathbb{E}\\left\\{D_{1}\\left(1-Y_{1}\\right)\\right\\}\\right]$ and $\\tilde{\\theta}_{L}^{(r)}=\\mathcal{N} / \\mathbb{E}\\left(D_{1} Y_{1}\\right)$, where $\\mathcal{N}$ is given in (15). Therefore, comparing $\\tilde{\\mathcal{N}}$ with $\\mathcal{N}$ shows that assumption M implies that $\\tilde{\\theta}_{L}=\\tilde{\\theta}_{L, Z}$, and $\\tilde{\\theta}_{L}^{(r)}=\\tilde{\\theta}_{L, Z}^{(r)}$. Also, it follows from\n\n$$\n\\mathbb{E}\\left\\{D_{1} \\mathbb{E}\\left(Y_{1} \\mid D_{1}=0, Z\\right)\\right\\}=\\mathbb{E}\\left\\{Y_{1}\\left(1-D_{1}\\right) \\frac{\\mathbb{P}\\left(D_{1}=1 \\mid Z\\right)}{\\mathbb{P}\\left(D_{1}=0 \\mid Z\\right)}\\right\\}\n$$\n\nthat assumption M is again sufficient to ensure that $\\tilde{\\theta}_{L, Z}=\\tilde{\\theta}_{L, X}$ as well as $\\tilde{\\theta}_{L, Z}^{(r)}=\\tilde{\\theta}_{L, X}^{(r)}$.\nOur discussion so far can be summarized as in the following remark.\n\nRemark 1. Suppose that assumption J holds.\n(i) If assumption M holds, then assumption C is the same as assumption L , while we have $\\tilde{\\theta}_{L}=\\tilde{\\theta}_{L, Z}=\\tilde{\\theta}_{L, X}$ and $\\tilde{\\theta}_{L}^{(r)}=\\tilde{\\theta}_{L, Z}^{(r)}=\\tilde{\\theta}_{L, X}^{(r)}$. Hence, $\\theta_{L}$ and $\\theta_{L}^{(r)}$ are identified by\n\n$\\bar{\\theta}_{L}=\\bar{\\theta}_{L, Z}=\\bar{\\theta}_{L, X}$ and $\\bar{\\theta}_{L}^{(r)}=\\bar{\\theta}_{L, Z}^{(r)}=\\bar{\\theta}_{L, X^{\\prime}}^{(r)}$, respectively, under either assumption C or assumption K. ${ }^{14}$\n(ii) If assumption M does not hold, then the researcher needs to take a stance among assumptions C, K, or L.\n\nFigure 3. Examples of Data Generating Processes with $Z=\\left[Y_{0}, X^{\\top}\\right]^{\\top}$\n![img-2.jpeg](img-2.jpeg)\n\nNotes: Variables in circles are unobserved and they are independent. Assumption K holds in both cases.\n\nFigure 3 illustrates some potential data generating processes. In Panel A, $Y_{0}$ is not independent of $D_{1}$ given $X$. Therefore, the researcher needs to take a stance. In this diagram, assumption K is satisfied, and therefore, we can identify $\\left(\\theta_{L}, \\theta_{L}^{(r)}\\right)$ by $\\left(\\bar{\\theta}_{L, Z}, \\bar{\\theta}_{L, Z}^{(r)}\\right)$. However, conditioning only on $X$ is not sufficient to deliver independence of $D_{1}$ and $Y_{1}(0)$, so $\\bar{\\theta}_{L, X}$ and $\\bar{\\theta}_{L, X}^{(r)}$ are not valid estimands to identify $\\theta_{L}$ and $\\theta_{L}^{(r)}$. It is not clear from this diagram whether assumption C is satisfied or not though. In Panel B, $D_{1}$ is independent of $Y_{0}$ given $X$. Hence, it does not matter what stance the researcher takes: $\\bar{\\theta}_{L}, \\bar{\\theta}_{L, Z}$, and $\\bar{\\theta}_{L, X}$ are all equal to $\\theta_{L}$, while $\\bar{\\theta}_{L}^{(r)}, \\bar{\\theta}_{L, Z}^{(r)}$, and $\\bar{\\theta}_{L, X}$ are all equal to $\\theta_{L}^{(r)}$. In the diagram, assumption K holds, and therefore, both assumptions C and L are satisfied as well.\n\nThe main takeaway from this discussion is that if $Y_{0}$ is independent of $D_{1}$ given $X$, then it is largely an unimportant question whether to control for $Y_{0}$ or not, or whether to rely on unconfoundedness or parallel trends. However, if $Y_{0}$ is not independent of $D_{1}$ given $X$, then the researcher needs to decide carefully which estimand to rely on to learn about $\\theta_{L}$ or $\\theta_{L}^{(r)}$.\n\n[^0]\n[^0]:    ${ }^{14}$ In fact, it is easy to verify that assumption K implies assumption C in this case.\n\nIt seems feasible to establish a bracketing relationship between the two identification approaches, drawing on the findings of Ding and $\\mathrm{Li}(2019)$ regarding ATT; however, we leave this extension for future research.", "tables": {}, "images": {"img-2.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 18, "text": "# APPENDix S-4. Derivation of the Tangent Space \n\nWe observe $\\left(Y_{0}, Y_{1}, D_{1}, X^{\\top}\\right)^{\\top}$ : we also observe $D_{0}$, but it is irrelevant for our discussion here, because $D_{0}=0$ with probability one by the setup. Let $f$ be the density of $X$, and let $P(X):=\\mathbb{P}\\left(D_{1}=1 \\mid X\\right)$. For $d \\in\\{0,1\\}$, let $Q_{d}(X):=\\mathbb{P}\\left(Y_{0}=1 \\mid D_{1}=d, X\\right)$. Further, for $d, y \\in\\{0,1\\}$, let $R_{d y}(X):=\\mathbb{P}\\left(Y_{1}=1 \\mid D_{1}=d, Y_{0}=y, X\\right)$. Then, the likelihood is the product of the following terms, while each line corresponds to one term:\n\n$$\n\\begin{aligned}\n& f(X) \\\\\n& P(X)^{D_{1}}\\{1-P(X)\\}^{1-D_{1}} \\\\\n& {\\left[Q_{1}(X)^{Y_{0}}\\left\\{1-Q_{1}(X)\\right\\}^{1-Y_{0}}\\right]^{D_{1}}} \\\\\n& {\\left[Q_{0}(X)^{Y_{0}}\\left\\{1-Q_{0}(X)\\right\\}^{1-Y_{0}}\\right]^{1-D_{1}}} \\\\\n& \\left(\\left[R_{11}(X)^{Y_{1}}\\left\\{1-R_{11}(X)\\right\\}^{1-Y_{1}}\\right]^{Y_{0}}\\left[R_{10}(X)^{Y_{1}}\\left\\{1-R_{10}(X)\\right\\}^{1-Y_{1}}\\right]^{1-Y_{0}}\\right)^{D_{1}} \\\\\n& \\left(\\left[R_{01}(X)^{Y_{1}}\\left\\{1-R_{01}(X)\\right\\}^{1-Y_{1}}\\right]^{Y_{0}}\\left[R_{00}(X)^{Y_{1}}\\left\\{1-R_{00}(X)\\right\\}^{1-Y_{1}}\\right]^{1-Y_{0}}\\right)^{1-D_{1}}\n\\end{aligned}\n$$\n\nWe will use $\\gamma$ to denote regular parametric submodels with $\\gamma_{0}$ being the truth: e.g., $f\\left(X ; \\gamma_{0}\\right)=f(X)$. Here is our first lemma.\n\nLemma 5. Suppose that assumption E holds. Then, the tangent space for $\\bar{\\theta}_{L}$ has the following form:\n\n$$\n\\begin{aligned}\n\\mathcal{T}:=\\left\\{\\begin{array}{l}\n\\alpha_{0}(X)+\\left\\{D_{1}-P(X)\\right\\} \\alpha_{1}(X)+D_{1}\\left\\{Y_{0}-Q_{1}(X)\\right\\} \\alpha_{2}(X) \\\\\n\\quad+\\left(1-D_{1}\\right)\\left\\{Y_{0}-Q_{0}(X)\\right\\} \\alpha_{3}(X)+D_{1} Y_{0}\\left\\{Y_{1}-R_{11}(X)\\right\\} \\alpha_{4}(X) \\\\\n\\quad+D_{1}\\left(1-Y_{0}\\right)\\left\\{Y_{1}-R_{10}(X)\\right\\} \\alpha_{5}(X)+\\left(1-D_{1}\\right) Y_{0}\\left\\{Y_{1}-R_{01}(X)\\right\\} \\alpha_{6}(X)\n\\end{array}\\right.\n\\end{aligned}\n$$\n\n$$\n+\\left(1-D_{1}\\right)\\left(1-Y_{0}\\right)\\left\\{Y_{1}-R_{00}(X)\\right\\} \\alpha_{7}(X)\\}\n$$\n\nwhere $\\alpha_{j}$ 's are all functions of $X$ such that $\\mathbb{E}\\left\\{\\alpha_{0}(X)\\right\\}=0$ and $\\mathbb{E}\\left\\{\\alpha_{j}^{2}(X)\\right\\}<\\infty$ for $j=0,1, \\cdots, 7$.\nProof. The loglikelihood of regular parametric submodel is given by\n\n$$\n\\ell(\\gamma):=\\ell_{0}(\\gamma)+\\ell_{D_{1}}(\\gamma)+\\ell_{Y_{0} \\mid D_{1}}(\\gamma)+\\ell_{Y_{1} \\mid D_{1}, Y_{0}}(\\gamma)\n$$\n\nwhere\n\n$$\n\\begin{gathered}\n\\ell_{0}(\\gamma):=\\log f(X ; \\gamma) \\\\\n\\ell_{D_{1}}(\\gamma):=D_{1} \\log P(X ; \\gamma)+\\left(1-D_{1}\\right) \\log \\{1-P(X ; \\gamma)\\} \\\\\n\\ell_{Y_{0} \\mid D_{1}}(\\gamma)=D_{1}\\left[Y_{0} \\log Q_{1}(X ; \\gamma)+\\left(1-Y_{0}\\right) \\log \\left\\{1-Q_{1}(X ; \\gamma)\\right\\}\\right] \\\\\n+\\left(1-D_{1}\\right)\\left[Y_{0} \\log Q_{0}(X ; \\gamma)+\\left(1-Y_{0}\\right) \\log \\left\\{1-Q_{0}(X ; \\gamma)\\right\\}\\right]\n\\end{gathered}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\ell_{Y_{1} \\mid D_{1}, Y_{0}}(\\gamma):=D_{1}\\left(Y_{0}\\left[Y_{1} \\log R_{11}(X ; \\gamma)+\\left(1-Y_{1}\\right) \\log \\left\\{1-R_{11}(X ; \\gamma)\\right\\}\\right]\\right. \\\\\n& \\left.+\\left(1-Y_{0}\\right)\\left[Y_{1} \\log R_{10}(X ; \\gamma)+\\left(1-Y_{1}\\right) \\log \\left\\{1-R_{10}(X ; \\gamma)\\right\\}\\right]\\right) \\\\\n& +\\left(1-D_{1}\\right)\\left(Y_{0}\\left[Y_{1} \\log R_{01}(X ; \\gamma)+\\left(1-Y_{1}\\right) \\log \\left\\{1-R_{01}(X ; \\gamma)\\right\\}\\right]\\right. \\\\\n& \\left.+\\left(1-Y_{0}\\right)\\left[Y_{1} \\log R_{00}(X ; \\gamma)+\\left(1-Y_{1}\\right) \\log \\left\\{1-R_{00}(X ; \\gamma)\\right\\}\\right]\\right)\n\\end{aligned}\n$$\n\nTherefore, the score at the truth has the following form:\n\n$$\n\\begin{aligned}\n& S\\left(Y_{0}, Y_{1}, D_{1}, X\\right):=\\frac{1}{f(X)} \\frac{\\partial f\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}+\\frac{D_{1}-P(X)}{P(X)\\{1-P(X)\\}} \\frac{\\partial P\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma} \\\\\n& +\\frac{D_{1}\\left\\{Y_{0}-Q_{1}(X)\\right\\}}{Q_{1}(X)\\left\\{1-Q_{1}(X)\\right\\}} \\frac{\\partial Q_{1}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}+\\frac{\\left(1-D_{1}\\right)\\left\\{Y_{0}-Q_{0}(X)\\right\\}}{Q_{0}(X)\\left\\{1-Q_{0}(X)\\right\\}} \\frac{\\partial Q_{0}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma} \\\\\n& +\\frac{D_{1} Y_{0}\\left\\{Y_{1}-R_{11}(X)\\right\\}}{R_{11}(X)\\left\\{1-R_{11}(X)\\right\\}} \\frac{\\partial R_{11}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}+\\frac{D_{1}\\left(1-Y_{0}\\right)\\left\\{Y_{1}-R_{10}(X)\\right\\}}{R_{10}(X)\\left\\{1-R_{10}(X)\\right\\}} \\frac{\\partial R_{10}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma} \\\\\n& +\\frac{\\left(1-D_{1}\\right) Y_{0}\\left\\{Y_{1}-R_{01}(X)\\right\\}}{R_{01}(X)\\left\\{1-R_{01}(X)\\right\\}} \\frac{\\partial R_{01}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}+\\frac{\\left(1-D_{1}\\right)\\left(1-Y_{0}\\right)\\left\\{Y_{1}-R_{00}(X)\\right\\}}{R_{00}(X)\\left\\{1-R_{00}(X)\\right\\}} \\frac{\\partial R_{00}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\n\\end{aligned}\n$$\n\nfrom which the lemma follows, because the derivatives are not restricted except for square integrability.", "tables": {}, "images": {}}, {"section_id": 19, "text": "# APPENDIX S-5. DERIVATION OF THE PATHWISE DERIVATIVES \n\nLet\n\n$$\n\\begin{aligned}\n\\tilde{\\theta}_{L, n u m}:= & \\mathbb{E}\\left[\\left\\{\\Pi_{1}(1, X)-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} P(X)\\right] \\\\\n\\tilde{\\theta}_{L, d e n}:= & \\mathbb{E}\\left[\\left\\{1-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} P(X)\\right]\n\\end{aligned}\n$$\n\nso that $\\tilde{\\theta}_{L}=\\tilde{\\theta}_{L, n u m} / \\tilde{\\theta}_{L, d e n}$. Define\n\n$$\n\\begin{aligned}\nF_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right) & :=\\left\\{\\Pi_{1}(1, X)-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} D_{1}-\\tilde{\\theta}_{L, n u m} \\\\\n& +D_{1}\\left[\\left\\{Y_{1}-\\Pi_{1}(1, X)\\right\\}-\\left\\{Y_{0}-\\Pi_{0}(1, X)\\right\\}\\right] \\\\\n& -\\frac{P(X)}{1-P(X)}\\left(1-D_{1}\\right)\\left[\\left\\{Y_{1}-\\Pi_{1}(0, X)\\right\\}-\\left\\{Y_{0}-\\Pi_{0}(0, X)\\right\\}\\right]\n\\end{aligned}\n$$\n\nSimilarly, define\n\n$$\n\\begin{aligned}\nF_{\\text {den }} & \\left(Y_{0}, Y_{1}, D_{1}, X\\right):=\\left\\{1-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} D_{1}-\\tilde{\\theta}_{L, d e n} \\\\\n& \\left.-D_{1}\\left\\{Y_{0}-\\Pi_{0}(1, X)\\right\\}-\\frac{P(X)}{1-P(X)}\\left(1-D_{1}\\right)\\left[\\left\\{Y_{1}-\\Pi_{1}(0, X)\\right\\}-\\left\\{Y_{0}-\\Pi_{0}(0, X)\\right\\}\\right]\\right\\}\n\\end{aligned}\n$$\n\nWe will derive the pathwise derivatives of the numerator and denominator of $\\tilde{\\theta}_{L}$ in a few steps. The following two lemmas show that $F_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ and $F_{\\text {dem }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ are the pathwise derivatives of $\\tilde{\\theta}_{L, n u m}$ and $\\tilde{\\theta}_{L, d e n}$, respectively.\n\nLemma 6. Suppose assumption $E$ is satisfied. Then, the pathwise derivative of $\\tilde{\\theta}_{L, n u m}$ is given by $F_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$.\n\nProof. Using the fact that\n\n$$\n\\Pi_{0}(d, X)=Q_{d}(X) \\quad \\text { and } \\quad \\Pi_{1}(d, X)=Q_{d}(X) R_{d 1}(X)+\\left\\{1-Q_{d}(X)\\right\\} R_{d 0}(X)\n$$\n\nwe can write\n\n$$\n\\begin{aligned}\n& \\bar{\\theta}_{L, \\text { num }}=\\int\\left[Q_{1}(x) R_{11}(x)+\\left\\{1-Q_{1}(x)\\right\\} R_{10}(x)-Q_{1}(x)\\right. \\\\\n& \\left.-Q_{0}(x) R_{01}(x)-\\left\\{1-Q_{0}(x)\\right\\} R_{00}(x)+Q_{0}(x)\\right] P(x) f(x) d x\n\\end{aligned}\n$$\n\nTherefore, the pathwise perturbation $\\bar{\\theta}_{L, \\text { num }}(\\gamma)$ of $\\bar{\\theta}_{L, \\text { num }}$ is given by\n\n$$\n\\begin{aligned}\n& \\bar{\\theta}_{L, \\text { num }}(\\gamma)=\\int\\left[Q_{1}(x, \\gamma) R_{11}(x, \\gamma)+\\left\\{1-Q_{1}(x, \\gamma)\\right\\} R_{10}(x, \\gamma)-Q_{1}(x, \\gamma)\\right. \\\\\n& \\left.\\quad-Q_{0}(x, \\gamma) R_{01}(x, \\gamma)-\\left\\{1-Q_{0}(x, \\gamma)\\right\\} R_{00}(x, \\gamma)+Q_{0}(x, \\gamma)\\right] P(x, \\gamma) f(x, \\gamma) d x\n\\end{aligned}\n$$\n\nNow, by straightforward algebra, $\\partial \\bar{\\theta}_{L, \\text { num }}\\left(\\gamma_{0}\\right) / \\partial \\gamma$ is equal to the sum of the following terms (with each line corresponding to one term):\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\{\\Pi_{1}(1, X)-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} P(X) \\frac{\\partial f\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma} \\frac{1}{f(X)}\\right] \\\\\n& \\mathbb{E}\\left[\\left\\{\\Pi_{1}(1, X)-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} \\frac{\\partial P\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[\\left\\{R_{11}(X)-R_{10}(X)-1\\right\\} P(X) \\frac{\\partial Q_{1}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[\\left\\{R_{00}(X)-R_{01}(X)+1\\right\\} P(X) \\frac{\\partial Q_{0}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[Q_{1}(X) P(X) \\frac{\\partial R_{11}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[\\left\\{1-Q_{1}(X)\\right\\} P(X) \\frac{\\partial R_{10}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[-Q_{0}(X) P(X) \\frac{\\partial R_{01}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[-\\left\\{1-Q_{0}(X)\\right\\} P(X) \\frac{\\partial R_{00}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right]\n\\end{aligned}\n$$\n\nWe are looking for some $F\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ with mean zero that satisfies\n\n$$\n\\frac{\\partial \\bar{\\theta}_{L, \\text { num }}\\left(\\gamma_{0}\\right)}{\\partial \\gamma}=\\mathbb{E}\\left\\{F\\left(Y_{0}, Y_{1}, D_{1}, X\\right) S\\left(Y_{0}, Y_{1}, D_{1}, X\\right)\\right\\}\n$$\n\nwhere $S\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ is the score described in equation (42). Using the fact that the variance of a binary variable is the \"\"success\" probability times that of \"failure,\" we know by\n\ninspection that such $F\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ must be given by\n\n$$\n\\begin{aligned}\nF\\left(Y_{0}, Y_{1}, D_{1}, X\\right) & :=\\left\\{\\Pi_{1}(1, X)-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} P(X)-\\bar{\\theta}_{L, \\text { num }} \\\\\n& +\\left\\{\\Pi_{1}(1, X)-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\}\\left\\{D_{1}-P(X)\\right\\} \\\\\n& +\\left\\{R_{11}(X)-R_{10}(X)-1\\right\\} P(X) \\frac{D_{1}\\left\\{Y_{0}-Q_{1}(X)\\right\\}}{P(X)} \\\\\n& +\\left\\{R_{00}(X)-R_{01}(X)+1\\right\\} P(X) \\frac{\\left(1-D_{1}\\right)\\left\\{Y_{0}-Q_{0}(X)\\right\\}}{1-P(X)} \\\\\n& +Q_{1}(X) P(X) \\frac{D_{1} Y_{0}\\left\\{Y_{1}-R_{11}(X)\\right\\}}{P(X) Q_{1}(X)} \\\\\n& +\\left\\{1-Q_{1}(X)\\right\\} P(X) \\frac{D_{1}\\left(1-Y_{0}\\right)\\left\\{Y_{1}-R_{10}(X)\\right\\}}{P(X)\\left\\{1-Q_{1}(X)\\right\\}} \\\\\n& -Q_{0}(X) P(X) \\frac{\\left(1-D_{1}\\right) Y_{0}\\left\\{Y_{1}-R_{01}(X)\\right\\}}{\\{1-P(X)\\} Q_{0}(X)} \\\\\n& -\\left\\{1-Q_{0}(X)\\right\\} P(X) \\frac{\\left(1-D_{1}\\right)\\left(1-Y_{0}\\right)\\left\\{Y_{1}-R_{00}(X)\\right\\}}{\\{1-P(X)\\}\\left\\{1-Q_{0}(X)\\right\\}}\n\\end{aligned}\n$$\n\nHowever, simplifying $F\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ by using equation (43) yields\n\n$$\nF\\left(Y_{0}, Y_{1}, D_{1}, X\\right)=F_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)\n$$\n\nLemma 7. Suppose assumption $E$ is satisfied. Then, the pathwise derivative of $\\bar{\\theta}_{L, \\text { den }}$ is given by $F_{\\text {den }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$.\n\nProof. Using (43), we can write\n\n$$\n\\bar{\\theta}_{L, d e n}=\\int\\left\\{1-Q_{1}(x)-Q_{0}(x) R_{01}(x)-\\left\\{1-Q_{0}(x)\\right\\} R_{00}(x)+Q_{0}(x)\\right\\} P(x) f(x) d x\n$$\n\nTherefore, the pathwise perturbation of $\\bar{\\theta}_{L, d e n}$ is given by\n\n$$\n\\begin{aligned}\n& \\bar{\\theta}_{L, d e n}(\\gamma):=\\int\\left\\{1-Q_{1}(x, \\gamma)-Q_{0}(x, \\gamma) R_{01}(x, \\gamma)\\right. \\\\\n& \\left.-\\left\\{1-Q_{0}(x, \\gamma)\\right\\} R_{00}(x, \\gamma)+Q_{0}(x, \\gamma)\\right\\} P(x, \\gamma) f(x, \\gamma) d x\n\\end{aligned}\n$$\n\nHence, by simple algebra, $\\partial \\bar{\\theta}_{L, \\text { den }}\\left(\\gamma_{0}\\right) / \\partial \\gamma$ is equal to the sum of the following terms (with each line corresponding to one term):\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\{1-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} P(X) \\frac{\\partial f\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma} \\frac{1}{f(X)}\\right] \\\\\n& \\mathbb{E}\\left[\\left\\{1-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} \\frac{\\partial P\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[-P(X) \\frac{\\partial Q_{1}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[\\left\\{R_{00}(X)-R_{01}(X)+1\\right\\} P(X) \\frac{\\partial Q_{0}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[-Q_{0}(X) P(X) \\frac{\\partial R_{01}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right] \\\\\n& \\mathbb{E}\\left[-\\left\\{1-Q_{0}(X)\\right\\} P(X) \\frac{\\partial R_{00}\\left(X ; \\gamma_{0}\\right)}{\\partial \\gamma}\\right]\n\\end{aligned}\n$$\n\nNow, we are looking for some $F\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ with mean zero that satisfies\n\n$$\n\\frac{\\partial \\bar{\\theta}_{L, \\text { den }}\\left(\\gamma_{0}\\right)}{\\partial \\gamma}=\\mathbb{E}\\left\\{F\\left(Y_{0}, Y_{1}, D_{1}, X\\right) S\\left(Y_{0}, Y_{1}, D_{1}, X\\right)\\right\\}\n$$\n\nwhere $S\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ is the score described in equation (42). Using the fact that the variance of a binary variable is the \"'success\" probability times that of \"failure,\" we know by inspection that such $F\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ must be given by\n\n$$\n\\begin{aligned}\nF\\left(Y_{0}, Y_{1}, D_{1}, X\\right) & :=\\left\\{1-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\} P(X)-\\bar{\\theta}_{L, \\text { den }} \\\\\n& +\\left\\{1-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\}\\left\\{D_{1}-P(X)\\right\\} \\\\\n& -P(X) \\frac{D_{1}\\left\\{Y_{0}-Q_{1}(X)\\right\\}}{P(X)} \\\\\n& +\\left\\{R_{00}(X)-R_{01}(X)+1\\right\\} P(X) \\frac{\\left(1-D_{1}\\right)\\left\\{Y_{0}-Q_{0}(X)\\right\\}}{1-P(X)} \\\\\n& -Q_{0}(X) P(X) \\frac{\\left(1-D_{1}\\right) Y_{0}\\left\\{Y_{1}-R_{01}(X)\\right\\}}{\\{1-P(X)\\} Q_{0}(X)} \\\\\n& -\\left\\{1-Q_{0}(X)\\right\\} P(X) \\frac{\\left(1-D_{1}\\right)\\left(1-Y_{0}\\right)\\left\\{Y_{1}-R_{00}(X)\\right\\}}{\\{1-P(X)\\}\\left\\{1-Q_{0}(X)\\right\\}}\n\\end{aligned}\n$$\n\nHowever, simplifying $F\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ by using equation (43) yields\n\n$$\nF\\left(Y_{0}, Y_{1}, D_{1}, X\\right)=F_{\\text {den }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)\n$$\n\nIn Section S-4, we have derived the scores at $\\gamma_{0}$ and the tangent space. Also, we have shown that the pathwise derivatives of $\\bar{\\theta}_{L, n u m}$ and $\\bar{\\theta}_{L, d e n}$ are given by $F_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ and $F_{\\text {den }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$, respectively. By using these results, we now derive the pathwise derivative of $\\bar{\\theta}_{L}=\\bar{\\theta}_{L, n u m} / \\bar{\\theta}_{L, d e n}$ below.\n\nLemma 8. Suppose assumption $E$ is satisfied. Then, the pathwise derivative of $\\bar{\\theta}_{L}$ is given by\n\n$$\nG\\left(Y_{0}, Y_{1}, D_{1}, X\\right):=\\frac{1}{\\bar{\\theta}_{L, d e n}}\\left(F_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)-\\bar{\\theta}_{L} F_{\\text {den }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)\\right)\n$$\n\nProof. Since $\\bar{\\theta}_{L}=\\bar{\\theta}_{L, n u m} / \\bar{\\theta}_{L, d e n}$, we write the pathwise perturbation of $\\bar{\\theta}_{L}$ as $\\bar{\\theta}_{L}(\\gamma)=$ $\\bar{\\theta}_{L, n u m}(\\gamma) / \\bar{\\theta}_{L, d e n}(\\gamma)$, where the truth is denoted by $\\gamma_{0}$ : see the section on the derivation of the pathwise derivatives of $\\bar{\\theta}_{L, n u m}$ and $\\bar{\\theta}_{L, d e n}$.\n\nNow, we need to show that $G\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ satisfies\n\n$$\n\\frac{\\partial \\bar{\\theta}_{L}\\left(\\gamma_{0}\\right)}{\\partial \\gamma}=\\mathbb{E}\\left\\{G\\left(Y_{0}, Y_{1}, D_{1}, X\\right) S\\left(Y_{0}, Y_{1}, D_{1}, X\\right)\\right\\}\n$$\n\nwhere $S\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ is the score described in equation (42) in Online Appendix S-4. But, noting that\n\n$$\n\\frac{\\partial \\bar{\\theta}_{L}\\left(\\gamma_{0}\\right)}{\\partial \\gamma}=\\frac{1}{\\bar{\\theta}_{L, d e n}}\\left(\\frac{\\partial \\bar{\\theta}_{L, n u m}\\left(\\gamma_{0}\\right)}{\\partial \\gamma}-\\bar{\\theta}_{L} \\frac{\\partial \\bar{\\theta}_{L, d e n}\\left(\\gamma_{0}\\right)}{\\partial \\gamma}\\right)\n$$\n\nit follows from the fact that the pathwise derivatives of $\\bar{\\theta}_{L, n u m}$ and $\\bar{\\theta}_{L, d e n}$ are given by $F_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ and $F_{\\text {den }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$, respectively: see Online Appendix S-5 on the derivations of the pathwise derivatives.\n\nLemma 9. We have $F_{D I D}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)=G\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$, where $F_{D I D}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ is defined in equation (17) in Theorem 4.\n\nProof. This is a simple algebraic result. Specifically, if we plug $F_{\\text {num }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ and $F_{\\text {den }}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ into equation (48), then it follows that\n\n$$\nG\\left(Y_{0}, Y_{1}, D_{1}, X\\right)=\\frac{1}{\\bar{\\theta}_{L, \\text { den }}}\\left(H_{1}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)+H_{2}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)+H_{3}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)\\right)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& H_{1}\\left(Y_{0}, Y_{1}, D_{1}, X\\right):=D_{1}\\left[\\left\\{\\Pi_{1}(1, X)-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\}\\right. \\\\\n& \\left.-\\bar{\\theta}_{L}\\left\\{1-\\Pi_{0}(1, X)-\\Pi_{1}(0, X)+\\Pi_{0}(0, X)\\right\\}\\right] \\\\\n& H_{2}\\left(Y_{0}, Y_{1}, D_{1}, X\\right):=D_{1}\\left[\\left\\{Y_{1}-\\Pi_{1}(1, X)\\right\\}-\\left(1-\\bar{\\theta}_{L}\\right)\\left\\{Y_{0}-\\Pi_{0}(1, X)\\right\\}\\right] \\\\\n& H_{3}\\left(Y_{0}, Y_{1}, D_{1}, X\\right):=\\left(\\bar{\\theta}_{L}-1\\right) \\frac{P(X)}{1-P(X)}\\left(1-D_{1}\\right)\\left[\\left\\{Y_{1}-\\Pi_{1}(0, X)\\right\\}-\\left\\{Y_{0}-\\Pi_{0}(0, X)\\right\\}\\right]\n\\end{aligned}\n$$\n\nFurther simplifications yield the form of $F_{D I D}\\left(Y_{0}, Y_{1}, D_{1}, X\\right)$ defined in equation (17) in Theorem 4. In addition, the form of $F_{D I D}\\left(Y_{0}, Y\\right) 1, D_{1}, X$ ) in (18) can be obtained by manipulating the terms in (17).", "tables": {}, "images": {}}, {"section_id": 20, "text": "# APPENDix S-6. Inference for the Event-Study Persuasion Rate \n\nThe asymptotic influence function for regular and asymptotically linear estimators of the multi-period parameters can be derived from our previous discussion on the twoperiod case. We will explain this by focusing on $\\hat{\\theta}_{E S P R, D R}(j)$, which we consider the most comprehensive summary parameter.\n\nLet $\\theta_{\\text {num }}(s, s+j)$ and $\\theta_{\\text {den }}(s, s+j)$ be the numerator and the denominator of $\\theta(s, s+j)$, respectively, as before. Let $\\mathcal{D}_{s, s+j}:=\\left(Y_{s-1}, Y_{s+j}, S, X\\right)$, and define\n\n$$\n\\begin{aligned}\n& G_{\\text {num }}\\left(\\mathcal{D}_{s, s+j}\\right):=\\mathbb{1}(S=s)\\left(Y_{s+j}-Y_{s-1}\\right)-\\frac{\\mathbb{P}\\left(S=s \\mid X, \\bar{S}_{s}=1\\right)}{\\mathbb{P}\\left(S=\\infty \\mid X, \\bar{S}_{s}=1\\right)}\\left(Y_{s+j}-Y_{s-1}\\right) \\mathbb{1}(S=\\infty) \\\\\n& G_{\\text {den }}\\left(\\mathcal{D}_{s, s+j}\\right):=\\mathbb{1}(S=s)\\left(1-Y_{s-1}\\right)-\\frac{\\mathbb{P}\\left(S=s \\mid X, \\bar{S}_{s}=1\\right)}{\\mathbb{P}\\left(S=\\infty \\mid X, \\bar{S}_{s}=1\\right)}\\left(Y_{s+j}-Y_{s-1}\\right) \\mathbb{1}(S=\\infty) \\\\\n& G_{\\text {adj }}\\left(\\mathcal{D}_{s, s+j}\\right):=-\\left\\{\\mathbb{1}(S=s)-\\mathbb{1}(S=\\infty) \\frac{\\mathbb{P}\\left(S=s \\mid X, \\bar{S}_{s}=1\\right)}{\\mathbb{P}\\left(S=\\infty \\mid X, \\bar{S}_{s}=1\\right)}\\right\\} \\Delta_{s, s+j}(\\infty, X)\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\Delta_{s, s+j}(\\infty, X):=\\mathbb{P}\\left(Y_{s+j}=1 \\mid S=\\infty, X\\right)-\\mathbb{P}\\left(Y_{s-1}=1 \\mid S=\\infty, X\\right)\n$$\n\nWe then have the following theorem, for which we consider a random sample of size $n$, $\\left\\{\\left(Y_{i 0}, Y_{i 1}, \\cdots, Y_{i T}, S_{i}, X_{i}\\right): i=1, \\cdots, n\\right\\}$. Let $p_{s}:=\\mathbb{P}\\left(S_{i}=s\\right)$, and $\\hat{p}_{s}:=n^{-1} \\sum_{i=1}^{n} \\mathbb{1}\\left(S_{i}=\\right.$ s). Further, for $r \\in\\{n u m$, den $\\}$, define\n\n$$\n\\begin{aligned}\n& \\tilde{P}:=\\left[\\begin{array}{llll}\np_{1} & \\cdots & p_{T-j} \\\\\n{\\mathbb{P}\\left(S_{i}=1 \\text { or } S_{i}=\\infty\\right)} & \\cdots \\cdots & \\mathbb{P}\\left(S_{i}=T-j \\text { or } S_{i}=\\infty\\right)\n\\end{array}\\right]^{\\top} \\\\\n& \\Theta_{r}:=\\left[\\begin{array}{llll}\n\\theta_{r}(1,1+j) & \\cdots \\cdots & \\theta_{r}(T-j, T)\n\\end{array}\\right]^{\\top}\n\\end{aligned}\n$$\n\nFinally, let\n\n$$\n\\mathbf{Q}_{i}:=\\left[\\begin{array}{ll}\n\\mathbf{G}_{n u m, i}^{\\top} & \\mathbf{G}_{d e n, i}^{\\top} & \\mathbf{H}_{i}^{\\top}\n\\end{array}\\right]^{\\top}\n$$\n\nwhere for $r \\in\\{n u m$, den $\\}$,\n\n$$\n\\begin{aligned}\n\\mathbf{G}_{r, i} & :=\\left[\\begin{array}{llll}\nG_{r}\\left(\\mathcal{D}_{i, 1,1+j}\\right)+G_{a d j}\\left(\\mathcal{D}_{i, 1,1+j}\\right) & \\cdots \\cdots & G_{r}\\left(\\mathcal{D}_{i, T-j, T}\\right)+G_{a d j}\\left(\\mathcal{D}_{i, T-j, T}\\right)\n\\end{array}\\right]^{\\top} \\\\\n\\mathbb{H}_{i} & :=\\left[\\begin{array}{llll}\n\\mathbb{1}\\left(S_{i}=1\\right)-p_{1} & \\cdots \\cdots & \\mathbb{1}\\left(S_{i}=T-j\\right)-p_{T-j}\n\\end{array}\\right]^{\\top}\n\\end{aligned}\n$$\n\nTheorem 7. For $r \\in\\{n u m$, den $\\}$, let $\\hat{\\theta}_{r}(s, s+j)$ be an estimator of $\\theta_{r}(s, s+j)$ based on a sumbsample $\\left\\{\\left(Y_{i s-1}, Y_{i s+j}, \\mathbb{1}\\left(S_{i}=s\\right), X_{i}\\right) \\mathbb{1}\\left(S_{i}=1\\right.\\right.$ or $\\left.\\left.S_{i}=\\infty\\right): i=1,2, \\cdots, n\\right\\}$. If they are regular and asymptotically linear (conditional on $\\left.\\mathbb{1}\\left(S_{i}=s\\right)+\\mathbb{1}\\left(S_{i}=\\infty\\right)=1\\right)$, then\n\n$$\n\\sqrt{n}\\left(\\frac{\\sum_{s=1}^{T-j} \\hat{p}_{s} \\hat{\\theta}_{n u m}(s, s+j)}{\\sum_{s=1}^{T-j} \\hat{p}_{s} \\hat{\\theta}_{d e n}(s, s+j)}-\\frac{\\sum_{s=1}^{T-j} p_{s} \\theta_{n u m}(s, s+j)}{\\sum_{s=1}^{T-j} p_{s} \\theta_{d e n}(s, s+j)}\\right) \\xrightarrow{d} N\\left(0, J P \\Sigma P^{\\top} J^{\\top}\\right)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\nJ & :=\\left[\\begin{array}{ll}\n\\frac{1}{\\sum_{s=1}^{T-j} p_{s} \\theta_{d e n}(s, s+j)} & -\\frac{\\sum_{s=1}^{T-j} p_{s} \\theta_{d e n}(s, s+j)}{\\left\\{\\sum_{s=1}^{T-j} p_{s} \\theta_{d e n}(s, s+j)\\right\\}^{2}}\n\\end{array}\\right] \\\\\nP & :=\\left[\\begin{array}{lll}\n\\hat{P}^{\\top} & \\boldsymbol{O}^{\\top} & \\Theta_{n u m}^{\\top} \\\\\n\\boldsymbol{O}^{\\top} & \\hat{P}^{\\top} & \\Theta_{d e n}^{\\top}\n\\end{array}\\right] \\\\\n\\Sigma & :=\\mathbb{E}\\left(\\mathbf{Q}_{1} \\mathbf{Q}_{1}^{\\top}\\right)\n\\end{aligned}\n$$\n\nwhere $\\mathbb{O}$ is a $(T-j) \\times 1$ vector of zeroes.\n\nTherefore, the DR estimator $\\hat{\\theta}_{E S P R, D R}(j)$ will be asymptotically normal with the asymptotic variance provided in theorem 7, as long as its component estimators are regular and asymptotically linear.\n\nProof of Theorem 7: By similar calculations to theorem 4 but by using the conditional likelihood given $\\mathbb{1}\\left(S_{i}=s\\right.$ or $\\left.S_{i}=\\infty\\right)=1$, we know that a regular and asymptotically linear estimator $\\hat{\\theta}_{r}(s, s+j)$ must have the following asymptotic expansion:\n\n$$\n\\begin{aligned}\n& \\sqrt{n}\\left\\{\\hat{\\theta}_{r}(s, s+j)-\\theta_{r}(s, s+j)\\right\\} \\\\\n& \\quad=\\frac{1}{\\mathbb{P}\\left(S_{i}=s \\text { or } S_{i}=\\infty\\right)} \\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n}\\left\\{G_{r}\\left(\\mathcal{D}_{i, s, s+j}\\right)+G_{a d j}\\left(\\mathcal{D}_{i, s, s+j}\\right)\\right\\}+o_{p}(1)\n\\end{aligned}\n$$\n\nwhere $\\mathcal{D}_{i, s, s+j}:=\\left(Y_{i s-1}, Y_{i s+j}, \\mathbb{1}\\left(S_{i}=s\\right), X_{i}\\right)$. Therefore, let $p_{s}:=\\mathbb{P}\\left(S_{i}=s\\right)$, and we have\n\n$$\n\\sqrt{n} \\sum_{s=1}^{T-j} p_{s}\\left\\{\\hat{\\theta}_{r}(s, s+j)-\\theta_{r}(s, s+j)\\right\\}=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\hat{P}^{\\top} \\mathrm{G}_{r, i}+o_{p}(1)\n$$\n\nNote also that\n\n$$\n\\sqrt{n} \\sum_{s=1}^{T-j} \\theta_{r}(s, s+j)\\left(\\hat{p}_{s}-p_{s}\\right)=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\Theta_{r}^{\\top} \\mathbb{H}_{i}\n$$\n\nTherefore, it follows that\n\n$$\n\\begin{aligned}\n& \\sqrt{n}\\left\\{\\sum_{s=1}^{T-j} \\hat{p}_{s} \\hat{\\theta}_{r}(s, s+j)-\\sum_{s=1}^{T-j} p_{s} \\theta_{r}(s, s+j)\\right\\} \\\\\n& =\\sqrt{n} \\sum_{s=1}^{T-j}\\left[p_{s}\\left\\{\\hat{\\theta}_{r}(s, s+j)-\\theta_{r}(s, s+j)\\right\\}+\\theta_{r}(s, s+j)\\left(\\hat{p}_{s}-p_{s}\\right)\\right]+O_{p}\\left(n^{-1 / 2}\\right) \\\\\n& =\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n}\\left(\\hat{P}^{\\top} \\mathrm{G}_{r, i}+\\Theta_{r}^{\\top} \\mathbb{H}_{i}\\right)+o_{p}(1)\n\\end{aligned}\n$$\n\nNow, use the expressions in equation (51) for $r \\in\\{n u m$, den $\\}$ together with the delta method.", "tables": {}, "images": {}}], "id": "2410.14871v2", "authors": ["Sung Jae Jun", "Sokbae Lee"], "categories": ["econ.EM", "stat.ME"], "abstract": "The persuasion rate is a key parameter for measuring the causal effect of a\ndirectional message on influencing the recipient's behavior. Its identification\nhas relied on exogenous treatment or the availability of credible instruments,\nbut the requirements are not always satisfied in observational studies.\nTherefore, we develop a novel econometric framework for the average persuasion\nrate on the treated and other related parameters by using the\ndifference-in-differences approach. The average treatment effect on the treated\nis a standard parameter in difference-in-differences, but we show that it is an\noverly conservative measure in the context of persuasion. For estimation and\ninference, we propose regression-based approaches as well as semiparametrically\nefficient estimators. Beginning with the two-period case, we extend the\nframework to staggered treatment settings, where we show how to conduct richer\nanalyses like the event-study design. We investigate the British election and\nthe Chinese curriculum reform as empirical examples.", "updated": "2024-12-06T03:30:53Z", "published": "2024-10-18T21:34:10Z"}