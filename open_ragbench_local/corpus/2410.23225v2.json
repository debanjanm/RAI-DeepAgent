{"title": "Deterministic counting from coupling independence", "sections": [{"section_id": 0, "text": "#### Abstract\n\nWe show that spin systems with bounded degrees and coupling independence admit fully polynomial time approximation schemes (FPTAS). We design a new recursive deterministic counting algorithm to achieve this. As applications, we give the first FPTASes for $q$-colourings on graphs of bounded maximum degree $\\Delta \\geq 3$, when $q \\geq\\left(11 / 6-\\varepsilon_{0}\\right) \\Delta$ for some small $\\varepsilon_{0} \\approx 10^{-5}$, or when $\\Delta \\geq 125$ and $q \\geq 1.809 \\Delta$, and on graphs with sufficiently large (but constant) girth, when $q \\geq \\Delta+3$. These bounds match the current best randomised approximate counting algorithms by Chen, Delcourt, Moitra, Perarnau, and Postle (2019), Carlson and Vigoda (2024), and Chen, Liu, Mani, and Moitra (2023), respectively.", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1. InTRODUCTION\n\nThe power of randomness is a classical topic in the theory of computing. Randomised algorithms have found many early successes in the field of approximate counting. A striking example is the polynomial-time volume estimation algorithm for convex bodies by Dyer, Frieze, and Kannan [DFK91], whereas deterministic approximation algorithms requires at least exponential membership queries [Ele86, BF87]. However, this lower bound is valid only for membership query models, and does not rule out efficient deterministic approximation algorithms in general. While volume estimation remains difficult for efficient deterministic approximation, deterministic approximate counting algorithms have been quickly catching up with their randomised counterparts for many other problems, since the introduction of the correlation decay technique [Wei06, BG06].\n\nBy now, a number of different deterministic approximate counting techniques have been developed. In addition to the correlation decay method, one may utilise zero-freeness of polynomials [Bar16, PR17], linear programming based methods [Moi19, GLLZ19, JPV21], statistical physics related techniques [HPR20, JPP23, JPSS22], or even direct derandomisation of Markov chains [FGW+23]. In many occasions, these methods have achieved optimal results, or at least match or even outperform the best randomised algorithms, such as for the hardcore gas model [Wei06, PR17], for Holant problems with log-concave signatures [HLQZ25], or in the local lemma settings [WY24].\n\nDespite all these successes, there is one problem where deterministic algorithms are still lagging behind, namely counting the number of proper colourings. The study of this problem was initiated by Jerrum [Jer95], who showed a rapid mixing bound for Glauber dynamics for $q$-colourings on graphs of maximum degree $\\Delta$, when $q>2 \\Delta$. This was subsequently improved by Vigoda [Vig00] to $q>11 / 6 \\Delta$ by considering the flip dynamics. Via more careful analysis, the constant was\n\n[^0]\n[^0]:    (Xiaoyu Chen, Xinyuan Zhang, Zongrui Zou) State Key Laboratory for Novel Software Technology, New Cornerstone Science Laboratory, Nanjing University, 163 Xianlin Avenue, Nanjing, Jiangsu Province, China.\n    (Weiming Feng) School of Computing and Data Science, The University of Hong Kong, Pokfulam Road, Hong Kong, China.\n    (Heng Guo) School of Informatics, University of Edinburgh, Informatics Forum, Edinburgh, EH8 9AB, United Kingdom.\n\n    This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 947778). Weiming Feng acknowledges the support from Dr. Max R\u00f6ssler, the Walter Haefner Foundation and the ETH Z\u00fcrich Foundation during his affiliation with ETH Z\u00fcrich.\n\nthen improved to $\\left(11 / 6-\\varepsilon_{0}\\right)$ for some $\\varepsilon_{0} \\approx 10^{-5}$ by Chen, Delcourt, Moitra, Perarnau, and Postle $\\left[\\mathrm{CDM}^{+} 19\\right]$, and to 1.809 by Carlson and Vigoda [CV24] for $\\Delta \\geq 125$. In contrast, on the deterministic side, the first efficient algorithm by Gamarnik and Katz [GK07] requires $q>2.844 \\Delta$ via the correlation decay method. This was later improved by Lu and Yin [LY13] to $q>2.581 \\Delta$. With the more recent technique using zero-freeness of polynomials, Liu, Srivastava, and Sinclair [LSS19a] gave a fully polynomial time approximation schemes (FPTAS) when $q \\geq 2 \\Delta$, and this bound was very recently improved to $q \\geq\\left(2-\\varepsilon_{1}\\right) \\Delta$ for some $\\varepsilon_{1} \\approx 0.002$ by Bencs, Berrekkal, and Regts [BBR24].\n\nIn this paper, we close this gap between deterministic and randomised algorithms for approximate counting colourings. We introduce a new algorithm that takes inspiration from both the linear programming method and the correlation decay method. We show that this algorithm is efficient as long as coupling independence holds and there is a marginal lower bound. Here coupling independence is a method to establish the so-called spectral independence [ALO20, AL20], a relatively new tool to analyse mixing times of Markov chains. The notion of coupling independence is formally introduced in [CZ23], although it has been implicitly established before that, such as in [FGYZ22, BCC+22, Liu21]. Previously, coupling independence is mainly used to analyse Markov chains, and here we show that it also implies deterministic approximate counting algorithms. We also show that contractive coupling for Markov chains can be used to establish coupling independence. Thus, with our technique, the contractive couplings from $\\left[\\mathrm{CDM}^{+} 19, \\mathrm{CV} 24\\right]$ imply FPTASes with matching bounds. We describe our main results in more detail in Section 1.1, and give a high-level technical overview in Section 1.2.\n1.1. Main results. We state our main results in the general context of spin systems. A spin system is specified by the tuple $\\mathcal{S}=\\left(G, q, A_{E}, A_{V}\\right)$. Given a graph $G=(V, E)$ and an integer $q>0$, a state of the system is a configuration $\\sigma: V \\rightarrow[q]$. Namely, the state space is $[q]^{V}$. The weight of a configuration are characterised by the matrix $A_{E} \\in \\mathbb{R}_{\\geq 0}^{q \\times q}$ and vector $A_{V} \\in \\mathbb{R}_{\\geq 0}^{q}$. The Gibbs distribution $\\mu$ of $\\mathcal{S}$ is defined by\n\n$$\n\\mu(\\sigma) \\propto w(\\sigma):=\\prod_{\\{u, v\\} \\in E} A_{E}(\\sigma(u), \\sigma(v)) \\prod_{v \\in V} A_{V}(\\sigma(v))\n$$\n\nThe normalising factor of $\\mu$, namely the so-called partition function of $\\mathcal{S}$, is defined by\n\n$$\nZ:=\\sum_{\\sigma \\in[q]^{V}} w(\\sigma)\n$$\n\nWhen $A_{E}=\\left(\\begin{array}{ll}1 & 1 \\\\ 1 & 0\\end{array}\\right)$ and $A_{v}=\\binom{1}{\\lambda}$, this encodes the hardcore gas model. When $A_{E}=J-I$, where $J$ is the all-1 matrix and $I$ is the identity matrix, and $A_{v}=\\mathbf{1}$ is the all-1 vector, $\\mu$ is uniform over proper $q$-colourings, and $Z$ is the number of them.\n\nTo introduce coupling independence, we need to define Hamming and Wasserstein distances. For two configurations $\\sigma$ and $\\tau$, let their Hamming distance be\n\n$$\n\\operatorname{dist}\\{\\sigma, \\tau\\}:=\\mid\\{v \\mid v \\in V, \\sigma(v) \\neq \\tau(v)\\} \\mid\n$$\n\nThe Wasserstein distance is defined next.\nDefinition 1. Let $(\\Omega, \\mathrm{d})$ be a finite metric space. For any two distributions $\\mu$ and $\\nu$ on $\\Omega$, the 1-Wasserstein distance (W1-distance) with respect to the metric d between $\\mu$ and $\\nu$ is defined as\n\n$$\n\\mathcal{W}_{\\mathrm{d}}(\\mu, \\nu):=\\inf _{\\mathcal{C}} \\mathbb{E}_{(X, Y) \\sim \\mathcal{C}}[\\mathrm{d}(X, Y)]\n$$\n\nwhere the infimum is taken over all the possible couplings $\\mathcal{C}$ between $\\mu$ and $\\nu$.\n\nFor two $\\Omega$-valued random variables $X, Y$ with distribution $\\mu, \\nu$, we may also use $\\mathcal{W}_{\\mathrm{d}}(X, Y)$ to denote $\\mathcal{W}_{\\mathrm{d}}(\\mu, \\nu)$. When the distance d is the Hamming distance, we also omit the subscript and write $\\mathcal{W}(\\mu, \\nu)$.\nDefinition 2 (Coupling independence). We say a Gibbs distribution $\\mu$ satisfies $C$-coupling independence if, for any two partial configurations $\\sigma$ and $\\tau$ on $\\Lambda \\subseteq V$ such that $\\operatorname{dist}\\{\\sigma, \\tau\\}=1$,\n\n$$\n\\mathcal{W}\\left(\\mu^{\\sigma}, \\mu^{\\tau}\\right) \\leq C\n$$\n\nwhere $\\mu^{\\sigma}$ and $\\mu^{\\tau}$ denote the Gibbs distribution conditional on $\\sigma$ and $\\tau$, respectively.\nIn addition to coupling independence (CI), our main theorem also requires marginal lower bound. Let $\\mu_{v}^{\\sigma}$ be the marginal distribution at $v$ conditional on $\\sigma$.\nDefinition 3 (Marginal lower bound). We say a Gibbs distribution $\\mu$ over $[q]^{V}$ is $b$-marginally bounded if for any partial configuration $\\sigma \\in[q]^{\\Lambda}$ on $\\Lambda \\subseteq V$, any vertex $v \\notin \\Lambda$, any spin $c \\in[q]$ with $\\mu_{v}^{\\sigma}(c)>0$,\n\n$$\n\\mu_{v}^{\\sigma}(c) \\geq b\n$$\n\nNow we are ready to state our main theorem.\nTheorem 4. Let $q \\geq 2, b>0, C>0, \\Delta \\geq 3$ be constants. There exists a deterministic algorithm such that given a permissive spin system $\\mathcal{S}=\\left(G, q, A_{E}, A_{V}\\right)$ and error bound $0<\\varepsilon<1$, if the Gibbs distribution of $\\mathcal{S}$ is b-marginally bounded and satisfies $C$-coupling independence, and the maximum degree of $G$ is at most $\\Delta$, then it returns $\\bar{Z}$ satisfying $(1-\\varepsilon) Z \\leq \\bar{Z} \\leq(1+\\varepsilon) Z$ in time $\\left(\\frac{n}{\\varepsilon}\\right)^{f(q, b, C, \\Delta)}$, where $f(q, b, C, \\Delta)=\\Delta^{O\\left(C\\left(\\log b^{-1}+\\log C+\\log \\log \\Delta\\right)\\right)} \\log q$ is a constant.\n\nBeing permissive is a mild technical condition we need (see Definition 9). It roughly requires that all the conditional distributions are well-defined. All applications considered in this paper satisfy it. The marginal lower bound is also a mild requirement, since if we treat $q, \\Delta, A_{E}$, and $A_{V}$ all as constants, then there is a constant $b$ such that any permissive system is $b$-marginally bounded (see Observation 10). However, for concrete systems, there are usually better lower bound than the generic one in Observation 10. Thus, we choose to make it explicit in the statement of Theorem 4. We also note that the exponent $f(q, b, C, \\Delta)$ is not optimised - our goal is to present the new algorithm as simple and as clearly as possible.\n\nTheorem 4 together with coupling independence from [CLMM23] and the marginal lower bound [GKM15, Lemma 3] ${ }^{1}$ directly implies the following result.\nCorollary 5 (Colouring: high-girth graphs). Let $q$ and $\\Delta$ be two integers satisfying $\\Delta \\geq 3$ and $q \\geq \\Delta+3$. There eixsts a constant $g_{0}>0$ depending only on $\\Delta$ such that the following holds. There exists an FPTAS for the number of proper $q$-colourings on graphs $G$ of maximum degree $\\Delta$ and girth at least $g_{0}$.\n\nThe bound in Corollary 5 matches the rapid mixing result by Chen, Liu, Mani and Moitra [CLMM23]. Prior to our work, no FPTAS is known in this setting.\n\nTo establish coupling independence for colourings in general bounded degree graphs, we make use of contractive coupling, a tool typically used to bound the mixing time of Markov chains. Given two copies of a Markov chain, a contractive coupling ensures that after a step, the expected distance decreases multiplicatively. See (18) and (19) for some examples. Previously, contractive couplings have been used to establish spectral independence $\\left[\\mathrm{BCC}^{+} 22\\right.$, Liu21]. In fact, their proof implicitly established the stronger result of coupling independence. In Section 5, we give a simpler and more direct argument on how to establish coupling independence from contractive coupling. Using the\n\n[^0]\n[^0]:    ${ }^{1}$ The bound yields $b \\geq q^{-1}\\left(\\frac{2}{3}\\right)^{\\Delta}$ in the setting of Corollary 5.\n\ncontractive couplings from [CV24] and [CDM+19], as well as the marginal lower bound [LY13, Lemma 3], ${ }^{2}$ we have the following result.\n\nCorollary 6 (Colouring: general graphs). Let $q$ and $\\Delta$ be two integers satisfying either\n\n- $\\Delta \\geq 125, q \\geq 1.809 \\Delta$;\n- or $\\Delta \\geq 3, q \\geq\\left(11 / 6-\\varepsilon_{0}\\right) \\Delta$ for some fixed parameter $\\varepsilon_{0} \\approx 10^{-5}$.\n\nThere exists an FPTAS for the number of proper $q$-colourings on graphs of maximum degree $\\Delta$.\nThe bounds in Corollary 6 are the same ones as the rapid mixing of Markov chains results by Carlson and Vigoda [CV24] or by Chen, Delcourt, Moitra, Perarnau and Postle [CDM+19]. As mentioned before, prior to our work, the best FPTAS [BBR24] requires $q \\geq\\left(2-\\varepsilon_{1}\\right) \\Delta$ for some $\\varepsilon_{1} \\approx 0.002$.\n\nTheorem 4 also implies FPTASes for spin systems satisfying the Dobrushin-Shlosman condition.\nDefinition 7 (Dobrushin-Shlosman condition). Let $\\mu$ be a Gibbs distribution on $[q]^{V}$. The Dobrushin influence matrix $\\rho \\in \\mathbb{R}_{\\geq 0}^{V \\times V}$ is defined by\n\n$$\n\\forall u, v \\in V, \\rho(u, v):=\\max _{\\substack{\\sigma, \\tau \\in[q]^{V-\\tau} \\\\ \\sigma \\oplus r \\subseteq\\{u\\}}} d_{\\mathrm{TV}}\\left(\\mu_{v}^{\\sigma}, \\mu_{v}^{\\tau}\\right)\n$$\n\nwhere we use $d_{\\mathrm{TV}}(\\cdot, \\cdot)$ to denote the TV-distance. The Gibbs distribution $\\mu$ is said to satisfy the Dobrushin-Shlosman condition with gap $\\delta \\in(0,1)$ if\n\n$$\n\\|\\rho\\|_{1}=\\max _{u \\in V} \\sum_{v \\in V} \\rho(u, v) \\leq 1-\\delta\n$$\n\nCorollary 8. Let $q \\geq 2, \\Delta \\geq 3, A_{E} \\in \\mathbb{R}_{>0}^{q \\times q}, A_{V} \\in \\mathbb{R}_{>0}^{q}$, and $\\delta \\in(0,1)$ be constant parameters. There exists an FPTAS for the partition function of permissive spin systems $\\mathcal{S}=\\left(G, q, A_{E}, A_{V}\\right)$ if $\\mathcal{S}$ satisfies the Dobrushin-Shlosman condition with gap $\\delta$ and the maximum degree of $G$ is at most $\\Delta$.\n\nThe Dobrushin-Shlosman condition [Dob70, DS85] is a sufficient condition for the uniqueness of the Gibbs measure in infinite graphs. It is well-known that the Dobrushin-Shlosman condition implies rapid mixing of Glauber dynamics [BD97, Hay06]. However, before our result, its implication on deterministic counting algorithms is not well-understood, especially for general multi-spin $(q>2)$ systems.\n\nIn addition, Theorem 4 provides a unified framework to derive FPTASes for a few problems for which FPTASes are known before via different methods. Similar to our method, all these FPTASes require a bounded maximum degree $\\Delta$ for the input graph. Examples in this category include $q$-colourings for triangle-free graphs if $q>1.764 \\Delta+C$ for some constant $C>0$ [LSS19a] (where CI is established in [FGYZ22, CGSV21, CF24]), antiferromagnetic two-state spin systems in the uniqueness regime [SST14] (where CI is established in [CF24, CLV23]), ferromagnetic Ising models with non-zero external fields [LSS19b] (where CI is established in [CZ23]), and Holant problems with log-concave signatures [HLQZ25] (where CI is established in [CG24]). ${ }^{3}$\n\nAfter this paper appeared on the arXiv, Chen, Wang, Zhang, and Zhang [CWZZ25] used Theorem 4 to give an FPTAS for edge colourings when $q \\geq 3 \\Delta$.\n\n[^0]\n[^0]:    ${ }^{2}$ The bound yields $b \\geq q^{-1} e^{-\\frac{1}{\\alpha-1}}$ in the setting of Corollary 6 , where $\\alpha$ is the constant in the assumption $q \\geq \\alpha \\Delta$.\n    ${ }^{3}$ Technically Holant problems are not spin systems, but our technique generalises to that setting easily.\n\n1.2. Our technique. We first show that coupling independence implies decay of total influences. Here, the influence from $u$ to $v$ is defined as $d_{\\mathrm{TV}}\\left(\\mu_{v}^{\\sigma}, \\mu_{v}^{\\tau}\\right)$ such that $\\sigma$ and $\\tau$ are partial configuations that differ only at $u$. Note that this is different from Doburshin's influence defined in (3). See Definition 11 for the formal definition of influences. Suppose $C$-coupling independence holds. By a simple averaging argument, there is $1 \\leq \\ell \\leq 2 C$ such that the total influence of a vertex $v$ to vertices of distance $\\ell$ from $v$ is at most $1 / 2$. We repeatedly use this fact to show that, roughly, for every $2 C$ distance, the total influence decays multiplicatively by a factor $1 / 2$, which leads to an exponential decay in the distance from $v$. This is in Section 3. Given the decay of total influence, we may choose a sufficiently large but constant $R$ such that the total influence at distance $R$ is less than a quantity that is linear in $R$. (To be precise, the requirement is given in (10).) This choice is a key guarantee in our error analysis.\n\nOur main algorithm estimates marginal probabilities of a vertex $v$ under arbitrary conditioning. The basic building block of our algorithm does this task but requires a constant number of other conditional marginal probabilities. These marginal probabilities are either of the same vertex $v$ but with different boundary conditions at distance $R$, or of some other vertices at distance $R$. We use the aforementioned bounds for $R$ to show that the output relative error is roughly half of the relative error of the input marginals. Thus, we achieved a recursive step with constant error decay. To achieve an $\\varepsilon$ relative error, we just need to run this up to $\\log \\varepsilon^{-1}$ depth, resulting in polynomial total running time.\n\nOn a high level, the structure of our algorithm is very similar to the correlation decay algorithms such as [Wei06, BG06]. Namely, in each step, we use the marginal probabilities in smaller instances to compute the desired one, and the computation is truncated at logarithmic depth. However, a key difference is that in previous algorithms, the recursive step is exact and often via a closed form formula, whereas ours is algorithmic and approximate. This is achieved via a linear programming (LP) algorithm inspired by [Moi19]. We use the LP to simulate a coupling similar to the one in [CLMM23], which is to couple vertices randomly chosen at distance $R$ one at a time. However, the coupling in [CLMM23] is recursive, and it appears difficult for the LP to handle recursion. We instead only use the LP to simulate a partial coupling up to the recursive point. Because this is a partial coupling, certain quantities in the LP cannot be computed efficiently. We rewrite these quantities in terms of marginal probabilities of smaller instances, and use recursion to solve this issue. To control the evolution of relative errors, we need to find some new linear constraints that can be computed efficiently and characterise the failure probability of the coupling process. These constraints, together with our choice of the radius $R$, ensure that the relative error decay by a constant factor each time. The description and analysis of our algorithm are given in Section 4. For readers not familiar with Moitra's approach, we also provide some heuristics and intuition behind it in Appendix A.\n\nComparing with all previous instantiation of the LP-based approximate counting algorithm [Moi19, GLLZ19, JPV21, WY24, HLQZ25], we do not write a polynomial-sized LP to solve. Instead, our LP is only of constant size, but we recursively construct polynomially many of them. Technically, because each of our sub-instances has only constant size, we are not obliged to use LP. (For example, we could write a quadratic system to solve.) We choose LP just for technical convenience. Moreover, because the (partial) coupling we use the LP to simulate has not been considered in this context before, we need to write a new set of constraints to certify this coupling.\n\nThe coupling in [CLMM23] establishes coupling independence for high-girth graphs when $q \\geq$ $\\Delta+3$. Thus our algorithm works in this setting, resulting in Corollary 5. To apply our algorithm on general bounded degree graphs, we still need to establish coupling independence. To this end, we show that contractive coupling for Markov chains (which is the main technique behind the rapid mixing results of $\\left[\\mathrm{CDM}^{+} 19\\right]$ and [CV24]) can be used to establish coupling independence. This argument is given in Section 5.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 2. Preliminaries \n\nLet $\\mu$ over $[q]^{V}$ be a Gibbs distribution on $G=(V, E)$ with matrix $A_{E}$ and vector $A_{V}$. Recall that $\\mu$ is defined by for any configuration $\\sigma \\in[q]^{V}$,\n\n$$\n\\mu(\\sigma)=\\frac{w(\\sigma)}{Z}, \\text { where } w(\\sigma)=\\prod_{\\{u, v\\} \\in E} A_{E}(\\sigma(u), \\sigma(v)) \\prod_{v \\in V} A_{V}(\\sigma(v)) \\text { and } Z=\\sum_{\\tau \\in[q]^{V}} w(\\tau)\n$$\n\nWe call $w(\\sigma)$ the weight of $\\sigma$ and $Z$ the partition function.\nFor any subset $S \\subseteq V$, we use subscript $S$ to denote the marginal distribution, namely, for any $\\tau \\in[q]^{S}, \\mu_{S}(\\tau):=\\sum_{\\tau^{\\prime} \\in[q]^{V \\backslash S}} \\mu\\left(\\tau \\cup \\tau^{\\prime}\\right)$. When $S=\\{v\\}$ is a singleton set, we may also write $\\mu_{v}$ instead of $\\mu_{\\{v\\}}$.\n\nLet $\\sigma \\in[q]^{V \\backslash \\Lambda}$ be a partial configuration on $V \\backslash \\Lambda$, where $\\sigma$ is allowed to be infeasible, namely we allow $\\mu_{V \\backslash \\Lambda}(\\sigma)=0$. For any $\\tau \\in[q]^{V}$, define the conditional weight\n\n$$\nw^{\\sigma}(\\tau)=\\mathbf{1}_{\\tau(V \\backslash \\Lambda)=\\sigma} \\cdot \\prod_{v \\in \\Lambda} A_{V}(\\tau(v)) \\prod_{\\{u, v\\} \\in E: u \\in \\Lambda \\wedge v \\notin \\Lambda} A_{E}(\\tau(u), \\sigma(v)) \\prod_{\\{u, v\\} \\in E: u \\in \\Lambda \\wedge v \\in \\Lambda} A_{E}(\\tau(u), \\tau(v))\n$$\n\nDefine the conditional distribution $\\mu^{\\sigma}$ over $[q]^{V}$ by for any $\\tau \\in[q]^{V}$,\n\n$$\n\\mu^{\\sigma}(\\tau)=\\frac{w^{\\sigma}(\\tau)}{Z^{\\sigma}}, \\text { where } Z^{\\sigma}=\\sum_{\\tau \\in[q]^{V}} w^{\\sigma}(\\tau)\n$$\n\nThe above definition works for all partial configurations $\\sigma$. In particular, if $\\sigma$ is a feasible partial configuration, then $\\mu^{\\sigma}$ is the distribution $\\mu$ conditional on $\\sigma$. Let $\\mu_{S}^{\\sigma}$ denote the marginal distribution on $S$ conditional on the partial configuration $\\sigma$.\n\nAs mentioned before, when $A_{E}=J-I$, where $J$ is the all-1 matrix and $I$ is the identity matrix, and $A_{v}=\\mathbf{1}$ is the all-1 vector, $\\mu$ is uniform over proper $q$-colourings, and $Z$ is the number of them. For technical purposes, we also need to consider list colourings later, where each $v$ may have a list $L_{v}$ of available colours, and no edge can be monochromatic. This can also be modelled as a spin system, with $q$ being the total number of possible colours of all vertices, and $A_{v}$ encoding what colours are available for $v$. Let $\\mu$ be the uniform distribution over all proper colourings. Our list colouring instances in fact come from $\\mu^{\\sigma}$ for some partial configuration $\\sigma$. This is because fixing a colour $c$ at a vertex $v$ is equivalent to removing the vertex $v$ and removing $c$ from the lists of all neighbours of $v$. Note that if $q-\\Delta \\geq k$ for some $k$, then for any list colouring instances obtained this way, for any $v,\\left|L_{v}\\right|-\\operatorname{deg}_{G}(v) \\geq k$ as well.\n\nMore generally, in this paper, we consider the permissive spin systems so that all $\\mu^{\\sigma}$ are welldefined.\n\nDefinition 9. A spin system $\\mu$ is permissive if for any $\\Lambda \\subseteq V$, any $\\sigma \\in[q]^{\\Lambda}, Z^{\\sigma}>0$.\nRemark. All spin systems with soft constraints $\\left(A_{E}(i, j)>0\\right.$ and $A_{V}(i)>0$ for all $\\left.i, j \\in[q]\\right)$ are permissive. Many natural spin systems with hard constraints are also permissive. For example, the hardcore model and any list colouring instance $(G, L)$ such that $\\left|L_{v}\\right| \\geq \\operatorname{deg}_{G}(v)+1$ for any $v \\in V$ are permissive.\n\nNote that in Definition 9, $\\sigma$ is allowed to be infeasible, as $w^{\\sigma}$ does not consider the weight contributed from inside $\\Lambda$. This allows $\\mu^{\\sigma}$ to be well-defined, even for infeasible $\\sigma$. Our definition of coupling independence, Definition 2, indeed allows infeasible partial configurations. Essentially, for any (feasible or infeasible) $\\sigma$ on $\\Lambda \\subset V$, only the values on the boundary of $\\Lambda$ matters for $\\mu^{\\sigma}$, and the values inside $\\Lambda$ do not.\n\nOn the other hand, for any partial configuration $\\sigma$ on $\\Lambda$, if $\\sigma$ is locally feasible, then $\\sigma$ is also globally feasible. Formally,\n\n$$\n\\prod_{v \\in \\Lambda} A_{V}(\\sigma(v)) \\prod_{\\{u, v\\} \\in E: u \\in \\Lambda \\wedge v \\in \\Lambda} A_{E}(\\sigma(u), \\sigma(v))>0 \\quad \\Longrightarrow \\quad \\mu_{\\Lambda}(\\sigma)>0\n$$\n\nWe use $\\operatorname{supp}\\left(\\mu_{S}^{\\sigma}\\right)$ to denote the support of $\\mu_{S}^{\\sigma}$. Formally,\n\n$$\n\\operatorname{supp}\\left(\\mu_{S}^{\\sigma}\\right):=\\left\\{\\tau \\in[q]^{S} \\mid \\mu_{S}^{\\sigma}(\\tau)>0\\right\\}\n$$\n\nNote that by (5), the set $\\operatorname{supp}\\left(\\mu_{S}^{\\sigma}\\right)$ is easy to compute as we only need to consider local assignments.\nLet us also observe that permissive systems always have a marginal lower bound.\nObservation 10. Let $q, \\Delta, A_{E}$, and $A_{V}$ be constants. Then there is a constant $b=b_{q, \\Delta, A_{E}, A_{V}}$ such that for any $G$ with maximum degree $\\Delta$, any permissive spin system $\\left(G, q, A_{E}, A_{V}\\right)$ is $b$-marginally bounded.\nProof. Given a partial configuration $\\sigma$ on $\\Lambda$ and a vertex $v \\in V \\backslash \\Lambda$, let $S$ be the set of 2-hop neighbours of $v$ that is not in $\\Lambda$. For any $c \\in \\operatorname{supp}\\left(\\mu_{v}^{\\sigma}\\right)$, we have that\n\n$$\n\\mu_{v}^{\\sigma}(c)=\\sum_{\\tau \\in \\operatorname{supp}\\left(\\mu_{S}^{\\sigma}\\right)} \\mu_{S}^{\\sigma}(\\tau) \\mu_{v}^{\\sigma \\cup \\tau}(c)\n$$\n\nFor any $c \\in \\operatorname{supp}\\left(\\mu_{v}^{\\sigma}\\right)$ and $\\tau \\in \\operatorname{supp}\\left(\\mu_{S}^{\\sigma}\\right)$, as $S$ is not adjacent to $v$ and the system is permissive, $\\mu_{v}^{\\sigma \\cup \\tau}(c)>0$. As both $\\operatorname{supp}\\left(\\mu_{S}^{\\sigma}\\right)$ and $\\operatorname{supp}\\left(\\mu_{v}^{\\sigma}\\right)$ are finite, there is a minimum of $\\mu_{v}^{\\sigma \\cup \\tau}(c)$, say $b$, over all choices of $\\tau \\in \\operatorname{supp}\\left(\\mu_{S}^{\\sigma}\\right)$ and $c \\in \\operatorname{supp}\\left(\\mu_{v}^{\\sigma}\\right)$. Note that this $b$ may depend on $q, \\Delta, A_{E}$, and $A_{V}$. It implies that\n\n$$\n\\mu_{v}^{\\sigma}(c) \\geq b \\sum_{\\tau \\in \\operatorname{supp}\\left(\\mu_{S}^{\\sigma}\\right)} \\mu_{S}^{\\sigma}(\\tau)=b\n$$", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 3. TOTAL INFLUENCE DECAY \n\nGiven a graph $G=(V, E)$, let $B_{\\ell}(v)$ denote the ball of radius $\\ell$ centered at $v$. Namely, $B_{\\ell}(v):=$ $\\{u \\mid u \\in V, \\operatorname{dist}_{G}(u, v) \\leq \\ell\\}$, where the distance is graph distance in $G$. Let $S_{\\ell}(v)$ denote the sphere of radius $\\ell$ centered at $v$, namely, $S_{\\ell}(v):=\\{u \\mid u \\in V, \\operatorname{dist}_{G}(u, v)=\\ell\\}$. In other words, $S_{\\ell+1}(v)=\\partial B_{\\ell}(v)$, where $\\partial B_{\\ell}(v)$ denote the out-boundary of $B_{\\ell}(v)$.\nDefinition 11 (Total influence decay). Let $\\delta: \\mathbb{N} \\rightarrow \\mathbb{R}$ be a non-increasing function. We say a Gibbs distribution $\\mu$ satisfies total influence decay with rate $\\delta$ if for any two partial configurations $\\sigma$ and $\\tau$ on $\\Lambda \\subseteq V$ such that they disagree only on some $v \\in \\Lambda$, for any integer $\\ell>0$,\n\n$$\n\\sum_{u \\in S_{\\ell}(v)} d_{\\mathrm{TV}}\\left(\\mu_{u}^{\\sigma}, \\mu_{u}^{\\tau}\\right) \\leq \\delta(\\ell)\n$$\n\nThe influence defined in (6) was often used in recent works for spectral independence [ALO20, FGYZ22, CGSV21]. The definition in (6) should be distinguished from the definition of Dobrushin's influence in (3). The following theorem shows that the coupling independence implies total influence decay.\nTheorem 12. Suppose $\\mu$ satisfies $C$-coupling independence. Let $R>0$ be an integer. For any two partial configurations $\\sigma$ and $\\tau$ on $\\Lambda \\subseteq V$ such that they disagree only on some $v \\in \\Lambda$, there is a coupling $\\mathcal{C}$ between $\\mu^{\\sigma}$ and $\\mu^{\\tau}$ such that\n\n$$\n\\mathbb{E}_{\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\sim \\mathcal{C}}\\left[\\sum_{u \\in S_{R}(v)} \\mathbf{1}_{\\sigma^{\\prime}(u) \\neq \\tau^{\\prime}(u)}\\right] \\leq 2 C\\left(\\frac{1}{2}\\right)^{\\left\\lceil\\frac{R}{2 C}\\right\\rceil}\n$$\n\nAs a consequence, $\\mu$ satisfies total influence decay with rate $\\delta(x)=2 C \\cdot 2^{-\\left\\lceil\\frac{\\sigma}{2 C}\\right\\rceil}$.\n\nProof. Let $T:=\\left\\lceil\\frac{R}{2 C}\\right\\rceil$. We construct $\\mathcal{C}$ via a $T$-step coupling procedure as follows. The whole procedure will generate a random pair $X, Y \\in[q]^{V}$ such that $X \\sim \\mu^{\\sigma}$ and $Y \\sim \\mu^{\\tau}$.\n\nStep 1. Since $\\mu$ satisfies $C$-coupling independence, there exists a coupling $\\mathcal{C}_{1}$ of $\\mu^{\\sigma}$ and $\\mu^{\\tau}$ such that $\\sum_{w \\in V} \\operatorname{Pr}_{\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\sim \\mathcal{C}_{1}}\\left[\\sigma^{\\prime}(w) \\neq \\tau^{\\prime}(w)\\right] \\leq C$. By an averaging argument, there exists an integer $1 \\leq \\ell(1) \\leq 2 C$ such that the expected disagreement on the sphere $S_{\\ell(1)}(v)$ is at most $\\frac{C}{2 C}=\\frac{1}{2}$, i.e.,\n\n$$\n\\sum_{w \\in S_{\\ell(1)}(v)} \\operatorname{Pr}_{\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\sim \\mathcal{C}_{1}}\\left[\\sigma^{\\prime}(w) \\neq \\tau^{\\prime}(w)\\right] \\leq \\frac{1}{2}\n$$\n\nIf there are multiple choices of such $\\ell(1)$, pick the smallest one. Denote $B_{1}=B_{\\ell(1)}(v)$ and $S_{1}=$ $S_{\\ell(1)}(v)$. We use the coupling $\\mathcal{C}_{1}$ to draw a pair of random configurations and project them to the set $D=B_{1} \\backslash \\Lambda$. We then extend $\\sigma, \\tau$ on $\\Lambda$ to a pair of random configurations $X, Y$ on $\\Lambda \\cup B_{1}$. Formally, $X(\\Lambda)=\\sigma, Y(\\Lambda)=\\tau$, and\n\n$$\nX(D) \\sim \\mu_{D}^{\\sigma}, Y(D) \\sim \\mu_{D}^{\\tau}, \\text { and } \\mathbb{E}\\left[\\operatorname{dist}\\left\\{X\\left(S_{1}\\right), Y\\left(S_{1}\\right)\\right\\}\\right] \\leq \\frac{1}{2}\n$$\n\nStep $k$ for $1<k<T$. Suppose we have obtained a pair of partial configurations $X$ and $Y$ on $B_{k-1} \\cup \\Lambda$, where $B_{k-1}=B_{\\ell(k-1)}(v)$ and $\\ell(k-1) \\geq 1$ is an integer. Next we extend the two configurations $X, Y$ onto a larger set $B_{k} \\cup \\Lambda$, where $B_{k}=B_{\\ell(k)}(v)$ for some $\\ell(k)>\\ell(k-1)$. Let $H=V \\backslash B_{k-1}$ be the set of vertices outside the ball $B_{k-1}$. Let $\\Lambda_{H}=\\Lambda \\cap H$. By conditional independence, $\\mu_{H}^{X}=\\mu_{H}^{X\\left(\\Lambda_{H} \\cup S_{k-1}\\right)}$ and $\\mu_{H}^{Y}=\\mu_{H}^{Y\\left(\\Lambda_{H} \\cup S_{k-1}\\right)}$. The two pinnings $X\\left(\\Lambda_{H} \\cup S_{k-1}\\right)$ and $Y\\left(\\Lambda_{H} \\cup S_{k-1}\\right)$ can disagree only at the set $S_{k-1}=S_{\\ell(k-1)}(v)$, the sphere of radius $\\ell(k-1)$ centered at $v$. Suppose the disagreements of $X\\left(S_{k-1}\\right)$ and $Y\\left(S_{k-1}\\right)$ can be listed as $v_{1}, v_{2}, \\ldots, v_{m} \\in S_{k-1}$, where $m=\\operatorname{dist}\\left\\{X\\left(S_{k-1}\\right), Y\\left(S_{k-1}\\right)\\right\\}$. We can define a sequence of pinnings $\\sigma_{0}, \\sigma_{1}, \\ldots, \\sigma_{m}$ on $S_{k-1} \\cup \\Lambda_{H}$ such that $\\sigma_{0}=X\\left(\\Lambda_{H} \\cup S_{k-1}\\right), \\sigma_{i}$ is obtained from $\\sigma_{i-1}$ by changing the value at $v_{i}$ from $X\\left(v_{i}\\right)$ to $Y\\left(v_{i}\\right)$, and so $\\sigma_{m}=Y\\left(\\Lambda_{H} \\cup S_{k-1}\\right)$. By the coupling independence, for any $i \\in[m]$, one can couple $\\mu^{\\sigma_{i}}$ and $\\mu^{\\sigma_{i-1}}$ such that the expected hamming distance is at most $C .{ }^{4}$ By the triangle inequality of the Wasserstein distance, we can couple $\\mu^{\\sigma_{0}}$ and $\\mu^{\\sigma_{m}}$ so that the expected Hamming distance is at most $m C$. By projecting this coupling into the subset $H$, we obtain a coupling $\\mathcal{C}_{k}$ of $\\mu_{H}^{X}=\\mu_{H}^{\\sigma_{0}}$ and $\\mu_{H}^{Y}=\\mu_{H}^{\\sigma_{m}}$ such that $\\mathbb{E}_{\\left(\\sigma_{k}, \\tau_{k}\\right) \\sim \\mathcal{C}_{k}}\\left[\\operatorname{dist}\\left\\{\\sigma_{k}, \\tau_{k}\\right\\}\\right] \\leq m C$. Again, by an averaging argument, there exists $\\ell(k)$ such that $\\ell(k-1)+1 \\leq \\ell(k) \\leq \\ell(k-1)+2 C$ and\n\n$$\n\\sum_{w \\in S_{\\ell(k)}(v)} \\operatorname{Pr}_{\\left(\\sigma_{k}, \\tau_{k}\\right) \\sim \\mathcal{C}_{k}}\\left[\\sigma_{k}(w) \\neq \\tau_{k}(w)\\right] \\leq \\frac{m C}{2 C}=\\frac{m}{2}=\\frac{\\operatorname{dist}\\left\\{X\\left(S_{k-1}\\right), Y\\left(S_{k-1}\\right)\\right\\}}{2}\n$$\n\nIf there are multiple choices of such $\\ell(k)$, pick the smallest one. Let $B_{k}=B_{\\ell(k)}(v)$ and $S_{k}=$ $S_{\\ell(k)}(v)$. Similar to Step 1, we use the coupling $\\mathcal{C}_{k}$ to draw a pair of configurations $\\left(\\sigma_{k}, \\tau_{k}\\right)$. Let $D_{k}=B_{k} \\backslash\\left(B_{k-1} \\cup \\Lambda\\right)$. We further extend $X, Y$ to the set $D_{k}$ by setting $X\\left(D_{k}\\right)=\\sigma_{k}\\left(D_{k}\\right)$ and $Y\\left(D_{k}\\right)=\\tau_{k}\\left(D_{k}\\right)$. We have\n\n$$\nX\\left(D_{k}\\right) \\sim \\mu_{D_{k}}^{X\\left(B_{k-1} \\cup \\Lambda\\right)}, Y\\left(D_{k}\\right) \\sim \\mu_{D_{k}}^{Y\\left(B_{k-1} \\cup \\Lambda\\right)}\n$$\n\nIn other words, $X$ and $Y$ are now partial configurations on set $B_{k-1} \\cup \\Lambda \\cup D_{k}=B_{k} \\cup \\Lambda$. It holds that\n(8) $\\mathbb{E}\\left[\\operatorname{dist}\\left\\{X\\left(S_{k}\\right), Y\\left(S_{k}\\right)\\right\\} \\mid X\\left(B_{k-1} \\cup \\Lambda\\right), Y\\left(B_{k-1} \\cup \\Lambda\\right), \\ell(k-1)\\right] \\leq \\frac{\\operatorname{dist}\\left\\{X\\left(S_{k-1}\\right), Y\\left(S_{k-1}\\right)\\right\\}}{2}$.\n\nStep $T$. The last step is similar to the general step $k$. Assume $X, Y$ are two fixed configurations on $B_{T-1} \\cup \\Lambda$, where $B_{T-1}=B_{\\ell(T-1)}(v)$ and $\\ell(T-1)$ is a fixed integer. Let $H=V \\backslash B_{T-1}$. Let $\\mathcal{C}_{T}$ be the coupling obtained in the same way as the general step from the triangle inequality. We use\n\n[^0]\n[^0]:    ${ }^{4}$ Two pinnings $\\sigma_{i}, \\sigma_{i-1}$ can be improper partial configurations but conditional distributions $\\mu^{\\sigma_{i}}$ and $\\mu^{\\sigma_{i-1}}$ are defined in (4). The coupling independence condition holds for all possible pinnings including infeasible ones.\n\nthe coupling $\\mathcal{C}_{T}$ to sample a pair of configurations $\\left(\\sigma_{T}, \\tau_{T}\\right)$. We then further extend $X, Y$ to the set $D_{T}=V \\backslash\\left(B_{T-1} \\cup \\Lambda\\right)$ by setting $X\\left(D_{T}\\right)=\\sigma^{\\prime}\\left(D_{T}\\right)$ and $Y\\left(D_{T}\\right)=\\tau^{\\prime}\\left(D_{T}\\right)$. We have\n\n$$\nX\\left(D_{T}\\right) \\sim \\mu_{D_{T}}^{X\\left(B_{T-1} \\cup \\Lambda\\right)}, Y\\left(D_{T}\\right) \\sim \\mu_{D_{T}}^{Y\\left(B_{T-1} \\cup \\Lambda\\right)}\n$$\n\nNote that $\\ell(T-1) \\leq 2 C(T-1)=2 C\\left(\\left\\lceil\\frac{H}{2 C}\\right\\rceil-1\\right)<R$, so that $S_{R}(v) \\subseteq H$. It holds that\n\n$$\n\\begin{gathered}\n\\mathbb{E}\\left[\\operatorname{dist}\\left\\{X\\left(S_{R}(v)\\right), Y\\left(S_{R}(v)\\right)\\right\\} \\mid X\\left(B_{T-1} \\cup \\Lambda\\right), Y\\left(B_{T-1} \\cup \\Lambda\\right), \\ell(T-1)\\right] \\\\\n\\leq C \\cdot \\operatorname{dist}\\left\\{X\\left(S_{T-1}\\right), Y\\left(S_{T-1}\\right)\\right\\}\n\\end{gathered}\n$$\n\nThe above inequality holds because the expected disagreement on $S_{R}(v)$ is at most the total expected disagreement produced by $\\mathcal{C}_{T}$, which is at most $C \\cdot \\operatorname{dist}\\left\\{X\\left(S_{T-1}\\right), Y\\left(S_{T-1}\\right)\\right\\}$.\n\nWe now show that the above procedure is a valid coupling between $\\mu^{\\sigma}$ and $\\mu^{\\tau}$. The procedure can be viewed as follows. Initially, $X$ and $Y$ are partial configurations on $D_{0}=\\Lambda$ with $X=\\sigma$ and $Y=\\tau$. In the $k$-th step, we extend $X, Y$ to a new set $D_{k}$. Hence $X, Y$ are partial configurations on $\\cup_{i \\leq k} D_{i}$ after the $k$-th step. The tricky part here is that all $D_{1}, D_{2}, \\ldots, D_{T}$ are random variables. The following property is the key to proving the correctness of the coupling: for every step $1 \\leq k \\leq T$, the coupling satisfies\n\n- given $\\left(D_{i}\\right)_{i<k}, X\\left(D_{0: k-1}\\right)$, and $Y\\left(D_{0: k-1}\\right)$, the set $D_{k}$ is fixed, where $D_{0: k-1}=\\cup_{0 \\leq i \\leq k-1} D_{i}$;\n- $X\\left(D_{k}\\right) \\sim \\mu_{D_{k}}^{X\\left(D_{0: k-1}\\right)}$ and $Y\\left(D_{k}\\right) \\sim \\mu_{D_{k}}^{Y\\left(D_{0: k-1}\\right)}$.\n\nThis property is easy to verify from the construction of $X$ and $Y$.\nNext we show $X \\sim \\mu^{\\sigma}$. A similar proof shows that $Y \\sim \\mu^{\\tau}$. In every step, we sample $X\\left(D_{k}\\right)$ and $Y\\left(D_{k}\\right)$ jointly from a coupling. This can be viewed as a two-step process. We first sample $X\\left(D_{k}\\right)$, and given the value of $X\\left(D_{k}\\right)$, the coupling specifies a conditional distribution of $Y\\left(D_{k}\\right)$. Then we sample $Y\\left(D_{k}\\right)$ from this conditional distribution using independent randomness $\\mathcal{R}_{k}$ (e.g., $\\mathcal{R}_{k}$ can be a uniform real number in $(0,1)$ ). We fix the randomness $\\mathcal{R}_{1}, \\mathcal{R}_{2}, \\ldots, \\mathcal{R}_{T}$. For any configuration $\\rho \\in[q]^{V}$, we compute the probability that $X=\\rho$. Initially, $D_{0}=\\Lambda, X\\left(D_{0}\\right)=\\sigma$ and $Y\\left(D_{0}\\right)=\\tau$. Hence, $X=\\rho$ only if $\\rho(\\Lambda)=\\sigma$. In the first step, by the property above, $D_{1}$ is fixed, and $X=\\rho$ implies that $X\\left(D_{1}\\right)=\\rho\\left(D_{1}\\right)$, which happens with probability $\\mu_{D_{1}}^{\\rho\\left(D_{0}\\right)}\\left(\\rho\\left(D_{1}\\right)\\right)$. Also, $Y\\left(D_{1}\\right)$ is fixed because $X\\left(D_{1}\\right)=\\rho\\left(D_{1}\\right)$ and $\\mathcal{R}_{1}$ is fixed. By induction, in the $k$-th step, $D_{k}$ is fixed, $X\\left(D_{k}\\right)=\\rho\\left(D_{k}\\right)$ with probability $\\mu_{D_{k}}^{\\rho\\left(D_{0: k-1}\\right)}\\left(\\rho\\left(D_{k}\\right)\\right)$, and $Y\\left(D_{k}\\right)$ is fixed by $X\\left(D_{k}\\right)$ and $\\mathcal{R}_{k}$. By the chain rule,\n\n$$\n\\operatorname{Pr}\\left[X=\\rho \\mid \\mathcal{R}_{1}, \\mathcal{R}_{2}, \\ldots, \\mathcal{R}_{T}\\right]=\\mathbf{1}_{\\rho(\\Lambda)=\\sigma} \\cdot \\prod_{k=1}^{T} \\mu_{D_{k}}^{\\rho\\left(D_{0: k-1}\\right)}\\left(\\rho\\left(D_{k}\\right)\\right)=\\mu^{\\sigma}(\\rho)\n$$\n\nTaking expectation over $\\mathcal{R}_{1}, \\mathcal{R}_{2}, \\ldots, \\mathcal{R}_{T}$ in both sides shows that $X \\sim \\mu^{\\sigma}$.\nFinally, using (7), (8), and (9), the expected disagreement on $S_{R}(v)$ can be bounded as\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\operatorname{dist}\\left\\{X\\left(S_{R}(v)\\right), Y\\left(S_{R}(v)\\right)\\right\\}\\right] & \\leq C \\mathbb{E}\\left[\\operatorname{dist}\\left\\{X\\left(S_{T-1}\\right), Y\\left(S_{T-1}\\right)\\right\\}\\right] \\\\\n& \\leq \\frac{C}{2^{T-2}} \\mathbb{E}\\left[\\operatorname{dist}\\left\\{X\\left(S_{1}\\right), Y\\left(S_{1}\\right)\\right\\}\\right] \\\\\n& \\leq \\frac{C}{2^{T-1}}\n\\end{aligned}\n$$\n\nThe total influence decay consequence follows from the coupling inequality.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 4. A recursive marginal estimator \n\nWe show Theorem 4 in this section. As in the condition of Theorem 4, throughout the section, we assume that the underlying graphs $G$ for the Gibbs distributions have constant maximum degree $\\Delta$. Moreover, we assume a marginal lower bound $0<b<1$ as in Definition 3, and exponential total influence decay, namely the bound in (6) with $\\delta(\\ell)=\\exp (-\\Omega(\\ell))$.\n\nOur algorithm requires a parameter $R$, the radius at which we couple vertices. For an integer $k \\geq 0$, let $H(k)$ be the harmonic sum defined by $H(k):=\\sum_{i=1}^{k} \\frac{1}{i}$ with the convention that $H(0)=0$. We choose a sufficiently large integer $R$ such that\n\n$$\n30 \\delta(R) H\\left(\\Delta^{R}\\right)<b^{4}\n$$\n\nBy Theorem 12, we can take $\\delta(R)=2 C 2^{-\\left\\lceil\\frac{R}{2 C}\\right\\rceil}$, where $C$ is the coupling independence constant. Since $H\\left(\\Delta^{R}\\right)=O(R \\log \\Delta)$, some $R=O\\left(C\\left(\\log b^{-1}+\\log C+\\log \\log \\Delta\\right)\\right)$ suffices.\n\nIn Section 4.1, we introduce an LP based algorithm that takes as inputs estimated ratios of marginal probabilities of some partial configurations, and outputs an estimation of the marginal ratio of a particular vertex, with a better approximation guarantee. The error analysis of this algorithm is given in Section 4.2. These input ratios can be written as the product of two ratios, each of which is regarding a single vertex. Thus, we have a recursive algorithm to estimate the marginal probability, described in Section 4.3.\n4.1. Marginal estimation via linear programming. Suppose we want to estimate the marginal probability of some vertex $u$. It is equivalent to approximate the ratio between marginals where any two different values are assigned to $u$. The basic building block of our algorithm is an estimator which takes estimations of marginal ratios of certain partial configurations, and outputs an estimation of the marginal ratio for $u$ with a better approximation guarantee. We construct a linear program, similar to the one used by Moitra [Moi19], to certify a coupling between assigning $u$ to two different values. However, the \"coupling\" we choose is different, and is inspired by the one used by Chen, Liu, Mani, and Moitra [CLMM23]. For readers not familiar with Moitra's approach, we provide some heuristics and intuition behind it in Appendix A.\n\nGiven two partial configurations $\\sigma$ and $\\tau$ on $\\Lambda \\subseteq V$ which differ at only one vertex $u \\in \\Lambda$, we want to couple $\\mu^{\\sigma}$ and $\\mu^{\\tau}$ by coupling vertices in $S_{R}(u)=\\partial B_{R}(u)$ for some radius $R>0$. We choose a vertex $v$ from $S_{R}(u)$ uniformly at random and couple it optimally (in the sense of TV distance) between its two marginal distributions. If the coupling failed, we immediately stop the whole process, and otherwise we continue to couple the next randomly chosen vertex. The process also ends when all of $S_{R}(u)$ has been considered. One may have noticed that this is not a complete coupling, but rather a partial one. Formally it is described in Algorithm 1. For $\\sigma \\in[q]^{\\Lambda}$ and $v \\notin \\Lambda$, the notation $\\sigma_{v \\leftarrow c}$ means a partial configuration which agrees with $\\sigma$ on $\\Lambda$ and assigns $c$ to $v$.\n\n```\nAlgorithm 1: The partial coupling\n1 Partial-Coupling \\(\\left.(\\sigma, \\tau)\\right)\\)\n    Input : Partial configurations \\(\\sigma, \\tau \\in[q]^{\\Lambda}\\) that only differ at a vertex \\(u \\in \\Lambda\\)\n    Output : A pair of partial configurations \\(\\sigma^{\\prime}\\) and \\(\\tau^{\\prime}\\) over some \\(\\Lambda^{\\prime} \\supseteq \\Lambda\\)\n    \\(\\sigma^{\\prime} \\leftarrow \\sigma, \\tau^{\\prime} \\leftarrow \\tau\\)\n    3 while \\(S_{R}(u) \\backslash \\Lambda \\neq \\emptyset\\) do\n        Choose \\(v\\) from \\(S_{R}(u) \\backslash \\Lambda\\) uniformly at random;\n        Draw \\(\\left(c_{1}, c_{2}\\right)\\) from the optimal coupling between \\(\\mu_{v}^{\\sigma^{\\prime}}\\) and \\(\\mu_{v}^{\\tau^{\\prime}} ;\\)\n        \\(\\sigma^{\\prime} \\leftarrow \\sigma_{v \\leftarrow c_{1}}^{\\prime}, \\tau^{\\prime} \\leftarrow \\tau_{v \\leftarrow c_{2}}^{\\prime} ;\\)\n        \\(\\Lambda \\leftarrow \\Lambda \\cup\\{v\\}\\)\n        if \\(c_{1} \\neq c_{2}\\) then\n            \\(\\left\\lfloor\\begin{array}{l}\\text { return }\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) ;\\end{array}\\)\n    return \\(\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right)\\)\n```\n\nNote that our algorithm does not really need to construct the partial coupling described in Algorithm 1. Instead, we construct linear programs that mimic the coupling process. We start by\n\ndefining the coupling tree $\\mathcal{T}$ with root $r t$. This tree essentially enumerates all possible intermediate states of Algorithm 1. Formally the coupling tree is constructed by Algorithm 2.\n\n```\nAlgorithm 2: Construction of the coupling tree\nCoupling-Tree \\((\\sigma, \\tau)\\);\n    Input : Partial configurations \\(\\sigma, \\tau \\in[q]^{\\Lambda}\\) that only differ at a vertex \\(u \\in \\Lambda\\)\n    Output : A coupling tree \\(\\mathcal{T}\\) with its root \\(r t\\)\n    Parameter : A positive integer \\(R\\)\n    Construct a tree \\(\\mathcal{T}\\) containing a single root node \\(r t\\) with label \\((r t) \\leftarrow(\\sigma, \\tau, \\Lambda, \\emptyset)\\);\n    for \\(v \\in S_{R}(u) \\backslash \\Lambda\\) do\n        for \\(\\left(c_{1}, c_{2}\\right) \\in \\operatorname{supp}\\left(\\mu_{v}^{\\sigma}\\right) \\times \\operatorname{supp}\\left(\\mu_{v}^{\\tau}\\right)\\) do\n            if \\(c_{1}=c_{2}=c\\) then\n                \\(\\left(\\mathcal{T}_{v, c}, r t_{v, c}\\right) \\leftarrow\\) Coupling-Tree \\(\\left(\\sigma_{v \\leftarrow c}, \\tau_{v \\leftarrow c}\\right) ;\\)\n                Append \\(\\mathcal{T}_{v, c}\\) to \\(r t\\) by connecting \\(r t\\) and \\(r t_{v, c}\\);\n            else\n                Introduce a leaf node \\(w\\) with label \\((w) \\leftarrow\\left(\\sigma_{v \\leftarrow c_{1}}, \\tau_{v \\leftarrow c_{2}}, \\Lambda \\cup\\{v\\}, v\\right) ;\\)\n                Connect \\(r t\\) and \\(w\\);\n    return \\((\\mathcal{T}, r t)\\);\n```\n\nDenote by $\\mathcal{V}(\\mathcal{T})$ the set of nodes of $\\mathcal{T}$ and $\\mathcal{L}(\\mathcal{T})$ the set of leaves. Each node $w$ of the tree represents an intermediate state of Algorithm 1. Its label $(\\sigma, \\tau, \\Lambda, D)$ represents the two partial configurations $\\sigma$ and $\\tau$, the pinned vertex set $\\Lambda$, and the set $D$ of differing vertices other than the initial disagreement $u$. In fact, $D$ can only be either some vertex $v$ or the empty set $\\emptyset$, because Algorithm 1 stops whenever the first disagreement other than $u$ is introduced. Thus, if $D \\neq \\emptyset$, then the node must be a leaf. We call a leaf node good if its $D=\\emptyset$. Namely, denote by $\\mathcal{G} \\mathcal{L}(\\mathcal{T})=\\{w \\in$ $\\mathcal{L}(\\mathcal{T}) \\mid \\operatorname{label}(w)=(*, *, *, \\emptyset)\\}$ the set of good leaves, where $*$ denotes any possible value at that position. The rest of the leaves $\\mathcal{B} \\mathcal{L}(\\mathcal{T})=\\mathcal{L}(\\mathcal{T}) \\backslash \\mathcal{G} \\mathcal{L}(\\mathcal{T})$ are bad. Moreover, denote by $\\mathcal{C}(w)$ the set of children of a node $w$.\n\nThe linear program is introduced in Algorithm 3. Again, some intuition and heuristics for this approach are given in Appendix A. For each $w \\in \\mathcal{V}(\\mathcal{T})$ whose label is $\\left(\\sigma^{w}, \\tau^{w}, \\Lambda^{w}, *\\right)$, we call $r_{w}:=\\frac{\\mu_{\\Lambda} \\times\\left(\\sigma^{w}\\right)}{\\mu_{\\Lambda} \\times\\left(\\tau^{w}\\right)}$ its marginal ratio. Our goal is to estimate the marginal ratio $r=r_{r t}=\\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda}(\\tau)}$ for the root of the coupling tree. We do so by combining the LP with a binary search. For some guessed upper and lower bounds $r_{+}$and $r_{-}$for $r$, ideally, we want to construst an LP such that it is feasible if and only if $r_{-} \\leq r \\leq r_{+}$. Because of the presence of errors, eventually, we will only establish an approximation version of this claim.\n\nThe LP contains two variables $x_{w}$ and $y_{w}$ for each node $w \\in \\mathcal{V}(\\mathcal{T})$ of the coupling tree. Intuitively, $x_{w}$ represents $\\frac{z_{w} \\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda} \\times\\left(\\sigma^{w}\\right)}$ and $y_{w}$ represents $\\frac{z_{w} \\mu_{\\Lambda}(r)}{\\mu_{\\Lambda} \\times\\left(\\tau^{w}\\right)}$, where $z_{w}$ is the probability that the coupling reaches the node $w$. The four types of constraints in Algorithm 3 can be intuitively interpreted when $x_{w}=\\frac{z_{w} \\mu_{\\Lambda}(s)}{\\mu_{\\Lambda} \\times\\left(\\sigma^{w}\\right)}$ and $y_{w}=\\frac{z_{w} \\mu_{\\Lambda}(v)}{\\mu_{\\Lambda} \\times\\left(\\tau^{w}\\right)}$ as follows. We only explain the intuitions for $x_{w}$, and the same applies to $y_{w}$.\n(1) Validity constraints: When $w=r t$ is the root, we have that $z_{w}=1$ and $x_{w}=\\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda}(\\sigma)}=1$.\n\n(2) Recursive constraints: Given a node $w$ with label $\\left(\\sigma^{w}, \\tau^{w}, \\Lambda^{w}, \\emptyset\\right)$, fix a vertex $v \\in S_{R}(u) \\backslash \\Lambda^{w}$ and a value $c \\in \\operatorname{supp}\\left(\\mu_{v}^{\\sigma^{w}}\\right)$. We verify the following identity from the coupling process\n\n$$\n\\sum_{\\substack{w^{\\prime} \\in \\mathcal{C}(w): \\\\ \\operatorname{label}\\left(w^{\\prime}\\right)=\\left(\\sigma_{\\sigma v-c}^{w}, *, *, *\\right)}} \\frac{x_{w^{\\prime}}}{x_{w}}=\\frac{1}{\\mu_{v}^{\\sigma^{w}}(c)} \\sum_{\\substack{w^{\\prime} \\in \\mathcal{C}(w): \\\\ \\operatorname{label}\\left(w^{\\prime}\\right)=\\left(\\sigma_{\\sigma v-c}^{w}, *, *, *\\right)}} \\frac{z_{w^{\\prime}}}{z_{w}}=\\frac{1}{\\ell(w)}\n$$\n\nwhere $\\ell(w):=\\left|S_{R}(u) \\backslash \\Lambda^{w}\\right|$ for a node $w \\in \\mathcal{V}(\\mathcal{T})$ with label $(*, *, \\Lambda^{w}, *)$. The first equality holds because $x_{w}=\\frac{z_{w} \\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}$. The second equality holds because $\\frac{z_{w^{\\prime}}}{z_{w}}$ is the probability of the coupling reaches $w^{\\prime}$ from $w$. Thus, the summation of $\\frac{z_{w^{\\prime}}}{z_{w}}$ is the probability that the vertex $v$ is chosen in this step of the coupling, and is given the value $c$ on $\\sigma$ 's side. This happens with probability $\\frac{\\mu_{v}^{\\sigma^{w}}(c)}{\\ell(w)}$.\n(3) Leaf constraints: The ratio $\\frac{x_{w}}{y_{w}}=\\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda}(\\tau)} \\cdot \\frac{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}$ is a product of two ratios. Assume $R_{w} \\approx \\frac{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}$ and $r_{-} \\leq \\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda}(\\tau)} \\leq r_{+}$. We have $r_{-} R_{w}^{-1} \\lesssim \\frac{z_{w}}{y_{w}} \\lesssim r_{+} R_{w}^{-1}$. Note that for good leaves, the ratio $R_{w}$ can be efficiently computed and the constraint contains no error. However, for bad leaves, we have to settle on an approximate version.\n(4) Overflow constraints: These control the probability of going to a bad leaf:\n\n$$\n\\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} \\frac{x_{w^{\\prime}}}{x_{w}}=\\frac{1}{\\mu_{v}^{\\sigma^{w}}(c)} \\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} \\frac{z_{w^{\\prime}}}{z_{w}} \\leq \\frac{\\operatorname{Pr}\\left[\\text { coupling goes from } w \\text { to a bad leaf }\\right]}{b}\n$$\n\nwhere the last inequality follows from the marginal lower bound $\\mu_{v}^{\\sigma^{w}}(c) \\geq b$ and the probability that $w$ goes to a bad leaf is at most $\\frac{\\delta(R)}{\\ell(w)}$ due to the influence bound. Here we need to use $b^{-1}$ to upper bound $\\mu_{v}^{\\sigma^{w}}(c)^{-1}$ because the latter is a hard to compute quantity. Because of the inequality above, we set the parameter $\\eta$ to\n\n$$\n\\eta:=b^{-1} \\delta(R) \\quad \\text { where } R \\text { is defined in (10). }\n$$\n\nLemma 15 formally verifies that $x_{w}=\\frac{z_{w} \\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}$ and $y_{w}=\\frac{z_{w} \\mu_{\\Lambda}(\\tau)}{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}$ do satisfy all the constraints. When applying Moitra's method [Moi19], it is standard to choose these variables and constraints (1) and (2), as well as the constraints for good leaves where there is no error. On the other hand, typical applications of Moitra's method involve some local uniformity constraints to control the probability of reaching bad leaves, and put no constraints on the bad leaves themselves. Local uniformity no longer holds in our setting. Instead, we include the overflow constraints to bound the effect of bad leaves. These constraints can only reduce bad leaves' effects on the overall error by some constant factor. ${ }^{5}$ Thus, we also introduce the leaf constraints on the bad leaves. Their marginal ratios are involved in these constraints, which we recursively solve. Overall, the error in our algorithm decreases by a constant factor in each iteration of the recursive call.\n\nIn other words, to construct our LP, we need the coupling tree $\\mathcal{T}$, as well as the marginal ratios for all the leaf nodes of $\\mathcal{T}$. The ratios for good leaves can be efficiently computed, and the ratios for bad leaves are recursively fed and are denoted $\\mathbf{R}$. The LP is combined with a binary search to find an estimate to $r$, and the other two inputs $r_{-}$and $r_{+}$are our current guesses of the upper and lower bounds of $r$, which we will keep adjusting during the binary search. When $\\mathbf{R}$ has no error, a solution of this LP is guaranteed to exist for $r_{-}=r_{+}$. However, as our $\\mathbf{R}$ may contain errors, we can only rely on binary search to reduce the gap $r_{+}-r_{-}$to an appropriate level, rather than require $r_{+}=r_{-}$.\n\n[^0]\n[^0]:    ${ }^{5} \\mathrm{~A}$ sharp-eyed reader may have noticed that we can reduce the effect of bad leaves to polynomially small by setting the radius $R$ to $\\Omega(\\log n)$. This is correct, but doing so would increase the size of $S_{R}(u)$ to polynomially large, and the overall LP would be exponentially large.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# Algorithm 3: The linear program \n\n1. $\\underline{\\mathrm{LP}}\\left(r_{-}, r_{+}, \\mathcal{T}, \\mathbf{R}, \\varepsilon\\right)$;\n\nInput : Positive real values $r_{-} \\leq r_{+}$, a coupling tree $\\mathcal{T}$, marginal ratio estimates $\\mathbf{R} \\in \\mathbb{R}_{>0}^{\\mathcal{L}(\\mathcal{T})}$, and their error margin $\\varepsilon$\nOutput : A Boolean value, indicating whether the LP has a feasible solution Parameter : $\\eta>0$\n2 return true if and only if the following LP has a feasible solution\n(1) Validity constraints:\n\n$$\n\\begin{aligned}\n\\forall w \\in \\mathcal{V}(\\mathcal{T}), \\quad & x_{w}, y_{w} \\geq 0 \\\\\n& x_{r t}=y_{r t}=1\n\\end{aligned}\n$$\n\n(2) Recursive constraints:\n\nFor any non-leaf node $w$ with label $(w)=\\left(\\sigma^{w}, \\tau^{w}, \\Lambda^{w}, \\emptyset\\right)$, and $v \\in S_{R}(u) \\backslash \\Lambda^{w}$,\n\n$$\n\\begin{aligned}\n& \\forall c \\in \\operatorname{supp}\\left(\\mu_{v}^{\\sigma^{w}}\\right), \\quad \\sum_{\\substack{w^{\\prime} \\in \\mathcal{C}(w): \\\\\n\\text { label }\\left(w^{\\prime}\\right)=\\left(\\sigma_{v-v-c}^{w}, *, *\\right)}} x_{w^{\\prime}}=\\frac{x_{w}}{\\ell(w)} \\\\\n& \\forall c \\in \\operatorname{supp}\\left(\\mu_{v}^{\\tau^{w}}\\right), \\quad \\sum_{\\substack{w^{\\prime} \\in \\mathcal{C}(w): \\\\\n\\text { label }\\left(w^{\\prime}\\right)=\\left(*, \\tau_{v-v-c}^{w}, *, *\\right)}} y_{w^{\\prime}}=\\frac{y_{w}}{\\ell(w)} \\\\\n& \\text { (3) Leaf constraints: }\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& \\forall w \\in \\mathcal{G} \\mathcal{L}(\\mathcal{T}), \\quad r_{-} R_{w}^{-1} y_{w} \\leq x_{w} \\leq r_{+} R_{w}^{-1} y_{w} \\\\\n& \\forall w \\in \\mathcal{B} \\mathcal{L}(\\mathcal{T}), \\quad r_{-}(1+\\varepsilon)^{-1} R_{w}^{-1} y_{w} \\leq x_{w} \\leq r_{+}(1+\\varepsilon) R_{w}^{-1} y_{w}\n\\end{aligned}\n$$\n\n(4) Overflow constraints: let $\\eta:=b^{-1} \\delta(R)$, where $R$ is defined in (10),\n\n$$\n\\begin{aligned}\n& \\forall w \\in \\mathcal{V}(\\mathcal{T}) \\backslash \\mathcal{L}(\\mathcal{T}), \\quad \\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} x_{w^{\\prime}} \\leq \\frac{\\eta}{\\ell(w)} x_{w} \\\\\n& \\forall w \\in \\mathcal{V}(\\mathcal{T}) \\backslash \\mathcal{L}(\\mathcal{T}), \\quad \\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} y_{w^{\\prime}} \\leq \\frac{\\eta}{\\ell(w)} y_{w}\n\\end{aligned}\n$$\n\nWith the coupling tree $\\mathcal{T}$ for $(\\sigma, \\tau)$ (in Algorithm 2) and the linear program (in Algorithm 3) in hand, we then estimate the marginal ratio $r=\\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda}(\\tau)}$ by the binary search mentioned above. We formally state this binary search in Algorithm 4.\n\nThe following bound is the main guarantee of the vector $\\mathbf{R}$.\nCondition 13 ( $\\varepsilon$-error bound). Let $\\varepsilon>0$ be a parameter. Let $\\mu$ be a Gibbs distribution on $[q]^{V}$. Let $\\mathcal{T}$ be the coupling tree of $(\\sigma, \\tau)$, where $\\sigma, \\tau \\in[q]^{\\Lambda}$ are two partial configurations on $\\Lambda \\subseteq V$ that only differ at some vertex $u \\in \\Lambda$. Then, let $\\mathbf{R} \\in \\mathbb{R}_{>0}^{\\mathcal{L}(\\mathcal{T})}$ be a vector defined on the leaves of $\\mathcal{T}$. For any leaf node $w \\in \\mathcal{L}(\\mathcal{T})$ with label $(w)=\\left(\\sigma^{w}, \\tau^{w}, \\Lambda^{w}, *\\right)$,\n\n- if $w \\in \\mathcal{G} \\mathcal{L}(\\mathcal{T}), R_{w}=\\frac{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}$;\n- if $w \\in \\mathcal{B} \\mathcal{L}(\\mathcal{T})$, it holds that\n\n$$\n(1+\\varepsilon)^{-1} \\leq R_{w} \\cdot \\frac{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)} \\leq(1+\\varepsilon)\n$$\n\nWhen Condition 13 holds, we say $\\mathbf{R}$ satisfies the $\\varepsilon$-error bound.", "tables": {}, "images": {}}, {"section_id": 6, "text": "# Algorithm 4: Marginal estimation based on LP \n\n```\nMarginal-estimator \\(\\left(\\mathcal{T}, \\mathbf{R}, \\varepsilon\\right)\\)\n    Input : A coupling tree \\(\\mathcal{T}\\) with root \\(r t\\), marginal ratio estimates \\(\\mathbf{R} \\in \\mathbb{R}_{>0}^{\\mathcal{L}(\\mathcal{T})}\\) and their\n        error margin \\(\\varepsilon\\)\n    Output : An estimate \\(\\hat{r}\\)\n    Parameters: An integer \\(R>0\\) and a real number \\(\\eta>0\\)\n    \\(r_{\\text {low }} \\leftarrow b\\) and \\(r_{\\text {upp }} \\leftarrow b^{-1}\\)\n    \\(\\widehat{\\varepsilon} \\leftarrow \\eta H\\left(\\Delta^{R}\\right) \\cdot \\varepsilon\\)\n    while \\(r_{\\text {upp }}>(1+\\widehat{\\varepsilon})^{2} r_{\\text {low }}\\) do\n        Let \\(m \\leftarrow\\left(r_{\\text {upp }}+r_{\\text {low }}\\right) / 2\\)\n        if both \\(L P\\left(r_{\\text {low }}, m, \\mathcal{T}, \\mathbf{R}, \\varepsilon\\right)\\) and \\(L P\\left(m, r_{\\text {upp }}, \\mathcal{T}, \\mathbf{R}, \\varepsilon\\right)\\) are true then\n            return \\(m\\);\n        // Assertion: otherwise either LP( \\(\\left.r_{\\text {low }}, m, \\mathcal{T}, \\mathbf{R}, \\varepsilon\\right)\\) or \\(\\operatorname{LP}\\left(m, r_{\\text {upp }}, \\mathcal{T}, \\mathbf{R}, \\varepsilon\\right)\\) is\n            true\n        if \\(L P\\left(r_{\\text {low }}, m, \\mathcal{T}, \\mathbf{R}, \\varepsilon\\right)\\) is true then\n            \\(\\mid r_{\\text {upp }} \\leftarrow m\\);\n        else\n            \\(r_{\\text {low }} \\leftarrow m\\)\n    return \\(\\hat{r}=\\sqrt{r_{\\text {low }} r_{\\text {upp }}} ;\\)\n```\n\nOur marginal estimator (Algorithm 4) takes $(\\mathcal{T}, \\mathbf{R}, \\varepsilon)$ as input and outputs an estimate $\\widehat{r}$ of $r$. The key property of this estimator is that if $\\mathbf{R}$ satisfies the $\\varepsilon$-error bound, then the error of $\\widehat{r}$ shrinks by a constant factor.\n\nLemma 14. Let $(\\mathcal{T}, \\mathbf{R}, \\varepsilon)$ be the input of Algorithm 4 such that $\\mathbf{R}$ satisfies the $\\varepsilon$-error bound (namely Condition 13 holds) for some $\\varepsilon \\leq 3 b^{-2}$. Then the output $\\widehat{r}$ of Algorithm 4 satisfies\n\n$$\n(1+\\widehat{\\varepsilon})^{-1} \\leq \\frac{\\widehat{r}}{r} \\leq 1+\\widehat{\\varepsilon}\n$$\n\nwhere $r=\\frac{\\mu_{\\Delta}(\\sigma)}{\\mu_{\\Delta}(\\tau)}$ and $\\widehat{\\varepsilon}:=5 b^{-2} \\eta H\\left(\\Delta^{R}\\right) \\cdot \\varepsilon$.\nNote that our choices of $R$ in (10) and $\\eta$ in (12) are stronger than requiring $5 b^{-2} \\eta H\\left(\\Delta^{R}\\right)<1$. This is because in the full algorithm, we need to rewrite each $R_{w}$ into a product of two marginal ratios so that the recursion can continue. Thus, at each recursion step, the error first increases because of this product, and then shrinks by Lemma 14.\n\nLemma 14 is a direct consequence of the following two lemmas.\nLemma 15. Suppose Condition 13 holds. If the input $r_{-}$and $r_{+}$to Algorithm 3 satisfies $r_{-} \\leq r \\leq$ $r_{+}$, then the $L P$ is feasible.\nLemma 16. Suppose Condition 13 holds with $\\varepsilon \\leq 3 b^{-2}$. If Algorithm 3 returns true for parameters $r_{-}$and $r_{+}$, then\n\n$$\n(1+\\hat{\\varepsilon})^{-1} r_{-} \\leq r \\leq(1+\\hat{\\varepsilon}) r_{+}\n$$\n\nThe proofs of Lemma 15 and Lemma 16 are deferred to the next section, Section 4.2.\nProof of Lemma 14. First suppose the binary search terminates early in Line 6 of Algorithm 4. In this case, both $\\operatorname{LP}\\left(r_{\\text {low }}, m, \\mathcal{T}, \\mathbf{R}, \\varepsilon\\right)$ and $\\operatorname{LP}\\left(m, r_{\\text {upp }}, \\mathcal{T}, \\mathbf{R}, \\varepsilon\\right)$ return true. Lemma 16 implies that $r \\leq(1+\\widehat{\\varepsilon}) m$ and $r \\geq(1+\\widehat{\\varepsilon})^{-1} m$. Thus $m$ satisfies the desired approximation bound.\n\nOtherwise, only one of the two LPs is feasible until the while loop ends. By induction $r \\in$ [ $r_{\\text {low }}, r_{\\text {upp }}$ ] throughout the while loop due to Lemma 15. The binary search exits the while loop when $r_{\\text {upp }} \\leq(1+\\hat{\\varepsilon})^{2} r_{\\text {low }}$. Thus the output satisfies the desired approximation bound.\n4.2. Error analysis of the LP. In this section, we prove Lemma 15 and Lemma 16.\n\nProof of Lemma 15. For each node $w \\in \\mathcal{V}(\\mathcal{T})$ with label $(w)=\\left(\\sigma^{w}, \\tau^{w}, \\Lambda^{w}, *\\right)$, let $z_{w}$ be the probability that Algorithm 1 reaches the node $w$ in the coupling tree. Furthermore, let $x_{w}=\\frac{z_{w} \\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda w}\\left(\\sigma^{w}\\right)}$ and $y_{w}=\\frac{z_{w} \\mu_{\\Lambda}(\\tau)}{\\mu_{\\Lambda w}\\left(\\sigma^{w}\\right)}$. It suffices to verify that $(\\mathbf{x}, \\mathbf{y})$ is a feasible solution to the LP. We verify the four sets of constraints one by one.\n\n- Validity constraints: it is obvious that $x_{w}, y_{w} \\geq 0$ and $x_{r t}=y_{r t}=z_{r t}=1$.\n- Recursive constraints: By symmetry, we only verify the first set. For each non-leaf node $w \\in \\mathcal{V}(\\mathcal{T})$ with label $(w)=\\left(\\sigma^{w}, \\tau^{w}, \\Lambda^{w}, \\emptyset\\right)$, it holds that\n$\\forall v \\in S_{R}(u) \\backslash \\Lambda^{w}, c \\in \\operatorname{supp}\\left(\\mu_{v}^{\\sigma^{w}}\\right)$,\n$\\sum_{\\substack{w^{\\prime} \\in \\mathcal{C}(w): \\\\(\\operatorname{label}\\left(w^{\\prime}\\right)=\\left(\\sigma_{v \\leftarrow c}^{w}, *, *,\\right.}}\\right.} z_{w^{\\prime}}=z_{w} \\cdot \\frac{1}{\\ell(w)} \\cdot \\frac{\\mu_{\\Lambda^{w} \\cup\\{v\\}}\\left(\\sigma_{v \\leftarrow c}^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}$.\nThe left hand side is the total probability of reaching $w^{\\prime} \\in \\mathcal{C}(w)$ whose first label is $\\sigma_{v \\leftarrow c}^{w}$. This can only happen by first reaching $w$, with probability $z_{w}$, and randomly chosen $v$, with probability $\\frac{1}{\\ell(w)}$. Then, as Line 5 of Algorithm 1 is a valid coupling, the probability of getting $\\sigma_{v \\leftarrow c}^{w}$ is $\\frac{\\mu_{\\Lambda^{w} \\cup\\{v\\}}\\left(\\sigma_{v \\leftarrow c}^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}$. This is exactly the right hand side. The recursive constraints then hold for $x_{w}=\\frac{z_{w} \\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}$.\n- Leaf constraints: For each leaf node $w \\in \\mathcal{L}(\\mathcal{T})$ with label $(w)=\\left(\\sigma^{w}, \\tau^{w}, \\Lambda^{w}, *\\right)$, we have\n\n$$\n\\frac{x_{w}}{y_{w}}=r \\cdot \\frac{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}\n$$\n\nBy Condition 13, the term $\\frac{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}$ is either exactly $R_{w}^{-1}$ (when $w$ is a good leaf) or approximated by $R_{w}^{-1}$ up to $1+\\varepsilon$ relative error (when $w$ is bad). As $r_{-} \\leq r \\leq r_{+}$, the leaf constraints hold.\n\n- Overflow constraints: By symmetry, we only verify the first set. For each non-leaf node $w \\in \\mathcal{V}(\\mathcal{T})$,\n\n$$\n\\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} z_{w^{\\prime}} \\leq \\frac{b \\eta}{\\ell(w)} z_{w}\n$$\n\nThis is because by Definition 11, the probability that Algorithm 1 reveals a disagreement is bounded by $\\frac{\\delta(R)}{\\ell(w)}=\\frac{b \\eta}{\\ell(w)}$. Note that for any $w^{\\prime} \\in \\mathcal{C}(w), \\Lambda^{w^{\\prime}}$ takes the form $\\Lambda^{w} \\cup\\{v\\}$ for some $v \\in S_{R}(u) \\backslash \\Lambda^{w}$, and $\\sigma^{w^{\\prime}}$ takes the form $\\sigma_{v \\leftarrow c}^{w}$ for some $c \\in \\operatorname{supp}\\left(\\sigma_{v}^{w}\\right)$. By Definition 3, the definition of the marginal lower bound $b$, we have\n\n$$\n\\frac{\\mu_{\\Lambda^{w^{\\prime}}}\\left(\\sigma^{w^{\\prime}}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w^{\\prime}}\\right)}=\\frac{\\mu_{\\Lambda^{w} \\cup\\{v\\}}\\left(\\sigma_{v \\leftarrow c}^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)} \\geq b\n$$\n\nTherefore, by (13),\n\n$$\n\\begin{aligned}\n\\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} x_{w^{\\prime}} & =\\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} \\frac{z_{w^{\\prime}} \\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda^{w^{\\prime}}}\\left(\\sigma^{w^{\\prime}}\\right)} \\leq b^{-1} \\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} \\frac{z_{w^{\\prime}} \\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)} \\\\\n& \\leq \\frac{\\eta}{\\ell(w)} \\cdot \\frac{z_{w} \\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}=\\frac{\\eta}{\\ell(w)} x_{w}\n\\end{aligned}\n$$\n\nFrom the proof above, one can observe that the overflow constraints are weaker than what the values we plug in for $x_{w}$ and $y_{w}$ really satisfy. However, to write the exact constraints these values satisfy, some marginal probabilities are required. There appears to be no efficient way to compute these marginal probabilities. Thus, we choose to use the marginal lower bound to enforce a weaker set of constraints.\n\nNext we show Lemma 16. Suppose there exists a solution, say, $(\\mathbf{x}, \\mathbf{y})$, to the linear program. For each $1 \\leq i \\leq \\ell$ where $\\ell=\\left|S_{R}(u) \\backslash \\Lambda\\right|$, define\n\n$$\n\\Gamma_{x, i}:=\\sum_{w \\in \\mathcal{B} \\mathcal{L}(\\mathcal{T}): \\ell(w)=i-1} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w}\n$$\n\nwhere we use $\\Lambda^{w}$ and $\\sigma^{w}$ to denote the corresponding labels for $w$. Intuitively, $\\Gamma_{x, i}$ is the sum of $\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w}$ over all bad leaves of a certain depth. Furthermore, let\n\n$$\n\\Gamma_{x, 0}:=\\sum_{w \\in \\mathcal{G} \\mathcal{L}(\\mathcal{T})} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w}\n$$\n\nSimilarly, we define $\\Gamma_{y, i}$ for $0 \\leq i \\leq \\ell$ by replacing $x_{w}$ with $y_{w}$. Before proving Lemma 16, we derive some basic properties of $\\Gamma_{\\cdot,}$.\n\nLemma 17. Assuming Condition 13, the following holds:\n(1) $\\sum_{i=0}^{\\ell} \\Gamma_{x, i}=\\mu_{\\Lambda}(\\sigma)$ and $\\sum_{i=0}^{\\ell} \\Gamma_{y, i}=\\mu_{\\Lambda}(\\tau)$;\n(2) For all $1 \\leq i \\leq \\ell, \\Gamma_{x, i} \\leq \\frac{\\eta}{i} \\sum_{j=0}^{i} \\Gamma_{x, j}$ and $\\Gamma_{y, i} \\leq \\frac{\\eta}{i} \\sum_{j=0}^{i} \\Gamma_{y, j}$;\n(3) $r_{-} \\Gamma_{y, 0} \\leq \\Gamma_{x, 0} \\leq r_{+} \\Gamma_{y, 0}$\n(4) For any $1 \\leq i \\leq \\ell,(1+\\varepsilon)^{-2} r_{-} \\Gamma_{y, i} \\leq \\Gamma_{x, i} \\leq(1+\\varepsilon)^{2} r_{+} \\Gamma_{y, i}$.\n\nProof. Note that for any non-leaf node $w \\in \\mathcal{V}(\\mathcal{T})$,\n\n$$\n\\begin{aligned}\n\\sum_{w^{\\prime} \\in \\mathcal{C}(w)} \\mu_{\\Lambda^{w^{\\prime}}}\\left(\\sigma^{w^{\\prime}}\\right) x_{w^{\\prime}} & =\\sum_{v \\in S_{R}(u) \\backslash \\Lambda^{w}} \\sum_{c \\in \\operatorname{supp}\\left(\\mu_{\\tau}^{x^{w}}\\right)} \\sum_{\\substack{w^{\\prime} \\in \\mathcal{C}(w): \\\\\n\\text { label }\\left(w^{\\prime}\\right)=\\left(\\sigma_{v+c}^{w}, *, *, *\\right)}} \\mu_{\\Lambda^{w} \\cup\\{v\\}}\\left(\\sigma_{v+-c}^{w}\\right) x_{w^{\\prime}} \\\\\n& =\\sum_{v \\in S_{R}(u) \\backslash \\Lambda^{w}} \\sum_{c \\in \\operatorname{supp}\\left(\\mu_{\\tau}^{x^{w}}\\right)} \\frac{\\mu_{\\Lambda^{w} \\cup\\{v\\}}\\left(\\sigma_{v+-c}^{w}\\right) x_{w}}{\\ell(w)} \\\\\n& =\\sum_{v \\in S_{R}(u) \\backslash \\Lambda^{w}} \\frac{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w}}{\\ell(w)}=\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w}\n\\end{aligned}\n$$\n\nwhere the second equality follows from recursive constraints of $\\mathbf{x}$, and the fourth equality follows from the definition of $\\ell(w)$. Recursively applying (14) gives us that\n\n$$\n\\sum_{i=0}^{\\ell} \\Gamma_{x, i}=\\mu_{\\Lambda}(\\sigma) x_{r t}=\\mu_{\\Lambda}(\\sigma)\n$$\n\nas $x_{r t}$ is set to 1 by the LP. A similar proof works for $\\Gamma_{y, \\cdot}$. The first item holds.\nFor any non-leaf node $w \\in \\mathcal{V}(\\mathcal{T})$ and $w^{\\prime} \\in \\mathcal{C}(w), \\sigma^{w^{\\prime}}$ must have the form $\\sigma_{v_{w^{\\prime}} \\leftarrow c_{w^{\\prime}}}^{w}$ for some $v_{w^{\\prime}} \\in S_{R}(u) \\backslash \\Lambda^{w}$ and $c_{w^{\\prime}} \\in \\operatorname{supp}\\left(\\mu_{v_{w^{\\prime}}}^{\\sigma^{w}}\\right)$, and $\\Lambda^{w^{\\prime}}$ must be $\\Lambda^{w} \\cup\\left\\{v_{w^{\\prime}}\\right\\}$. Thus,\n\n$$\n\\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} \\mu_{\\Lambda^{w} \\cup\\left\\{v_{w^{\\prime}}\\right\\}}\\left(\\sigma_{v_{w^{\\prime}} \\leftarrow c_{w^{\\prime}}}^{w}\\right) x_{w^{\\prime}} \\leq \\sum_{w^{\\prime} \\in \\mathcal{C}(w) \\cap \\mathcal{B} \\mathcal{L}(\\mathcal{T})} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w^{\\prime}} \\leq \\frac{\\eta}{\\ell(w)} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w}\n$$\n\nwhere the first inequality follows from the fact that $\\mu_{\\Lambda^{w} \\cup\\{v\\}}\\left(\\sigma_{v+-c}^{w}\\right) \\leq \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)$ for any $v \\in S_{R}(u) \\backslash \\Lambda^{w}$ and $c \\in \\operatorname{supp}\\left(\\mu_{v}^{\\sigma^{w}}\\right)$, and the second inequality follows from the overflow constraints in Algorithm 3.\n\nAlso notice that $\\ell(w)=\\ell\\left(w^{\\prime}\\right)+1$. It implies\n\n$$\n\\Gamma_{x, i} \\leq \\frac{\\eta}{i} \\sum_{w \\in \\mathcal{V}(\\mathcal{T}) \\backslash \\mathcal{L}(\\mathcal{T}): \\ell(w)=i} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w}\n$$\n\nOn the other hand, recursively applying (14), we have\n\n$$\n\\sum_{j=0}^{i} \\Gamma_{x, j}=\\sum_{w \\in \\mathcal{V}(\\mathcal{T}) \\backslash \\mathcal{L}(\\mathcal{T}): \\ell(w)=i} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w}\n$$\n\nA similar proof works for $\\Gamma_{y,}$. The second item follows.\nFor the last two items, by the leaf constraints and Condition 13,\n\n$$\n\\begin{aligned}\n\\Gamma_{x, 0} & =\\sum_{w \\in \\mathcal{G} \\mathcal{L}(\\mathcal{T})} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w} \\leq \\sum_{w \\in \\mathcal{G} \\mathcal{L}(\\mathcal{T})} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) r_{+} \\cdot \\frac{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)} \\cdot y_{w}=r_{+} \\Gamma_{y, 0} \\\\\n\\Gamma_{x, i} & =\\sum_{w \\in \\mathcal{B} \\mathcal{L}(\\mathcal{T}): \\ell(w)=i-1} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) x_{w} \\leq(1+\\varepsilon)^{2} \\sum_{w \\in \\mathcal{B} \\mathcal{L}(\\mathcal{T}): \\ell(w)=i-1} \\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right) r_{+} \\cdot \\frac{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}{\\mu_{S^{w}}\\left(\\sigma^{w}\\right)} \\cdot y_{w} \\\\\n& =(1+\\varepsilon)^{2} r_{+} \\Gamma_{y, i}\n\\end{aligned}\n$$\n\nSimilarly, $\\Gamma_{x, 0} \\geq r_{-} \\Gamma_{y, 0}$ and $\\Gamma_{x, i} \\geq(1+\\varepsilon)^{-2} r_{-} \\Gamma_{y, i}$.\nWith the help of Lemma 17, we are now ready to give the proof of Lemma 16.\nProof of Lemma 16. Let $\\varepsilon>0$ be input to Algorithm 3. For $0 \\leq \\ell \\leq \\Delta^{R}$, define\n\n$$\n\\varepsilon_{\\ell}:=5 b^{-2} \\eta H(\\ell) \\cdot \\varepsilon\n$$\n\nwhere the function $H(\\ell)=\\sum_{i=1}^{\\ell} \\frac{1}{\\epsilon}$ is the harmonic sum. Clearly $\\varepsilon_{\\ell}$ is increasing in $\\ell$.\nTo prove Lemma 16, by Item (1) of Lemma 17, we only need to show that\n\n$$\n\\left(1+\\varepsilon_{\\ell}\\right)^{-1} r_{-} \\leq \\frac{\\sum_{i=0}^{\\ell} \\Gamma_{x, i}}{\\sum_{i=0}^{\\ell} \\Gamma_{y, i}} \\leq\\left(1+\\varepsilon_{\\ell}\\right) r_{+}\n$$\n\nWe do an induction on $\\ell$. By Item (3) in Lemma 17, (16) holds for $\\ell=0$. Now assume (16) holds for $\\ell-1$. We prove the upper bound first. By induction hypothesis,\n\n$$\n\\sum_{i=0}^{\\ell-1} \\Gamma_{x, i} \\leq\\left(1+\\varepsilon_{\\ell-1}\\right) r_{+} \\sum_{i=0}^{\\ell-1} \\Gamma_{y, i}\n$$\n\nFurthermore, by Item (4) of Lemma 17, $\\Gamma_{x, \\ell} \\leq(1+\\epsilon)^{2} r_{+} \\Gamma_{y, \\ell}$. Therefore,\n\n$$\n\\sum_{i=0}^{\\ell} \\Gamma_{x, i} \\leq r_{+}\\left(\\left(1+\\varepsilon_{\\ell-1}\\right) \\sum_{i=0}^{\\ell-1} \\Gamma_{y, i}+(1+\\varepsilon)^{2} \\Gamma_{y, \\ell}\\right)\n$$\n\nWe claim that\n\n$$\n\\left(1+\\varepsilon_{\\ell-1}\\right) \\sum_{i=0}^{\\ell-1} \\Gamma_{y, i}+(1+\\varepsilon)^{2} \\Gamma_{y, \\ell} \\leq\\left(1+\\varepsilon_{\\ell}\\right) \\sum_{i=0}^{\\ell} \\Gamma_{y, i}\n$$\n\nwhich would finish the proof. Our choice of parameters satisfy (10) and (12), which implies that\n\n$$\n\\varepsilon_{\\ell-1}<\\varepsilon_{\\ell} \\leq \\eta H\\left(\\Delta^{R}\\right) \\varepsilon<\\varepsilon\n$$\n\nThus, the claim is equivalent to\n\n$$\n\\Gamma_{y, \\ell} \\leq \\frac{\\varepsilon_{\\ell}-\\varepsilon_{\\ell-1}}{2 \\varepsilon+\\varepsilon^{2}-\\varepsilon_{\\ell}} \\sum_{i=0}^{\\ell} \\Gamma_{y, i}\n$$\n\nAs $\\varepsilon \\leq 3 b^{-2}$ and $0<b<1,2 \\varepsilon+\\varepsilon^{2}-\\varepsilon_{\\ell} \\leq 5 b^{-2} \\varepsilon$, we just need to show\n\n$$\n\\Gamma_{y, \\ell} \\leq \\frac{\\varepsilon_{\\ell}-\\varepsilon_{\\ell-1}}{5 b^{-2} \\varepsilon} \\sum_{i=0}^{\\ell} \\Gamma_{y, i}=\\frac{\\eta}{\\ell} \\sum_{i=0}^{\\ell} \\Gamma_{y, i}\n$$\n\nThis is just Item (2) of Lemma 17.\nThe lower bound in (16) holds by a similar argument. Specifically, we can flip the role of $x$ and $y$ and replace $r_{+}$with $\\frac{1}{r_{-}}$in the proof above to show the lower bound.\n4.3. The full algorithm. With the Lemma 14 in hand, we are now able to construct the full algorithm for Theorem 4, which is a recursive marginal estimator. It uses Algorithm 2 and Algorithm 4 recursively and do a truncation at some depth $k$. Recall that we have set the parameter $R$ to satisfy (10) and $\\eta=b^{-1} \\delta(R)$ in (12). Moreover, set the error parameter $\\zeta_{k}$ of depth $k$ to be\n\n$$\n\\zeta_{k}:= \\begin{cases}b^{-1}, & \\text { if } k=0 \\\\ 2^{-k+1}, & \\text { if } k \\geq 1\\end{cases}\n$$\n\nThe full algorithm is given in Algorithm 5.\nNext we analyse the accuracy and the running time of Algorithm 5.\nLemma 18 (accuracy). Let $\\tilde{r}$ be the estimation given by Algorithm 5, it holds that for $k \\geq 0$,\n\n$$\n\\left(1+\\zeta_{k}\\right)^{-1} \\leq \\tilde{r} / r \\leq 1+\\zeta_{k}\n$$\n\nProof. We prove by induction on $k$. When $k=0$, Algorithm 5 simply returns 1. Let $\\gamma=\\sigma_{\\Lambda \\backslash\\{u\\}}$ be the common configuration between $\\sigma$ and $\\tau$. Then\n\n$$\n\\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda}(\\tau)}=\\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda \\backslash\\{u\\}}(\\gamma)} \\cdot \\frac{\\mu_{\\Lambda \\backslash\\{u\\}}(\\gamma)}{\\mu_{\\Lambda}(\\tau)} \\leq b^{-1}\n$$\n\nSimilarly, $\\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda}(\\tau)} \\geq b$. Since $\\zeta_{0}=b^{-1}$, the base case holds.\nWhen $k \\geq 1$, by the induction hypothesis, we know that $X$ and $Y$ obtained at Line 15 and Line 16 in Algorithm 5 are estimates to $\\frac{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\gamma^{w}\\right)}$ and $\\frac{\\mu_{\\Lambda^{w}}\\left(\\gamma^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}$, respectively, both with $\\left(1+\\zeta_{k-1}\\right)$ relative error. Note that here $\\mu_{\\Lambda^{w}}\\left(\\gamma^{w}\\right)>0$. This is because $\\gamma^{w}$ is locally feasible by its definition, and as the spin system is permissive (recall Definition 9), by (5), $\\gamma^{w}$ is globally feasible. Since $R_{w}=X \\cdot Y$, this implies that the relative error of $R_{w}$ to $\\frac{\\mu_{\\Lambda^{w}}\\left(\\sigma^{w}\\right)}{\\mu_{\\Lambda^{w}}\\left(\\tau^{w}\\right)}$ is\n\n$$\n\\left(1+\\zeta_{k-1}\\right)^{2}=1+2 \\zeta_{k-1}+\\zeta_{k-1}^{2}\n$$\n\nwhich means that $\\mathbf{R}$ satisfies a $\\left(2 \\zeta_{k-1}+\\zeta_{k-1}^{2}\\right)$-error bound (recall Condition 13).\nFor $k=1,\\left(2 \\zeta_{0}+\\zeta_{0}^{2}\\right) \\leq 3 b^{-2}$ and thus Lemma 14 applies. The lemma holds because in this case, by our choice of parameters in (10) and (12),\n\n$$\n5 b^{-2} \\eta H\\left(\\Delta^{R}\\right)\\left(2 \\zeta_{0}+\\zeta_{0}^{2}\\right) \\leq 15 b^{-4} \\eta H\\left(\\Delta^{R}\\right)<1=\\zeta_{1}\n$$\n\nFor $k \\geq 2$, by (10) and (12), again,\n\n$$\n5 b^{-2} \\eta H\\left(\\Delta^{R}\\right)\\left(2 \\zeta_{k-1}+\\zeta_{k-1}^{2}\\right) \\leq 15 b^{-2} \\eta H\\left(\\Delta^{R}\\right) \\zeta_{k-1}<\\zeta_{k-1} / 2=\\zeta_{k}\n$$\n\nThus the lemma follows from Lemma 14 as well.\nNow, we can finish the proof of Theorem 4.\n\n```\nRecursive-estimator \\((\\sigma, \\tau, \\Lambda, k, u)\\);\n    Input : Partial configurations \\(\\sigma, \\tau \\in[q]^{\\Lambda}\\) that only differ at a vertex \\(u \\in \\Lambda\\), and\n        \\(k \\in \\mathbb{N}_{\\geq 0}\\)\n    Output : An estimate \\(\\tilde{r}\\)\n    Parameters: A positive integer \\(R\\), a positive real \\(\\eta\\), and a function \\(\\zeta_{k}\\) for \\(k \\in \\mathbb{N}_{\\geq 0}\\)\n    \\(\\mathcal{T} \\leftarrow\\) Coupling-Tree \\((\\sigma, \\tau)\\);\n    if \\(k=0\\) then\n        return 1 ;\n    \\(\\ell \\leftarrow\\left|S_{R}(u) \\backslash \\Lambda\\right|\\);\n    if \\(\\ell=0\\) then\n    return \\(r=\\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda}(\\tau)\\), computed by brute force;\n    // Calculate the marginal ratio of the leaves in \\(\\mathcal{L}(\\mathcal{T})\\) via recursion\n    for \\(w \\in \\mathcal{L}(\\mathcal{T})\\) do\n        \\(\\left(\\sigma^{w}, \\tau^{w}, \\Lambda^{w}, D^{w}\\right) \\leftarrow \\operatorname{label}(w) ;\\)\n        if \\(w \\in \\mathcal{G} \\mathcal{L}(\\mathcal{T})\\) then\n            \\(R_{w} \\leftarrow\\) the marginal ratio of \\(r=\\frac{\\mu_{\\Lambda w}\\left(\\sigma^{w}\\right)}{\\mu_{\\Lambda w}\\left(\\tau^{w}\\right)}\\) by brute force;\n        else\n            \\(v \\leftarrow D^{w} ;\\)\n            \\(\\gamma^{w} \\leftarrow \\sigma_{u \\leftarrow \\tau^{w}(u)}^{w} ;\\)\n            \\(X \\leftarrow \\operatorname{Recursive-estimator}\\left(\\sigma^{w}, \\gamma^{w}, \\Lambda^{w}, k-1, u\\right) ;\\)\n            \\(Y \\leftarrow \\operatorname{Recursive-estimator}\\left(\\gamma^{w}, \\tau^{w}, \\Lambda^{w}, k-1, v\\right) ;\\)\n            \\(R_{w} \\leftarrow X \\cdot Y\\);\nreturn Marginal-estimator \\(\\left(\\mathcal{T}, \\mathbf{R}, 2 \\zeta_{k-1}+\\zeta_{k-1}^{2}\\right)\\);\n```\n\nProof of Theorem 4. First note that the assumptions of the marginal lower bound and coupling independence both hold with respect to an arbitrary conditioning. Thus, by standard self-reduction [JVV86], approximating $Z$ up to relative error $1+\\varepsilon$ can be reduced to approximate $n$ marginal probabilities up to relative error $1+\\frac{\\varepsilon}{n}$. The latter task then can be reduced to approximate $q-1$ marginal ratios up to the same relative error $1+\\frac{\\varepsilon}{n}$, for which we use Algorithm 5. Suppose the running time of Algorithm 5 with relative error $1+\\frac{\\varepsilon}{n}$ is $T_{\\varepsilon / n}$, then the overall running time is $O\\left(q n T_{\\varepsilon / n}\\right)$.\n\nBy Lemma 18, to achieve relative error $1+\\frac{\\varepsilon}{n}$, we only need to pick $k=\\left\\lceil\\log _{2}(n / \\varepsilon)\\right\\rceil+1$. Denote\n\n- by $T$ an upper bound of the cost of Algorithm 2 and Algorithm 4 (i.e., the max cost of a single recursive step);\n- and by $N$ maximum number of the nodes of the coupling tree.\n\nClearly the total cost of Algorithm 5 is bounded by $T \\cdot \\sum_{i=0}^{k}(2 N)^{i} \\leq 2 T(2 N)^{k}$. We claim that\n\n$$\nT=O\\left(k+\\log b^{-1}\\right) \\cdot \\operatorname{poly}(N) \\quad \\text { and } \\quad N \\leq 2\\left(q^{2} \\Delta^{R}\\right)^{\\Delta^{R}}\n$$\n\nwhich implies\n\n$$\nT_{\\varepsilon / n} \\leq 2 T \\cdot(2 N)^{k}=\\log b^{-1}(n / \\varepsilon)^{O\\left(\\Delta^{R} \\log \\left(q \\Delta^{R}\\right)\\right)}\n$$\n\nBy Theorem 12, we can take $\\delta(R)=2 C 2^{-\\lceil R / 2 C\\rceil}$, where $C$ is the coupling independence parameter. To satisfy (10), $R=O\\left(C\\left(\\log b^{-1}+\\log C+\\log \\log \\Delta\\right)\\right)$ suffices. Thus the bound on the running time\n\nin terms of $b, C, q, \\Delta$ is\n\n$$\nT_{\\varepsilon / n}=\\left(\\frac{n}{\\varepsilon}\\right)^{\\Delta^{O\\left(C\\left(\\log b^{-1}+\\log C+\\log \\log \\Delta\\right)\\right)} \\log q}\n$$\n\nTo show the claim, for $T$, it is direct to see that it would take $O(N)$ time to construct $\\mathcal{T}$ in Algorithm 2. Then, for Algorithm 4, the binary search step requires repeated calls to Algorithm 3 to estimate the marginal ratio. The number of calls is at most $O\\left(\\log \\frac{n}{b c}\\right)=O\\left(k+\\log b^{-1}\\right)$. For each call to Algorithm 3, it costs poly $(N)$ time to construct and solve the LP.\n\nFor $N$, each step of the coupling tree has at most $\\Delta^{R}$ choices for the vertex $v$, and at most $q^{2}$ choices for the colours on $v$. This continues for at most $\\Delta^{R}$ times, giving the bound on $N$.\n\nFor Corollary 5, the colouring instance in Corollary 5 has $O_{\\Delta . q}(1)$-CI, proved in [CLMM23, Theorem 5.10 and Lemma 5.13]. ${ }^{6}$ Thus Corollary 5 follows from Theorem 4 and Observation 10. The generic marginal lower bound from Observation 10 can be improved using [GKM15, Lemma 3], which implies $b \\geq q^{-1}\\left(\\frac{2}{3}\\right)^{\\Delta}$ in this setting. Moreover, [CLMM23, Theorem 5.9 and Lemma 5.14] directly establish influence decay for colourings in high-girth graphs. One can directly use them to set the parameter $R$ in (10) to obtain an algorithm with a better running time.", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 5. Low Disagreement Coupling from Markov Chain Coupling \n\nIn this section, we establish coupling independence from contractive coupling for Markov chains. We note that previous results like $\\left[\\mathrm{BCC}^{+} 22\\right.$, Liu21] have investigated how to establish spectral independence by using contractive couplings of Markov chains. As a byproduct, their results already implicitly suggested how to establish coupling independence via contractive coupling. However, the proofs of previous works $\\left[\\mathrm{BCC}^{+} 22\\right.$, Liu21] rely heavily on Stein's method (see [BN19, RR19]), which is analytical and arguably unintuitive. In this section, we first give a direct and simple proof that contractive coupling can be used to establish coupling independence.\n\nGiven a Markov chain $P$ and the current state $X$, we use $P(X)$ to denote the next (random) state of $P$. Also, recall the Wasserstein distance from Definition 1.\n\nLemma 19. Let $(\\Omega, d)$ be a finite metric space. Let $C>0$ and $\\delta \\in(0,1)$ be two parameters. Let $P, Q$ be Markov chains on $\\Omega$ with stationary distribution $\\mu$ and $\\nu$. Suppose the following conditions are satisfied:\n\n- ( $C$-disagreement) for $X \\in \\operatorname{supp}(\\mu), \\mathcal{W}_{\\mathrm{d}}(P(X), Q(X)) \\leq C$;\n- ( $Q$ has $\\delta$-contraction) for $X \\in \\operatorname{supp}(\\mu), Y \\in \\operatorname{supp}(\\nu), \\mathcal{W}_{\\mathrm{d}}(Q(X), Q(Y)) \\leq(1-\\delta) \\mathrm{d}(X, Y)$.\n\nThen, it holds that the W1-distance between $\\mu, \\nu$ can be bounded by $\\mathcal{W}_{\\mathrm{d}}(\\mu, \\nu) \\leq C / \\delta$.\nProof. Let $X, Y$ be sampled from $\\mu$ and $\\nu$. Then we have $P(X) \\sim \\mu$ and $Q(Y) \\sim \\nu$, which implies\n\n$$\n\\begin{aligned}\n\\mathcal{W}_{\\mathrm{d}}(X, Y)=\\mathcal{W}_{\\mathrm{d}}(P(X), Q(Y)) & \\leq \\mathcal{W}_{\\mathrm{d}}(P(X), Q(X))+\\mathcal{W}_{\\mathrm{d}}(Q(X), Q(Y)) \\\\\n& \\leq C+(1-\\delta) \\mathcal{W}_{\\mathrm{d}}(X, Y)\n\\end{aligned}\n$$\n\nWe finish the proof by rearranging terms.\nRemark. Note that $P$ and $Q$ in Lemma 19 are Markov chains on $\\Omega$. This does not require the support of $\\mu$ and $\\nu$ to be exactly $\\Omega$. In fact, it is possible that $\\operatorname{supp}(\\mu) \\subset \\Omega$ or $\\operatorname{supp}(\\nu) \\subset \\Omega$, and states in $\\Omega \\backslash \\operatorname{supp}(\\mu)$ or $\\Omega \\backslash \\operatorname{supp}(\\nu)$ are transient.\n\n[^0]\n[^0]:    ${ }^{6}$ Theorem 5.10 in [CLMM23] is stated for spectral independnce. However, [CLMM23, Lemma 5.13] proved the stronger coupling independence result.\n\nTo establish coupling independence, one could apply Lemma 19 with $\\mu^{\\sigma}$ and $\\mu^{\\tau}$ where $\\sigma$ and $\\tau$ are two partial configurations such that $\\operatorname{dist}(\\mu, \\nu)=1$, as in the definition of Definition 2. The first condition typically holds for any Markov chain with local moves. This is because all new disagreement in one step must be near where $\\sigma$ and $\\tau$ disagrees. The second condition is implied by the existence of contractive couplings, and there are plenty available in the literature. We will see Lemma 19 in action in the applications next. In fact, we will design an intermediate distribution and use triangle inequality in the next section.\n5.1. Application: coupling independence for proper colourings. In this section we show coupling independence for colourings using Lemma 19.\n\nTheorem 20. Let $G=(V, E)$ be a graph with maximum degree $\\Delta$. Let $\\mu$ be the uniform distribution over $q$-colouring of $G$. If either of the following holds:\n\n- $\\Delta \\geq 125, q \\geq 1.809 \\Delta$; or\n- $\\Delta \\geq 3, q \\geq\\left(11 / 6-\\epsilon_{0}\\right) \\Delta$ for some fixed parameter $\\epsilon_{0} \\approx 10^{-5}$,\nthen $\\mu$ satisfies $O_{\\Delta, q}(1)$-coupling independence. Moreover, the CI constant is $\\operatorname{poly}(q \\Delta)$.\nTo apply Lemma 19, we consider flip dynamics, for which contractive couplings are known under the conditions of Theorem 20 [CDM $\\left.{ }^{+} 19\\right], \\mathrm{CV} 24]$. In fact, we need to consider the conditional distribution $\\mu^{\\sigma}$ for some (not necessarily proper) partial colouring $\\sigma$ on $\\Lambda \\subset V$. As mentioned in Section 2, the distribution $\\mu^{\\sigma}$ is the same as the uniform distribution over a list colouring instance, obtained by removing $\\Lambda$ and letting $L_{v}=[q] \\backslash\\left\\{\\sigma(u) \\mid u \\in \\Lambda \\cap N_{G}(v)\\right\\}$. Note that if $q \\geq(1+\\alpha) \\Delta$ for some $\\alpha>0$, then for any list colouring instance obtained this way, $\\left|L_{v}\\right|-\\operatorname{deg}_{G}(v) \\geq \\alpha \\Delta$. Also, as long as $q \\geq \\Delta+1$, even if $\\sigma$ is not proper, the induced list colouring instance still has solutions. Namely the system is permissive.\n\nWe define the flip dynamics for list colouring instances obtained this way. Let the remaining graph be $G=(V, E)$ and $\\left\\{L_{v}\\right\\}_{v \\in V}$ be the lists. Notice $L_{v} \\subseteq[q]$ for all $v \\in V$. Let $X$ be a (not necessarily proper) colouring. We say a path $w_{1}-w_{2}-\\cdots-w_{\\ell}$ is an $\\left(X\\left(w_{1}\\right), c\\right)$-alternating path from $w_{1}$ to $w_{\\ell}$ if for all $i$, we have $X\\left(w_{i}\\right) \\subseteq\\left\\{X\\left(w_{1}\\right), c\\right\\}$ and $X\\left(w_{i}\\right) \\neq X\\left(w_{i+1}\\right)$. The Kempe component (or Kempe chain) for $X$, a vertex $v \\in V$, and a colour $c \\in[q]$ is defined by:\n\n$$\nS_{X}(v, c):=\\{u \\in V \\mid \\text { there is a }(X(v), c) \\text {-alternating path from } v \\text { to } u\\}\n$$\n\nMoreover, let $S_{X}(v, X(v)):=\\emptyset$. Let $\\left\\{p_{\\ell}\\right\\}_{\\ell \\geq 1}$ be some parameters. Given a colouring $X$, the flip dynamics updates $X$ to $X^{\\prime}$ as follows:\n(1) pick a vertex $v \\in V$ and a colour $c \\in[q]$ uniformly at random;\n(2) let $S=S_{X}(v, c)$ and $\\ell=|S|$;\n(3) if the colouring on $S$ can be flipped, do so with probability $p_{\\ell} / \\ell$ to obtain $X^{\\prime}$;\n(4) otherwise, let $X^{\\prime}=X$.\n\nBy flipping, we mean changing the colours of all $c$-coloured vertices in $S$ to $X(v)$ and change the colours of all $X(v)$-coloured vertices in $S$ to $c$. Note that the new colour may not be available in the lists of corresponding vertices. In that case, we simply let $X^{\\prime}=X$. Note that a Kempe component of size $\\ell$ is flipped with probability at most $\\frac{p_{\\ell}}{q \\eta}$. Given a list colouring instance $(G, L)$, the above transition rule is defined for all $X \\in[q]^{V}$ even if $X$ is improper for the list colouring instance $(G, L)$.\n\nFlip dynamics was first considered by Vigoda [Vig00], who designed a set of parameters $\\left\\{p_{\\ell}\\right\\}_{\\ell \\geq 1}$ and a contractive coupling with respect to the Hamming distance when $q>(11 / 6) \\Delta$. As a consequence, the flip dynamics is rapidly mixing. Later, Chen, Delcourt, Moitra, Perarnau and Postle $\\left[\\mathrm{CDM}^{+} 19\\right]$ improved the rapid mixing regime to $q \\geq\\left(11 / 6-\\epsilon_{0}\\right) \\Delta$ for some $\\epsilon_{0} \\approx 10^{-5}$. They showed that a contractive coupling exists for the flip dynamics with some different $\\left\\{p_{\\ell}\\right\\}_{\\ell \\geq 1}$ by considering either variable length coupling with Hamming distance or using an alternative metric. Very recently, Carlson and Vigoda [CV24] showed that if $\\Delta \\geq 125$ and $q \\geq 1.809 \\Delta$ (note that\n\n$1.809<11 / 6-\\epsilon_{0}$ ), there is another set of $\\left\\{p_{\\ell}\\right\\}_{\\ell \\geq 1}$ ensuring contractive coupling, by considering a more refined metric.\n\nThese alternative metrics can be related to the Hamming distance. Given the metric space $(\\Omega, \\mathrm{d})$, we say that the metric d is $\\alpha$-equivalent (to the Hamming distance) for some $\\alpha \\geq 1$, if for all $x, y \\in \\Omega$, it holds that\n\n$$\n\\alpha^{-1} \\operatorname{dist}\\{x, y\\} \\leq \\mathrm{d}\\{x, y\\} \\leq \\alpha \\operatorname{dist}\\{x, y\\}\n$$\n\nAll metrics mentioned above are in fact 2-equivalent.\nMoreover, what we really consider is list colouring instances. One can verify (see [CV24, Theorem 4.12 and Section 4.6], [CDM ${ }^{+} 19$, Section 6 and Appendix E], and [BCC+22, Lemma 4.12]) that the aforementioned contractive couplings extend to list colourings as well, summarised as follows.\n\nProposition 21 ([CV24]). Let $\\Delta \\geq 125$ and $q$ be two integers. There exists a sequence of parameters $\\left\\{p_{\\ell}\\right\\}_{\\ell \\geq 0}$ satisfying $p_{i}=0$ for all $i>6$ such the the following holds. Given a list colouring instance $(G, L)$ such that for any $v \\in V, L_{v} \\subseteq[q]$ and $\\left|L_{v}\\right|-\\operatorname{deg}_{G}(v) \\in[0.809 \\Delta, 5 / 6 \\Delta]$, let $\\Omega \\subseteq[q]^{V}$ denote the set of proper list colourings for $(G, L)$, there is a 2-equivalent metric d on $\\Omega$ such that the flip dynamics $P$ with $\\left\\{p_{\\ell}\\right\\}_{\\ell \\geq 1}$ satisfies the following: for any two proper list colourings $X, Y \\in \\Omega$,\n\n$$\n\\mathcal{W}_{\\mathrm{d}}(P(X), P(Y)) \\leq\\left(1-\\frac{10^{-5}}{n q}\\right) \\mathrm{d}(X, Y)\n$$\n\nwhere $n$ is the number of vertices in $G$.\nProposition 22 ( $\\left[\\mathrm{CDM}^{+} 19\\right]$ ). Let $\\Delta \\geq 3$ and $q$ be two integers, and $\\epsilon_{0} \\approx 10^{-5}>0$ be a constant. There exists a sequence of parameters $\\left\\{p_{\\ell}\\right\\}_{\\ell \\geq 0}$ satisfying $p_{i}=0$ for all $i>6$ such the the following holds. Given a list colouring instance $(G, L)$ such that for any $v \\in V, L_{v} \\subseteq[q]$ and $\\left|L_{v}\\right|-\\operatorname{deg}_{G}(v) \\geq$ $\\left(5 / 6-\\varepsilon_{0}\\right) \\Delta$, let $\\Omega \\subseteq[q]^{V}$ denote the set of proper list colourings for $(G, L)$, there is a 2-equivalent metric d on $\\Omega$ such that the flip dynamics $P$ with $\\left\\{p_{\\ell}\\right\\}_{\\ell \\geq 1}$ satisfies the following: for any two proper colourings $X, Y \\in \\Omega$,\n\n$$\n\\mathcal{W}_{\\mathrm{d}}(P(X), P(Y)) \\leq\\left(1-\\frac{10^{-9}}{n q}\\right) \\mathrm{d}(X, Y)\n$$\n\nwhere $n$ is the number of vertices in $G$.\nRemark (Domains of colourings). Given a list colouring instance $(G, L)$, let $\\Omega=\\operatorname{supp}(\\mu)=\\{\\sigma \\mid$ $\\sigma \\in[q]^{V}$ and $\\sigma$ is a proper list colouring for $(G, L)\\}$, where $\\mu$ denotes the uniform distribution over all proper list colourings. In Proposition 21 and Proposition 22, the metric d is defined over $\\Omega$ and the contraction results in (18) and (19) hold for all $X, Y \\in \\Omega$, which is sufficient for our proof. We remark that $\\left[\\mathrm{CDM}^{+} 19, \\mathrm{CV} 24\\right]$ actually proved stronger results, their metric is defined over a superset of $\\Omega$ and the contraction results holds even for $X, Y$ beyond $\\Omega$, because they both used path coupling [BD97] which requires them to prove contraction even for improper list colourings.\n\nNote that in Proposition 21 and Proposition 22, we state the results in terms of W1-distance. This is implied by the contractive couplings from [CV24] and [CDM ${ }^{+} 19$ ]. More precisely, they show that for any $X, Y \\in \\Omega$, there exists a coupling of $P(X), P(Y)$ such that $\\mathbb{E}[\\mathrm{d}(P(X), P(Y))] \\leq$ $\\left(1-\\frac{C}{n q}\\right) \\mathrm{d}(X, Y)$, where $C=10^{-5}$ or $10^{-9}$ is the constant in (18) and (19).\n\nNow we are ready to prove Theorem 20.\nProof of Theorem 20. Let $\\sigma$ and $\\tau$ be two (not necessarily proper) partial colourings on $\\Lambda \\subseteq V$ such that they differ at only $v$. Let $\\mu$ denote the unifrom distribution over all proper $q$-colurings in $G=(V, E)$. We goal is to bound $\\mathcal{W}\\left(\\mu^{\\sigma}, \\mu^{\\tau}\\right)$ to establish the CI for $\\mu$.\n\nLet $\\left(G^{\\sigma}, L^{\\sigma}\\right)$ and $\\left(G^{\\tau}, L^{\\tau}\\right)$ be the two list colouring instances induced by $\\sigma$ and $\\tau$. Note that $G^{\\sigma}=G^{\\tau}$ and $L^{\\sigma}(u) \\neq L^{\\tau}(u)$ only if $u \\in N(v)$, where $N(v)$ is the set of neighbours of $v$ in $G$.\n\nWe introduce a middle list colouring instance $\\left(G^{\\prime}, L^{\\prime}\\right)$ such that $G^{\\prime}=G^{\\sigma}=G^{\\tau}=\\left(V^{\\prime}, E^{\\prime}\\right)$, where $V^{\\prime}=V \\backslash \\Lambda$, and for all $u \\in V^{\\prime} \\backslash N(v), L^{\\prime}(u)=L^{\\sigma}(u)=L^{\\tau}(u)$ and for all $u \\in V^{\\prime} \\cap N(v), L^{\\prime}(u)=[q]$. Note that $\\sigma$ and $\\tau$ differ only at one vertex $v$. We use the following bound\n\n$$\n\\mathcal{W}\\left(\\mu^{\\sigma}, \\mu^{\\tau}\\right)=1+\\mathcal{W}\\left(\\mu_{V^{\\prime}}^{\\sigma}, \\mu_{V^{\\prime}}^{\\tau}\\right) \\leq 1+\\mathcal{W}\\left(\\mu_{V^{\\prime}}^{\\sigma}, \\mu^{\\prime}\\right)+\\mathcal{W}\\left(\\mu^{\\prime}, \\mu_{V^{\\prime}}^{\\tau}\\right)\n$$\n\nwhere $\\mu^{\\prime}$ is the uniform distribution for list colouring $\\left(G^{\\prime}, L^{\\prime}\\right)$.\nWe show how to bound $\\mathcal{W}\\left(\\mu_{V^{\\prime}}^{\\sigma}, \\mu^{\\prime}\\right)$. The bound for $\\mathcal{W}\\left(\\mu^{\\prime}, \\mu_{V^{\\prime}}^{\\tau}\\right)$ can be obtained from the same proof. Let the parameters $\\left\\{p_{\\alpha}\\right\\}_{\\alpha \\geq 1}$ and the metric d be the same as in either Proposition 21 or Proposition 22 for the list colouring $\\left(G^{\\prime}, L^{\\prime}\\right)$. Specifically, if we assume the first condition in Theorem 20, we use Proposition 21, otherwise, we use Proposition 22. The space $\\Omega$ in both propositions is the same one, $\\Omega=\\operatorname{supp}\\left(\\mu^{\\prime}\\right) \\subseteq[q]^{V^{\\prime}}$, which is the set of all proper list colourings for $\\left(G^{\\prime}, L^{\\prime}\\right)$. Let $P$ and $Q$ be flip dynamics for $\\left(G^{\\sigma}, L^{\\sigma}\\right)$ and $\\left(G^{\\prime}, L^{\\prime}\\right)$, respectively, using the same $\\left\\{p_{\\alpha}\\right\\}_{\\alpha \\geq 1}$. Let $\\mu_{P}=\\mu_{V^{\\prime}}^{\\sigma}$ and $\\mu_{Q}=\\mu^{\\prime}$ denote the uniform distributions of list colourings $\\left(G^{\\sigma}, L^{\\sigma}\\right)$ and $\\left(G^{\\prime}, L^{\\prime}\\right)$ respectively, which are the stationary distributions of $P$ and $Q$ respectively. By the definition of $\\left(G^{\\prime}, L^{\\prime}\\right), \\operatorname{supp}\\left(\\mu_{P}\\right) \\subseteq \\operatorname{supp}\\left(\\mu_{Q}\\right)$. Both $P$ and $Q$ can be viewed as Markov chains over the space $\\operatorname{supp}\\left(\\mu_{Q}\\right)$ because $P(X), Q(X) \\in \\operatorname{supp}\\left(\\mu_{Q}\\right)$ if $X \\in \\operatorname{supp}\\left(\\mu_{Q}\\right)$. Let $\\left(\\operatorname{supp}\\left(\\mu_{Q}\\right), \\mathrm{d}\\right)$ be the metric assumed in the two propositions. The contraction results in (18) and (19) hold for $Q$.\n\nNext, we use Lemma 19 with $P$ and $Q$ to bound $\\mathcal{W}\\left(\\mu_{P}, \\mu_{Q}\\right)$. For any colouring $X \\in \\operatorname{supp}\\left(\\mu_{P}\\right)$, notice that $p_{\\alpha}>0$ only for $\\ell \\leq 6$, and the transitions of flip dynamics $P$ and $Q$ can be different only if a neighbour of $v$ is in the Kempe component. Let $N(v)$ denotes the set of neighbours of $v$ in $G$. This allows us to couple $P(X)$ and $Q(X)$ as follows:\n(1) in both copies, choose the same vertex $u$ and the same colour $c$ and let $S=S_{X}(u, c)$;\n(2) if $|S| \\geq 7$, let $P(X)=Q(X)=X$;\n(3) if $v$ is not adjacent to any vertex in $S$ in graph $G$, couple $P(X)$ and $Q(X)$ perfectly;\n(4) otherwise, couple $P(X)$ and $Q(X)$ independently.\n\nIf the coupling goes to the last step, the vertex $u$ must be within distance 6 from $v$ in the graph $G$. There are at most $\\Delta^{7}$ such choices. When that happens, there are at most 12 new disagreement. Thus, $\\mathcal{W}(P(X), Q(X)) \\leq \\frac{12 \\Delta^{7}}{\\left|V^{\\prime}\\right|}$. As d is 2-equivalent, it implies\n\n$$\n\\mathcal{W}_{\\mathrm{d}}(P(X), Q(X)) \\leq \\frac{24 \\Delta^{7}}{\\left|V^{\\prime}\\right|}\n$$\n\nThis verifies the first condition of Lemma 19.\nFor the second condition of Lemma 19, we need to bound the distance $\\mathcal{W}_{\\mathrm{d}}(Q(X), Q(Y))$ for $X \\in$ $\\operatorname{supp}\\left(\\mu_{P}\\right)$ and $Y \\in \\operatorname{supp}\\left(\\mu_{Q}\\right)$. By the definition of the instance $\\left(G^{\\prime}, L^{\\prime}\\right)$, it holds that $\\operatorname{supp}\\left(\\mu_{P}\\right) \\subseteq$ $\\operatorname{supp}\\left(\\mu_{Q}\\right)$, and thus $X \\in \\operatorname{supp}\\left(\\mu_{Q}\\right)$. Moreover, $\\left(G^{\\sigma}, L^{\\prime}\\right)$ has larger colour lists than $\\left(G^{\\sigma}, L^{\\sigma}\\right)$ and $\\left(G^{\\tau}, L^{\\tau}\\right)$. Thus $\\left(G^{\\prime}, L^{\\prime}\\right)$ satisfies the condition of either Proposition 21 or Proposition 22, which implies that $Q$ has $C /\\left(q\\left|V^{\\prime}\\right|\\right)$-contraction for some constant $C>0$ with respect to d. Thus, together with (21), we apply Lemma 19 with $\\mu=\\mu_{P}, \\nu=\\mu_{Q}$ and $\\Omega=\\operatorname{supp}\\left(\\mu_{Q}\\right)$ to derive\n\n$$\n\\mathcal{W}_{\\mathrm{d}}\\left(\\mu_{V^{\\prime}}^{\\sigma}, \\mu^{\\prime}\\right)=\\mathcal{W}_{\\mathrm{d}}\\left(\\mu_{P}, \\mu_{Q}\\right) \\leq \\frac{24 \\Delta^{7} /\\left|V^{\\prime}\\right|}{C /(q\\left|V^{\\prime}\\right|)}=\\operatorname{poly}(q \\Delta)\n$$\n\nAs d is 2-equivalent to the Hamming distance, we have $\\mathcal{W}\\left(\\mu_{V^{\\prime}}^{\\sigma}, \\mu^{\\prime}\\right)=\\operatorname{poly}(q \\Delta)$. Since the same proof works for $\\mathcal{W}\\left(\\mu_{V^{\\prime}}^{\\tau}, \\mu^{\\prime}\\right)$, the lemma follows from (20).\n\nIn the proof above, all analysis except the first equation in (20) considers list colouring instances on the subgraph $G^{\\prime}=G\\left[V^{\\prime}\\right]$. The intermediate instance $\\left(G^{\\prime}, L^{\\prime}\\right)$ is introduced because we need to ensure that $X \\in \\operatorname{supp}\\left(\\mu_{Q}\\right)$ when applying Proposition 21 or Proposition 22. When verifying the first condition in Lemma 19, we only use the fact that the flip chain is local and the metric d is\n\n2-equivalent. When verifying the second condition, we use the contraction results in Proposition 21 or Proposition 22 for the flip chain $Q$ over list colourings of $\\left(G^{\\prime}, L^{\\prime}\\right)$.\n\nCorollary 6 is then a direct consequence of Theorem 4, Theorem 20, and the marginal lower bound [LY13, Lemma 3].\n5.2. Application: coupling independence via the Dobrushin-Shlosman condition. In this section we show Corollary 8. Recall the Dobrushin-Shlosman condition from Definition 7.\n\nTheorem 23. Let $\\delta \\in(0,1)$ be a parameter. Suppose $\\mu$ is the Gibbs distribution of a permissive spin system $\\left(G, q, A_{E}, A_{V}\\right)$ satisfying the Dobrushin-Shlosman condition with gap $\\delta$. Then, $\\mu$ satisfies $\\left(\\frac{\\Delta}{\\delta}+1\\right)$-coupling independence.\n\nThe proof of Theorem 23 follows from Lemma 19 and a well-known contractive coupling for the Glauber dynamics when the 1-norm of the Dobrushin influence matrix $\\rho$ is bounded (see [BD97, Hay06] and references therein). Here Glauber dynamics is a well-studied Markov chain where in each step, we uniformly at random select a variable, and update it conditioning on the rest of the configuration.\n\nProposition 24. Let $\\delta \\in(0,1)$ be a parameter. Suppose $\\mu$ is a distribution over $[q]^{V}$ satisfying the Dobrushin-Shlosman condition with gap $\\delta$. Let $P$ be the Glauber dynamics for $\\mu$. Then for every $X, Y \\in[q]^{V}$\n\n$$\n\\mathcal{W}_{\\text {dist }}(P(X), P(Y)) \\leq\\left(1-\\frac{\\delta}{n}\\right) \\operatorname{dist}(X, Y)\n$$\n\nThe above contractive coupling result holds for all $X, Y \\in[q]^{V}$ because the definition of Dobrushin influence in (3) considers all possible pinnings, including improper pinnings for $\\mu$.\n\nProof of Theorem 23. Let $\\sigma$ and $\\tau$ be two (not necessarily feasible) partial configuration on $\\Lambda \\subseteq V$ such that they differ at only $v \\in V$. Let $V^{\\prime}=V \\backslash \\Lambda$. As the system is permissive, $\\mu_{V^{\\prime}}^{\\sigma}$ and $\\mu_{V^{\\prime}}^{\\tau}$ are well-defined. To apply Lemma 19, let $P$ and $Q$ be Glauber dynamics for $\\mu_{V^{\\prime}}^{\\sigma}$ and $\\mu_{V^{\\prime}}^{\\tau}$, respectively. Consider the coupling where we always choose the same vertex $u \\in V^{\\prime}$ to update, and optimally couple the updates at $u$. Clearly, only neighbours of $v$ could be the new disagreement. Thus,\n\n$$\n\\mathcal{W}(P(X), Q(X)) \\leq \\frac{\\Delta}{\\left|V^{\\prime}\\right|}\n$$\n\nThis verifies the first condition of Lemma 19.\nFor the second condition of Lemma 19, notice that the Dobrushin influence matrix $\\rho^{\\tau}$ for $\\mu_{V^{\\prime}}^{\\tau}$ is dominated by the corresponding matrix $\\rho$ for $\\mu$. Thus, $\\left\\|\\rho^{\\tau}\\right\\|_{1} \\leq\\|\\rho\\|_{1} \\leq 1-\\delta$. By Proposition 24, $Q$ has $\\delta /\\left|V^{\\prime}\\right|$-contraction. Note that $\\sigma$ and $\\tau$ differ only at a single vertex $v$. Thus, we can apply Lemma 19 to derive\n\n$$\n\\mathcal{W}\\left(\\mu^{\\sigma}, \\mu^{\\tau}\\right)=\\mathcal{W}\\left(\\mu_{V^{\\prime}}^{\\sigma}, \\mu_{V^{\\prime}}^{\\tau}\\right)+1 \\leq \\frac{\\Delta}{\\delta}+1\n$$\n\nCorollary 8 directly follows from Theorem 4, Observation 10, and Theorem 23.", "tables": {}, "images": {}}, {"section_id": 8, "text": "# ACKNOWLEDGEMENT \n\nWe thank Konrad Anand and Graham Freifeld for some helpful discussions at an early stage of this project. We thank Charlie Carlson, Eric Vigoda, and Hongyang Liu for clarifying some questions regarding the flip dynamics and useful feedback.", "tables": {}, "images": {}}, {"section_id": 9, "text": "# REFERENCES \n\n[AL20] Vedat Levi Alev and Lap Chi Lau. Improved analysis of higher order random walks and applications. In STOC, pages 1198-1211. ACM, 2020. 2\n[ALO20] Nima Anari, Kuikui Liu, and Shayan Oveis Gharan. Spectral independence in high-dimensional expanders and applications to the hardcore model. In FOCS, pages 1319-1330. IEEE, 2020. 2, 7\n[Bar16] Alexander I. Barvinok. Combinatorics and Complexity of Partition Functions, volume 30 of Algorithms and combinatorics. Springer, 2016.1\n[BBR24] Ferenc Bencs, Khallil Berrekkal, and Guus Regts. Deterministic approximate counting of colorings with fewer than $2 \\delta$ colors via absence of zeros. arXiv, abs/2408.04727, 2024. 2, 4\n[BCC+22] Antonio Blanca, Pietro Caputo, Zongchen Chen, Daniel Parisi, Daniel \u0160tefankovi\u010d, and Eric Vigoda. On mixing of Markov chains: coupling, spectral independence, and entropy factorization. Electron. J. Probab., 27:Paper No. 142, 42, 2022. 2, 3, 20, 22\n[BD97] Russ Bubley and Martin E. Dyer. Path coupling: A technique for proving rapid mixing in markov chains. In FOCS, pages 223-231. IEEE Computer Society, 1997. 4, 22, 24\n[BF87] Imre B\u00e1r\u00e1ny and Zolt\u00e1n F\u00fcredi. Computing the volume is difficult. Discret. Comput. Geom., 2(4):319-326, 1987. 1\n[BG06] Antar Bandyopadhyay and David Gamarnik. Counting without sampling: new algorithms for enumeration problems using statistical physics. In SODA, pages 890-899. ACM, 2006. 1, 5\n[BN19] Guy Bresler and Dheeraj Nagaraj. Stein's method for stationary distributions of Markov chains and application to Ising models. Ann. Appl. Probab., 29(5):3230-3265, 2019. 20\n[CDM ${ }^{+}$19] Sitan Chen, Michelle Delcourt, Ankur Moitra, Guillem Perarnau, and Luke Postle. Improved bounds for randomly sampling colorings via linear programming. In SODA, pages 2216-2234. SIAM, 2019. 2, 4, 5, 21,22\n[CF24] Xiaoyu Chen and Weiming Feng. Rapid mixing via coupling independence for spin systems with unbounded degree. arXiv, abs/2407.04672, 2024.4\n[CG24] Zongchen Chen and Yuzhou Gu. Fast sampling of $b$-matchings and $b$-edge covers. In SODA, pages 49724987. SIAM, 2024. 4\n[CGSV21] Zongchen Chen, Andreas Galanis, Daniel Stefankovic, and Eric Vigoda. Rapid mixing for colorings via spectral independence. In SODA, pages 1548-1557. SIAM, 2021. 4, 7\n[CLMM23] Zongchen Chen, Kuikui Liu, Nitya Mani, and Ankur Moitra. Strong spatial mixing for colorings on trees and its algorithmic applications. In FOCS, pages 810-845. IEEE, 2023. 3, 5, 10, 20\n[CLV23] Zongchen Chen, Kuikui Liu, and Eric Vigoda. Rapid mixing of Glauber dynamics up to uniqueness via contraction. SIAM J. Comput., 52(1):196-237, 2023. 4\n[CV24] Charlie Carlson and Eric Vigoda. Flip dynamics for sampling colorings: Improving (11/6- $\\epsilon$ ) using a simple metric. arXiv, abs/2407.04870, 2024. 2, 4, 5, 21, 22\n[CWZZ25] Zejia Chen, Yulin Wang, Chihao Zhang, and Zihan Zhang. Decay of correlation for edge colorings when $q \\geq 3 \\Delta$. arXiv, abs/2502.06586, 2025. 4\n[CZ23] Xiaoyu Chen and Xinyuan Zhang. A near-linear time sampler for the Ising model with external field. In SODA, pages 4478-4503. SIAM, 2023. 2, 4\n[DFK91] Martin E. Dyer, Alan M. Frieze, and Ravi Kannan. A random polynomial time algorithm for approximating the volume of convex bodies. J. ACM, 38(1):1-17, 1991. 1\n[Dob70] Roland L. Dobrushin. Prescribing a system of random variables by conditional distributions. Theor. Probability Appl., 15(3):458-486, 1970. 4\n[DS85] Roland L. Dobrushin and Senya B. Shlosman. Constructive criterion for the uniqueness of Gibbs field. In Statistical physics and dynamical systems (K\u00f6szeg, 1984), volume 10 of Progr. Phys., pages 347-370. Birkh\u00e4user Boston, Boston, MA, 1985. 4\n[Ele86] Gy\u00f6rgy Elekes. A geometric inequality and the complexity of computing volume. Discret. Comput. Geom., $1(4): 289-292,1986.1$\n[FGW+23] Weiming Feng, Heng Guo, Chunyang Wang, Jiaheng Wang, and Yitong Yin. Towards derandomising Markov chain Monte Carlo. In FOCS, pages 1963-1990. IEEE, 2023. 1\n[FGYZ22] Weiming Feng, Heng Guo, Yitong Yin, and Chihao Zhang. Rapid mixing from spectral independence beyond the Boolean domain. ACM Trans. Algorithms, 18(3):28:1-28:32, 2022. 2, 4, 7\n[GK07] David Gamarnik and Dmitriy Katz. Correlation decay and deterministic FPTAS for counting list-colorings of a graph. In SODA, pages 1245-1254. SIAM, 2007. 2\n[GKM15] David Gamarnik, Dmitriy Katz, and Sidhant Misra. Strong spatial mixing of list coloring of graphs. Random Struct. Algorithms, 46(4):599-613, 2015. 3, 20\n\n[GLLZ19] Heng Guo, Chao Liao, Pinyan Lu, and Chihao Zhang. Counting hypergraph colorings in the local lemma regime. SIAM J. Comput., 48(4):1397-1424, 2019. 1, 5\n[Hay06] Thomas P. Hayes. A simple condition implying rapid mixing of single-site dynamics on spin systems. In FOCS, pages 39-46. IEEE Computer Society, 2006. 4, 24\n[HLQZ25] Kun He, Zhidan Li, Guoliang Qiu, and Chihao Zhang. FPTAS for Holant problems with log-concave signatures. In SODA, pages 5468-5503. SIAM, 2025. 1, 4, 5\n[HPR20] Tyler Helmuth, Will Perkins, and Guus Regts. Algorithmic Pirogov-Sinai theory. Probab. Theory Related Fields, 176(3-4):851-895, 2020.1\n[Jer95] Mark Jerrum. A very simple algorithm for estimating the number of $k$-colorings of a low-degree graph. Random Struct. Algorithms, 7(2):157-166, 1995. 1\n[JPP23] Matthew Jenssen, Will Perkins, and Aditya Potukuchi. Approximately counting independent sets in bipartite graphs via graph containers. Random Struct. Algorithms, 63(1):215-241, 2023. 1\n[JPSS22] Vishesh Jain, Will Perkins, Ashwin Sah, and Mehtaab Sawhney. Approximate counting and sampling via local central limit theorems. In STOC, pages 1473-1486. ACM, 2022. 1\n[JPV21] Vishesh Jain, Huy Tuan Pham, and Thuy-Duong Vuong. Towards the sampling Lov\u00e1sz local lemma. In FOCS, pages 173-183. IEEE, 2021. 1, 5\n[JVV86] Mark Jerrum, Leslie G. Valiant, and Vijay V. Vazirani. Random generation of combinatorial structures from a uniform distribution. Theor. Comput. Sci., 43:169-188, 1986. 19\n[Liu21] Kuikui Liu. From coupling to spectral independence and blackbox comparison with the down-up walk. In APPROX-RANDOM, volume 207 of LIPIcs, pages 32:1-32:21. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2021. 2, 3, 20\n[LSS19a] Jingcheng Liu, Alistair Sinclair, and Piyush Srivastava. A deterministic algorithm for counting colorings with 2-delta colors. In FOCS, pages 1380-1404. IEEE Computer Society, 2019. 2, 4\n[LSS19b] Jingcheng Liu, Alistair Sinclair, and Piyush Srivastava. The Ising partition function: zeros and deterministic approximation. J. Stat. Phys., 174(2):287-315, 2019. 4\n[LY13] Pinyan Lu and Yitong Yin. Improved FPTAS for multi-spin systems. In APPROX-RANDOM, volume 8096 of Lecture Notes in Computer Science, pages 639-654. Springer, 2013. 2, 4, 24\n[Moi19] Ankur Moitra. Approximate counting, the Lov\u00e1sz local lemma, and inference in graphical models. J. $A C M, 66(2): 10: 1-10: 25,2019.1,5,10,12,26,27$\n[PR17] Viresh Patel and Guus Regts. Deterministic polynomial-time approximation algorithms for partition functions and graph polynomials. SIAM J. Comput., 46(6):1893-1919, 2017. 1\n[RR19] Gesine Reinert and Nathan Ross. Approximating stationary distributions of fast mixing Glauber dynamics, with applications to exponential random graphs. Ann. Appl. Probab., 29(5):3201-3229, 2019. 20\n[SST14] Alistair Sinclair, Piyush Srivastava, and Marc Thurley. Approximation algorithms for two-state antiferromagnetic spin systems on bounded degree graphs. J. Stat. Phys., 155(4):666-686, 2014. 4\n[Vig00] Eric Vigoda. Improved bounds for sampling colorings. J. Math. Phys., 41(3):1555-1569, 2000. 1, 21\n[Wei06] Dror Weitz. Counting independent sets up to the tree threshold. In STOC, pages 140-149. ACM, 2006. 1,5\n[WY24] Chunyang Wang and Yitong Yin. A sampling lov\u00e1sz local lemma for large domain sizes. In FOCS, pages 129-150. IEEE, 2024. 1, 5", "tables": {}, "images": {}}, {"section_id": 10, "text": "# APPENDIX A. HEURISTICS BEHIND THE LINEAR PROGRAM TO SOLVE COUPLINGS \n\nIn this section we provide some heuristics behind the linear programming approach of Moitra [Moi19], used in Section 4.1. Let $\\sigma$ and $\\tau$ be two partial configurations on $\\Lambda$ such that they differ on one vertex, say, $v$. Let $\\Omega_{\\sigma}$ be the set of states that are consistent with $\\sigma$, and similarly for $\\Omega_{\\tau}$. Suppose $\\mathcal{C}$ over $\\Omega_{\\sigma} \\times \\Omega_{\\tau}$ is a coupling between $\\mu^{\\sigma}$ and $\\mu^{\\tau}$.\n\nAs $\\mathcal{C}$ is a valid coupling, it must satisfy\n\n$$\n\\begin{array}{ll}\n\\forall \\sigma^{\\prime} \\in \\Omega_{\\sigma}, & \\sum_{\\tau^{\\prime} \\in \\Omega_{\\tau}} \\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right)=\\mu^{\\sigma}\\left(\\sigma^{\\prime}\\right) \\\\\n\\forall \\tau^{\\prime} \\in \\Omega_{\\tau}, & \\sum_{\\sigma^{\\prime} \\in \\Omega_{\\sigma}} \\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right)=\\mu^{\\tau}\\left(\\tau^{\\prime}\\right)\n\\end{array}\n$$\n\nUsing $\\mu^{\\sigma}\\left(\\sigma^{\\prime}\\right)=\\frac{\\mu\\left(\\sigma^{\\prime}\\right)}{\\mu_{\\Lambda}(\\sigma)}$ and $\\mu^{\\tau}\\left(\\tau^{\\prime}\\right)=\\frac{\\mu\\left(\\tau^{\\prime}\\right)}{\\mu_{\\Lambda}(\\tau)}$, we have\n\n$$\n\\begin{aligned}\n& \\forall \\sigma^{\\prime} \\in \\Omega_{\\sigma}, \\quad \\sum_{\\tau^{\\prime} \\in \\Omega_{\\tau}} \\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\mu_{\\Lambda}(\\sigma) / \\mu\\left(\\sigma^{\\prime}\\right)=1 \\\\\n& \\forall \\tau^{\\prime} \\in \\Omega_{\\tau}, \\quad \\sum_{\\sigma^{\\prime} \\in \\Omega_{\\sigma}} \\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\mu_{\\Lambda}(\\tau) / \\mu\\left(\\tau^{\\prime}\\right)=1\n\\end{aligned}\n$$\n\nThis gives us a linear system, where we may treat $\\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\mu_{\\Lambda}(\\sigma) / \\mu\\left(\\sigma^{\\prime}\\right)$ and $\\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\mu_{\\Lambda}(\\tau) / \\mu\\left(\\tau^{\\prime}\\right)$ as variables. This system is under constrained and the coupling is not unique, while our goal is to solve $r=\\frac{\\mu_{\\Lambda}(\\sigma)}{\\mu_{\\Lambda}(\\tau)}$. To do so, notice that for any $\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\in \\Omega_{\\sigma} \\times \\Omega_{\\tau}$,\n\n$$\nr=\\frac{\\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\mu_{\\Lambda}(\\sigma) / \\mu\\left(\\sigma^{\\prime}\\right)}{\\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\mu_{\\Lambda}(\\tau) / \\mu\\left(\\tau^{\\prime}\\right)} \\cdot \\frac{\\mu\\left(\\sigma^{\\prime}\\right)}{\\mu\\left(\\tau^{\\prime}\\right)}\n$$\n\nAdding $r$ as a variable and (22) to the system would make the system non-linear. Moreover, it also makes the system less robust. Instead, we introduce variables $x_{\\sigma^{\\prime}, \\tau^{\\prime}}$ to represent $\\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\mu_{\\Lambda}(\\sigma) / \\mu\\left(\\sigma^{\\prime}\\right)$, $y_{\\sigma^{\\prime}, \\tau^{\\prime}}$ to represent $\\mathcal{C}\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\mu_{\\Lambda}(\\tau) / \\mu\\left(\\tau^{\\prime}\\right)$, and $r^{-}$and $r^{+}$as guessed lower and upper bound for $r$. Then consider the following set of linear equalities and inequalities:\n\n$$\n\\begin{aligned}\n& \\forall \\sigma^{\\prime} \\in \\Omega_{\\sigma}, \\quad \\sum_{\\tau^{\\prime} \\in \\Omega_{\\tau}} x_{\\sigma^{\\prime}, \\tau^{\\prime}}=1 \\\\\n& \\forall \\tau^{\\prime} \\in \\Omega_{\\tau}, \\quad \\sum_{\\sigma^{\\prime} \\in \\Omega_{\\sigma}} y_{\\sigma^{\\prime}, \\tau^{\\prime}}=1 \\\\\n& \\forall\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\in \\Omega_{\\sigma} \\times \\Omega_{\\tau}, \\quad r^{-} y_{\\sigma^{\\prime}, \\tau^{\\prime}} \\leq x_{\\sigma^{\\prime}, \\tau^{\\prime}} \\cdot \\frac{\\mu\\left(\\sigma^{\\prime}\\right)}{\\mu\\left(\\tau^{\\prime}\\right)} \\leq r^{+} y_{\\sigma^{\\prime}, \\tau^{\\prime}}\n\\end{aligned}\n$$\n\nNote that while we cannot compute $\\mu\\left(\\sigma^{\\prime}\\right)$ or $\\mu\\left(\\tau^{\\prime}\\right)$ easily, their ratio $\\frac{\\mu\\left(\\sigma^{\\prime}\\right)}{\\mu\\left(\\tau^{\\prime}\\right)}$ is easy to compute exactly.\nIt is easy to see that if $r^{-} \\leq r \\leq r^{+}$, then the system (23) has a solution. On the other hand, if the system (23) has a solution, we have that\n\n$$\nr^{-} \\leq \\frac{\\sum_{\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\in \\Omega_{\\sigma} \\times \\Omega_{\\tau}} x_{\\sigma^{\\prime}, \\tau^{\\prime}} \\mu\\left(\\sigma^{\\prime}\\right)}{\\sum_{\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\in \\Omega_{\\sigma} \\times \\Omega_{\\tau}} y_{\\sigma^{\\prime}, \\tau^{\\prime}} \\mu\\left(\\tau^{\\prime}\\right)} \\leq r^{+}\n$$\n\nNotice that\n\n$$\n\\begin{aligned}\n\\sum_{\\left(\\sigma^{\\prime}, \\tau^{\\prime}\\right) \\in \\Omega_{\\sigma} \\times \\Omega_{\\tau}} x_{\\sigma^{\\prime}, \\tau^{\\prime}} \\mu\\left(\\sigma^{\\prime}\\right) & =\\sum_{\\sigma^{\\prime} \\in \\Omega_{\\sigma}} \\mu\\left(\\sigma^{\\prime}\\right) \\sum_{\\tau^{\\prime} \\in \\Omega_{\\tau}} x_{\\sigma^{\\prime}, \\tau^{\\prime}} \\\\\n& =\\sum_{\\sigma^{\\prime} \\in \\Omega_{\\sigma}} \\mu\\left(\\sigma^{\\prime}\\right)=\\mu_{\\Lambda}(\\sigma)\n\\end{aligned}\n$$\n\nwhere in the second line we used the first constraint in (23). Similarly for $\\tau$. Thus, (24) implies that $r^{-} \\leq r \\leq r^{+}$. In conclusion, the system (23) has a solution if and only if $r^{-} \\leq r \\leq r^{+}$. Therefore, we can do a binary search to find a very accurate estimate to $r$ by repeatedly solving the LP (23).\n\nThere is one issue with the above though, namely the system has an exponential size. Moitra [Moi19] considered constructing the coupling in a greedy way, instead of listing all final outcomes at once. He greedily couples vertices one by one in an exploratory way, conditioning on previous choices at each step. Each intermediate state gets its own $x$ and $y$ variables, and the transition probabilities are reflected by linear constraints.\n\nIn fact, in Moitra's process, we can stop at any pair of intermediate partial configurations $\\sigma_{0}$ and $\\tau_{0}$ over $\\Lambda_{0}$, and write down the corresponding LP. The main issue of doing this is that there is no good way of computing $\\frac{\\mu_{\\Lambda_{0}}\\left(\\sigma_{0}\\right)}{\\mu_{\\Lambda_{0}}\\left(\\tau_{0}\\right)}$. Moitra's choice is to prioritise getting the same boundary between $\\sigma_{0}$ and $\\tau_{0}$. If this is achieved, and the inside of the boundary has a logarithmic size, then\n\n$\\frac{\\mu_{\\lambda_{0}}\\left(\\sigma_{0}\\right)}{\\mu_{\\lambda_{0}}\\left(\\tau_{0}\\right)}$ can be computed efficiently. The key property for Moitra's process to succeed is to have an exponentially small (in the number of steps of the coupling process) probability of failing to get the same boundary between the two copies. This property guarantees that one can truncate the coupling process at an logarithmic depth, and maintain the size of the LP to be a polynomial in the input size. To certify the exponential tail of failure probability, his LP involves linear constraints derived from local uniformity for each transition step, which no longer holds in our setting.\n\nThe main innovation of our approach in Section 4.1 is that we do not try to efficiently compute $\\frac{\\mu_{\\lambda_{0}}\\left(\\sigma_{0}\\right)}{\\mu_{\\lambda_{0}}\\left(\\tau_{0}\\right)}$ for intermediate states. Instead, we use recursion. To do so, notice that our partial coupling, Algorithm 1, outputs $\\left(\\sigma_{0}, \\tau_{0}\\right)$ that either share the same boundary or differ by exactly 2 vertices. In the first case, $\\frac{\\mu_{\\lambda_{0}}\\left(\\sigma_{0}\\right)}{\\mu_{\\lambda_{0}}\\left(\\tau_{0}\\right)}$ can be computed exactly and efficiently. In the second case, there is a partial configuration $\\rho$ such that both $\\left(\\sigma_{0}, \\rho\\right)$ and $\\left(\\rho, \\tau_{0}\\right)$ differ on only one vertex. As $\\frac{\\mu_{\\lambda_{0}}\\left(\\sigma_{0}\\right)}{\\mu_{\\lambda_{0}}\\left(\\tau_{0}\\right)}=$ $\\frac{\\mu_{\\lambda_{0}}\\left(\\sigma_{0}\\right)}{\\mu_{\\lambda_{0}}\\left(\\rho\\right)} \\cdot \\frac{\\mu_{\\lambda_{0}}\\left(\\rho\\right)}{\\mu_{\\lambda_{0}}\\left(\\tau_{0}\\right)}$, we apply recursion to approximate both $\\frac{\\mu_{\\lambda_{0}}\\left(\\sigma_{0}\\right)}{\\mu_{\\lambda_{0}}\\left(\\rho\\right)}$ and $\\frac{\\mu_{\\lambda_{0}}\\left(\\rho\\right)}{\\mu_{\\lambda_{0}}\\left(\\tau_{0}\\right)}$. Doing so apparently doubles the approximation error. However, this error occurs only in the second case, where the partial coupling exits early. Luckily, the probability of early exits is $O(\\delta(R) R \\log \\Delta)$. As $\\delta(R)$ decays exponentially with $R$, we can make this probability as small as we want. There is one more wrinkle, that is, we cannot really write down linear constraints that exactly capture the early exit probability, because doing so would involve probabilities that we cannot compute efficiently. Instead, we choose to use the marginal lower bound $b$ and the total influence bound in our linear program to give an upper bound of the early exit probability. See the overflow constraints in Algorithm 3. Eventually, we choose $R$ such that $\\delta(R)$ absorbs $R \\log \\Delta$ together with some polynomial factors in $b^{-1}$. This makes sure that the error decays by a constant factor at each recursive call.", "tables": {}, "images": {}}], "id": "2410.23225v2", "authors": ["Xiaoyu Chen", "Weiming Feng", "Heng Guo", "Xinyuan Zhang", "Zongrui Zou"], "categories": ["cs.DS", "cs.DM"], "abstract": "We show that spin systems with bounded degrees and coupling independence\nadmit fully polynomial time approximation schemes (FPTAS). We design a new\nrecursive deterministic counting algorithm to achieve this. As applications, we\ngive the first FPTASes for $q$-colourings on graphs of bounded maximum degree\n$\\Delta\\ge 3$, when $q\\ge (11/6-\\varepsilon_0)\\Delta$ for some small\n$\\varepsilon_0\\approx 10^{-5}$, or when $\\Delta\\ge 125$ and $q\\ge 1.809\\Delta$,\nand on graphs with sufficiently large (but constant) girth, when\n$q\\geq\\Delta+3$. These bounds match the current best randomised approximate\ncounting algorithms by Chen, Delcourt, Moitra, Perarnau, and Postle (2019),\nCarlson and Vigoda (2024), and Chen, Liu, Mani, and Moitra (2023),\nrespectively.", "updated": "2025-04-05T04:05:43Z", "published": "2024-10-30T17:14:54Z"}