{
  "title": "Generalized Principal-Agent Problem with a Learning Agent",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nClassic principal-agent problems such as Stackelberg games, contract design, and Bayesian persuasion, often assume that the agent is able to best respond to the principal's committed strategy. We study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) If the agent uses contextual no-regret learning algorithms with regret $\\operatorname{Reg}(T)$, then the principal can guarantee utility at least $U^{*}-\\Theta\\left(\\sqrt{\\frac{\\operatorname{Reg}(T)}{T}}\\right)$, where $U^{*}$ is the principal's optimal utility in the classic model with a best-responding agent. (2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret $\\operatorname{SReg}(T)$, then the principal cannot obtain utility more than $U^{*}+O\\left(\\frac{\\operatorname{SReg}(T)}{T}\\right)$. But (3) if the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can sometimes do significantly better than $U^{*}$. These results not only refine previous results in Stackelberg games and contract design, but also lead to new results for Bayesian persuasion with a learning agent and all generalized principal-agent problems where the agent does not have private information.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "## 1 Introduction\n\nClassic economic models of principal-agent interactions, including auction design, contract design, and Bayesian persuasion, often assume that the agent is able to best respond to the strategy committed by the principal. For example, in Bayesian persuasion, the agent (receiver) needs to compute the posterior belief about the state of the world after receiving some information from the principal (sender) and take an optimal action based on the posterior belief; this requires the receiver accurately knowing the prior of the state as well as the signaling scheme used by the sender. In contract design, where a principal specifies an outcome-dependent payment scheme to incentivize the agent to take certain actions, the agent has to know the action-dependent outcome distribution in order to best respond to the contract. Requiring strong rationality assumptions, the best-responding behavior is often observed to be violated in practice (Camerer, 1998; Benjamin, 2019).\n\nIn this work, using Bayesian persuasion as the main example, we study general principal-agent problems under an alternative behavioral model for the agent: learning. The use of learning as a behavioral model dates back to early economic literature on learning in games (Brown, 1951; Fudenberg and Levine, 1998) and has been actively studied by computer scientists in recent years\n\n[^0]\n[^0]:    *A short version of this paper has been accepted by ICLR'25 (spotlight).\n    ${ }^{\\dagger}$ Harvard University, tlingg.harvard.edu.\n    ${ }^{\\ddagger}$ Harvard University, yiling@seas.harvard.edu.\n\n(Nekipelov et al., 2015; Braverman et al., 2018; Deng et al., 2019; Mansour et al., 2022; Cai et al., 2024; Lin et al., 2023; Rubinstein and Zhao, 2024; Guruganesh et al., 2024; Scheid et al., 2024). A learning agent no longer has perfect knowledge of the parameter of the game or the principal's strategy. Instead of best responding, which is no longer possible or well-defined, the agent chooses his action based on past interactions with the principal. We focus on no-regret learning, which requires the agent to not suffer a large average regret at the end of repeated interactions with the principal, for not taking the optimal action at hindsight. This is a mild requirement satisfied by many natural learning algorithms (e.g., $\\varepsilon$-greedy, MWU, UCB, EXP-3) and can reasonably serve as a possible behavioral assumption for real-world agents.\n\nWith a learning agent, can the principal achieve a better outcome than that in the classic model with a best-responding agent? Previous works on playing against learning agents (Deng et al., 2019; Guruganesh et al., 2024) showed that, in Stackelberg games and contract design, the leader/principal can obtain utility $U^{*}-o(1)$ against a no-regret learning follower/agent, where $U^{*}$ is the Stackelberg value, defined to be the principal's optimal utility in the classic model with a best-responding agent. On the other hand, if the agent does a stronger version of no-regret learning, called no-swapregret learning (Hart and Mas-Colell, 2000; Blum and Mansour, 2007), then the principal cannot obtain utility more than the Stackelberg value $U^{*}+o(1)$. Interestingly, the conclusion that noswap-regret learning can cap the principal's utility at $U^{*}+o(1)$ does not hold when the agent has private information, such as in auctions (Braverman et al., 2018) and Bayesian Stackelberg games (Mansour et al., 2022): the principal can sometimes exploit a no-swap-regret learning agent with private information to do much better than $U^{*}$ in those games.\n\nThree natural questions then arise: (1) What is the largest class of principal-agent problems under which the agent's no-swap-regret learning can cap the principal's utility at the Stackelberg value $U^{*}+o(1)$ ? (2) In cases where the principal's optimal utility against a learning agent is bounded by $\\left[U^{*}-o(1), U^{*}+o(1)\\right]$, what is the exact magnitude of the $o(1)$ terms? (3) Instead of analyzing games like Stackelberg games and contract design separately, can we analyze all principal-agent problems with learning agents in a unified way?\n\nOur contributions. Our work defines a general model of principal-agent problems with a learning agent, answering all questions (1) - (3). For (1), we show that the principal's utility is bounded around the Stackelberg value $U^{*}$ in all generalized principal-agent problems where the agent does not have private information but the principal can be privately informed. In particular, this includes complete-information games like Stackelberg games and contract design, as well as Bayesian persuasion where the sender/principal privately observes the state of the world.\n\nFor (2) and (3), we provide a unified analytical framework to derive tight bounds on the principal's achievable utility against a no-regret or no-swap-regret learning agent in all generalized principal-agent problems where the agent does not have private information. Specifically, we explicitly characterize the $o(1)$ difference between the principal's utility and $U^{*}$ in terms of the agent's regret.\nResult 1 (from Theorems 3.1, 4.2, 4.3). Against a no-regret learning agent with regret $\\operatorname{Reg}(T)$ in $T$ periods, the principal can obtain an average utility of at least $U^{*}-O\\left(\\sqrt{\\frac{\\operatorname{Reg}(T)}{T}}\\right)$. The principal can do this using a fixed strategy in all $T$ periods and only knowing the regret bound of the agent but not the exact learning algorithm.\nResult 2 (from Theorems 3.4, 4.2, 4.3). Against a no-swap-regret learning agent with swap-regret $\\operatorname{SReg}(T)$ in $T$ periods, the principal cannot obtain average utility larger than $U^{*}+O\\left(\\frac{\\operatorname{SReg}(T)}{T}\\right)$. This holds even if the principal knows the learning algorithm of the agent and uses time-varying\n\nstrategies.\nInterestingly, the squared root bound $U^{*}-O\\left(\\sqrt{\\frac{\\operatorname{Reg}(T)}{T}}\\right)$ in Result 1 and the linear bound $U^{*}+O\\left(\\frac{\\operatorname{SReg}(T)}{T}\\right)$ in Result 2 are not symmetric. We show that such an asymmetry is intrinsic: there exist cases where the principal cannot achieve better than $U^{*}-O\\left(\\sqrt{\\frac{\\operatorname{Reg}(T)}{T}}\\right)$ utility.\nResult 3 (from Theorem 3.3 and Example 4.1). There is a Bayesian persuasion instance where, for any strategy of the principal, there is a no-swap-regret learning algorithm for the agent under which the principal's utility is at most $U^{*}-\\Omega\\left(\\sqrt{\\frac{\\operatorname{SReg}(T)}{T}}\\right)$. The same holds for no-regret algorithms.\n\nResults 1, 2, 3 together characterize the range of utility achievable by the principal against a no-swap-regret learning agent: $\\left[U^{*}-\\Theta\\left(\\sqrt{\\frac{\\operatorname{SReg}(T)}{T}}\\right), U^{*}+O\\left(\\frac{\\operatorname{SReg}(T)}{T}\\right)\\right]$. As $T \\rightarrow \\infty$, the range converges to $U^{*}$, which means that the agent's no-swap-regret learning behavior is essentially equivalent to best responding behavior. This justifies the classical economic notion that the equilibria of games are results of repeated interactions between learning players.\n\nHowever, for no-regret but not necessarily no-swap-regret algorithms, the upper bound result $U^{*}+O\\left(\\frac{\\operatorname{Reg}(T)}{T}\\right)$ does not hold. The repeated interaction between a principal and a no-regret learning agent does not always lead to the Stackelberg equilibrium outcome $U^{*}$ :\nResult 4 (Theorem 3.5). There exists a Bayesian persuasion instance where, against a no-regret but not no-swap-regret learning agent (in particular, mean-based learning agent), the principal can do significantly better than the Stackelberg value $U^{*}$.\n\nIn summary, our Results 1, 2, 3 exactly characterize the principal's optimal utility in principalagent problems with a no-swap-regret agent, which not only refines previous works on playing against learning agents in specific games (Stackelberg games and contract design) but also generalizes to all principal-agent problems where the agent does not have private information. In particular, when applied to Bayesian persuasion, our results imply that the sender cannot exploit a no-swap-regret learning receiver even if the sender possesses informational advantage over the receiver.\n\nSome intuitions. As we alluded above, the main intuition behind our first three results is: the agent's learning behavior is closely related to approximately best response. A no-regret learning agent makes sub-optimal decisions with the sub-optimality measured by the regret. When the suboptimality/regret is small, the principal-agent problem with a no-regret agent (or approximately best responding agent) converges to the problem with an exactly best responding agent. This explains why the principal against a no-regret learning agent can obtain a payoff that is close to the optimal payoff $U^{*}$ against a best responding agent.\n\nHowever, there are two subtleties behind the above intuition.\nFirst, the intuition that a no-regret learning agent is approximately best responding is true only when the principal uses a fixed strategy throughout the interactions with the agent. If the principal uses time-varying strategies, then a no-regret agent is not necessarily approximately best responding to the \"average\" strategy of the principal across $T$ periods, while a no-swap-regret agent is still approximately best responding. This is because a no-swap-regret algorithm ensures that, whenever the algorithm recommends some action $a^{t}$ at a period $t$, it is almost optimal for the agent to take the recommended action $a^{t}$. But a no-regret algorithm only ensures the agent to not regret when comparing to taking any fixed action in all $T$ periods. The agent could have done better by deviating to different actions given different recommendations from the algorithm. This means\n\nthat a no-regret agent is not approximately best responding when the principal's strategy changes over time, which explains why the principal can exploit a no-regret agent sometimes (our Result 4).\n\nSecond, what is the reason for the asymmetry between the worst-case utility $U^{*}-\\Theta\\left(\\sqrt{\\frac{\\operatorname{SReg}(T)}{T}}\\right)$ and the best-case utility $U^{*}+O\\left(\\frac{\\operatorname{SReg}(T)}{T}\\right)$ that the principal can obtain against a no-swap-regret learning agent? Roughly speaking, a no-swap-regret learning agent is approximately best responding to the principal's average strategy over all $T$ periods, with the degree of approximate best response measured by the average regret $\\frac{\\operatorname{SReg}(T)}{T}=\\delta$. However, because no-swap-regret learning algorithms are randomized ${ }^{1}$, they correspond to randomized approximately best responding strategies of the agent that are worse than the best responding strategy by a margin of $\\delta$ in expectation, which means that the agent might take $\\sqrt{\\delta}$-sub-optimal actions with probability $\\sqrt{\\delta}$. That can cause a loss of 1 to the principal's utility with probability $\\sqrt{\\delta}$. So, the principal's expected utility can be decreased to $U^{*}-\\sqrt{\\delta}=U^{*}-\\sqrt{\\frac{\\operatorname{SReg}(T)}{T}}$ in the worst case. On the other hand, when considering the principal's best-case utility, we care about the $\\delta$-approximately-best-responding strategy of the agent that maximizes the principal's utility. That strategy turns out to be equivalent to a deterministic strategy that gives the principal a utility of at most $U^{*}+O(\\delta)=U^{*}+O\\left(\\frac{\\operatorname{SReg}(T)}{T}\\right)$. This explains the asymmetry between the worst-case and best-case bounds.\n\nStructure of the paper. We define our model of generalized principal-agent problems with a learning agent in Section 2. Since Bayesian persuasion is the main motivation of our work, we also present the specific model of persuasion with a learning agent in Section 2.3. We develop our main results in Sections 3 and 4, by first reducing the generalized principal-agent problem with a learning agent to the problem with approximate best response, then characterizing the problem with approximate best response. Section 6 offers additional discussions.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 1.1 Related Works \n\nLearning agents have been studied in principal-agent problems like auctions (Braverman et al., 2018; Cai et al., 2024; Rubinstein and Zhao, 2024; Kumar et al., 2024), bimatrix Stackelberg games (Deng et al., 2019; Mansour et al., 2022; Arunachaleswaran et al., 2024), contract design (Guruganesh et al., 2024; Scheid et al., 2024), and Bayesian persuasion (Lin et al., 2023; Jain and Perchet, 2024). These problems belong to the class of generalized principal-agent problems (Myerson, 1982; Gan et al., 2024). We thus propose a general framework of generalized principal-agent problem with a learning agent, which encompasses several previous models, refines previous results, and provides new results.\n\nCamara et al. (2020) also propose a general framework of principal-agent problems with learning players, but has two key differences with ours: (1) They drop the common prior assumption while we still keep it. This assumption allows us to compare the principal's utility in the learning model with the classic model with common prior. (2) Their principal has commitment power, which is reasonable in, e.g., auction design, but less realistic in information design where the principal's strategy is a signaling scheme. Our principal does not commit.\n\nDeng et al. (2019) show that the follower's no-swap-regret learning can cap the leader's utility at $U^{*}+o(1)$ in Stackelberg games. We find that this conclusion holds for all generalized principalagent problems where the agent does not have private information. This conclusion does not hold when the agent is privately informed, as shown by Mansour et al. (2022) in Bayesian Stackelberg\n\n[^0]\n[^0]:    ${ }^{1}$ It is well known that deterministic algorithms cannot satsify the no-regret property (see, e.g., Roughgarden (2016)).\n\ngames. We view our work as characterizing the largest class of games under which this conclusion holds.\n\nThe literature on information design (Bayesian persuasion) has investigated various relaxations of the strong rationality assumptions in the classic models. For the sender, known prior (Camara et al., 2020; Ziegler, 2020; Zu et al., 2021; Kosterina, 2022; Wu et al., 2022; Dworczak and Pavan, 2022; Harris et al., 2023; Lin and Li, 2025) and known utility (Babichenko et al., 2021; Castiglioni et al., 2020; Feng et al., 2022; Bacchiocchi et al., 2024) are relaxed. For the receiver, the receiver may make mistakes in Bayesian updates (de Clippel and Zhang, 2022), be risk-conscious (Anunrojwong et al., 2023), do quantal response (Feng et al., 2024) or approximate best response (Yang and Zhang, 2024). Independently and concurrently of us, Jain and Perchet (2024) also study Bayesian persuasion with a learning agent. Their work has a few differences with us: First, their model is a general Bayesian persuasion model with imperfect and non-stationary dynamics for the state of the world. Our model generalizes Bayesian persuasion in another direction (namely, generalized principal-agent problems), while still assuming a perfect and stationary environment. Second, their results are qualitatively similar to our Result 1 and Result 4, while our results are more quantitative and precise. Third, we additionally show that no-swap-regret learning can cap the sender's utility (Result 2).\n\nAs our problem reduces to generalized principal-agent problems with approximate best response, our work is also related to recent works on approximately-best-responding agents in Stackelberg games (Gan et al., 2023) and Bayesian persuasion (Yang and Zhang, 2024). We focus on the range of payoff that can be obtained by a computationally-unbounded principal, ignoring the computational aspect considered by Gan et al. (2023); Yang and Zhang (2024). Besides the \"maxmin/robust\" objective, we also study the \"maxmax\" objective where the agent approximately best responds in favor of the principal, which is usually not studied in the literature.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 2 Generalized Principal-Agent Problem with a Learning Agent \n\nThis section defines our model, generalized principal-agent problem with a learning agent. This model includes Stackelberg games, contract design, and Bayesian persuasion with learning agents.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "### 2.1 Generalized Principal-Agent Problem\n\nGeneralized principal-agent problem, proposed by Myerson (1982); Gan et al. (2024), is a general model that includes auction design, contract design, Stackelberg games, and Bayesian persuasion. While Myerson (1982) and Gan et al. (2024) allow the agent to have private information, our model assumes an agent with no private information. There are two players in a generalized principalagent problem: a principal and an agent. The principal has a convex, compact decision space $\\mathcal{X}$ and the agent has a finite action set $A$. The principal and the agent have utility functions $u, v: \\mathcal{X} \\times A \\rightarrow \\mathbb{R}$. We assume that $u(x, a), v(x, a)$ are linear in $x \\in \\mathcal{X}$, which is satisfied by all the examples of generalized principal-agent problems we will consider (Bayesian persuasion, Stackelberg games, contract design). There is a signal/message set $S$. Signals are usually interpreted as recommendations of actions for the agent, where $S=A$, but we allow any signal set of size $|S| \\geq|A|$. A strategy of the principal is a distribution $\\pi \\in \\Delta(\\mathcal{X} \\times S)$ over pairs of decision and signal. When the utility functions $u, v$ are linear, it is without loss of generality to assume that the principal does not randomize over multiple decisions for one signal (Gan et al., 2024), namely, the principal chooses a distribution over signals and a unique decision $x_{s}$ associated with each signal $s \\in S$. So, we can write a principal strategy as $\\pi=\\left\\{\\left(\\pi_{s}, x_{s}\\right)\\right\\}_{s \\in S}$ where $\\pi_{s} \\geq 0$ is the probability\n\nof signal $s \\in S, \\sum_{s \\in S} \\pi_{s}=1$, and $x_{s} \\in \\mathcal{X}$. There are two variants of generalized principal-agent problems:\n\n- Unconstrained (Myerson, 1982): there is no restriction on the principal's strategy $\\pi$.\n- Constrained (Gan et al., 2024): the principal's strategy $\\pi$ has to satisfy constraint $\\sum_{s \\in S} \\pi_{s} x_{s} \\in$ $\\mathcal{C}$ where $\\mathcal{C} \\subseteq \\mathcal{X}$ is some convex set.\n\nUnconstrained generalized principal-agent problems include contract design and Stackelberg games. Constrained generalized principal-agent problems include Bayesian persuasion (see Section 2.3).\n\nIn a one-shot generalized principal-agent problem where the principal has commitment power, the principal first commits to a strategy $\\pi=\\left\\{\\left(\\pi_{s}, x_{s}\\right)\\right\\}_{s \\in S}$, then nature draws a signal $s \\in S$ according to the distribution $\\left\\{\\pi_{s}\\right\\}_{s \\in S}$ and sends $s$ to the agent (note: due to the commitment assumption, this is equivalent to revealing the pair $\\left(s, x_{s}\\right)$ to the agent), then the agent takes an action $a_{s} \\in \\arg \\max _{a \\in A} v\\left(x_{s}, a\\right)$ that maximizes its utility (breaking ties in favor of the principal), and the principal obtains utility $u\\left(x_{s}, a_{s}\\right)$. The principal aims to maximize its expected utility $\\mathbb{E}_{s \\sim \\pi}\\left[u\\left(x_{s}, a_{s}\\right)\\right]$ by choosing the strategy $\\pi$. Denote the maximal expected utility that the principal can obtain by $U^{*}$ :\n\n$$\nU^{*}=\\max _{\\pi} \\sum_{s \\in S} \\pi_{s} \\max _{a_{s} \\in \\arg \\max _{a \\in A} v\\left(x_{s}, a\\right)} u\\left(x_{s}, a_{s}\\right)\n$$\n\n$U^{*}$ is called the Stackelberg value in the literature.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 2.2 Learning Agent \n\nNow we define the model of generalized principal-agent problem with a learning agent. The game is repeated for $T$ rounds. Unlike the above static model, the principal now does not commit to its strategy $\\pi^{t}$ every round. The agent does not know the strategy $\\pi^{t}$ or the decision $x^{t}$ of the principal at each round. Instead, the agent uses some adaptive algorithm to learn from history which action to take in response to each possible signal. We allow the agent's strategy to be randomized.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "## Generalized Principal-Agent Problem with a Learning Agent\n\nIn each round $t=1, \\ldots, T$ :\n(1) Using some algorithm that learns from history (including signals, actions, and utility feedback in the past, described in details later), the agent chooses a strategy $\\rho^{t}: S \\rightarrow \\Delta(A)$ that maps each possible signal $s \\in S$ to a distribution over actions $\\rho^{t}(s) \\in \\Delta(A)$.\n(2) The principal chooses a strategy $\\pi^{t}=\\left\\{\\left(\\pi_{s}^{t}, x_{s}^{t}\\right)\\right\\}_{s \\in S}$, which is a distribution over signals $S$ and a decision $x_{s}^{t} \\in \\mathcal{X}$ associated with each signal.\n(3) Nature draws signal $s^{t} \\sim \\pi^{t}$ and reveals it. The principal makes decision $x^{t}=x_{s^{t}}^{t}$. The agent draws action $a^{t} \\sim \\rho^{t}\\left(s^{t}\\right)$.\n(4) The principal and the agent obtain utility $u^{t}=u\\left(x^{t}, a^{t}\\right)$ and $v^{t}=v\\left(x^{t}, a^{t}\\right)$. The agent observes some feedback (e.g., $v^{t}\\left(x^{t}, a^{t}\\right)$ or $\\left.x^{t}\\right)$.\n\nWe assume that the principal knows the utility functions $u$ and $v$ of both players, and has some knowledge about the agent's learning algorithm (which will be specified later). The principal's goal is to maximize the expected average utility $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right]$.\n\nCompared with the static model in Section 2.1 where the principal moves before the agent, we flip the decision-making order of the principal and the agent in the learning model: the agent\n\nmoves first by choosing $\\rho^{t}$, then the principal chooses $\\pi^{t}$. This gives the principal an opportunity to \"exploit\" the agent by choosing a $\\pi^{t}$ that best responds to $\\rho^{t}$, hence potentially do much better than the Stackelberg value $U^{*}$ where the principal moves first. However, one of our main results (Result 2 in the Introduction) will show that the principal cannot do much better than $U^{*}$ if the agent uses a particular type of learning algorithm, called contextual no-swap-regret algorithm, which we define below.\n\nAgent's learning problem. The agent's learning problem can be regarded as a contextual multiarmed bandit problem (Tyler Lu et al., 2010) where $A$ is the set of arms, and a signal $s^{t} \\in S$ serves as a context that affects the utility of each arm $a \\in A$. The agent picks an arm to pull based on the current context $s^{t}$ and the historical information about each arm under different contexts, adjusting its strategy over time based on the feedback collected after each round.\n\nWhat feedback can the agent observe after each round? One may assume that the agent sees the principal's decision $x^{t}$ after each round (this is call full-information feedback in the multi-armed bandit literature), or the utility $v^{t}=v\\left(x^{t}, a^{t}\\right)$ obtained in that round (this is called bandit feedback) but not the $x^{t}$, or some unbiased estimate of $v\\left(x^{t}, a^{t}\\right)$. We do not make specific assumptions on the feedback. All we need is that the feedback is sufficient for the agent to achieve contextual no-regret or contextual no-swap-regret, which are defined below:\n\nDefinition 2.1. The agent's learning algorithm is said to satisfy:\n\n- contextual no-regret if: there is a function $\\operatorname{CReg}(T)=o(T)^{2}$ such that, for any strategy of the principal, for any deviation function $d: S \\rightarrow A$, the regret of the agent not deviating according to $d$ is at most $\\operatorname{CReg}(T)$ :\n\n$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(v\\left(x^{t}, d\\left(s^{t}\\right)\\right)-v\\left(x^{t}, a^{t}\\right)\\right)\\right] \\leq \\operatorname{CReg}(T)\n$$\n\n- contextual no-swap-regret if: there is a function $\\operatorname{CSReg}(T)=o(T)$ such that, for any strategy of the principal, for any deviation function $d: S \\times A \\rightarrow A$, the regret of the receiver not deviating according to $d$ is at most $\\operatorname{CSReg}(T)$ :\n\n$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(v\\left(x^{t}, d\\left(s^{t}, a^{t}\\right)\\right)-v\\left(x^{t}, a^{t}\\right)\\right)\\right] \\leq \\operatorname{CSReg}(T)\n$$\n\nWe call $\\operatorname{CReg}(T)$ and $\\operatorname{CSReg}(T)$ the contextual regret and contextual swap-regret of the agent.\nContextual no-regret is implied by contextual no-swap-regret because the latter has a larger set of deviation functions. Contextual no-(swap-)regret algorithms are known to exist under bandit feedback. In fact, they can be easily constructed by running an ordinary no-(swap-)regret algorithm for each context independently. Formally:\n\nProposition 2.1. There exist learning algorithms with contextual regret $\\operatorname{CReg}(T)=O(\\sqrt{|A||\\bar{T}|T})$ and contextual swap-regret $\\operatorname{CSReg}(T)=O(|A| \\sqrt{|S| T})$. They can be constructed by running an ordinary no-(swap-)regret multi-armed bandit algorithm for each context independently.\n\nSee Appendix A for a proof of this Proposition.\n\n[^0]\n[^0]:    ${ }^{2} \\mathrm{~A}$ function $f(T)=o(T)$ means $\\frac{f(T)}{T} \\rightarrow 0$ as $T \\rightarrow+\\infty$. So, the average regret $\\frac{\\operatorname{CReg}(T)}{T} \\rightarrow 0$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 2.3 Special Case: Bayesian Persuasion with a Learning Agent \n\nWe show that Bayesian persuasion (Kamenica and Gentzkow, 2011) is a special case of constrained generalized principal-agent problems. We will also show that Bayesian persuasion is in fact equivalent to cheap talk (Crawford and Sobel, 1982) under our learning agent model.\n\nBayesian persuasion as a generalized principal-agent problem. There are two players in Bayesian persuasion: a sender (principal) and a receiver (agent). There are a finite set $\\Omega$ of states of the world, a signal set $S$, an action set $A$, a prior distribution $\\mu_{0} \\in \\Delta(\\Omega)$ over the states, and utility functions $u, v: \\Omega \\times A \\rightarrow \\mathbb{R}$ for the sender and the receiver. When the state is $\\omega \\in \\Omega$ and the receiver takes action $a \\in A$, the sender and the receiver obtain utility $u(\\omega, a), v(\\omega, a)$, respectively. Both players know $\\mu_{0}$, but only the sender has access to the realized state $\\omega \\sim \\mu_{0}$. The sender commits to some signaling scheme $\\pi: \\Omega \\rightarrow \\Delta(S)$, mapping any state to a probability distribution over signals, to partially reveal information about the state $w$ to the receiver. In the classic model, after receiving a signal $s \\in S$, the receiver will form the posterior belief $\\mu_{s} \\in \\Delta(\\Omega)$ about the state: $\\mu_{s}(\\omega)=\\frac{\\mu_{0}(\\omega) \\pi(s \\mid \\omega)}{\\pi_{s}}$, where $\\pi_{s}=\\sum_{\\omega \\in \\Omega} \\mu_{0}(\\omega) \\pi(s \\mid \\omega)$ is the total probability that signal $s$ is sent, and take an optimal action with respect to $\\mu_{s}$, i.e., $a_{s} \\in \\arg \\max _{a \\in A} \\sum_{\\omega \\in \\Omega} \\mu_{s}(\\omega) v(\\omega, a)$. The sender aims to find a signaling scheme to maximizde its expected utility $\\mathbb{E}\\left[u\\left(\\omega, a_{s}\\right)\\right]$.\n\nIt is well-known (Kamenica and Gentzkow, 2011) that a signaling scheme $\\pi: \\Omega \\rightarrow \\Delta(S)$ decomposes the prior $\\mu_{0}$ into a distribution over posteriors whose average is equal to the prior $\\mu_{0}$ :\n\n$$\n\\sum_{s \\in S} \\pi_{s} \\mu_{s}=\\mu_{0} \\in\\left\\{\\mu_{0}\\right\\}=: \\mathcal{C}, \\quad \\sum_{s \\in S} \\pi_{s}=1\n$$\n\nEquation (2) is called the Bayes plausibility condition. Conversely, any distribution over posteriors $\\left\\{\\left(p_{s}, \\mu_{s}\\right)\\right\\}_{s \\in S}$ satisfying Bayes plausibility $\\sum_{s \\in S} p_{s} \\mu_{s}=\\mu_{0}$ can be converted into a signaling scheme that sends signal $s$ with probability $p_{s}$. Thus, we can use a distribution over posteriors $\\left\\{\\left(\\pi_{s}, \\mu_{s}\\right)\\right\\}_{s \\in S}$ satisfying Bayes plausibility to represent a signaling scheme. Then, let's equate the posterior belief $\\mu_{s}$ in Bayesian persuasion to the principal's decision $x_{s}$ in the generalized principal-agent problem, so the principal/sender's decision space becomes $\\mathcal{X}=\\Delta(\\Omega)$. The Bayes plausibility condition (2) becomes the constraint in the constrained generalized principal-agent problem. When the agent/receiver takes action $a$, the principal/sender's (expected) utility under decision/posterior $x_{s}=\\mu_{s}$ is $u\\left(x_{s}, a\\right)=\\mathbb{E}_{\\omega \\sim \\mu_{s}} u(\\omega, a)=\\sum_{\\omega \\in \\Omega} \\mu_{s}(\\omega) u(\\omega, a)$. Suppose the agent takes action $a_{s}$ given signal $s \\in S$. Then we see that the sender's utility of using signaling scheme $\\pi$ in Bayesian persuasion (left) is equal to the principal's utility of using strategy $\\pi$ in the generalized principal-agent problem (right):\n\n$$\n\\sum_{\\omega \\in \\Omega} \\mu_{0}(\\omega) \\sum_{s \\in S} \\pi(s \\mid \\omega) u\\left(\\omega, a_{s}\\right)=\\sum_{s \\in S} \\pi_{s} \\sum_{\\omega \\in \\Omega} \\mu_{s}(\\omega) u\\left(\\omega, a_{s}\\right)=\\sum_{s \\in S} \\pi_{s} u\\left(x_{s}, a_{s}\\right)=\\mathbb{E}_{s \\sim \\pi}\\left[u\\left(x_{s}, a\\right)\\right]\n$$\n\nSimilarly, the agent/receiver's utilities in the two problems are equal. The utility functions $u(x, a)$, $v(x, a)$ are linear in the principal's decision $x \\in \\mathcal{X}$, satisfying our assumption.\n\nPersuasion (or cheap talk) with a learning agent When specialized to Bayesian persuasion, the generalized principal-agent problem with a learning agent becomes the following:",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "## Persuasion (or Cheap Talk) with a Learning Receiver\n\nIn each round $t=1, \\ldots, T$, the following events happen:\n(1) Using some algorithm that learns from history, the receiver chooses a strategy $\\rho^{t}: S \\rightarrow \\Delta(A)$\n\nthat maps each signal $s \\in S$ to a distribution over actions $\\rho^{t}(s) \\in \\Delta(A)$.\n(2) The sender chooses a signaling scheme $\\pi^{t}: \\Omega \\rightarrow \\Delta(S)$.\n(3) A state of the world $\\omega^{t} \\sim \\mu_{0}$ is realized, observed by the sender but not the receiver. The sender sends signal $s^{t} \\sim \\pi^{t}\\left(\\omega^{t}\\right)$ to the receiver. The receiver draws action $a^{t} \\sim \\rho^{t}(s)$.\n(4) The sender obtains utility $u^{t}=u\\left(\\omega^{t}, a^{t}\\right)$ and the receiver obtains utility $v^{t}=v\\left(\\omega^{t}, a^{t}\\right) .^{a}$\n\"The definition of utility here, $u\\left(\\omega^{t}, a^{t}\\right), v\\left(\\omega^{t}, a^{t}\\right)$, is slightly different from the definition in Section 2.2, which was the expected utility on decision/posterior $x^{t}, u\\left(x^{t}, a^{t}\\right), v\\left(x^{t}, a^{t}\\right)$. Because we eventually only care about the sender's utility and the receiver's regret in expectation, this difference does not matter.\n\nThe receiver does not need to know the prior $\\mu_{0}$ if its learning algorithm does not make use of $\\mu_{0}$. And same as the model in Section 2.2, the receiver chooses $\\rho^{t}$ without knowing the sender's signaling scheme $\\pi^{t}$, and the sender does not commit. In the classical cheap talk model (Crawford and Sobel, 1982), the sender does not have commitment power and the two players move simultaneously. So, under our learning receiver model, cheap talk and Bayesian persuasion are equivalent. Our \"persuasion with a learning receiver\" model can also be called \"cheap talk with a learning receiver\".",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 3 Reduction from Learning to Approximate Best Response \n\nIn this section, we reduce the generalized principal-agent problem with a learning agent to the problem with an approximately-best-responding agent. We show that, if the agent uses contextual no-regret learning algorithms, then the principal can obtain an average utility that is at least the \"maxmin\" approximate-best-response objective $\\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\operatorname{CReg}(T) / T)$ (to be defined below). On the other hand, if the agent does contextual no-swap-regret learning, then the principal cannot do better than the \"maxmax\" approximate-best-response objective $\\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\operatorname{CSReg}(T) / T)$. In addition, if the agent uses some learning algorithms that are no-regret but not no-swap-regret, the principal can sometimes do better than the \"maxmax\" objective $\\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\operatorname{CSReg}(T) / T)$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "### 3.1 Generalized Principal-Agent Problem with Approximate Best Response\n\nWe first define the generalized principal-agent problem with an approximately-best-responding agent. The classic generalized principal-agent problem (Section 2.1) assumes that, after receiving a signal $s \\in S$ (and observing the principal's decision $x_{s} \\in \\mathcal{X}$ ), the agent will take an optimal action with respect to $x_{s}$. This means that the agent uses a strategy $\\rho^{*}$ that best responds to the principal's strategy $\\pi$ :\n\n$$\n\\rho^{*}(s) \\in \\underset{a \\in A}{\\arg \\max } v\\left(x_{s}, a\\right), \\quad \\forall s \\in S \\quad \\Longrightarrow \\quad \\rho^{*} \\in \\underset{\\rho: S \\rightarrow \\Delta(A)}{\\arg \\max } V(\\pi, \\rho)\n$$\n\nHere, $V(\\pi, \\rho)=\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in A} \\rho(a \\mid s) v\\left(x_{s}, a\\right)$ denotes the expected utility of the agent when the principal uses strategy $\\pi$ and the agent uses randomized strategy $\\rho: S \\rightarrow \\Delta(A)$.\n\nHere, we allow the agent to approximately best respond. Let $\\delta \\geq 0$ be a parameter. We define two types of $\\delta$-best-responding strategies for the agent: deterministic and randomized.\n\n- A deterministic strategy $\\rho$ : for each signal $s \\in S$, the agent takes an action $a$ that is $\\delta$-optimal for $x_{s}$. Denote this set of strategies by $\\mathcal{D}_{\\delta}(\\pi)$ :\n\n$$\n\\mathcal{D}_{\\delta}(\\pi)=\\left\\{\\rho: S \\rightarrow A \\mid v\\left(x_{s}, \\rho(s)\\right) \\geq v\\left(x_{s}, a^{\\prime}\\right)-\\delta, \\forall a^{\\prime} \\in A\\right\\}\n$$\n\n- A randomized strategy $\\rho$ : for each signals $s$, the agent can take a randomized action. The expected utility of $\\rho$ is at most $\\delta$-worst than the best strategy $\\rho^{*}$.\n\n$$\n\\mathcal{R}_{\\delta}(\\pi)=\\left\\{\\rho: S \\rightarrow \\Delta(A) \\mid V(\\pi, \\rho) \\geq V\\left(\\pi, \\rho^{*}\\right)-\\delta\\right\\}\n$$\n\nEquivalently, $\\mathcal{R}_{\\delta}(\\pi)=\\left\\{\\rho: S \\rightarrow \\Delta(A) \\mid V(\\pi, \\rho) \\geq V\\left(\\pi, \\rho^{\\prime}\\right)-\\delta, \\forall \\rho^{\\prime}: S \\rightarrow A\\right\\}$.\nOur model of approximately-best-responding agent includes, for example, two other models in the Bayesian persuasion literature that also relax the agent's Bayesian rationality assumption: the quantal response model (proposed by McKelvey and Palfrey (1995) in normal-form games and studied by Feng et al. (2024) in Bayesian persuasion) and a model where the agent makes mistakes in Bayesian update (de Clippel and Zhang, 2022).\n\nExample 3.1. Assume that the receiver's utility is in $[0,1]$. In Bayesian persuasion, the following strategies of the receiver are $\\delta$-best-responding (see Appendix B. 1 for a proof):\n\n- Quantal response: given signal $s \\in S$, the agent chooses action $a \\in A$ with probability $\\frac{\\exp \\left(\\lambda v\\left(\\mu_{s}, a\\right)\\right)}{\\sum_{a^{\\prime} \\in A} \\exp \\left(\\lambda v\\left(\\mu_{s}, a^{\\prime}\\right)\\right)}$, with $\\lambda>0$. This strategy belongs to $\\mathcal{R}_{\\delta}(\\pi)$ with $\\delta=\\frac{1+\\log (|A| \\lambda)}{\\lambda}$.\n- Inaccurate belief: given signal $s \\in S$, the agent forms some posterior $\\mu_{s}^{\\prime}$ that is different yet close to the true posterior $\\mu_{s}$ in total variation distance $d_{\\mathrm{TV}}\\left(\\mu_{s}^{\\prime}, \\mu_{s}\\right) \\leq \\varepsilon$. The agent picks an optimal action for $\\mu_{s}^{\\prime}$. This strategy belongs to $\\mathcal{D}_{2 \\varepsilon}(\\pi)$.\n\nPrincipal's objectives. With an approximately-best-responding agent, we will study two types of objectives for the principal. The first type is the maximal utility that the principal can obtain if the agent approximately best responds in the worst way for the principal: for $X \\in\\{\\mathcal{D}, \\mathcal{R}\\}$, define\n\n$$\n\\underline{\\mathrm{OBJ}}^{X}(\\delta)=\\sup _{\\pi} \\min _{\\rho \\in X_{\\delta}(\\pi)} U(\\pi, \\rho)\n$$\n\nwhere $U(\\pi, \\rho)=\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in A} \\rho(a \\mid s) u\\left(x_{s}, a\\right)$ is the principal's expected utility when the principal uses strategy $\\pi$ and the agent uses strategy $\\rho$. We used \"sup\" in (6) because the maximizer does not necessarily exist. $\\underline{\\mathrm{OBJ}}^{X}(\\delta)$ is a \"maxmin\" objective and can be regarded as the objective of a \"robust generalized principal-agent problem\".\n\nThe second type of objectives is the maximal utility that the principal can obtain if the agent approximately best responds in the best way:\n\n$$\n\\overline{\\mathrm{OBJ}}^{X}(\\delta)=\\max _{\\pi} \\max _{\\rho \\in X_{\\delta}(\\pi)} U(\\pi, \\rho)\n$$\n\nThis is a \"maxmax\" objective that quantifies the maximal extent to which the principal can exploit the agent's irrational behavior.\n\nClearly, $\\underline{\\mathrm{OBJ}}^{X}(\\delta) \\leq \\underline{\\mathrm{OBJ}}^{X}(0) \\leq \\overline{\\mathrm{OBJ}}^{X}(0) \\leq \\overline{\\mathrm{OBJ}}^{X}(\\delta)$. And we note that $\\overline{\\mathrm{OBJ}}^{X}(0)=\\overline{\\mathrm{OBJ}}(0)$ is independent of $X$ and equal to the Stackelberg value $U^{*}$ defined in (1):\n\n$$\n\\overline{\\mathrm{OBJ}}(0)=\\max _{\\pi} \\max _{\\text {p. best-response to } \\pi} U(\\pi, \\rho)=U^{*}\n$$\n\nFinally, we note that, because $\\mathcal{D}_{0}(\\pi) \\subseteq \\mathcal{D}_{\\delta}(\\pi) \\subseteq \\mathcal{R}_{\\delta}(\\pi)$, the chain of inequalities $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\leq$ $\\underline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta) \\leq U^{*} \\leq \\overline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta) \\leq \\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta)$ hold.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "# 3.2 Agent's No-Regret Learning: Lower Bound on Principal's Utility \n\nTheorem 3.1. Suppose the agent uses a contextual no-regret learning algorithm with a contextual regret upper bounded by $\\operatorname{CReg}(T)$. The principal knows $\\operatorname{CReg}(T)$ but not the exact algorithm of the agent. By using some fixed strategy $\\pi^{t}=\\pi$ in all $T$ rounds, the principal can obtain an average utility $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right]$ that is arbitrarily close to $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}\\left(\\frac{\\mathrm{CReg}(T)}{T}\\right)$.\n\nTo prove Theorem 3.1, we provide a lemma to relate the agent's regret and the principal's utility in the learning model to those in the static model. We define some notations. Let the principal use some fixed strategy $\\pi^{t}=\\pi$ and the agent use some learning algorithm. Let $p_{a \\mid s}^{t}=\\operatorname{Pr}\\left[a^{t}=a \\mid s^{t}=s\\right]$ be the probability that the agent's algorithm chooses action $a$ conditioning on signal $s$ being sent in round $t$. Let $\\rho: S \\rightarrow \\Delta(A)$ be a randomized agent strategy that, given signal $s$, chooses each action $a \\in A$ with probability $\\rho(a \\mid s)=\\frac{\\sum_{t=1}^{T} p_{a \\mid s}^{t}}{T}$.\nLemma 3.2. When the principal uses a fixed strategy $\\pi^{t}=\\pi$ in all $T$ rounds, the regret of the agent not deviating according to $d: S \\rightarrow A$ is equal to $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(v\\left(x^{t}, d\\left(s^{t}\\right)\\right)-v\\left(x^{t}, a^{t}\\right)\\right)\\right]=$ $V(\\pi, d)-V(\\pi, \\rho)$, and the average utility of the principal $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right]$ is equal to $U(\\pi, \\rho)$.\n\nProof. Since $\\pi^{t}=\\pi$ is fixed, we have $\\pi_{s}^{t}=\\pi_{s}$ and $x_{s}^{t}=x_{s}, \\forall s \\in S$. The regret of the agent not deviating according to $d$ is:\n\n$$\n\\begin{aligned}\n& \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(v\\left(x^{t}, d\\left(s^{t}\\right)\\right)-v\\left(x^{t}, a^{t}\\right)\\right)\\right]=\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{s \\in S} \\pi_{s}^{t} \\sum_{a \\in A} p_{a \\mid s}^{t}\\left(v\\left(x_{s}^{t}, d(s)\\right)-v\\left(x_{s}^{t}, a\\right)\\right) \\\\\n& =\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in A} \\frac{\\sum_{t=1}^{T} p_{a \\mid s}^{t}}{T}\\left(v\\left(x_{s}, d(s)\\right)-v\\left(x_{s}, a\\right)\\right) \\\\\n& =\\sum_{s \\in S} \\pi_{s} v\\left(x_{s}, d(s)\\right)-\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in A} \\rho(a \\mid s) v\\left(x_{s}, a\\right)=V(\\pi, d)-V(\\pi, \\rho)\n\\end{aligned}\n$$\n\nHere, $d$ is interpreted as an agent strategy that deterministically takes action $d(s)$ for signal $s$.\nBy a similar derivation, we see that the principal's expected utility is equal to $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right]=$ $\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in A} \\frac{\\sum_{t=1}^{T} p_{a \\mid s}^{t}}{T} u\\left(x_{s}, a\\right)=U(\\pi, \\rho)$, which proves the lemma.\n\nProof of Theorem 3.1. By Lemma 3.2 and the no-regret condition that the agent's regret $\\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(v\\left(x^{t}, d\\left(s^{t}\\right)\\right)-v\\left(x^{t}, a^{t}\\right)\\right)\\right] \\leq \\operatorname{CReg}(T)$, we have\n\n$$\nV(\\pi, d)-V(\\pi, \\rho)=\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(v\\left(x^{t}, d\\left(s^{t}\\right)\\right)-v\\left(x^{t}, a^{t}\\right)\\right)\\right] \\leq \\frac{\\operatorname{CReg}(T)}{T}, \\quad \\forall d: S \\rightarrow A\n$$\n\nThis means that the agent's randomized strategy $\\rho$ is a $\\delta=\\frac{\\operatorname{CReg}(T)}{T}$ best-response to the principal's fixed signaling scheme $\\pi, \\rho \\in \\mathcal{R}_{\\delta=\\frac{\\operatorname{CReg}(T)}{T}}(\\pi)$. This holds for any $\\pi$. In particular, if for any $\\varepsilon>0$ the principal uses a signaling scheme $\\pi^{\\varepsilon}$ that obtains an objective that is $\\varepsilon$-close to $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta)=$ $\\sup _{\\pi} \\min _{\\rho \\in \\mathcal{R}_{\\delta}(\\pi)} U(\\pi, \\rho)$, then the principal obtains an expected utility of, by Lemma 3.2,\n\n$$\n\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(a^{t}, \\omega^{t}\\right)\\right]=U\\left(\\pi^{\\varepsilon}, \\rho\\right) \\geq \\min _{\\rho \\in \\mathcal{R}_{\\delta}\\left(\\pi^{\\varepsilon}\\right)} U\\left(\\pi^{\\varepsilon}, \\rho\\right) \\geq \\underline{\\mathrm{OBJ}}^{\\mathcal{R}}\\left(\\delta=\\frac{\\operatorname{CReg}(T)}{T}\\right)-\\varepsilon\n$$\n\nin the learning model. Letting $\\varepsilon \\rightarrow 0$ proves the theorem.\n\nWe then show that the result in Theorem 3.1 is tight: there exist cases where the principal cannot do better than $\\overline{\\mathrm{OBJ}}^{\\mathcal{R}}\\left(\\frac{\\mathrm{CReg}(T)}{2 T}\\right)$ even using adaptive strategies:\nTheorem 3.3. For any adaptive strategy of the principal, there exists a contextual no-regret learning algorithm for the agent under which the principal's average utility is no more than $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}\\left(\\frac{\\mathrm{CReg}(T)}{2 T}\\right)$. There also exists a contextual no-swap-regret learning algorithm for the agent under which the principal's average utility is no more than $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}\\left(\\frac{\\mathrm{CSReg}(T)}{2 T}\\right)$.\n\nSee Appendix B. 2 for the proof of this Theorem.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# 3.3 Agent's No-Swap-Regret Learning: Upper Bound on Principal's Utility \n\nAs we mentioned in Section 2.2, the fact that the principal moves after the learning agent in each round gives the principal a possibility to exploit the agent to do better than $U^{*}$. However, exploiting the agent in a single round may cause the agent to learn a bad strategy for the principal in later rounds. It turns out that, if the agent's learning algorithm satisfies the contextual no-swap-regret property, then the principal cannot exploit the agent in the long run. Formally:\nTheorem 3.4. Against a contextual no-swap-regret learning agent, the principal cannot obtain utility more than $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right] \\leq \\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}\\left(\\frac{\\mathrm{CSReg}(T)}{T}\\right)$, even if the principal knows the agent's learning algorithm and chooses $\\pi^{t}$ based on $\\rho^{t}$.\n\nBefore presenting the full proof of this theorem, we give the high level idea of the proof. The key idea is to think of the signal $s^{t} \\sim \\pi^{t}$ from the principal and the action $a^{t} \\sim \\rho^{t}\\left(s^{t}\\right)$ recommended by the agent's learning algorithm together as a joint signal $\\left(s^{t}, a^{t}\\right)$ from some hypothetical signaling scheme $\\pi^{\\prime}$. In response to $\\pi^{\\prime}$, the agent takes the action $a^{t}$ recommended by the algorithm, namely using the mapping $\\left(s^{t}, a^{t}\\right) \\mapsto a^{t}$ as his strategy. A contextual no-swap-regret algorithm guarantees that the agent is at most $\\frac{\\operatorname{CSReg}(T)}{T}$ worse compared to using the strategy $d^{*}: S \\times A \\rightarrow A$ that best responds to $\\pi^{\\prime}$. So, the agent's overall strategy is a $\\frac{\\operatorname{CSReg}(T)}{T}$-approximate best response to $\\pi^{\\prime}$. This limits the principal's overall utility to be at most $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right)$. See details below:\nProof. Let $p_{s}^{t}=\\operatorname{Pr}\\left[s^{t}=s\\right]=\\mathbb{E}\\left[\\mathbb{1}\\left[s^{t}=s\\right]\\right]=\\mathbb{E}\\left[\\pi_{s}^{t}\\right]$ be the (unconditional) probability that signal $s \\in S$ is sent in round $t$. Let $p_{a \\mid s}^{t}=\\operatorname{Pr}\\left[a^{t}=a \\mid s^{t}=s\\right]$ be the probability that the agent's algorithm takes action $a$ conditioning on signal $s^{t}=s$ being sent in round $t$. Let $d: S \\times A \\rightarrow A$ be any deviation function for the agent. The agent's utility gain by deviation is upper bounded by the contextual swap-regret:\n\n$$\n\\begin{aligned}\n& \\frac{\\operatorname{CSReg}(T)}{T} \\geq \\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(v\\left(x^{t}, d\\left(s^{t}, a^{t}\\right)\\right)-v\\left(x^{t}, a^{t}\\right)\\right)\\right] \\\\\n& =\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{s \\in S} p_{s}^{t} \\sum_{a \\in A} p_{a \\mid s}^{t} \\mathbb{E}_{x_{s}^{t} \\mid s^{t}=s}\\left[v\\left(x_{s}^{t}, d(s, a)\\right)-v\\left(x_{s}^{t}, a\\right)\\right] \\\\\n& =\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{s \\in S} p_{s}^{t} \\sum_{a \\in A} p_{a \\mid s}^{t}\\left(v\\left(\\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right], d(s, a)\\right)-v\\left(\\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right], a\\right)\\right) \\quad \\text { by linearity of } v(\\cdot, a) \\\\\n& =\\sum_{s \\in S} \\sum_{a \\in A} \\frac{\\sum_{j=1}^{T} p_{s}^{t} p_{a \\mid s}^{j}}{T} \\frac{1}{\\sum_{j=1}^{T} p_{s}^{t} p_{a \\mid s}^{j}} \\sum_{t=1}^{T} p_{s}^{t} p_{a \\mid s}^{j}\\left(v\\left(\\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right], d(s, a)\\right)-v\\left(\\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right], a\\right)\\right) \\\\\n& =\\sum_{s \\in S} \\sum_{a \\in A} \\frac{\\sum_{j=1}^{T} p_{s}^{t} p_{a \\mid s}^{j}}{T}\\left[v\\left(\\frac{\\sum_{t=1}^{T} p_{s}^{t} p_{a \\mid s}^{t} \\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right]}{\\sum_{j=1}^{T} p_{s}^{t} p_{a \\mid s}^{j}}, d(s, a)\\right)-v\\left(\\frac{\\sum_{t=1}^{T} p_{s}^{t} p_{a \\mid s}^{t} \\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right]}{\\sum_{j=1}^{T} p_{s}^{t} p_{a \\mid s}^{j}}, a\\right)\\right]\n\\end{aligned}\n$$\n\nDefine $q_{s, a}=\\frac{\\sum_{j=1}^{T} p_{s}^{j} p_{a \\mid s}^{j}}{T}$ and $y_{s, a}=\\frac{\\sum_{t=1}^{T} p_{s}^{t} p_{a \\mid s}^{t} \\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right]}{\\sum_{j=1}^{T} p_{s}^{j} p_{a \\mid s}^{j}}$ $\\in \\mathcal{X}$. Then the above is equal to\n\n$$\n=\\sum_{s \\in S} \\sum_{a \\in A} q_{s, a}\\left[v\\left(y_{s, a}, d(s, a)\\right)-v\\left(y_{s, a}, a\\right)\\right]\n$$\n\nWe note that $\\sum_{s \\in S} \\sum_{a \\in A} q_{s, a}=\\frac{\\sum_{j=1}^{T} \\sum_{s \\in S} \\sum_{a \\in A} p_{s}^{j} p_{a \\mid s}^{j}}{T}=1$, so $q$ is a probability distribution over $S \\times A$. And note that\n\n$$\n\\begin{aligned}\n\\sum_{s, a \\in S \\times A} q_{s, a} y_{s, a} & =\\sum_{s, a \\in S \\times A} \\frac{1}{T} \\sum_{t=1}^{T} p_{s}^{t} p_{a \\mid s}^{t} \\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right]=\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{s \\in S} p_{s}^{t} \\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right] \\\\\n& =\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{s \\in S} \\mathbb{E}\\left[\\mathbb{1}\\left[s^{t}=s \\mid x_{s}^{t}\\right]=\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}\\left[\\sum_{s \\in S} \\mathbb{1}\\left[s^{t}=s\\right] x_{s}^{t}\\right]=\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}\\left[x^{t}\\right]\\right. \\\\\n& =\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}\\left[\\sum_{s \\in S} \\pi_{s}^{t} x_{s}^{t}\\right] \\in \\mathcal{C} \\quad \\text { because } \\sum_{s \\in S} \\pi_{s}^{t} x_{s}^{t} \\in \\mathcal{C}\n\\end{aligned}\n$$\n\nThis means that $\\pi^{\\prime}=\\left\\{\\left(q_{s, a}, y_{s, a}\\right)\\right\\}_{(s, a) \\in S \\times A}$ defines a valid principal strategy with the larger signal space $S \\times A .{ }^{3}$ Then, we note that (10) is the difference between the agent's expected utility under principal strategy $\\pi^{\\prime}$ when responding using strategy $d: S \\times A \\rightarrow A$ and using the strategy that maps signal $(s, a)$ to action $a$. And (10) is upper bounded by $\\frac{\\operatorname{CSReg}(T)}{T}$ by (9):\n\n$$\n(10)=V\\left(\\pi^{\\prime}, d\\right)-V\\left(\\pi^{\\prime},(s, a) \\mapsto a\\right) \\leq \\frac{\\operatorname{CSReg}(T)}{T}, \\quad \\forall d: S \\times A \\rightarrow A\n$$\n\nIn particular, this holds when $d$ is the agent's best-responding strategy. This means that the agent strategy $(s, a) \\mapsto a$ is a $\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right)$-best-response to $\\pi^{\\prime}$. So, the principal's expected utility is upper bounded by the utility in the approximate-best-response model:\n\n$$\n\\begin{aligned}\n\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right] & =\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{s \\in S} p_{s}^{t} \\sum_{a \\in A} p_{a \\mid s}^{t} v\\left(\\mathbb{E}\\left[x_{s}^{t} \\mid s^{t}=s\\right], a\\right) \\\\\n& =\\sum_{s \\in S} \\sum_{a \\in A} q_{s, a} u\\left(y_{s, a}, a\\right)=U\\left(\\pi^{\\prime},(s, a) \\rightarrow a\\right) \\leq \\overline{\\operatorname{OBJ}}^{\\mathcal{R}}\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right)\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# 3.4 Agent's Mean-Based Learning: Exploitable by the Principal \n\nMany no-regret (but not no-swap-regret) learning algorithms (e.g., MWU, FTPL, EXP-3) satisfy the following contextual mean-based property:\n\nDefinition 3.1 (Braverman et al. (2018)). Let $\\sigma_{s}^{t}(a)=\\sum_{j \\in[t]: s^{j}=s} v\\left(\\omega^{j}, a\\right)$ be the sum of historical utilities of the receiver in the first $t$ rounds if he takes action a when the signal/context is $s$. An algorithm is called $\\gamma$-mean-based if: whenever $\\exists a^{\\prime}$ such that $\\sigma_{s}^{t-1}(a)<\\sigma_{s}^{t-1}\\left(a^{\\prime}\\right)-\\gamma T$, the probability that the algorithm chooses action $a$ at round $t$ if the context is $s$ is $\\operatorname{Pr}\\left[a^{t}=a \\mid s^{t}=s\\right]<\\gamma$, with $\\gamma=o(1)$.\n\n[^0]\n[^0]:    ${ }^{3}$ As long as $|S| \\geq|A|$, enlarging the signal space from $S$ to $S \\times A$ will not change the optimal objective for the principal, because the optimal strategy of the principal only needs to use $|A|$ signals by the revelation principle.\n\nWe show that if the agent uses mean-based learning algorithm, then the principal can indeed exploit the agent, in particular in the Bayesian persuasion setting:\n\nTheorem 3.5. There exists a Bayesian persuasion instance where, as long as the receiver does $\\gamma$-mean-based learning, the sender can obtain a utility significantly larger than $\\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\gamma)$ and $U^{*}$.\n\nProof. The instance has 2 states (A, B), 3 actions (L, M, R), uniform prior $\\mu_{0}(\\mathrm{~A})=\\mu_{0}(\\mathrm{~B})=0.5$, with the following utility matrices (left for sender's, right for receiver's):\n\n![table_0](table_0)\n\n\n![table_1](table_1)\n\nClaim 3.6. In this instance, the optimal sender utility $U^{*}$ in the classic BP model is 0 , and the approximate-best-response objective $\\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\gamma)=O(\\gamma)$.\n\nProof. Recall that any signaling scheme decomposes the prior $\\mu_{0}$ into multiple posteriors $\\left\\{\\mu_{s}\\right\\}_{s \\in S}$. If a posterior $\\mu_{s}$ puts probability $>0.5$ to state B , then the receiver will take action M , which gives the sender a utility $\\leq 0$; if the posterior $\\mu_{s}$ puts probability $\\leq 0.5$ to state B , then no matter what action the receiver takes, the sender's expected utility on $\\mu_{s}$ cannot be greater than 0 . So, the sender's expected utility is $\\leq 0$ under any signaling scheme. An optimal signaling scheme is to reveal no information (keep $\\mu_{s}=\\mu_{0}$ ); the receiver takes R and the sender gets utility 0 .\n\nThis instance satisfies the assumptions of Theorem 4.3, so $\\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\gamma) \\leq U^{*}+O(\\gamma)=O(\\gamma)$.\nClaim 3.7. By doing the following, the sender can obtain utility $\\approx \\frac{1}{2}-O(\\sqrt{\\gamma})$ if the receiver is $\\gamma$-mean-based learning:\n\n- in the first $T / 2$ rounds: if the state is A , send signal 1 ; if the state is B , send 2 .\n- in the remaining $T / 2$ rounds, switch the scheme: if the state is A , send 2 ; if state is B , send 1 .\n\nProof. In the first $T / 2$ rounds, the receiver finds that signal 1 corresponds to state A so he will take action L with high probability when signal 1 is sent; signal 2 corresponds to B so he will take action M with high probability. In this phase, the sender obtains utility $\\approx 0$ per round. At the end of this phase, for signal 1 , the receiver accumulates utility $\\approx \\frac{T}{2} \\frac{1}{2} \\sqrt{\\gamma}=\\frac{T}{4} \\sqrt{\\gamma}$ for action L. For signal 2 , the receiver accumulates utility $\\approx \\frac{T}{2} \\frac{1}{2} \\cdot 1=\\frac{T}{4}$ for action M .\n\nIn the remaining $T / 2$ rounds, the following will happen:\n\n- For signal 1 , the receiver finds that the state is now B , so the utility of action L decreases by 1 every time signal 1 is sent. Because the utility of L accumulated in the first phase was $\\approx \\frac{T}{4} \\sqrt{\\gamma}$, after $\\approx \\frac{T}{4} \\sqrt{\\gamma}$ rounds in second phase the utility of L should decrease to below 0 , and the receiver will no longer play L (with high probability) at signal 1. The receiver will not play M at signal 1 in most of the second phase either, because there are more A states than B states at signal 1 historically. So, the receiver will play action R most times, roughly $\\frac{T}{4}-\\frac{T}{4} \\sqrt{\\gamma}$ rounds. This gives the sender a total utility of $\\approx\\left(\\frac{T}{4}-\\frac{T}{4} \\sqrt{\\gamma}\\right) \\cdot 2=\\frac{T}{2}-O(T \\sqrt{\\gamma})$.\n- For signal 2, the state is now A. But the receiver will continue to play action M in most times. This because: R has utility $0 ; \\mathrm{L}$ accumulated $\\approx-\\frac{T}{4}$ utility in the first phase, and only increases by $\\sqrt{\\gamma}$ per round in the second phase, so its accumulated utility is always negative;\n\ninstead, M has accumulated $\\frac{T}{4}$ utility in the first phase, and decreases by 1 every time signal 2 is sent in the second phase, so its utility is positive until near the end. So, the receiver will play M. This gives the sender utility 0 .\n\nSumming up, the sender obtains total utility $\\approx \\frac{T}{2}-O(T \\sqrt{\\gamma})$ in these two phases, which is $\\frac{1}{2}-O(\\sqrt{\\gamma})>0$ per round in average.\n\nThe above two claims together prove the theorem.",
      "tables": {
        "table_0": "| $u(\\omega, a)$ | L | M | R |\n| :-- | :-- | :-- | :-- |\n| A | 0 | -2 | -2 |\n| B | 0 | 0 | 2 |",
        "table_1": "| $v(\\omega, a)$ | L | M | R |\n| :-- | :-- | :-- | :-- |\n| A | $\\sqrt{\\gamma}$ | -1 | 0 |\n| B | -1 | 1 | 0 |"
      },
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# 4 Generalized Principal-Agent Problems with Approximate Best Response \n\nAfter presenting the reduction from learning to approximate best response, we now study generalized principal-agent problems with approximate best response. We will show that both the maxmin objectives $\\overline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta), \\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta)$ and the maxmax objectives $\\overline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta), \\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta)$ are close to the optimal principal objective $U^{*}$ in the best-response model when the degree $\\delta$ of the agent's approximate best response is small, under some natural assumptions described below.\n\nAssumptions and notations. We make some innocuous assumptions. First, the agent has no weakly dominated action:\n\nAssumption 4.1 (No Dominated Action). An action $a_{0} \\in A$ of the agent is weakly dominated if there exists a mixed action $\\alpha^{\\prime} \\in \\Delta\\left(A \\backslash\\left\\{a_{0}\\right\\}\\right)$ such that $v\\left(x, \\alpha^{\\prime}\\right)=\\mathbb{E}_{a \\sim \\alpha^{\\prime}}[v(x, a)] \\geq v\\left(x, a_{0}\\right)$ for all $x \\in \\mathcal{X}$. We assume that the agent has no weakly dominated action.\n\nClaim 4.1. Assumption 4.1 implies: there exists a constant $G>0$ such that, for any agent action $a \\in A$, there exists a principal decision $x \\in \\mathcal{X}$ such that $v(x, a)-v\\left(x, a^{\\prime}\\right) \\geq G$ for every $a^{\\prime} \\in A \\backslash\\{a\\}$.\n\nThe proof of this claim is in Appendix C.1. The constant $G>0$ in Claim 4.1 is analogous to the concept of \"inducibility gap\" in Stackelberg games (Von Stengel and Zamir, 2004; Gan et al., 2023). In fact, Gan et al. (2023) show that, if the inducibility gap $G>\\delta$, then the maximin approximate-best-response objective satisfies $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta) \\geq U^{*}-\\frac{\\delta}{G}$ in Stackelberg games. Our results will significantly generalize theirs to any generalized principal-agent problem, to randomized agent strategies, and to the maximax objectives $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta), \\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta)$.\n\nTo present our results, we need to introduce a few more notions and assumptions. Let $\\operatorname{diam}(\\mathcal{X} ; \\|$. $\\|)=\\max _{x_{1}, x_{2} \\in \\mathcal{X}}\\left\\|x_{1}-x_{2}\\right\\|$ be the diameter of the space $\\mathcal{X}$, where $\\|\\cdot\\|$ is some norm. For convenience we assume $\\mathcal{X} \\subseteq \\mathbb{R}^{d}$ and use the $\\ell_{1}$-norm $\\|x\\|_{1}=\\sum_{i=1}^{d}\\left|x_{(i)}\\right|$ or the $\\ell_{\\infty}$-norm $\\|x\\|_{\\infty}=\\max _{i=1}^{d}\\left|x_{(i)}\\right|$. For a generalized principal-agent problem with constraint $\\sum_{s \\in S} \\pi_{s} x_{s} \\in \\mathcal{C}$, let $\\partial \\mathcal{X}$ be the boundary of $\\mathcal{X}$ and let $\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})=\\min _{c \\in \\mathcal{C}, x \\in \\partial X}\\|c-x\\|$ be the distance from $\\mathcal{C}$ to the boundary of $\\mathcal{X}$. We assume that $\\mathcal{C}$ is away from the boundary of $\\mathcal{X}$ :\n\nAssumption 4.2 ( $\\mathcal{C}$ is in the interior of $\\mathcal{X}$ ). $\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})>0$.\nAssumption 4.3 (Bounded and Lipschitz utility). The principal's utility function is bounded: $|u(x, a)| \\leq B$, and L-Lipschitz in $x \\in \\mathcal{X}:\\left|u\\left(x_{1}, a\\right)-u\\left(x_{2}, a\\right)\\right| \\leq L\\left\\|x_{1}-x_{2}\\right\\|$.\n\nMain results. We now present the main results of this section: lower bounds on $\\underline{\\mathrm{OBJ}}^{X}(\\delta)$ and upper bounds on $\\overline{\\mathrm{OBJ}}^{X}(\\delta)$ in generalized principal-agent problems without and with constraints.\n\nTheorem 4.2 (Without constraint). For an unconstrained generalized principal-agent problem, under Assumptions 4.1 and 4.3, for $0 \\leq \\delta<G$, we have\n\n- $\\underline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta) \\geq U^{*}-\\operatorname{diam}(\\mathcal{X}) L \\frac{\\delta}{G}$.\n- $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\geq U^{*}-2 \\sqrt{\\frac{2 B L}{G}} \\operatorname{diam}(\\mathcal{X}) \\delta$ for $\\delta<\\frac{\\operatorname{diam}(\\mathcal{X}) G L}{2 B}$.\n- $\\overline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta) \\leq \\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\leq U^{*}+\\operatorname{diam}(\\mathcal{X}) L \\frac{\\delta}{G}$.\n\nTheorem 4.3 (With constraint). For a generalized principal-agent problem with the constraint $\\sum_{s \\in S} \\pi_{s} x_{s} \\in \\mathcal{C}$, under Assumptions 4.1, 4.2 and 4.3, for $0 \\leq \\delta<\\frac{G \\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}{\\operatorname{diam}(\\mathcal{X})}$, we have\n\n- $\\underline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta) \\geq U^{*}-\\left(\\operatorname{diam}(\\mathcal{X}) L+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G}$.\n- $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\geq U^{*}-2 \\sqrt{\\frac{2 B}{G}}\\left(\\operatorname{diam}(\\mathcal{X}) L+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\delta$.\n- $\\overline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta) \\leq \\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\leq U^{*}+\\left(\\operatorname{diam}(\\mathcal{X}) L+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G}$.\n\nThe expression \" $\\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})} \\delta$ \" suggests that $\\frac{1}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}$ is similar to a \"condition number\" (Renegar, 1994) that quantifies the \"stability\" of the principal-agent problem against the agent's approximate-best-responding behavior. When $\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})$ is larger $(\\mathcal{C}$ is further away from the boundary of $\\mathcal{X})$, the condition number is smaller, the problem is more stable, and the $\\delta$-best-response objectives $\\underline{\\mathrm{OBJ}}^{X}(\\delta)$ and $\\overline{\\mathrm{OBJ}}^{X}(\\delta)$ are closer to the best-response objective $U^{*}$.\n\nHigh-level idea: perturbation. The high level idea to prove Theorems 4.2 and 4.3 is a perturbation argument. Consider proving the upper bounds on $\\overline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta)$ for example. Let $(\\pi, \\rho)$ be any pair of principal's strategy and agent's $\\delta$-best-responding strategy. We perturb the principal's strategy $\\pi$ slightly to be a strategy $\\pi^{\\prime}$ such that $\\rho$ is exactly best-responding to $\\pi^{\\prime}$ (such a perturbation is possible due to Assumption 4.1). Since $\\rho$ is best-responding to $\\pi^{\\prime}$, the pair $\\left(\\pi^{\\prime}, \\rho\\right)$ cannot give the principal a higher utility than $U^{*}$ (which is the optimal principal utility under the bestresponse model). This means that the original pair $(\\pi, \\rho)$ cannot give the principal a utility much higher than $U^{*}$, thus implying an upper bound on $\\overline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta)$. Extra care is needed when dealing with randomized strategies of the agent. See details in Appendix C.3.\n\nThe bound $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\geq U^{*}-O(\\sqrt{\\delta})$ is tight. We note that, in Theorems 4.2 and 4.3, the maxmin objective with randomized agent strategies is bounded by $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\geq U^{*}-O(\\sqrt{\\delta})$, while the objective with deterministic agent strategies is bounded by $\\underline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta) \\geq U^{*}-O(\\delta)$. This is not because our analysis is not tight. In fact, the squared root bound $U^{*}-\\Theta(\\sqrt{\\delta})$ for $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta)$ is tight. We prove this by giving an example where $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\leq U^{*}-\\Omega(\\sqrt{\\delta})$. Consider the following classical Bayesian persuasion example:\n\nExample 4.1. There are 2 states $\\Omega=\\{$ Good, Bad $\\}$, 2 actions $A=\\{a, b\\}$, with the following utility matrices\n\n![table_2](table_2)\n\n\n![table_3](table_3)\n\nThe prior probability of Good state is $\\mu_{0}<\\frac{1}{2}$, so the receiver takes action $b$ by default. In this example, for $\\delta<\\frac{\\mu_{0}}{2}, \\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\leq U^{*}-2 \\sqrt{2 \\mu_{0} \\delta}+\\delta=U^{*}-\\Omega(\\sqrt{\\delta})$. See Appendix C. 2 for a proof.",
      "tables": {
        "table_2": "| sender | $a$ | $b$ |\n| :-- | :-- | :-- |\n| Good | 1 | 0 |\n| Bad | 1 | 0 |",
        "table_3": "| receiver | $a$ | $b$ |\n| :-- | :-- | :-- |\n| Good | 1 | 0 |\n| Bad | -1 | 0 |"
      },
      "images": {}
    },
    {
      "section_id": 15,
      "text": "# 5 Applications to Specific Principal-Agent Problems \n\nWe apply the general results in Section 3 and 4 to derive concrete results for three specific principalagent problems: Bayesian persuasion, Stackelberg games, and contract design.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "### 5.1 Bayesian Persuasion\n\nAs noted in Section 2, Bayesian persuasion is a generalized principal-agent problem with constraint $\\sum_{s \\in S} \\pi_{s} x_{s} \\in \\mathcal{C}=\\left\\{\\mu_{0}\\right\\}$ where each $x_{s}=\\mu_{s}=\\left(\\mu_{s}(\\omega)\\right)_{\\omega \\in \\Omega} \\in \\mathcal{X}=\\Delta(\\Omega)$ is a posterior belief. Suppose the principal's utility is bounded: $|u(\\omega, a)| \\leq B$. Then, the principal's utility function $u\\left(\\mu_{s}, a\\right)=$ $\\sum_{\\omega \\in \\Omega} \\mu_{s}(\\omega) u(\\omega, a)$ is $(L=B)$-Lipschitz in $\\mu_{s}$ (under $\\ell_{1}$-norm), so Assumption 4.3 is satisfied. Suppose the prior $\\mu_{0}$ has positive probability for every $\\omega \\in \\Omega$, and let $p_{0}=\\min _{\\omega \\in \\Omega} \\mu_{0}(\\omega)>0$. Then, we have the distance\n\n$$\n\\operatorname{dist}(\\mathcal{C}, \\partial X)=\\min \\left\\{\\left\\|\\mu_{0}-\\mu\\right\\|_{1}: \\mu \\in \\Delta(\\Omega) \\text { s.t. } \\mu(\\omega)=0 \\text { for some } \\omega \\in \\Omega\\right\\} \\geq p_{0}>0\n$$\n\nso Assumption 4.2 is satisfied. The diameter satisfies\n\n$$\n\\operatorname{diam}\\left(\\mathcal{X} ; \\ell_{1}\\right)=\\max _{\\mu_{1}, \\mu_{2} \\in \\Delta(\\Omega)}\\left\\|\\mu_{1}-\\mu_{2}\\right\\|_{1} \\leq 2\n$$\n\nFinally, we assume Assumption 4.1 (no dominated action for the agent). Then, Theorem 4.3 gives bounds on the approximate-best-response objectives in Bayesian persuasion:\n\nCorollary 5.1 (Bayesian persuasion with approximate best response). For $0 \\leq \\delta<\\frac{G p_{0}}{2}$,\n\n- $\\underline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta) \\geq U^{*}-2 B\\left(1+\\frac{2}{p_{0}}\\right) \\frac{\\delta}{G}$.\n- $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\geq U^{*}-4 B \\sqrt{\\left(1+\\frac{2}{p_{0}}\\right) \\frac{\\delta}{G}}$.\n- $\\overline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta) \\leq \\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\leq U^{*}+2 B\\left(1+\\frac{2}{p_{0}}\\right) \\frac{\\delta}{G}$.\n\nFurther applying Theorem 3.1 and 3.4, we obtain the central result for our motivating problem, persuasion with a learning agent:\n\nCorollary 5.2 (Persuasion with a learning agent). Suppose $T$ is sufficiently large such that $\\frac{\\operatorname{CReg}(T)}{T}<$ $\\frac{G p_{0}}{2}$ and $\\frac{\\operatorname{CSReg}(T)}{T}<\\frac{G p_{0}}{2}$, then\n\n- with a contextual no-regret learning agent, the principal can obtain utility at least\n\n$$\n\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right] \\geq \\underline{\\mathrm{OBJ}}^{\\mathcal{R}}\\left(\\frac{\\operatorname{CReg}(T)}{T}\\right) \\geq U^{*}-4 B \\sqrt{\\left(1+\\frac{2}{p_{0}}\\right) \\frac{1}{G}} \\sqrt{\\frac{\\operatorname{CReg}(T)}{T}}\n$$\n\nusing a fixed signaling scheme in all rounds.\n\n- with a contextual no-swap-regret learning agent, the principal's obtainable utility is at most\n\n$$\n\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right] \\leq \\overline{\\operatorname{OBJ}}^{\\mathcal{D}}\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right) \\leq U^{*}+2 B\\left(1+\\frac{2}{p_{0}}\\right) \\frac{1}{G} \\frac{\\operatorname{CSReg}(T)}{T}\n$$\n\neven knowing the receiver's learning algorithm and using time-varying signaling schemes.\nResult (13) is interesting because it shows that the sender cannot exploit a no-swap-regret learning receiver beyond $U^{*}+o(1)$ even if the sender has informational advantage (knowing the state $\\omega$ ) and knows the receiver's algorithm or strategy $\\rho^{t}$ before choosing the signaling scheme. Result (12) is interesting because it shows that the sender can achieve the Bayesian persuasion optimal objective (which is $U^{*}$ ) in the problem of cheap talk with a learning agent (recall from Section 2.3 that persuasion and cheap talk are equivalent in our model).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# 5.2 Stackelberg Games \n\nIn a Stackelberg game, the principal (leader), having a finite action set $B$, first commits to a mixed strategy $x=\\left(x_{(b)}\\right)_{b \\in B} \\in \\Delta(B)$, which is a distribution over actions. So the principal's decision space $\\mathcal{X}$ is $\\Delta(B)$. The agent (follower) then takes an action $a \\in A$ in response to $x$. The (expected) utilities for the two players are $u(x, a)=\\sum_{b \\in B} x_{(b)} u(b, a)$ and $v(x, a)=\\sum_{b \\in B} x_{(b)} u(b, a)$. The signal $s$ can (but not necessarily) be an action that the principal recommends the agent to take.\n\nAssume bounded utility $|u(b, a)| \\leq B$. Then, the principal's utility function $u(x, a)$ is bounded in $[-B, B]$ and $(L=B)$-Lipschitz in $x$. The diameter $\\operatorname{diam}(\\mathcal{X})=\\max _{x_{1}, x_{2} \\in \\Delta(B)}\\left\\|x_{1}-x_{2}\\right\\|_{1} \\leq 2$. Applying the theorem for unconstrained generalized principal-agent problems (Theorem 4.2) and the theorems for learning agent (Theorem 3.1 and 3.4), we obtain:\n\nCorollary 5.3 (Stackelberg game with a learning agent). Suppose $T$ is sufficiently large such that $\\frac{\\operatorname{CReg}(T)}{T}<G$ and $\\frac{\\operatorname{CSReg}(T)}{T}<G$, then:\n\n- with a contextual no-regret learning agent, the principal can obtain utility $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right] \\geq$ $\\frac{\\operatorname{OBJ}}{\\operatorname{CR}}\\left(\\frac{\\operatorname{CReg}(T)}{T}\\right) \\geq U^{*}-\\frac{4 B}{\\sqrt{G}} \\sqrt{\\frac{\\operatorname{CReg}(T)}{T}}$.\n- with a contextual no-swap-regret learning agent, the principal cannot obtain utility more than $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right] \\leq \\overline{\\operatorname{OBJ}}^{\\mathcal{D}}\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right) \\leq U^{*}+\\frac{2 B}{G} \\frac{\\operatorname{CSReg}(T)}{T}$.\n\nThe conclusion that the principal can obtain utility at least $U^{*}-o(1)$ against a no-regret learning agent and no more than $U^{*}+o(1)$ against a no-swap-regret agent in Stackelberg games was proved by Deng et al. (2019). Our Corollary 5.3 reproduces this conclusion and moreover provides bounds on the $o(1)$ terms, namely, $U^{*}-O\\left(\\sqrt{\\frac{\\operatorname{CReg}(T)}{T}}\\right)$ and $U^{*}+O\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right)$. This demonstrates the generality and usefulness of our framework.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "### 5.3 Contract Design\n\nIn contract design, there is a finite outcome space $O=\\left\\{r_{1}, \\ldots, r_{d}\\right\\}$ where each $r_{i} \\in \\mathbb{R}$ is a monetary reward to the principal. When the agent takes action $a \\in A$, outcome $r_{i}$ will happen with probability $p_{a i} \\geq 0, \\sum_{i=1}^{d} p_{a i}=1$. The principal cannot observe the action taken by the agent but can observe the realized outcome. The principal's decision space $\\mathcal{X}$ is the set of contracts, where a contract\n\n$x=\\left(x_{(i)}\\right)_{i=1}^{d} \\in[0,+\\infty]^{d}$ is a vector that specifies the payment to the agent for each possible outcome. So, if the agent takes action $a$ under contract $x$, the principal obtains expected utility\n\n$$\nu(x, a)=\\sum_{i=1}^{d} p_{a i}\\left(r_{i}-x_{(i)}\\right)\n$$\n\nand the agent obtains $v(x, a)=\\sum_{i=1}^{d} p_{a i} x_{(i)}-c_{a}$, where $c_{a} \\geq 0$ is the cost of action $a \\in A$ for the agent. The signal $s$ can (but not necessarily) be an action that the principal recommends the agent to take. The principal's decision space $\\mathcal{X} \\subseteq[0,+\\infty]^{d}$ in contract design, however, may be unbounded and violate the requirement of bounded diameter $\\operatorname{diam}(\\mathcal{X})$ that we need. We have two remedies for this.\n\nThe first remedy is to require the principal's payment to the agent be upper bounded by some constant $P<+\\infty$, so $0 \\leq x_{(i)} \\leq P$ and $\\mathcal{X}=[0, P]^{d}$. Under this requirement and the assumption of bounded reward $\\left|r_{i}\\right| \\leq R$, the principal's utility becomes bounded by $|u(x, a)| \\leq \\sum_{i=1}^{d} p_{a i}(R+P)=$ $R+P=B$ and $(L=1)$-Lipschitz under $\\ell_{\\infty}$-norm:\n\n$$\n\\left|u\\left(x_{1}, a\\right)-u\\left(x_{2}, a\\right)\\right|=\\left|\\sum_{i=1}^{d} p_{a i}\\left(x_{1(i)}-x_{2(i)}\\right)\\right| \\leq \\max _{i=1}^{d}\\left|x_{1(i)}-x_{2(i)}\\right| \\sum_{i=1}^{d} p_{a i}=\\left\\|x_{1}-x_{2}\\right\\|_{\\infty}\n$$\n\nAnd the diameter of $\\mathcal{X}$ is bounded by (under $\\ell_{\\infty}$-norm)\n\n$$\n\\operatorname{diam}\\left(\\mathcal{X} ; \\ell_{\\infty}\\right)=\\max _{x_{1}, x_{2} \\in \\mathcal{X}}\\left\\|x_{1}-x_{2}\\right\\|_{\\infty}=\\max _{x_{1}, x_{2} \\in[0, P]^{d}} \\max _{i=1}^{d}\\left|x_{1(i)}-x_{2(i)}\\right| \\leq P\n$$\n\nNow, we can apply the theorem for unconstrained generalized principal-agent problems (Theorem 4.2) and the theorems for learning agent (Theorem 3.1 and Theorem 3.4) to obtain:\n\nCorollary 5.4 (Contract design (with bounded payment) with a learning agent). Suppose $T$ is sufficiently large such that $\\frac{\\mathrm{CReg}(T)}{T}<\\frac{P G}{2(R+P)}$ and $\\frac{\\operatorname{CSReg}(T)}{T}<G$, then:\n\n- with a contextual no-regret learning agent, the principal can obtain utility at least $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right]$ $\\geq \\overline{\\operatorname{OBJ}}^{\\mathbb{E}}\\left(\\frac{\\mathrm{CReg}(T)}{T}\\right) \\geq U^{*}-2 \\sqrt{\\frac{2(R+P) P}{G}} \\sqrt{\\frac{\\mathrm{CReg}(T)}{T}}$.\n- with contextual a no-swap-regret learning agent, the principal cannot obtain utility more than $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right] \\leq \\overline{\\operatorname{OBJ}}^{\\mathcal{P}}\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right) \\leq U^{*}+\\frac{P}{G} \\frac{\\operatorname{CSReg}(T)}{T}$.\n\nThe second remedy is to write contract design as a generalized principal-agent problem in another way. Let $\\tilde{x}=\\left(\\tilde{x}_{(a)}\\right)_{a \\in A} \\in[0,+\\infty]^{|A|}$ be a vector recording the expected payment from the principal to the agent for each action $a \\in A$ :\n\n$$\n\\tilde{x}_{(a)}=\\sum_{i=1}^{d} p_{a i} x_{(i)}\n$$\n\nAnd let $\\tilde{r}_{(a)}$ be the expected reward of action $a, \\tilde{r}_{(a)}=\\sum_{i=1}^{d} p_{a i} r_{i}$. Then, the principal and the agent's utility can be rewritten as functions of $\\tilde{x}$ and $a$ :\n\n$$\nu(\\tilde{x}, a)=\\tilde{r}_{(a)}-\\tilde{x}_{(a)}, \\quad v(\\tilde{x}, a)=\\tilde{x}_{(a)}-c_{a}\n$$\n\nwhich are linear (strictly speaking, affine) in $\\tilde{x} \\in \\tilde{\\mathcal{X}}$. Assuming bounded reward $\\left|\\tilde{r}_{(a)}\\right| \\leq R$, we can without loss of generality assume that the expected payment $\\tilde{x}_{(a)}$ is bounded by $R$ as well, because\n\notherwise the principal will get negative utility. So, the principal's decision space can be restricted to\n\n$$\n\\tilde{\\mathcal{X}}=\\left\\{\\tilde{x} \\mid \\exists x \\in[0,+\\infty]^{d} \\text { such that } \\tilde{x}_{(a)}=\\sum_{i=1}^{d} p_{a i} x_{(i)} \\text { for every } a \\in A\\right\\} \\cap[0, R]^{|A|}\n$$\n\nwhich is convex and has bounded diameter (under $\\ell_{\\infty}$ norm)\n\n$$\n\\operatorname{diam}\\left(\\tilde{\\mathcal{X}} ; \\ell_{\\infty}\\right) \\leq \\operatorname{diam}\\left([0, R]^{|A|} ; \\ell_{\\infty}\\right)=R\n$$\n\nThe utility function $u(\\tilde{x}, a)$ is bounded by $2 R$ and $(L=1)$-Lipschitz (under $\\ell_{\\infty}$ norm):\n\n$$\n\\left|u\\left(\\tilde{x}_{1}, a\\right)-u\\left(\\tilde{x}_{2}, a\\right)\\right|=\\left|\\tilde{x}_{1(a)}-\\tilde{x}_{2(a)}\\right| \\leq \\max _{a \\in A}\\left|\\tilde{x}_{1(a)}-\\tilde{x}_{2(a)}\\right|=\\left\\|\\tilde{x}_{1}-\\tilde{x}_{2}\\right\\|_{\\infty}\n$$\n\nThus, we can apply the theorem for unconstrained generalized principal-agent problems (Theorem 4.2) and the theorems for learning agent (Theorem 3.1 and Theorem 3.4) to obtain:\n\nCorollary 5.5 (Contract design with a learning agent). Suppose $T$ is sufficiently large such that $\\frac{\\operatorname{CReg}(T)}{T}<\\frac{G}{2}$ and $\\frac{\\operatorname{CSReg}(T)}{T}<G$, then:\n\n- with a contextual no-regret learning agent, the principal can obtain utility at least $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right]$ $\\geq \\underline{\\mathrm{OBJ}}^{\\mathrm{R}}\\left(\\frac{\\mathrm{CReg}(T)}{T}\\right) \\geq U^{*}-\\frac{4 R}{\\sqrt{G}} \\sqrt{\\frac{\\mathrm{CReg}(T)}{T}}$.\n- with a contextual no-swap-regret learning agent, the principal cannot obtain utility more than $\\frac{1}{T} \\mathbb{E}\\left[\\sum_{t=1}^{T} u\\left(x^{t}, a^{t}\\right)\\right] \\leq \\overline{\\mathrm{OBJ}}{ }^{T}\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right) \\leq U^{*}+\\frac{R}{G} \\frac{\\operatorname{CSReg}(T)}{T}$.\n\nProviding the quantitative lower and upper bounds, the above results refine the result in Guruganesh et al. (2024) that the principal can obtain utility at least $U^{*}-o(1)$ against a noregret learning agent and no more than $U^{*}+o(1)$ against a no-swap-regret agent. This again demonstrates the versatility of our general framework.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 19,
      "text": "# 6 Discussion \n\nIn summary, our work provides an explicit characterization of the principal's achievable utility in generalized principal-agent problems with a contextual no-swap-regret learning agent. It is an asymmetric range $\\left[U^{*}-O\\left(\\sqrt{\\frac{\\operatorname{CSReg}(T)}{T}}\\right), U^{*}+O\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right)\\right]$. We show that this conclusion holds in all generalized principal-agent problems where the agent does not have private information, in particular including Bayesian persuasion where the principal is privately informed. As we mentioned in the Introduction, the upper bound $U^{*}+O\\left(\\frac{\\operatorname{CSReg}(T)}{T}\\right)$ does not hold when the agent has private information or does certain types of no-regret but not no-swap-regret learning. Deriving the exact upper bound in the latter cases is an interesting direction for future work.\n\nOther directions for future work include, for example, relaxing the assumption that the principal has perfect knowledge of the environment - what if both principal and agent are learning players? And what if the environment is non-stationary, like a Markovian environment (Jain and Perchet, 2024) or an adversarial dynamic environment (Camara et al., 2020)? In unknown or non-stationary environments, the benchmark $U^{*}$ needs to be redefined, and a joint design of both players' learning algorithms might be interesting.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "# References \n\nJerry Anunrojwong, Krishnamurthy Iyer, and David Lingenbrink. Persuading Risk-Conscious Agents: A Geometric Approach. Operations Research, page opre.2023.2438, March 2023. URL http://pubsonline.informs.org/doi/10.1287/opre.2023.2438. 5\n\nEshwar Ram Arunachaleswaran, Natalie Collina, and Jon Schneider. Pareto-Optimal Algorithms for Learning in Games. In Proceedings of the 25th ACM Conference on Economics and Computation, pages 490-510, New Haven CT USA, July 2024. ACM. URL https://dl.acm.org/doi/10.1145/3670865.3673517.4\n\nJean-Yves Audibert and S\u00e9bastien Bubeck. Regret Bounds and Minimax Policies under Partial Monitoring. Journal of Machine Learning Research, pages 2785-2836, 2010. URL http://jmlr.org/papers/v11/audibert10a.html. 24\n\nPeter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The Nonstochastic Multiarmed Bandit Problem. SIAM Journal on Computing, pages 48-77, January 2002. URL http://epubs.siam.org/doi/10.1137/S0097539701398375. 24\n\nYakov Babichenko, Inbal Talgam-Cohen, Haifeng Xu, and Konstantin Zabarnyi. RegretMinimizing Bayesian Persuasion. In Proceedings of the 22nd ACM Conference on Economics and Computation, pages 128-128, Budapest Hungary, July 2021. ACM. URL https://dl.acm.org/doi/10.1145/3465456.3467574. 5\n\nFrancesco Bacchiocchi, Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, and Nicola Gatti. Markov persuasion processes: Learning to persuade from scratch. arXiv preprint arXiv:2402.03077, 2024. 5\n\nDaniel J Benjamin. Errors in probabilistic reasoning and judgment biases. Handbook of Behavioral Economics: Applications and Foundations 1, 2:69-186, 2019. 1\n\nAvrim Blum and Yishay Mansour. From External to Internal Regret. Journal of Machine Learning Research, pages 1307-1324, 2007. URL http://jmlr.org/papers/v8/blum07a.html. 2, 24\n\nMark Braverman, Jieming Mao, Jon Schneider, and Matt Weinberg. Selling to a No-Regret Buyer. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 523-538, Ithaca NY USA, June 2018. ACM. URL https://dl.acm.org/doi/10.1145/3219166.3219233. $2,4,13$\n\nGeorge W. Brown. Iterative Solution of Games by Fictitious Play. In Activity Analysis of Production and Allocation. Wiley, New York, 1951. 1\n\nLinda Cai, S. Matthew Weinberg, Evan Wildenhain, and Shirley Zhang. Selling to Multiple NoRegret Buyers. In Web and Internet Economics, pages 113-129, Cham, 2024. Springer Nature Switzerland. URL https://link.springer.com/10.1007/978-3-031-48974-7_7. 2, 4\n\nModibo K. Camara, Jason D. Hartline, and Aleck Johnsen. Mechanisms for a No-Regret Agent: Beyond the Common Prior. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pages 259-270, Durham, NC, USA, November 2020. IEEE. URL https://ieeexplore.ieee.org/document/9317992/. 4, 5, 20\n\nColin Camerer. Bounded rationality in individual decision making. Experimental economics, 1: $163-183,1998.1$\n\nMatteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online Bayesian Persuasion. In Advances in Neural Information Processing Systems, pages 16188-16198. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ba5451d3c91a0f982f103cdbe249bc78-Paper.pdf. 5\n\nVincent P. Crawford and Joel Sobel. Strategic Information Transmission. Econometrica, page 1431, November 1982. URL https://www.jstor.org/stable/1913390?origin=crossref. 8, 9\n\nGeoffroy de Clippel and Xu Zhang. Non-Bayesian Persuasion. Journal of Political Economy, pages 2594-2642, October 2022. URL https://www.journals.uchicago.edu/doi/10.1086/720464. 5,10\n\nYuan Deng, Jon Schneider, and Balasubramanian Sivan. Strategizing against No-regret Learners. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/8b6dd7db9af49e67306feb59a8bdc52c-Paper.pdf. $2,4,18$\n\nPiotr Dworczak and Alessandro Pavan. Preparing for the Worst but Hoping for the Best: Robust (Bayesian) Persuasion. Econometrica, pages 2017-2051, 2022. URL https://www.econometricsociety.org/doi/10.3982/ECTA19107. 5\n\nYiding Feng, Wei Tang, and Haifeng Xu. Online Bayesian Recommendation with No Regret. In Proceedings of the 23rd ACM Conference on Economics and Computation, pages 818-819, Boulder CO USA, July 2022. ACM. URL https://dl.acm.org/doi/10.1145/3490486.3538327. 5\n\nYiding Feng, Chien-Ju Ho, and Wei Tang. Rationality-robust information design: Bayesian persuasion under quantal response. In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 501-546. SIAM, 2024. 5, 10\n\nDrew Fudenberg and David K. Levine. The theory of learning in games. MIT Press series on economic learning and social evolution. MIT Press, Cambridge, Mass, 1998. 1\n\nJiarui Gan, Minbiao Han, Jibang Wu, and Haifeng Xu. Robust Stackelberg Equilibria. In Proceedings of the 24th ACM Conference on Economics and Computation, pages 735-735, London United Kingdom, July 2023. ACM. URL https://dl.acm.org/doi/10.1145/3580507.3597680. 5, 15\n\nJiarui Gan, Minbiao Han, Jibang Wu, and Haifeng Xu. Generalized principal-agency: Contracts, information, games and beyond. In Proceedings of the 20th Conference on Web and Internet Economics, WINE '24, 2024. 4, 5, 6\n\nGuru Guruganesh, Yoav Kolumbus, Jon Schneider, Inbal Talgam-Cohen, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Joshua Ruizhi Wang, and S. Matthew Weinberg. Contracting with a Learning Agent. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=BhOLLUp8OA. 2, 4, 20\n\nKeegan Harris, Nicole Immorlica, Brendan Lucier, and Aleksandrs Slivkins. Algorithmic Persuasion Through Simulation: Information Design in the Age of Generative AI. arXiv preprint arXiv:2311.18138, 2023. 5\n\nSergiu Hart and Andreu Mas-Colell. A Simple Adaptive Procedure Leading to Correlated Equilibrium. Econometrica, pages 1127-1150, September 2000. URL http://doi.wiley.com/10.1111/1468-0262.00153. 2\n\nShinji Ito. A Tight Lower Bound and Efficient Reduction for Swap Regret. In Advances in Neural Information Processing Systems, pages 18550-18559. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/d79c8788088c2193f0244d8f1f36d2db25\n\nAtulya Jain and Vianney Perchet. Calibrated Forecasting and Persuasion. In Proceedings of the 25th ACM Conference on Economics and Computation, pages 489-489, New Haven CT USA, July 2024. ACM. URL https://dl.acm.org/doi/10.1145/3670865.3673455. 4, 5, 20\n\nEmir Kamenica and Matthew Gentzkow. Bayesian Persuasion. American Economic Review, pages 2590-2615, October 2011. URL https://pubs.aeaweb.org/doi/10.1257/aer.101.6.2590. 8\nB. H. Korte and Jens Vygen. Combinatorial optimization: theory and algorithms. Algorithms and combinatorics. Springer, Heidelberg ; New York, 5th ed edition, 2012. 29\n\nSvetlana Kosterina. Persuasion with unknown beliefs. Theoretical Economics, pages 1075-1107, 2022. URL https://econtheory.org/ojs/index.php/te/article/view/4742. 5\n\nRachitesh Kumar, Jon Schneider, and Balasubramanian Sivan. Strategically-Robust Learning Algorithms for Bidding in First-Price Auctions. In Proceedings of the 25th ACM Conference on Economics and Computation, pages 893-893, New Haven CT USA, July 2024. ACM. URL https://dl.acm.org/doi/10.1145/3670865.3673514. 4\n\nTao Lin and Ce Li. Information Design with Unknown Prior. In Proceedings of Innovations in Theoretical Computer Science, 2025. 5\n\nYue Lin, Wenhao Li, Hongyuan Zha, and Baoxiang Wang. Information design in multi-agent reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=NyQwBttTnG. 2, 4\n\nYishay Mansour, Mehryar Mohri, Jon Schneider, and Balasubramanian Sivan. Strategizing against Learners in Bayesian Games. In Proceedings of Thirty Fifth Conference on Learning Theory, Proceedings of Machine Learning Research, pages 5221-5252. PMLR, July 2022. URL https://proceedings.mlr.press/v178/mansour22a.html. 2, 4\n\nRichard D. McKelvey and Thomas R. Palfrey. Quantal Response Equilibria for Normal Form Games. Games and Economic Behavior, pages 6-38, July 1995. URL https://linkinghub.elsevier.com/retrieve/pii/S0899825685710238. 10\n\nRoger B Myerson. Optimal coordination mechanisms in generalized principal-agent problems. Journal of Mathematical Economics, pages 67-81, June 1982. URL https://linkinghub.elsevier.com/retrieve/pii/0304406882900064. 4, 5, 6\n\nDenis Nekipelov, Vasilis Syrgkanis, and Eva Tardos. Econometrics for Learning Agents. In Proceedings of the Sixteenth ACM Conference on Economics and Computation - EC '15, pages 1-18, Portland, Oregon, USA, 2015. ACM Press. URL http://dl.acm.org/citation.cfm?doid=2764468.2764522. 2\n\nJames Renegar. Some perturbation theory for linear programming. Mathematical Programming, pages 73-91, February 1994. URL http://link.springer.com/10.1007/BF01581690. 16\n\nTim Roughgarden. Lecture \\#17: No-Regret Dynamics. In Twenty lectures on algorithmic game theory. Cambridge University Press, Cambridge ; New York, NY, 2016. URL https://theory.stanford.edu/ tim/f13/l/117.pdf. 4\n\nAviad Rubinstein and Junyao Zhao. Strategizing against No-Regret Learners in FirstPrice Auctions. In Proceedings of the 25th ACM Conference on Economics and Computation, pages 894-921, New Haven CT USA, July 2024. ACM. URL https://dl.acm.org/doi/10.1145/3670865.3673613. 2, 4\n\nAntoine Scheid, Aymeric Capitaine, Etienne Boursier, Eric Moulines, Michael Jordan, and Alain Oliviero Durmus. Learning to mitigate externalities: the coase theorem with hindsight rationality. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=omyzrkacme. 2, 4\n\nTyler Lu, David Pal, and Martin Pal. Contextual Multi-Armed Bandits. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 485-492. PMLR, March 2010. URL https://proceedings.mlr.press/v9/lu10a.html. 7\n\nBernhard Von Stengel and Shmuel Zamir. Leadership with commitment to mixed strategies. Technical report, Citeseer, 2004. 15\n\nJibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I. Jordan, and Haifeng Xu. Sequential Information Design: Markov Persuasion Process and Its Efficient Reinforcement Learning. In Proceedings of the 23rd ACM Conference on Economics and Computation, pages 471-472, Boulder CO USA, July 2022. ACM. URL https://dl.acm.org/doi/10.1145/3490486.3538313. 5\n\nKunhe Yang and Hanrui Zhang. Computational Aspects of Bayesian Persuasion under Approximate Best Response. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=9B0iOkn3UP. 5\n\nGabriel Ziegler. Adversarial bilateral information design. Technical report, 2020. 5\nYou Zu, Krishnamurthy Iyer, and Haifeng Xu. Learning to Persuade on the Fly: Robustness Against Ignorance. In Proceedings of the 22nd ACM Conference on Economics and Computation, pages 927-928, Budapest Hungary, July 2021. ACM. URL https://dl.acm.org/doi/10.1145/3465456.3467593. 5",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "# A Details about Contextual No-(Swap-)Regret Algorithms: Proof of Proposition 2.1 \n\nLet $\\mathcal{A}$ be an arbitrary no-regret (no-swap-regret) learning algorithm for a multi-armed bandit (MAB) problem with $|A|$ arms. There exist such algorithms with regret $O(\\sqrt{T|A| \\log |A|})$ (variants of Exp3 (Auer et al., 2002)) and even $O(\\sqrt{T|A|})$ (doubling trick + polyINF (Audibert and Bubeck, 2010)) for any time horizon $T>0$. By swap-to-external regret reductions, they can be converted to multi-armed bandit algorithms with swap regret $O\\left(\\sqrt{T|A|^{3} \\log |A|}\\right)$ (Blum and Mansour, 2007) and\n\n$O(|A| \\sqrt{T})$ (Ito, 2020). We then convert $\\mathcal{A}$ into a contextual no-regret (contextual no-swap-regret) algorithm, in the following way:\n\n```\nAlgorithm 1: Convert any MAB algorithm to a contextual MAB algorithm\n    Input: MAB algortihm \\(\\mathcal{A}\\). Arm set \\(A\\). Context set \\(S\\).\n    Instantiate \\(|S|\\) copies \\(\\mathcal{A}_{1}, \\ldots, \\mathcal{A}_{|S|}\\) of \\(\\mathcal{A}\\), and initialize their round number by\n    \\(t_{1}=\\cdots=t_{|S|}=0\\).\n    for round \\(t=1,2, \\ldots\\) do\n        Receive context \\(s^{t}\\). Call \\(\\mathcal{A}_{s^{t}}\\) to obtain an action \\(a^{t}\\).\n        Play \\(a^{t}\\) and obtain feedback (which includes the reward \\(v^{t}\\left(a^{t}\\right)\\) of action \\(a^{t}\\) ).\n        Feed the feedback to \\(\\mathcal{A}_{s^{t}}\\). Increase its round number \\(t_{s^{t}}\\) by 1 .\n    end\n```\n\nProposition A.1. The contextual regret of Algorithm 1 is at most\n\n$$\n\\operatorname{CReg}(T) \\leq \\max \\left\\{\\sum_{s=1}^{|S|} \\operatorname{Reg}\\left(T_{s}\\right) \\mid T_{1}+\\cdots+T_{|S|}=T\\right\\}\n$$\n\nwhere $\\operatorname{Reg}\\left(T_{s}\\right)$ is the regret of $\\mathcal{A}$ for time horizon $T_{s}$.\nThe contextual swap-regret of Algorithm 1 is at most\n\n$$\n\\operatorname{CSReg}(T) \\leq \\max \\left\\{\\sum_{s=1}^{|S|} \\operatorname{SReg}\\left(T_{s}\\right) \\mid T_{1}+\\cdots+T_{|S|}=T\\right\\}\n$$\n\nwhere $\\operatorname{SReg}\\left(T_{s}\\right)$ is the swap-regret of $\\mathcal{A}$ for time horizon $T_{s}$.\nWhen plugging in $\\operatorname{Reg}\\left(T_{s}\\right)=O\\left(\\sqrt{|A| T_{s}}\\right)$, we obtain $\\operatorname{CReg}(T) \\leq O(\\sqrt{|A||S| T})$.\nWhen plugging in $\\operatorname{SReg}\\left(T_{s}\\right)=O\\left(|A| \\sqrt{T_{s}}\\right)$, we obtain $\\operatorname{CSReg}(T) \\leq O(|A| \\sqrt{|S| T})$.\nProof. The contextual regret of Algorithm 1 is\n\n$$\n\\begin{aligned}\n\\operatorname{CReg}(T) & =\\max _{d: S \\rightarrow A} \\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(v^{t}\\left(d\\left(s^{t}\\right)\\right)-v^{t}\\left(a^{t}\\right)\\right)\\right] \\\\\n& =\\max _{d: S \\rightarrow A} \\mathbb{E}\\left[\\sum_{s=1}^{|S|} \\sum_{t: s^{t}=s}\\left(v^{t}(d(s))-v^{t}\\left(a^{t}\\right)\\right)\\right] \\\\\n& \\leq \\sum_{s=1}^{|S|} \\max _{a^{\\prime} \\in A} \\mathbb{E}\\left[\\sum_{t: s^{t}=s}\\left(v^{t}\\left(a^{t}\\right)-v^{t}\\left(a^{t}\\right)\\right)\\right] \\\\\n& \\leq \\sum_{s=1}^{|S|} \\mathbb{E}_{T_{s}}\\left[\\operatorname{Reg}\\left(T_{s}\\right)\\right] \\quad \\text { where } T_{s} \\text { is the number of rounds where } s^{t}=s \\\\\n& \\leq \\max \\left\\{\\sum_{s=1}^{|S|} \\operatorname{Reg}\\left(T_{s}\\right) \\mid T_{1}+\\cdots+T_{|S|}=T\\right\\}\n\\end{aligned}\n$$\n\nWhen $\\operatorname{Reg}\\left(T_{s}\\right)=O\\left(\\sqrt{|A| T_{s}}\\right)$, by Jensen's inequality we obtain\n\n$$\n\\operatorname{CReg}(T) \\leq \\sum_{s=1}^{|S|} O\\left(\\sqrt{|A| T_{s}}\\right) \\leq O(\\sqrt{|A|}) \\sqrt{|S|} \\sqrt{\\sum_{s=1}^{|S|} T_{s}}=O(\\sqrt{|A||S| T})\n$$\n\nThe argument for contextual swap-regret is similar:\n\n$$\n\\begin{aligned}\n\\operatorname{CSReg}(T) & =\\max _{d: S \\times A \\rightarrow A} \\mathbb{E}\\left[\\sum_{t=1}^{T}\\left(v^{t}\\left(d\\left(s^{t}, a^{t}\\right)\\right)-v^{t}\\left(a^{t}\\right)\\right)\\right] \\\\\n& =\\max _{d: S \\times A \\rightarrow A} \\mathbb{E}\\left[\\sum_{s=1}^{|S|} \\sum_{t: s^{t}=s}\\left(v^{t}\\left(d\\left(s, a^{t}\\right)\\right)-v^{t}\\left(a^{t}\\right)\\right)\\right] \\\\\n& \\leq \\sum_{s=1}^{|S|} \\max _{d^{\\prime}: A \\rightarrow A} \\mathbb{E}\\left[\\sum_{t: s^{t}=s}\\left(v^{t}\\left(d^{\\prime}\\left(a^{t}\\right)\\right)-v^{t}\\left(a^{t}\\right)\\right)\\right] \\\\\n& \\leq \\sum_{s=1}^{|S|} \\mathbb{E}_{T_{s}}\\left[\\operatorname{SReg}\\left(T_{s}\\right)\\right] \\quad \\text { where } T_{s} \\text { is the number of rounds where } s^{t}=s \\\\\n& \\leq \\max \\left\\{\\sum_{s=1}^{|S|} \\operatorname{SReg}\\left(T_{s}\\right) \\mid T_{1}+\\cdots+T_{|S|}=T\\right\\}\n\\end{aligned}\n$$\n\nWhen $\\operatorname{SReg}\\left(T_{s}\\right)=O\\left(|A| \\sqrt{T_{s}}\\right)$, by Jensen's inequality we obtain\n\n$$\n\\operatorname{CSReg}(T) \\leq \\sum_{s=1}^{|S|} O\\left(|A| \\sqrt{T_{s}}\\right) \\leq O(|A|) \\sqrt{|S|} \\sqrt{\\sum_{s=1}^{|S|} T_{s}}=O(|A| \\sqrt{|S| T)}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 22,
      "text": "# B Missing Proofs from Section 3\n## B. 1 Proof of Example 3.1\n\nConsider the quantal response model. Let $\\gamma=\\frac{\\log (|A| \\lambda)}{\\lambda}$. Given signal $s$, with posterior $\\mu_{s}$, we say an action $a \\in A$ is not $\\gamma$-optimal for posterior $\\mu_{s}$ if\n\n$$\nv\\left(\\mu_{s}, a_{s}^{*}\\right)-v\\left(\\mu_{s}, a\\right) \\geq \\gamma\n$$\n\nwhere $a_{s}^{*}$ is an optimal action for $\\mu_{s}$. The probability that the receiver chooses not $\\gamma$-optimal action $a$ is at most:\n\n$$\n\\begin{aligned}\n\\frac{\\exp \\left(\\lambda v\\left(\\mu_{s}, a\\right)\\right)}{\\sum_{a \\in A} \\exp \\left(\\lambda v\\left(\\mu_{s}, a\\right)\\right)} \\leq \\frac{\\exp \\left(\\lambda v\\left(\\mu_{s}, a\\right)\\right)}{\\exp \\left(\\lambda v\\left(\\mu_{s}, a_{s}^{*}\\right)\\right)} & =\\exp \\left(-\\lambda\\left[v\\left(\\mu_{s}, a_{s}^{*}\\right)-v\\left(\\mu_{s}, a\\right)\\right]\\right) \\\\\n& \\leq \\exp (-\\lambda \\gamma)=\\frac{1}{|A| \\lambda}\n\\end{aligned}\n$$\n\nBy a union bound, the probability that the receiver chooses any not $\\gamma$-approxiamtely optimal action is at most $\\frac{1}{\\lambda}$. So, the expected loss of utility of the receiver due to not taking the optimal action is at most\n\n$$\n\\left(1-\\frac{1}{\\lambda}\\right) \\cdot \\gamma+\\frac{1}{\\lambda} \\cdot 1 \\leq \\frac{\\log (|A| \\lambda)+1}{\\lambda}\n$$\n\nThis means that the quantal response strategy is a $\\frac{\\log (|A| \\lambda)+1}{\\lambda}$-best-responding randomized strategy.\n\nConsider inaccurate belief. Given signal $s$, the receiver has belief $\\mu_{s}^{\\prime}$ with total variation distance $d_{\\mathrm{TV}}\\left(\\mu_{s}^{\\prime}, \\mu_{s}\\right) \\leq \\varepsilon$ to the true posterior $\\mu_{s}$. For any action $a \\in A$, the difference of expected utility of action $a$ under beliefs $\\mu_{s}^{\\prime}$ and $\\mu_{s}$ is at most $\\varepsilon$ :\n\n$$\n\\left[\\mathbb{E}_{\\omega \\sim \\mu_{s}^{\\prime}}[v(\\omega, a)]-\\mathbb{E}_{\\omega \\sim \\mu_{s}}[v(\\omega, a)]\\right] \\leq d_{\\mathrm{TV}}\\left(\\mu_{s}^{\\prime}, \\mu_{s}\\right) \\leq \\varepsilon\n$$\n\nSo, the optimal action for $\\mu_{s}^{\\prime}$ is a $2 \\varepsilon$-optimal action for $\\mu_{s}$. This means that the receiver strategy is a deterministic $2 \\varepsilon$-best-responding strategy.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 23,
      "text": "# B. 2 Proof of Theorem 3.3 \n\nWe prove this theorem for no-swap-regret learning algorithms. The argument for no-regret learning algorithms is analogous.\n\nFix the principal's adaptive strategy $\\sigma=\\left(\\sigma^{t}\\right)_{t=1}^{T}$ for the $T$ rounds, where each $\\sigma^{t}$ is a mapping from the history $h^{t-1}=\\left(s^{i}, a^{i}\\right)_{i=1}^{t-1}$ (including past signals and actions) to a single-round strategy $\\pi^{t}$ for round $t$. Given any function $\\operatorname{CSReg}(T)$, let $\\delta=\\frac{\\operatorname{CSReg}(T)}{2 T}$. We construct the following algorithm $\\mathcal{A}$ for the agent: at each round $t$, given history $h^{t-1}=\\left(s^{i}, a^{i}\\right)_{i=1}^{t-1}$,\n\n- If the single-round strategy chosen by the principal at round $t$ is equal to $\\pi^{t}=\\sigma^{t}\\left(h^{t-1}\\right)$, then the agent plays a strategy $\\rho^{t} \\in \\arg \\min _{p \\in \\mathcal{R}_{\\delta}\\left(\\pi^{t}\\right)} U\\left(\\pi^{t}, \\rho\\right)$, namely, a randomized $\\delta$-bestresponding strategy that minimizes the principal's utility.\n- If the single-round strategy chosen by the principal at round $t$ is not equal to $\\pi^{t}=\\sigma^{t}\\left(h^{t-1}\\right)$, then the agent switches to any existing contextual no-swap-regret algorithm with swap regret at most $\\frac{\\operatorname{CSReg}(T)}{2}$ (see Proposition 2.1 for examples of such algorithms).\n\nWe show that the agent's algorithm has swap regret at most $\\operatorname{CSReg}(T)$ no matter what strategy the principal uses:\n\n- If the principal keeps using strategy $\\sigma$, namely, at each round $t$ the principal uses single-round strategy $\\pi^{t}=\\sigma^{t}\\left(h^{t-1}\\right)$, denoted by $\\pi^{t}=\\left\\{\\left(\\pi_{s}^{t}, x_{s}^{t}\\right)\\right\\}_{s \\in S}$, then the agent will respond by strategy $\\rho^{t}$. For any deviation function $d: S \\times A \\rightarrow A$, the expected regret of the agent not deviating according to $d$ in this round is\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[v\\left(x^{t}, d\\left(s^{t}, a^{t}\\right)\\right)-v\\left(x^{t}, a^{t}\\right)\\right]=\\mathbb{E}_{h^{t-1}}\\left[\\sum_{s \\in S} \\pi_{s}^{t} \\sum_{a \\in A} \\rho^{t}(a \\mid s)\\left(v\\left(x_{s}^{t}, d(s, a)\\right)-v\\left(x_{s}^{t}, a\\right)\\right)\\right] \\\\\n& =\\mathbb{E}_{h^{t-1}}\\left[\\sum_{s \\in S} \\pi_{s}^{t} \\sum_{a \\in A} \\rho^{t}(a \\mid s) v\\left(x_{s}^{t}, d(s, a)\\right)-\\sum_{s \\in S} \\pi_{s}^{t} \\sum_{a \\in A} \\rho^{t}(a \\mid s) v\\left(x_{s}^{t}, a\\right)\\right] \\\\\n& \\leq \\mathbb{E}_{h^{t-1}}\\left[\\sum_{s \\in S} \\pi_{s}^{t} \\max _{a \\in A} v\\left(x_{s}^{t}, a\\right)-\\sum_{s \\in S} \\pi_{s}^{t} \\sum_{a \\in A} \\rho^{t}(a \\mid s) v\\left(x_{s}^{t}, a\\right)\\right] \\\\\n& =\\mathbb{E}_{h^{t-1}}\\left[\\sum_{s \\in S} \\pi_{s}^{t} \\max _{a \\in A} v\\left(x_{s}^{t}, a\\right)-V\\left(\\pi^{t}, \\rho^{t}\\right)\\right]\n\\end{aligned}\n$$\n\nSince $\\rho^{t} \\in \\mathcal{R}_{\\delta}\\left(\\pi^{t}\\right)$, we have $V\\left(\\pi^{t}, \\rho^{t}\\right) \\geq \\sum_{s \\in S} \\pi_{s}^{t} \\max _{a \\in A} v\\left(x_{s}^{t}, a\\right)-\\delta$, so the above is\n\n$$\n\\leq \\mathbb{E}_{h^{t-1}}[\\delta]=\\delta\n$$\n\nSumming over all $T$ rounds, we obtain:\n\n$$\n\\sum_{t=1}^{T} \\mathbb{E}\\left[v\\left(x^{t}, d\\left(s^{t}, a^{t}\\right)\\right)-v\\left(x^{t}, a^{t}\\right)\\right] \\leq T \\delta=\\frac{\\operatorname{CSReg}(T)}{2}\n$$\n\n- If the principal does not play according to $\\sigma$ in any round, then the agent will switch to an algorithm with swap regret at most $\\frac{\\operatorname{CSReg}(T)}{2}$, so the total swap regret of the agent is at most:\n\n$$\nT \\delta+\\frac{\\operatorname{CSReg}(T)}{2} \\leq \\operatorname{CSReg}(T)\n$$\n\nwhich proves that the agent's learning algorithm has swap regret at most $\\operatorname{CSReg}(T)$.\nThe principal's average utility, when the principal uses strategy $\\sigma$ and the agent uses the above no-swap-regret algorithm, is\n\n$$\n\\begin{aligned}\n\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}\\left[u\\left(x^{t}, a^{t}\\right)\\right] & =\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}_{h^{t-1}}\\left[\\sum_{s \\in S} \\pi_{s}^{t} \\sum_{a \\in A} \\rho^{t}(a \\mid s) u\\left(x_{s}^{t}, a\\right)\\right] \\\\\n& =\\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}_{h^{t-1}}\\left[U\\left(\\pi^{t}, \\rho^{t}\\right)\\right] \\\\\n& \\leq \\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{E}_{h^{t-1}}\\left[\\sup _{\\pi} \\min _{\\rho \\in \\mathcal{R}_{\\delta}(\\pi)} U(\\pi, \\rho)\\right] \\quad \\text { because } \\rho^{t} \\in \\underset{\\rho \\in \\mathcal{R}_{\\delta}\\left(\\pi^{t}\\right)}{\\arg \\min } U\\left(\\pi^{t}, \\rho\\right) \\\\\n& =\\sup _{\\pi} \\min _{\\rho \\in \\mathcal{R}_{\\delta}(\\pi)} U(\\pi, \\rho) \\\\\n& =\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta) \\\\\n& =\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}\\left(\\frac{\\operatorname{CSReg}(T)}{2 T}\\right)\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 24,
      "text": "# C Missing Proofs from Section 4\n## C. 1 Proof of Claim 4.1\n\nIf no $G>0$ satisfies the claim, then there must exist an $a_{0} \\in A$ such that for all $x \\in \\mathcal{X}, v\\left(a_{0}, \\mu\\right)-$ $v\\left(a^{\\prime}, \\mu\\right) \\leq 0$ for some $a^{\\prime} \\in A \\backslash\\left\\{a_{0}\\right\\}$. Namely,\n\n$$\n\\max _{x \\in \\mathcal{X}} \\min _{a^{\\prime} \\in A \\backslash\\left\\{a_{0}\\right\\}}\\left\\{v\\left(x, a_{0}\\right)-v\\left(x, a^{\\prime}\\right)\\right\\} \\leq 0\n$$\n\nThen, by the minimax theorem, we have\n\n$$\n\\min _{\\alpha^{\\prime} \\in \\Delta\\left(A \\backslash\\left\\{a_{0}\\right\\}\\right)} \\max _{x \\in \\mathcal{X}}\\left\\{v\\left(x, a_{0}\\right)-v\\left(x, \\alpha^{\\prime}\\right)\\right\\}=\\max _{x \\in \\mathcal{X}} \\min _{a^{\\prime} \\in A \\backslash\\left\\{a_{0}\\right\\}}\\left\\{v\\left(x, a_{0}\\right)-v\\left(x, a^{\\prime}\\right)\\right\\} \\leq 0\n$$\n\nThis means that $a_{0}$ is weakly dominated by some mixed action $\\alpha^{\\prime} \\in \\Delta\\left(A \\backslash\\left\\{a_{0}\\right\\}\\right)$, violating Assumption 4.1.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 25,
      "text": "## C. 2 Proof of Example 4.1\n\nWe use the probability $\\mu \\in[0,1]$ of the Good state to represent a belief (so the probability of Bad state is $1-\\mu)$.\n\nFirst, the sender's optimal utility when the receiver exactly best responds is $2 \\mu_{0}$ :\n\n$$\nU^{*}=2 \\mu_{0}\n$$\n\nThis is achieved by decomposing the prior $\\mu_{0}$ into two posteriors $\\mu_{a}=\\frac{1}{2}$ and $\\mu_{b}=0$ with probability $2 \\mu_{0}$ and $1-2 \\mu_{0}$ respectively, with the receiver taking action $a$ under posterior $\\mu_{a}$ and $b$ under $\\mu_{b}$.\n\nThen, consider any signaling scheme of the sender, $\\pi=\\left\\{\\left(\\pi_{s}, \\mu_{s}\\right)\\right\\}_{s \\in S}$, which is a decomposition of the prior $\\mu_{0}$ into $|S|$ posteriors $\\mu_{s} \\in[0,1]$ such that $\\sum_{s \\in S} \\pi_{s} \\mu_{s}=\\mu_{0}$. Let $\\rho: S \\rightarrow \\Delta(A)$ be a randomized strategy of the receiver, where $\\rho(a \\mid s)$ (and $\\rho(b \\mid s)$ ) denotes the probability that the receiver takes action $a$ (and $b$ ) under signal $s$. The sender's expected utility under $\\pi$ and $\\rho$ is:\n\n$$\nU(\\pi, \\rho)=\\sum_{s \\in S} \\pi_{s}[\\rho(a \\mid s) \\cdot 1+\\rho(b \\mid s) \\cdot 0]=\\sum_{s \\in S} \\pi_{s} \\rho(a \\mid s)\n$$\n\nThe receiver's utility when taking action $a$ at posterior $\\mu_{s}$ is $\\mu_{s} \\cdot 1+\\left(1-\\mu_{s}\\right) \\cdot(-1)=2 \\mu_{s}-1$. So, the receiver's expected utility under $\\pi$ and $\\rho$ is\n\n$$\nV(\\pi, \\rho)=\\sum_{s \\in S} \\pi_{s}\\left[\\rho(a \\mid s) \\cdot\\left(2 \\mu_{s}-1\\right)+\\rho(b \\mid s) \\cdot 0\\right]=\\sum_{s \\in S} \\pi_{s} \\rho(a \\mid s)\\left(2 \\mu_{s}-1\\right)\n$$\n\nClearly, the receiver's best response $\\rho^{*}$ is to take action $a$ with certainty if and only if $\\mu_{s}>\\frac{1}{2}$, with expected utility\n\n$$\nV\\left(\\pi, \\rho^{*}\\right)=\\sum_{s: \\mu_{s}>\\frac{1}{2}} \\pi_{s}\\left(2 \\mu_{s}-1\\right)\n$$\n\nTo find $\\operatorname{OBJ}^{\\mathcal{R}}(\\delta)=\\sup _{\\pi} \\min _{\\rho \\in \\mathcal{R}_{\\delta}(\\pi)} U(\\pi, \\rho)$, we fix any $\\pi$ and solve the inner optimization problem (minimizing the sender's utility) regarding $\\rho$ :\n\n$$\n\\begin{aligned}\n& \\min _{\\rho} \\quad U(\\pi, \\rho)=\\sum_{s \\in S} \\pi_{s} \\rho(a \\mid s) \\\\\n& \\text { s.t. } \\quad \\rho \\in \\mathcal{R}_{\\delta}(\\pi) \\Longleftrightarrow \\delta \\geq V\\left(\\pi, \\rho^{*}\\right)-V(\\pi, \\rho) \\\\\n& =\\sum_{s: \\mu_{s}>\\frac{1}{2}} \\pi_{s}\\left(2 \\mu_{s}-1\\right)-\\sum_{s \\in S} \\pi_{s} \\rho(a \\mid s)\\left(2 \\mu_{s}-1\\right)\n\\end{aligned}\n$$\n\nWithout loss of generality, we can assume that the solution $\\rho$ satisfies $\\rho(a \\mid s)=0$ whenever $\\mu_{s} \\leq \\frac{1}{2}$ (if $\\rho(a \\mid s)>0$ for some $\\mu_{s} \\leq \\frac{1}{2}$, then making $\\rho(a \\mid s)$ to be 0 can decrease the objective $\\sum_{s \\in S} \\pi_{s} \\rho(a \\mid s)$ while still satisfying the constraint). So, the optimization problem can be simplified to:\n\n$$\n\\begin{aligned}\n& \\min _{\\rho} \\quad U(\\pi, \\rho)=\\sum_{s: \\mu_{s}>\\frac{1}{2}} \\pi_{s} \\rho(a \\mid s) \\\\\n& \\text { s.t. } \\quad \\delta \\geq \\sum_{s: \\mu_{s}>\\frac{1}{2}} \\pi_{s}\\left(2 \\mu_{s}-1\\right)-\\sum_{s: \\mu_{s}>\\frac{1}{2}} \\pi_{s} \\rho(a \\mid s)\\left(2 \\mu_{s}-1\\right) \\\\\n& =\\sum_{s: \\mu_{s}>\\frac{1}{2}} \\pi_{s}\\left(2 \\mu_{s}-1\\right)(1-\\rho(a \\mid s)) \\\\\n& \\rho(a \\mid s) \\in[0,1], \\quad \\forall s \\in S: \\mu_{s}>\\frac{1}{2}\n\\end{aligned}\n$$\n\nWe note that this is a fractional knapsack linear program, which has a greedy solution (e.g., (Korte and Vygen, 2012)): sort the signals with $\\mu_{s}>\\frac{1}{2}$ in increasing order of $2 \\mu_{s}-1$ (equivalently, increasing order of $\\mu_{s}$ ); label those signals by $s=1, \\ldots, n$; find the first position $k$ for which $\\sum_{s=1}^{k} \\pi_{s}\\left(2 \\mu_{s}-1\\right)>\\delta$ :\n\n$$\nk=\\min \\left\\{j: \\sum_{s=1}^{j} \\pi_{s}\\left(2 \\mu_{s}-1\\right)>\\delta\\right\\}\n$$\n\nthen, an optimal solution $\\rho$ is given by:\n\n$$\n\\begin{cases}\\rho(a \\mid s)=0 & \\text { for } s=1, \\ldots, k-1 \\\\ \\rho(a \\mid k)=1-\\frac{\\delta-\\sum_{k=1}^{k-1} \\pi_{s}\\left(2 \\mu_{s}-1\\right)}{\\pi_{k}\\left(2 \\mu_{k}-1\\right)} & \\text { for } s=k \\\\ \\rho(a \\mid s)=1 & \\text { for } s=k+1, \\ldots, n\\end{cases}\n$$\n\nThe objective value (sender's expected utility) of the above solution $\\rho$ is\n\n$$\n\\begin{aligned}\nU(\\pi, \\rho) & =\\sum_{s: \\mu_{s}>\\frac{1}{2}} \\pi_{s} \\rho(a \\mid s) \\\\\n& =\\pi_{k}\\left(1-\\frac{\\delta-\\sum_{s=1}^{k-1} \\pi_{s}\\left(2 \\mu_{s}-1\\right)}{\\pi_{k}\\left(2 \\mu_{k}-1\\right)}\\right)+\\sum_{s=k+1}^{n} \\pi_{s} \\\\\n& =\\sum_{s=k}^{n} \\pi_{s}-\\frac{\\delta}{2 \\mu_{k}-1}+\\sum_{s=1}^{k-1} \\frac{\\pi_{s}\\left(2 \\mu_{s}-1\\right)}{2 \\mu_{k}-1}\n\\end{aligned}\n$$\n\nSince the signaling scheme $\\pi$ must satisfy $\\sum_{s \\in S} \\pi_{s} \\mu_{s}=\\mu_{0}$, we have\n\n$$\n\\begin{aligned}\n\\mu_{0} & =\\sum_{s \\in S} \\pi_{s} \\mu_{s} \\geq \\sum_{s=1}^{n} \\pi_{s} \\mu_{s}=\\sum_{s=1}^{k-1} \\pi_{s} \\mu_{s}+\\sum_{s=k}^{n} \\pi_{s} \\mu_{s} \\geq \\sum_{s=1}^{k-1} \\pi_{s} \\mu_{s}+\\sum_{s=k}^{n} \\pi_{s} \\mu_{k} \\\\\n& \\Longrightarrow \\quad \\sum_{s=k}^{n} \\pi_{s} \\leq \\frac{\\mu_{0}-\\sum_{s=1}^{k-1} \\pi_{s} \\mu_{s}}{\\mu_{k}}\n\\end{aligned}\n$$\n\nSo,\n\n$$\n\\begin{aligned}\nU(\\pi, \\rho) & \\leq \\frac{\\mu_{0}-\\sum_{s=1}^{k-1} \\pi_{s} \\mu_{s}}{\\mu_{k}}-\\frac{\\delta}{2 \\mu_{k}-1}+\\sum_{s=1}^{k-1} \\frac{\\pi_{s}\\left(2 \\mu_{s}-1\\right)}{2 \\mu_{k}-1} \\\\\n& =\\frac{\\mu_{0}}{\\mu_{k}}-\\frac{\\delta}{2 \\mu_{k}-1}+\\sum_{s=1}^{k-1} \\pi_{s}\\left(\\frac{2 \\mu_{s}-1}{2 \\mu_{k}-1}-\\frac{\\mu_{s}}{\\mu_{k}}\\right)\n\\end{aligned}\n$$\n\nSince $\\frac{2 \\mu_{s}-1}{2 \\mu_{k}-1}-\\frac{\\mu_{s}}{\\mu_{k}}=\\frac{\\mu_{s}-\\mu_{k}}{\\left(2 \\mu_{s}-1\\right) \\mu_{k}} \\leq 0$ for any $s \\leq k-1$, we get\n\n$$\nU(\\pi, \\rho) \\leq \\frac{\\mu_{0}}{\\mu_{k}}-\\frac{\\delta}{2 \\mu_{k}-1}=f\\left(\\mu_{k}\\right)\n$$\n\nWe find the maximal value of $f\\left(\\mu_{k}\\right)=\\frac{\\mu_{0}}{\\mu_{k}}-\\frac{\\delta}{2 \\mu_{k}-1}$. Take its derivative:\n\n$$\nf^{\\prime}\\left(\\mu_{k}\\right)=-\\frac{\\mu_{0}}{\\mu_{k}^{2}}+\\frac{2 \\delta}{\\left(2 \\mu_{k}-1\\right)^{2}}=\\frac{\\left[(\\sqrt{2 \\delta}+2 \\sqrt{\\mu_{0}}) \\mu_{k}-\\sqrt{\\mu}_{0}\\right] \\cdot\\left[(\\sqrt{2 \\delta}-2 \\sqrt{\\mu_{0}}) \\mu_{k}+\\sqrt{\\mu}_{0}\\right]}{\\mu_{k}^{2}\\left(2 \\mu_{k}-1\\right)^{2}}\n$$\n\nwhich has two roots $\\frac{\\sqrt{\\mu}_{0}}{\\sqrt{2 \\delta}+2 \\sqrt{\\mu_{0}}}<\\frac{1}{2}$ and $\\frac{\\sqrt{\\mu}_{0}}{2 \\sqrt{\\mu_{0}}-\\sqrt{2 \\delta}} \\in\\left(\\frac{1}{2}, 1\\right)$ when $0<\\delta<\\frac{\\mu_{0}}{2}$. So, $f(x)$ is increasing in $\\left[\\frac{1}{2}, \\frac{\\sqrt{\\mu}_{0}}{2 \\sqrt{\\mu_{0}}-\\sqrt{2 \\delta}}\\right.$ ) and decreasing in $\\left(\\frac{\\sqrt{\\mu}_{0}}{2 \\sqrt{\\mu_{0}}-\\sqrt{2 \\delta}}, 1\\right]$. Since $\\mu_{k}>\\frac{1}{2}, f\\left(\\mu_{k}\\right)$ is maximized at $\\mu_{k}=\\frac{\\sqrt{\\mu}_{0}}{2 \\sqrt{\\mu_{0}}-\\sqrt{2 \\delta}}$. This implies\n\n$$\nU(\\pi, \\rho) \\leq f\\left(\\frac{\\sqrt{\\mu}_{0}}{2 \\sqrt{\\mu_{0}}-\\sqrt{2 \\delta}}\\right)=\\frac{\\mu_{0}}{\\sqrt{\\mu_{0}}}\\left(2 \\sqrt{\\mu_{0}}-\\sqrt{2 \\delta}\\right)-\\frac{\\delta}{2 \\frac{\\sqrt{\\mu_{0}}}{2 \\sqrt{\\mu_{0}}-\\sqrt{2 \\delta}}-1}=2 \\mu_{0}-2 \\sqrt{2 \\mu_{0} \\delta}+\\delta\n$$\n\nThis holds for any $\\pi$. So, $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta)=\\sup _{\\pi} \\min _{\\rho \\in \\mathcal{R}_{2}(\\pi)} U(\\pi, \\rho) \\leq U^{*}-2 \\sqrt{2 \\mu_{0} \\delta}+\\delta=U^{*}-\\Omega(\\sqrt{\\delta})$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 26,
      "text": "# C. 3 Proof of Theorems 4.2 and 4.3 \n\nLower bounds on $\\overline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\delta)$ and upper bounds on $\\overline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta)$. First, we prove the lower bounds on $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta)$ and the upper bounds on $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta)$ in Theorems 4.2 and 4.3, given by the following two lemmas:\n\nLemma C.1. In an unconstrained generalized principal-agent problem, $\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta) \\geq U^{*}-\\operatorname{diam}(\\mathcal{X}) L \\frac{\\delta}{G}$. With constraint $\\sum_{s \\in S} \\pi_{s} x_{s} \\in \\mathcal{C}, \\underline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta) \\geq U^{*}-\\left(\\operatorname{diam}(\\mathcal{X}) L+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G}$.\n\nLemma C.2. In an unconstrained generalized principal-agent problem, $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta) \\leq U^{*}+\\operatorname{diam}(\\mathcal{X}) L \\frac{\\delta}{G}$. With constraint $\\sum_{s \\in S} \\pi_{s} x_{s} \\in \\mathcal{C}, \\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta) \\leq U^{*}+\\left(\\operatorname{diam}(\\mathcal{X}) L+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G}$.\n\nThe proofs of Lemmas C. 1 and C. 2 are similar and given in Appendix C. 4 and C.5. The main idea to prove Lemma C. 2 is the following. Let $(\\pi, \\rho)$ be any pair of principal's strategy and agent's $\\delta$-best-responding strategy. We perturb the principal's strategy $\\pi$ slightly to be a strategy $\\pi^{\\prime}$ for which $\\rho$ is exactly best-responding (such a perturbation is possible due to Assumption 4.1). Since $\\rho$ is best-responding to $\\pi^{\\prime}$, the pair $\\left(\\pi^{\\prime}, \\rho\\right)$ cannot give the principal a higher utility than $U^{*}$ (which is the optimal principal utility under the best-response model). This means that the original pair $(\\pi, \\rho)$ cannot give the principal a utility much higher than $U^{*}$, implying an upper bound on $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta)$.\n\nUpper bounds on $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta)$ imply upper bounds on $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta)$. Then, because $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta) \\leq$ $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta)$, we immediately obtain the upper bounds on $\\overline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta)$ in the two theorems.\n\nLower bounds for $\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta)$ imply lower bounds for $\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta)$ Finally, we show that the lower bounds for $\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\delta)$ imply the lower bounds for $\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta)$, using the following lemma:\n\nLemma C.3. For any $\\delta \\geq 0, \\Delta>0, \\underline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta) \\geq \\underline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\Delta)-\\frac{2 B \\delta}{\\Delta}$.\nThe proof of this lemma is in Appendix C.6.\nUsing Lemma C. 3 with $\\Delta=\\sqrt{\\frac{2 B G \\delta}{\\operatorname{diam}(\\mathcal{X}) L}}$ and the lower bound for $\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\Delta)$ in Lemma C. 1 for the unconstrained case, we obtain:\n\n$$\n\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta) \\geq \\underline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\Delta)-\\frac{2 B \\delta}{\\Delta} \\geq U^{*}-\\operatorname{diam}(\\mathcal{X}) L \\frac{\\Delta}{G}-\\frac{2 B \\delta}{\\Delta}=U^{*}-2 \\sqrt{\\frac{2 B L}{G} \\operatorname{diam}(\\mathcal{X}) \\delta}\n$$\n\nwhich gives the lower bound for $\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta)$ in Theorem 4.2.\nUsing Lemma C. 3 with $\\Delta=\\sqrt{\\frac{2 B G \\delta}{L \\operatorname{diam}(\\mathcal{X})+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}}}$ and the lower bound for $\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\Delta)$ in Lemma C. 1 for the constrained case, we obtain:\n\n$$\n\\begin{aligned}\n\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta) \\geq \\underline{\\mathrm{OBJ}}{ }^{\\mathcal{D}}(\\Delta)-\\frac{2 B \\delta}{\\Delta} & \\geq U^{*}-\\left(\\operatorname{diam}(\\mathcal{X}) L+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\Delta}{G}-\\frac{2 B \\delta}{\\Delta} \\\\\n& =U^{*}-2 \\sqrt{\\frac{2 B}{G}\\left(\\operatorname{diam}(\\mathcal{X}) L+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\delta}\n\\end{aligned}\n$$\n\nThis proves the lower bound for $\\underline{\\mathrm{OBJ}}{ }^{\\mathcal{R}}(\\delta)$ in Theorem 4.3.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 27,
      "text": "# C. 4 Proof of Lemma C. 1 \n\nLet $(\\pi, \\rho)$ be a pair of principal strategy and agent strategy that achieves the optimal principal utility with an exactly-best-responding agent, namely, $U(\\pi, \\rho)=U^{*}$. Without loss of generality $\\rho$ can be assumed to be deterministic, $\\rho: S \\rightarrow A$. The strategy $\\pi$ consists of pairs $\\left\\{\\left(\\pi_{s}, x_{s}\\right)\\right\\}_{s \\in S}$ that satisfy\n\n$$\n\\sum_{s \\in S} \\pi_{s} x_{s}=: \\mu_{0} \\in \\mathcal{C}\n$$\n\nand the action $a=\\rho(s)$ is optimal for the agent with respect to $x_{s}$. We will construct another principal strategy $\\pi^{\\prime}$ such that, even if the agent chooses the worst $\\delta$-best-responding strategy to $\\pi^{\\prime}$, the principal can still obtain utility arbitrarily close to $U^{*}-\\left(L \\operatorname{diam}\\left(\\mathcal{X} ; \\ell_{1}\\right)+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G}$.\n\nTo construct $\\pi^{\\prime}$ we do the following: For each signal $s \\in S$, with corresponding action $a=\\rho(s)$, by Claim 4.1 there exists $y_{a} \\in \\mathcal{X}$ such that $v\\left(y_{a}, a\\right)-v\\left(y_{a}, a^{\\prime}\\right) \\geq G$ for any $a^{\\prime} \\neq a$. Let $\\theta=\\frac{\\delta}{G}+\\varepsilon \\in$ $[0,1]$ for arbitrarily small $\\varepsilon>0$, and let $\\tilde{x}_{s}$ be the convex combination of $x_{s}$ and $y_{\\rho(s)}$ with weights $1-\\theta, \\theta:$\n\n$$\n\\tilde{x}_{s}=(1-\\theta) x_{s}+\\theta y_{\\rho(s)}\n$$\n\nWe note that $a=\\rho(s)$ is the agent's optimal action for $\\tilde{x}_{s}$ and moreover it is better than any other action $a^{\\prime} \\neq a$ by more than $\\delta$ :\n\n$$\n\\begin{aligned}\nv\\left(\\tilde{x}_{s}, a\\right)-v\\left(\\tilde{x}_{s}, a^{\\prime}\\right) & =(1-\\theta)\\left[\\underbrace{v\\left(x_{s}, a\\right)-v\\left(x_{s}, a^{\\prime}\\right)}_{\\geq 0 \\text { because } a=\\rho(s) \\text { is optimal for } x_{s}}\\right]+\\theta\\left[\\underbrace{v\\left(y_{a}, a\\right)-v\\left(y_{a}, a^{\\prime}\\right)}_{\\geq G \\text { by our choice of } y_{a}}\\right] \\\\\n& \\geq 0+\\theta G>\\frac{\\delta}{G} G=\\delta\n\\end{aligned}\n$$\n\nLet $\\mu^{\\prime}$ be the convex combination of $\\left\\{\\tilde{x}_{s}\\right\\}_{s \\in S}$ with weights $\\left\\{\\pi_{s}\\right\\}_{s \\in S}$ :\n\n$$\n\\mu^{\\prime}=\\sum_{s \\in S} \\pi_{s} \\tilde{x}_{s}\n$$\n\nNote that $\\mu^{\\prime}$ might not satisfy the constraint $\\mu^{\\prime} \\in \\mathcal{C}$. So, we want to find another vector $z \\in \\mathcal{X}$ and a coefficient $\\eta \\in[0,1]$ such that\n\n$$\n(1-\\eta) \\mu^{\\prime}+\\eta z \\in \\mathcal{C}\n$$\n\n(If $\\mu^{\\prime}$ already satisfies $\\mu^{\\prime} \\in \\mathcal{C}$, then let $\\eta=0$.) To do this, we consider the ray starting from $\\mu^{\\prime}$ pointing towards $\\mu_{0}:\\left\\{\\mu^{\\prime}+t\\left(\\mu_{0}-\\mu^{\\prime}\\right) \\mid t \\geq 0\\right\\}$. Let $z$ be the intersection of the ray with the boundary of $\\mathcal{X}$ :\n\n$$\nz=\\mu^{\\prime}+t^{*}\\left(\\mu_{0}-\\mu^{\\prime}\\right), \\quad t^{*}=\\arg \\max \\left\\{t \\geq 0 \\mid \\mu^{\\prime}+t\\left(\\mu_{0}-\\mu^{\\prime}\\right) \\in \\mathcal{X}\\right\\}\n$$\n\nThen, rearranging $z=\\mu^{\\prime}+t^{*}\\left(\\mu_{0}-\\mu^{\\prime}\\right)$, we get\n\n$$\n\\frac{1}{t^{*}}\\left(z-\\mu^{\\prime}\\right)=\\mu_{0}-\\mu^{\\prime} \\quad \\Longleftrightarrow \\quad\\left(1-\\frac{1}{t^{*}}\\right) \\mu^{\\prime}+\\frac{1}{t^{*}} z=\\mu_{0} \\in \\mathcal{C}\n$$\n\nwhich satisfies (21) with $\\eta=\\frac{1}{t^{*}}$. We then give an upper bound on $\\eta=\\frac{1}{t^{*}}$ :\nClaim C.4. $\\eta=\\frac{1}{t^{*}} \\leq \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})} \\theta$.\nProof. On the one hand,\n\n$$\n\\begin{aligned}\n\\left\\|\\mu_{0}-\\mu^{\\prime}\\right\\| & =\\left\\|\\sum_{s \\in S} \\pi_{s} x_{s}-\\sum_{s \\in S} \\pi_{s} \\tilde{x}_{s}\\right\\|=\\left\\|\\sum_{s \\in S} \\pi_{s} \\theta\\left(y_{\\rho(s)}-x_{s}\\right)\\right\\| \\\\\n& \\leq \\theta \\sum_{s \\in S} \\pi_{s}\\left\\|y_{\\rho(s)}-x_{s}\\right\\| \\leq \\theta \\sum_{s \\in S} \\pi_{s} \\cdot \\operatorname{diam}(\\mathcal{X})=\\theta \\cdot \\operatorname{diam}(\\mathcal{X})\n\\end{aligned}\n$$\n\nOn the other hand, because $z-\\mu^{\\prime}$ and $\\mu_{0}-\\mu^{\\prime}$ are in the same direction, we have\n\n$$\n\\left\\|z-\\mu^{\\prime}\\right\\|=\\left\\|z-\\mu_{0}\\right\\|+\\left\\|\\mu_{0}-\\mu^{\\prime}\\right\\| \\geq\\left\\|z-\\mu_{0}\\right\\| \\geq \\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})\n$$\n\nbecause $\\mu_{0}$ is in $\\mathcal{C}$ and $z$ is on the boundary of $\\mathcal{X}$. Therefore, $\\eta=\\frac{1}{t^{s}}=\\frac{\\left\\|\\mu_{0}-\\mu^{\\prime}\\right\\|}{\\left\\|z-\\mu^{\\prime}\\right\\|} \\leq \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})} \\theta$.\nThe convex combinations (21) (20) define a new principal strategy $\\pi^{\\prime}$ with $|S|+1$ signals, consisting of $\\tilde{x}_{s}$ with probability $(1-\\eta) \\pi_{s}$ and $z$ with probability $\\eta$, satisfying $\\sum_{s \\in S}(1-\\eta) \\pi_{s} \\tilde{x}_{s}+\\eta z=$ $\\mu_{0} \\in \\mathcal{C}$. Consider the agent's worst (for the principal) $\\delta$-best-responding strategies $\\rho^{\\prime}$ to $\\pi^{\\prime}$ :\n\n$$\n\\rho^{\\prime} \\in \\underset{\\rho \\in \\mathcal{D}_{\\delta}\\left(\\pi^{\\prime}\\right)}{\\arg \\min } U\\left(\\pi^{\\prime}, \\rho\\right)\n$$\n\nWe note that $\\rho^{\\prime}\\left(\\tilde{x}_{s}\\right)$ must be equal to $\\rho(s)$ for each $s \\in S$. This is because $a=\\rho(s)$ is strictly better than any other action $a^{\\prime} \\neq a$ by a margin of $\\delta$ (19), so $a$ is the only $\\delta$-optimal action for $\\tilde{x}_{s}$.\n\nThen, the principal's expected utility under $\\pi^{\\prime}$ and $\\rho^{\\prime}$ is\n\n$$\n\\begin{aligned}\nU\\left(\\pi^{\\prime}, \\rho^{\\prime}\\right) \\stackrel{(21),(20)}{=}(1-\\eta) \\sum_{s \\in S} \\pi_{s} u\\left(\\tilde{x}_{s}, \\rho^{\\prime}\\left(\\tilde{x}_{s}\\right)\\right)+\\eta u\\left(z, \\rho^{\\prime}(z)\\right) \\\\\n\\geq(1-\\eta) \\sum_{s \\in S} \\pi_{s} u\\left(\\tilde{x}_{s}, \\rho(s)\\right)-\\eta B \\\\\n\\geq(1-\\eta) \\sum_{s \\in S} \\pi_{s}\\left(u\\left(x_{s}, \\rho(s)\\right)-L \\quad\\left\\|\\tilde{x}_{s}-x_{s}\\right\\| \\leq \\frac{\\left\\|\\tilde{x}_{s}-x_{s}\\right\\|}{\\left.\\operatorname{diam}(\\mathcal{X})-\\eta B\\right)}-\\eta B \\\\\n\\geq(1-\\eta) U(\\pi, \\rho)-L \\theta \\operatorname{diam}(\\mathcal{X})-\\eta B \\\\\n\\geq U(\\pi, \\rho)-L \\theta \\operatorname{diam}(\\mathcal{X})-2 \\eta B \\\\\n\\text { by Claim C. } 4 \\geq U(\\pi, \\rho)-L \\theta \\operatorname{diam}(\\mathcal{X})-2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})} \\theta \\\\\n=U(\\pi, \\rho)-\\left(L \\operatorname{diam}(\\mathcal{X})+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right)\\left(\\frac{\\delta}{G}+\\varepsilon\\right) \\\\\n=U^{*}-\\left(L \\operatorname{diam}(\\mathcal{X})+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G}-O(\\varepsilon)\n\\end{aligned}\n$$\n\nSo, we conclude that\n\n$$\n\\begin{aligned}\n\\underline{\\operatorname{OBJ}}^{\\mathcal{D}}(\\delta)=\\sup _{\\pi} \\min _{\\rho \\in \\mathcal{D}_{\\delta}(\\pi)} U(\\pi, \\rho) & \\geq \\min _{\\rho \\in \\mathcal{D}_{\\delta}\\left(\\pi^{\\prime}\\right)} U\\left(\\pi^{\\prime}, \\rho\\right) \\\\\n& =U\\left(\\pi^{\\prime}, \\rho^{\\prime}\\right) \\geq U^{*}-\\left(L \\operatorname{diam}(\\mathcal{X})+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G}-O(\\varepsilon)\n\\end{aligned}\n$$\n\nLetting $\\varepsilon \\rightarrow 0$ finishes the proof for the case with the constraint $\\sum_{s \\in S} \\pi_{s} x_{s} \\in \\mathcal{C}$.\nThe case without $\\sum_{s \\in S} \\pi_{s} x_{s} \\in \\mathcal{C}$ is proved by letting $\\eta=0$ in the above argument.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 28,
      "text": "# C. 5 Proof of Lemma C. 2 \n\nLet $\\pi$ be a principal strategy and $\\rho \\in \\mathcal{R}_{\\delta}(\\pi)$ be a $\\delta$-best-responding randomized strategy of the agent. The principal strategy $\\pi$ consists of pairs $\\left\\{\\left(\\pi_{s}, x_{s}\\right)\\right\\}_{s \\in S}$ with\n\n$$\n\\sum_{s \\in S} \\pi_{s} x_{s}=: \\mu_{0} \\in \\mathcal{C}\n$$\n\nAt signal $s$, the agent takes action $a$ with probability $\\rho(a \\mid s)$. Let $\\delta_{s, a}$ be the \"suboptimality\" of action $a$ with respect to $x_{s}$ :\n\n$$\n\\delta_{s, a}=\\max _{a^{\\prime} \\in A}\\left\\{v\\left(x_{s}, a^{\\prime}\\right)-v\\left(x_{s}, a\\right)\\right\\}\n$$\n\nBy Claim 4.1, for action $a$ there exists $y_{a} \\in \\mathcal{X}$ such that $v\\left(y_{a}, a\\right)-v\\left(y_{a}, a^{\\prime}\\right) \\geq G$ for any $a^{\\prime} \\neq a$. Let $\\theta_{s, a}=\\frac{\\delta_{s, a}}{G+\\delta_{s, a}} \\in[0,1]$ and let $\\tilde{x}_{s, a}$ be the convex combination of $x_{s}$ and $y_{a}$ with weights $1-\\theta_{s, a}$ and $\\theta_{s, a}:$\n\n$$\n\\tilde{x}_{s, a}=\\left(1-\\theta_{s, a}\\right) x_{s}+\\theta_{s, a} y_{a}\n$$\n\nClaim C.5. We have two useful claims regarding $\\tilde{x}_{s, a}$ and $\\theta_{s, a}$ :\n(1) $a$ is an optimal action for the agent with respect to $\\tilde{x}_{s, a}: v\\left(\\tilde{x}_{s, a}, a\\right)-v\\left(\\tilde{x}_{s, a}, a^{\\prime}\\right) \\geq 0, \\forall a^{\\prime} \\in A$.\n(2) $\\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\theta_{s, a} \\leq \\frac{\\delta}{G}$.\n\nProof. (1) For any $a^{\\prime} \\neq a$, by the definition of $\\tilde{x}_{s, a}$ and $\\theta_{s, a}$,\n\n$$\n\\begin{aligned}\nv\\left(\\tilde{x}_{s, a}, a\\right)-v\\left(\\tilde{x}_{s, a}, a^{\\prime}\\right) & =\\left(1-\\theta_{s, a}\\right)\\left[v\\left(x_{s}, a\\right)-v\\left(x_{s}, a^{\\prime}\\right)\\right]+\\theta_{s, a}\\left[v\\left(y_{a}, a\\right)-v\\left(y_{a}, a^{\\prime}\\right)\\right] \\\\\n& \\geq\\left(1-\\theta_{s, a}\\right)\\left(-\\delta_{s, a}\\right)+\\theta_{s, a} G=\\frac{G}{G+\\delta_{s, a}}\\left(-\\delta_{s, a}\\right)+\\frac{\\delta_{s, a}}{G+\\delta_{s, a}} G=0\n\\end{aligned}\n$$\n\n(2) By the condition that $\\rho$ is a $\\delta$-best-response to $\\pi$, we have\n\n$$\n\\begin{aligned}\n\\delta & \\geq \\max _{\\rho^{*}: S \\rightarrow A} V\\left(\\pi, \\rho^{*}\\right)-V(\\pi, \\rho)=\\sum_{s \\in S} \\pi_{s}\\left(\\max _{a^{\\prime} \\in A}\\left\\{v\\left(x_{s}, a^{\\prime}\\right)\\right\\}-\\sum_{a \\in A} \\rho(a \\mid s) v\\left(x_{s}, a\\right)\\right) \\\\\n& =\\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\max _{a^{\\prime} \\in A}\\left\\{v\\left(x_{s}, a^{\\prime}\\right)-v\\left(x_{s}, a\\right)\\right\\}=\\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\delta_{s, a}\n\\end{aligned}\n$$\n\nSo, $\\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\theta_{s, a}=\\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\frac{\\delta_{s, a}}{G+\\delta_{s, a}} \\leq \\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\frac{\\delta_{s, a}}{G} \\leq \\frac{\\delta}{G}$.\nWe let $\\mu^{\\prime}$ be the convex combination of $\\left\\{\\tilde{x}_{s, a}\\right\\}_{s, a \\in S \\times A}$ with weights $\\left\\{\\pi_{s} \\rho(a \\mid s)\\right\\}_{s, a \\in S \\times A}$ :\n\n$$\n\\mu^{\\prime}=\\sum_{s, a \\in S \\times A} \\pi_{s} \\rho(a \\mid s) \\tilde{x}_{s, a}\n$$\n\nNote that $\\mu^{\\prime}$ might not satisfy the constraint $\\mu^{\\prime} \\in \\mathcal{C}$. So, we want to find another vector $z \\in \\mathcal{X}$ and a coefficient $\\eta \\in[0,1]$ such that\n\n$$\n(1-\\eta) \\mu^{\\prime}+\\eta z \\in \\mathcal{C}\n$$\n\n(If $\\mu^{\\prime}$ already satisfies $\\mu^{\\prime} \\in \\mathcal{C}$, then let $\\eta=0$.) To do this, we consider the ray pointing from $\\mu^{\\prime}$ to $\\mu_{0}:\\left\\{\\mu^{\\prime}+t\\left(\\mu_{0}-\\mu^{\\prime}\\right) \\mid t \\geq 0\\right\\}$. Let $z$ be the intersection of the ray with the boundary of $\\mathcal{X}$ :\n\n$$\nz=\\mu^{\\prime}+t^{*}\\left(\\mu_{0}-\\mu^{\\prime}\\right), \\quad t^{*}=\\arg \\max \\left\\{t \\geq 0 \\mid \\mu^{\\prime}+t\\left(\\mu_{0}-\\mu^{\\prime}\\right) \\in \\mathcal{X}\\right\\}\n$$\n\nThen, rearranging $z=\\mu^{\\prime}+t^{*}\\left(\\mu_{0}-\\mu^{\\prime}\\right)$, we get\n\n$$\n\\frac{1}{t^{*}}\\left(z-\\mu^{\\prime}\\right)=\\mu_{0}-\\mu^{\\prime} \\quad \\Longleftrightarrow \\quad\\left(1-\\frac{1}{t^{*}}\\right) \\mu^{\\prime}+\\frac{1}{t^{*}} z=\\mu_{0} \\in \\mathcal{C}\n$$\n\nwhich satisfies (26) with $\\eta=\\frac{1}{t^{*}}$. We then give an upper bound on $\\eta=\\frac{1}{G}$ :\nClaim C.6. $\\eta=\\frac{1}{t^{*}} \\leq \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})} \\frac{\\delta}{G}$.\nProof. On the one hand,\n\n$$\n\\begin{aligned}\n\\left\\|\\mu_{0}-\\mu^{\\prime}\\right\\| & =\\left\\|\\sum_{s \\in S} \\pi_{s} x_{s}-\\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\tilde{x}_{s, a}\\right\\|=\\left\\|\\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\theta_{s, a}\\left(y_{a}-x_{s}\\right)\\right\\| \\\\\n& \\leq \\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\theta_{s, a}\\left\\|y_{a}-x_{s}\\right\\| \\leq \\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\theta_{s, a} \\operatorname{diam}(\\mathcal{X}) \\stackrel{\\text { Claim C. } 5}{\\leq} \\operatorname{diam}(\\mathcal{X}) \\frac{\\delta}{G}\n\\end{aligned}\n$$\n\nOn the other hand, because $z-\\mu^{\\prime}$ and $\\mu_{0}-\\mu^{\\prime}$ are in the same direction, we have\n\n$$\n\\left\\|z-\\mu^{\\prime}\\right\\|=\\left\\|z-\\mu_{0}\\right\\|+\\left\\|\\mu_{0}-\\mu^{\\prime}\\right\\| \\geq\\left\\|z-\\mu_{0}\\right\\| \\geq \\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})\n$$\n\nbecause $\\mu_{0}$ is in $\\mathcal{C}$ and $z$ is on the boundary of $\\mathcal{X}$. Therefore, $\\eta=\\frac{1}{t^{s}}=\\frac{\\left\\|\\mu_{0}-\\mu^{\\prime}\\right\\|}{\\left\\|z-\\mu^{\\prime}\\right\\|} \\leq \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})} \\frac{\\delta}{G}$.\n\nThe convex combinations (26) (25) define a new principal strategy $\\pi^{\\prime}$ (with $|S| \\times|A|+1$ signals) consisting of $\\tilde{x}_{s, a}$ with probability $(1-\\eta) \\pi_{s} \\rho(a \\mid s)$ and $z$ with probability $\\eta$. Consider the following deterministic agent strategy $\\rho^{\\prime}$ in response to $\\pi^{\\prime}$ : for $\\tilde{x}_{s, a}$, take action $\\rho^{\\prime}\\left(\\tilde{x}_{s, a}\\right)=a$; for $z$, take any action that is optimal for $z$. We note that $\\rho^{\\prime}$ is a best-response to $\\pi^{\\prime}, \\rho^{\\prime} \\in \\mathcal{R}_{0}\\left(\\pi^{\\prime}\\right)$, because, according to Claim C.5, $a$ is an optimal action with respect to $\\tilde{x}_{s, a}$.\n\nThen, consider the principal's utility under $\\pi^{\\prime}$ and $\\rho^{\\prime}$ :\n\n$$\n\\begin{aligned}\nU\\left(\\pi^{\\prime}, \\rho^{\\prime}\\right) & \\stackrel{(26),(25)}{=}(1-\\eta) \\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) u\\left(\\tilde{x}_{s, a}, \\rho^{\\prime}\\left(\\tilde{x}_{s, a}\\right)\\right)+\\eta u\\left(z, \\rho^{\\prime}(z)\\right) \\\\\n& \\geq(1-\\eta) \\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) u\\left(\\tilde{x}_{s, a}, a\\right)-\\eta B \\\\\n& \\geq(1-\\eta) \\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s)\\left(u\\left(x_{s}, a\\right)-L \\quad\\left\\|\\tilde{x}_{s}-x_{s}\\right\\|=\\left\\|\\theta_{s, a}\\left(y_{s}-x_{s}\\right)\\right\\| \\leq \\theta_{s, a} \\operatorname{diam}(\\mathcal{X})\\right)-\\eta B \\\\\n& \\geq(1-\\eta) U(\\pi, \\rho)-L \\operatorname{diam}(\\mathcal{X}) \\sum_{s \\in S} \\sum_{a \\in A} \\pi_{s} \\rho(a \\mid s) \\theta_{s, a}-\\eta B \\\\\n& \\text { (Claim C.5) } \\geq U(\\pi, \\rho)-L \\operatorname{diam}(\\mathcal{X}) \\frac{\\delta}{G}-2 \\eta B \\\\\n& \\text { (Claim C.6) } \\geq U(\\pi, \\rho)-\\left(L \\operatorname{diam}(\\mathcal{X})+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G} \\text {. }\n\\end{aligned}\n$$\n\nRearranging, $U(\\pi, \\rho) \\leq U\\left(\\pi^{\\prime}, \\rho^{\\prime}\\right)+\\left(L \\operatorname{diam}(\\mathcal{X})+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G}$. Note that this argument holds for any pair $(\\pi, \\rho)$ that satisfies $\\rho \\in \\mathcal{R}_{\\delta}(\\pi)$. And recall that $\\rho^{\\prime} \\in \\mathcal{R}_{0}\\left(\\pi^{\\prime}\\right)$. So, we conclude that\n\n$$\n\\begin{aligned}\n\\overline{\\operatorname{OBJ}}^{\\mathcal{R}}(\\delta)=\\max _{\\pi} \\max _{\\rho \\in \\mathcal{R}_{\\delta}(\\pi)} U(\\pi, \\rho) & \\leq \\max _{\\pi^{\\prime}} \\max _{\\rho^{\\prime} \\in \\mathcal{R}_{0}(\\pi)} U\\left(\\pi^{\\prime}, \\rho^{\\prime}\\right)+\\left(L \\operatorname{diam}\\left(\\mathcal{X} ; \\ell_{1}\\right)+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G} \\\\\n& =U^{*}+\\left(L \\operatorname{diam}\\left(\\mathcal{X} ; \\ell_{1}\\right)+2 B \\frac{\\operatorname{diam}(\\mathcal{X})}{\\operatorname{dist}(\\mathcal{C}, \\partial \\mathcal{X})}\\right) \\frac{\\delta}{G}\n\\end{aligned}\n$$\n\nThis proves the case with the constraint $\\sum_{s \\in S} \\pi_{s} x_{s} \\in \\mathcal{C}$.\nThe case without $\\sum_{s \\in S} \\pi_{s} x_{s} \\in \\mathcal{C}$ is proved by letting $\\eta=0$ in the above argument.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 29,
      "text": "# C. 6 Proof of Lemma C. 3 \n\nLet $A_{\\Delta}(x)=\\left\\{a \\in A \\mid v(x, a) \\geq v\\left(x, a^{\\prime}\\right)-\\Delta, \\forall a^{\\prime} \\in A\\right\\}$ be the set of $\\Delta$-optimal actions of the agent in response to principal decision $x \\in \\mathcal{X}$. The proof of Lemma C. 3 uses another lemma that relates the principal utility under a randomized $\\delta$-best-responding agent strategy $\\rho \\in \\mathcal{R}_{\\delta}(\\pi)$ and that under an agent strategy $\\rho^{\\prime}$ that only randomizes over $A_{\\Delta}\\left(x_{s}\\right)$.\n\nLemma C.7. Let $\\pi=\\left\\{\\left(\\pi_{s}, x_{s}\\right)\\right\\}_{s \\in S}$ be a principal strategy and $\\rho \\in \\mathcal{R}_{\\delta}(\\pi)$ be a randomized $\\delta$-bestresponse to $\\pi$. For any $\\Delta>0$, there exists an agent strategy $\\rho^{\\prime}: s \\mapsto \\Delta\\left(A_{\\Delta}\\left(x_{s}\\right)\\right)$ that randomizes over $\\Delta$-optimal actions only for each $x_{s}$, such that the principal's utility under $\\rho^{\\prime}$ and $\\rho$ satisfies: $\\left|U\\left(\\pi, \\rho^{\\prime}\\right)-U(\\pi, \\rho)\\right| \\leq \\frac{2 B \\delta}{\\Delta}$.\n\nProof. Let $a_{s}^{*}=\\max _{a \\in A} v\\left(x_{s}, a\\right)$ be the agent's optimal action for $x_{s}$. Let $\\overline{A_{\\Delta}\\left(x_{s}\\right)}=A \\backslash A_{\\Delta}\\left(x_{s}\\right)$ be the set of actions that are not $\\Delta$-optimal for $x_{s}$. By the definition that $\\rho \\in \\mathcal{R}_{\\delta}(\\pi)$ is a $\\delta$-bestresponse to $\\pi$, we have\n\n$$\n\\begin{aligned}\n\\delta & \\geq \\sum_{s \\in S} \\pi_{s}\\left[v\\left(x_{s}, a_{s}^{*}\\right)-\\sum_{a \\in A} \\rho(a \\mid s) v\\left(x_{s}, a\\right)\\right] \\\\\n& =\\sum_{s \\in S} \\pi_{s}\\left(\\sum_{a \\in A_{\\Delta}\\left(x_{s}\\right)} \\rho(a \\mid s)\\left[\\underbrace{v\\left(x_{s}, a_{s}^{*}\\right)-v\\left(x_{s}, a\\right)}_{\\geq 0}\\right]+\\sum_{a \\in A_{\\Delta}\\left(x_{s}\\right)} \\rho(a \\mid s)\\left[\\underbrace{v\\left(x_{s}, a_{s}^{*}\\right)-v\\left(x_{s}, a\\right)}_{>\\Delta}\\right]\\right) \\\\\n& \\geq 0+\\Delta \\sum_{s \\in S} \\pi_{s} \\sum_{a \\in A_{\\Delta}\\left(x_{s}\\right)} \\rho(a \\mid s) \\\\\n& =\\Delta \\sum_{s \\in S} \\pi_{s} \\rho\\left(\\overline{A_{\\Delta}\\left(x_{s}\\right)} \\mid s\\right)\n\\end{aligned}\n$$\n\nRearranging,\n\n$$\n\\sum_{s \\in S} \\pi_{s} \\rho\\left(\\overline{A_{\\Delta}\\left(x_{s}\\right)} \\mid s\\right) \\leq \\frac{\\delta}{\\Delta}\n$$\n\nThen, we consider the randomized strategy $\\rho^{\\prime}$ that, for each $s$, chooses each action $a \\in A_{\\Delta}\\left(x_{s}\\right)$ with the conditional probability that $\\rho$ chooses $a$ given $a \\in A_{\\Delta}\\left(x_{s}\\right)$ :\n\n$$\n\\rho^{\\prime}(a \\mid s)=\\frac{\\rho(a \\mid s)}{\\rho\\left(A_{\\Delta}\\left(x_{s}\\right) \\mid s\\right)}\n$$\n\nThe sender's utility under $\\rho^{\\prime}$ is:\n\n$$\nU\\left(\\pi, \\rho^{\\prime}\\right)=\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in A_{\\Delta}\\left(x_{s}\\right)} \\frac{\\rho(a \\mid s)}{\\rho\\left(A_{\\Delta}\\left(x_{s}\\right) \\mid s\\right)} u\\left(x_{s}, a\\right)\n$$\n\nThe sender's utility under $\\rho$ is\n\n$$\nU(\\pi, \\rho)=\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in A_{\\Delta}\\left(x_{s}\\right)} \\rho(a \\mid s) u\\left(x_{s}, a\\right)+\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in \\overline{A_{\\Delta}\\left(x_{s}\\right)}} \\rho(a \\mid s) u\\left(x_{s}, a\\right)\n$$\n\nTaking the difference between the two utilities, we get\n\n$$\n\\begin{aligned}\n& \\left|U\\left(\\pi, \\rho^{\\prime}\\right)-U(\\pi, \\rho)\\right| \\\\\n& \\leq\\left|\\sum_{s \\in S} \\pi_{s}\\left(\\frac{1}{\\rho\\left(A_{\\Delta}\\left(x_{s}\\right) \\mid s\\right)}-1\\right) \\sum_{a \\in A_{\\Delta}\\left(x_{s}\\right)} \\rho(a \\mid s) u\\left(x_{s}, a\\right)\\right|+\\left|\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in \\overline{A_{\\Delta}\\left(x_{s}\\right)}} \\rho(a \\mid s) u\\left(x_{s}, a\\right)\\right| \\\\\n& =\\left|\\sum_{s \\in S} \\pi_{s} \\frac{1-\\rho\\left(A_{\\Delta}\\left(x_{s}\\right) \\mid s\\right)}{\\rho\\left(A_{\\Delta}\\left(x_{s}\\right) \\mid s\\right)} \\sum_{a \\in A_{\\Delta}\\left(x_{s}\\right)} \\rho(a \\mid s) u\\left(x_{s}, a\\right)\\right|+\\left|\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in \\overline{A_{\\Delta}\\left(x_{s}\\right)}} \\rho(a \\mid s) u\\left(x_{s}, a\\right)\\right| \\\\\n& \\leq \\sum_{s \\in S} \\pi_{s} \\frac{1-\\rho\\left(A_{\\Delta}\\left(x_{s}\\right) \\mid s\\right)}{\\rho\\left(A_{\\Delta}\\left(x_{s}\\right) \\mid s\\right)} \\sum_{a \\in A_{\\Delta}\\left(x_{s}\\right)} \\rho(a \\mid s) \\cdot B+\\sum_{s \\in S} \\pi_{s} \\sum_{a \\in \\overline{A_{\\Delta}\\left(\\mu_{s}\\right)}} \\rho(a \\mid s) \\cdot B \\\\\n& =B \\sum_{s \\in S} \\pi_{s} \\frac{\\rho\\left(\\overline{A_{\\Delta}\\left(x_{s}\\right)} \\mid s\\right)}{\\rho\\left(A_{\\Delta}\\left(x_{s}\\right) \\mid s\\right)} \\rho\\left(A_{\\Delta}\\left(x_{s}\\right) \\mid s\\right)+B \\sum_{s \\in S} \\pi_{s} \\rho\\left(\\overline{A_{\\Delta}\\left(x_{s}\\right)} \\mid s\\right) \\\\\n& =2 B \\sum_{s \\in S} \\pi_{s} \\rho\\left(\\overline{A_{\\Delta}\\left(x_{s}\\right)} \\mid s\\right) \\stackrel{(27)}{\\leq} \\frac{2 B \\delta}{\\Delta}\n\\end{aligned}\n$$\n\nThis proves the lemma.\n\nWe now prove Lemma C.3.\nProof of Lemma C.3. Consider the objective $\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta)=\\sup _{\\pi} \\min _{\\rho \\in \\mathcal{R}_{\\delta}(\\pi)} U(\\pi, \\rho)$. By Lemma C.7, for any $(\\pi, \\rho)$ there exists an agent strategy $\\rho^{\\prime}: s \\mapsto \\Delta\\left(A_{\\Delta}\\left(x_{s}\\right)\\right)$ that only randomizes over $\\Delta$ optimal actions such that $\\left|U\\left(\\pi, \\rho^{\\prime}\\right)-U(\\pi, \\rho)\\right| \\leq \\frac{2 H \\delta}{\\Delta}$. Because minimizing over $\\Delta\\left(A_{\\Delta}\\left(x_{s}\\right)\\right)$ is equivalent to minimizing over $A_{\\Delta}\\left(x_{s}\\right)$, which corresponds to deterministic $\\Delta$-best-responding strategies, we get:\n\n$$\n\\begin{aligned}\n\\underline{\\mathrm{OBJ}}^{\\mathcal{R}}(\\delta)=\\sup _{\\pi} \\min _{\\rho \\in \\mathcal{R}_{\\delta}(\\pi)} U(\\pi, \\rho) & \\geq \\sup _{\\pi} \\min _{\\rho^{\\prime}: s \\mapsto \\Delta\\left(A_{\\Delta}\\left(x_{s}\\right)\\right)} U\\left(\\pi, \\rho^{\\prime}\\right)-\\frac{2 H \\delta}{\\Delta} \\\\\n& =\\sup _{\\pi} \\min _{\\rho^{\\prime}: s \\mapsto A_{\\Delta}\\left(x_{s}\\right)} U\\left(\\pi, \\rho^{\\prime}\\right)-\\frac{2 H \\delta}{\\Delta} \\\\\n& =\\underline{\\mathrm{OBJ}}^{\\mathcal{D}}(\\Delta)-\\frac{2 H \\delta}{\\Delta}\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2402.09721v6",
  "authors": [
    "Tao Lin",
    "Yiling Chen"
  ],
  "categories": [
    "cs.GT",
    "cs.AI",
    "cs.LG",
    "econ.TH"
  ],
  "abstract": "Classic principal-agent problems such as Stackelberg games, contract design,\nand Bayesian persuasion, often assume that the agent is able to best respond to\nthe principal's committed strategy. We study repeated generalized\nprincipal-agent problems under the assumption that the principal does not have\ncommitment power and the agent uses algorithms to learn to respond to the\nprincipal. We reduce this problem to a one-shot generalized principal-agent\nproblem where the agent approximately best responds. Using this reduction, we\nshow that: (1) If the agent uses contextual no-regret learning algorithms with\nregret $\\mathrm{Reg}(T)$, then the principal can guarantee utility at least\n$U^* - \\Theta\\big(\\sqrt{\\tfrac{\\mathrm{Reg}(T)}{T}}\\big)$, where $U^*$ is the\nprincipal's optimal utility in the classic model with a best-responding agent.\n(2) If the agent uses contextual no-swap-regret learning algorithms with\nswap-regret $\\mathrm{SReg}(T)$, then the principal cannot obtain utility more\nthan $U^* + O(\\frac{\\mathrm{SReg(T)}}{T})$. But (3) if the agent uses\nmean-based learning algorithms (which can be no-regret but not no-swap-regret),\nthen the principal can sometimes do significantly better than $U^*$. These\nresults not only refine previous results in Stackelberg games and contract\ndesign, but also lead to new results for Bayesian persuasion with a learning\nagent and all generalized principal-agent problems where the agent does not\nhave private information.",
  "updated": "2025-02-22T06:58:43Z",
  "published": "2024-02-15T05:30:47Z"
}