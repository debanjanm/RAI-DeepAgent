{"title": "On Approximability of $\\ell_2^2$ Min-Sum Clustering", "sections": [{"section_id": 0, "text": "## Abstract\n\nThe $\\ell_{2}^{2}$ min-sum $k$-clustering problem is to partition an input set into clusters $C_{1}, \\ldots, C_{k}$ to minimize $\\sum_{i=1}^{k} \\sum_{p, q \\in C_{i}}\\|p-q\\|_{2}^{2}$. The objective is a density-based clustering and can be more effective than the traditional centroid-based clustering like $k$-median and $k$-means in capturing complex structures in data that may not be linearly separable, such as when the clusters have irregular, non-convex shapes or are overlapping. Although $\\ell_{2}^{2}$ min-sum $k$-clustering is NP-hard, it is not known whether it is NP-hard to approximate $\\ell_{2}^{2}$ min-sum $k$-clustering beyond a certain factor.\n\nIn this paper, we give the first hardness-of-approximation result for the $\\ell_{2}^{2}$ min-sum $k$ clustering problem. We show that it is NP-hard to approximate the objective to a factor better than 1.056 and moreover, assuming a balanced variant of the Johnson Coverage Hypothesis, it is NP-hard to approximate the objective to a factor better than 1.327.\n\nWe then complement our hardness result by giving a nearly linear time parameterized PTAS for $\\ell_{2}^{2}$ min-sum $k$-clustering running in time $O\\left(n^{1+\\sigma(1)} d \\cdot \\exp \\left(\\left(k \\cdot \\varepsilon^{-1}\\right)^{O(1)}\\right)\\right)$, where $d$ is the underlying dimension of the input dataset.\n\nFinally, we consider a learning-augmented setting, where the algorithm has access to an oracle that outputs a label $i \\in[k]$ for input point, thereby implicitly partitioning the input dataset into $k$ clusters that induce an approximately optimal solution, up to some amount of adversarial error $\\alpha \\in\\left[0, \\frac{1}{2}\\right)$. We give a polynomial-time algorithm that outputs a $\\frac{1+\\gamma \\alpha}{(1-\\alpha)^{2}}$-approximation to $\\ell_{2}^{2}$ min-sum $k$-clustering, for a fixed constant $\\gamma>0$. Therefore, our algorithm improves smoothly with the performance of the oracle and can be used to achieve approximation guarantees better than the NP-hard barriers for sufficiently accurate oracles.\n\n[^0]\n[^0]:    *Rutgers University. E-mail: karthik.cs@rutgers.edu\n    ${ }^{\\dagger}$ University of Michigan. E-mail: euivoong@umich.edu\n    ${ }^{\\ddagger}$ The Hebrew University of Jerusalem. E-mail: yrabani@cs.huji.ac.il\n    ${ }^{\\S}$ Aarhus University. E-mail: cschwiegelshohn@gmail.com\n    ${ }^{\\vee}$ Texas A\\&M University. E-mail: samsonzhou@gmail.com", "tables": {}, "images": {}}, {"section_id": 1, "text": "# 1 Introduction \n\nClustering is a fundamental technique that partitions an input dataset into distinct groups called clusters, which facilitate the identification and subsequent utilization of latent structural properties underlying the dataset. Consequently, various formulations of clustering are used across a wide range of applications, such as computational biology, computer vision, data mining, and machine learning [JMF99, XW05].\n\nIdeally, the elements of each cluster are more similar to each other than to elements in other clusters. To formally capture this notion, a dissimilarity metric is often defined on the set of input elements, so that more closer objects in the metric correspond to more similar objects. Perhaps the most natural goal would be to minimize the intra-cluster dissimilarity in a partitioning of the input dataset. This objective is called the min-sum $k$-clustering problem and has received significant attention due to its intuitive clustering objective [GH98, Ind99, Mat00, Sch00, BCR01, dIVKKR03, CS07, ADHP09, BFSS19, BOR21, CKL21].\n\nIn this paper, we largely focus on the $\\ell_{2}^{2}$ min-sum $k$-clustering formulation. Formally, the input is a set $X$ of $n$ points in $\\mathbb{R}^{d}$ and the goal is to partition $X=C_{1} \\cup \\cdots \\cup C_{k}$ into $k$ clusters to minimize the quantity\n\n$$\n\\min _{C_{1}, \\ldots, C_{k}} \\sum_{i=1}^{k} \\sum_{p, q \\in C_{i}}\\|p-q\\|_{2}^{2}\n$$\n\nwhere $\\|\\cdot\\|_{2}$ denotes the standard Euclidean $\\ell_{2}$ norm.\nWhereas classical centroid-based clustering problems such as $k$-means and $k$-median leverage distances between data points and cluster centroids to identify convex shapes that partition the dataset, min-sum $k$-clustering is a density-based clustering that can handle complex structures in data that may not be linearly separable. In particular, min-sum $k$-clustering can be more effective than traditional centroid-based clustering in scenarios where clusters have irregular, non-convex shapes or overlapping clusters. A simple example of the ability of min-sum clustering to capture more natural structure is an input that consists of two concentric dense rings of points in the plane. Whereas min-sum clustering can partition the points into the separate rings, centroid-based clustering will instead create a separating hyperplane between these points, thereby \"incorrectly\" grouping together points of different rings. See Figure 1 for an example of the ability of min-sum clustering to capture natural structure in cases where centroid-based clustering fails.\n\nMoreover, min-sum clustering satisfies Kleinberg's consistency axiom [Kle02], which informally demands that the optimal clustering for a particular objective should be preserved when distances between points inside a cluster are shrunk and distances between points in different clusters are expanded. By contrast, many centroid-based clustering objectives, including $k$-means and $k$-median, do not satisfy Kleinberg's consistency axiom [MNV12].\n\nOn the other hand, theoretical understanding of density-based clustering objectives such as min-sum $k$-clustering is far less developed than that of their centroid-based counterparts. It can be shown that min-sum $k$-clustering with the $\\ell_{2}^{2}$ cost function is NP-hard, using arguments from [ADHP09]. The problem is NP-hard even for $k=2$ [dIVK01] in the metric case, where the only available information about the points is their pairwise dissimilarity, c.f., Section 1.3 for a summary of additional related work. In fact, for general $k$ in the metric case, it is NP-hard to approximate the problem within a 1.415-multiplicative factor [GI03, CKL21]. However, no such hardness of approximation is known for the Euclidean case, i.e., $\\ell_{2}^{2}$ min-sum, where the selected cost function is\n\n![img-0.jpeg](img-0.jpeg)\n\nFig. 1: Clustering of input dataset in Figure 1a with $k=2$. Figure 1b is an optimal centroid-based clustering, e.g., $k$-median or $k$-means, while the more natural clustering in Figure 1c is an optimal density-based clustering, e.g., $\\ell_{2}$ min-sum $k$-clustering.\nbased on the geometry of the underlying space; the only known lower bound is the NP-hardness of the problem [ADHP09, BOR21, AKP24]. Thus a fundamental open question is:\n\nQuestion 1.1. Is $\\ell_{2}^{2}$ min-sum $k$-clustering APX-hard? That is, does there exist a natural hardness-ofapproximation barrier for polynomial time algorithms?\n\nDue to existing APX-hardness results for centroid-based clustering such as $k$-means and $k$ median [LSW17, CK19, CKL22], it is widely believed that $\\ell_{2}^{2}$ min-sum clustering is indeed APX-hard. Thus, there has been a line of work preemptively seeking to overcome such limitations. Indeed, on the positive side, [IKI94] first showed that min-sum $k$-clustering in the $d$-dimensional $\\ell_{2}^{2}$ case can be solved in polynomial time if both $d$ and $k$ are constants. For general graphs and fixed constant $k$, [GH98] gave a 2-approximation algorithm using runtime $n^{\\mathcal{O}(k)}$. The approximation guarantees were improved by a line of work [Ind99, Mat00, Sch00], culminating in polynomial-time approximation schemes by [dIVKKR03] for both the $\\ell_{2}^{2}$ case and the metric case. Without any assumptions on $d$ and $k$, [BCR01] introduced a polynomial algorithm that achieves an $\\mathcal{O}\\left(\\frac{1}{k} \\log ^{1+\\varepsilon} n\\right)$-multiplicative approximation. Therefore, a long-standing direction in the study of $\\ell_{2}^{2}$ min-sum clustering is:\nQuestion 1.2. How can we algorithmically bridge the gap between the NP-hardness of solving the $\\ell_{2}^{2}$ min-sum clustering and the large multiplicative guarantees of existing approximation algorithms?\n\nA standard approach to circumvent poor dependencies on the size of the input dataset is to sparsify the problem. Informally, we would like to reduce the search space by considering fewer candidate solutions and reduce the dependency on the number of input points by aggregating them. For min-sum clustering this is a particular challenge, as a candidate solution is a partition and the cost of that partition depends on all pairwise distances between all the points. While sparsification algorithms exist for graph clustering [JLS23, Lee23] and $k$-means clustering [CSS21, CLS+22], where the output is typically called a coreset, similar constructions are not known to exist for min-sum clustering.\n\nAnother standard approach to overcome limitations inherent in worst-case impossibility barriers is to consider beyond worst case analysis. To that end, recent works have observed that in many applications, auxiliary information is often available and can potentially form the foundation upon which machine learning models are built. For example, previous datasets with potentially similar behavior can be used as training data for models to label future datasets. However, these\n\nheuristics lack provable guarantees and can produce embarrassingly inaccurate predictions when generalizing to unfamiliar inputs [SZS ${ }^{+} 14$ ]. Nevertheless, learning-augmented algorithms [MV20] have been shown to achieved both good algorithmic performance when the oracle is accurate, i.e., consistency, and standard algorithmic performance when the oracle is inaccurate, i.e., robustness for a wide range of settings, such as data structure design [ $\\mathrm{KBC}^{+} 18$, Mit18, LLW22], algorithms with faster runtime [DIL ${ }^{+} 21$, CSVZ22, DMVW23], online algorithms with better competitive ratio [PSK18, GP19, LLMV20, WLW20, WZ20, BMS20, IKQP21, LV21, ACI22, AGKP22, APT22, GLS ${ }^{+} 22$, KBTV22, JLL ${ }^{+} 22$, ACE ${ }^{+} 23$, SLLA23], and streaming algorithms that are more space-efficient [HIKV19, IVY19, JLL ${ }^{+} 20$, CIW22, CEI ${ }^{+} 22$, LLL ${ }^{+} 23$ ]. In particular, [EFS ${ }^{+} 22$, NCN23] introduce algorithms for $k$-means and $k$-median clustering that can achieve approximation guarantees beyond the known APX-hardness limits.", "tables": {}, "images": {"img-0.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 2, "text": "# 1.1 Our Contributions \n\nIn this paper, we perform a comprehensive study on the approximability of the $\\ell_{2}^{2}$ min-sum $k$-clustering by answering Question 1.1 and Question 1.2.\n\nHardness-of-approximation of min-sum $k$-clustering. We first answer Question 1.1 in the affirmative, by not only showing that the $\\ell_{2}^{2}$ min-sum $k$-clustering is APX-hard but further giving an explicit constant NP-hardness of approximation result for the problem.\nTheorem 1.3 (Hardness of approximation of $\\ell_{2}^{2}$ min-sum $k$-clustering). It is NP-hard to approximate $\\ell_{2}^{2}$ min-sum $k$-clustering to a factor better than 1.056. Moreover, assuming the Dense and Balanced Johnson Coverage Hypothesis (Balanced - JCH*), we have that the $\\ell_{2}^{2}$ min-sum $k$-clustering is NP-hard to approximate to a factor better than 1.327 .\n\nWe remark that Balanced $-\\mathrm{JCH}^{*}$ in the theorem statement above is simply a balanced formulation of the recently introduced Johnson Coverage Hypothesis [CKL22].\n\nFast polynomial-time approximation scheme. In light of Theorem 1.3, a natural question would be to closely examine alternative conditions in which we can achieve a $(1+\\varepsilon)$-approximation to min-sum $k$-clustering, i.e., Question 1.2. To that end, there are a number of existing polynomial-time approximation schemes (PTAS) [Ind99, Mat00, Sch00, dlVKKR03], the best of which uses runtime $n^{O\\left(k / \\varepsilon^{2}\\right)}$ for the $\\ell_{2}^{2}$ case. However, as noted by [CS07], even algorithms with runtime quadratic in the size $n$ of the input dataset are generally not sufficiently scalable to handle large datasets. In this paper, we present an algorithm with a running time that is nearly nearly linear. Specifically, we show\n\nTheorem 1.4. There exists an algorithm running in time\n\n$$\nO\\left(n^{1+o(1)} d \\cdot 2^{\\eta \\cdot k^{2} \\cdot \\varepsilon^{-12} \\log ^{2}(k /(\\varepsilon \\delta))}\\right)\n$$\n\nfor some absolute constant $\\eta$, that computes a $(1+\\varepsilon)$-approximate solution to $\\ell_{2}^{2} k$-MinSum Clustering with probability $1-\\delta$.\n\nWe again emphasize that the runtime of 1.4 is linear in the size $n$ of the input dataset, though it has exponential dependencies in both the number $k$ of clusters and the approximation parameter $\\varepsilon>0$. By contrast, the best previous PTAS uses runtime $n^{O\\left(k / \\varepsilon^{2}\\right)}$ [CS07], which has substantially worse dependency on the size $n$ of the input dataset.\n\nLearning-augmented algorithms. Unfortunately, exponential dependencies on the number $k$ of clusters can still be prohibitive for moderate values of $k$. To that end, we turn our attention to learning-augmented papers. We consider the standard label oracle model for clustering, where the algorithm has access to an oracle that provides a label for each input point. Formally, for each point $x$ of the $n$ input points, the oracle outputs a label $i \\in[k]$ for $x$, so that the labels implicitly partition the input dataset into $k$ clusters that induce an approximately optimal solution. However, the oracle also has some amount of adversarial error that respects the precision and recall of each cluster; we defer the formal definition to Definition 4.3.\n\nOne of the reasons label oracles have been used for learning-augmented algorithms for clustering is their relative ease of acquisition via machine learning models that are trained on a similar distribution of data. For example, a smaller separate dataset can be observed and used as a \"training\" data, an input to some heuristic to cluster the initial data, which we can then use to form a predictor for the actual input dataset. Indeed, implementations of label oracles have been shown to perform well in practice [EFS+22, NCN23].\n![img-1.jpeg](img-1.jpeg)\n\nFig. 2: Note that with arbitrarily small error rate, i.e., $\\frac{1}{n}$, a single mislabeled point among the $n$ input points causes the resulting clustering to be arbitrarily bad for $\\Delta \\gg n^{2} \\cdot R$.\n\nWe also remark that perhaps counter-intuitively, a label oracle with arbitrarily high accuracy does not trivialize the problem. In particular, the na\u00efve algorithm of outputting the clustering induced by the labels does not work. As a simple example, consider an input dataset where half of the $n$ points are at $x=0$ and the other half of the points are at $x=1$. Then for $k=2$, the clear optimal clustering is to cluster the points at the origin together, and cluster the points at $x=1$ together, which induces the optimal cost of zero. However, if even one of the $n$ points is incorrect, then the clustering output by the labels has cost at least 1. Therefore, even with error rate as small as $\\frac{1}{n}$, the multiplicative approximation of the na\u00efve algorithm can be arbitrarily bad. See Figure 2 for an illustration of this example. Of course, this example does not rule out more complex algorithms that combines the labels with structural properties of optimal clustering and indeed, our algorithm utilizes such properties.\n\nWe give a polynomial-time algorithm for the $\\ell_{2}^{2}$ min-sum $k$-clustering that can provide guarantees beyond the computational limits of Theorem 1.3, given a sufficiently accurate oracle.\nTheorem 1.5. There exists a polynomial-time algorithm that uses a label predictor with error rate $\\alpha \\in\\left[0, \\frac{1}{2}\\right)$ and outputs a $\\frac{1+\\gamma \\alpha}{(1-\\alpha)^{2}}$-approximation to the $\\ell_{2}^{2}$ min-sum $k$-clustering problem, where $\\gamma=7.7$ for $\\alpha \\in\\left[0, \\frac{1}{7}\\right)$ or $\\gamma=\\frac{5 \\alpha-2 \\alpha^{2}}{(1-2 \\alpha)(1-\\alpha)}$ for $\\alpha \\in\\left[0, \\frac{1}{2}\\right)$.\n\nWe remark that Theorem 1.5 does not require the true error rate $\\alpha$ as an input parameter. Because we are in an offline setting, where can run Theorem 1.5 multiple times with guesses for the true error rate $\\alpha$, in decreasing powers of $\\frac{1}{\\lambda}$ for any constant $\\lambda>1$. We can then compare the resulting clustering output by each guess for $\\alpha$ and take the output the best clustering.", "tables": {}, "images": {"img-1.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 3, "text": "# 1.2 Technical Overview \n\nHardness of approximation. Recently, the authors of [CKL22] put forth the Johnson Coverage Hypothesis (JCH) and introduced a framework to obtain (optimal) hardness of approximation results for $k$-median and $k$-means in $\\ell_{p}$-metrics. The proof of Theorem 1.3 builds on this framework.\n\nJCH roughly asserts that for large enough constant $z$, given as input an integer $k$ and a collection of $z$-sets (i.e., sets each of size $z$ ) over some universe, it is NP-hard to distinguish the completeness case where there is a collection $C$ of $k$ many $(z-1)$-sets such that every input set is covered ${ }^{1}$ by some set in $C$, from the soundness case where every collection $C$ of $k$ many $(z-1)$-sets does not cover much more than $1-\\frac{1}{z}$ fraction of the input sets (see Hypothesis 2.2 for a formal statement).\n\nIn this paper, we consider a natural generalization of JCH , called Balanced $-\\mathrm{JCH}^{+}$, where we assume that the number of input sets is \"dense\", i.e., $\\omega(k)$, and more importantly that in the completeness case, the collection $C$ covers the input $z$-sets in a balanced manner, i.e., we can partition the input to $k$ equal parts such that each part is completely covered by a single set in $C$ (see Hypothesis 2.4 for a formal statement).\n\nWe now sketch the proof of Theorem 1.3 assuming Balanced $-\\mathrm{JCH}^{+}$. Given a collection of $m$ many $z$-sets over a universe $[n]$ as input, we create a point for each input set, which is simply the characteristic vector of the set as a subset of $[n]$, i.e., the points are all $n$-dimensional Boolean vectors of Hamming weight $z$.\n\nIn the completeness case, from the guarantees of Balanced $-\\mathrm{JCH}^{+}$, it is easy to see that the points created can be divided into $k$ equal clusters of size $m / k$ such that all the $z$-sets of a cluster are completely covered by a single $(z-1)$-set. This implies that the squared Euclidean distance between a pair of points within a cluster is exactly 2 and thus the $\\ell_{2}^{2}$ min-sum $k$-clustering cost is $k \\cdot 2 \\cdot(m / k)(m / k-1) \\approx 2 m^{2} / k$.\n\nOn the other hand, in the soundness case, we first use the density guarantees of Balanced $-\\mathrm{JCH}^{+}$ to argue that most clusters are not small. Then suppose that we had a low cost $\\ell_{2}^{2}$ min-sum $k$ clustering, we look at a typical cluster and observe that the squared distance of any two points in the cluster must be a positive even integer, and it is exactly 2 only when the two input sets corresponding to the points intersect on a $(z-1)$-set. Thus, if the cost of the clustering is close to $\\alpha \\cdot 2 m^{2} / k$ (for some $\\alpha \\geq 1$ ), then we argue (using convexity) that for a typical cluster that there must be a $(z-1)$-set that covers $\\left(1-\\alpha^{\\prime}\\right) m / k$ many $z$-sets in that cluster, where $\\alpha^{\\prime}$ depends on $\\alpha$. Thus, from this we decode $k$-many $(z-1)$-sets which cover a large fraction of the input $z$-sets.\n\nIn order to obtain the unconditional NP-hardness result, much like in [CKL22], we need to extend the above reduction to a more general problem. This is indeed established in Theorem 2.7, and after this we prove a special case of a generalization of Balanced $-\\mathrm{JCH}^{+}$(when $z=3$ ) which is done in Theorem 2.6 and this involved proving additional properties of the reduction of [CKL22] from the multilayered PCPs of [DGKR05, Kho02] to 3-Hypergraph Vertex Coverage.\n\nNearly Linear Time PTAS. An important feature of $\\ell_{2}^{2}$ Min-Sum Clustering is that we can use assignments of clusters to their mean to obtain the cost of the points in the cluster, an idea previously used in [Ind99, Mat00, Sch00, dIVKKR03]. We show how to reduce the number of candidate means to a constant (depending only on $k$ and $\\varepsilon$. The idea here is to use $D^{2}$ sampling methods akin to $k$-means++ [AV07]. Unfortunately, by itself, it is not sufficient as there may exist clusters that have significant min-sum clustering cost, but are not detectable by $D^{2}$ sampling. To this end, we augment $D^{2}$ sampling via a careful pruning strategy that removes high costing points,\n\n[^0]\n[^0]:    ${ }^{1} \\mathrm{~A}(z-1)$-set covers a $z$-set if the former is a subset of the latter.\n\nincreasing the relative cost of clusters of high density. Thereafter, we show that given sufficiently many samples, we can find a small set of suitable candidate means that are induced by a nearly optimal clustering.\n\nWhat remains to be shown is how to find an assignment of points to these centers with similar cost. For this, we could use a flow-based approach, but this results in a $n^{3}$ running time. Instead, we employ a discretization and bucketing strategy that allows us to sparsify the point set while preserving the min-sum clustering cost, akin to coresets.\n\nLearning-augmented algorithm. Our starting point for our learning-augmented algorithm for min-sum $k$-clustering is the learning-augmented algorithms for $k$-means clustering by [EFS+22, NCN23]. The algorithms note that the $k$-means clustering objective can be decomposed across the points that are given each label $i \\in[k]$. Thus we consider the subset $P_{i}$ of points of the input dataset $X$ that are given label $i$ by the oracle. Since $k$-means clustering objective can be further decomposed along the $d$ dimensions, then the algorithms consider $P_{i}$ along each dimension.\n\nThe cluster $P_{i}$ can have an $\\alpha$ fraction of incorrect points. The main observation is that there can be two cases. Either $P_{i}$ includes a number of \"bad\" points that are far from the true mean and thus easy to identify, or $P_{i}$ includes a number of \"bad\" points that are difficult to identify but also are close to the true mean and thus do not largely affect the overall $k$-means clustering cost. Thus the algorithm simply needs to prune away the points that are far away, which can be achieved by selecting the interval of $(1-\\mathcal{O}(\\alpha))$ points that has the best clustering cost. It is then shown that the resulting centers provide a good approximate solution to the $k$-means clustering cost.\n\nUnfortunately, we cannot immediately utilize the previous approach because min-sum $k$ clustering is a density-based clustering rather than a centroid-based clustering. However, it is known [IKI94] that we can rewrite\n\n$$\n\\sum_{i \\in[k]} \\sum_{p, q \\in C_{i}}\\|p-q\\|_{2}^{2}=\\sum_{i \\in[k]}\\left|C_{i}\\right| \\cdot \\sum_{p \\in C_{i}}\\left\\|p-c_{i}\\right\\|_{2}^{2}\n$$\n\nwhere $c_{i}$ is the centroid of the points in the cluster $C_{i}$ in an approximately optimal clustering $\\mathcal{C}=\\left\\{C_{1}, \\ldots, C_{k}\\right\\}$. We can use the learning-augmented $k$-means clustering algorithm to identify good proxies for each centroid $c_{i}$. Moreover, by our assumptions on the precision and recall of each cluster, we have that $\\left|P_{i}\\right|$ is a good estimate of $\\left|C_{i}\\right|$. Therefore, we have a good approximation of the cost of the optimal min-sum $k$-clustering; it remains to identify the actual clusters.\n\nIn standard centroid-based clustering, each point is assigned to its closest center. However, this is not true for min-sum $k$-clustering. Thus, we seek alternative approaches to identifying a set of approximately $\\left|P_{i}\\right|$ to each centroid returned by the learning-augmented $k$-means algorithm. To that end, we define a constrained min-cost flow problem as follows. We create a source node $s$ and a sink node $t$, requiring $n=|X|$ flow from $s$ to $t$. We then create a directed edge from $s$ to each node $u_{x}$ representing a separate $x \\in X$ with capacity 1 and cost 0 . These two gadgets ensure that a unit of flow must be pushed across each node representing a point in the input dataset.\n\nWe also create a directed edge to $t$ from each node $v_{i}$ representing a separate $c_{i}$ with capacity $\\frac{1}{1-\\alpha} \\cdot\\left|P_{i}\\right|$ and cost 0 . For each $x \\in X, i \\in[k]$, create a directed edge from $u_{x}$ to $v_{i}$ with capacity 1 and $\\operatorname{cost} \\frac{1}{1-\\alpha} \\cdot\\left|P_{i}\\right| \\cdot\\left\\|x-c_{i}\\right\\|_{2}^{2}$. These two gadgets ensure that when a flow is pushed across some node to the corresponding node representing a center, then the cost of the flow is almost precisely the cost of assigning a point to the corresponding center toward the min-sum $k$-clustering objective. Finally, we require that at least $(1-\\alpha) \\cdot\\left|P_{i}\\right|$ flow goes through node $v_{i}$ correpsonding to center $c_{i}$. This\n\nensures that the correct number of points is assigned to each center consistent with the precision and recall assumptions.\n\nWe note that the constrained min-cost flow problem can be written as a linear program. Therefore to identify the overall clusters, we run any standard polynomial-time algorithm for solving linear programs [Kar84, Vai89, Vai90, LS15, LSZ19, CLS21, JSWZ21]. It then follows by that wellknown integrality theorems for min-cost flow, the resulting solution is integral and thus provides a valid clustering with approximately optimal $\\ell_{2}^{2}$ min-sum $k$-clustering objective.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 1.3 Related Works \n\nThe min-sum $k$-clustering problem was first introduced for general graphs by [SG76]. The problem is complement of the max $k$-cut problem, in which the goal is to partition the vertices of an input graph into $k$ subsets as to maximize the number or weight of the edges crossing any pair of subsets, c.f., [PY91]. [GH98] showed that the $\\ell_{2}$ min-sum $k$-clustering problem is also closely related to the balanced $k$-median problem, in which the goal is to identify $k$ centers $c_{1}, \\ldots, c_{k}$ and partition the input dataset $X$ into clusters $C_{1}, \\ldots, C_{k}$ to minimize $\\sum_{i=1}^{k}\\left|C_{i}\\right| \\sum_{x \\in X}\\left\\|x-c_{i}\\right\\|_{2}$. In particular, [GH98] showed that an $\\alpha$-approximation to balanced $k$-median yields a $2 \\alpha$-approximation to min-sum $k$-clustering. [GH98] then showed that balanced $k$-median can be solved in time $n^{\\mathcal{O}(k)}$ by guessing the cluster centers and sizes, and then subsequently determining the assignment between the input points and the centers, which also results in a 2-approximation for min-sum $k$-clustering in $n^{\\mathcal{O}(k)}$ time. For the structurally different $\\ell_{2}$ min-sum $k$-clustering problem, [BFSS19] achieved a polynomial-time algorithm that achieves the best known approximation of $\\mathcal{O}(\\log n)$, by considering the embedding of metric spaces into hierarchically separated trees using dynamic programming. However, these techniques do not immediately translate into a good approximation for $\\ell_{2}^{2}$ min-sum $k$-clustering. Even more recently, [NRS24] provided a QPTAS in metrics induced by graphs of bounded treewidth, and graphs of bounded doubling dimension.\n\nFor the prize-collecting version of $\\ell_{2}$ min-sum $k$-clustering, [HO10] gave a 2-approximation algorithm in the metric setting that uses polynomial time for fixed constant $k$. In a separate line of work, [BB09, BBG09] address conditions under which the clustering would be stable. Namely for the metric case and small $k$, they compute a clustering that is to the optimal $\\ell_{2}$ min-sum $k$-clustering in the sense that most of the labels are correct, though the objective value may not be close to the optimal value.\n\nOn the lower bound side, [GH98] showed that the general min-sum $k$-clustering problem is NP-hard, while [ADHP09] showed that even the $\\ell_{2}^{2}$ min-sum $k$-clustering problem is NP-hard even when $k=2$. [KKLP97] first showed that it is NP-hard to approximate non-metric min-sum $k$-clustering within a multiplicative $\\mathcal{O}\\left(n^{2-\\varepsilon}\\right)$-factor for any $\\varepsilon>0$ and $k>3$. Recently, [CKL21] showed that for metric min-sum $k$-clustering, it is NP-hard to approximate within a multiplicative 1.415-factor. However, prior to this work, no such hardness-of-approximation was known for the $\\ell_{2}^{2}$ min-sum $k$-clustering problem.\n\nA popular way of obtaining polynomial time approximation schemes are coresets, which are succinct summaries of a data set with respect to a given clustering objective. For $\\ell_{2}^{2}$ minsum clustering, the most closesly related construction is the classic $k$-means problem, as well as variants such as non-uniform $k$-clustering. Following a long line of work [BBC ${ }^{+} 19$, FSS20, HV20, CWZ23, WZZ23], a $k$-means coreset in Euclidean space of size $\\tilde{O}\\left(\\frac{k}{\\varepsilon^{3}} \\cdot \\min \\left(\\frac{1}{\\varepsilon^{k}}, \\sqrt{k}\\right)\\right)$ is known to exist [CSS21, CLS ${ }^{+} 22, \\mathrm{BCP}^{+} 24$ ], which was surprisingly shown to be optimal [HLW24, ZTHH24].\n\nFor non-uniform clustering, centers are associated with weights and the clustering cost is $\\sum_{i=1}^{k} \\sum_{p \\in C_{i}} w_{c_{i}}\\left\\|p-c_{i}\\right\\|^{2}$, where $c_{i}$ is the center associated with the cluster $C_{i}$ and $w_{c_{i}}$ denotes its weight. Min-sum clustering is a related problem where the weight is not arbitrary, but chosen to be equal to $\\left|C_{i}\\right|$. Unfortunately, the only known coreset constructions for the weighted $k$-means problem [FS12] only apply to the line metric and even in this case have size at least $(\\log n)^{k}$. Nevertheless, coreset based approaches have been successfully used to obtain fast algorithms with additive errors in general metric spaces, see [CS07]. It is unclear if these ideas can improve algorithms for $\\ell_{2}^{2}$ min-sum clustering, even when using additive errors.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 1.4 Preliminaries \n\nWe use the notation $[n]$ to denote the set $\\{1,2, \\ldots, n\\}$ for an integer $n>0$. For a set $X$, we use the notation $X=A \\dot{\\cup} B$ to denote that $A$ and $B$ partition $X$, i.e., $A \\cup B=X$ and $A \\cap B=\\varnothing$. For a matrix $A \\in \\mathbb{R}^{n \\times d}$, we define its Frobenius norm as\n\n$$\n\\|A\\|_{F}:=\\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{d} A_{i, j}^{2}}\n$$\n\nWe use $\\operatorname{poly}(n)$ to denote a fixed polynomial in $n$ whose degree can be determined by setting appropriate constants in the algorithms or proofs. We use $\\operatorname{polylog}(n)$ to denote poly $(\\log n)$. For a function $f(\\cdot, \\ldots, \\cdot)$, we use the notation $\\tilde{\\mathcal{O}}(f)$ to denote $f \\cdot \\operatorname{polylog}(f)$.\n$k$-means clustering. In the Euclidean $k$-means clustering problem, the input is a dataset $X \\subset \\mathbb{R}^{d}$ and the goal is to partition $X$ into clusters $C_{1}, \\ldots, C_{k}$ by assigning a centroid $c_{i}$ to each cluster $C_{i}$ as to minimize the objective\n\n$$\n\\min _{c_{1}, \\ldots, c_{k}} \\sum_{x \\in X} \\min _{i \\in[k]}\\left\\|x-c_{i}\\right\\|_{2}^{2}\n$$", "tables": {}, "images": {}}, {"section_id": 6, "text": "## 2 Hardness of Approximation of $\\ell_{2}^{2}$ Min-Sum $k$-Clustering\n\nIn this section, we show the hardness of approximation of $\\ell_{2}^{2}$ min-sum $k$-clustering, i.e., Theorem 1.3. We first define the relevant formulations of Johnson Coverage Hypothesis in Section 2.1. Next, in Section 2.2 we provide the main reduction from the Johnson coverage problem to the $\\ell_{2}^{2}$ minsum $k$-clustering problem. Finally, in Section 2.3 we prove a special case of a generalization of Balanced $-\\mathrm{JCH}^{*}$ which yields the unconditional NP-hardness factor claimed in Theorem 1.3.", "tables": {}, "images": {}}, {"section_id": 7, "text": "### 2.1 Johnson Coverage Hypothesis\n\nIn this section, we recall the Johnson Coverage problem, followed by the Johnson Coverage hypothesis [CKL22].\n\nLet $n, z, y \\in \\mathbb{N}$ such that $n \\geq z>y$. Let $E \\subseteq\\binom{[n]}{z}$ and $S \\in\\binom{[n]}{y}$. We define the coverage of $S$ w.r.t. $E$, denoted by $\\operatorname{cov}(S, E)$ as follows:\n\n$$\n\\operatorname{cov}(S, E)=\\{T \\in E \\mid S \\subset T\\}\n$$\n\nDefinition 2.1 (Johnson Coverage Problem). In the $(\\alpha, z, y)$-Johnson Coverage problem with $z>y \\geq 1$, we are given a universe $U:=[n]$, a collection of subsets of $U$, denoted by $E \\subseteq\\binom{[n]}{z}$, and a parameter $k$ as input. We would like to distinguish between the following two cases:\n\n- Completeness: There exists $\\mathcal{C}:=\\left\\{S_{1}, \\ldots, S_{k}\\right\\} \\subseteq\\binom{[n]}{y}$ such that\n\n$$\n\\operatorname{cov}(\\mathcal{C}):=\\underset{i \\in[k]}{\\cup} \\operatorname{cov}\\left(S_{i}, E\\right)=E\n$$\n\n- Soundness: For every $\\mathcal{C}:=\\left\\{S_{1}, \\ldots, S_{k}\\right\\} \\subseteq\\binom{[n]}{y}$ we have $|\\operatorname{cov}(\\mathcal{C})| \\leq \\alpha \\cdot|E|$.\n\nWe call $(\\alpha, z, z-1)$-Johnson Coverage as $(\\alpha, z)$-Johnson Coverage.\nNotice that $(\\alpha, 2)$-Johnson Coverage Problem is simply the well-studied vertex coverage problem (with gap $\\alpha$ ). Also, notice that if instead of picking the collection $\\mathcal{C}$ from $\\binom{[n]}{y}$, we replace it with picking the collection $\\mathcal{C}$ from $\\binom{[n]}{1}$ with a similar notion of coverage, then we simply obtain the Hypergraph Vertex Coverage problem (which is equivalent to the Max $k$-Coverage problem for unbounded $z$ ). In Figure 3 we provide a few examples of instances of the Johnson coverage problem.\n![img-2.jpeg](img-2.jpeg)\n(a)\n![img-3.jpeg](img-3.jpeg)\n(b)\n![img-4.jpeg](img-4.jpeg)\n(c)\n\nFig. 3: Examples of input instances of the Johnson Coverage Hypothesis for $k=2$. Figure 3a shows an example of a completeness instance of $(0.7,2,1)$, since all subsets of size 2 , i.e., all edges, can be covered by $k=2$ choices of subset of size 1 , i.e., two vertices. Figure 3 b shows an example of a completeness instance of $(0.7,3,1)$, since all subsets of size 3 can be covered by $k=2$ vertices. Figure 3c shows an example of a soundness instance of $(0.7,3,2)$, since at most $2 \\leq 0.7 \\cdot 4$ subsets of size 3 can be covered by any choice of $k=2$ edges.\n\nWe now state the following hypothesis.\nHypothesis 2.2 (Johnson Coverage Hypothesis (JCH) [CKL22]). For every constant $\\varepsilon>0$, there exists a constant $z:=z(\\varepsilon) \\in \\mathbb{N}$ such that deciding the $\\left(1-\\frac{1}{\\varepsilon}+\\varepsilon, z\\right)$-Johnson Coverage Problem is NP-Hard.\n\nNote that since Vertex Coverage problem is a special case of the Johnson Coverage problem, we have that the NP-Hardness of $(\\alpha, z)$-Johnson Coverage problem is already known for $\\alpha=0.944$ [AKS11] (under unique games conjecture).\n\nOn the other hand, if we replace picking the collection $\\mathcal{C}$ from $\\binom{[n]}{z-1}$ by picking from $\\binom{[n]}{1}$, then for the Hypergraph Vertex Coverage problem, we do know that for every $\\varepsilon>0$ there is some constant $z$ such that the Hypergraph Vertex Coverage problem is NP-Hard to decide for a factor of $\\left(1-\\frac{1}{\\varepsilon}+\\varepsilon\\right)$ [Fei98].\n\nFor continuous clustering objectives, a dense version of JCH is sometimes needed to prove inapproximability results (see [CKL22] for a discussion on this). Thus, we state:\n\nHypothesis 2.3 (Dense Johnson Coverage Hypothesis ( $\\mathrm{JCH}^{*}$ ) [CKL22]). JCH holds for instances $(U, E, k)$ of Johnson Coverage problem where $|E|=\\omega(k)$.\n\nMore generally, let $(\\alpha, z, y)$-Johnson Coverage* problem be the special case of the $(\\alpha, z, y)$ Johnson Coverage problem where the instances satisfy $|E|=\\omega\\left(k \\cdot|U|^{z-y-1}\\right)$. Then $\\mathrm{JCH}^{*}$ states that for any $\\varepsilon>0$, there exists $z=z(\\varepsilon)$ such that $(1-1 / e+\\varepsilon, z, z-1)$-Johnson Coverage* is NP-Hard. This additional property has always been obtained in literature by looking at the hard instances that were constructed. In [CK19], where the authors proved the previous best inapproximability results for continuous case $k$-means and $k$-median, it was observed that hard instances of $(0.94,2,1)$ Johnson Coverage constructed in [AKS11] can be made to satisfy the above property.\n\nNow we are ready to define the variant of JCH needed for proving inapproximability of $\\ell_{2}^{2}$ min-sum $k$-clustering. For any two non-empty finite sets $A, B$, and a constant $\\delta \\in[0,1]$, we say a function $f: A \\rightarrow B$ is $\\delta$-balanced if for all $b \\in B$ we have:\n\n$$\n|\\{a \\in A: f(a)=b\\}| \\leq(1+\\delta) \\cdot \\frac{|A|}{|B|}\n$$\n\nWe then put forth the following hypothesis.\nHypothesis 2.4 (Dense and Balanced Johnson Coverage Hypothesis (Balanced - JCH* )). JCH holds for instances $(U, E, k)$ of Johnson Coverage problem where $|E|=\\omega(k)$ and in the completeness case there exists $\\mathcal{C}:=\\left\\{S_{1}, \\ldots, S_{k}\\right\\} \\subseteq\\binom{[n]}{z-1}$ and a 0 -balanced function $\\psi: E \\rightarrow[k]$ such that for all $T \\in E$ we have $S_{\\psi(T)} \\subset T$.\n\nMore generally, let $(\\alpha, z, y, \\delta)$-Balanced Johnson Coverage* problem be the special case of the $(\\alpha, z, y)$-Johnson Coverage* problem where the instances admit a $\\delta$-balanced function $\\psi: E \\rightarrow[k]$ in the completeness case which partitions $E$ to $k$ parts, say $E_{1} \\dot{\\cup} \\cdots \\dot{\\cup} E_{k}$ such that for all $i \\in[k]$ we have $\\operatorname{cov}\\left(S_{i}, E_{i}\\right)=E_{i}$ and $\\left|E_{i}\\right| \\leq \\frac{|E|}{k} \\cdot(1+\\delta)$. Then Balanced - JCH* states that for any $\\varepsilon>0$, there exists $z=z(\\varepsilon)$ such that $(1-1 / e+\\varepsilon, z, z-1,0)$-Balanced Johnson Coverage* is NP-Hard.\n\nAs with the case of $\\mathrm{JCH}^{*}$, the balanced addition to $\\mathrm{JCH}^{*}$ is also quite natural and candidate constructions typically give this property for free. To support this point, we will prove some special case of this.\n\nIn [CKL22] the authors had proved the following special case of $\\mathrm{JCH}^{*}$.\nTheorem 2.5 ([CKL22]). For any $\\varepsilon>0$, given a simple 3-hypergraph $\\mathcal{H}=(V, H)$ with $n=|V|$, it is NP-hard to distinguish between the following two cases:\n\n- Completeness: There exists $S \\subseteq V$ with $|S|=n / 2$ that intersects every hyperedge.\n- Soundness: Any subset $S \\subseteq V$ with $|S| \\leq n / 2$ intersects at most a $(7 / 8+\\varepsilon)$ fraction of hyperedges.\n\nFurthermore, under randomized reductions, the above hardness holds when $|H|=\\omega\\left(n^{2}\\right)$.\nIn Section 2.3, we further analyze the proof of the above theorem and prove the following:\nTheorem 2.6. Theorem 2.5 holds even with the following additional completeness guarantee for all $\\delta>0$ : there exists $S:=\\left\\{v_{1}, \\ldots, v_{k}\\right\\} \\subseteq V$ and a $\\delta$-balanced function $\\psi: H \\rightarrow[k]$ such that for all $e \\in H$ we have $v_{\\psi(e)} \\in e$.\n\nThis result will be used to prove the unconditional NP-hardness of approximating $\\ell_{2}^{2}$ min-sum $k$-clustering problem.", "tables": {}, "images": {"img-2.jpeg": "data:image/jpeg;base64,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", "img-3.jpeg": "data:image/jpeg;base64,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", "img-4.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 8, "text": "# 2.2 Inapproximability of $\\ell_{2}^{2}$ min-sum $k$-clustering \n\nIn the following subsection, we will prove the below theorem.\nTheorem 2.7. Assume $(\\alpha, z, y, \\delta)$-Balanced Johnson Coverage* is NP-Hard. For every constant $\\varepsilon>0$, given a point-set $P \\subset \\mathbb{R}^{d}$ of size $n$ (and $d=\\mathcal{O}(\\log n)$ ) and a parameter $k$ as input, it is NP-Hard to distinguish between the following two cases:\n\n- Completeness: There exists partition $P_{1}^{*} \\dot{\\cup} \\cdots \\dot{\\cup} P_{k}^{*}:=P$ such that\n\n$$\n\\sum_{i \\in[k]} \\sum_{p, q \\in P_{i}^{*}}\\|p-q\\|_{2}^{2} \\leq(1+3 \\delta) \\cdot(z-y) \\cdot \\rho n^{2} / k\n$$\n\n- Soundness: For every partition $P_{1} \\dot{U} \\cdots \\dot{U} P_{k}:=P$ we have\n\n$$\n\\sum_{i \\in[k]} \\sum_{p, q \\in P_{i}}\\|p-q\\|_{2}^{2} \\geq(1-o(1)) \\cdot(\\alpha \\cdot \\sqrt{z-y}+(1-\\alpha) \\cdot \\sqrt{z-y+1})^{2} \\cdot \\rho n^{2} / k\n$$\n\nfor some constant $\\rho>0$.\nPutting together the above theorem with Theorem 2.6 (i.e., NP-hardness of $(7 / 8+\\varepsilon, 3,1, \\delta)$ Balanced Johnson Coverage* problem for all $\\varepsilon, \\delta>0$ ), we obtain the NP-hardness of approximating $\\ell_{2}^{2}$ min-sum $k$-clustering. The above theorem also immediately yields the hardness of approximating $\\ell_{2}^{2}$ min-sum $k$-clustering under Balanced $-\\mathrm{JCH}^{*}$ (i.e., conditional NP-hardness of $(1-1 / e+\\varepsilon, z, z-$ 1,0)-Balanced Johnson Coverage* problem for all $\\varepsilon>0$ and some $z=z(\\varepsilon) \\in \\mathbb{N}$ ). This completes the proof of Theorem 1.3.", "tables": {}, "images": {}}, {"section_id": 9, "text": "### 2.2.1 Proof of Theorem 2.7\n\nFix $\\varepsilon>0$ as in the theorem statement. Let $\\varepsilon^{\\prime}:=\\varepsilon / 11$. Starting from a hard instance of $(\\alpha, z, y, \\delta)$ Balanced Johnson Coverage* problem $(U, E, k)$ with $|U|=n$ and $|E|=\\omega\\left(n^{z-y}\\right)$,\n\nConstruction. The $\\ell_{2}^{2}$ min-sum $k$-clustering instance consists of the set of points to be clustered $P \\subseteq\\{0,1\\}^{n}$ where for every $T \\in E$ we have the point $p_{t} \\in P$ defined as follows:\n\n$$\np_{T}:=\\sum_{i \\in T} \\vec{e}_{i}\n$$\n\n. From the construction, it follows that for every distinct $T, T^{\\prime} \\in E$, we have:\n\n$$\n\\left\\|p_{T}-p_{T^{\\prime}}\\right\\|_{2}^{2}=2 z-2 \\cdot\\left|T \\cap T^{\\prime}\\right|\n$$\n\nCompleteness. Suppose there exist $S_{1}, \\ldots, S_{k} \\in\\left(\\begin{array}{l}{[n]} \\\\ y\\end{array}\\right)$ and a $\\delta$-balanced function $\\psi: E \\rightarrow[k]$ such that for all $T \\in E$ we have $S_{\\psi(T)} \\subset T$. Then, we define a clustering $C_{1} \\dot{U} \\cdots \\dot{U} C_{k}=P$ as follows: for every $p_{T} \\in P$, we include it in cluster $C_{\\psi(T)}$. We now provide an upper bound on the $\\ell_{2}^{2}$ min-sum cost of clustering $\\mathcal{C}:=\\left\\{C_{1}, \\ldots, C_{k}\\right\\}$. (1) implies that for each $C_{i}$, for any pair $T, T^{\\prime}$ such\n\nthat $p_{T}, p_{T^{\\prime}} \\in C_{i}$, we have that $S_{\\varphi(T)} \\subseteq T \\cap T^{\\prime}$ and thus $\\left\\|p_{T}-p_{T^{\\prime}}\\right\\|_{2}^{2} \\leq 2 z-2 y$. Thus, the cost of clustering $\\mathcal{C}$ is bounded as follows:\n\n$$\n\\sum_{i \\in[k]} \\sum_{p, q \\in C_{i}}\\|p-q\\|_{2}^{2} \\leq \\sum_{i \\in[k]}\\left(\\left(\\left|C_{i}\\right|^{2}-\\left|C_{i}\\right|\\right) \\cdot 2 \\cdot(z-y)\\right) \\leq 2|P| \\cdot\\left(\\frac{|P|}{k}-1\\right) \\cdot(1+\\delta)^{2} \\cdot(z-y)\n$$\n\nThe completeness analysis is now completed by noting that $(1+\\delta)^{2} \\leq 1+3 \\delta$. Thus we turn to the soundness analysis.\n\nSoundness. Consider the optimal $\\ell_{2}^{2}$ min-sum $k$-clustering $\\mathcal{C}:=\\left\\{C_{1}, \\ldots, C_{k}\\right\\}$ of the instance (i.e., $C_{1} \\cup \\cdots \\cup C_{k}=P$ ). We aim at showing that the $\\ell_{2}^{2}$ min-sum $k$-clustering cost of $\\mathcal{C}$ is at least $((z-y)+2(1-\\alpha)-o(1)) \\ell|P|$. Given a cluster $C_{i}$, let $E_{i}:=\\left\\{T \\in E: p_{T} \\in C_{i}\\right\\}$ be the collection of $z$-sets of $E$ corresponding to $C_{i}$. For each $S \\in\\binom{|n|}{y}$, we define the degree of $S$ in $C_{i}$ to be\n\n$$\nd_{i, S}:=\\left|\\left\\{T \\mid S \\subset T \\text { and } p_{T} \\in C_{i}\\right\\}\\right|\n$$\n\nLet $t_{1}=2 z-2 y$ and $t_{2}=2 z-2 y+2$. For each cluster $C_{i}$, let\n\n$$\n\\begin{aligned}\nF_{i} & =\\left|\\left\\{(p, q) \\in C_{i}^{2}:\\|p-q\\|_{2}^{2} \\geq t_{2}\\right\\}\\right| \\\\\nM_{i} & =\\left|\\left\\{(p, q) \\in C_{i}^{2}:\\|p-q\\|_{2}^{2}=t_{1}\\right\\}\\right| \\\\\nN_{i} & =\\left|\\left\\{(p, q) \\in C_{i}^{2}:\\|p-q\\|_{2}^{2}<t_{1}\\right\\}\\right|\n\\end{aligned}\n$$\n\nBy (1), $F_{i}, M_{i}$, and $N_{i}$ are the number of (ordered) pairs within $C_{i}$ whose corresponding $z$-sets in the Balanced Johnson Coverage* instance intersect in $<y,=y$, and $>y$ elements respectively. Let $\\Delta_{i}=\\max _{S \\in\\binom{|n|}{y}} d_{i, S}$ and observe that $\\Delta_{i} \\leq\\left|C_{i}\\right|$. We write the total cost of the clustering as follows.\n\n$$\n\\sum_{i \\in[k]} \\sum_{p, q \\in C_{i}}\\|p-q\\|_{2}^{2} \\geq \\sum_{i \\in[k]}\\left(F_{i} t_{2}+M_{i} t_{1}\\right)=\\sum_{i \\in[k]}\\left(\\left(\\left|C_{i}\\right|^{2}-M_{i}\\right) t_{2}+M_{i} t_{1}-N_{i} t_{2}\\right)\n$$\n\nWe first upper bound $\\sum_{i \\in[k]}\\left(N_{i} t_{2}\\right)$. For each $z$-set $T$, there are at most $\\left(\\sum_{\\ell=y+1}^{z}\\binom{z}{\\ell}\\binom{n-z}{\\ell}\\right.$ many sets in $\\binom{|n|}{z}$ that intersect with $T$ in at least $y+1$ elements. Therefore, we have:\n\n$$\n\\begin{aligned}\n\\sum_{i \\in[k]} N_{i} & \\leq\\left(\\sum_{i \\in[k]}\\left|C_{i}\\right| \\cdot\\left(\\sum_{\\ell=y+1}^{z}\\binom{z}{\\ell}\\binom{n-z}{z-\\ell}\\right)\\right) \\\\\n& \\leq \\sum_{i \\in[k]}\\left|C_{i}\\right| \\cdot 2^{z} \\cdot(z-y) \\cdot n^{z-y-1} \\\\\n& =\\mathcal{O}\\left(|P| \\cdot n^{z-y-1}\\right)\n\\end{aligned}\n$$\n\nBy the definition of Balanced Johnson Coverage*, $|P|=|E|=\\omega\\left(k \\cdot n^{z-y-1}\\right)$, so $\\sum_{i \\in[k]} N_{i} t_{2}$ is at most $o\\left(|P|^{2} / k\\right)$.\n\nNext, we invoke a technical claim in [CKL22] which bounds $M_{i} /\\left|C_{i}\\right|$ in terms of $\\Delta_{i}$ and $\\left|C_{i}\\right|$.\n\nClaim 2.8 (Claim 3.18 in [CKL22]). For every $i \\in[k]$, either $\\left|C_{i}\\right|=o\\left(|P| / k\\right)$ or $M_{i} /\\left|C_{i}\\right| \\leq(1+$ $o(1)) \\Delta_{i}+o\\left(\\left|C_{i}\\right|\\right)$.\n\nWe can thus lower bound the cost of the clustering in (2) as follows:\n\n$$\n\\sum_{i \\in[k]} \\sum_{p, q \\in \\mathrm{C}_{i}}\\|p-q\\|_{2}^{2} \\geq\\left(\\sum_{i \\in[k]}\\left|C_{i}\\right|^{2}(2 z-2 y+2)\\right)-\\left(\\sum_{i \\in[k]} 2 \\Delta_{i}\\left|C_{i}\\right|\\right)-o\\left(\\left|P^{2}\\right| / k+\\sum_{i \\in[k]}\\left|C_{i}\\right|^{2}\\right)\n$$\n\nThus, we now look at upper bounding $\\sum_{i \\in[k]} 2 \\Delta_{i}\\left|C_{i}\\right|$. From the soundness case assumption, we have that $s:=\\sum_{i \\in[k]} \\Delta_{i} \\leq \\alpha \\cdot|E|$. Without loss of generality, we may assume that $\\left|C_{1}\\right| \\geq\\left|C_{2}\\right| \\geq$ $\\cdots \\geq\\left|C_{k}\\right|$. Let $t \\in[k]$ be smallest integer such that $\\sum_{i \\in[t]}\\left|C_{i}\\right|>s$. Since $\\Delta_{i} \\leq\\left|C_{i}\\right|$, then $\\sum_{i \\in[k]} 2 \\Delta_{i}\\left|C_{i}\\right|$ is maximized when $\\Delta_{i}=\\left|C_{i}\\right|$ for all $i \\in[t]$. Thus, $\\sum_{i \\in[k]} 2 \\Delta_{i}\\left|C_{i}\\right| \\leq \\sum_{i \\in[t]} 2\\left|C_{i}\\right|^{2}$, and we can rewrite (3) as follows:\n\n$$\n\\sum_{i \\in[k]} \\sum_{p, q \\in \\mathrm{C}_{i}}\\|p-q\\|_{2}^{2} \\geq\\left(\\sum_{i \\in[t]}\\left|C_{i}\\right|^{2}(2 z-2 y)\\right)+\\left(\\sum_{i=t+1}^{k}\\left|C_{i}\\right|^{2}(2 z-2 y+2)\\right)-o\\left(\\left|P^{2}\\right| / k+\\sum_{i \\in[k]}\\left|C_{i}\\right|^{2}\\right)\n$$\n\nThen the quantity $\\left(\\sum_{i \\in[t]}\\left|C_{i}\\right|^{2}(2 z-2 y)\\right)+\\left(\\sum_{i=t+1}^{k}\\left|C_{i}\\right|^{2}(2 z-2 y+2)\\right)$ is minimized when for all $i \\in[t]$, we have all $\\left|C_{i}\\right|$ 's to be equal and for all $i \\in\\{t+1, \\ldots, k\\}$, we have all $\\left|C_{i}\\right|$ 's to be equal (by convexity). Thus,\n\n$$\n\\left(\\sum_{i \\in[t]}\\left|C_{i}\\right|^{2}(z-y)\\right)+\\left(\\sum_{i=t+1}^{k}\\left|C_{i}\\right|^{2}(z-y+1)\\right) \\geq\\left(\\frac{\\alpha^{2}|P|^{2}}{t}(z-y)\\right)+\\left(\\frac{(1-\\alpha)^{2}|P|^{2}}{(k-t)}(z-y+1)\\right)\n$$\n\nWe may rewrite the left side as follows:\n\n$$\n\\frac{|P|^{2}}{k}\\left(\\frac{\\alpha^{2} \\cdot(z-y)}{t / k}+\\frac{(1-\\alpha)^{2} \\cdot(z-y+1)}{1-(t / k)}\\right)\n$$\n\nIf we look at the first derivative of the above expression w.r.t. $t / k$, then we have that the minima of the above expression is attained when:\n\n$$\n\\frac{(1-\\alpha)^{2} \\cdot(z-y+1)}{(1-(t / k))^{2}}=\\frac{\\alpha^{2} \\cdot(z-y)}{(t / k)^{2}}\n$$\n\nSimplifying, we obtain:\n\n$$\nt=k \\cdot\\left(\\frac{\\alpha \\cdot \\sqrt{z-y}}{\\alpha \\cdot \\sqrt{z-y}+(1-\\alpha) \\cdot \\sqrt{z-y+1}}\\right)\n$$\n\nReturning to the cost of clustering, we have from (4):\n\n$$\n\\begin{aligned}\n\\sum_{i \\in[k]} \\sum_{p, q \\in \\mathrm{C}_{i}}\\|p-q\\|_{2}^{2} & \\geq(1-o(1)) \\cdot \\frac{2\\left|P^{2}\\right|}{k} \\cdot\\left(\\frac{\\alpha^{2} \\cdot(z-y)}{t / k}+\\frac{(1-\\alpha)^{2} \\cdot(z-y+1)}{1-(t / k)}\\right) \\\\\n& \\geq(1-o(1)) \\cdot \\frac{2\\left|P^{2}\\right|}{k} \\cdot\\left(\\frac{\\alpha^{2} \\cdot(z-y)}{t / k}+\\left(\\frac{\\alpha^{2} \\cdot(z-y)}{t / k} \\cdot \\frac{1-(t / k)}{t / k}\\right)\\right)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =(1-o(1)) \\cdot \\frac{2\\left|P^{2}\\right|}{k} \\cdot\\left(\\frac{\\alpha^{2} \\cdot(z-y)}{(t / k)^{2}}\\right) \\\\\n& \\geq(1-o(1)) \\cdot \\frac{2\\left|P^{2}\\right|}{k} \\cdot(\\alpha \\cdot \\sqrt{z-y}+(1-\\alpha) \\cdot \\sqrt{z-y+1})^{2}\n\\end{aligned}\n$$\n\nDimensionality reduction. The proof of the theorem with the reduced dimension (i.e., $d=$ $O(\\log n))$ of the hard instances follows from the Johnson-Lindenstrauss lemma. Elaborating, given a set of $n$ points in $\\mathbb{R}^{d}$, we have that the $\\ell_{2}^{2}$ min-sum $k$-clustering cost of a given partition $\\left\\{C_{1}, \\ldots, C_{k}\\right\\}$ expressed as $\\sum_{i=1}^{k} \\sum_{p, q \\in C_{i}}\\|p-q\\|_{2}^{2}$. Thus, applying the Johnson-Lindenstrauss lemma with target dimension $\\mathcal{O}\\left(\\log n / \\varepsilon^{2}\\right)$ for small enough $\\varepsilon$, yields an instance where the $\\ell_{2}^{2}$ min-sum $k$-clustering cost of any clustering $C$ is within a factor $(1+\\varepsilon)$ of the $\\ell_{2}^{2}$ min-sum $k$-clustering cost of $C$ in the original $d$-dimensional instance. It follows that the gap is preserved up to a $(1+\\varepsilon)$ factor and the theorem follows. Note that this can be made deterministic (for example, see the result of Engebretsen et al. [EIO02]).", "tables": {}, "images": {}}, {"section_id": 10, "text": "# 2.3 Proof of Theorem 2.6 \n\nTheorem 2.6 follows from observing additional properties of the reduction of [CKL22] from the multilayered PCPs of [DGKR05, Kho02] to 3-Hypergraph Vertex Coverage. The description of the reduction is taken verbatim from [CKL22]. We first describe the multilayered PCPs that we use.\n\nDefinition 2.9. An $\\ell$-layered $P C P \\mathcal{M}$ consists of\n\n- An $\\ell$-partite graph $G=(V, E)$ where $V=\\cup_{i=1}^{\\ell} V_{i}$. Let $E_{i, j}=E \\cap\\left(V_{i} \\times V_{j}\\right)$.\n- Sets of alphabets $\\Sigma_{1}, \\ldots, \\Sigma_{\\ell}$.\n- For each edge $e=\\left(v_{i}, v_{j}\\right) \\in E_{i, j}$, a surjective projection $\\pi_{e}: \\Sigma_{j} \\rightarrow \\Sigma_{i}$.\n\nGiven an assignment $\\left(\\sigma_{i}: V_{i} \\rightarrow \\Sigma_{i}\\right)_{i \\in \\mid \\ell}$, an edge $e=\\left(v_{i}, v_{j}\\right) \\in E_{i, j}$ is satisfied if $\\pi_{e}\\left(\\sigma_{j}\\left(v_{j}\\right)\\right)=\\sigma_{i}\\left(v_{i}\\right)$. There are additional properties that $\\mathcal{M}$ can satisfy.\n\n- $\\eta$-smoothness: For any $i<j, v_{j} \\in V$, and $x, y \\in \\Sigma_{j}, \\operatorname{Pr}_{\\left(v_{i}, v_{j}\\right) \\in E_{i, j}}\\left[\\pi_{\\left(v_{i}, v_{j}\\right)}(x)=\\pi_{\\left(v_{i}, v_{j}\\right)}(y)\\right] \\leq \\eta$.\n- Path-regularity: Call a sequence $p=\\left(v_{1}, \\ldots, v_{\\ell}\\right)$ full path if $\\left(v_{i}, v_{i+1}\\right) \\in E_{i, i+1}$ for every $1 \\leq i<\\ell$, and let $\\mathcal{P}$ be the distribution of full paths obtained by (1) sampling a random vertex $v_{1} \\in V_{1}$ and (2) for $i=2, \\ldots, \\ell$, sampling $v_{i}$ from the neighbors of $v_{i-1}$ in $E_{i-1, i} . \\mathcal{M}$ is called path-regular if for any $i<j$, sampling $p=\\left(v_{1}, \\ldots, v_{\\ell}\\right)$ from $\\mathcal{P}$ and taking $\\left(v_{i}, v_{j}\\right)$ is the same as sampling uniformly at random from $E_{i, j}$.\n\nTheorem 2.10. [DGKR05, Kho02] For any $\\tau, \\eta>0$ and $\\ell \\in \\mathbb{N}$, given an $\\ell$-layered PCP $\\mathcal{M}$ with $\\eta$-smoothness and path-regularity, it is NP-hard to distinguish between the following cases.\n\n- Completeness: There exists an assignment that satisfies every edge $e \\in E$.\n- Soundness: For any $i<j$, no assignment can satisfy more than an $\\tau$ fraction of edges in $E_{i, j}$.\n\nGiven an $\\ell$-layered PCP $\\mathcal{M}$ described above, in [CKL22] they design the reduction to the Johnson Coverage problem as follows. First, the produced instance will be vertex-weighted and edge-weighted, so that the problem becomes \"choose a set of vertices of total weight at most $k$ to maximize the total weight of covered edges.\" We will explain how to obtain an unweighted instance at the end of this section.\n\n- Let $C_{i}:=\\{ \\pm 1\\}^{\\left|\\Sigma_{i}\\right|}$ and $U_{i}:=V_{i} \\times C_{i}$. The resulting hypergraph will be denoted by $\\mathcal{H}=$ $(U, H)$ where $U=\\cup_{i=1}^{\\ell}\\left(V_{i} \\times C_{i}\\right)$. The weight of vertex $(v, x) \\in V_{i} \\times C_{i}$ is\n\n$$\nw(v, x):=\\frac{1}{\\ell} \\cdot \\frac{1}{\\left|V_{i}\\right|} \\cdot \\frac{1}{\\left|C_{i}\\right|}\n$$\n\nNote that the sum of all vertex weights is 1 .\n\n- Let $\\mathcal{D}_{l}$ be the distribution where $i \\in[\\ell]$ is sampled with probability ${ }^{2} 6(\\ell-i)^{2} /(\\ell(\\ell-1)(2 \\ell-$ 1)), and $\\mathcal{D}$ be the distribution over $(i, j) \\in[\\ell]^{2}$ where $i$ is sampled from $\\mathcal{D}_{l}$ and $j$ is sampled uniformly from $\\{i+1, \\ldots, \\ell\\}$. For each $i<j$, we create a set of hyperedges $H_{i, j}$ that have one vertex in $U_{i}$ and two vertices in $U_{j}$. Fix each $e=\\left(v_{i}, v_{j}\\right) \\in E_{i, j}$ and a set of three vertices $t \\subseteq\\left(\\left\\{v_{i}\\right\\} \\times C_{i}\\right) \\cup\\left(\\left\\{v_{j}\\right\\} \\times C_{j}\\right)$. The weight $w(t)$ is (the probability that $(i, j)$ is sampled from $\\mathcal{D}) \\cdot\\left(1 /\\left|E_{i, j}\\right|\\right) \\cdot$ (the probability that $t$ is sampled from the following procedure). The reduction is parameterized by $\\delta>0$ determined later.\n- For each $a \\in \\Sigma_{i}$, sample $x_{a} \\in\\{ \\pm 1\\}$.\n- For each $b \\in \\Sigma_{j}$,\n\n\u25a0 Sample $y_{b} \\in\\{ \\pm 1\\}$.\n\n- If $x_{\\pi(b)}=-1$, let $z_{b}=y_{b}$ with probability $1-\\delta$ and $z_{b}=-y_{b}$ otherwise.\n- If $x_{\\pi(b)}=1$, let $z_{b}=-y_{b}$.\n- Output $\\left\\{\\left(v_{i}, x\\right),\\left(v_{j}, y\\right),\\left(v_{j}, z\\right)\\right\\}$.\n\nNote that the sum of all hyperedge weights is also 1 .\nSoundness. The soundness of the reduction is proved in [CKL22].\nLemma 2.11 ([CKL22]). Any subset of weight at most $1 / 2$ intersects hyperedges of total weight at most $7 / 8+o(1)$.\n(Almost) regularity. We prove the (almost) regularity of the reduction; for every vertex, the ratio between the weight of the vertex and the total weight of the hyperedges containing it is $(3 \\pm o(1))$. Note that 3 is natural as both total vertex weights and total edge weights are normalized to 1 and each hyperedge contains three vertices.\n\nFix a vertex $(v, x)$ where $v \\in V_{i}$ for some $i \\in[\\ell]$. Its vertex weight $w(v, x)=\\frac{1}{\\ell} \\cdot \\frac{1}{\\left|V_{i}\\right|} \\cdot \\frac{1}{\\left|C_{i}\\right|}$. We now consider the edge weight (described as a sampling procedure) and compute the probability that a random hyperedge contains $(v, x)$. There are two possibilities.\n\n[^0]\n[^0]:    ${ }^{2}$ [CKL22] states $(\\ell-i)^{2} /(6 \\ell(\\ell-1)(2 \\ell-1))$, which is a typo corrected in their analysis.\n\n- The hyperedge is from the $j$ th layer and $i$ th layer for some $j<i$. For fixed $j<i$, the probability of the pair $(j, i)$ is\n\n$$\n\\frac{6(\\ell-j)^{2}}{\\ell(\\ell-1)(2 \\ell-1)} \\cdot \\frac{1}{\\ell-j}=\\frac{6(\\ell-j)}{\\ell(\\ell-1)(2 \\ell-1)}\n$$\n\nand given $(j, i)$, the probability that $v$ is contained in the sampled hyperedge is $\\frac{2 \\pm o(1)}{\\left|V_{i}\\right|\\left|C_{i}\\right|}$. (Note that the distribution of either $\\left(v_{j}, y\\right)$ or $\\left(v_{i}, z\\right)$ in the procedure is the uniform distribution on $V_{i} \\times C_{i}$. The factor 2 comes from the fact that the hyperedge samples two points from the $i$ th layer; the probability that the same point is sampled twice is exponentially small and can be absorbed in the $o(1)$ term.)\n\n- The hyperedge is from the $i$ th layer and $j$ th layer for some $i<j$. For fixed $i<j$, the probability of the pair $(i, j)$ is\n\n$$\n\\frac{6(\\ell-i)^{2}}{\\ell(\\ell-1)(2 \\ell-1)} \\cdot \\frac{1}{\\ell-i}=\\frac{6(\\ell-i)}{\\ell(\\ell-1)(2 \\ell-1)}\n$$\n\nSumming the above events for all $j$ values, we get\n\n$$\n\\begin{aligned}\n&(1 \\pm o(1))\\left(\\left(\\sum_{j=1}^{i-1} \\frac{6(\\ell-j)}{\\ell(\\ell-1)(2 \\ell-1)} \\frac{2}{\\left|V_{i}\\right|\\left|C_{i}\\right|}\\right)+\\left(\\sum_{j=i+1}^{\\ell} \\frac{6(\\ell-i)}{\\ell(\\ell-1)(2 \\ell-1)} \\frac{1}{\\left|V_{i}\\right|\\left|C_{i}\\right|}\\right)\\right)\\right. \\\\\n&=\\frac{6 \\pm o(1)}{\\ell(\\ell-1)\\left(2 \\ell-1_{\\left|V_{i}\\right|}\\left|C_{i}\\right|\\right)}\\left(\\left(\\sum_{j=1}^{i-1} 2(\\ell-j)\\right)+\\left(\\sum_{j=i+1}^{\\ell}(\\ell-i)\\right)\\right) \\\\\n&=\\frac{6 \\pm o(1)}{\\ell(\\ell-1)(2 \\ell-1)\\left|V_{i}\\right|\\left|C_{i}\\right|}\\left(\\left(\\sum_{j=1}^{i-1} 2(\\ell-j)\\right)+(\\ell-i)^{2}\\right) \\\\\n&=\\frac{6 \\pm o(1)}{\\ell(\\ell-1)(2 \\ell-1)\\left|V_{i}\\right|\\left|C_{i}\\right|}\\left(2 \\ell(i-1)-i(i-1)+\\ell^{2}-2 \\ell i+i^{2}\\right) \\\\\n&=\\frac{6 \\pm o(1)}{\\ell(\\ell-1)(2 \\ell-1)\\left|V_{i}\\right|\\left|C_{i}\\right|}\\left(\\ell^{2}-2 \\ell+i\\right)=\\frac{3 \\pm \\mathcal{O}(1 / \\ell) \\pm o(1)}{\\ell\\left|V_{i}\\right|\\left|C_{i}\\right|}\n\\end{aligned}\n$$\n\nBy increasing $\\ell$ to be an arbitrarily large constant, we established that the total weight of the hyperedges containing $(v, x)$ is $(3 \\pm o(1))$ times its vertex weight $\\frac{1}{\\ell\\left|V_{i}\\right|\\left|C_{i}\\right|}$.\n\nCompleteness. If $\\mathcal{M}$ admits an assignment $\\left(\\sigma_{i}: V_{i} \\rightarrow \\Sigma_{i}\\right)_{i \\in[\\ell]}$ that satisfies every edge $e \\in E$, let $S:=\\left\\{\\left(v_{i}, x\\right): v_{i} \\in V_{i}, x_{\\sigma_{i}\\left(v_{i}\\right)}=-1\\right\\}$. Fix any $e=\\left(v_{i}, v_{j}\\right) \\in E_{i, j}$ and consider the above sampling procedure to sample $x \\in\\{ \\pm 1\\}^{\\Sigma_{i}}$ and $y \\in\\{ \\pm 1\\}^{\\Sigma_{j}}$ when $b=\\sigma_{j}\\left(v_{j}\\right)$. Since $\\pi_{e}\\left(\\sigma_{j}\\left(v_{j}\\right)\\right)=\\sigma_{i}\\left(v_{i}\\right)$, at least one of $x_{\\sigma_{i}\\left(v_{i}\\right)}, y_{\\sigma_{i}\\left(v_{j}\\right)}, z_{\\sigma_{j}\\left(v_{j}\\right)}$ must be -1 always. So, $S$ intersects every hyperedge with nonzero weight.\n\nFurthermore, an inspection of the sampling procedure reveals that for a fixed vertex $\\left(v_{i}, x\\right)$ and $j>i$, a $1 / 2 \\pm \\mathcal{O}(\\delta)$ fraction of the hyperedges containing it has all three vertices in $S$ and a $1 / 2 \\pm \\mathcal{O}(\\delta)$ fraction of the hyperedges containing it has only $\\left(v_{i}, x\\right)$ in $S$. Therefore, there must be an assignment from all the hyperedges to $S$ such that (1) a hyperedge is assigned to a vertex contained by it, and (2) every vertex is assigned a $1 / 2+1 /(2 \\cdot 3) \\pm \\mathcal{O}(\\delta)=2 / 3 \\pm \\mathcal{O}(\\delta)$ fraction of the hyperedges containing it (which is consistent with the fact that $S$ contains half of the vertices).\n\nTherefore, each vertex has almost the same ratio (up to $1 \\pm o(1)$ by taking $\\delta$ arbitrarily small) between its weight and the total weight of the hyperedges assigned to it. In order to obtain an unweighted instance, for each vertex $(v, x)$, we create a new cloud of vertices $C_{v, x}$ whose cardinality is proportional to $w(v, x)$, and replace each edge $\\left(\\left(v_{1}, x_{1}\\right),\\left(\\overline{v_{2}, x_{2}}\\right),\\left(v_{3}, x_{3}\\right)\\right)$ by all possible edges between $C_{v_{1}, x_{1}}, C_{v_{2}, x_{2}}, C_{v_{3}, x_{3}}$ (with the total weight equal to the weight of the original edge).", "tables": {}, "images": {}}, {"section_id": 11, "text": "# 3 PTAS based on $D^{2}$ Sampling \n\nFor a set $A \\subset \\mathbb{R}^{d}$, let $\\mu(A):=\\frac{1}{|A|} \\sum_{p \\in A} p$ denote its mean. Let $\\mathcal{C}=\\left\\{C_{1}, \\ldots C_{k}\\right\\}$ be an optimal $k$-MinSum clustering of a point set $A$. We use $\\mu_{i}=\\mu\\left(C_{i}\\right)$ to denote the mean of $C_{i}$ and we use $\\Delta_{i}=\\frac{\\sum_{p \\in C_{i}}\\left\\|p-\\mu_{i}\\right\\|^{2}}{\\left|C_{i}\\right|}$ to denote the average mean squared distance of $C_{i}$ to $\\mu_{i}$. We further use $C_{i}^{\\beta}$ to denote the subset of $C_{i}$ with $\\left\\|p-\\mu_{i}\\right\\|^{2} \\leq \\beta \\cdot \\Delta_{i}$. Finally, let OPT denote the cost of an optimal solution. So, $\\mathrm{OPT}=\\sum_{i=1}^{k}\\left|C_{i}\\right|^{2} \\cdot \\Delta_{i}$.\n\nDefinition 3.1. We say that $m$ is an $\\varepsilon$-approximate mean of $C_{i}$ if $\\left\\|m-\\mu_{i}\\right\\|^{2} \\leq \\varepsilon \\cdot \\Delta_{i}$. We say that a set $S \\subset A$ is an $(\\varepsilon, \\beta)$-mean seeding set for $C_{i} \\in \\mathcal{C}$, if there exists a subset $S^{\\prime} \\cup\\{s\\} \\subset S$ with $\\left\\|s-\\mu_{i}\\right\\|^{2} \\leq \\beta \\cdot \\Delta_{i}$ and a weight assignment $w: S^{\\prime} \\rightarrow \\mathbb{R}_{\\geq 0}$ such that\n\n$$\n\\left\\|\\frac{1}{\\sum_{p \\in S^{\\prime}} w(p)} \\sum_{p \\in S^{\\prime}} w(p) \\cdot p-\\mu_{i}\\right\\|^{2} \\leq \\varepsilon \\cdot \\Delta_{i}\n$$\n\nWe will use the following well-known identities for Euclidean means.\nLemma 3.2. [IKI94] Let $A \\subset \\mathbb{R}^{d}$ be a set of points. Then for any $c \\in \\mathbb{R}^{d}$ :\n\n- $\\sum_{p \\in A}\\|p-c\\|^{2}=\\sum_{p \\in A}\\|p-\\mu(A)\\|^{2}+|A| \\cdot\\|\\mu(A)-c\\|^{2}$.\n- $\\sum_{p, q \\in A}\\|p-q\\|^{2}=2 \\cdot|A| \\cdot \\sum_{p \\in A}\\|p-\\mu(A)\\|^{2}$.\n\nWe note that as an immediate corollary, the lemma implies that the sum of squared distances of all points in a cluster $C_{i}$ to an approximate mean is at most $(1+\\varepsilon)\\left|C_{i}\\right| \\Delta_{i}$ and the MinSum clustering cost is at most $(1+\\varepsilon)\\left|C_{i}\\right|^{2} \\Delta_{i}$.\n\nCorollary 3.3. For any set of points $A \\subset \\mathbb{R}^{d}$. Then $c \\in \\mathbb{R}^{d}$ is an $\\varepsilon$-approximate mean of $A$ if and only if $\\sum_{p \\in A}\\|p-c\\|^{2} \\leq(1+\\varepsilon) \\cdot\\left|C_{i}\\right| \\cdot \\Delta_{i}$.\n\nLemma 3.4. [BBC ${ }^{+}$19] Given numbers $a, b, c$, we have for all $\\varepsilon>0$\n\n$$\n(a-b)^{2} \\leq(1+\\varepsilon) \\cdot(a-c)^{2}+\\left(1+\\frac{1}{\\varepsilon}\\right)(b-c)^{2}\n$$\n\nWe also show that we only have to consider seeding sets with $\\beta \\in \\Theta\\left(\\varepsilon^{-2}\\right)$.\nLemma 3.5. For any cluster $C_{i}, \\varepsilon \\in(0,1)$ and $\\beta \\geq 12 \\varepsilon^{-2}$, we have that $\\mu_{i}\\left(C_{i}^{\\beta}\\right)=\\frac{1}{\\left|C_{i}^{\\beta}\\right|} \\sum_{p \\in C_{i}^{\\beta}} p$ is a $\\varepsilon$-approximate mean of $C_{i}$.\n\nProof. By Markov's inequality, $\\left|C_{i} \\backslash C_{i}^{\\beta}\\right| \\leq \\beta^{-1} \\cdot\\left|C_{i}\\right| \\leq \\frac{\\varepsilon^{2}}{12} \\cdot\\left|C_{i}\\right|$. Since $\\frac{1}{\\left|C_{i}^{\\beta}\\right|} \\sum_{p \\in C_{i}^{\\beta}}\\left\\|p-\\mu_{i}\\left(C_{i}^{\\beta}\\right)\\right\\|^{2} \\leq$ $\\frac{1}{\\left|C_{i}^{\\beta}\\right|} \\sum_{p \\in C_{i}^{\\beta}}\\left\\|p-\\mu_{i}\\right\\|^{2} \\leq \\Delta_{i} \\cdot \\frac{\\left|C_{i}\\right|}{\\left|C_{i}^{\\beta}\\right|} \\leq 2 \\Delta_{i}$, Lemma 3.2 implies $\\left\\|\\mu_{i}\\left(C_{i}^{\\beta}\\right)-\\mu_{i}\\right\\|^{2}=\\frac{1}{\\left|C_{i}^{\\beta}\\right|} \\sum_{p \\in C_{i}^{\\beta}}\\left\\|p-\\mu_{i}\\right\\|^{2}-$ $\\frac{1}{\\left|C_{i}^{\\beta}\\right|} \\sum_{p \\in C_{i}^{\\beta}}\\left\\|p-\\mu_{i}\\left(C_{i}^{\\beta}\\right)\\right\\|^{2} \\leq 2 \\Delta_{i}$. We then have due to Lemma 3.4\n\n$$\n\\begin{aligned}\n\\sum_{q \\in C_{i} \\backslash C_{i}^{\\beta}}\\left\\|q-\\mu_{i}\\left(C_{i}^{\\beta}\\right)\\right\\|^{2}-\\left\\|q-\\mu_{i}\\right\\|^{2} & \\leq \\frac{\\varepsilon}{2} \\sum_{q \\in C_{i} \\backslash C_{i}^{\\beta}}\\left\\|p-\\mu_{i}\\right\\|^{2}+\\left(1+\\frac{2}{\\varepsilon}\\right) \\cdot\\left\\|\\mu_{i}-\\mu_{i}\\left(C_{i}^{\\beta}\\right)\\right\\|^{2} \\\\\n& \\leq \\frac{\\varepsilon}{2} \\sum_{q \\in C_{i} \\backslash C_{i}^{\\beta}}\\left\\|p-\\mu_{i}\\right\\|^{2}+\\frac{\\varepsilon^{2}}{12}\\left|C_{i}\\right| \\cdot \\frac{3}{\\varepsilon} \\cdot 2 \\Delta_{i} \\leq \\varepsilon\\left|C_{i}\\right| \\Delta_{i}\n\\end{aligned}\n$$\n\nThe cost of the points in $C_{i}^{\\beta}$ to $\\mu_{i}\\left(C_{i}^{\\beta}\\right)$ only gets smaller compared to the cost of these points to $\\mu_{i}$. Hence, the increase in cost is bounded by $\\varepsilon\\left|C_{i}\\right| \\Delta_{i}$, which with Corollary 3.3 yields the claim.\n\nFinally, we also show how to efficiently extract a mean from a mean seeding set, while being oblivious to $\\Delta_{i}$.\nLemma 3.6. Let $S$ be an $(\\varepsilon / 4, \\beta)$-mean seeding set of a cluster $C_{j}$ with mean $\\mu_{j}$. Then we can compute $\\left(\\frac{10 \\beta \\cdot|S|}{\\varepsilon}+1\\right)^{|S|}$ choices of weights in time linear in the size of choices such that at least one of the computed choices satisfies\n\n$$\n\\left\\|\\frac{1}{\\sum_{p \\in S} w(p)} \\sum_{p \\in S} w(p) \\cdot p-\\mu_{j}\\right\\|^{2} \\leq \\varepsilon \\cdot \\Delta_{j}\n$$\n\nProof. We first introduce some preprocessing. By an affine transformation of the space, subtract $q=\\underset{p \\in S}{\\operatorname{argmin}}\\left\\|p-\\mu_{j}\\right\\|$ from all points. Now all points $p$ in $S$ with $\\left\\|p-\\mu_{j}\\right\\| \\leq \\sqrt{\\beta \\cdot \\Delta_{j}}$ have norm at $\\operatorname{most} 2 \\sqrt{\\beta \\cdot \\Delta_{j}}$.\n\nLet $S^{\\prime} \\subset S$ be the set with weights $w$ such that\n\n$$\n\\left\\|\\frac{1}{\\sum_{p \\in S^{\\prime}} w(p)} \\sum_{p \\in S^{\\prime}} w(p) p-\\mu_{j}\\right\\|^{2} \\leq \\frac{\\varepsilon}{4} \\cdot \\Delta_{j}\n$$\n\nLet $w_{\\max }$ be the maximum weight of the points in $S^{\\prime}$. For $w(p)$ every $p$, we set $w^{\\prime}(p)$ to be the largest multiple of $\\frac{\\varepsilon}{10 \\beta \\cdot|S|} \\cdot w_{\\max }$ that is at most $w(p)$ (where we extend $w$ to all of $S$ by setting $w(p)=0$ for all $\\left.p \\notin S^{\\prime}\\right)$. So, $w^{\\prime}(p)=\\frac{\\varepsilon}{10 \\beta \\cdot|S|} \\cdot i \\cdot w_{\\max } \\leq w(p)<\\frac{\\varepsilon}{10 \\beta \\cdot|S|} \\cdot(i+1) \\cdot w_{\\max }$ for some $i \\in\\left\\{0,1, \\ldots \\frac{10 \\beta \\cdot|S|}{\\varepsilon}\\right\\}$. Observe that there are at most $\\left(\\frac{10 \\beta \\cdot|S|}{\\varepsilon}+1\\right)^{|S|}$ choices of weights of points in S. Furthermore, we have\n\n$$\n\\left|\\sum_{p \\in S}\\left(w^{\\prime}(p)-w(p)\\right)\\right| \\leq \\frac{\\varepsilon}{10 \\beta \\cdot|S|} \\sum_{p \\in S^{\\prime}} w(p)\n$$\n\nWe now argue that $\\mu^{\\prime}=\\frac{1}{\\sum_{p \\in S^{\\prime}} w^{\\prime}(p)} \\sum_{p \\in S^{\\prime}} w^{\\prime}(p)$ is a $\\varepsilon$-approximate mean of $C_{j}$. We have\n\n$$\n\\left\\|\\frac{1}{\\sum_{p \\in S^{\\prime}} w(p)} \\sum_{p \\in S^{\\prime}} w(p) p-\\frac{1}{\\sum_{p \\in S^{\\prime}} w^{\\prime}(p)} \\sum_{p \\in S^{\\prime}} w^{\\prime}(p) p\\right\\|\n$$\n\n$$\n\\begin{aligned}\n& =\\frac{1}{\\sum_{p \\in S^{\\prime}} w^{\\prime}(p)}\\left\\|\\frac{\\sum_{p \\in S^{\\prime}} w^{\\prime}(p)}{\\sum_{p \\in S^{\\prime}} w(p)} \\sum_{p \\in S^{\\prime}} w(p) p-\\sum_{p \\in S^{\\prime}} w^{\\prime}(p) p\\right\\| \\\\\n& =\\frac{1}{\\sum_{p \\in S^{\\prime}} w^{\\prime}(p)}\\left\\|\\frac{\\sum_{p \\in S^{\\prime}} w^{\\prime}(p)-\\sum_{p \\in S^{\\prime}} w(p)}{\\sum_{p \\in S^{\\prime}} w(p)} \\sum_{p \\in S^{\\prime}} w(p) p\\right\\|+\\frac{1}{\\sum_{p \\in S^{\\prime}} w(p)}\\left\\|\\sum_{p \\in S^{\\prime}}\\left(w(p)-w^{\\prime}(p)\\right) p\\right\\| \\\\\n& \\leq \\frac{2 \\varepsilon}{10 \\beta \\cdot|S|}\\left\\|\\frac{1}{\\sum_{p \\in S^{\\prime}} w(p)} \\sum_{p \\in^{\\prime} S} w(p) p\\right\\|+\\sum_{p \\in S^{\\prime}} \\frac{\\varepsilon}{10 \\beta \\cdot|S|}\\|p\\| \\\\\n& \\leq \\frac{5 \\varepsilon}{10 \\beta \\cdot|S|} \\cdot\\left|S^{\\prime}\\right| \\cdot \\sqrt{\\beta \\cdot \\Delta_{j}} \\leq \\frac{\\varepsilon}{2} \\sqrt{\\Delta_{j}}\n\\end{aligned}\n$$\n\nBy the triangle inequality, we can therefore conclude that $\\mu^{\\prime}$ is a $\\varepsilon$-approximate mean of $\\mu_{j}$", "tables": {}, "images": {}}, {"section_id": 12, "text": "# Computing a Mean-Seeding Set via Uniform Sampling. \n\nLemma 3.7. Let $\\varepsilon \\in(0,1)$ and $\\beta>48 \\varepsilon^{2}$. With probability at least $1-\\delta$, a set of $32 k \\varepsilon^{-1} \\log \\delta^{-1}$ points $S$ sampled uniformly at random with replacement from $A$ contains is a $(\\varepsilon, \\beta)$-mean seeding set of any $C_{i}$ with $\\left|C_{i}\\right| \\geq \\frac{n}{k}$.\n\nProof. Due to Lemma 3.5, The mean of $C_{i}^{\\beta}$ is an $\\frac{\\varepsilon}{2}$-approximate mean. Hence, if we obtain a $(\\varepsilon / 2, \\beta)$ seeding set of $C_{i}^{\\beta}$, the claim follows. By Markov's inequality, $C_{i}^{\\beta}$ contains at least $\\frac{n}{2 k}$ points. For any $p \\in C_{i}^{\\beta}$, we have $\\mathbb{E}\\left[\\left\\|p-\\mu\\left(C_{i}^{\\beta}\\right)\\right\\|^{2}\\right]=\\Delta_{i}^{\\beta}:=\\frac{1}{\\left|C_{i}^{\\beta}\\right|} \\sum_{p \\in C_{i}^{\\beta}}\\left\\|p-\\mu\\left(C_{i}^{\\beta}\\right\\|^{2} \\leq \\Delta_{i}\\right.$ and therefore for any set of $m$ points $S_{i}$ sampled independently with replacement from $C_{i}^{\\beta}, \\mathbb{E}\\left[\\left\\|\\frac{1}{m} \\sum_{p \\in S_{i}} p-\\mu\\left(C_{i}^{\\beta}\\right)\\right\\|^{2}\\right]=\\frac{1}{m} \\Delta_{i}^{\\beta}$. Therefore, if $m \\geq 4 \\varepsilon^{-1}, S_{i}$ is an $(\\varepsilon / 2, \\beta)$-mean seeding set of $C_{i}$ with probability at least $\\frac{1}{2}$. Hence, sampling $\\log \\delta^{-1}$ many copies of $S_{i}$ implies that at least one of them is an $(\\varepsilon / 2, \\beta)$-mean seeding set of $C_{i}$ with probability $1-2^{-\\log \\delta^{-1}}=1-\\delta$.\n\nA sample from the point set is contained in $C_{i}^{\\beta}$ with probability at least $\\frac{1}{2 k}$. Hence, sampling at least $16 k \\cdot \\varepsilon^{-1} \\cdot \\log \\delta^{-1}$ implies that with probability at least $1-\\delta$, the number $X$ of points sampled from $S_{i}$ is at least $4 \\varepsilon^{-1} \\log \\delta^{-1}$, as follows. By the above analysis $\\mathbb{E}[X] \\geq 8 \\varepsilon^{-1} \\log \\delta^{-1}$. Therefore, by standard Chernoff bounds, $\\operatorname{Pr}\\left[X<4 \\varepsilon^{-1} \\log \\delta^{-1}\\right]<e^{-\\frac{1}{8} \\cdot 8 \\varepsilon^{-1} \\log \\delta^{-1}} \\leq \\delta$.\n$D^{2}$ Subsampling We now define an algorithm for sampling points that induce means from the target clusters. The high level idea is as follows. We construct a rooted tree in which every node is labeled by a set of candidate cluster means. For a parent and child pair of nodes, the parent's set is a subset of the child's set. The construction is iterative. Given an interior node, we construct its children by adding a candidate mean to the parent's set. The candidantes are generated using points sampled at random from a distribution that will be defined later. The goal is to have, eventually, an $\\varepsilon$-approximate mean for every optimal cluster. This will be achieved with high probability at one of the leaves of the tree. The root of the tree is labeled with the empty set, and its children are constructed via uniform sampling. Subsequently, we refine the sampling distribution to account for various costs and densities of the clusters.\n\nWe now go into more detail for the various sampling stages of the algorithm.\nPreprocessing: We ensure that all points are not too far from each other.\n\nInitialization: We initialize the set of means via uniform sampling. Due to Lemma 3.7, we can enumerate over potential sets of $\\varepsilon$-approximate means for all clusters of size $\\frac{n}{k}$. Each candidate mean defines a child of the root.\n\nSampling Stage: Consider a node of the tree labeled with a non-empty set of candidate means $M$. We put $\\Gamma_{i}=2^{-i} \\cdot \\sum_{q \\in A} \\min _{m \\in M}\\|q-m\\|^{2}$ for $i \\in\\{0,1, \\ldots, 13 \\log (n k / \\varepsilon)\\}$, where $\\eta$ is an absolute constant to be defined later. Let $A_{i, M}=\\left\\{q \\in A: \\min _{m \\in M}\\|q-m\\|^{2} \\leq \\Gamma_{i}\\right\\}$. (Note that $A_{0, M}$ includes all the points.) Let $\\mathbb{P}_{i}$ denote the probability distribution on $A_{i, M}$ induced by setting, for each $p \\in A_{i, M}$,\n\n$$\n\\mathbb{P}_{i}[p]=\\frac{\\min _{m \\in M}\\|p-m\\|^{2}}{\\sum_{p \\in A_{i, M}} \\min _{m \\in M}\\|p-m\\|^{2}}\n$$\n\nWe'll use $\\mathbb{P}$ to denote $\\mathbb{P}_{0}$. For each $i$, we sample a sufficient (polynomial in $k$ and $\\varepsilon$, but independent of $n$ ) number of points independently from the distribution $\\mathbb{P}_{i}$. Let $S$ denote the set of sampled points.\n\nMean Extraction Stage: We enumerate over combinations of points in $M \\cup S$, using some nonuniform weighing to fix a mean to add to $M$, see Lemma 3.6. Each choice of mean is added to $M$ to create a child of the node labeled $M$.\n\nThroughout this section we will use the following definition. Given a set of centers $M$, we say that a cluster $C_{i}$ is $\\varepsilon$-covered by $M$ if $\\left|C_{i}\\right|^{2} \\cdot \\min _{m \\in M}\\left\\|\\mu_{i}-m\\right\\|^{2} \\leq \\frac{\\varepsilon}{2} \\cdot\\left(\\frac{1}{k} \\cdot O P T+\\left|C_{i}\\right|^{2} \\Delta_{i}\\right)$. Our goal will be to prove the following lemma.\n\nLemma 3.8. Let $\\mathcal{C}=\\left\\{C_{1}, \\ldots C_{k}\\right\\}$ be the clusters of an optimal Min-Sum $k$-clustering and let $\\eta$ be an absolute constant. For every $\\delta, \\epsilon>0$, there is a randomized algorithm that outputs a collection of at most $n^{n(1)} \\cdot 2^{\\eta \\cdot k^{2} \\cdot \\varepsilon^{-12} \\log ^{2}(k /(\\varepsilon \\delta))}$ sets of at most $k$ centers $M$, such that with probability $1-\\delta$ at least one of them that $\\varepsilon$-covers every $C_{i} \\in \\mathcal{C}$. The algorithm runs in time $n^{1+o(1)} \\cdot d \\cdot 2^{\\eta \\cdot k^{2} \\cdot \\varepsilon^{-12} \\log ^{2}(k /(\\varepsilon \\delta))}$.\n\nNote that if all clusters of $\\mathcal{C}$ are $\\varepsilon$-covered, then there exists an assignment of points to centers, such that Min-Sum clustering cost of the resulting clustering is at most $(1+\\varepsilon) \\cdot$ OPT. To see this, notice that if we use $\\mathcal{C}$ as the clustering with $m_{i}=\\operatorname{argmin}_{m \\in M}\\left\\|\\mu_{i}-m\\right\\|^{2}$, then\n\n$$\n\\sum_{i=1}^{k}\\left|C_{i}\\right| \\sum_{p \\in}\\left\\|p-m_{i}\\right\\|^{2} \\leq \\mathrm{OPT}+\\sum_{i=1}^{k}\\left|C_{i}\\right| \\sum_{p \\in} \\frac{\\varepsilon}{2}\\left(\\frac{1}{k} \\cdot \\frac{\\mathrm{OPT}}{\\left|C_{i}\\right|^{2}}+\\frac{1}{2} \\Delta_{i}\\right) \\leq(1+\\varepsilon) \\cdot \\mathrm{OPT}\n$$\n\nPreprocessing The first lemma allows us to assume that all points are in some sense close to each other.\n\nLemma 3.9. Suppose $n>20$. Given an set of $n$ points $A \\subset \\mathbb{R}^{d}$, we can partition a point set into subsets $A_{1}, \\ldots A_{k}$, such that $\\|p-q\\|^{2} \\leq n^{10} \\cdot$ OPT for any two points $p, q \\in A_{i}$ and such that any cluster $C_{j}$ is fully contained in one of the $A_{i}$. The partitioning takes time $\\tilde{O}\\left(n d+k^{2}\\right)$.\n\nProof. The proof uses similar arguments found throughout $k$-means and $k$-median research, with only difference being that some of the discretization arguments are slightly finer to account for the MinSum clustering objective.\n\nConsider a candidate 20-approximate $k$-means clustering with cost $T$, which can be computed in time $\\tilde{O}\\left(n d+k^{2}\\right)$ [DSS24]. Then we have $\\frac{1}{20} T \\cdot \\mathrm{OPT} \\leq 20 n^{2} \\cdot T$. Now, suppose that there are two centers $c_{1}$ and $c_{2}$ such that $\\left\\|c_{1}-c_{2}\\right\\|^{2} \\leq 20 n^{2} \\cdot T$. Then for any point $p \\in C_{1}$ and $q \\in C_{2}$, we have by the triangle inequality $\\|p-q\\|^{2} \\leq 20 n^{9} \\cdot T \\leq n^{10} \\cdot T$. Conversely, if $\\left\\|c_{1}-c_{2}\\right\\|^{2}>n^{8} \\cdot T$, we know that no two points in the clusters induced by $C_{1}$ and $C_{2}$ can be in the same cluster of the optimal MinSum clustering.\n\nComputing a Mean-Seeding Set via $D^{2}$ Sampling. We now consider a slight modification of Lemma 3.7 to account for sampling points from a cluster non-uniformly. We introduce the notion of a distorted core as follows. Given a cluster $C_{j}$, a set of centers $M$, and parameters $\\alpha, \\beta$, we say that a subset of $C_{j}^{\\beta} \\cup M$ is a $\\left(C_{j}, \\beta, \\alpha, M\\right)$-distorted core (denoted $\\operatorname{core}\\left(C_{j}, \\beta, \\alpha, M\\right)$ ) iff it is the image of a mapping $\\pi_{\\alpha, M}: C_{j}^{\\beta} \\rightarrow C_{j}^{\\beta} \\cup M$ such that for any point $p \\in C_{j}^{\\beta}$, we have\n\n$$\n\\pi_{\\alpha, M}(p)= \\begin{cases}p & \\text { if } \\min _{m \\in M}\\|p-m\\|^{2} \\geq \\alpha \\cdot \\Delta_{j} \\\\ \\underset{m \\in M}{\\operatorname{argmin}}\\|p-m\\|^{2} & \\text { if } \\min _{m \\in M}\\|p-m\\|^{2}<\\alpha \\cdot \\Delta_{j}\\end{cases}\n$$\n\nWe use $D\\left(C_{j}, \\beta, \\alpha, M\\right)$ to denote the set of points in $C_{j}^{\\beta}$ such that $\\min _{m \\in M}\\|p-m\\|^{2}<\\alpha \\cdot \\Delta_{j}$.\nThe following lemmas relate the goodness of a mean computed on an $\\alpha$-distorted core to the mean on the entire set of points when sampling points proportionate to squared distances. We start by proving an analogue of Lemma 3.5.\nLemma 3.10. Let $\\alpha \\leq \\frac{\\varepsilon}{4}$ and let $\\beta \\geq \\frac{144}{\\varepsilon^{2}}$. Given a set of centers $M$ and a cluster $C_{j}$, let\n\n$$\n\\hat{\\mu}_{j}=\\frac{1}{\\left|C_{j}^{\\beta}\\right|} \\sum_{p \\in C_{j}^{\\beta}} \\pi_{\\alpha, M}(p)\n$$\n\nThen,\n\n$$\n\\left\\|\\hat{\\mu}_{j}-\\mu_{j}\\right\\|^{2} \\leq \\varepsilon \\cdot \\Delta_{j}\n$$\n\nProof. First, let $\\mu_{j}^{\\prime}$ be the mean of $C_{j}^{\\beta}$. Due to Markov's inequality $\\left|C_{j}^{\\beta}\\right| \\geq \\frac{\\left|C_{j}\\right|}{2}$. Using Lemma 3.2, we have $\\left|C_{j}\\right| \\cdot \\Delta_{j} \\geq \\sum_{p \\in C_{j}^{\\beta}}\\left\\|p-\\mu_{j}\\right\\|^{2} \\geq\\left|C_{j}^{\\beta}\\right| \\cdot\\left\\|\\mu_{j}^{\\prime}-\\mu_{j}\\right\\|^{2}$, which implies that $\\left\\|\\mu_{j}^{\\prime}-\\mu_{j}\\right\\|^{2} \\cdot\\left|C_{j}\\right| \\leq 2\\left|C_{j}\\right| \\cdot \\Delta_{j}$. Then\n\n$$\n\\begin{aligned}\n\\sum_{p \\in C_{j}}\\left\\|p-\\mu_{j}^{\\prime}\\right\\|^{2}= & \\sum_{p \\in C_{j}^{\\beta}}\\left\\|p-\\mu_{j}^{\\prime}\\right\\|^{2}+\\sum_{p \\in C_{j} \\backslash C_{j}^{\\beta}}\\left\\|p-\\mu_{j}^{\\prime}\\right\\|^{2} \\\\\n\\leq & \\sum_{p \\in C_{j}^{\\beta}}\\left\\|p-\\mu_{j}\\right\\|^{2}+ \\\\\n& +\\sum_{p \\in C_{j} \\backslash C_{j}^{\\beta}}\\left(1+\\frac{\\varepsilon}{8}\\right) \\cdot\\left\\|p-\\mu_{j}\\right\\|^{2}+\\left|C_{j} \\backslash C_{j}^{\\beta}\\right| \\cdot\\left(1+\\frac{8}{\\varepsilon}\\right) \\cdot\\left\\|\\mu_{j}^{\\prime}-\\mu_{j}\\right\\|^{2} \\\\\n\\leq & \\left(1+\\frac{\\varepsilon}{8}\\right) \\cdot \\sum_{p \\in C_{j}}\\left\\|p-\\mu_{j}\\right\\|^{2}+\\frac{9}{\\varepsilon \\beta} \\cdot\\left|C_{j}\\right| \\cdot\\left\\|\\mu_{j}^{\\prime}-\\mu_{j}\\right\\|^{2}\n\\end{aligned}\n$$\n\n$$\n\\leq\\left(1+\\frac{\\varepsilon}{8}\\right) \\cdot \\sum_{p \\in C_{j}}\\left\\|p-\\mu_{j}\\right\\|^{2}+\\frac{18}{\\varepsilon \\beta} \\cdot\\left|C_{j}\\right| \\Delta_{j}\n$$\n\nwhere we used Lemma 3.4 in the second inequality. In other words, $\\mu_{j}^{\\prime}$ is an $\\left(\\frac{\\varepsilon}{8}+\\frac{18}{\\varepsilon \\beta}\\right)$-approximate mean of $C_{j}$. We now turn our attention to $\\hat{\\mu}_{j}$. We have\n\n$$\n\\left\\|\\hat{\\mu}_{j}-\\mu_{j}\\right\\| \\leq \\frac{1}{\\left|C_{j}^{\\beta}\\right|} \\cdot \\sum_{p \\in C_{j}^{\\alpha}}\\left\\|p-\\pi_{\\alpha, M}(p)\\right\\| \\leq \\sqrt{\\alpha \\cdot \\Delta_{j}}\n$$\n\nBy the triangle inequality, we therefore have\n\n$$\n\\left\\|\\hat{\\mu}_{j}-\\mu_{j}\\right\\| \\leq\\left\\|\\hat{\\mu}_{j}-\\mu_{j}^{\\prime}\\right\\|+\\left\\|\\mu_{j}^{\\prime}-\\mu_{j}\\right\\| \\leq \\sqrt{\\alpha \\cdot \\Delta_{j}}+\\sqrt{\\left(\\frac{\\varepsilon}{8}+\\frac{18}{\\varepsilon \\beta}\\right) \\cdot \\Delta_{j}}\n$$\n\nBy our choice of $\\alpha$ and $\\beta$, this implies that $\\hat{\\mu}_{j}$ is an $\\varepsilon$-approximate mean of $C_{j}$.\nWe now characterize when $M$ either covers a cluster $C_{j}$, or when $M$ is a suitable seeding set for $C_{j}$. The following lemma says that if $M$ is not a seeding set of $C_{j}$, then there exist many points in the core $C_{j}^{\\beta}$ of $C_{j}$ that are far from $M$.\n\nLemma 3.11. Given $\\alpha \\leq \\frac{\\varepsilon}{16}, \\beta \\geq \\frac{2400}{\\varepsilon^{2}}$, and $\\gamma \\leq \\sqrt{\\frac{\\varepsilon}{16(\\beta+\\alpha)}}$, and a set of centers $M$, let $C_{j}$ be a cluster for which $\\left|D\\left(C_{j}, \\beta, \\alpha, M\\right)\\right| \\geq(1-\\gamma) \\cdot\\left|C_{j}^{\\beta}\\right|$. Then $M$ is an $(\\varepsilon, \\beta)$-mean seeding set of $C_{j}$.\n\nProof. First, let $\\hat{\\mu}_{j}=\\frac{1}{\\left|C_{j}^{\\beta}\\right|} \\sum_{p \\in C_{j}^{\\beta}} \\pi_{\\alpha, M}(p)$ and let $\\mu_{j}^{\\prime}=\\frac{1}{\\left|D\\left(C_{j}, \\beta, \\alpha, M\\right)\\right|} \\sum_{p \\in D\\left(C_{j}, \\beta, \\alpha, M\\right)} p$ be the mean of $D\\left(C_{j}, \\beta, \\alpha, M\\right)$. Now, observe that for any pairs of points $p \\in C_{j}^{\\alpha}$ and $q \\in C_{j}^{\\beta}$, by the triangle inequality\n\n$$\n\\|q-p\\| \\leq\\left\\|q-\\mu_{j}\\right\\|+\\left\\|\\mu_{j}-p\\right\\| \\leq \\sqrt{(\\beta+\\alpha) \\cdot \\Delta_{j}}\n$$\n\nThen\n\n$$\n\\begin{aligned}\n& \\left\\|\\hat{\\mu}_{j}-\\mu_{j}^{\\prime}\\right\\| \\\\\n= & \\frac{1}{\\left|C_{j}^{\\beta}\\right|}\\left\\|\\sum_{p \\in C_{j}^{\\beta}} \\pi_{\\alpha, M}(p)-\\frac{\\left|C_{j}^{\\beta}\\right|}{\\left|D\\left(C_{j}, \\beta, \\alpha, M\\right)\\right|} \\sum_{p \\in D\\left(C_{j}, \\beta, \\alpha, M\\right)} p\\right\\| \\\\\n= & \\frac{1}{\\left|C_{j}^{\\beta}\\right|} \\cdot\\left\\|\\left(\\sum_{p \\in D\\left(C_{j}, \\beta, \\alpha, M\\right)}\\left(\\pi_{\\alpha, M}(p)-p\\right)\\right)+\\right. \\\\\n& \\left.+\\left(\\sum_{p \\in C_{j}^{\\beta} \\backslash D\\left(C_{j}, \\beta, \\alpha, M\\right)} \\pi_{\\alpha, M}(p)-\\frac{\\left|C_{j}^{\\beta} \\backslash D\\left(C_{j}, \\beta, \\alpha, M\\right)\\right|}{\\left|D\\left(C_{j}, \\beta, \\alpha, M\\right)\\right|} \\sum_{p \\in D\\left(C_{j}, \\beta, \\alpha, M\\right)} p\\right)\\right\\| \\\\\n\\leq & \\frac{1}{\\left|C_{j}^{\\beta}\\right|} \\cdot\\left\\|\\sum_{p \\in D\\left(C_{j}, \\beta, \\alpha, M\\right)}\\left(\\pi_{\\alpha, M}(p)-p\\right)\\right\\|+\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& +\\frac{1}{\\left|C_{j}^{\\beta}\\right|} \\cdot\\left\\|\\sum_{p \\in C_{j}^{\\beta} \\backslash D\\left(C_{j}, \\beta, \\alpha, M\\right)} \\pi_{\\alpha, M}(p)-\\frac{\\left|C_{j}^{\\beta} \\backslash D\\left(C_{j}, \\beta, \\alpha, M\\right)\\right|}{\\left|D\\left(C_{j}, \\beta, \\alpha, M\\right)\\right|} \\sum_{p \\in D\\left(C_{j}, \\beta, \\alpha, M\\right)} p\\right\\| \\\\\n\\leq & \\sqrt{\\alpha \\cdot \\Delta_{j}}+\\gamma \\cdot \\sqrt{(\\beta+\\alpha) \\cdot \\Delta_{j}}\n\\end{aligned}\n$$\n\nFinally, by the triangle inequality, Lemma 3.10 and our choice of $\\alpha, \\beta$, and $\\gamma$, we have\n\n$$\n\\left\\|\\mu_{j}^{\\prime}-\\mu_{j}\\right\\| \\leq\\left\\|\\mu_{j}^{\\prime}-\\hat{\\mu}_{j}\\right\\|+\\left\\|\\hat{\\mu}_{j}-\\mu_{j}\\right\\| \\leq \\sqrt{\\alpha \\cdot \\Delta_{j}}+\\gamma \\cdot \\sqrt{(\\beta+\\alpha) \\cdot \\Delta_{j}}+\\sqrt{\\frac{\\varepsilon}{4} \\Delta_{j}} \\leq \\sqrt{\\varepsilon \\Delta_{j}}\n$$\n\nthus completing the proof.\nAs a consequence of this lemma and the preprocessing, we show under the assumption of Lemma 3.9, the largest value of $i$ such that $C_{j}^{\\beta} \\in A_{i, M}$ for an uncovered cluster $C_{j}$ cannot be too large.\nLemma 3.12. Given $\\beta \\geq \\frac{2400}{c^{2}}$, suppose we have a set of points $A$ such that $\\|p-q\\|^{2} \\leq n^{10} \\cdot$ OPT as per Lemma 3.9. Let $M$ be a set of points and suppose there exists a cluster $C_{j}$ such that such $C_{j}$ is uncovered and such that $M$ is not an $(\\varepsilon / 4, \\beta)$ mean seeding set of $A$. We then have that $C_{j}^{\\beta} \\subset A_{i, M}$ implies $i \\leq 13 \\log (n k / \\varepsilon)$.\nProof. Suppose $i>13 \\log (n k / \\varepsilon)$. Due to Lemma 3.11, we know there exists a point $p^{\\prime} \\in C_{j}^{\\beta}$ such that $\\min _{m \\in M}\\|p-m\\|^{2} \\geq \\varepsilon / 16 \\cdot \\Delta_{j}$. This implies via Lemma 3.9 that $\\Delta_{j} \\leq\\left(\\frac{k \\cdot n}{\\varepsilon}\\right)^{-13} \\cdot 16 \\varepsilon^{-1} \\cdot n^{10} \\cdot$ OPT.\n\nConsider the point $p \\in C_{j}^{\\beta}$ with minimumal distance to $\\mu_{j}$ and let $m_{p}=\\operatorname{argmin}_{m \\in M}\\|p-m\\|^{2}$. Then $\\|p-m\\|^{2} \\leq n^{-20} \\cdot$ OPT, which implies that\n\n$$\n\\begin{aligned}\n\\left|C_{j}\\right| \\cdot \\sum_{q \\in C_{j}}\\|q-m\\|^{2} & \\leq\\left|C_{j}\\right| \\cdot \\sum_{q \\in C_{j}} 2 \\cdot\\|q-p\\|^{2}+2 \\cdot\\|p-m\\|^{2} \\\\\n& \\leq 4\\left|C_{j}\\right| \\cdot \\sum_{q \\in C_{j}}\\left\\|q-\\mu_{j}\\right\\|^{2}+2\\left|C_{j}\\right|^{2} \\cdot\\|p-m\\|^{2} \\\\\n& \\leq 4\\left|C_{j}\\right|^{2} \\cdot 16 \\varepsilon^{-1} \\cdot\\left(\\frac{k \\cdot n}{\\varepsilon}\\right)^{-13} \\cdot n^{10} \\cdot \\mathrm{OPT}+2\\left|C_{j}\\right|^{2}\\left(\\frac{k \\cdot n}{\\varepsilon}\\right)^{-13} \\cdot n^{10} \\cdot \\mathrm{OPT} \\\\\n& \\leq 66 \\cdot\\left|C_{j}\\right|^{2} \\cdot\\left(\\frac{k \\cdot n}{\\varepsilon}\\right)^{-13} \\cdot 16 \\varepsilon^{-1} \\cdot n^{10} \\cdot \\mathrm{OPT} \\leq \\frac{\\varepsilon}{2 k} \\cdot \\mathrm{OPT}\n\\end{aligned}\n$$\n\nwhich is a contradiction to $M$ not covering $C_{j}$.\nWe now show that, given that $M$ is not a seeding set of some cluster $C_{j}$, that the weighted squared distance of $\\mu_{j}$ to its closest point in $M$ is a reasonably accurate proxy for the squared distance of the points in the core $C_{j}^{\\beta}$ to their respectively closest points in $M$.\n\nLemma 3.13. Let $M$ be a set of centers and let $C_{j}$ be a cluster that is not $\\varepsilon$-covered by $M$. Also assume that $M$ is not an $(\\varepsilon, \\beta)$-mean seeding set of $C_{j}$. Then,\n\n$$\n\\sum_{p \\in C_{j}^{\\beta}} \\min _{m \\in M}\\|p-m\\|^{2} \\geq \\frac{1}{272}\\left(\\frac{\\varepsilon}{\\beta}\\right)^{3 / 2} \\cdot\\left|C_{j}\\right| \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}\n$$\n\nProof. For all $p \\in C_{j}^{\\beta} \\backslash D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)$, we have:\n\n$$\n\\begin{aligned}\n\\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} & \\leq\\left(\\min _{m \\in M}\\|p-m\\|+\\left\\|\\mu_{j}-p\\right\\|\\right)^{2} \\\\\n& \\leq 2 \\min _{m \\in M}\\|p-m\\|^{2}+2\\left\\|\\mu_{j}-p\\right\\|^{2} \\\\\n& \\leq \\frac{34 \\beta}{\\varepsilon} \\cdot \\min _{m \\in M}\\|p-m\\|^{2}\n\\end{aligned}\n$$\n\nwhere the first inequality uses the triangle inequality and that for $m^{\\prime}=\\arg \\min _{m \\in M}\\|p-m\\|$, we have that $\\left\\|\\mu_{j}-m^{\\prime}\\right\\|^{2} \\geq \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}$, and the last inequality uses $\\min _{m \\in M}\\|p-m\\|^{2} \\geq \\frac{\\varepsilon}{16} \\Delta_{j}$ and $\\left\\|\\mu_{j}-p\\right\\|^{2} \\leq \\beta \\Delta_{j}$ and $\\frac{\\beta}{\\varepsilon} \\geq 1$.\n\nWe first consider the case that $\\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\| \\geq 2 \\sqrt{\\beta \\cdot \\Delta_{j}}$. In this case, all points in $C_{j}^{\\beta}$ are closer to $\\mu_{j}$ than to any point in $M$. This implies\n\n$$\n\\sum_{p \\in C_{j}^{\\beta}} \\min _{m \\in M}\\|p-m\\|^{2} \\geq \\frac{1}{4}\\left|C_{j}^{\\beta}\\right| \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} \\geq \\frac{1}{8}\\left|C_{j}\\right| \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}\n$$\n\nNow, we consider the case that $\\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\| \\leq 2 \\sqrt{\\beta \\cdot \\Delta_{j}}$. As $M$ is not an $\\varepsilon$-mean seeding set for $C_{j}$, Lemma 3.11 implies that $\\left|C_{j}^{\\beta} \\backslash D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)\\right|>\\sqrt{\\frac{\\varepsilon}{16 \\beta+\\varepsilon}}\\left|C_{j}^{\\beta}\\right| \\geq \\frac{1}{2} \\sqrt{\\frac{\\varepsilon}{16 \\beta+\\varepsilon}}\\left|C_{j}\\right|$. Therefore,\n\n$$\n\\begin{aligned}\n\\sum_{p \\in C_{j}^{\\beta}} \\min _{m \\in M}\\|p-m\\|^{2} & \\geq \\sum_{p \\in C_{j}^{\\beta} \\backslash D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)} \\min _{m \\in M}\\|p-m\\|^{2} \\\\\n& \\geq\\left|C_{j}^{\\beta} \\backslash D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)\\right| \\cdot \\frac{\\varepsilon}{34 \\beta} \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} \\\\\n& \\geq \\frac{1}{2} \\sqrt{\\frac{\\varepsilon}{16 \\beta+\\varepsilon}} \\cdot \\frac{\\varepsilon}{34 \\beta} \\cdot\\left|C_{j}\\right| \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} \\\\\n& \\geq \\frac{1}{272}\\left(\\frac{\\varepsilon}{\\beta}\\right)^{3 / 2} \\cdot\\left|C_{j}\\right| \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}\n\\end{aligned}\n$$\n\nwhich completes the proof.\nNext, we show that the marginal probability of picking a point from an uncovered cluster $C_{j}$ cannot be significantly smaller than the marginal probability of picking a point from the union of covered clusters with larger cardinality than $C_{j}$.\nLemma 3.14. Let $M$ be a set of centers, and let $\\mathcal{C}$ denote a set of clusters that are $\\varepsilon$-covered by $M$. Let $\\mathcal{H}$ denote the set of points in all the clusters in $\\mathcal{C}$. Let $\\beta>\\frac{2400}{\\varepsilon^{2}}$. Consider a cluster $C_{j} \\notin \\mathcal{C}$. Let $i$ be the largest index such that $C_{i} \\in \\mathcal{C}$. Suppose that $M$ is not an $(\\varepsilon, \\beta)$-mean seeding set of $C_{j}$, and that $i<j$. Then\n\n$$\n\\mathbb{P}\\left\\{p \\in C_{j}^{\\beta} \\mid p \\in \\mathcal{H} \\cup C_{j}\\right\\} \\geq \\frac{\\varepsilon^{4} \\cdot \\beta^{-3 / 2}}{1088 k}\n$$\n\nProof. For the points in $\\mathcal{H} \\cup C_{j}$, we have\n\n$$\n\\sum_{p \\in \\mathcal{H} \\cup C_{j}} \\min _{m \\in M}\\|p-m\\|^{2}=\\sum_{C_{h} \\in C}\\left|C_{h}\\right| \\cdot\\left(\\Delta_{h}+\\min _{m \\in M}\\left\\|\\mu_{h}-m\\right\\|^{2}\\right)+\\left|C_{j}\\right| \\cdot\\left(\\Delta_{j}+\\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}\\right)\n$$\n\n$$\n\\begin{aligned}\n& \\leq \\sum_{C_{h} \\in \\mathcal{C}}(1+\\varepsilon) \\cdot\\left|C_{h}\\right| \\cdot \\Delta_{h}+\\left|C_{j}\\right| \\cdot\\left(\\Delta_{j}+\\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}\\right) \\\\\n& \\leq 2 \\cdot\\left(\\sum_{C_{h} \\in \\mathcal{C}}\\left|C_{h}\\right| \\cdot \\Delta_{h}+\\varepsilon^{-1} \\cdot\\left|C_{j}\\right| \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}\\right)\n\\end{aligned}\n$$\n\nwhere the first inequality holds by definition of an $\\varepsilon$-covered cluster and the second inequality holds as $M$ does not $\\varepsilon$-cover $C_{j}$ and thus in particular $\\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} \\geq \\varepsilon \\cdot \\Delta_{j}$ due to Corollary 3.3.\n\nAssume for contradiction that the lemma does not hold, so\n\n$$\n\\sum_{p \\in C_{j}^{\\beta}}\\|p-m\\|^{2}<\\frac{\\varepsilon^{4} \\cdot \\beta^{-3 / 2}}{1088 k} \\cdot \\sum_{p \\in \\mathcal{H} \\cup C_{j}} \\min _{m \\in M}\\|p-m\\|^{2}\n$$\n\nThis yields\n\n$$\n\\begin{aligned}\n\\frac{1}{272}\\left(\\frac{\\varepsilon}{\\beta}\\right)^{3 / 2}\\left|C_{j}\\right| \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} & \\leq \\sum_{p \\in C_{j}^{\\beta}}\\|p-m\\|^{2} \\\\\n& <\\frac{\\varepsilon^{4} \\cdot \\beta^{-3 / 2}}{1088 k} \\cdot \\sum_{p \\in \\mathcal{H} \\cup C_{j}} \\min _{m \\in M}\\|p-m\\|^{2} \\\\\n& \\leq \\frac{\\varepsilon^{4} \\cdot \\beta^{-3 / 2}}{544 k} \\cdot\\left(\\sum_{C_{h} \\in \\mathcal{C}}\\left|C_{h}\\right| \\cdot \\Delta_{h}+\\varepsilon^{-1} \\cdot|C j| \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}\\right)\n\\end{aligned}\n$$\n\nwhere the first inequality uses Lemma 3.13. (Note that this lemma assumes that $M$ is not an $(\\varepsilon, \\beta)$-seeding set for $C_{j}$.) Rearranging the terms, we get\n\n$$\n\\begin{aligned}\n\\frac{\\varepsilon^{3} \\cdot \\beta^{-3 / 2}}{544} \\cdot\\left|C_{j}\\right| \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} & \\leq\\left(\\frac{(\\varepsilon / \\beta)^{3 / 2}}{272}-\\frac{(\\varepsilon / \\sqrt{\\beta})^{3}}{544 k}\\right) \\cdot\\left|C_{j}\\right| \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} \\\\\n& \\leq \\frac{\\varepsilon^{4} \\cdot \\beta^{-3 / 2}}{544 k} \\cdot \\sum_{C_{h} \\in \\mathcal{C}} \\cdot\\left|C_{h}\\right| \\cdot \\Delta_{h}\n\\end{aligned}\n$$\n\nTherefore, as $\\left|C_{j}\\right| \\leq\\left|C_{h}\\right|$ for all $C_{h} \\in \\mathcal{C}$,\n\n$$\n\\left|C_{j}\\right|^{2} \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} \\leq \\frac{\\varepsilon}{k} \\sum_{C_{h} \\in \\mathcal{C}}\\left|C_{h}\\right| \\cdot \\Delta_{h} \\cdot\\left|C_{j}\\right| \\leq \\frac{\\varepsilon}{k} \\cdot\\left|C_{h}\\right|^{2} \\cdot \\Delta_{h}\n$$\n\nThis, however, implies that $C_{j}$ is $\\varepsilon$-covered by $M$, contradicting the lemma's assumption.\nWe now consider a cluster $C_{j}$ that is small compared to the union of the clusters $C_{j}^{\\prime}$ with $j^{\\prime}>j$. In this case, we show that one of the distance-proportional distributions that we use guarantees that the probability of sampling points from the core of $C_{j}$ is large.\nLemma 3.15. Let $M$ be a set of centers. Let $\\beta>\\frac{2400}{c^{2}}$. Let $j$ be the smallest index such that $C_{j}$ is not $\\varepsilon$-covered by $M$. If $M$ is not an $(\\varepsilon, \\beta)$-mean seeding set for $C_{j}$, then there exists $i \\in\\{0,1, \\ldots, \\eta \\log (n k / \\varepsilon)\\}$ such that $C_{j}^{\\beta} \\in A_{i, M}$ and\n\n$$\n\\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta}\\right] \\geq \\frac{1}{4352 \\cdot k} \\cdot\\left(\\frac{\\varepsilon}{\\beta^{5 / 8}}\\right)^{4}\n$$\n\nProof. By Markov's inequality $\\left|C_{j}\\right| / 2<\\left|C_{j}^{\\beta}\\right|$. Let $i$ be the smallest value such that $C_{j}^{\\beta} \\subset A_{i, M}$. (Clearly, $C_{j}^{\\beta} \\subset A_{0, M}$, so $i$ exists.) We have due to Lemma 3.13\n\n$$\n\\sum_{p \\in C_{j}^{\\beta}} \\min _{m \\in M}\\|p-m\\|^{2} \\geq \\frac{1}{272}\\left(\\frac{\\varepsilon}{\\beta}\\right)^{3 / 2} \\cdot\\left|C_{j}\\right| \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}\n$$\n\nAlso, for all $p \\in C_{j}^{\\beta}$,\n\n$$\n\\min _{m \\in M}\\|p-m\\| \\leq \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|+\\left\\|p-\\mu_{j}\\right\\| \\leq \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|+\\sqrt{\\beta \\cdot \\Delta_{j}}<2 \\sqrt{\\frac{\\beta}{\\varepsilon}} \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|\n$$\n\nwhere the last inequality follows from the fact that $M$ does not $\\varepsilon$-cover $C_{j}$, so $\\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}>$ $\\varepsilon \\cdot \\Delta_{j}$ Note that this implies $\\min _{m \\in M}\\|p-m\\|^{2} \\leq 8 \\cdot \\frac{\\beta}{\\varepsilon} \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2}$ for all $p \\in A_{i, M}$, as $\\Gamma_{i}<2 \\max _{p \\in C_{j}} \\min _{m \\in M}\\|p-m\\|$. Since for any cluster $C_{j^{\\prime}}$ with $j^{\\prime}>j$ we have $\\left|C_{j^{\\prime}}\\right| \\leq\\left|C_{j}\\right|$ and therefore\n\n$$\n\\begin{aligned}\n\\sum_{p \\in C_{j^{\\prime}} \\cap A_{i, M}}\\|p-m\\|^{2} & \\leq\\left|C_{j^{\\prime}} \\cap A_{i, M}\\right| \\cdot 8 \\cdot \\frac{\\beta}{\\varepsilon} \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} \\leq\\left|C_{j}\\right| \\cdot 8 \\cdot \\frac{\\beta}{\\varepsilon} \\cdot \\min _{m \\in M}\\left\\|\\mu_{j}-m\\right\\|^{2} \\\\\n& \\leq 2176 \\cdot\\left(\\frac{\\beta}{\\varepsilon}\\right)^{5 / 2} \\cdot \\sum_{p \\in C_{j}^{\\beta}} \\min _{m \\in M}\\|p-m\\|^{2}\n\\end{aligned}\n$$\n\nDefine $\\mathcal{H}=\\cup_{h=1}^{j-1} C_{h}$ and $\\mathcal{L}=\\cup_{h=j+1}^{k} C_{h}$. Clearly\n\n$$\n\\mathbb{P}_{i}\\left[p \\in\\left(\\mathcal{H} \\cup C_{j} \\cup \\mathcal{L}\\right) \\cap A_{i, M}\\right]=1\n$$\n\nBy Lemma 3.14,\n\n$$\n\\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta} \\mid p \\in\\left(C_{j} \\cup \\mathcal{H}\\right) \\cap A_{i, M}\\right] \\geq \\frac{1}{1088 \\cdot k} \\cdot\\left(\\frac{\\varepsilon}{\\beta^{3 / 8}}\\right)^{4}\n$$\n\nBy Inequality (5),\n\n$$\n\\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta} \\mid p \\in\\left(C_{j} \\cup \\mathcal{L}\\right) \\cap A_{i, M}\\right] \\geq \\frac{1}{2176 \\cdot k} \\cdot\\left(\\frac{\\varepsilon}{\\beta}\\right)^{5 / 2}\n$$\n\nNow,\n\n$$\n\\max \\left\\{\\mathbb{P}_{i}\\left[p \\in\\left(\\mathcal{H} \\cup C_{j}\\right) \\cap A_{i, M}\\right], \\mathbb{P}_{i}\\left[p \\in\\left(C_{j} \\cup \\mathcal{L}\\right) \\cap A_{i, M}\\right]\\right\\} \\geq \\frac{1}{2}\n$$\n\nso,\n\n$$\n\\begin{aligned}\n\\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta}\\right] & =\\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta} \\mid p \\in\\left(\\mathcal{H} \\cup C_{j}\\right) \\cap A_{i, M}\\right] \\cdot \\mathbb{P}_{i}\\left[p \\in\\left(\\mathcal{H} \\cup C_{j}\\right) \\cap A_{i, M}\\right] \\\\\n& =\\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta} \\mid p \\in\\left(C_{j} \\cup \\mathcal{L}\\right) \\cap A_{i, M}\\right] \\cdot \\mathbb{P}_{i}\\left[p \\in\\left(C_{j} \\cup \\mathcal{L}\\right) \\cap A_{i, M}\\right] \\\\\n& \\geq \\frac{1}{2} \\cdot \\min \\left\\{\\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta} \\mid p \\in\\left(\\mathcal{H} \\cup C_{j}\\right) \\cap A_{i, M}\\right], \\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta} \\mid p \\in\\left(C_{j} \\cup \\mathcal{L}\\right) \\cap A_{i, M}\\right]\\right\\} \\\\\n& \\geq \\frac{1}{2} \\min \\left\\{\\frac{1}{2176 \\cdot k} \\cdot\\left(\\frac{\\varepsilon}{\\beta}\\right)^{5 / 2} \\cdot \\frac{1}{1088 \\cdot k} \\cdot\\left(\\frac{\\varepsilon}{\\beta^{3 / 8}}\\right)^{4}\\right\\} \\geq \\frac{1}{4352 \\cdot k} \\cdot\\left(\\frac{\\varepsilon}{\\beta^{5 / 8}}\\right)^{4}\n\\end{aligned}\n$$\n\nWe remark that by Lemma 3.9 we may assume that all non-zero squared distances are within a factor $n^{30}$ of each other. Thus, the desired $i<30 \\log n$.\n\nFinally, we show that we can account for the bias in the sampling in order to estimate an approximate mean.\n\nLemma 3.16. Let $M$ be a set of centers. Let $j$ be the smallest index such that $C_{j}$ is not $\\varepsilon$-covered by $M$. Suppose that $M$ is not an $(\\varepsilon / 4, \\beta)$-mean seeding set for $C_{j}$. Consider a set of points $S^{\\prime}$ sampled iid from $\\mathbb{P}_{i}$, and let $S=S^{\\prime} \\cap C_{j}^{\\beta}$. If $\\beta \\geq 2400 \\epsilon^{-2}$ and $S>17825792 \\cdot k\\left(\\frac{\\beta^{7 / 12}}{\\varepsilon}\\right)^{6} \\log (2 / \\delta)$, then with probability at least $1-\\delta$, we have that $S^{\\prime} \\cup M$ is an $(\\varepsilon / 4, \\beta)$-mean seeding set of $C_{j}$.\n\nProof. We first apply some preprocessing. Let $q$ be an arbitrary point in $S$. We subtract $q$ from all points. Therefore, we may assume that all points $p \\in C_{j}^{\\beta}$, as well as any point $m \\in M$ that has distance at most $\\sqrt{\\varepsilon^{2} \\Delta_{j} / 2}$ to some point in $C_{j}^{\\beta}$ have norm at most $\\sqrt{\\left(\\beta+\\varepsilon^{2} / 2\\right) \\Delta_{j}}$.\n\nFurthermore, let $\\mu_{D}$ be the mean of $D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)$, and let $\\mu_{C}$ be the mean of $C=C_{j}^{\\beta} \\backslash$ $D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)$. Due to Lemma 3.10, we have that $\\hat{\\mu}_{j}=\\frac{1}{\\left|C_{j}^{\\beta}\\right|} \\cdot\\left(\\mu_{C} \\cdot|C|+\\mu_{D} \\cdot\\left|D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)\\right|\\right)$ is an $\\frac{\\varepsilon}{4}$-approximate mean of $\\mu_{j}$, or more specifically\n\n$$\n\\left\\|\\hat{\\mu}_{j}-\\mu_{j}\\right\\| \\leq \\sqrt{\\frac{\\varepsilon}{4} \\cdot \\Delta_{j}}\n$$\n\nThus, if we can show that $S$ is an $\\frac{\\varepsilon}{4}$-mean seeding set of $\\mu_{C}$ (yielding an $\\frac{\\varepsilon}{4}$-approximate mean $\\widehat{\\mu_{C}}$, then\n\n$$\n\\begin{aligned}\n& \\left\\|\\frac{1}{\\left|C_{j}^{\\beta}\\right|}\\left(\\widehat{\\mu_{C}} \\cdot|C|+\\mu_{D} \\cdot\\left|D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)\\right|\\right)-\\mu_{j}\\right\\| \\\\\n\\leq & \\left\\|\\widehat{\\mu_{C}}-\\mu_{C}\\right\\|+\\left\\|\\frac{1}{\\left|C_{j}^{\\beta}\\right|}\\left(\\mu_{C} \\cdot|C|+\\mu_{D} \\cdot\\left|D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)\\right|\\right)-\\mu_{j}\\right\\| \\\\\n\\leq & \\sqrt{\\varepsilon / 4 \\Delta_{j}}+\\sqrt{\\varepsilon / 4 \\Delta_{j}} \\leq \\sqrt{\\varepsilon \\Delta_{j}}\n\\end{aligned}\n$$\n\nwhere we used Lemma 3.2 in the first inequality.\nLet $i$ to denote the largest index for which $A_{i, M}$ contains $C_{j}^{\\beta}$. Define for every point $p \\in C$ a weight $w_{p}=\\frac{1}{\\left|C| \\cdot \\mathbb{P}_{i}[p \\mid C]\\right.}$. To clarify, $\\mathbb{P}_{i}[p \\mid C]$ is the conditional probability that a single sample drawn from the probability distribution $\\mathbb{P}_{i}$ is $p$, conditioned on the sampled point being from $C$. We can then write\n\n$$\n\\mu_{C}=\\sum_{p \\in C}\\left(w_{p} p\\right) \\cdot \\mathbb{P}_{i}[p \\mid C]\n$$\n\nIn other words, $\\mu_{C}$ is the expectation of the scaled vector $w_{p} p$ under the conditional distribution $\\mathbb{P}_{i}[\\cdot \\mid C]$ Let $S_{C}=S \\cap C$. Conditioning on $s=|S \\cap C|$, the sample $S_{C}$ can be generated by taking $s$ independent samples from the distribution $\\mathbb{P}_{i}[\\cdot \\mid C]$. We write $S_{C}=\\left\\{p_{1}, p_{2}, \\ldots, p_{s}\\right\\}$, where the points are random variables. Define\n\n$$\n\\widehat{\\mu_{C}}=\\frac{1}{s} \\cdot \\sum_{p \\in S_{C}} w_{p} p\n$$\n\nTaking expectation over $\\mathbb{P}_{i}[\\cdot \\mid C \\wedge s]$, we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left\\|\\widehat{\\mu_{C}}-\\mu_{C}\\right\\|^{2}\\right] & =\\mathbb{E}\\left[\\frac{1}{s^{2}} \\cdot \\sum_{i=1}^{s} \\sum_{j=1}^{s}\\left(w_{p_{i}} p_{i}-\\mu_{C}\\right) \\cdot\\left(w_{q_{i}} q_{j}-\\mu_{C}\\right)\\right] \\\\\n& =\\frac{1}{s^{2}} \\cdot \\sum_{i=1}^{s} \\mathbb{E}\\left[\\left\\|w_{p_{i}} p_{i}-\\mu_{C}\\right\\|^{2}\\right]\n\\end{aligned}\n$$\n\nThe cross terms vanish as the sampled points are independent and the expectation of $w_{p} p$ is $\\mu_{C}$.\nTo complete the proof, notice that for $p \\in C,\\left\\|w_{p} p-\\mu_{C}\\right\\|^{2} \\leq 2 w_{p}\\|p\\|^{2}+2\\left\\|\\mu_{C}\\right\\|^{2}$. We may assume without loss of generality that the entire point-set is shifted so that $\\mu_{C}=\\overrightarrow{0}$. Hence, as $\\mu_{C} \\in \\operatorname{conv}\\left(C_{j}^{\\beta}\\right)$ and $p \\in C_{j}^{\\beta}$, we have that $\\|p\\|^{2} \\leq 4 \\beta \\Delta_{j}$. Also, $\\frac{\\varepsilon}{16} \\Delta_{j} \\leq \\min _{m \\in M}\\|p-m\\|^{2} \\leq \\beta \\cdot \\Delta_{j}$, where the lower bound holds by definition of $D\\left(C_{j}, \\beta, \\varepsilon / 16, M\\right)$ and the upper bound holds by definition of $C_{j}^{\\beta}$. Thus, $\\frac{\\varepsilon}{16 \\cdot|C|} \\leq \\mathbb{P}_{i}[p \\mid p \\in C] \\leq \\frac{\\beta}{|C|}$. This implies that $w_{p} \\leq \\frac{16}{\\varepsilon}$. Therefore,\n\n$$\n\\mathbb{E}\\left[\\left\\|\\widehat{\\mu_{C}}-\\mu_{C}\\right\\|^{2}\\right] \\leq \\frac{1}{s} \\cdot \\frac{64 \\beta}{\\varepsilon} \\cdot \\Delta_{j}, \\text { so } \\mathbb{P}_{i}\\left[\\left\\|\\widehat{\\mu_{C}}-\\mu_{C}\\right\\|^{2}>\\frac{1}{s} \\cdot \\frac{128 \\beta}{\\varepsilon} \\cdot \\Delta_{j}\\right]<\\frac{1}{2}\n$$\n\nIf $s \\geq \\frac{512 \\beta}{\\varepsilon^{2}}$, we get that $\\widehat{\\mu_{C}} \\frac{\\varepsilon}{4}$-covers $\\mu_{C}$ with probability at least $\\frac{1}{2}$. Thus, if $s \\geq \\frac{512 \\beta}{\\varepsilon^{2}} \\cdot \\log (2 / \\delta)$ we can apply this $\\log \\delta^{-1}$ times to boost the success probability to $1-\\frac{\\delta}{2}$.\n\nWe now bound the number of samples that we need to obtain $S_{C}$. Due to Lemma 3.15, we have $\\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta}\\right] \\geq \\frac{1}{4352 \\cdot k} \\cdot\\left(\\frac{\\varepsilon}{\\beta^{5 / 8}}\\right)^{4}$. Therefore, $\\mathbb{E}_{i}\\left[\\left|S_{C}\\right|\\right]=|S| \\cdot \\mathbb{P}_{i}\\left[p \\in C_{j}^{\\beta}\\right] \\geq|S| \\cdot \\frac{1}{4352 \\cdot k} \\cdot\\left(\\frac{\\varepsilon}{\\beta^{5 / 8}}\\right)^{4}$. Setting $|S| \\geq 17825792 \\cdot k\\left(\\frac{\\beta^{7 / 12}}{\\varepsilon}\\right)^{6} \\log (2 / \\delta)$ and applying the Chernoff bound, we have\n\n$$\n\\mathbb{P}\\left[\\left|S_{C}\\right|<\\frac{512 \\beta}{\\varepsilon^{2}} \\cdot \\log (2 / \\delta)\\right] \\leq \\exp \\left(-8 \\cdot \\mathbb{E}\\left[\\left|S_{C}\\right|\\right]\\right) \\leq \\delta / 2\n$$\n\nConversely, with probability $1-\\delta, S \\cup M$ contains a $(\\varepsilon / 4, \\beta)$-mean seeding set of $C_{j}$.\nWe are now ready to give a proof of Lemma 3.8.\nProof of Lemma 3.8. Due to Lemma 3.9, we know that we have at most $k$ point $A_{1}, \\ldots A_{k}$ sets such that any cluster of the optimum clustering is fully contained in one the $A_{i}$. We guess the correct number of centers from each $A_{i}$, which takes at most $\\binom{2 k-1}{k-1}$ guesses. For each $A_{i}$, we then find a set of centers $M$ that $\\varepsilon$ covers all clusters of the optimum in $A_{i}$.\n\nWe simplify the calculation by assuming that $A_{i}$ contains all $k$ clusters. We iteratively add centers to $M$, writing $M_{j}$ after the $j$-th iteration. Our goal is to ensure that $M_{j}$ covers the clusters $C_{1}, \\ldots C_{j}$. In every iteration, we first sample to obtain a suitable mean seeding set and then apply Lemma 3.6 to extract the mean from the set.\n\nWe start with $C_{1}$. We know that $\\left|C_{1}\\right| \\geq \\frac{9}{k}$, so we can use Lemma 3.7 to sample a set $S_{1}$ of $32 k \\varepsilon^{-1} \\log (k / \\delta)$ points uniformly at random and then enumerate over all candidate means induced by uniformly weighted subsets of $S_{1}$ and the to obtain an $\\varepsilon$-approximate mean of $C_{1}$. This takes time $2^{\\left|S_{1}\\right|}$ and yields $2^{\\left|S_{1}\\right|}$ candidate means, of which one is an $\\varepsilon$-covers $C_{1}$ with probability $1-\\delta / k$.\n\nFor subsequent iterations, Lemma 3.16 guarantees us that there exists a distribution $\\mathbb{P}_{i}$ such that if we sample a set $S_{j}$ of $17825792 \\cdot k\\left(\\frac{\\beta^{7 / 12}}{\\varepsilon}\\right)^{6} \\log (2 k / \\delta)$ points, then $M_{j-1} \\cup S$ is an $(\\varepsilon / 4, \\beta)$ mean\n\nseeding set of $C_{j}$ with probability $1-\\delta / k$. Moreover, Lemma 3.12 guarantees us that we have to try at most $13 \\log (n k / \\varepsilon)$ distributions to do find the correct $\\mathbb{P}_{i}$. Extracting all candiate means for each $\\mathbb{P}_{i}$ via Lemma 3.6 takes time $\\left(\\frac{10 \\beta \\cdot\\left|S_{j}\\right|}{\\varepsilon}+1\\right)^{\\left|S_{j}\\right|}$ and results in $\\left(\\frac{10 \\beta \\cdot\\left|S_{j}\\right|}{\\varepsilon}+1\\right)^{\\left|S_{j}\\right|}$ candidate means.\n\nThus, the overall number of candidate centers $M_{k}$ generated by the procedure, as well as the running time, is\n\n$$\n2^{\\left|S_{1}\\right|} \\cdot \\prod_{j=2}^{k} 13 \\log (n k / \\varepsilon) \\cdot\\left(\\frac{10 \\beta \\cdot\\left|S_{j}\\right|}{\\varepsilon}+1\\right)^{\\left|S_{j}\\right|}=\\log ^{k} n \\cdot 2^{\\eta \\cdot k^{2} \\cdot \\varepsilon^{-12} \\log ^{2}(k /(\\varepsilon \\delta))}\n$$\n\nfor some absolute constant $\\eta$. Moreover by the union bound, one of the $M_{k}$ must $\\varepsilon$ cover all clusters with probability $1-\\delta$. Notice that if $\\log n<k^{2}$, then $\\log ^{k} n$ is absorbed by $2^{\\eta \\cdot k^{2} \\cdot \\varepsilon^{-12} \\log ^{2}(k /(\\varepsilon \\delta))}$ with a suitable rescaling of $\\eta$. If $\\log n>k^{2}$, then $\\log ^{k} n<2 \\sqrt{\\log n} \\log \\log n<n^{o(1)}$.\n\nWe account for the enumeration over the number of clusters from each $A_{i}$ via another rescaling of $\\eta$. For a given $M$ and $\\mathbb{P}_{i}$, the probabilities can be computed in time $O(n \\cdot d \\cdot|M|)$ Thus, the overall running time to obtain a set of centers that $\\varepsilon$ covers all clusters of the optimum is\n\n$$\nn^{1+o(1)} \\cdot d \\cdot 2^{\\eta \\cdot k^{2} \\cdot \\varepsilon^{-12} \\log ^{2}(k /(\\varepsilon \\delta))}\n$$\n\nand this completes the proof.\nEnumerating over Sizes and Obtaining the Parameterized PTAS. We complete this section by funneling the mean-seeding procedure into a PTAS.\n\nTheorem 3.17. There exists an algorithm running in time\n\n$$\nO\\left(n^{1+o(1)} d \\cdot 2^{\\eta \\cdot k^{2} \\cdot \\varepsilon^{-12} \\log ^{2}(k /(\\varepsilon \\delta))}\\right)\n$$\n\nfor some absolute constant $\\eta$, that computes a $(1+\\varepsilon)$-approximate solution to $\\ell_{2}^{2} k$-MinSum Clustering with probability $1-\\delta$.\n\nProof. Given a set of candidate centers obtained via Lemma 3.8 and an estimate $\\widehat{O P T}$ of the optimal MinSum clustering cost OPT, we wish to find an assignment of points to centers such that the clustering that has cost $(1+\\varepsilon) \\cdot \\widehat{\\mathrm{OPT}}$, or verify that no such assignment exists. Note that given a clustering, we can verify its cost in time $O(n d k)$ by computing the mean of every cluster and then using the first identity of Lemma 3.2.\n\nWe first notice that if we are given an $\\alpha$-approximation $\\widehat{\\mathrm{OPT}}_{\\text {kmeans }}$ to an $k$-means clustering $\\mathrm{OPT}_{\\text {kmeans }}$, we also know $\\mathrm{OPT} \\in\\left[\\widehat{\\mathrm{OPT}}_{\\text {kmeans }}, n \\cdot \\widehat{\\mathrm{OPT}}_{\\text {kmeans }}\\right]$. A constant, say 20, approximation to $k$ means can be found in time $\\tilde{O}\\left(n d+k^{2}\\right)$ [DSS24]. We thus can efficiently obtain $(1+\\varepsilon)$ approximate value of OPT using at most $2 \\varepsilon^{-1} \\log (20 n)$ estimates.\n\nSuppose we are given $\\widehat{\\mathrm{OPT}}$, as well as a candidate set of centers $C=\\left\\{c_{1}, c_{2}, \\ldots c_{k}\\right\\}$. Now, we discretize the cost of all points to each cluster $c_{i}$, starting at $\\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}}$ by powers of $(1+\\varepsilon)$, going all the way up to $\\widehat{\\mathrm{OPT}}$. Define\n\n$$\nG_{i, j}=\\left\\{p \\mid(1+\\varepsilon)^{j-1} \\cdot \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}} \\leq\\left\\|p-c_{i}\\right\\|^{2} \\leq(1+\\varepsilon)^{j} \\cdot \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}}\\right\\}\n$$\n\nwith $G_{i, 0}=\\left\\{p \\mid\\left\\|p-c_{i}\\right\\|^{2} \\leq \\frac{\\varepsilon}{n} \\cdot \\widehat{\\mathrm{OPT}}\\right\\}$. Notice that if $\\left\\|p-c_{i}\\right\\|^{2}>(1+\\varepsilon) \\cdot \\widehat{\\mathrm{OPT}}$, then $p$ cannot be served by $c_{i}$ without invalidating $\\widehat{O P T}$ as an accurate estimate of OPT. Thus we have at most $2 \\varepsilon^{-1} \\log \\frac{n^{2}}{\\varepsilon}$ many sets $G_{i, j}$. Finally, consider the set $B_{1_{j}, 2_{j} \\sigma, 1_{j} \\sigma, \\ldots}$ which is the intersection of $G_{1, j} \\cap G_{2, j^{\\prime}} \\cap G_{2, j^{\\prime \\prime}} \\ldots$. Notice that there are $\\left(2 \\varepsilon^{-1} \\log \\frac{n^{2}}{\\varepsilon}\\right)^{k}$ many sets $B$ and that we can compute the partitioning of the point set $A$ into the sets $B$ in time $n d k \\cdot\\left(2 \\varepsilon^{-1} \\log \\frac{n^{2}}{\\varepsilon}\\right)^{k}$. We finally discretize the size of subsets of any set $B$ by powers of $(1+\\varepsilon)$, for which there are $2 \\varepsilon^{-1} \\log |B| \\leq 2 k \\varepsilon^{-1} \\log n$ discretizations.\n\nWe now enumerate over all possible assignments of subsets of sets $B$ to centers $c_{i}$. Notice that there are at most $\\left(2 \\varepsilon^{-1} \\log n\\right)^{k}$ possible sizes, which we multiply by the number $\\left(2 \\varepsilon^{-1} \\log \\frac{n}{\\varepsilon}\\right)^{k}$ of sets $B$.\n\nWe claim that if $C$ is the center set of a $(1+\\varepsilon)$-approximate solution, then there exists an assignment of the $B$ that is $(1+O(\\varepsilon))$ approximate as well. Specifically, consider any assignment $\\pi: A \\rightarrow C \\operatorname{cost} \\operatorname{cost}_{\\pi}(A, C)=\\sum_{p \\in A}\\left\\|p-\\pi(p)\\right\\|^{2}$. In the following, we use $B_{j}$ to refer to the intersection of $B$ with $C_{i}$, i.e. $B_{i, j}=C_{i} \\cap B_{1_{j}, 2_{j} \\sigma, \\ldots,}$. Then rewriting the sum, we obtain\n\n$$\n\\begin{aligned}\n\\sum_{p \\in C_{i}}\\left(\\sum_{j}\\left|B_{i, j}\\right|\\right) \\sum_{j>0}\\left|B_{i, j}\\right| \\cdot 2^{j-1} \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}} & \\leq \\operatorname{cost}_{\\pi}(A, C) \\\\\n& \\leq(1+\\varepsilon) \\cdot \\sum_{p \\in C_{i}}\\left(\\sum_{j}\\left|B_{i, j}\\right|\\right) \\sum_{j>0}\\left|B_{i, j}\\right| \\cdot 2^{j} \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}}+n^{2} \\cdot \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}} \\\\\n& =(1+\\varepsilon) \\cdot \\sum_{p \\in C_{i}}\\left(\\sum_{j}\\left|B_{i, j}\\right|\\right) \\sum_{j>0}\\left|B_{i, j}\\right| \\cdot 2^{j} \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}}+\\varepsilon \\cdot \\widehat{\\mathrm{OPT}}\n\\end{aligned}\n$$\n\nand moreover\n\n$$\n(1+\\varepsilon) \\cdot \\sum_{p \\in C_{i}}\\left(\\sum_{j}\\left|C_{i, j}\\right|\\right) \\sum_{j>0}\\left|C_{i, j}\\right| \\cdot 2^{j} \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}} \\leq(1+\\varepsilon) \\cdot \\sum_{p \\in C_{i}}\\left(\\sum_{j}\\left|C_{i, j}\\right|\\right) \\sum_{j>0}\\left|C_{i, j}\\right| \\cdot 2^{j-1} \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}}\n$$\n\nIn other words, using the discretizations $B$ instead of the correct points in the assignment of $A$ to $C$ preserves the cost up to a multiplicative factor $(1+\\varepsilon)$ and an additive $\\varepsilon \\cdot \\widehat{\\mathrm{OPT}}$.\n\nNext, observe that if we have an estimate $\\left|B_{i, j}\\right| \\leq B_{i, j}^{\\prime} \\leq(1+\\varepsilon) \\cdot\\left|B_{i, j}\\right|$, then $\\sum_{j}\\left|B_{i, j}\\right| \\leq \\sum_{j} B_{i, j}^{\\prime} \\leq$ $(1+\\varepsilon) \\cdot \\sum_{j}\\left|B_{i, j}\\right|$. Therefore, using the discretized estimates of $\\left|B_{i, j}\\right|$, we also have\n\n$$\n\\begin{aligned}\n\\sum_{p \\in C_{i}}\\left(\\sum_{j} B_{i, j}^{\\prime}\\right) \\sum_{j>0} B_{i, j}^{\\prime}\\left|\\cdot 2^{j-1} \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}} \\leq\\right. & \\operatorname{cost}_{\\pi}(A, C) \\\\\n& \\leq(1+\\varepsilon)^{3} \\sum_{p \\in C_{i}}\\left(\\sum_{j} B_{i, j}^{\\prime}\\right) \\sum_{j>0} B_{i, j}^{\\prime} \\cdot 2^{j} \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}}+\\varepsilon \\cdot \\widehat{\\mathrm{OPT}}\n\\end{aligned}\n$$\n\nGiven a (discretized) assignment of the sets $B$ to $C$, we can now extract a clustering as follows. In the following the value of $j$ is not necessary so we omit the subscript $j$ from $B_{i, j}$. We sort $\\hat{B}_{i}$ by sizes, breaking ties arbitrarily. We assign $\\hat{B}_{i}$ many arbitrary points of $B$ to cluster $C_{i}$ with center $c_{i}$. The final cluster $C_{i^{\\prime}}$ in the ordering is assigned the remaining points. Notice that assigning fewer points to $C_{i^{\\prime}}$ can only decrease the cost of $C_{i^{\\prime}}$.\n\nThe cost of this assignment can only be cheaper than the estimated upper bound\n\n$$\n(1+\\varepsilon)^{3} \\sum_{p \\in C_{i}}\\left(\\sum_{j} B_{i, j}\\right) \\sum_{j>0} B_{i, j} \\cdot 2^{j} \\frac{\\varepsilon}{n^{2}} \\cdot \\widehat{\\mathrm{OPT}}+\\varepsilon \\cdot \\widehat{\\mathrm{OPT}}\n$$\n\nas we can only assign fewer points from every group $B$ to a cluster and the cost of the points can only be cheaper than the estimated upper bound. As mentioned above, evaluating the cost of the resulting clustering takes time $O(n d k)$.\n\nThus, assuming that $\\mathrm{OPT} \\leq O \\bar{P} T \\leq(1+\\varepsilon) \\cdot \\mathrm{OPT}$ and that we were working with a suitable $\\varepsilon$ approximate candidate set of centers $C$, we can extract a clustering with cost at most $(1+\\varepsilon)^{5} \\cdot$ OPT in time $O\\left(n d\\left(2 \\varepsilon^{-1} \\log n\\right)^{k} \\cdot\\left(2 \\varepsilon^{-1} \\log \\frac{n}{\\varepsilon}\\right)^{k}\\right)$ multiplying this figure by the number of candidate values of $O P T$ and the number of candidate centers obtained via Lemma 3.8 yields a running time of\n\n$$\nO\\left(n d \\cdot\\left(2 \\varepsilon^{-1} \\log \\frac{20 n}{\\varepsilon}\\right)^{3 k} \\cdot n^{o(1)} \\cdot 2^{\\eta \\cdot k^{2} \\cdot \\varepsilon^{-12} \\log ^{2}(k /(\\varepsilon \\delta))}\\right)\n$$\n\nplus the running time for computing the candidate centers. Using $(\\log n)^{k} \\leq k^{3 k}+2 \\sqrt{\\log n} \\log \\log n \\leq$ $k^{3 k}+n^{o(1)}$, rescaling $\\varepsilon$ by a factor of 10 , this yields a $(1+\\varepsilon)$ approximation with probability $1-\\delta$ in time\n\n$$\nO\\left(n^{1+o(1)} d \\cdot 2^{\\eta \\cdot k^{2} \\cdot \\varepsilon^{-12} \\log ^{2}(k /(\\varepsilon \\delta))}\\right)\n$$\n\nfor some absolute constant $\\eta$.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# 4 Learning-Augmented $\\ell_{2}^{2}$ Min-Sum $k$-Clustering \n\nIn this section, we describe and analyze our learning-augmented algorithm for $\\ell_{2}^{2} \\mathrm{~min}$-sum $k$ clustering, corresponding to Theorem 1.5.\n\nWe first recall the following property describing the 1-means optimizer for a set of points.\nFact 4.1. [IKI94] Given a set $X \\subset \\mathbb{R}^{d}$ of points, the unique minimizer of the 1-means objective is\n\n$$\n\\frac{1}{|X|} \\sum_{x \\in X} x=\\underset{c \\in \\mathbb{R}^{d}}{\\operatorname{argmin}} \\sum_{x \\in X}\\|x-c\\|_{2}^{2}\n$$\n\nWe next recall the following identity, which presents an equivalent formulation of the $\\ell_{2}^{2}$ min$\\operatorname{sum} k$-clustering objective.\n\nFact 4.2. [IKI94] For each cluster $C_{i}$ of points, let $c_{i}$ be the geometric mean of the points, i.e.,\n\n$$\nc_{i}=\\frac{1}{\\left|C_{i}\\right|} \\sum_{x \\in C_{i}} x\n$$\n\nThen\n\n$$\n\\frac{1}{2} \\sum_{i \\in[k]} \\sum_{x_{u}, x_{v} \\in C_{i}}\\left\\|x_{u}-x_{v}\\right\\|_{2}^{2}=\\sum_{i \\in[k]}\\left|C_{i}\\right| \\cdot \\sum_{x \\in C_{i}}\\left\\|x-c_{i}\\right\\|_{2}^{2}\n$$\n\nGiven Fact 4.2, it is more convenient for us to rescale the $\\ell_{2}^{2}$ min-sum $k$-clustering objective for an input set $X$ in this section to be defined as:\n\n$$\n\\min _{C_{1}, \\ldots, C_{k}} \\frac{1}{2} \\sum_{i \\in[k]} \\sum_{p, q \\in C_{i} \\cap X}\\|p-q\\|_{2}^{2}\n$$\n\nWe now formally define the precision and recall guarantees of a label predictor.\nDefinition 4.3 (Label predictor). Suppose that there is an oracle that produces a label $i \\in[k]$ for each $x \\in X$, so that the labeling partitions $X=P_{1} \\dot{\\cup} \\ldots \\dot{\\cup} P_{k}$ into $k$ clusters $P_{1}, \\ldots, P_{k}$, where all points in $P_{i}$ have the same label $i \\in[k]$. We say the oracle is a label predictor with error rate $\\alpha$ if there exists some fixed optimal min-sum clustering $P_{1}^{*}, \\ldots, P_{k}^{*}$ such that for all $i \\in[k]$,\n\n$$\n\\left|P_{i} \\cap P_{i}^{*}\\right| \\geq(1-\\alpha) \\max \\left(\\left|P_{i}\\right|,\\left|P_{i}^{*}\\right|\\right)\n$$\n\nWe say that $P^{*}=\\left\\{P_{1}^{*}, \\ldots, P_{k}^{*}\\right\\}$ is the clustering consistent with the label oracle.\nWe also recall the following guarantees of previous work on learning-augmented $k$-means clustering for a label predictor with error rate $\\alpha \\in\\left[0, \\frac{1}{2}\\right)$.\n\nTheorem 4.4. [NCN23] Given a label predictor with error rate $\\alpha<\\frac{1}{2}$ consistent with some clustering $P^{*}=$ $\\left\\{P_{1}^{*}, \\ldots, P_{k}^{*}\\right\\}$ with centers $\\left\\{c_{1}^{*}, \\ldots, c_{k}^{*}\\right\\}$, there exists a polynomial-time algorithm LeARNEDCENTERS that outputs a set of centers $\\left\\{c_{1}, \\ldots, c_{k}\\right\\}$, so that for each $i \\in[k]$,\n\n$$\n\\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}\\right\\|_{2}^{2} \\leq\\left(1+\\gamma_{\\alpha} \\alpha\\right) \\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}^{*}\\right\\|_{2}^{2}\n$$\n\nwhere $\\gamma_{\\alpha}=7.7$ for $\\alpha \\in\\left[0, \\frac{1}{2}\\right)$ or $\\gamma_{\\alpha}=\\frac{5 \\alpha-2 \\alpha^{2}}{(1-2 \\alpha)(1-\\alpha)}$ for $\\alpha \\in\\left[0, \\frac{1}{2}\\right)$.\nDescription of LeARNEDCenters. For the sake of completeness, we briefly describe the algorithm LeARNEDCENTERS underlying Theorem 4.4. The algorithm decomposes the $k$-means clustering objective by considering the subset $P_{i}$ of the input dataset $X$ that are assigned each label $i \\in[k]$ by the oracle. The algorithm further decomposes the $k$-means clustering objective along the $d$ dimensions, by considering the $j$-th coordinate of each subset $P_{i}$, for each $j \\in[d]$. Now, although an $\\alpha$ fraction of the points in $P_{i}$ can be incorrectly labeled, there are two main cases: 1) $P_{i}$ includes a number of mislabeled points that are far from the true mean and hence easy to prune away, or 2) $P_{i}$ includes a number of mislabeled points that are difficult to identify due to their proximity to the true mean. However, in the latter case, these mislabeled points only has a small effect on the overall $k$-means clustering objective. Hence, it suffices for the algorithm to handle the first case, which it does by selecting the interval of $(1-\\mathcal{O}(\\alpha))$ points of $P_{i}$ in dimension $j$ that has the best clustering cost. The mean of the points of $P_{i}$ in dimension $j$ that lie in that interval then forms the $j$-th coordinate of the $i$-th centroid output by algorithm. The algorithm repeats across $j \\in[d]$ and $i \\in[k]$ to form $k$ centers that are well-defined in all $d$ dimensions. We give the algorithm formally in Algorithm 1.\n\nBy Fact 4.1 and Fact 4.2, it follows that these centers are also good centers for the clustering induced by a near-optimal $\\ell_{2}^{2}$ min-cost $k$-clustering. Specifically, the optimal center of a cluster of points for $\\ell_{2}^{2}$ min-cost $k$-clustering is the centroid of the cluster and similarly, the optimal center of a\n\n```\nAlgorithm 1 LEARNEDCENTERS: learning-augmented \\(k\\)-means clustering [NCN23]\nInput: Dataset \\(X\\) with partition \\(P_{1}, \\ldots, P_{k}\\) induced by label predictor with error rate \\(\\alpha\\)\nOutput: Centers \\(c_{1}, \\ldots, c_{k}\\) for \\((1+\\mathcal{O}(\\alpha))\\)-optimal \\(k\\)-means clustering\n    for \\(i \\in[k]\\) do\n        for \\(j \\in[d]\\) do\n            Let \\(\\omega_{i, j}\\) be the collection of all intervals that contain \\((1-\\mathcal{O}(\\alpha))\\left|P_{i}\\right|\\) points of \\(P_{i, j}\\)\n            Let \\(c_{i, j}\\) be the center with the lowest \\(k\\)-means clustering cost of any interval in \\(\\omega_{i, j}\\)\n            \\(c_{i} \\leftarrow\\left\\{c_{i, j}\\right\\}_{j \\in[d]}\\) for all \\(i \\in[d]\\)\n            return \\(\\left\\{c_{1}, \\ldots, c_{k}\\right\\}\\)\n```\n\ncluster of points for $k$-means clustering is the centroid of the cluster. See Lemma 4.6 for the formal details.\n\nUnfortunately, although the centers $\\left\\{c_{1}, \\ldots, c_{k}\\right\\}$ returned by LEARNEDCENTERS are good centers for the clustering induced by a near-optimal $\\ell_{2}^{2}$ min-cost $k$-clustering, it is not clear what the resulting assignment should be. In fact, we emphasize that unlike $k$-means clustering, the optimal $\\ell_{2}^{2}$ min-cost $k$-clustering may not assign each point to its closest center.\n\nConstrained min-cost flow. To that end, we now create a constrained min-cost flow problem as follows. We first create a source node $s$ and a sink node $t$ and require that $n=|X|$ flow must be pushed from $s$ to $t$. We create a node $u_{x}$ for each point $x \\in X$ and create a directed edge from $s$ to each node $u_{x}$ with capacity 1 and cost 0 . There are no more outgoing edges from $s$ or incoming edges to each $u_{x}$. This ensures that to achieve $n$ flow from $s$ to $t$, a unit of flow must be pushed across each node $u_{x}$.\n\nFor each center $c_{i}$ output by our learning-augmented algorithm, we create a node $v_{i}$. For each $x \\in X, i \\in[k]$, create a directed edge from $u_{x}$ to $v_{i}$ with capacity 1 and $\\operatorname{cost} \\frac{1}{1-\\alpha} \\cdot\\left|P_{i}\\right| \\cdot\\left\\|x-c_{i}\\right\\|_{2}^{2}$. There are no other outgoing edges from $u_{x}$, thus ensuring that a unit of flow must exit each node $u_{x}$ to the nodes $v_{i}$ representing the clusters, and with approximately the corresponding cost if $x$ were assigned to center $c_{i}$. We then create a directed edge from each node $v_{i}$ to $t$ with capacity $\\frac{1}{1-\\alpha} \\cdot\\left|P_{i}\\right|$ and cost 0 . Finally, we require that at least $(1-\\alpha) \\cdot\\left|P_{i}\\right|$ flow goes through node $v_{i}$, so that the number of points assigned to each center $c_{i}$ is consistent with the oracle. The construction in its entirety appears in Figure 4.\n\n```\nAlgorithm 2 Learning-augmented min-sum \\(k\\)-clustering\nInput: Dataset \\(X\\) with partition \\(P_{1}, \\ldots, P_{k}\\) induced by label predictor with error rate \\(\\alpha\\)\nOutput: Labels for all points consistent with a \\((1+\\mathcal{O}(\\alpha))\\)-optimal min-sum \\(k\\)-clustering\n    Let \\(c_{1}, \\ldots, c_{k}\\) be the output centers of LEARNEDCENTERS on \\(P_{1}, \\ldots, P_{k}\\)\n    Create a min-cost flow problem \\(\\mathcal{F}\\) with required flow \\(n\\) as in Figure 4\n    Solve the flow problem \\(\\mathcal{F}\\)\n    For each \\(x \\in X\\), let the flow from \\(u_{x}\\) be sent to the node \\(v_{\\ell_{x}\\), so that \\(\\ell_{x} \\in[k]\\)\n    Label \\(x\\) with \\(\\ell_{x}\\)\n```\n\nWe first show that the $\\ell_{2}^{2}$ min-sum $k$-clustering cost induced by Algorithm 2 has objective value at most the cost of the optimal flow in the problem $\\mathcal{F}$ created by Algorithm 2.\n\nLet $X=P_{1} \\cup \\ldots \\cup P_{k}$ and $c_{1}, \\ldots, c_{k}$ be inputs\n(1) Create a source node $s$ and a sink node $t$, requiring $n=|X|$ flow from $s$ to $t$\n(2) Create a directed edge from $s$ to each node $u_{x}$ representing a separate $x \\in X$ with capacity 1 and cost 0\n(3) Create a directed edge to $t$ from each node $v_{i}$ representing a separate $c_{i}$ with capacity $\\left\\lfloor\\frac{1}{1-\\alpha} \\cdot\\left|P_{t}\\right|\\right\\rfloor$ and cost 0\n(4) Require that at least $\\left\\lceil(1-\\alpha) \\cdot\\left|P_{t}\\right|\\right\\rceil$ flow goes through node $c_{i}$\n(5) For each $x \\in X, i \\in[k]$, create a directed edge from $u_{x}$ to $v_{i}$ with capacity 1 and cost $\\frac{1}{1-\\alpha} \\cdot\\left|P_{t}\\right| \\cdot\\left\\|x-c_{i}\\right\\|_{2}^{2}$\n\nFig. 4: Constrained min-cost flow problem\n\nLemma 4.5. Let $F$ be the cost of the flow output by Algorithm 2. Then for the corresponding clustering $Q_{1}, \\ldots, Q_{k}$ output by Algorithm 2, we have\n\n$$\n\\frac{1}{2} \\sum_{i \\in[k]} \\sum_{x_{u}, x_{v} \\in Q_{i}}\\left\\|x_{u}-x_{v}\\right\\|_{2}^{2} \\leq F\n$$\n\nProof. Let $\\mathcal{S}$ be any flow output by Algorithm 2 and let $Q_{1}, \\ldots, Q_{k}$ be the corresponding clustering of $X$. Note that $Q_{1}, \\ldots, Q_{k}$ are well-defined, since each point of $x$ receives exactly one label by Algorithm 2. Let $q_{1}, \\ldots, q_{k}$ be the geometric mean of the points in $Q_{1}, \\ldots, Q_{k}$, respectively, so that $q_{i}=\\frac{1}{|Q_{i}|} \\sum_{x \\in Q_{i}} x$ for all $i \\in[k]$.\n\nBy Fact 4.1 and Fact 4.2, we have that\n\n$$\n\\begin{aligned}\n\\frac{1}{2} \\sum_{i \\in[k]} \\sum_{x_{u}, x_{v} \\in Q_{i}}\\left\\|x_{u}-x_{v}\\right\\|_{2}^{2} & =\\sum_{i \\in[k]}\\left|Q_{i}\\right| \\cdot \\sum_{x \\in Q_{i}}\\left\\|x-q_{i}\\right\\|_{2}^{2} \\\\\n& \\leq \\sum_{i \\in[k]}\\left|Q_{i}\\right| \\cdot \\sum_{x \\in Q_{i}}\\left\\|x-c_{i}\\right\\|_{2}^{2}\n\\end{aligned}\n$$\n\nSince each node $v_{i}$ has capacity $\\frac{1}{1-\\alpha} \\cdot\\left|P_{t}\\right|$, then we have $\\left|Q_{i}\\right| \\leq \\frac{1}{1-\\alpha} \\cdot\\left|P_{t}\\right|$. Therefore,\n\n$$\n\\frac{1}{2} \\sum_{i \\in[k]} \\sum_{x_{u}, x_{v} \\in Q_{i}}\\left\\|x_{u}-x_{v}\\right\\|_{2}^{2} \\leq \\sum_{i \\in[k]} \\frac{1}{1-\\alpha} \\cdot\\left|P_{t}\\right| \\cdot \\sum_{x \\in Q_{i}}\\left\\|x-c_{i}\\right\\|_{2}^{2}\n$$\n\nBecause each $x \\in Q_{i}$ is mapped to $c_{i}$, then the cost induced by the mapping in the flow $\\mathcal{S}$ is exactly $\\frac{1}{1-\\alpha} \\cdot\\left|P_{t}\\right| \\cdot\\left\\|x-c_{i}\\right\\|_{2}^{2}$. Therefore, the right-hand side is exactly the cost $F$ of the flow $\\mathcal{S}$. Hence, we have\n\n$$\n\\frac{1}{2} \\sum_{i \\in[k]} \\sum_{x_{u}, x_{v} \\in Q_{i}}\\left\\|x_{u}-x_{v}\\right\\|_{2}^{2} \\leq F\n$$\n\nas desired.\n\nWe next show that the cost of the optimal $\\ell_{2}^{2}$ min-sum $k$-clustering has objective value at least the cost of the optimal in the problem $\\mathcal{F}$ created by Algorithm 2, up to a $(1+\\mathcal{O}(\\alpha))$ factor.\n\nLemma 4.6. Let $F$ be the cost of the optimal solution to the min-cost flow problem $\\mathcal{F}$ in Algorithm 2 and let OPT be cost of the optimal min-sum $k$-clustering on $X$. Let $\\gamma_{\\alpha}$ be the fixed constant from Theorem 4.4. Then\n\n$$\n\\operatorname{OPT} \\geq(1-\\alpha)^{2} \\cdot \\frac{1}{1+\\gamma_{\\alpha} \\alpha} \\cdot F\n$$\n\nProof. Let $P_{1}^{*}, \\ldots, P_{k}^{*}$ be an optimal clustering consistent with the label oracle. Let $c_{1}^{*}, \\ldots, c_{k}^{*}$ be the optimal centers for $P_{1}^{*}, \\ldots, P_{k}^{*}$ respectively and let $c_{1}, \\ldots, c_{k}$ be the $k$ centers output by Algorithm 2.\n\nBy the definition of the label oracle, we have\n\n$$\n\\left|P_{i} \\cap P_{i}^{*}\\right| \\geq(1-\\alpha) \\max \\left(\\left|P_{i}\\right|,\\left|P_{i}^{*}\\right|\\right)\n$$\n\nso that\n\n$$\n\\left|P_{i}^{*}\\right| \\geq\\left|P_{i} \\cap P_{i}^{*}\\right| \\geq(1-\\alpha) \\max \\left(\\left|P_{i}\\right|,\\left|P_{i}^{*}\\right|\\right) \\geq(1-\\alpha) \\cdot\\left|P_{i}\\right|\n$$\n\nThus, by Fact 4.2,\n\n$$\n\\begin{aligned}\n\\frac{1}{2} \\sum_{i \\in[k]} \\sum_{x_{\\mu}, x_{v} \\in P_{i}^{*}}\\left\\|x_{u}-x_{v}\\right\\|_{2}^{2} & =\\sum_{i \\in[k]}\\left|P_{i}^{*}\\right| \\cdot \\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}^{*}\\right\\|_{2}^{2} \\\\\n& \\geq \\sum_{i \\in[k)}(1-\\alpha) \\cdot\\left|P_{i}\\right| \\cdot \\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}^{*}\\right\\|_{2}^{2} \\\\\n& =(1-\\alpha)^{2} \\sum_{i \\in[k]} \\frac{1}{1-\\alpha} \\cdot\\left|P_{i}\\right| \\cdot \\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}^{*}\\right\\|_{2}^{2}\n\\end{aligned}\n$$\n\nLet $\\gamma_{\\alpha}$ be the fixed constant from Theorem 4.4. Then by Theorem 4.4, we have that\n\n$$\n\\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}^{*}\\right\\|_{2}^{2} \\geq \\frac{1}{1+\\gamma_{\\alpha} \\alpha} \\cdot \\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}\\right\\|_{2}^{2}\n$$\n\nTherefore,\n\n$$\n\\frac{1}{2} \\sum_{i \\in[k]} \\sum_{x_{\\mu}, x_{v} \\in P_{i}^{*}}\\left\\|x_{u}-x_{v}\\right\\|_{2}^{2} \\geq(1-\\alpha)^{2} \\cdot \\frac{1}{1+\\gamma_{\\alpha} \\alpha} \\cdot \\sum_{i \\in[k]} \\frac{1}{1-\\alpha} \\cdot\\left|P_{i}\\right| \\cdot \\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}\\right\\|_{2}^{2}\n$$\n\nNote that since $\\left|P_{i}\\right| \\geq\\left|P_{i} \\cap P_{i}^{*}\\right| \\geq(1-\\alpha) \\max \\left(\\left|P_{i}\\right|,\\left|P_{i}^{*}\\right|\\right) \\geq(1-\\alpha) \\cdot\\left|P_{i}^{*}\\right|$, then we have $\\left|P_{i}^{*}\\right| \\leq$ $\\frac{1}{1-\\alpha} \\cdot\\left|P_{i}^{*}\\right|$. Thus a valid flow for $\\mathcal{F}$ would be to send $\\left|P_{i}^{*}\\right|$ units of flow across each $x \\in P_{i}^{*}$. In other words, $\\sum_{i \\in[k]} \\frac{1}{1-\\alpha} \\cdot\\left|P_{i}\\right| \\cdot \\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}\\right\\|_{2}^{2}$ is the cost of a valid flow for $\\mathcal{F}$.\n\nTherefore, by the optimality of the optimal min-cost flow, we have\n\n$$\n\\sum_{i \\in[k]} \\frac{1}{1-\\alpha} \\cdot\\left|P_{i}\\right| \\cdot \\sum_{x \\in P_{i}^{*}}\\left\\|x-c_{i}\\right\\|_{2}^{2} \\geq F\n$$\n\nand so\n\n$$\n\\frac{1}{2} \\sum_{i \\in[k]} \\sum_{x_{\\mu}, x_{v} \\in P_{i}^{*}}\\left\\|x_{u}-x_{v}\\right\\|_{2}^{2} \\geq(1-\\alpha)^{2} \\cdot \\frac{1}{1+\\gamma_{\\alpha} \\alpha} \\cdot F\n$$\n\nas desired.\n\nPutting together Lemma 4.5 and Lemma 4.6, it follows that the cost of the clustering induced by Algorithm 2 is a good approximation to the optimal $\\ell_{2}^{2}$ min-sum $k$-clustering.\n\nCorollary 4.7. Let $\\gamma_{\\alpha}$ be the fixed constant from Theorem 4.4. Algorithm 2 outputs a clustering $Q_{1}, \\ldots, Q_{k}$ of $X$ such that\n\n$$\n\\frac{1}{2} \\sum_{i \\in[k]} \\sum_{x_{\\alpha}, x_{\\nu} \\in Q_{i}}\\left\\|x_{u}-x_{\\nu}\\right\\|_{2}^{2} \\leq \\frac{1+\\gamma_{\\alpha} \\alpha}{(1-\\alpha)^{2}} \\cdot \\mathrm{OPT}\n$$\n\nwhere OPT is cost of an optimal min-sum $k$-clustering on $X$.\nProof. Let $\\mathcal{S}$ be the flow output by Algorithm 2 and let $Q_{1}, \\ldots, Q_{k}$ be the corresponding clustering of $X$. We again remark that $Q_{1}, \\ldots, Q_{k}$ is a valid clustering of $X$, since each point of $x$ receives exactly one label by Algorithm 2. The claim then follows from Lemma 4.5 and Lemma 4.6.\n\nWe recall the following folklore integrality theorem for uncapacitated min-cost flow.\nTheorem 4.8. Any minimum cost network flow problem with integral demands has an optimal solution with integral flow on each edge.\n\nProof. Though the proof is well-known, e.g., [Con12], we repeat it here for the sake of completeness. Consider induction on $n$, the number of nodes in the flow graph. The statement is vacuously true for $n=0$ and $n=1$, which serve as our base cases. Observe that we can write the linear program with $n-1$ constraints and thus there exists an optimal solution where at most $n-1$ edges have positive flow. By a simple averaging argument, there exists a vertex $v$ that has at most one incident edge $e$ with positive flow. Let $u$ be the other endpoint of the the edge $e=(u, v)$. Since it is the only edge incident to $v$, it must satisfy the entire demand of $v$. Because $v$ has integer demand, then $e$ has integer flow. However, the remainder of the graph has $n-1$ vertices and thus by induction, the remaining of the vertex demands are satisfied by a flow with integer demands.\n\nWe now adjust the integrality theorem to handle capacitated edges, thereby showing that the resulting solution for the min-cost flow problem in Figure 4 is integral.\n\nCorollary 4.9. Any minimum cost network flow problem with integral demands and capacities has an optimal solution with integral flow on each edge.\n\nProof. The proof follows from a simple gadget to transform a min-cost flow problem with integercapacitated edges into an uncapacitated min-cost flow problem. Suppose there exists a directed edge $e$ from $u$ to $v$ with capacity $c$ and cost $p$. Suppose furthermore that $u$ has demand $d_{1}$ and $v$ has demand $d_{2}$. Then we create an additional vertex $w$ and we replace $e$ with directed edges $e_{1}$ going from $u$ to $w$ and $e_{2}$ going from $v$ to $w$. We change the demand of $v$ to $d_{2}-c$, noting this can be negative. We also require vertex $w$ to have demand $c$. We then have cost $p$ on edge $e_{2}$ and cost 0 on edge $e_{2}$. See Figure 5 for an illustration of the transformation. Since the resulting graph after the reduction does not have any capacities on the edges, it follows from Theorem 4.8 that there exists an integral solution to the original input problem.\n\nHence, the min-cost flow solution defines a valid clustering that approximately optimal with respect to the $\\ell_{2}^{2}$ min-sum $k$-clustering objective. However, we further want to show the property holds for the solution returned by a linear program solver. In fact, it is well-known the constraint matrix is totally unimodular, i.e., all submatrices have determinant $-1,0$, or 1 .\n\n![img-5.jpeg](img-5.jpeg)\n\nFig. 5: Example of transformation of capacitated min-cost flow problem into uncapacitated min-cost flow problem.\n\nTheorem 4.10 (Theorem 19.1 in [Sch98]). Let $A$ be a totally unimodular matrix and let $b$ be an integer vector. Then all vertices of the polyhedron $P=\\{x \\mid A x \\leq b\\}$ are integral.\n\nSince the solution of a linear program must lie at a vertex of the feasible polytope, then Theorem 4.10 implies any solution to the linear program will also be integral. Thus a valid clustering can be recovered by using the output of a linear program solver. We recall the following various implementations of solvers for linear programs.\n\nTheorem 4.11. [Kar84, Vai89, Vai90, LS15, LSZ19, CLS21, [SWZ21] There exists an algorithm that solves a linear program with $n$ variables that can be encoded in $L$ bits, using $\\operatorname{poly}(n, L)$ time.\n\nPutting things together, we have the following guarantees for our learning-augmented algorithm.\n\nTheorem 4.12. There exists a polynomial-time algorithm that uses a label predictor with error rate $\\alpha$ and outputs a $\\frac{1+\\gamma_{\\alpha} \\alpha}{(1-\\alpha)^{2}}$-approximation to min-sum $k$-clustering, where $\\gamma_{\\alpha}$ is the fixed constant from Theorem 4.4.\n\nProof. Correctness follows from Corollary 4.7.\nFor the runtime analysis, first observe that the centers $c_{1}, \\ldots, c_{k}$ can be computed in polynomial time by Theorem 4.4. Subsequently, $\\mathcal{F}$ can be written as a linear programming problem with at most poly $(n)$ constraints and variables. Therefore, the desired claim follows by running any polynomial-time linear programming solver, i.e., Theorem 4.11 and observing that the output solution induces a valid clustering, by Theorem 4.10.", "tables": {}, "images": {"img-5.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 14, "text": "# Acknowledgements \n\nThe work was conceptualized while all the authors were visiting the Institute for Emerging CORE Methods in Data Science (EnCORE) supported by the NSF grant 2217058. Karthik C. S. was supported by the National Science Foundation under Grants CCF-2313372 and CCF-2443697, a grant from the Simons Foundation, Grant Number 825876, Awardee Thu D. Nguyen, and partially funded by the Ministry of Education and Science of Bulgaria's support for INSAIT, Sofia University \"St. Kliment Ohridski\" as part of the Bulgarian National Roadmap for Research Infrastructure. Euiwoong Lee was supported in part by NSF grant CCF-2236669 and Google. Yuval Rabani was supported in part by ISF grants 3565-21 and 389-22, and by BSF grant 2023607. Chris\n\nSchwiegelshohn was partially supported by the Independent Research Fund Denmark (DFF) under a Sapere Aude Research Leader grant No 1051-00106B. Samson Zhou is supported in part by NSF CCF-2335411. The work was conducted in part while Samson Zhou was visiting the Simons Institute for the Theory of Computing as part of the Sublinear Algorithms program.", "tables": {}, "images": {}}, {"section_id": 15, "text": "# References \n\n[ACE +23$]$ Antonios Antoniadis, Christian Coester, Marek Eli\u00e1s, Adam Polak, and Bertrand Simon. Online metric algorithms with untrusted predictions. ACM Trans. Algorithms, 19(2):19:1-19:34, 2023. 3\n[ACI22] Anders Aamand, Justin Y. Chen, and Piotr Indyk. (optimal) online bipartite matching with degree information. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems, NeurIPS, 2022. 3\n[ADHP09] Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. Np-hardness of euclidean sum-of-squares clustering. Mach. Learn., 75(2):245-248, 2009. 1, 2, 7\n[AGKP22] Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. Online algorithms with multiple predictions. In International Conference on Machine Learning, ICML, pages 582-598, 2022. 3\n[AKP24] Enver Aman, Karthik C. S., and Sharath Punna. On connections between k-coloring and Euclidean k-means. In 32nd Annual European Symposium on Algorithms, ESA 2024, 2024. To appear. 2\n[AKS11] Per Austrin, Subhash Khot, and Muli Safra. Inapproximability of vertex cover and independent set in bounded degree graphs. Theory Comput., 7(1):27-43, 2011. 9, 10\n[APT22] Yossi Azar, Debmalya Panigrahi, and Noam Touitou. Online graph algorithms with predictions. In Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 35-66, 2022. 3\n[AV07] David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In Nikhil Bansal, Kirk Pruhs, and Clifford Stein, editors, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, New Orleans, Louisiana, USA, January 7-9, 2007, pages 1027-1035. SIAM, 2007. 5\n[BB09] Maria-Florina Balcan and Mark Braverman. Finding low error clusterings. In COLT 2009 - The 22nd Conference on Learning Theory, 2009. 7\n$\\left[\\mathrm{BBC}^{+}\\right.$19] Luca Becchetti, Marc Bury, Vincent Cohen-Addad, Fabrizio Grandoni, and Chris Schwiegelshohn. Oblivious dimension reduction for k-means: beyond subspaces and the johnson-lindenstrauss lemma. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC, pages 1039-1050, 2019. 7, 17\n[BBG09] Maria-Florina Balcan, Avrim Blum, and Anupam Gupta. Approximate clustering without the approximation. In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 1068-1077, 2009. 7\n\n[BCP+24] Nikhil Bansal, Vincent Cohen-Addad, Milind Prabhu, David Saulpic, and Chris Schwiegelshohn. Sensitivity sampling for k-means: Worst case and stability optimal coreset bounds. CoRR, abs/2405.01339, 2024. 7\n[BCR01] Yair Bartal, Moses Charikar, and Danny Raz. Approximating min-sum k-clustering in metric spaces. In Proceedings on 33rd Annual ACM Symposium on Theory of Computing, pages 11-20, 2001. 1, 2\n[BFSS19] Babak Behsaz, Zachary Friggstad, Mohammad R. Salavatipour, and Rohit Sivakumar. Approximation algorithms for min-sum k-clustering and balanced k-median. Algorithmica, 81(3):1006-1030, 2019. 1, 7\n[BMS20] \u00c9tienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented algorithms. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, 2020. 3\n[BOR21] Sandip Banerjee, Rafail Ostrovsky, and Yuval Rabani. Min-sum clustering (with outliers). In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM, pages 16:1-16:16, 2021. 1, 2\n[CEI+22] Justin Y. Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, Tal Wagner, David P. Woodruff, and Michael Zhang. Triangle and four cycle counting with predictions in graph streams. In The Tenth International Conference on Learning Representations, ICLR, 2022. 3\n[CIW22] Justin Y. Chen, Piotr Indyk, and Tal Wagner. Streaming algorithms for supportaware histograms. In International Conference on Machine Learning, ICML, pages 3184-3203, 2022. 3\n[CK19] Vincent Cohen-Addad and Karthik C. S. Inapproximability of clustering in lp metrics. In 60th IEEE Annual Symposium on Foundations of Computer Science, FOCS, pages 519-539, 2019. 2, 10\n[CKL21] Vincent Cohen-Addad, Karthik C. S., and Euiwoong Lee. On approximability of clustering problems without candidate centers. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 2635-2648, 2021. 1, 7\n[CKL22] Vincent Cohen-Addad, Karthik C. S., and Euiwoong Lee. Johnson coverage hypothesis: Inapproximability of k-means and k-median in $\\ell_{p}$-metrics. In Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 1493-1530, 2022. $2,3,5,8,9,10,12,13,14,15$\n[CLS21] Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time. J. ACM, 68(1):3:1-3:39, 2021. 7, 37\n[CLS+22] Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn, and Omar Ali Sheikh-Omar. Improved coresets for euclidean k-means. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems, NeurIPS, 2022. 2, 7\n\n[Con12] Vincent Conitzer. Computer science 590, lecture notes. https://courses.cs.duke. edu/fall12/compsci590.1/network_flow.pdf, 2012. 36\n[CS07] Artur Czumaj and Christian Sohler. Sublinear-time approximation algorithms for clustering via random sampling. Random Struct. Algorithms, 30(1-2):226-256, 2007. $1,3,8$\n[CSS21] Vincent Cohen-Addad, David Saulpic, and Chris Schwiegelshohn. A new coreset framework for clustering. In STOC '21: 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 169-182, 2021. 2, 7\n[CSVZ22] Justin Y. Chen, Sandeep Silwal, Ali Vakilian, and Fred Zhang. Faster fundamental graph algorithms via learned predictions. In International Conference on Machine Learning, ICML, pages 3583-3602, 2022.3\n[CWZ23] Vincent Cohen-Addad, David P. Woodruff, and Samson Zhou. Streaming euclidean k-median and k-means with o(log n) space. In 64th IEEE Annual Symposium on Foundations of Computer Science, FOCS, pages 883-908, 2023.7\n[DGKR05] Irit Dinur, Venkatesan Guruswami, Subhash Khot, and Oded Regev. A new multilayered PCP and the hardness of hypergraph vertex cover. SIAM J. Comput., 34(5):11291146, 2005. 5, 14\n[DIL ${ }^{+}$21] Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster matchings via learned duals. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS, pages 10393-10406, 2021. 3\n[dIVK01] Wenceslas Fernandez de la Vega and Claire Kenyon. A randomized approximation scheme for metric MAX-CUT. J. Comput. Syst. Sci., 63(4):531-541, 2001. 1\n[dIVKKR03] Wenceslas Fernandez de la Vega, Marek Karpinski, Claire Kenyon, and Yuval Rabani. Approximation schemes for clustering problems. In Proceedings of the 35th Annual ACM Symposium on Theory of Computing, pages 50-58, 2003. 1, 2, 3, 5\n[DMVW23] Sami Davies, Benjamin Moseley, Sergei Vassilvitskii, and Yuyan Wang. Predictive flows for faster ford-fulkerson. In International Conference on Machine Learning, ICML, volume 202, pages 7231-7248, 2023. 3\n[DSS24] Andrew Draganov, David Saulpic, and Chris Schwiegelshohn. Settling time vs. accuracy tradeoffs for clustering big data. Proc. ACM Manag. Data, 2(3):173, 2024. 21, 29\n[EFS ${ }^{+}$22] Jon C. Ergun, Zhili Feng, Sandeep Silwal, David P. Woodruff, and Samson Zhou. Learning-augmented $k$-means clustering. In The Tenth International Conference on Learning Representations, ICLR, 2022. 3, 4, 6\n[EIO02] Lars Engebretsen, Piotr Indyk, and Ryan O\u2019Donnell. Derandomized dimensionality reduction with applications. In David Eppstein, editor, Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 705-712, 2002. 14\n\n[Fei98] Uriel Feige. A threshold of ln n for approximating set cover. J. ACM, 45(4):634-652, 1998. 9\n[FS12] Dan Feldman and Leonard J. Schulman. Data reduction for weighted and outlierresistant clustering. In Yuval Rabani, editor, Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2012, Kyoto, Japan, January 17-19, 2012, pages 1343-1354. SIAM, 2012. 8\n[FSS20] Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant-size coresets for k-means, pca, and projective clustering. SIAM J. Comput., 49(3):601-657, 2020.7\n[GH98] Nili Guttmann-Beck and Refael Hassin. Approximation algorithms for min-sum p-clustering. Discret. Appl. Math., 89(1-3):125-142, 1998. 1, 2, 7\n[GI03] Venkatesan Guruswami and Piotr Indyk. Embeddings and non-approximability of geometric problems. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 537-538, 2003.1\n[GLS ${ }^{+}$22] Elena Grigorescu, Young-San Lin, Sandeep Silwal, Maoyuan Song, and Samson Zhou. Learning-augmented algorithms for online linear and semidefinite programming. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems, NeurIPS, 2022. 3\n[GP19] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice. In Proceedings of the 36th International Conference on Machine Learning, ICML, pages 2319-2327, 2019.3\n[HIKV19] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation algorithms. In 7th International Conference on Learning Representations, ICLR, 2019. 3\n[HLW24] Lingxiao Huang, Jian Li, and Xuan Wu. On optimal coreset construction for euclidean (k, z)-clustering. In Proceedings of the 56th Annual ACM Symposium on Theory of Computing, STOC, pages 1594-1604, 2024.7\n[HO10] Refael Hassin and Einat Or. Min sum clustering with penalties. Eur. J. Oper. Res., 206(3):547-554, 2010.7\n[HV20] Lingxiao Huang and Nisheeth K. Vishnoi. Coresets for clustering in euclidean spaces: importance sampling is nearly optimal. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC, pages 1416-1429, 2020.7\n[IKI94] Mary Inaba, Naoki Katoh, and Hiroshi Imai. Applications of weighted voronoi diagrams and randomization to variance-based k-clustering (extended abstract). In Proceedings of the Tenth Annual Symposium on Computational Geometry, pages 332-339, 1994. 2, 6, 17, 31\n\n[IKQP21] Sungjin Im, Ravi Kumar, Mahshid Montazer Qaem, and Manish Purohit. Online knapsack with frequency predictions. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS, pages 2733-2743, 2021. 3\n[Ind99] Piotr Indyk. A sublinear time approximation scheme for clustering in metric spaces. In 40th Annual Symposium on Foundations of Computer Science, FOCS, pages 154159, 1999. 1, 2, 3, 5\n[IVY19] Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS, pages 7400-7410, 2019. 3\n[JLL+20] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learningaugmented data stream algorithms. In 8th International Conference on Learning Representations, ICLR, 2020. 3\n[JLL+22] Shaofeng H.-C. Jiang, Erzhi Liu, You Lyu, Zhihao Gavin Tang, and Yubo Zhang. Online facility location with predictions. In The Tenth International Conference on Learning Representations, ICLR, 2022. 3\n[JLS23] Arun Jambulapati, Yang P. Liu, and Aaron Sidford. Chaining, group leverage score overestimates, and fast spectral hypergraph sparsification. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023, pages 196-206. ACM, 2023. 2\n[JMF99] Anil K Jain, M Narasimha Murty, and Patrick J Flynn. Data clustering: a review. ACM computing surveys (CSUR), 31(3):264-323, 1999. 1\n[JSWZ21] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for solving general lps. In STOC '21: 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 823-832, 2021. 7, 37\n[Kar84] Narendra Karmarkar. A new polynomial-time algorithm for linear programming. Comb., 4(4):373-396, 1984. 7, 37\n[KBC+18] Tim Kraska, Alex Beutel, Ed H. Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures. In Proceedings of the 2018 International Conference on Management of Data, SIGMOD Conference, pages 489-504, 2018. 3\n[KBTV22] Misha Khodak, Maria-Florina Balcan, Ameet Talwalkar, and Sergei Vassilvitskii. Learning predictions for algorithms with predictions. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems, NeurIPS, 2022. 3\n[Kho02] Subhash Khot. Hardness results for coloring 3 -colorable 3 -uniform hypergraphs. In 43rd Symposium on Foundations of Computer Science (FOCS), Proceedings, pages 23-32, 2002. 5, 14\n\n[KKLP97] Viggo Kann, Sanjeev Khanna, Jens Lagergren, and Alessandro Panconesi. On the hardness of approximating max k-cut and its dual. Chic. J. Theor. Comput. Sci., 1997. 7\n[Kle02] Jon M. Kleinberg. An impossibility theorem for clustering. In Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS, pages 446-453, 2002. 1\n[Lee23] James R. Lee. Spectral hypergraph sparsification via chaining. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023, pages 207218. ACM, 2023. 2\n[LLL+23] Yi Li, Honghao Lin, Simin Liu, Ali Vakilian, and David P. Woodruff. Learning the positions in countsketch. In The Eleventh International Conference on Learning Representations, ICLR, 2023. 3\n[LLMV20] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling via learned weights. In Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA, pages 1859-1877, 2020. 3\n[LLW22] Honghao Lin, Tian Luo, and David P. Woodruff. Learning augmented binary search trees. In International Conference on Machine Learning, ICML, pages 13431-13440, 2022. 3\n[LS15] Yin Tat Lee and Aaron Sidford. Efficient inverse maintenance and faster algorithms for linear programming. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS, pages 230-249, 2015. 7, 37\n[LSW17] Euiwoong Lee, Melanie Schmidt, and John Wright. Improved and simplified inapproximability for k-means. Inf. Process. Lett., 120:40-43, 2017. 2\n[LSZ19] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix multiplication time. In Conference on Learning Theory, COLT, pages 2140-2157, 2019. 7, 37\n[LV21] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. J. ACM, 68(4):24:1-24:25, 2021. 3\n[Mat00] Jir\u00ed Matousek. On approximate geometric k-clustering. Discret. Comput. Geom., 24(1):61-84, 2000. 1, 2, 3, 5\n[Mit18] Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems, NeurIPS, pages 462-471, 2018. 3\n[MNV12] Meena Mahajan, Prajakta Nimbhorkar, and Kasturi R. Varadarajan. The planar k-means problem is np-hard. Theor. Comput. Sci., 442:13-21, 2012. 1\n\n[MV20] Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. In Tim Roughgarden, editor, Beyond the Worst-Case Analysis of Algorithms, pages 646-662. Cambridge University Press, 2020.\n[NCN23] Thy Dinh Nguyen, Anamay Chaturvedi, and Huy L. Nguyen. Improved learningaugmented algorithms for k-means and k-medians clustering. In The Eleventh International Conference on Learning Representations, ICLR, 2023. 3, 4, 6, 32, 33\n[NRS24] Ismail Naderi, Mohsen Rezapour, and Mohammad R Salavatipour. Approximation schemes for min-sum k-clustering. Discrete Optimization, 54:100860, 2024.7\n[PSK18] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML predictions. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS, pages 96849693, 2018.\n[PY91] Christos H. Papadimitriou and Mihalis Yannakakis. Optimization, approximation, and complexity classes. J. Comput. Syst. Sci., 43(3):425-440, 1991. 7\n[Sch98] Alexander Schrijver. Theory of linear and integer programming. John Wiley \\& Sons, 1998. 37\n[Sch00] Leonard J. Schulman. Clustering for edge-cost minimization (extended abstract). In Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing, pages 547-555, 2000. 1, 2, 3, 5\n[SG76] Sartaj Sahni and Teofilo F. Gonzalez. P-complete approximation problems. J. ACM, 23(3):555-565, 1976. 7\n[SLLA23] Yongho Shin, Changyeol Lee, Gukryeol Lee, and Hyung-Chan An. Improved learningaugmented algorithms for the multi-option ski rental problem via best-possible competitive analysis. In International Conference on Machine Learning, ICML, pages 31539-31561, 2023. 3\n[SZS ${ }^{+}$14] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR, Conference Track Proceedings, 2014. 3\n[Vai89] Pravin M. Vaidya. Speeding-up linear programming using fast matrix multiplication. In 30th Annual Symposium on Foundations of Computer Science, pages 332-337, 1989. 7,37\n[Vai90] Pravin M. Vaidya. An algorithm for linear programming which requires o(((m+n)n $\\left.{ }^{2}\\right.$ $+(m+n)^{1.5} n) 1$ ) arithmetic operations. Math. Program., 47:175-201, 1990. 7, 37\n[WLW20] Shufan Wang, Jian Li, and Shiqiang Wang. Online algorithms for multi-shop ski rental with machine learned advice. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, 2020. 3\n\n[WZ20] Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-offs for learning-augmented online algorithms. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, 2020.3\n[WZZ23] David P. Woodruff, Peilin Zhong, and Samson Zhou. Near-optimal k-clustering in the sliding window model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems, NeurIPS, 2023. 7\n[XW05] Rui Xu and Donald Wunsch. Survey of clustering algorithms. IEEE Transactions on neural networks, 16(3):645-678, 2005.1\n[ZTHH24] Xiaoyi Zhu, Yuxiang Tian, Lingxiao Huang, and Zengfeng Huang. Space complexity of euclidean clustering. In 40th International Symposium on Computational Geometry, SoCG, pages 82:1-82:16, 2024. 7", "tables": {}, "images": {}}], "id": "2412.03332v2", "authors": ["Karthik C. S.", "Euiwoong Lee", "Yuval Rabani", "Chris Schwiegelshohn", "Samson Zhou"], "categories": ["cs.DS", "cs.CC", "cs.CG", "cs.LG"], "abstract": "The $\\ell_2^2$ min-sum $k$-clustering problem is to partition an input set\ninto clusters $C_1,\\ldots,C_k$ to minimize $\\sum_{i=1}^k\\sum_{p,q\\in\nC_i}\\|p-q\\|_2^2$. Although $\\ell_2^2$ min-sum $k$-clustering is NP-hard, it is\nnot known whether it is NP-hard to approximate $\\ell_2^2$ min-sum\n$k$-clustering beyond a certain factor.\n  In this paper, we give the first hardness-of-approximation result for the\n$\\ell_2^2$ min-sum $k$-clustering problem. We show that it is NP-hard to\napproximate the objective to a factor better than $1.056$ and moreover,\nassuming a balanced variant of the Johnson Coverage Hypothesis, it is NP-hard\nto approximate the objective to a factor better than 1.327.\n  We then complement our hardness result by giving a nearly linear time\nparameterized PTAS for $\\ell_2^2$ min-sum $k$-clustering running in time\n$O\\left(n^{1+o(1)}d\\cdot \\exp((k\\cdot\\varepsilon^{-1})^{O(1)})\\right)$, where\n$d$ is the underlying dimension of the input dataset.\n  Finally, we consider a learning-augmented setting, where the algorithm has\naccess to an oracle that outputs a label $i\\in[k]$ for input point, thereby\nimplicitly partitioning the input dataset into $k$ clusters that induce an\napproximately optimal solution, up to some amount of adversarial error\n$\\alpha\\in\\left[0,\\frac{1}{2}\\right)$. We give a polynomial-time algorithm that\noutputs a $\\frac{1+\\gamma\\alpha}{(1-\\alpha)^2}$-approximation to $\\ell_2^2$\nmin-sum $k$-clustering, for a fixed constant $\\gamma>0$.", "updated": "2025-04-10T19:07:00Z", "published": "2024-12-04T14:03:27Z"}