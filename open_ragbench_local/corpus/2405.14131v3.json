{
  "title": "Statistical Advantages of Perturbing Cosine Router in Mixture of Experts",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nThe cosine router in Mixture of Experts (MoE) has recently emerged as an attractive alternative to the conventional linear router. Indeed, the cosine router demonstrates favorable performance in image and language tasks and exhibits better ability to mitigate the representation collapse issue, which often leads to parameter redundancy and limited representation potentials. Despite its empirical success, a comprehensive analysis of the cosine router in MoE has been lacking. Considering the least square estimation of the cosine routing MoE, we demonstrate that due to the intrinsic interaction of the model parameters in the cosine router via some partial differential equations, regardless of the structures of the experts, the estimation rates of experts and model parameters can be as slow as $\\mathcal{O}\\left(1 / \\log ^{7}(n)\\right)$ where $\\tau>0$ is some constant and $n$ is the sample size. Surprisingly, these pessimistic non-polynomial convergence rates can be circumvented by the widely used technique in practice to stabilize the cosine router - simply adding noises to the $\\ell^{2}$-norms in the cosine router, which we refer to as perturbed cosine router. Under the strongly identifiable settings of the expert functions, we prove that the estimation rates for both the experts and model parameters under the perturbed cosine routing MoE are significantly improved to polynomial rates. Finally, we conduct extensive simulation studies in both synthetic and real data settings to empirically validate our theoretical results.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "## 1 INTRODUCTION\n\nProposed by Jacobs et al. (1991) and Jordan \\& Jacobs (1994), Mixture of Experts (MoE) has been known as an effective statistical method to incorporate the capabilities of various specialized models called experts. Different from conventional mixture models (Lindsay, 1995) in which the mixture weights are scalars, the MoE rather utilizes a routing mechanism to determine a set of weights depending on an input token. In particular, the router first computes the similarity scores between each token and experts and then assigns more weights to the more relevant experts determined based on those scores. To further improve the scalability of the MoE, Shazeer et al. (2017) has recently introduced a sparse variant of this model, which routes each input to only a subset of experts. This sparse MoE model allows us to increase the number of learnable parameters with nearly constant computational overhead. As a consequence, the sparse MoE has been leveraged in several applications, including large language models (Jiang et al., 2024; Puigcerver et al., 2024; Zhou et al., 2023b; Dai et al., 2024; Pham et al., 2024), computer vision (Riquelme et al., 2021; Liang et al., 2022), speech recognition (You et al., 2021; Gulati et al., 2020; Peng et al., 1996), continual learning (Le et al., 2024; Li et al., 2025), multi-task learning (Hazimeh et al., 2021), and other applications (Han et al., 2024; Chow et al., 2023; Le et al., 2025).\n\nIn the above applications, practitioners often use a linear router which calculates the similarity score by taking the inner product of a token hidden representation and an expert embedding. Nevertheless, Chi et al. (2022) discovered that utilizing the linear router might induce the representation collapse issue. This phenomenon occurs when a fraction of experts govern the decision-making process, leading to the redundancy of other experts. In response, Chi et al. (2022) proposed an alternative known as a cosine router. In particular, this router begins with projecting the token hidden representation into\n\n[^0]\n[^0]:    *Equal Contribution\n\na low-dimensional space, followed by applying $L^{2}$ normalization to both the token representations and expert embeddings. By doing so, the similarity scores become more stable, circumventing the dominance of certain experts. The efficacy of the cosine routing MoE has been experimentally demonstrated in language modeling (Chi et al., 2022), and domain generalization (Li et al., 2023). On the other hand, a comprehensive theoretical study of the cosine router has remained lacking.\n\nIn the literature, there have been some attempts to understand the MoE models with different types of gating functions whose outputs are the composition of some functions and the routing scores. First of all, considering the classification problem with cluster structures, Chen et al. (2022) demonstrated that the router operated by a neural network could learn the cluster-center features, which helped divide a complex problem into simpler classification sub-problems that individual expert could handle. Next, Ho et al. (2022) studied the expert estimation under an input-free gating Gaussian MoE model, showing that the rates for estimating experts depend on the algebraic structures among experts. Subsequently, the Gaussian MoE model with softmax gating was explored in (Nguyen et al., 2023; 2024a) which pointed out that interactions among model parameters via some partial differential equations (PDE) did harm the expert estimation rates. Saying that the setting of Gaussian MoE was far from practice, Nguyen et al. (2024c) rather took into account a regression framework with the regression function being a linear router MoE model. They verified the benefits of formulating experts as feed-forward networks with popular activation functions like ReLU and sigmoid from the perspective of the expert estimation problem.\n\nIt is worth noting that the expert estimation problem allows us to capture how fast an expert specializes in a specific task, which is one of the most important problems in the MoE literature known as expert specialization (Dai et al., 2024; Krishnamurthy et al., 2023). Furthermore, from the convergence analysis of expert estimation, we can gain several insights for designing the router and expert networks (see Section 4). Therefore, we will investigate the effects of the cosine router on the convergence of expert estimation in this paper. For that sake, let us now present the problem setting formally.\nProblem setting. We assume that $\\left(X_{1}, Y_{1}\\right),\\left(X_{2}, Y_{2}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right) \\in \\mathbb{R}^{d_{1}} \\times \\mathbb{R}$ is an i.i.d sample of size $n$ generated according to the following model\n\n$$\nY_{i}=f_{G_{*}}\\left(X_{i}\\right)+\\varepsilon_{i}, \\quad i=1, \\ldots, n\n$$\n\nwhere regression function $f_{G_{*}}(\\cdot)$ takes the following form:\n\n$$\nf_{G_{*}}(x):=\\sum_{i=1}^{k_{*}} \\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 i}^{*}\\right\\| \\cdot\\|x\\|}+\\beta_{0 i}^{*}\\right) \\cdot h\\left(x, \\eta_{i}^{*}\\right)\n$$\n\nHere, the function $h(x, \\eta)$ is known as the expert function, which we assumed to be of parametric form. Meanwhile, $\\left(\\beta_{0 i}^{*}, \\beta_{1 i}^{*}, \\eta_{i}^{*}\\right)_{i=1}^{k_{*}}$ are true yet unknown parameters in the parameter space $\\Theta \\subset$ $\\mathbb{R} \\times \\mathbb{R}^{d_{1}} \\times \\mathbb{R}^{d_{2}}$ and $G_{*}:=\\sum_{i=1}^{k_{*}} \\exp \\left(\\beta_{0 i}^{*}\\right) \\delta_{\\left(\\beta_{1 i}^{*}, \\eta_{i}^{*}\\right)}$ denotes the associated mixing measure, i.e. a weighted sum of Dirac measures $\\delta$. Additionally, we define for any vector $v=\\left(v_{1}, v_{2}, \\ldots, v_{k_{*}}\\right)$ in $\\mathbb{R}^{k_{*}}$ that $\\operatorname{Softmax}\\left(v_{i}\\right):=\\exp \\left(v_{i}\\right) / \\sum_{j=1}^{k_{*}} \\exp \\left(v_{j}\\right)$. In the cosine router in equation (2), we omit the step of reducing the dimension of the input token $x$, and assume that it has already been in a low-dimensional space for simplicity. Furthermore, we assume that $X_{1}, X_{2}, \\ldots, X_{n}$ are i.i.d. samples from some probability distribution $\\mu$. Lastly, $\\varepsilon_{1}, \\varepsilon_{2}, \\ldots, \\varepsilon_{n}$ are independent Gaussian noise variables such that $\\mathbb{E}\\left[\\varepsilon_{i} \\mid X_{i}\\right]=0$ and $\\operatorname{Var}\\left(\\varepsilon_{i} \\mid X_{i}\\right)=\\sigma^{2}$ for all $1 \\leq i \\leq n$. Notably, the Gaussian assumption is just for the simplicity of the proof argument.\n\nLeast squares estimation (LSE). To estimate the true parameters $\\left(\\beta_{0 i}^{*}, \\beta_{1 i}^{*}, \\eta_{i}^{*}\\right)_{i=1}^{k_{*}}$ or, equivalently, to estimate the true mixing measure $G_{*}$, we leverage the popular least squares method (see, e.g., van de Geer, 2000). Formally, the mixing measure $G_{*}$ is approximated by\n\n$$\n\\widehat{G}_{n}:=\\underset{G}{\\arg \\min } \\sum_{i=1}^{n}\\left(Y_{i}-f_{G}\\left(X_{i}\\right)\\right)^{2}\n$$\n\nUnder the exact-specified setting, i.e., when the true number of expert $k_{*}$ is known, the minimum in the above equation is subject to the set of all mixing measures with $k_{*}$ atoms, denoted by $\\mathcal{E}_{k_{*}}(\\Theta):=\\left\\{G=\\sum_{i=1}^{k_{*}} \\exp \\left(\\beta_{0 i}\\right) \\delta_{\\left(\\beta_{i i}, \\eta_{i}\\right)}:\\left(\\beta_{0 i}, \\beta_{1 i}, \\eta_{i}\\right) \\in \\Theta\\right\\}$. On the other hand, under the over-specified setting, i.e., when $k_{*}$ is unknown and the true model (2) is over-specified by a mixture\n\nof $k$ experts where $k>k_{*}$, the minimum is subject to the set of all mixing measures with at most $k$ atoms, i.e., $\\mathcal{G}_{k}(\\Theta):=\\left\\{G=\\sum_{i=1}^{k^{\\prime}} \\exp \\left(\\beta_{0 i}\\right) \\delta_{\\left(\\beta_{1 i}, \\eta_{i}\\right)}: 1 \\leq k^{\\prime} \\leq k,\\left(\\beta_{0 i}, \\beta_{1 i}, \\eta_{i}\\right) \\in \\Theta\\right\\}$.\nUniversal assumptions. In the sequel, we implicitly impose four following mild assumptions on the model parameters, which were widely used in previous works (Nguyen et al., 2024c), unless stating otherwise:\n(A.1) Convergence of LSE: The parameter space $\\Theta \\subseteq \\mathbb{R} \\times \\mathbb{R}^{d_{1}} \\times \\mathbb{R}^{d_{2}}$ is compact, while the input space $\\mathcal{X} \\subseteq \\mathbb{R}^{d_{1}}$ is bounded. This helps ensure the convergence of least squares estimation.\n(A.2) Distinct experts: The true parameters $\\eta_{1}^{*}, \\ldots, \\eta_{k_{*}}^{*}$ are distinct so that the experts $h\\left(\\cdot, \\eta_{1}^{*}\\right), \\ldots, h\\left(\\cdot, \\eta_{k_{*}}^{*}\\right)$ are different from each other. Furthermore, the expert function $h(\\cdot, \\eta)$ is Lipschitz continuous w.r.t its parameters and bounded.\n(A.3) Identifiability of the MoE: In order that the cosine routing MoE is identifiable, i.e., $f_{G}(x)=$ $f_{G_{*}}(x)$ for almost every $x$ implies that $G \\equiv G_{*}$, we let $\\beta_{0 k_{*}}^{*}=0$.\n(A.4) Input-dependent router: To ensure that the router is input-dependent, we assume that at least one among the parameters $\\beta_{11}^{*}, \\ldots, \\beta_{1 k_{*}}^{*}$ is non-zero.\nTechnical challenges. The normalization of parameters in the cosine router leads to a fundamental challenge in theory. In particular, to establish parameter and expert estimation rates based on the convergence rate of the regression function, we rely on decomposing the discrepancy $f_{\\widehat{G}_{*}}(x)-f_{G_{*}}(x)$ into a combination of linearly independent terms by applying Taylor expansions to the product of the softmax's numerator and the expert function, i.e. $H\\left(x, \\beta_{1}, \\eta\\right):=\\exp \\left(\\frac{\\beta_{1}^{*} x}{\\left\\|\\beta_{1}\\right\\|\\left\\|x\\right\\|}\\right) h(x, \\eta)$. However, the normalization of $\\beta_{1}$ in the cosine router leads to an intrinsic interaction among the elements of the parameter $\\beta_{1}$ via the following PDE:\n\n$$\n\\beta_{1}^{*} \\frac{\\partial H}{\\partial \\beta_{1}}\\left(x, \\beta_{1}, \\eta\\right)=0\n$$\n\nAlthough parameter interactions expressed in the language of PDEs have been observed in Nguyen et al. (2024c), the structure of the above interaction is much more sophisticated (even hold for the first-order derivatives while those in Nguyen et al. (2024c) occurs only when taking the second-order derivatives). Thus, this PDE induces several linearly dependent terms in the Taylor expansion, and we have to aggregate their coefficients in order to form the desired combination of linearly independent terms. Then, the resulting coefficients become complex, thereby negatively affecting the convergence of expert estimation. To the best of our knowledge, such a phenomenon with the cosine router has never been observed in previous works.\nMain contributions. In this work, we develop a comprehensive theoretical analysis of regression function estimation as well as parameter and expert estimations under the cosine router MoE model (1). Our contributions are two-fold and can be summarized as follows (see also Table 1):\n\n1. Cosine router: Equipped with the cosine router, we demonstrate that under both the exact-specified and the over-specified settings, the rates for estimating ground-truth parameters $\\beta_{0 i}^{*}, \\beta_{1 i}^{*}$ and $\\eta_{i}^{*}$ are slower than any polynomial rates and, therefore, could be as slow as $\\mathcal{O}_{P}\\left(1 / \\log ^{\\tau}(n)\\right)$, where $\\tau>0$ is some constant. These slow rates are attributed to the internal interaction among router parameters expressed by the PDE in equation (4). As a result, the estimation rates for experts $h\\left(\\cdot, \\eta_{i}^{*}\\right)$ are also negatively affected, and could be of order $\\mathcal{O}_{P}\\left(1 / \\log ^{\\tau}(n)\\right)$.\n2. Perturbed cosine router: In response, we propose a novel router called perturbed cosine router in which we add noises to the $L^{2}$ norms of the token representations and the expert embeddings. This not only helps stabilize the router but also eliminates the intrinsic interaction in equation (4). Additionally, we also establish identifiability conditions to characterize expert functions that have faster estimation rates than others under the exact-specified and over-specified settings, respectively. Those conditions indicate that the rates for estimating experts, which are formulated as feed-forward networks (FFNs) with widely used activation functions such as ReLU and GeLU , are significantly improved, ranging from $\\mathcal{O}_{P}(\\sqrt[4]{\\log (n) / n})$ to $\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$.\nOutline. In Section 2, we establish the convergence rates of parameter and expert estimations under the setting of the cosine router MoE. Then, we derive these rates when the cosine router is replaced by the perturbed cosine router in Section 3. Based on these theoretical results, we provide a few practical\n\nTable 1: Summary of worst possible estimation rates for linear experts, polynomial experts and FFN experts equipped with the ReLU activation function under the MoE with linear router (Nguyen et al., 2024c), cosine router (ours) and perturbed cosine routers (ours).\n\n![table_0](table_0)\n\nimplications in Section 4. Next, we empirically verify the (theoretical) benefits of the perturbed cosine router over the cosine router under both the synthetic and real data settings in Section 5 before concluding the paper in Section 6. Finally, proofs and additional details of the experiments are deferred to the Appendices.\n\nNotations. We let $[n]$ stand for the set $\\{1,2, \\ldots, n\\}$ for any $n \\in \\mathbb{N}$. Next, for any set $S$, we denote $|S|$ as its cardinality. For any vector $v \\in \\mathbb{R}^{d}$ and $\\alpha:=\\left(\\alpha_{1}, \\alpha_{2}, \\ldots, \\alpha_{d}\\right) \\in \\mathbb{N}^{d}$, we let $v^{\\alpha}=v_{1}^{\\alpha_{1}} v_{2}^{\\alpha_{2}} \\ldots v_{d}^{\\alpha_{d}},|v|:=v_{1}+v_{2}+\\ldots+v_{d}$ and $\\alpha!:=\\alpha_{1}!\\alpha_{2}!\\ldots \\alpha_{d}!$, while $\\|v\\|$ stands for its $L^{2}$-norm value. Lastly, for any two positive sequences $\\left(a_{n}\\right)_{n \\geq 1}$ and $\\left(b_{n}\\right)_{n \\geq 1}$, we write $a_{n}=\\mathcal{O}\\left(b_{n}\\right)$ or $a_{n} \\lesssim b_{n}$ if there exists $C>0$ such that $a_{n} \\leq C b_{n}$ for all $n \\in \\mathbb{N}$. Meanwhile, the notation $a_{n}=\\mathcal{O}_{P}\\left(b_{n}\\right)$ indicates that $a_{n} / b_{n}$ is stochastically bounded.",
      "tables": {
        "table_0": "| Routers/ Experts | Linear: $a^{\\top} x+b$ | Polynomial: $(a^{\\top} x+b)^{p}, p \\geq 2$ | ReLU FFN |\n| :--: | :--: | :--: | :--: |\n| Linear | $1 / \\log ^{r}(n)$ | $1 / \\log ^{r}(n)$ | $n^{-1 / 4}$ |\n| Cosine | $1 / \\log ^{r}(n)$ | $1 / \\log ^{r}(n)$ | $1 / \\log ^{r}(n)$ |\n| Perturbed cosine | $1 / \\log ^{r}(n)$ | $n^{-1 / 4}$ | $n^{-1 / 4}$ |"
      },
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 2 COSINE ROUTER Mixture of EXPERTS \n\nIn this section, we characterize the parameter and expert estimation rates under the over-specified setting of the cosine router MoE. We first start with the convergence rate of the regression function estimation $f_{\\widehat{G}_{n}}$ to the true regression function $f_{G_{*}}$ under the $L^{2}(\\mu)$ norm in the following theorem:\n\nTheorem 1. Given the least-square estimator $\\widehat{G}_{n}$ defined in equation (3), the regression estimator $f_{\\widehat{G}_{n}}($.$) converges to the true regression function f_{G_{*}}(.) at the following rate:$\n\n$$\n\\left\\|f_{\\widehat{G}_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}=\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})\n$$\n\nThe proof of Theorem 1 is in Appendix B.1. The result of Theorem 1 indicates that the regression estimation rate is parametric. Therefore, as long as we can establish the lower bound $\\left\\|f_{\\widehat{G}_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)} \\gtrsim \\mathcal{L}\\left(\\widehat{G}_{n}, G_{*}\\right)$ where $\\mathcal{L}$ is some loss function among parameters, we arrive at the parameter estimation rate $\\mathcal{L}\\left(\\widehat{G}_{n}, G_{*}\\right)=\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$. This approach is the key component of the convergence rates of parameter and expert estimations under the cosine router MoE. In the sequel, we will consider the over-specified setting of the cosine router, while the results for the exact-specified setting will be presented in Appendix A.2.\nRecall that under the over-specified setting, the true number of experts $k_{*}$ is unknown. Then, based on the notion of Voronoi cells (Manole \\& Ho, 2022), we will construct a Voronoi loss function among parameters tailored to this setting.\nVoronoi loss. Let $G$ be a mixing measure with $k^{\\prime}$ atoms $\\omega_{i}:=\\left(\\beta_{1 i}, \\eta_{i}\\right)$. Then, we distribute these atoms to the Voronoi cells generated by the atoms $\\omega_{j}^{*}:=\\left(\\beta_{1 j}^{*}, \\eta_{j}^{*}\\right)$ of $G_{*}$, which are defined as\n\n$$\n\\mathcal{A}_{j} \\equiv \\mathcal{A}_{j}(G):=\\left\\{i \\in\\left[k^{\\prime}\\right]:\\left\\|\\omega_{i}-\\omega_{j}^{*}\\right\\| \\leq\\left\\|\\omega_{i}-\\omega_{\\ell}^{*}\\right\\|, \\forall \\ell \\neq j\\right\\}\n$$\n\nThen, the Voronoi loss of interest is given by\n\n$$\n\\mathcal{L}_{1, r}\\left(G, G_{*}\\right):=\\sum_{j=1}^{k_{*}}\\left|\\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}\\right)-\\exp \\left(\\beta_{0 j}^{*}\\right)\\right|+\\sum_{j=1}^{k_{*}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}\\right)\\left[\\left\\|\\Delta \\beta_{1 i j}\\right\\|^{r}+\\left\\|\\Delta \\eta_{i j}\\right\\|^{r}\\right]\n$$\n\nwhere $r \\geq 1$ is some constant, $\\Delta \\beta_{1 i j}:=\\beta_{1 i}-\\beta_{1 j}^{*}$ and $\\Delta \\eta_{i j}:=\\eta_{i}-\\eta_{j}^{*}$.\nNote that, due to the parameter interaction inside the cosine router captured by the PDE (4), the lower bound $\\left\\|f_{\\widehat{G}_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)} \\gtrsim \\mathcal{L}_{1, r}\\left(\\widehat{G}_{n}, G_{*}\\right)$ does not hold true, and thus, we cannot achieve\n\nthe desired bound $\\mathcal{L}_{1, r}\\left(\\widehat{G}_{n}, G_{*}\\right)=\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$ mentioned at the beginning of Section 2. By contrast, we show in Appendix B. 3 an opposed result to the previous lower bound, saying that\n\n$$\n\\lim _{x \\rightarrow 0} \\inf _{G \\in \\mathcal{E}_{k_{*}}(\\Theta) \\mid \\mathcal{L}_{1, r}\\left(G, G_{*}\\right) \\leq x} \\inf _{G} \\int_{G} f_{G} \\cdot f_{G_{*}}\\left(\\overline{\\mathcal{L}}_{1, r}\\left(G, G_{*}\\right)=0\\right.\n$$\n\nfor any $r \\geq 1$. This result implies the following minimax lower bound of parameter estimation:\nTheorem 2. Under the over-specified setting, the following minimax lower bound of estimating $G_{*}$\n\n$$\n\\inf _{\\widehat{G}_{n} \\in \\mathcal{G}_{k}(\\Theta)} \\sup _{G \\in \\mathcal{G}_{k}(\\Theta) \\backslash \\widehat{\\mathcal{G}}_{k_{*}-1}(\\Theta)} \\mathbb{E}_{f_{G}}\\left[\\mathcal{L}_{1, r}\\left(\\overline{G_{n}}, G\\right)\\right] \\gtrsim n^{-1 / 2}\n$$\n\nholds true for any $r \\geq 1$, where $\\mathbb{E}_{f_{G}}$ indicates the expectation taken w.r.t the product measure with $f_{G}^{n}$ and the infimum is over all estimators taking values in $\\mathcal{G}_{k}(\\Theta)$.\n\nSee Appendix B. 3 for the proof of Theorem 2. There are two main implications of the above result:\n(i) Parameter estimation rates. The minimax lower bound together with the formulation of $\\mathcal{L}_{1, r}$ indicate that at least one among the rates for estimating parameters $\\beta_{1 j}^{*}, \\beta_{0 i}^{*}, \\eta_{j}^{*}$ is slower than any polynomial rates $\\mathcal{O}_{P}\\left(n^{-1 / 2 r}\\right)$ and, thus, could be of order $\\mathcal{O}_{P}\\left(1 / \\log ^{\\tau}(n)\\right)$, for some constant $\\tau>0$.\n(ii) Router estimation rates: When the estimation rate of either $\\beta_{1}^{*}$ or $\\beta_{0}^{*}$ is slower than any polynomial rates, since the softmax function is Lipschitz w.r.t the Euclidean norm (Gao \\& Pavel, 2018), we deduce that the worst possible rate of estimating the cosine router or the mixture weights in equation (2) could also be slower than any polynomial rates. In practice, the router and the expert networks are trained simultaneously (see Section 1.2 in (Shazeer et al., 2017)). Thus, the slow convergence of the router might decelerate the model convergence.\n(iii) Expert estimation rates. Assume that $\\widehat{G}_{n}:=\\sum_{i=1}^{k_{*}} \\exp \\left(\\widehat{\\beta}_{0 i}\\right) \\delta_{\\left(\\widehat{\\beta}_{1 i}^{*}, \\widehat{\\eta}_{i}^{*}\\right)}$. Since the expert $h(\\cdot, \\eta)$ is Lipschitz continuous, it follows that\n\n$$\n\\sup _{x}\\left|h\\left(x, \\widehat{\\eta}_{i}^{n}\\right)-h\\left(x, \\eta_{j}^{*}\\right)\\right| \\lesssim\\left\\|\\widehat{\\eta}_{i}^{n}-\\eta_{j}^{*}\\right\\|\n$$\n\nConsequently, the worst possible rate for estimating the expert $h\\left(x, \\eta_{j}^{*}\\right)$ is identical to the worst possible rate for estimating the parameter $\\eta_{j}^{*}$. For instance, if the expert function takes the polynomial form of $h(x, \\eta):=(\\eta x)^{2}$ (considered in (Mendes \\& Jiang, 2012; Nguyen et al., 2021; 2024c)), where we assume $x \\in \\mathbb{R}$ for simplicity, then we have $\\left|h(x, \\tilde{\\eta})-h\\left(x, \\eta^{*}\\right)\\right|=\\left|\\tilde{\\eta}-\\eta^{*}\\right| \\cdot\\left|\\tilde{\\eta}+\\eta^{*}\\right| \\cdot|x|$. As a result, the expert estimation rate is exactly the parameter estimation rate. Thus, when the estimation rate of $\\eta_{j}^{*}$ is slower than any polynomial rates, the worst possible estimation rates for the experts $h\\left(\\cdot, \\eta_{j}^{*}\\right)$ could also be slower than any polynomial rates and be as slow as $\\mathcal{O}_{P}\\left(1 / \\log ^{\\tau}(n)\\right)$. This indicates that the cosine router is even less sample efficient than the linear router (see also Table 1). Note that by employing techniques of partitioning the input space in (Nguyen et al., 2024b), we can show that these results still hold when using the Top- $K$ sparse softmax gating function. Therefore, in order to improve the sample efficiency while preserving the robustness to the representation collapse, we need to slightly modify the structure of the cosine router in the next section.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 3 Perturbed Cosine Router Mixture of Experts \n\nIn this section, we demonstrate that the pessimistic non-polynomial convergence rates of parameter and expert estimation under the cosine router can be easily circumvented by the widely used technique in practice to stabilize the cosine router: adding noises to the $L^{2}$ norm in the cosine router. Although this perturbation technique and the cosine router have been well studied in the literature, we would like to emphasize that to the best of our knowledge, our work is the first to be aware of combining these two methods together which we refer to as the perturbed cosine router MoE. We now present the formulation of a MoE with the perturbed cosine router under the regression setting.\nProblem setup for the perturbed cosine router MoE model. We assume that an i.i.d. sample of size $n:\\left(X_{1}, Y_{1}\\right),\\left(X_{2}, Y_{2}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right) \\in \\mathbb{R}^{d_{1}} \\times \\mathbb{R}$ is generated according to the model\n\n$$\nY_{i}=g_{G_{*}}\\left(X_{i}\\right)+\\varepsilon_{i}, \\quad i=1, \\ldots, n\n$$\n\nwhere regression function $g_{G_{*}}(\\cdot)$ takes the following form:\n\n$$\ng_{G_{*}}(x):=\\sum_{i=1}^{k_{*}} \\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}+\\beta_{0 i}^{*}\\right) \\cdot h\\left(x, \\eta_{i}^{*}\\right)\n$$\n\nHere, $\\tau_{1}, \\tau_{2}>0$ are two noise hyper-parameters. The main difference between the two regression functions $f_{G_{*}}$ and $g_{G_{*}}$ is the noise hyper-parameters $\\tau_{1}, \\tau_{2}$ that we add to the norms of the expert embeddings $\\beta_{1 i}^{*}$ and the token input $x$, which leads to the perturbed cosine router. By doing so, the parameter interaction inside the router as in equation (4) does not occur. More specifically, let us denote $\\widetilde{H}\\left(x, \\beta_{1}, \\eta\\right):=\\exp \\left(\\frac{\\beta_{1}^{\\top} x}{\\left(\\left\\|\\beta_{1}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}\\right) h(x, \\eta)$, then it can be verified that $\\beta_{1}^{\\top} \\frac{\\partial \\widetilde{H}}{\\partial \\beta_{1}}\\left(x, \\beta_{1}, \\eta\\right) \\neq 0$.\nLeast squares estimation. Similar to the cosine router setting, we can estimate the unknown ground-truth parameters $\\left(\\beta_{0 i}^{*}, \\beta_{1 i}^{*}, \\eta_{i}^{*}\\right)_{i=1}^{k_{*}}$ using the least-square estimator, which is given by:\n\n$$\n\\widehat{G}_{n}:=\\underset{G}{\\arg \\min } \\sum_{i=1}^{n}\\left(Y_{i}-g_{G}\\left(X_{i}\\right)\\right)^{2}\n$$\n\nIn the following theory, we provide a convergence rate of regression function estimation under the perturbed cosine router MoE model.\nTheorem 3. Given a least squares estimator $\\widehat{G}_{n}$ defined in equation (9), the regression function estimation $g_{\\widehat{G}_{n}}(\\cdot)$ admits the following convergence rate:\n\n$$\n\\left\\|g_{\\widehat{G}_{n}}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)}=\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})\n$$\n\nProof of Theorem 3 is in Appendix C.1. The result of Theorem 3 proves that the regression function estimation rate $\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$ under the perturbed cosine router MoE is of the same order as that with the vanilla cosine router in Theorem 1. Following the similar proof strategy in the cosine router MoE in Section 2 for capturing the convergence rates of parameter and expert estimations under the perturbed cosine router MoE model, it is sufficient to establish the lower bound between the difference of regression functions and the difference of parameters under both the exact-specified and over-specified settings.\nIn this section, we study the over-specified setting of the perturbed cosine router. The results for the exact-specified setting of the perturbed cosine router is in Appendix A.2.\nWe now derive a condition called strong identifiability on the expert function $h(\\cdot, \\eta)$ to identify which experts exhibit faster estimation rates than others under the over-specified setting.\nDefinition 1 (Strong identifiability). An expert function $x \\mapsto h(x, \\eta)$ is called strongly identifiable if it is twice differentiable with respect to its parameter $\\eta$, and the set of functions in $x$\n\n$$\n\\left\\{\\frac{\\partial^{\\left|\\alpha_{1}\\right|+\\left|\\alpha_{2}\\right|} \\widetilde{H}}{\\partial \\beta_{1}^{\\alpha_{1}} \\partial \\eta^{\\alpha_{2}}}\\left(x, \\beta_{1 i}, \\eta_{i}\\right): \\alpha_{1} \\in \\mathbb{N}^{d_{1}}, \\alpha_{2} \\in \\mathbb{N}^{d_{2}}, 0 \\leq\\left|\\alpha_{1}\\right|+\\left|\\alpha_{2}\\right| \\leq 2\\right\\}\n$$\n\nis linearly independent for almost every $x$ for any $k \\geq 1$ and distinct parameters $\\eta_{1}, \\ldots, \\eta_{k}$, where we denote $\\widetilde{H}\\left(x, \\beta_{1}, \\eta\\right):=\\exp \\left(\\frac{\\beta_{1}^{\\top} x}{\\left(\\left\\|\\beta_{1}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}\\right) h(x, \\eta)$.\nExample. For experts formulated as neural networks, i.e. $h(x,(a, b))=\\phi\\left(a^{\\top} x+b\\right)$, if the activation $\\phi$ is selected as $\\phi(z)=\\operatorname{ReLU}(z), \\phi(z)=\\tanh (\\cdot)$ or $\\phi(z)=z^{p}$ for $p \\geq 2$, then they are strongly identifiable. Conversely, a linear expert $h(x,(a, b))=a^{\\top} x+b$ fails to meet the strong identifiability the condition.\n\nTo capture the convergence behavior of expert estimation rate under the over-specified setting in Theorem 4, we will use the Voronoi loss defined as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{2}\\left(G, G_{*}\\right):=\\sum_{j=1}^{k_{*}}\\left|\\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}\\right)-\\right. & \\exp \\left(\\beta_{0 j}^{*}\\right)\\left|+\\sum_{j \\in\\left[k_{*}\\right]:\\left|\\mathcal{A}_{j}\\right|=1} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}\\right)\\left[\\left\\|\\Delta \\beta_{1 i j}\\right\\|+\\left\\|\\Delta \\eta_{i j}\\right\\|\\right]\\right| \\\\\n& +\\sum_{j \\in\\left[k_{*}\\right]:\\left|\\mathcal{A}_{j}\\right|>1} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}\\right)\\left[\\left\\|\\Delta \\beta_{1 i j}\\right\\|^{2}+\\left\\|\\Delta \\eta_{i j}\\right\\|^{2}\\right]\n\\end{aligned}\n$$\n\nTheorem 4. Suppose that the expert function $h(x, \\eta)$ satisfies the strong identifiability condition in Definition 1, then the following $L^{2}$-lower bound holds for any mixing measure $G \\in \\mathcal{G}_{k}(\\Theta)$ :\n\n$$\n\\left\\|g_{G}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} \\geqq \\mathcal{L}_{2}\\left(G, G_{*}\\right)\n$$\n\nFurthermore, this bound and the result in Theorem 3 imply that $\\mathcal{L}_{2}\\left(\\widehat{G}_{n}, G_{*}\\right)=\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$.\n\nThe proof of Theorem 4 is in Appendix C.3. A few comments regarding this theorem are in order:\n(i) Parameter estimation rates. Under the over-specified setting, parameters $\\beta_{1 j}^{*}, \\eta_{j}^{*}$ which are fitted by one atom, i.e. $\\left|\\mathcal{A}_{j}\\left(\\widehat{G}_{n}\\right)\\right|=1$, share the same estimation rate of order $\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$. Meanwhile, those for parameters fitted by more than one atom, i.e. $\\left|\\mathcal{A}_{j}\\left(\\widehat{G}_{n}\\right)\\right|>1$, are slightly slower, standing at order $\\mathcal{O}_{P}(\\sqrt[4]{\\log (n) / n})$.\n(ii) Expert estimation rates. Given the above parameter estimation rates and the inequality (6), we observe that the rates for estimating strongly identifiable experts $h\\left(\\cdot, \\eta_{j}^{*}\\right)$ range from $\\mathcal{O}_{P}(\\sqrt[4]{\\log (n) / n})$ to $\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$. Notably, these rates apply even for polynomial experts of degree at least two, i.e. $h(x,(a, b))=\\left(a^{\\top} x+b\\right)^{p}$ with $p \\geq 2$, as they satisfy the strong identifiability condition. By contrast, the estimation rates for polynomial experts when using the vanilla cosine router (see Theorem 2) and the linear router (see Theorem 4.6, (Nguyen et al., 2024c)) are significantly slower and could be of order $\\mathcal{O}_{P}\\left(1 / \\log ^{\\tau}(n)\\right)$, where $\\tau>0$ is some constant (see also Table 1). This observation highlights that our proposed perturbed cosine router is more sample efficient than both the linear router and the cosine router.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 4 Practical Implications \n\nWe now discuss two important practical implications from the theoretical results of the paper.\n\n1. Router and expert design: From the benefits of the perturbed cosine router for the expert estimation of MoE models, our theories suggest that when using the cosine router to avoid the representation collapse, practitioners should add noises to $L^{2}$ norms of the token hidden representations and the expert embeddings to achieve a favorable performance. Additionally, the strong identifiability condition also verifies the advantages of using non-linear expert networks over linear ones.\n2. Misspecified settings. Thus far in the paper, we have only considered well-specified settings, namely, the data are assumed to be sampled from the (perturbed) cosine router MoE. Although it may look restrictive, the results under this setting lay an important foundation for a more realistic misspecified setting where the data are not necessarily generated from those models. Under that misspecified setting, we assume that the data are generated from a regression framework as in equation (1) but with an arbitrary regression function $q(\\cdot)$, which is not a (perturbed) cosine router MoE. Then, we can demonstrate that the LSE $\\widehat{G}_{n}$ converges to a mixing measure $\\bar{G} \\in \\arg \\min _{G \\in \\mathcal{G}_{k}(\\Theta)}\\left\\|q-f_{G}\\right\\|_{L^{2}(\\mu)}$, where $f_{G}(\\cdot)$ is a regression function taking the form of the (perturbed) cosine router MoE. Furthermore, the optimal mixing measure will be in the boundary of the parameter space $\\mathcal{G}_{k}(\\Theta)$, namely, $\\bar{G}$ has $k$ atoms. Thus, as $n$ becomes sufficiently large, $\\widehat{G}_{n}$ also has $k$ atoms. The insights from our theories for the well-specified setting indicate that the Voronoi losses can be used to obtain the estimation rates of individual parameters of the LSE $\\widehat{G}_{n}$ to those of $\\bar{G}$ and therefore, achieve the following expert estimation rates under the misspecified settings, which will be empirically validated via numerical experiments in Appendix F:\n(2.1) Cosine router MoE: the worst expert estimation rate could be as slow as $\\mathcal{O}_{P}\\left(1 / \\log ^{\\tau}(n)\\right)$ for some $\\tau>0$. It indicates that we still need an exponential number of data (roughly $\\exp \\left(1 / \\epsilon^{\\tau}\\right)$ where $\\epsilon$ is the desired approximation error) to estimate the experts as well as select important experts.\n(2.2) Perturbed cosine router MoE: the slowest expert estimation rate is of order $\\mathcal{O}_{P}\\left(n^{-1 / 4}\\right)$. Thus, we only need a polynomial number of data (roughly $\\epsilon^{-4}$ ) to estimate the experts. This explains why the perturbed cosine router is a solution to the parameter estimation problem, or more generally, the expert estimation problem of the MoE models.\nHowever, the convergence analysis under the misspecified setting suffers from the challenges of understanding the universal approximation power of the (perturbed) cosine router, which have remained elusive in the literature. However, since this is beyond the scope of our paper, we leave it for future development.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "## 5 EXPERIMENTS\n\nIn this section, we first conduct numerical experiments on synthetic data (cf. Section 5.1), and then carry out experiments with real data on language modeling (cf. Section 5.2) and domain generalization (cf. Section 5.3) tasks. Our main goal is to empirically demonstrate the efficacy of the perturbed cosine router over the vanilla cosine router and the linear router in MoE models.\n\n![img-0.jpeg](img-0.jpeg)\n(a) Exact-specified setting with $k=k_{*}=8$ experts\n![img-1.jpeg](img-1.jpeg)\n(b) Over-specified setting with $k=k_{*}+1=9$\n\nFigure 1: Logarithmic plots displaying empirical convergence rates. Subfigures 1a and 1b depict the empirical averages of the Voronoi losses $\\mathcal{L}_{3}\\left(\\widehat{G}_{n}, G_{*}\\right)$ (cf. equation (12)) and $\\mathcal{L}_{2}\\left(\\widehat{G}_{n}, G_{*}\\right)$ (cf. equation (11)) for the exact and over-specified settings, respectively. The blue lines depict the Voronoi loss associated with the perturbed router, whereas the green lines are indicative of the Voronoi loss associated with the standard cosine router. The red dash-dotted lines are used to illustrate the fitted lines for determining the empirical convergence rate.",
      "tables": {},
      "images": {
        "img-0.jpeg": "data:image/jpeg;base64,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",
        "img-1.jpeg": "data:image/jpeg;base64,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"
      }
    },
    {
      "section_id": 6,
      "text": "# 5.1 NUMERICAL EXPERIMENTS \n\nWe first perform numerical experiments on synthetic data to empirically verify the theoretical convergence rates of the least squares estimation for both perturbed and vanilla cosine router MoE models. We generate synthetic data based on the model described in equation (1). Specifically, we generate $\\left\\{\\left(X_{i}, Y_{i}\\right)\\right\\}_{i=1}^{n} \\subset \\mathbb{R}^{d} \\times \\mathbb{R}$ by first sampling $X_{i} \\sim \\operatorname{Uniform}\\left([-1,1]^{d}\\right)$ for $i=1, \\ldots, n$. Then, we generate $Y_{i}$ according to the model: $Y_{i}=f_{G_{*}}\\left(X_{i}\\right)+\\varepsilon_{i}$ for $i \\in[n]$, where the regression function $f_{G_{*}}(\\cdot)$ is defined as: $f_{G_{*}}(x):=\\sum_{i=1}^{k_{*}} \\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1}^{*}\\right)^{\\top} x}{\\left(\\|\\beta_{1}^{*}\\|+\\tau\\right)(\\|x\\|+\\tau)}+\\beta_{0}^{*}\\right) \\cdot \\phi\\left(\\left(a_{i}^{*}\\right)^{\\top} x+b_{i}^{*}\\right)$. The input data dimension is set at $d=32$. We employ $k_{*}=8$ experts of the form $\\phi\\left(\\left(a_{i}^{*}\\right)^{\\top} x+b_{i}^{*}\\right)$, where the activation function $\\phi$ is set to be the ReLU function. The details of the values of the parameters as well as the training procedure are in Appendix E.1.\nResults. Two experimental settings are examined: (1) Exact-specified, and (2) Over-specified. In the exact-specified setting, the model is fitted with the same number of experts as the data generation model, specifically $k=k_{*}=8$. In the over-specified setting, the model includes one additional expert, totaling $k=k_{*}+1=9$ experts. In each setting, experiments are conducted using both the standard and the perturbed cosine routers, with $\\tau$ set to zero for the standard router and 0.1 for the perturbed router. For each experiment, we calculate the Voronoi losses for every model and report the mean values for each sample size in Figure 1. Error bars representing two standard deviations are also shown. In Figure 1a, the empirical convergence rates of both the standard and perturbed routers are analyzed under the exact-specified setting. The perturbed router shows a rapid convergence rate of $\\mathcal{O}\\left(n^{-0.5}\\right)$, while the standard vanilla router has a noticeably slower rate of $\\mathcal{O}\\left(n^{-0.11}\\right)$. Similarly, in Figure 1b, the convergence rates are assessed for the same routers under the over-specified setting. Here, the perturbed router again shows a faster convergence rate of $\\mathcal{O}\\left(n^{-0.47}\\right)$, compared to the cosine router's slower rate of $\\mathcal{O}\\left(n^{-0.05}\\right)$.\nMisspecified settings. In Appendix F, we will also conduct numerical experiments for comparing the sample efficiency of the cosine router and its perturbed variant under the setting where the data are generated from the regression framework with the same regression function.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "### 5.2 LANGUAGE MODELING\n\nIn this section, we focus on the language modeling task (Bahl et al., 1983), a fundamental challenge in natural language processing that involves predicting the next word or character in a sequence to evaluate a model's ability to generate and understand text. To assess how different routers influence the model's ability to capture linguistic structures and enhance performance across varying levels of textual granularity, we compare the performance of perturbed and vanilla cosine router Mixture of Experts (MoEs) on both character-level (Graves, 2013) and word-level (Bengio et al., 2000) tasks.\nDatasets. We evaluate the model's pre-training capabilities on character-level language modeling using Enwik8 and Text8 datasets (Mahoney, 2011), and assess its word-level language modeling performance on Wikitext-103 (Merity et al., 2016).\n\nTable 2: Performance of vanilla and perturbed cosine routers on language modeling tasks.\n\n![table_1](table_1)\n\nMetrics. To quantify the performance of our perturbed cosine router relative to the vanilla cosine one, we utilize the Bit Per Character (BPC) metric (Graves, 2013) for character-level language modeling and Perplexity (PPL) (Jelinek et al., 1977) for word-level language modeling tasks.\nArchitecture and training procedure. In order to alleviate the representation collapse issue associated with estimating routing scores in the original space, we follow Chi et al. (2022) to first project input representations on lower-dimensional space and parameterize experts with corresponding lowerdimensional embeddings. Subsequently, we calculate the routing scores of inputs and embeddings in this reduced-dimensional space using our proposed perturbed cosine router. Our experiments adopt the Switch Transformer (Fedus et al., 2021), which is fundamentally a sparse variant of the T5 encoder-decoder (Raffel et al., 2020), with MoE layers replacing the MLPs. Detailed information regarding the datasets, metrics, training setup, and hyperparameters for this task is provided in Appendix E.2.\nResults. The empirical advantage of our proposed cosine router over the vanilla version when applied to language modeling tasks is demonstrated in Table 2. The results indicate that the perturbed cosine router enhances the performance of the original cosine router in all datasets across both small and medium configurations. It notably improves results for the Enwik8 and Text8 datasets at various scales and slightly outperforms the original cosine router for the Wikitext-103 dataset.",
      "tables": {
        "table_1": "| Router/Experts | Enwik8 (BPC $\\downarrow$ ) |  | Text8 (BPC $\\downarrow$ ) |  | Wikitext-103 (PPL $\\downarrow$ ) |  |\n| :-- | :--: | :--: | :--: | :--: | :--: | :--: |\n|  | Small | Medium | Small | Medium | Small | Medium |\n| Cosine | 1.213 | 1.161 | 1.310 | 1.271 | 90.070 | 38.018 |\n| Perturbed cosine | $\\mathbf{1 . 1 9 7}$ | $\\mathbf{1 . 1 4 7}$ | $\\mathbf{1 . 3 0 3}$ | $\\mathbf{1 . 2 5 1}$ | $\\mathbf{8 9 . 9 1 0}$ | $\\mathbf{3 7 . 8 5 9}$ |"
      },
      "images": {}
    },
    {
      "section_id": 8,
      "text": "# 5.3 DOMAIN GENERALIZATION \n\nWe conduct experiments on the applications of MoE models in domain generalization. Our objective is to empirically demonstrate the efficacy of our proposed perturbed cosine router over the vanilla cosine router in this field. Domain generalization (Zhou et al., 2023a) aims to generalize a model's performance to unseen test domains with distributions different from those encountered during training. Specifically, in domain generalization, a model is expected to leverage multiple training datasets gathered from various domains and exhibit robustness to domain shifts during testing. Such ability of out-of-distribution generalization largely hinges on the model's capability to incorporate invariances across multiple domains ( Li et al., 2023). Given that distribution shifts in data correspond to distribution shifts in (visual) attributes (Wiles et al., 2022), capturing these diverse attributes and aligning them with invariant correlations is crucial. Mixture of Experts emerges as a powerful tool for efficiently capturing these visual attributes, and it has been proven effective in enhancing performance in domain generalization (Li et al., 2023). Therefore, we further justify the effectiveness of our perturbed cosine router in domain generalization.\nDatasets. We followed the experimental setting of Li et al. (2023) and evaluated our method using 5 benchmark datasets in DomainBed: PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Each dataset is comprised of images for classification tasks from different domains.\nArchitecture. Following Gulrajani \\& Lopez-Paz (2021), we conduct experiments on ViT-S/16, which has an input patch size of $16 \\times 16$, comprising 6 heads in multi-head attention layers, and a total of 12 transformer blocks. We adopt a last-two two-layer configuration, where each MoE block comprises 6 experts. The router selects the top 2 out of 6 experts for each image patch.\nTraining procedure and result. We follow the training-domain validation procedure outlined in (Li et al., 2023; Gulrajani \\& Lopez-Paz, 2021), where each training domain is split into training and validation subsets. The final overall validation set consists of the validation subsets from all training domains. Subsequently, we select the model with the highest performance on the overall validation set. To ensure fair comparisons, the results are averaged over three runs.\nTable 3 summarizes the experimental results. For each dataset, we report the average results across test domains. The results demonstrate that our perturbed cosine router consistently outperforms the linear and vanilla cosine router across all datasets, thereby convincingly justifying the effectiveness of adding noise to cosine routers. Detailed performances for each domain are reported in Tables 4 and 5.\n\nTable 3: Average out-of-distribution test accuracies.\n\n![table_2](table_2)\n\nTable 4: Per-domain performance of PACS, VLCS, OfficeHome, TerraIncognita.\n\n![table_3](table_3)\n\nTable 5: Per-domain performance of DomainNet.\n\n![table_4](table_4)",
      "tables": {
        "table_2": "| Router/Experts | PACS | VLCS | OfficeHome | TerraIncognita | DomainNet | Avg. |\n| :-- | :--: | :--: | :--: | :--: | :--: | :--: |\n| Linear | 86.33 | 78.15 | 73.02 | 41.30 | 48.19 | 65.40 |\n| Cosine | 87.22 | 78.99 | 73.27 | 45.55 | 48.45 | 66.70 |\n| Perturbed cosine | $\\mathbf{8 9 . 3 6}$ | $\\mathbf{8 0 . 0 1}$ | $\\mathbf{7 4 . 0 9}$ | $\\mathbf{4 9 . 8 7}$ | $\\mathbf{4 8 . 5 1}$ | $\\mathbf{6 8 . 3 7}$ |",
        "table_3": "|  |  | Router/Experts | A | C | $\\mathbf{P}$ | $\\mathbf{S}$ |\n| :-- | :-- | :--: | :--: | :--: | :--: | :--: |\n| PACS | Linear | 87.29 | 81.20 | $\\mathbf{9 8 . 5 0}$ | 78.34 |  |\n|  | Cosine | 89.24 | 86.11 | 97.60 | 75.92 |  |\n|  | Perturbed cosine | $\\mathbf{8 9 . 8 7}$ | $\\mathbf{8 6 . 9 7}$ | 97.90 | $\\mathbf{8 2 . 6 8}$ |  |\n|  |  | Router/Experts | $\\mathbf{C}$ | $\\mathbf{L}$ | $\\mathbf{S}$ | $\\mathbf{V}$ |\n| VLCS | Linear | 97.53 | 63.65 | 74.09 | 77.33 |  |\n|  | Cosine | 98.59 | 67.42 | 70.88 | $\\mathbf{7 9 . 0 7}$ |  |\n|  | Perturbed cosine | 98.59 | $\\mathbf{6 7 . 8 0}$ | $\\mathbf{7 4 . 7 0}$ | 78.95 |  |\n|  |  | Router/Experts | $\\mathbf{A}$ | $\\mathbf{C}$ | $\\mathbf{P}$ | $\\mathbf{R}$ |\n| OfficeHome | Linear | 72.99 | 57.27 | 79.03 | 82.78 |  |\n|  | Cosine | 73.40 | 57.27 | 78.69 | 83.70 |  |\n|  | Perturbed cosine | $\\mathbf{7 4 . 6 4}$ | $\\mathbf{5 7 . 8 5}$ | $\\mathbf{7 9 . 5 9}$ | $\\mathbf{8 4 . 2 7}$ |  |\n|  |  | Router/Experts | $\\mathbf{1 . 1 0 0}$ | $\\mathbf{1 . 3 0}$ | $\\mathbf{1 . 4 3}$ | $\\mathbf{1 . 4 6}$ |\n| TerraIncognita | Linear | 45.99 | 28.51 | 54.66 | 36.05 |  |\n|  | Cosine | 50.00 | 37.49 | 53.02 | 41.67 |  |\n|  | Perturbed cosine | $\\mathbf{5 7 . 5 9}$ | $\\mathbf{4 3 . 3 0}$ | $\\mathbf{5 6 . 9 3}$ | 41.67 |  |",
        "table_4": "|  | Router/Experts | clipart | infograph | painting | quickdraw | real | sketch |\n| :-- | :-- | :--: | :--: | :--: | :--: | :--: | :--: |\n| DomainNet | Linear | $\\mathbf{6 9 . 1 1}$ | $\\mathbf{2 4 . 9 5}$ | 54.81 | 16.88 | 68.95 | 54.41 |\n|  | Cosine | 68.05 | 24.48 | $\\mathbf{5 5 . 7 5}$ | 17.39 | 69.41 | 55.59 |\n|  | Perturbed | 68.31 | 24.52 | 55.03 | $\\mathbf{1 7 . 9 0}$ | $\\mathbf{6 9 . 4 6}$ | $\\mathbf{5 5 . 8 3}$ |"
      },
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 6 CONCLUSION \n\nIn this paper, we investigate the impacts of the cosine router on the convergence rates of least squares estimation in MoE models. We figure out that owing to the parameter interaction inside the cosine router expressed by a PDE, the rates for estimating parameters and experts are slower than any polynomial rates and therefore, could be as slow as $\\mathcal{O}_{P}\\left(1 / \\log ^{r}(n)\\right)$. In response to this issue, we propose using the perturbed cosine router where we add noises to the $L^{2}$ norms of the token representations and the expert embeddings in the cosine router in order to eliminate the previous parameter interaction. Equipped with this novel router, we demonstrate that if the expert function satisfies the strong identifiability condition, then the parameter and expert estimation rates are significantly improved to be of polynomial orders. Finally, we conduct several experiments on both synthetic and real-world data to empirically justify the theoretical results.\n\nThere are a few limitations in our current analysis. First of all, the assumption that the data are sampled from the (perturbed) cosine router MoE is often violated in real-world settings. However, as discussed in Section 4, our theories can totally be extended to a more realistic misspecified setting where the data are not necessarily generated from those models, which we leave for future development. Second, since the ground-truth parameters are implicitly assumed to be independent of the sample size $n$, the parameter and expert estimation rates presented in this work are point-wise rather than uniform. To deal with this problem, we can utilize the techniques for characterizing the uniform parameter estimation rates in traditional mixture models (see (Heinrich \\& Kahn, 2018; Do et al., 2023; Yan et al., 2025)). Nevertheless, since the adaptation of those techniques to the setting of the (perturbed) cosine router MoE is still challenging due to the complex structures of the (perturbed) cosine router, we believe that further technical tools need to be developed to achieve the desired uniform estimation rates.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# ACKNOWLEDGEMENTS \n\nNH acknowledges support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "## REPRODUCIBILITY STATEMENT\n\nTo facilitate the reproduction of our empirical results, we present detailed descriptions of the data and the experimental setup in Section 5 and Appendix E. We will release our code upon the acceptance of our submission. All datasets used in this study are publicly available, enabling full replication of our experiments.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "## REFERENCES\n\nLalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-5(2): 179-190, 1983. doi: 10.1109/TPAMI.1983.4767370.\n\nYoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. In T. Leen, T. Dietterich, and V. Tresp (eds.), Advances in Neural Information Processing Systems, volume 13. MIT Press, 2000. URL https://proceedings.neurips.cc/paper files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf.\n\nZixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. Towards understanding the mixture-of-experts layer in deep learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 23049-23062. Curran Associates, Inc., 2022.\n\nZewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation collapse of sparse mixture of experts. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=mWaYC6CZf5.\n\nYinlam Chow, Azamat Tulepbergenov, Ofir Nachum, Dhawal Gupta, Moonkyung Ryu, Mohammad Ghavamzadeh, and Craig Boutilier. A Mixture-of-Expert Approach to RL-based Dialogue Management. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=4FBUihxz5nm.\n\nDamai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.04088, 2024.\n\nDat Do, Huy Nguyen, Khai Nguyen, and Nhat Ho. Minimax optimal rate for parameter estimation in multivariate deviated models. In Advances in Neural Information Processing Systems, volume 36, pp. 30096-30133. Curran Associates, Inc., 2023.\n\nNikoli Dryden and Torsten Hoefler. Spatial mixture-of-experts. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=AlkMMzUX95.\n\nWilliam Fedus, Barret Zoph, and Noam M. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1-120:39, 2021. URL https://api.semanticscholar.org/CorpusID:231573431.\n\nBolin Gao and Lacra Pavel. On the properties of the softmax function with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 2018.\n\nAlex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented Transformer for Speech Recognition. In Proc. Interspeech 2020, pp. 5036-5040, 2020. doi: 10.21437/Interspeech.2020-3015.\n\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=1QdXeXDoWtI.\n\nXing Han, Huy Nguyen, Carl Harris, Nhat Ho, and Suchi Saria. Fusemoe: Mixture-of-experts transformers for fleximodal fusion. In Advances in Neural Information Processing Systems, 2024.\n\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong, and Ed Chi. DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning. In M. Ranzato, A. Beygetizimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 29335-29347. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/ 2021/file/f5ac21cd0ef1b88e9848571aeb53551a-Paper.pdf.\nP. Heinrich and J. Kahn. Strong identifiability and optimal minimax rates for finite mixture estimation. The Annals of Statistics, 46(6):2844-2870, 2018.\n\nNhat Ho, Chiao-Yu Yang, and Michael I. Jordan. Convergence rates for Gaussian mixtures of experts. Journal of Machine Learning Research, 23(323):1-81, 2022.\n\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computation, 3, 1991.\n\nFred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity-a measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1): S63-S63, 1977.\n\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts. arsiv preprint arsiv 2401.04088, 2024.\nM. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6:181-214, 1994.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\nYamuna Krishnamurthy, Chris Watkins, and Thomas Gaertner. Improving expert specialization in mixture of experts. arXiv preprint arXiv:2302.14703, 2023.\n\nMinh Le, An Nguyen, Huy Nguyen, Trang Nguyen, Trang Pham, Linh Van Ngo, and Nhat Ho. Mixture of experts meets prompt-based continual learning. In Advances in Neural Information Processing Systems, volume 38, 2024.\n\nMinh Le, Chau Nguyen, Huy Nguyen, Quyen Tran, Trung Le, and Nhat Ho. Revisiting prefixtuning: Statistical benefits of reparameterization among prompts. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=QjTSaFXg25.\n\nBo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, and Ziwei Liu. Sparse mixture-of-experts are domain generalizable learners. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=RecZ9nB9Q4.\n\nHongbo Li, Sen Lin, Lingjie Duan, Yingbin Liang, and Ness Shroff. Theory on mixture-of-experts in continual learning. In The Thirteenth International Conference on Learning Representations, 2025.\n\nHanxue Liang, Zhiwen Fan, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai Zou, Yu Cheng, Cong Hao, and Zhangyang Wang. $\\mathrm{M}^{3}$ ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b653f34d576d1790481e3797cb740214-Abstract-Conference.html.\n\nBruce G. Lindsay. Mixture models: Theory, geometry and applications. In NSF-CBMS Regional Conference Series in Probability and Statistics. IMS, Hayward, CA., 1995.\n\nMatt Mahoney. Large text compression benchmark. http://www.mattmahoney.net/dc/ text.html, 2011.\n\nTudor Manole and Nhat Ho. Refined convergence rates for maximum likelihood estimation under finite mixture models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 14979-15006. PMLR, 17-23 Jul 2022.\n\nEduardo F. Mendes and Wenxin Jiang. On convergence rates of mixtures of polynomial experts. Neural Computation, 24(11):3025-3051, 2012. doi: 10.1162/NECO_a_00354.\n\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n\nHuy Nguyen, TrungTin Nguyen, and Nhat Ho. Demystifying softmax gating function in Gaussian mixture of experts. In Advances in Neural Information Processing Systems, 2023.\n\nHuy Nguyen, Pedram Akbarian, and Nhat Ho. Is temperature sample efficient for softmax Gaussian mixture of experts? In Proceedings of the ICML, 2024a.\n\nHuy Nguyen, Pedram Akbarian, Fanqi Yan, and Nhat Ho. Statistical perspective of top-k sparse softmax gating mixture of experts. In International Conference on Learning Representations, 2024b.\n\nHuy Nguyen, Nhat Ho, and Alessandro Rinaldo. On least square estimation in softmax gating mixture of experts. In Proceedings of the ICML, 2024c.\n\nTrungTin Nguyen, Faicel Chamroukhi, Hien Duy Nguyen, and Florence Forbes. Non-asymptotic model selection in block-diagonal mixture of polynomial experts models. arXiv preprint arXiv:2104.08959, 2021.\n\nFengchun Peng, Robert A. Jacobs, and Martin A. Tanner. Bayesian Inference in Mixtures-of-Experts and Hierarchical Mixtures-of-Experts Models With an Application to Speech Recognition. Journal of the American Statistical Association, 91(435):953-960, 1996. ISSN 01621459.\n\nQuang Pham, Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, and Nhat Ho. Competesmoe - effective training of sparse mixture of experts via competition. arXiv preprint arXiv:2402.02526, 2024.\n\nJoan Puigcerver, Carlosx Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts. In The Twelfth International Conference on Learning Representations, 2024.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.\n\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. In Advances in Neural Information Processing Systems, volume 34, pp. 8583-8595. Curran Associates, Inc., 2021.\n\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations (ICLR), 2017.\n\nSara van de Geer. Empirical processes in M-estimation. Cambridge University Press, 2000.\nOlivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. A fine-grained analysis on distribution shift. In International Conference on Learning Representations, 2022. URL https: / openreview. net / forum? id=D14LetuLdyK.\n\nFanqi Yan, Huy Nguyen, Dung Le, Pedram Akbarian, and Nhat Ho. Understanding expert structures on minimax parameter estimation in contaminated mixture of experts. In Proceedings of The 28th International Conference on Artificial Intelligence and Statistics, 2025.\n\nZhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts. In Interspeech, 2021.\n\nBin Yu. Assouad, Fano, and Le Cam. Festschrift for Lucien Le Cam, pp. 423-435, 1997.\nKaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):4396-4415, 2023a. doi: 10.1109/TPAMI.2022.3195549.\n\nYanqi Zhou, Nan Du, Yanping Huang, Daiyi Peng, Chang Lan, Da Huang, Siamak Shakeri, David So, Andrew M Dai, Yifeng Lu, et al. Brainformers: Trading simplicity for efficiency. In International Conference on Machine Learning, pp. 42531-42542. PMLR, 2023b.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# Supplementary Material for \"Statistical Advantages of Perturbing Cosine Router in Mixture of Experts\" \n\nIn this supplementary material, we first explore the exact-specified settings of the (perturbed) cosine router MoE model in Appendix A. Next, we provide proofs of theoretical results associated with the cosine router MoE and its perturbed counterpart in Appendix B and Appendix C, respectively. Those proofs are partially supported by auxiliary results presented in Appendix D. Subsequently, in Appendix E, we specify the details for the experiments performed in Section 5. Finally, we conduct further numerical experiments on the convergence of least squares estimation under the misspecified settings in Appendix F.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "## A Additional ReSults\n\nIn this appendix, we provide the convergence analysis of parameter and expert estimation under the exact-specified settings of the cosine router MoE and its perturbed variant in Appendix A. 1 and Appendix A.2, respectively.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "## A. 1 ExAct-SPECIFIED SETTING OF THE COSINE ROUTER MOE\n\nFirstly, we start with the exact-specified setting of the cosine router MoE.\nRecall that under the exact-specified setting, the true number of experts $k_{*}$ is known. According to the proof technique for deriving parameter estimation rates under the exact-specified setting in the literature (Nguyen et al., 2023), a key step is to apply the first-order Taylor expansions to the product of the softmax's numerator and the expert function, i.e. $H\\left(x, \\beta_{1}, \\eta\\right):=\\exp \\left(\\frac{\\beta_{1}^{\\top} x}{\\left\\|\\beta_{1}\\right\\|\\|x\\|}\\right) h(x, \\eta)$. However, since the parameter interaction via the PDE in equation (4), i.e. $\\beta_{1}^{\\gamma} \\frac{\\partial H}{\\partial \\beta_{1}}\\left(x, \\beta_{1}, \\eta\\right)=0$, holds even for the first derivatives of the function $H$, the convergence of LSE under the exact-specified setting is illustrated by minimax lower bound for estimating $\\bar{G}_{*}$ in Theorem 5.\nTheorem 5. Under the exact-specified setting, the following minimax lower bound of estimating $G_{*}$\n\n$$\n\\inf _{\\bar{G}_{n} \\in \\mathcal{E}_{k_{*}}(\\Theta)} \\sup _{G \\in \\mathcal{E}_{k_{*}}(\\Theta)} \\mathbb{E}_{f_{G}}\\left[\\mathcal{L}_{1, r}\\left(\\bar{G}_{n}, G\\right)\\right] \\gtrsim n^{-1 / 2}\n$$\n\nholds true for any $r \\geq 1$, where $\\mathbb{E}_{f_{G}}$ indicates the expectation taken w.r.t the product measure with $f_{G}^{n}$ and the infimum is over all estimators taking values in $\\mathcal{E}_{k_{*}}(\\Theta)$.\n\nProof of Theorem 5 is deferred to Appendix B.2. It can be seen that the convergence behavior of parameter and expert estimation under the exact-specified setting is analogous to that under the over-specified setting. That is, the rates for estimating parameters $\\beta_{1 j}^{*}$ and $\\eta_{j}^{*}$ as well as experts $h\\left(\\cdot, \\eta_{j}^{*}\\right)$ are slower than any polynomial rates and thus, could be as slow as $\\mathcal{O}_{P}\\left(1 / \\log ^{\\tau}(n)\\right)$, where $\\tau>0$ is some constant.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "## A. 2 Exact-SPECIfied Setting of the Perturbed Cosine Router MoE\n\nWe now consider the exact-specified setting of the perturbed cosine router MoE model (7). To begin with, we introduce a condition called weak identifiability on the expert function $h(\\cdot, \\eta)$ to characterize which experts have faster estimation rates than others under this setting.\nDefinition 2 (Weak identifiability). An expert function $x \\mapsto h(x, \\eta)$ is said to be weakly identifiable if it is differentiable w.r.t its parameter $\\eta$ and the set of functions in $x$\n\n$$\n\\left\\{\\frac{\\partial^{\\left|\\alpha_{1}\\right|+\\left|\\alpha_{2}\\right|} \\tilde{H}}{\\partial \\beta_{1}^{\\alpha_{1}} \\partial \\eta^{\\alpha_{2}}}\\left(x, \\beta_{1 i}, \\eta_{i}\\right): \\alpha_{1} \\in \\mathbb{N}^{d_{1}}, \\alpha_{2} \\in \\mathbb{N}^{d_{2}}, 0 \\leq\\left|\\alpha_{1}\\right|+\\left|\\alpha_{2}\\right| \\leq 1\\right\\}\n$$\n\nis linearly independent for almost every $x$, for any $k \\geq 1$ and pair-wise distinct parameters $\\eta_{1}, \\ldots, \\eta_{k}$, where we denote $\\tilde{H}\\left(x, \\beta_{1}, \\eta\\right):=\\exp \\left(\\frac{\\beta_{1}^{\\gamma} x}{\\left(\\left\\|\\beta_{1}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}\\right) h(x, \\eta)$.\n\nRecall from the \"Technical challenges\" paragraph in Section 1 that a key step to establish the expert estimation rates is to decompose the difference $f_{\\widehat{G}_{n}}(x)-f_{G_{*}}(x)$ into a combination of linearly independent terms via Taylor expansions to the function $H\\left(\\cdot, \\beta_{1}, \\eta\\right)$. Therefore, the purpose of the weak identifiability condition is to avoid all potential parameter interactions as in equation (4), which may lead to undesirable linearly dependent terms.\n\nExample. For simplicity, we consider experts formulated as neural networks, i.e. $h(x,(a, b))=$ $\\phi\\left(a^{\\top} x+b\\right)$. It can be validated that if the function $\\phi(\\cdot)$ is either a popular activation such as $\\operatorname{ReLU}(\\cdot)$ and $\\tanh (\\cdot)$ or a polynomial $\\phi(z)=z^{p}$, for any $p \\in \\mathbb{N}$, then the expert $h(x,(a, b))$ is weakly identifiable. On the other hand, a constant expert $h(\\cdot, \\eta)=$ constant fails to satisfy the weak identifiability the condition.\n\nNext, we will use the Voronoi loss function $\\mathcal{L}_{3}\\left(G, G_{*}\\right)$ defined below to determine the estimation rates for weakly identifiable experts in Theorem 6, whose proof can be found in Appendix C.2:\n\n$$\n\\mathcal{L}_{3}\\left(G, G_{*}\\right):=\\sum_{j=1}^{k_{*}}\\left|\\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}\\right)-\\exp \\left(\\beta_{0 j}^{*}\\right)\\right|+\\sum_{j=1}^{k_{*}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}\\right)\\left[\\left\\|\\Delta \\beta_{1 i j}\\right\\|+\\left\\|\\Delta \\eta_{i j}\\right\\|\\right]\n$$\n\nTheorem 6. Assume that $h(\\cdot, \\eta)$ is a weakly identifiable expert function, then the following lower bound holds true for any $G \\in \\mathcal{E}_{k_{*}}(\\Theta)$ :\n\n$$\n\\left\\|g_{G}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} \\gtrsim \\mathcal{L}_{3}\\left(G, G_{*}\\right)\n$$\n\nFurthermore, this bound and the result in Theorem 3 imply that $\\mathcal{L}_{3}\\left(\\widehat{G}_{n}, G_{*}\\right)=\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$. The bound $\\mathcal{L}_{3}\\left(\\widehat{G}_{n}, G_{*}\\right)=\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$ all the parameters $\\beta_{1 j}^{*}, \\eta_{j}^{*}$ enjoy the same parametric estimation rates, standing at order $\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$. Furthermore, by employing the argument in equation (6), we deduce that the rates for estimating experts $h\\left(\\cdot, \\eta_{j}^{*}\\right)$ are also of order $\\mathcal{O}_{P}(\\sqrt{\\log (n) / n})$. Those rates are substantially faster than their counterparts when using the vanilla cosine router, which could be as slow as $\\mathcal{O}_{P}\\left(1 / \\log ^{s}(n)\\right)$ (see Theorem 5). This comparison highlights the benefits of our proposed perturb cosine router over the vanilla cosine router.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# B Proof of Results for Cosine Router MoE \n\nIn this appendix, we provide proofs for the theoretical results regarding the cosine router in stated in Section 2, including Theorem 1, Theorem 5, and Theorem 2, in that order.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "## B. 1 Proof of Theorem 1\n\nFirst of all, let us introduce the definitions of some necessary concepts for the proof, namely an $\\varepsilon$-bracket, a bracketing number, a bracketing entropy, an $\\varepsilon$-cover and a covering number. In particular, let $(\\mathcal{R},\\|\\cdot\\|)$ be the space of real-valued functions $f: \\mathcal{X} \\rightarrow \\mathbb{R}$. Then, the aforementioned concepts are defined as follows:\nDefinition 3 ( $\\varepsilon$-bracket). Given two functions $L(\\cdot)$ and $U(\\cdot)$, the bracket $[L, U]$ is the set of all functions $f \\in \\mathcal{R}$ such that $L(x) \\leq f(x) \\leq U(x)$ for all $x \\in \\mathcal{X}$, and $\\|U-L\\| \\leq \\varepsilon$.\nDefinition 4 (Bracketing number). The bracketing number $N_{\\|}(\\varepsilon, \\mathcal{R},\\|\\cdot\\|)$ is the minimum number of $\\varepsilon$-brackets needed to cover $\\mathcal{R}$.\nDefinition 5 (Bracketing entropy). The bracketing entropy $H_{B}(\\varepsilon, \\mathcal{R},\\|\\cdot\\|)$ is the logarithm of the bracketing number $N_{\\|}(\\varepsilon, \\mathcal{R},\\|\\cdot\\|)$.\nDefinition 6 ( $\\varepsilon$-cover). An $\\varepsilon$-cover of the set $\\mathcal{R}$ under some norm $\\|\\cdot\\|$ is a set $\\left\\{\\pi_{1}, \\ldots, \\pi_{N}\\right\\}$ such that for any $f \\in \\mathcal{R}$, there exists some $i \\in[N]$ such that $\\left\\|f-\\pi_{i}\\right\\| \\leq \\varepsilon$.\nDefinition 7 (Covering number). The $\\varepsilon$-covering number $N(\\varepsilon, \\mathcal{R},\\|\\cdot\\|)$ is the minimum number of balls $B(\\pi ; \\varepsilon)=\\{f \\in \\mathcal{R}:\\|f-\\pi\\| \\leq \\varepsilon\\}$ need to cover $\\mathcal{R}$.\n\nSubsequently, we denote by $\\mathcal{R}_{k}(\\Theta)$ the set of regression functions w.r.t mixing measures in $\\mathcal{G}_{k}(\\Theta)$, that is, $\\mathcal{R}_{k}(\\Theta):=\\left\\{f_{G}(x): G \\in \\mathcal{G}_{k}(\\Theta)\\right\\}$. Additionally, for each $\\delta>0$, the $L^{2}$ ball centered around the regression function $f_{G_{*}}$ and intersected with the set $\\mathcal{R}_{k}(\\Theta)$ is defined as\n\n$$\n\\mathcal{R}_{k}(\\Theta, \\delta):=\\left\\{f \\in \\mathcal{R}_{k}(\\Theta):\\left\\|f-f_{G_{*}}\\right\\|_{L^{2}(\\mu)} \\leq \\delta\\right\\}\n$$\n\nIn order to measure the size of the above set, van de Geer (2000) suggest using the following quantity:\n\n$$\n\\mathcal{J}_{B}\\left(\\delta, \\mathcal{R}_{k}(\\Theta, \\delta)\\right):=\\int_{\\delta^{2} / 2^{13}}^{\\delta} H_{B}^{1 / 2}\\left(t, \\mathcal{R}_{k}(\\Theta, t), \\|\\cdot\\|_{L^{2}(\\mu)}\\right) \\mathrm{d} t \\vee \\delta\n$$\n\nwhere $H_{B}\\left(t, \\mathcal{R}_{k}(\\Theta, t), \\|\\cdot\\|_{L^{2}(\\mu)}\\right)$ stands for the bracketing entropy (van de Geer, 2000) of $\\mathcal{R}_{k}(\\Theta, u)$ under the $L^{2}$-norm, and $t \\vee \\delta:=\\max \\{t, \\delta\\}$. By using the similar proof argument of Theorem 7.4 and Theorem 9.2 in (van de Geer, 2000) with notations being adapted to this work, we obtain the following lemma:\nLemma 1. Take $\\Psi(\\delta) \\geq \\mathcal{J}_{B}\\left(\\delta, \\mathcal{R}_{k}(\\Theta, \\delta)\\right)$ that satisfies $\\Psi(\\delta) / \\delta^{2}$ is a non-increasing function of $\\delta$. Then, for some universal constant $c$ and for some sequence $\\left(\\delta_{n}\\right)$ such that $\\sqrt{n} \\delta_{n}^{2} \\geq c \\Psi\\left(\\delta_{n}\\right)$, we achieve that\n\n$$\n\\mathbb{P}\\left(\\left\\|f_{\\tilde{G}_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}>\\delta\\right) \\leq c \\exp \\left(-\\frac{n \\delta^{2}}{c^{2}}\\right)\n$$\n\nfor all $\\delta \\geq \\delta_{n}$.\nGeneral picture. We first show that when the expert functions are Lipschitz continuous, the following bound holds for any $0<\\varepsilon \\leq 1 / 4$ :\n\n$$\nH_{B}\\left(\\varepsilon, \\mathcal{R}_{k}(\\Theta, \\varepsilon),\\|.\\|_{L^{2}(\\mu)}\\right) \\lesssim \\log (1 / \\varepsilon)\n$$\n\nGiven this bound, it follows that\n\n$$\n\\mathcal{J}_{B}\\left(\\delta, \\mathcal{R}_{k}(\\Theta, \\delta)\\right)=\\int_{\\delta^{2} / 2^{13}}^{\\delta} H_{B}^{1 / 2}\\left(t, \\mathcal{R}_{k}(\\Theta, t), \\|\\cdot\\|_{L^{2}(\\mu)}\\right) \\mathrm{d} t \\vee \\delta \\lesssim \\int_{\\delta^{2} / 2^{13}}^{\\delta} \\log (1 / t) \\mathrm{d} t \\vee \\delta\n$$\n\nLet $\\Psi(\\delta)=\\delta \\cdot[\\log (1 / \\delta)]^{1 / 2}$, then $\\Psi(\\delta) / \\delta^{2}$ is a non-increasing function of $\\delta$. Furthermore, equation (15) indicates that $\\Psi(\\delta) \\geq \\mathcal{J}_{B}\\left(\\delta, \\mathcal{R}_{k}(\\Theta, \\delta)\\right)$. In addition, let $\\delta_{n}=\\sqrt{\\log (n) / n}$, then we get that $\\sqrt{n} \\delta_{n}^{2} \\geq c \\Psi\\left(\\delta_{n}\\right)$ for some universal constant $c$. Finally, by applying Lemma 1, we achieve the desired conclusion of the theorem. As a consequence, it suffices to demonstrate the bound (14).\nProof for the bound (14). In order to prove the bracketing entropy bound in equation (14), we leverage the proof arguments for the convergence of regression estimation in (Nguyen et al., 2024c).\nSince the expert functions are Lipschitz continuous, then for any function $f_{G} \\in \\mathcal{R}_{k}(\\Theta)$, we have that $f_{G}(x) \\leq M$ for all $x$ where $M>0$ is some constant.\nLet $\\tau \\leq \\varepsilon$ and $\\left\\{\\pi_{1}, \\ldots, \\pi_{N}\\right\\}$ be the $\\tau$-cover under the $L^{\\infty}$ norm of the set $\\mathcal{R}_{k}(\\Theta)$ where $N:=$ $N\\left(\\tau, \\mathcal{R}_{k}(\\Theta), \\|\\cdot\\|_{L^{\\infty}}\\right)$ is the $\\tau$-covering number of the metric space $\\left(\\mathcal{R}_{k}(\\Theta), \\|\\cdot\\|_{L^{\\infty}}\\right)$. Then, we construct the brackets of the form $\\left[L_{i}(x), U_{i}(x)\\right]$ for all $i \\in[N]$ as follows:\n\n$$\n\\begin{aligned}\n& L_{i}(x):=\\max \\left\\{\\pi_{i}(x)-\\tau, 0\\right\\} \\\\\n& U_{i}(x):=\\max \\left\\{\\pi_{i}(x)+\\tau, M\\right\\}\n\\end{aligned}\n$$\n\nFrom the above formulation, it can be checked that $\\mathcal{R}_{k}(\\Theta) \\subset \\cup_{i=1}^{N}\\left[L_{i}(x), U_{i}(x)\\right]$, and $U_{i}(x)-$ $L_{i}(x) \\leq \\min \\{2 \\tau, M\\}$. Additionally, we get that\n\n$$\n\\left\\|U_{i}-L_{i}\\right\\|_{L^{2}(\\mu)}=\\left(\\int\\left[U_{i}(x)-L_{i}(x)\\right]^{2}\\right)^{1 / 2} \\mathrm{~d} \\mu(x) \\leq 2 \\tau\n$$\n\nBy definition of the bracketing entropy, we achieve that\n\n$$\n\\begin{aligned}\nH_{B}\\left(2 \\tau, \\mathcal{R}_{k}(\\Theta), \\|\\cdot\\|_{L^{2}(\\mu)}\\right) & =\\log N_{\\|}\\left(2 \\tau, \\mathcal{R}_{k}(\\Theta), \\|\\cdot\\|_{L^{2}(\\mu)}\\right) \\\\\n& \\leq \\log N=\\log N\\left(\\tau, \\mathcal{R}_{k}(\\Theta), \\|\\cdot\\|_{L^{\\infty}}\\right)\n\\end{aligned}\n$$\n\nTherefore, it is necessary to provide an upper bound for the covering number $N$. Indeed, let us denote $\\Delta:=\\left\\{\\left(\\beta_{0}, \\beta_{1}\\right) \\in \\mathbb{R} \\times \\mathbb{R}^{d_{1}}:\\left(\\beta_{0}, \\beta_{1}, \\eta\\right) \\in \\Theta\\right\\}$ and $\\Omega:=\\left\\{\\eta \\in \\mathbb{R}^{d_{2}}:\\left(\\beta_{0}, \\beta_{1}, \\eta\\right) \\in \\Theta\\right\\}$. Since $\\Theta$ is a compact set, $\\Delta$ and $\\Omega$ are also compact. Therefore, we can find $\\tau$-covers $\\Delta_{\\tau}$ and $\\Omega_{\\tau}$ for $\\Delta$ and $\\Omega$, respectively. Furthermore, it can be validated that\n\n$$\n\\left|\\Delta_{\\tau}\\right| \\leq \\mathcal{O}_{P}\\left(\\tau^{-\\left(d_{1}+1\\right) k}\\right), \\quad\\left|\\Omega_{\\tau}\\right| \\leq \\mathcal{O}_{P}\\left(\\tau^{-d_{2} k}\\right)\n$$\n\nFor each mixing measure $G=\\sum_{i=1}^{k} \\exp \\left(\\beta_{0 i}\\right) \\delta_{\\left(\\beta_{1 i}, \\eta_{i}\\right)} \\in \\mathcal{G}_{k}(\\Theta)$, we consider two other mixing measures $G^{\\prime}$ and $\\bar{G}$ defined as\n\n$$\nG^{\\prime}:=\\sum_{i=1}^{k} \\exp \\left(\\beta_{0 i}\\right) \\delta_{\\left(\\beta_{1 i}, \\bar{\\eta}_{i}\\right)}, \\quad \\bar{G}:=\\sum_{i=1}^{k} \\exp \\left(\\bar{\\beta}_{0 i}\\right) \\delta_{\\left(\\bar{\\beta}_{1 i}, \\bar{\\eta}_{i}\\right)}\n$$\n\nHere, $\\bar{\\eta}_{i} \\in \\Omega_{\\tau}$ such that $\\bar{\\eta}_{i}$ is the closest to $\\eta_{i}$ in that set, while $\\left(\\bar{\\beta}_{0 i}, \\bar{\\beta}_{1 i}\\right) \\in \\Delta_{\\tau}$ is the closest to $\\left(\\beta_{0 i}, \\beta_{1 i}\\right)$ in that set. Now, we aim to upper bound the term $\\left\\|f_{G}-f_{G^{\\prime}}\\right\\|_{\\infty}$. In particular, we have\n\n$$\n\\begin{aligned}\n\\left\\|f_{G}-f_{G^{\\prime}}\\right\\|_{\\infty} & =\\sup _{x \\in \\mathcal{X}}\\left|\\sum_{i=1}^{k} \\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}\\right)^{\\top} x}{\\left\\|\\beta_{1 i}\\right\\| \\cdot\\|x\\|}+\\beta_{0 i}\\right) \\cdot\\left[h\\left(x, \\eta_{i}\\right)-h\\left(x, \\bar{\\eta}_{i}\\right)\\right]\\right. \\\\\n& \\leq \\sum_{i=1}^{k} \\sup _{x \\in \\mathcal{X}} \\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}\\right)^{\\top} x}{\\left\\|\\beta_{1 i}\\right\\| \\cdot\\|x\\|}+\\beta_{0 i}\\right) \\cdot\\left|h\\left(x, \\eta_{i}\\right)-h\\left(x, \\bar{\\eta}_{i}\\right)\\right| \\\\\n& \\leq \\sum_{i=1}^{k} \\sup _{x \\in \\mathcal{X}}\\left|h\\left(x, \\eta_{i}\\right)-h\\left(x, \\bar{\\eta}_{i}\\right)\\right| \\\\\n& \\lesssim \\sum_{i=1}^{k} \\sup _{x \\in \\mathcal{X}}\\left\\|\\eta_{i}-\\bar{\\eta}_{i}\\right\\| \\cdot\\|x\\| \\lesssim \\tau\n\\end{aligned}\n$$\n\nAbove, the second inequality holds as the softmax weight is bounded by 1 , and the third inequality is due to the fact that the expert $h(x, \\cdot)$ is a Lipschitz function w.r.t $\\eta$ and the input space $\\mathcal{X}$ is bounded, i.e., $\\|x\\| \\leq B$ for any $x \\in \\mathcal{X}$ for some constant $B>0$.\n\nNext, we demonstrate that $\\left\\|f_{G^{\\prime}}-f_{\\bar{G}}\\right\\|_{\\infty} \\lesssim \\tau$ as follows:\n\n$$\n\\begin{aligned}\n& \\left\\|f_{G^{\\prime}}-f_{\\bar{G}}\\right\\|_{\\infty}=\\sup _{x \\in \\mathcal{X}}\\left|\\sum_{i=1}^{k}\\left[\\operatorname{Softmax}\\left(\\frac{\\beta_{1 i}^{\\top} x}{\\left\\|\\beta_{1 i}\\right\\| \\cdot\\|x\\|}+\\beta_{0 i}\\right)-\\operatorname{Softmax}\\left(\\frac{\\bar{\\beta}_{1 i}^{\\top} x}{\\left\\|\\bar{\\beta}_{1 i}\\right\\| \\cdot\\|x\\|}+\\bar{\\beta}_{0 i}\\right)\\right] \\cdot h\\left(x, \\bar{\\eta}_{i}\\right)\\right| \\\\\n& \\leq \\sum_{i=1}^{k} \\sup _{x \\in \\mathcal{X}}\\left|\\operatorname{Softmax}\\left(\\frac{\\beta_{1 i}^{\\top} x}{\\left\\|\\beta_{1 i}\\right\\| \\cdot\\|x\\|}+\\beta_{0 i}\\right)-\\operatorname{Softmax}\\left(\\frac{\\bar{\\beta}_{1 i}^{\\top} x}{\\left\\|\\bar{\\beta}_{1 i}\\right\\| \\cdot\\|x\\|}+\\bar{\\beta}_{0 i}\\right)\\right| \\cdot\\left|h\\left(x, \\bar{\\eta}_{\\ell_{i}}\\right)\\right| \\\\\n& \\lesssim \\sum_{i=1}^{k} \\sup _{x \\in \\mathcal{X}}\\left[\\left\\|\\beta_{1 i}-\\bar{\\beta}_{1 i}\\right\\| \\cdot\\|x\\|+\\left|\\beta_{0 i}-\\bar{\\beta}_{0 i}\\right|\\right] \\\\\n& \\leq \\sum_{i=1}^{k} \\sup _{x \\in \\mathcal{X}}[\\tau \\cdot B+\\tau] \\lesssim \\tau\n\\end{aligned}\n$$\n\nBy the triangle inequality, we have\n\n$$\n\\left\\|f_{G}-f_{\\bar{G}}\\right\\|_{\\infty} \\leq\\left\\|f_{G}-f_{G^{\\prime}}\\right\\|_{\\infty}+\\left\\|f_{G^{\\prime}}-f_{\\bar{G}}\\right\\|_{\\infty} \\lesssim \\tau\n$$\n\nBy definition of the covering number, we deduce that\n\n$$\n\\begin{aligned}\nN\\left(\\tau, \\mathcal{R}_{k}(\\Theta), L^{\\infty}\\right) & \\leq\\left|\\Delta_{\\tau}\\right| \\times\\left|\\Omega_{\\tau}\\right| \\\\\n& \\leq \\mathcal{O}_{P}\\left(n^{-\\left(d_{1}+1\\right) k}\\right) \\times \\mathcal{O}\\left(n^{-d_{2} k}\\right) \\\\\n& \\leq \\mathcal{O}\\left(n^{-\\left(d_{1}+1+d_{2}\\right) k}\\right)\n\\end{aligned}\n$$\n\nPutting the results in equations (16) and (17) together, we achieve that\n\n$$\nH_{B}\\left(2 \\tau, \\mathcal{R}_{k}(\\Theta),\\|\\cdot\\|_{L^{2}(\\mu)}\\right) \\lesssim \\log (1 / \\tau)\n$$\n\nBy setting $\\tau=\\varepsilon / 2$, we achieve that\n\n$$\nH_{B}\\left(\\varepsilon, \\mathcal{R}_{k}(\\Theta),\\|\\cdot\\|_{L^{2}(\\mu)}\\right) \\lesssim \\log (1 / \\varepsilon)\n$$\n\nwhich completes the proof.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 19,
      "text": "# B. 2 Proof of Theorem 5 \n\nLemma 2. If the following holds for any $r \\geq 1$ :\n\n$$\n\\lim _{\\varepsilon \\rightarrow 0} \\inf _{G \\in \\mathcal{E}_{k_{*}}(\\Theta): \\mathcal{L}_{1, r}\\left(G, G_{*}\\right) \\leq \\varepsilon} \\frac{\\left\\|f_{G}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}}{\\mathcal{L}_{1, r}\\left(G, G_{*}\\right)}=0\n$$\n\nthen we obtain that\n\n$$\n\\inf _{\\bar{G}_{n} \\in \\mathcal{E}_{k_{*}}(\\Theta)} \\sup _{G \\in \\mathcal{E}_{k_{*}}(\\Theta)} \\mathbb{E}_{f_{G}}\\left[\\mathcal{L}_{1, r}\\left(\\bar{G}_{n}, G\\right)\\right] \\gtrsim n^{-1 / 2}\n$$\n\nProof of Lemma 2. Indeed, from the Gaussian assumption on the noise variables $\\epsilon_{i}$, we obtain that $Y_{i}\\left|X_{i} \\sim \\mathcal{N}\\left(f_{G_{*}}\\left(X_{i}\\right), \\sigma^{2}\\right)\\right.$ for all $i \\in[n]$. Next, the assumption in equation (18) indicates for sufficiently small $\\varepsilon>0$ and a fixed constant $C_{1}>0$ which we will choose later, we can find a mixing measure $G_{*}^{\\prime} \\in \\mathcal{E}_{k_{*}}(\\Theta)$ such that $\\mathcal{L}_{1, r}\\left(G_{*}^{\\prime}, G_{*}\\right)=2 \\varepsilon$ and $\\left\\|f_{G_{*}^{\\prime}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)} \\leq C_{1} \\varepsilon$. From Le Cam's lemma (Yu, 1997), as the Voronoi loss function $\\mathcal{L}_{1, r}$ satisfies the weak triangle inequality, we obtain that\n\n$$\n\\begin{aligned}\n& \\inf _{\\bar{G}_{n} \\in \\mathcal{E}_{k_{*}}(\\Theta)} \\sup _{G \\in \\mathcal{E}_{k_{*}}(\\Theta)} \\mathbb{E}_{f_{G}}\\left[\\mathcal{L}_{1, r}\\left(\\bar{G}_{n}, G\\right)\\right] \\\\\n& \\quad \\gtrsim \\frac{\\mathcal{L}_{1, r}\\left(G_{*}^{\\prime}, G_{*}\\right)}{8} \\exp \\left(-n \\mathbb{E}_{X \\sim \\mu}\\left[\\operatorname{KL}\\left(\\mathcal{N}\\left(f_{G_{*}^{\\prime}}(X), \\sigma^{2}\\right), \\mathcal{N}\\left(f_{G_{*}}(X), \\sigma^{2}\\right)\\right)\\right]\\right) \\\\\n& \\quad \\gtrsim \\varepsilon \\cdot \\exp \\left(-n\\left\\|f_{G_{*}^{\\prime}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}^{2}\\right) \\\\\n& \\quad \\gtrsim \\varepsilon \\cdot \\exp \\left(-C_{1} n \\varepsilon^{2}\\right)\n\\end{aligned}\n$$\n\nwhere the second inequality is due to the fact that\n\n$$\n\\operatorname{KL}\\left(\\mathcal{N}\\left(f_{G_{*}^{\\prime}}(X), \\sigma^{2}\\right), \\mathcal{N}\\left(f_{G_{*}}(X), \\sigma^{2}\\right)\\right)=\\frac{\\left(f_{G_{*}^{\\prime}}(X)-f_{G_{*}}(X)\\right)^{2}}{2 \\sigma^{2}}\n$$\n\nBy choosing $\\varepsilon=n^{-1 / 2}$, we obtain that $\\varepsilon \\cdot \\exp \\left(-C_{1} n \\varepsilon^{2}\\right)=n^{-1 / 2} \\exp \\left(-C_{1}\\right)$. As a consequence, we achieve the desired minimax lower bound in equation (19).\n\nMain proof. It is sufficient to show that the following limit holds true for any $r \\geq 1$ :\n\n$$\n\\lim _{\\varepsilon \\rightarrow 0} \\inf _{G \\in \\mathcal{E}_{k_{*}}(\\Theta): \\mathcal{L}_{1, r}\\left(G, G_{*}\\right) \\leq \\varepsilon} \\frac{\\left\\|f_{G}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}}{\\mathcal{L}_{1, r}\\left(G, G_{*}\\right)}=0\n$$\n\nTo this end, we need to construct a sequence of mixing measures $G_{n} \\in \\mathcal{E}_{k_{*}}(\\Theta)$ that satisfies $\\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ and\n\n$$\n\\frac{\\left\\|f_{G_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}}{\\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right)} \\rightarrow 0\n$$\n\nas $n \\rightarrow \\infty$. Next, let us take into account the sequence $G_{n}=\\sum_{i=1}^{k_{*}} \\exp \\left(\\beta_{0 i}^{n}\\right) \\delta_{\\left(\\beta_{1 i}^{n}, \\eta_{i}^{n}\\right)}$ in which\n\n- $\\exp \\left(\\beta_{0 i}^{n}\\right)=\\exp \\left(\\beta_{0 i}^{*}\\right)$ for any $1 \\leq i \\leq k_{*}$;\n- $\\beta_{11}^{n}=\\left(1+\\frac{1}{n}\\right) \\beta_{11}^{*}$ and $\\beta_{1 i}^{n}=\\beta_{1 i}^{*}$ for any $2 \\leq i \\leq k_{*}$;\n- $\\eta_{i}^{n}=\\eta_{i}^{*}$ for any $1 \\leq i \\leq k_{*}$.\n\nConsequently, it can be verified that when $n \\rightarrow \\infty$, we have\n\n$$\n\\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right)=\\exp \\left(\\beta_{01}^{*}\\right)\\left[\\left\\|\\beta_{11}^{n}-\\beta_{11}^{*}\\right\\|^{r}\\right]=\\exp \\left(\\beta_{01}^{*}\\right) \\cdot\\left(\\frac{\\sqrt{d}}{n}\\right)^{r} \\rightarrow 0\n$$\n\nNext, we demonstrate that $\\left\\|f_{G_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$. For that purpose, we consider the quantity\n\n$$\nQ_{n}(x):=\\left[\\sum_{j=1}^{k_{*}} \\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}+\\beta_{0 j}^{*}\\right)\\right] \\cdot\\left[f_{G_{n}}(x)-f_{G_{*}}(x)\\right]\n$$\n\nwhich can be decomposed as follows:\n\n$$\n\\begin{aligned}\nQ_{n}(x) & =\\sum_{j=1}^{k_{n}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[\\exp \\left(\\frac{\\left(\\beta_{1 i}^{n}\\right)^{\\top} x}{\\left\\|\\beta_{1 i}^{n}\\right\\| \\cdot\\|x\\|}\\right) h\\left(x, \\eta_{i}^{n}\\right)-\\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}\\right) h\\left(x, \\eta_{j}^{*}\\right)\\right] \\\\\n& -\\sum_{j=1}^{k_{r}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[\\exp \\left(\\frac{\\left(\\beta_{1 i}^{n}\\right)^{\\top} x}{\\left\\|\\beta_{1 i}^{n}\\right\\| \\cdot\\|x\\|}\\right) f_{G_{n}}(x)-\\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}\\right) f_{G_{n}}(x)\\right] \\\\\n& +\\sum_{j=1}^{k_{r}}\\left(\\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 j}^{*}\\right)\\right) \\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}\\right)\\left[h\\left(x, \\eta_{j}^{*}\\right)-f_{G_{n}}(x)\\right] \\\\\n& :=A_{n}(x)-B_{n}(x)+C_{n}(x)\n\\end{aligned}\n$$\n\nSince $\\exp \\left(\\beta_{0 i}^{n}\\right)=\\exp \\left(\\beta_{0 i}^{*}\\right)$ for all $i \\in\\left[k_{*}\\right]$, we deduce that $C_{n}(x)=0$. Additionally, from the choices of $\\beta_{1 i}^{n}$ and $\\eta_{i}^{n}$, we can rewrite $A_{n}(x)$ as\n\n$$\nA_{n}(x)=\\exp \\left(\\beta_{01}^{*}\\right)\\left[\\exp \\left(\\frac{\\left(\\beta_{11}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{11}^{*}\\right\\| \\cdot\\|x\\|}\\right)-\\exp \\left(\\frac{\\left(\\beta_{11}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{11}^{*}\\right\\| \\cdot\\|x\\|}\\right)\\right] h\\left(x, \\eta_{1}^{*}\\right)\n$$\n\nLet us denote $F\\left(x, \\beta_{1}\\right):=\\exp \\left(\\frac{\\beta_{1}^{\\top} x}{\\left\\|\\beta_{1}\\right\\| \\cdot\\|x\\|}\\right)$. By applying the Taylor expansion of order $r$, we have\n\n$$\n\\begin{aligned}\nA_{n}(x) & =\\exp \\left(\\beta_{01}^{*}\\right) h\\left(x, \\eta_{1}^{*}\\right) \\sum_{|\\alpha|=1}^{r} \\frac{1}{\\alpha!} \\cdot\\left(\\beta_{11}^{n}-\\beta_{11}^{*}\\right)^{\\alpha} \\cdot \\frac{\\partial^{|\\alpha|} F}{\\partial \\beta_{1}^{n}}\\left(x, \\beta_{11}^{*}\\right)+R(x) \\\\\n& =\\exp \\left(\\beta_{01}^{*}\\right) h\\left(x, \\eta_{1}^{*}\\right) \\sum_{|\\alpha|=1}^{r} \\frac{1}{\\alpha!}\\left(1+\\frac{1}{n}\\right)^{|\\alpha|}\\left(\\beta_{11}^{*}\\right)^{\\alpha} \\cdot \\frac{\\partial^{|\\alpha|} F}{\\partial \\beta_{1}^{n}}\\left(x, \\beta_{11}^{*}\\right)+R(x)\n\\end{aligned}\n$$\n\nwhere $R(x)$ is a Taylor remainder such that $R(x) / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ as $n \\rightarrow \\infty$. It is implied from Lemma 3 (see Appendix D) that\n\n$$\n\\sum_{\\substack{|\\alpha|=t}}^{t}} \\frac{1}{\\alpha!}\\left(\\beta_{11}^{*}\\right)^{\\alpha} \\cdot \\frac{\\partial^{|\\alpha|} F}{\\partial \\beta_{1}^{n}}\\left(x, \\beta_{11}^{*}\\right)=0\n$$\n\nfor any $1 \\leq t \\leq r$, it follows that $A_{n}(x)=R(x)$. This result indicates that $A_{n}(x) / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow$ 0 as $n \\rightarrow \\infty$. By arguing similarly, we also obtain that $B_{n}(x) / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ as $n \\rightarrow \\infty$. Combine the previous results together, we achieve that\n\n$$\nQ_{n}(x) / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0\n$$\n\nSince the input space $\\mathcal{X}$ and the parameter space $\\Theta$ are both bounded, the term $\\sum_{j=1}^{k_{*}} \\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}+\\beta_{0 j}^{*}\\right)$ is also bounded. This result together with the formulation of $Q_{n}(x)$ in equation (22) suggests that $\\left[f_{G_{n}}(x)-f_{G_{*}}(x)\\right] / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ for almost every $x \\in \\mathcal{X}$. As a consequence, we get that\n\n$$\n\\frac{\\left\\|f_{G_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}}{\\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right)} \\rightarrow 0\n$$\n\nas $n \\rightarrow \\infty$, and hence, achieve the result in equation (21).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "# B. 3 Proof of Theorem 2 \n\nSimilar to the proof of Theorem 5 in Appendix B.2, we only need to demonstrate that the following limit holds true for any $r \\geq 1$ :\n\n$$\n\\lim _{\\varepsilon \\rightarrow 0} \\inf _{G \\in \\mathcal{G}_{k}(\\Theta): \\mathcal{L}_{1, r}\\left(G, G_{*}\\right) \\leq \\varepsilon} \\frac{\\left\\|\\bar{f}_{G}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}}{\\mathcal{L}_{1, r}\\left(G, G_{*}\\right)}=0\n$$\n\nFor that purpose, it suffices to build a sequence of mixing measures $G_{n} \\in \\mathcal{G}_{k}(\\Theta)$ that satisfies $\\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ and\n\n$$\n\\frac{\\left\\|\\bar{f}_{G_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}}{\\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right)} \\rightarrow 0\n$$\n\nas $n \\rightarrow \\infty$. Let us consider the sequence $G_{n}=\\sum_{i=1}^{k_{*}+1} \\exp \\left(\\beta_{0 i}^{n}\\right) \\delta_{\\left(\\beta_{1 i}^{n}, \\eta_{i}^{n}\\right)}$ in which\n\n- $\\exp \\left(\\beta_{01}^{n}\\right)=\\exp \\left(\\beta_{02}^{n}\\right)=\\frac{1}{2} \\exp \\left(\\beta_{01}^{*}\\right)$, and $\\exp \\left(\\beta_{0 i}^{n}\\right)=\\exp \\left(\\beta_{0(i-1)}^{*}\\right)$ for any $3 \\leq i \\leq k_{*}+1$;\n- $\\beta_{11}^{n}=\\left(1-\\frac{1}{n}\\right) \\beta_{11}^{*}, \\beta_{12}^{n}=\\left(1+\\frac{1}{n}\\right) \\beta_{11}^{*}$ and $\\beta_{1 i}^{n}=\\beta_{1(i-1)}^{*}$ for any $3 \\leq i \\leq k_{*}+1$;\n- $\\eta_{1}^{n}=\\eta_{2}^{n}=\\eta_{1}^{*}$, and $\\eta_{i}^{n}=\\eta_{i-1}^{*}$ for any $3 \\leq i \\leq k_{*}+1$.\n\nConsequently, it can be verified that when $n \\rightarrow \\infty$, we have\n\n$$\n\\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right)=\\frac{1}{2} \\exp \\left(\\beta_{01}^{*}\\right)\\left[\\left\\|\\beta_{11}^{n}-\\beta_{11}^{*}\\right\\|^{r}+\\left\\|\\beta_{12}^{n}-\\beta_{11}^{*}\\right\\|^{r}\\right]=\\exp \\left(\\beta_{01}^{*}\\right) \\cdot\\left(\\frac{\\sqrt{d_{1}}}{n}\\right)^{r} \\rightarrow 0\n$$\n\nNext, we demonstrate that $\\left\\|\\bar{f}_{G_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$.\nTo this end, we consider the quantity\n\n$$\nQ_{n}(x):=\\left[\\sum_{j=1}^{k_{*}} \\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}+\\beta_{0 j}^{*}\\right)\\right] \\cdot\\left[f_{G_{n}}(x)-f_{G_{*}}(x)\\right]\n$$\n\nwhich can be decomposed as follows:\n\n$$\n\\begin{aligned}\nQ_{n}(x) & =\\sum_{j=1}^{k_{*}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[\\exp \\left(\\frac{\\left(\\beta_{1 i}^{n}\\right)^{\\top} x}{\\left\\|\\beta_{1 i}^{*}\\right\\| \\cdot\\|x\\|}\\right) h\\left(x, \\eta_{i}^{n}\\right)-\\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}\\right) h\\left(x, \\eta_{j}^{*}\\right)\\right] \\\\\n& -\\sum_{j=1}^{k_{*}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[\\exp \\left(\\frac{\\left(\\beta_{1 i}^{n}\\right)^{\\top} x}{\\left\\|\\beta_{1 i}^{*}\\right\\| \\cdot\\|x\\|}\\right) f_{G_{n}}(x)-\\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}\\right) f_{G_{n}}(x)\\right] \\\\\n& +\\sum_{j=1}^{k_{*}}\\left(\\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 j}^{*}\\right)\\right) \\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}\\right)\\left[h\\left(x, \\eta_{j}^{*}\\right)-f_{G_{n}}(x)\\right] \\\\\n& :=A_{n}(x)-B_{n}(x)+C_{n}(x)\n\\end{aligned}\n$$\n\nFrom the choices of $\\beta_{1 i}^{n}$ and $\\eta_{i}^{n}$, we can rewrite $A_{n}(x)$ as\n\n$$\nA_{n}(x)=\\frac{1}{2} \\exp \\left(\\beta_{01}^{*}\\right) h\\left(x, \\eta_{1}^{*}\\right) \\sum_{i=1}^{2}\\left[\\exp \\left(\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 i}^{*}\\right\\| \\cdot\\|x\\|}\\right)-\\exp \\left(\\frac{\\left(\\beta_{11}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{11}^{*}\\right\\| \\cdot\\|x\\|}\\right)\\right]\n$$\n\nLet us denote $F\\left(x, \\beta_{1}\\right):=\\exp \\left(\\frac{\\beta_{1}^{\\top} x}{\\left\\|\\beta_{1}\\right\\| \\cdot\\|x\\|}\\right)$. By applying the Taylor expansion of order $r$, we have\n\n$$\n\\begin{aligned}\nA_{n}(x) & =\\frac{1}{2} \\exp \\left(\\beta_{01}^{*}\\right) h\\left(x, \\eta_{1}^{*}\\right) \\sum_{i=1}^{2} \\sum_{|\\alpha|=1}^{r} \\frac{1}{\\alpha!} \\cdot\\left(\\beta_{1 i}^{n}-\\beta_{11}^{*}\\right)^{\\alpha} \\cdot \\frac{\\partial^{|\\alpha|} F}{\\partial \\beta_{1}^{n}}\\left(x, \\beta_{1 i}^{*}\\right)+R(x) \\\\\n& =\\frac{1}{2} \\exp \\left(\\beta_{01}^{*}\\right) h\\left(x, \\eta_{1}^{*}\\right) \\sum_{i=1}^{2} \\sum_{|\\alpha|=1}^{r} \\frac{1}{\\alpha!}\\left(1+\\frac{(-1)^{i}}{n}\\right)^{|\\alpha|}\\left(\\beta_{11}^{*}\\right)^{\\alpha} \\cdot \\frac{\\partial^{|\\alpha|} F}{\\partial \\beta_{1}^{n}}\\left(x, \\beta_{11}^{*}\\right)+R(x)\n\\end{aligned}\n$$\n\nwhere $R(x)$ is a Taylor remainder such that $R(x) / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ as $n \\rightarrow \\infty$. It follows from Lemma 3 (see Appendix D) that\n\n$$\n\\begin{aligned}\n\\sum_{|\\alpha|=t} \\frac{1}{\\alpha!} & \\left(1+\\frac{(-1)^{i}}{n}\\right)^{|\\alpha|}\\left(\\beta_{11}^{*}\\right)^{\\alpha} \\cdot \\frac{\\partial^{|\\alpha|} F}{\\partial \\beta_{1}^{n}}\\left(x, \\beta_{11}^{*}\\right) \\\\\n& =\\left(1+\\frac{(-1)^{i}}{n}\\right)^{t} \\sum_{|\\alpha|=t} \\frac{1}{\\alpha!}\\left(\\beta_{11}^{*}\\right)^{\\alpha} \\cdot \\frac{\\partial^{|\\alpha|} F}{\\partial \\beta_{1}^{n}}\\left(x, \\beta_{11}^{*}\\right)=0\n\\end{aligned}\n$$\n\nfor any $1 \\leq t \\leq r$. Thus, we get that $A_{n}(x)=R(x)$, which implies that $A_{n}(x) / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ as $n \\rightarrow \\infty$. By arguing similarly, we also obtain that $B_{n}(x) / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ as $n \\rightarrow \\infty$.\n\nFurthermore, we have\n\n$$\n\\begin{aligned}\nC_{n}(x) & =\\left(\\sum_{i=1}^{2} \\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{01}^{*}\\right)\\right) \\exp \\left(\\frac{\\left(\\beta_{11}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{11}^{*}\\right\\| \\cdot\\|x\\|}\\right)\\left[h\\left(x, \\eta_{1}^{*}\\right)-\\bar{f}_{G_{n}}(x)\\right] \\\\\n& +\\sum_{j=2}^{k_{*}}\\left(\\exp \\left(\\beta_{0(j+1)}^{n}\\right)-\\exp \\left(\\beta_{0 j}^{*}\\right)\\right) \\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}\\right)\\left[h\\left(x, \\eta_{j}^{*}\\right)-\\bar{f}_{G_{n}}(x)\\right] \\\\\n& =0\n\\end{aligned}\n$$\n\nPutting the previous results together, we achieve that\n\n$$\nQ_{n}(x) / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0\n$$\n\nSince the input space $\\mathcal{X}$ and the parameter space $\\Theta$ are both bounded, the term $\\sum_{j=1}^{k_{*}} \\exp \\left(\\frac{\\left(\\beta_{j}^{*}\\right)^{\\top} x}{\\left\\|\\beta_{1 j}^{*}\\right\\| \\cdot\\|x\\|}+\\beta_{0 j}^{*}\\right)$ is also bounded. This result together with the formulation of $Q_{n}(x)$ in equation (24) suggests that $\\left[f_{G_{n}}(x)-f_{G_{*}}(x)\\right] / \\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ for almost every $x \\in \\mathcal{X}$. As a consequence, we get that\n\n$$\n\\frac{\\left\\|f_{G_{n}}-f_{G_{*}}\\right\\|_{L^{2}(\\mu)}}{\\mathcal{L}_{1, r}\\left(G_{n}, G_{*}\\right)} \\rightarrow 0\n$$\n\nas $n \\rightarrow \\infty$, and hence, achieve the result in equation (23).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "# C Proof of Results for Perturbed Cosine Router MoE \n\nIn this appendix, we provide proofs for the theoretical results regarding the perturbed cosine router, namely Theorem 3, Theorem 6, and Theorem 4, in that order.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 22,
      "text": "## C. 1 Proof of Theorem 3\n\nSince the proof of Theorem 3 can be done in a similar fashion to that of Theorem 1, it is omitted.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 23,
      "text": "## C. 2 Proof of Theorem 6\n\nIn this proof, we aim to establish the following inequality:\n\n$$\n\\inf _{G \\in \\mathcal{E}_{k *}(\\Theta)}\\left\\|g_{G}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{3}\\left(G, G_{*}\\right)>0\n$$\n\nFor that purpose, we divide the proof of the above inequality into local and global parts in the sequel.\nLocal part: In this part, we demonstrate that\n\n$$\n\\lim _{\\varepsilon \\rightarrow 0} \\inf _{G \\in \\mathcal{G}_{k}(\\Theta) \\mid \\mathcal{L}_{3}\\left(G, G_{*}\\right) \\leq \\varepsilon}\\left\\|g_{G}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{3}\\left(G, G_{*}\\right)>0\n$$\n\nAssume by contrary that the above inequality does not hold true, then there exists a sequence of mixing measures $G_{n}=\\sum_{i=1}^{k_{*}} \\exp \\left(\\beta_{0 i}^{n}\\right) \\delta_{\\left(\\beta_{1 i}^{n}, \\eta_{i}^{n}\\right)}$ in $\\mathcal{G}_{k}(\\Theta)$ such that $\\mathcal{L}_{3 n}:=\\mathcal{L}_{3}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ and\n\n$$\n\\left\\|g_{G_{n}}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{3 n} \\rightarrow 0\n$$\n\nas $n \\rightarrow \\infty$. Let us denote by $\\mathcal{A}_{j}^{n}:=\\mathcal{A}_{j}\\left(G_{n}\\right)$ a Voronoi cell of $G_{n}$ generated by the $j$-th components of $G_{*}$. Since our arguments are asymptotic, we may assume that those Voronoi cells do not depend on the sample size, i.e. $\\mathcal{A}_{j}=\\mathcal{A}_{j}^{n}$. Moreover, recall that under the exact-specified setting, each Voronoi cell has only one element. Therefore, we may assume WLOG that $\\mathcal{A}_{j}=\\{j\\}$, and\n\n$$\n\\mathcal{L}_{3 n}:=\\sum_{i=1}^{k_{*}}\\left|\\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 i}^{*}\\right)\\right|+\\sum_{i=1}^{k_{*}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[\\left\\|\\Delta \\beta_{1 i}^{n}\\right\\|+\\left\\|\\Delta \\eta_{i}^{n}\\right\\|\\right]\n$$\n\nwhere we denote $\\Delta \\beta_{1 i}^{n}:=\\beta_{1 i}^{n}-\\beta_{1 i}^{*}$ and $\\Delta \\eta_{i}^{n}:=\\eta_{i}^{n}-\\eta_{i}^{*}$.\n\nSince $\\mathcal{L}_{3 n} \\rightarrow 0$, we get that $\\left(\\beta_{1 i}^{n}, \\eta_{i}^{n}\\right) \\rightarrow\\left(\\beta_{1 i}^{*}, \\eta_{i}^{*}\\right)$ and $\\exp \\left(\\beta_{0 i}^{n}\\right) \\rightarrow \\exp \\left(\\beta_{0 i}^{*}\\right)$ as $n \\rightarrow \\infty$ for any $i \\in\\left[k_{*}\\right]$. Now, we divide the proof of the local part into three steps as follows:\nStep 1: Taylor expansion. In this step, we decompose the term\n\n$$\nQ_{n}(x):=\\left[\\sum_{j=1}^{k_{*}} \\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 j}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}+\\beta_{0 j}^{*}\\right)\\right] \\cdot\\left[g_{G_{n}}(x)-g_{G_{*}}(x)\\right]\n$$\n\ninto a combination of linearly independent elements using Taylor expansion. In particular, let us denote $F\\left(x, \\beta_{1}\\right):=\\exp \\left(\\frac{\\beta_{1}^{*} x}{\\left(\\left\\|\\beta_{1}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}\\right)$, then we have\n\n$$\n\\begin{aligned}\nQ_{n}(x) & =\\sum_{i=1}^{k_{*}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[F\\left(x, \\beta_{1 i}^{n}\\right) h\\left(x, \\eta_{i}^{n}\\right)-F\\left(x, \\beta_{1 i}^{*}\\right) h\\left(x, \\eta_{i}^{*}\\right)\\right] \\\\\n& -\\sum_{i=1}^{k_{*}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[F\\left(x, \\beta_{1 i}^{n}\\right)-F\\left(x, \\beta_{1 i}^{*}\\right)\\right] g_{G_{n}}(x) \\\\\n& +\\sum_{i=1}^{k_{*}}\\left(\\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 i}^{*}\\right)\\right)\\left[F\\left(x, \\beta_{1 i}^{*}\\right) h\\left(x, \\eta_{i}^{*}\\right)-F\\left(x, \\beta_{1 i}^{*}\\right) g_{G_{n}}(x)\\right] \\\\\n& :=A_{n}(x)-B_{n}(x)+C_{n}(x)\n\\end{aligned}\n$$\n\nBy means of the first-order Taylor expansion, we have\n\n$$\nA_{n}(x)=\\sum_{i=1}^{k_{*}} \\sum_{|\\alpha|=1} \\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\alpha!}\\left(\\Delta \\beta_{1 i}^{n}\\right)^{\\alpha_{1}}\\left(\\Delta \\eta_{i}^{n}\\right)^{\\alpha_{2}} \\cdot \\frac{\\partial^{\\left|\\alpha_{1}\\right|} F}{\\partial \\beta_{1}^{\\alpha_{1}}}\\left(x, \\beta_{1 i}^{*}\\right) \\frac{\\partial^{\\left|\\alpha_{2}\\right|} h}{\\partial \\eta^{\\alpha_{2}}}\\left(x, \\eta_{i}^{*}\\right)+R_{1}(x)\n$$\n\nwhere $R_{1}(x)$ is a Taylor remainder such that $R_{1}(x) / \\mathcal{L}_{3 n} \\rightarrow 0$ as $n \\rightarrow \\infty$. Similarly, we also get that\n\n$$\nB_{n}(x)=\\sum_{i=1}^{k_{*}} \\sum_{|\\gamma|=1} \\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\gamma!}\\left(\\Delta \\beta_{1 i}^{n}\\right)^{\\gamma} \\cdot \\frac{\\partial^{|\\gamma|} F}{\\partial \\beta_{1}^{\\gamma}}\\left(x, \\beta_{1 i}^{*}\\right) g_{G_{n}}(x)+R_{2}(x)\n$$\n\nwhere $R_{2}(x)$ is a Taylor remainder such that $R_{2}(x) / \\mathcal{L}_{3 n} \\rightarrow 0$ as $n \\rightarrow \\infty$. As a result, we deduce that\n\n$$\n\\begin{aligned}\nQ_{n}(x) & =\\sum_{i=1}^{k_{*}} \\sum_{|\\alpha|=0}^{1} T_{i, \\alpha_{1}, \\alpha_{2}}^{n} \\cdot \\frac{\\partial^{\\left|\\alpha_{1}\\right|} F}{\\partial \\beta_{1}^{\\alpha_{1}}}\\left(x, \\beta_{1 i}^{*}\\right) \\frac{\\partial^{\\left|\\alpha_{2}\\right|} h}{\\partial \\eta^{\\alpha_{2}}}\\left(x, \\eta_{i}^{*}\\right)+R_{1}(x) \\\\\n& -\\sum_{i=1}^{k_{*}} \\sum_{|\\gamma|=0}^{1} S_{i, \\gamma}^{n} \\cdot \\frac{\\partial^{|\\gamma|} F}{\\partial \\beta_{1}^{\\gamma}}\\left(x, \\beta_{1 i}^{*}\\right) g_{G_{n}}(x)-R_{2}(x)\n\\end{aligned}\n$$\n\nwhere we define\n\n$$\n\\begin{aligned}\nT_{i, \\alpha_{1}, \\alpha_{2}}^{n} & :=\\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\alpha!}\\left(\\Delta \\beta_{1 i}^{n}\\right)^{\\alpha_{1}}\\left(\\Delta \\eta_{i}^{n}\\right)^{\\alpha_{2}} \\\\\nS_{i, \\gamma}^{n} & :=\\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\gamma!}\\left(\\Delta \\beta_{1 i j}^{n}\\right)^{\\gamma}\n\\end{aligned}\n$$\n\nfor any $\\left(\\alpha_{1}, \\alpha_{2}\\right) \\neq\\left(\\mathbf{0}_{d}, 0\\right)$ and $\\gamma \\neq \\mathbf{0}_{d}$. Otherwise, $T_{i, \\mathbf{0}_{d}, 0}^{n}=S_{i, \\mathbf{0}_{d}}^{n}:=\\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 i}^{*}\\right)$.\nStep 2: Non-vanishing coefficients. In this step, we show that not all the ratios $T_{i, \\alpha_{1}, \\alpha_{2}}^{n} / \\mathcal{L}_{3 n}$, and $S_{i, \\gamma}^{n} / \\mathcal{L}_{3 n}$ converge to zero. Indeed, assume by contrary that all of them converge to zero, i.e.\n\n$$\n\\frac{T_{i, \\alpha_{1}, \\alpha_{2}}^{n}}{\\mathcal{L}_{3 n}} \\rightarrow 0, \\quad \\frac{S_{i, \\gamma}^{n}}{\\mathcal{L}_{3 n}} \\rightarrow 0\n$$\n\nas $n \\rightarrow \\infty$. Then, it follows that\n\n- $\\frac{1}{\\mathcal{L}_{3 n}} \\cdot \\sum_{i=1}^{k_{*}}\\left|\\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 i}^{*}\\right)\\right|=\\frac{1}{\\mathcal{L}_{3 n}} \\cdot \\sum_{i=1}^{k_{*}}\\left|S_{i, \\mathbf{0}_{d}}^{n}\\right| \\rightarrow 0$\n\n- $\\frac{1}{\\mathcal{L}_{3 n}} \\cdot \\sum_{i=1}^{k_{s}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\beta_{1 i}^{n}\\right\\|_{1}=\\frac{1}{\\mathcal{L}_{3 n}} \\sum_{i=1}^{k_{s}} \\sum_{u=1}^{d_{1}}\\left|T_{i, e_{d_{1}, u, \\mathbf{0}_{g}}}^{n}\\right| \\rightarrow 0$;\n- $\\frac{1}{\\mathcal{L}_{3 n}} \\cdot \\sum_{i=1}^{k_{s}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\eta_{i}^{n}\\right\\|_{1}=\\frac{1}{\\mathcal{L}_{3 n}} \\sum_{i=1}^{k_{s}} \\sum_{v=1}^{d_{2}}\\left|T_{i, \\mathbf{0}_{d}, e_{d_{2}, v}}^{n}\\right| \\rightarrow 0$.\n\nDue to the topological equivalence of norm-1 and norm-2, we deduce that\n\n$$\n\\frac{1}{\\mathcal{L}_{3 n}} \\cdot \\sum_{i=1}^{k_{s}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\beta_{1 i}^{n}\\right\\| \\rightarrow 0, \\quad \\frac{1}{\\mathcal{L}_{3 n}} \\cdot \\sum_{i=1}^{k_{s}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\eta_{i}^{n}\\right\\| \\rightarrow 0\n$$\n\nAs a result, we obtain that\n\n$$\n1=\\frac{\\mathcal{L}_{3 n}}{\\mathcal{L}_{3 n}}=\\frac{1}{\\mathcal{L}_{3 n}}\\left\\{\\sum_{i=1}^{k_{s}}\\left[\\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 i}^{*}\\right)\\right]+\\sum_{i=1}^{k_{s}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[\\left\\|\\Delta \\beta_{1 i}^{n}\\right\\|+\\left\\|\\Delta \\eta_{i}^{n}\\right\\|\\right]\\right\\} \\rightarrow 0\n$$\n\nwhich is a contradiction. Thus, at least one among the ratios $T_{i, \\alpha_{1}, \\alpha_{2}}^{n} / \\mathcal{L}_{3 n}$, and $S_{i, \\gamma}^{n} / \\mathcal{L}_{3 n}$ must not go to zero as $n \\rightarrow \\infty$.\nStep 3: Application of Fatou's lemma. In this step, we demonstrate a result opposed to that in Step 2, i.e. the ratios $T_{i, \\alpha_{1}, \\alpha_{2}}^{n} / \\mathcal{L}_{3 n}$, and $S_{i, \\gamma}^{n} / \\mathcal{L}_{3 n}$ all converge to zero.\n\nIn particular, let us denote by $m_{n}$ the maximum of the absolute values of $T_{i, \\alpha_{1}, \\alpha_{2}}^{n} / \\mathcal{L}_{3 n}$, and $S_{i, \\gamma}^{n} / \\mathcal{L}_{3 n}$. Since at least one among those ratios must not approach zero as $n \\rightarrow \\infty$, we get that $1 / m_{n} \\nrightarrow \\infty$ as $n \\rightarrow \\infty$.\nRecall from the hypothesis in equation (27) that $\\left\\|g_{G_{n}}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{3 n} \\rightarrow 0$ as $n \\rightarrow \\infty$, which indicates that $\\left\\|g_{G_{n}}-g_{G_{*}}\\right\\|_{L^{1}(\\mu)} / \\mathcal{L}_{3 n} \\rightarrow 0$ due to the equivalence between $L^{1}(\\mu)$-norm and $L^{2}(\\mu)$ norm. By means of the Fatou's lemma, we have\n\n$$\n0=\\lim _{n \\rightarrow \\infty} \\frac{\\left\\|g_{G_{n}}-g_{G_{*}}\\right\\|_{L^{1}(\\mu)}}{m_{n} \\mathcal{L}_{3 n}} \\geq \\int \\liminf _{n \\rightarrow \\infty} \\frac{\\left|g_{G_{n}}(x)-g_{G_{*}}(x)\\right|}{m_{n} \\mathcal{L}_{3 n}} \\mathrm{~d} \\mu(x) \\geq 0\n$$\n\nThis result implies that $\\left[g_{G_{n}}(x)-g_{G_{*}}(x)\\right] /\\left[m_{n} \\mathcal{L}_{3 n}\\right] \\rightarrow 0$ for almost every $x$.\nLet us denote\n\n$$\n\\begin{aligned}\nT_{i, \\alpha_{1}, \\alpha_{2}}^{n} / m_{n} \\mathcal{L}_{3 n} & \\rightarrow t_{i, \\alpha_{1}, \\alpha_{2}} \\\\\nS_{i, \\gamma}^{n} / m_{n} \\mathcal{L}_{3 n} & \\rightarrow s_{i, \\gamma}\n\\end{aligned}\n$$\n\nwith a note that at least one among the limits $t_{i, \\alpha_{1}, \\alpha_{2}}, s_{i, \\gamma}$ is non-zero. Then, from the decomposition in equation (30), we deduce that\n\n$$\n\\sum_{i=1}^{k_{s}} \\sum_{|\\alpha|=0}^{1} t_{i, \\alpha_{1}, \\alpha_{2}} \\cdot \\frac{\\partial^{\\left|\\alpha_{1}\\right|} F}{\\partial \\beta_{1}^{\\alpha_{1}}}\\left(x, \\beta_{1 i}^{*}\\right) \\frac{\\partial^{\\left|\\alpha_{2}\\right|} h}{\\partial \\eta^{\\alpha_{2}}}\\left(x, \\eta_{i}^{*}\\right)-\\sum_{i=1}^{k_{s}} \\sum_{|\\gamma|=0}^{1} s_{i, \\gamma} \\cdot \\frac{\\partial^{|\\gamma|} F}{\\partial \\beta_{1}^{\\gamma}}\\left(x, \\beta_{1 i}^{*}\\right) g_{G_{n}}(x)=0\n$$\n\nfor almost every $x$. Note that the expert function $h(\\cdot, \\eta)$ satisfies the condition in Definition 2, then the above equation implies that $t_{i, \\alpha_{1}, \\alpha_{2}}=s_{i, \\gamma}=0$, for any $i \\in\\left[k_{*}\\right], \\alpha_{1} \\in \\mathbb{N}^{d_{1}}, \\alpha_{2} \\in \\mathbb{N}^{d_{2}}$ and $\\gamma \\in \\mathbb{N}^{d_{1}}$ such that $0 \\leq\\left|\\alpha_{1}\\right|+\\left|\\alpha_{2}\\right|,|\\gamma| \\leq 2$. This contradicts the fact that at least one among the limits $t_{i, \\alpha_{1}, \\alpha_{2}}, s_{i, \\gamma}$ is different from zero.\nHence, we obtain the local inequality in equation (26). Thus, we can find an $\\varepsilon^{\\prime}>0$ such that\n\n$$\n\\inf _{G \\in \\mathcal{E}_{k_{*}}(\\Theta) \\cdot \\mathcal{L}_{3}\\left(G, G_{*}\\right) \\leq \\varepsilon^{\\prime}}\\left\\|g_{G}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{3}\\left(G, G_{*}\\right)>0\n$$\n\nGlobal part: Given the above result, it suffices to demonstrate that\n\n$$\n\\inf _{G \\in \\mathcal{E}_{k_{*}}(\\Theta) \\cdot \\mathcal{L}_{3}\\left(G, G_{*}\\right)>\\varepsilon^{\\prime}}\\left\\|g_{G}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{3}\\left(G, G_{*}\\right)>0\n$$\n\nAssume by contrary that the inequality (31) does not hold true, then we can find a sequence of mixing measures $G_{n}^{\\prime} \\in \\mathcal{E}_{k_{*}}(\\Theta)$ such that $\\mathcal{L}_{3}\\left(G_{n}^{\\prime}, G_{*}\\right)>\\varepsilon^{\\prime}$ and\n\n$$\n\\lim _{n \\rightarrow \\infty} \\frac{\\left\\|g_{G_{n}^{\\prime}}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)}}{\\mathcal{L}_{3}\\left(G_{n}^{\\prime}, G_{*}\\right)}=0\n$$\n\nwhich indicates that $\\left\\|g_{G_{n}^{\\prime}}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} \\rightarrow 0$ as $n \\rightarrow \\infty$. Recall that $\\Theta$ is a compact set, therefore, we can replace the sequence $G_{n}^{\\prime}$ by one of its subsequences that converge to a mixing measure $G^{\\prime} \\in \\mathcal{E}_{k_{*}}(\\Omega)$. Since $\\mathcal{L}_{3}\\left(G_{n}^{\\prime}, G_{*}\\right)>\\varepsilon^{\\prime}$, we deduce that $\\mathcal{L}_{3}\\left(G^{\\prime}, G_{*}\\right)>\\varepsilon^{\\prime}$.\n\nNext, by invoking the Fatou's lemma, we have that\n\n$$\n0=\\lim _{n \\rightarrow \\infty}\\left\\|g_{G_{n}^{\\prime}}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)}^{2} \\geq \\int \\liminf _{n \\rightarrow \\infty}\\left|g_{G_{n}^{\\prime}}(x)-g_{G_{*}}(x)\\right|^{2} \\mathrm{~d} \\mu(x)\n$$\n\nThus, we get that $g_{G^{\\prime}}(x)=g_{G_{*}}(x)$ for almost every $x$. From Proposition 1, we deduce that $G^{\\prime} \\equiv G_{*}$. Consequently, it follows that $\\mathcal{L}_{3}\\left(G^{\\prime}, G_{*}\\right)=0$, contradicting the fact that $\\mathcal{L}_{3}\\left(G^{\\prime}, G_{*}\\right)>\\varepsilon^{\\prime}>0$.\n\nHence, the proof is completed.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 24,
      "text": "# C. 3 Proof of Theorem 4 \n\nIn this proof, it is sufficient to demonstrate the following inequality:\n\n$$\n\\inf _{G \\in \\mathcal{G}_{k}(\\Theta)}\\left\\|g_{G}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{2}\\left(G, G_{*}\\right)>0\n$$\n\nThis can be done by deriving its local part and the global part as in Appendix C.2. Since the global part can be argued similarly, our main goal is to prove the local part:\n\n$$\n\\lim _{\\varepsilon \\rightarrow 0} \\inf _{G \\in \\mathcal{G}_{k}(\\Theta): \\mathcal{L}_{2}\\left(G, G_{*}\\right) \\leq \\varepsilon}\\left\\|g_{G}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{2}\\left(G, G_{*}\\right)>0\n$$\n\nAssume by contrary that the above inequality does not hold true, then there exists a sequence of mixing measures $G_{n}=\\sum_{i=1}^{k} \\exp \\left(\\beta_{0 i}^{n}\\right) \\delta_{\\left(\\beta_{1 i}^{n}, \\eta_{i}^{n}\\right)}$ in $\\mathcal{G}_{k}(\\Theta)$ such that $\\mathcal{L}_{2 n}:=\\mathcal{L}_{2}\\left(G_{n}, G_{*}\\right) \\rightarrow 0$ and\n\n$$\n\\left\\|g_{G_{n}}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{2 n} \\rightarrow 0\n$$\n\nas $n \\rightarrow \\infty$. Let us denote by $\\mathcal{A}_{j}^{n}:=\\mathcal{A}_{j}\\left(G_{n}\\right)$ a Voronoi cell of $G_{n}$ generated by the $j$-th components of $G_{*}$. Since our arguments are asymptotic, we may assume that those Voronoi cells do not depend on the sample size, i.e., $\\mathcal{A}_{j}=\\mathcal{A}_{j}^{n}$. Therefore, we may assume WLOG that\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{2 n}:=\\sum_{j=1}^{k_{*}}\\left|\\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 j}^{*}\\right)\\right| & +\\sum_{j \\in\\left[k_{*}\\right]: \\mid \\mathcal{A}_{j} \\mid>1} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[\\left\\|\\Delta \\beta_{1 i j}^{n}\\right\\|^{2}+\\left\\|\\Delta \\eta_{i j}^{n}\\right\\|^{2}\\right] \\\\\n& +\\sum_{j \\in\\left[k_{*}\\right]: \\mid \\mathcal{A}_{j} \\mid=1} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[\\left\\|\\Delta \\beta_{1 i j}^{n}\\right\\|+\\left\\|\\Delta \\eta_{i j}^{n}\\right\\|\\right]\n\\end{aligned}\n$$\n\nwhere we denote $\\Delta \\beta_{1 i j}^{n}:=\\beta_{1 i}^{n}-\\beta_{1 j}^{*}$ and $\\Delta \\eta_{i j}^{n}:=\\eta_{i}^{n}-\\eta_{j}^{*}$.\nNow, we divide the proof of the local part into three steps as follows:\nStep 1: Taylor expansion. In this step, we decompose the term\n\n$$\nQ_{n}(x):=\\left[\\sum_{j=1}^{k_{*}} \\exp \\left(\\frac{\\left(\\beta_{1 j}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 j}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}+\\beta_{0 j}^{*}\\right)\\right] \\cdot\\left[g_{G_{n}}(x)-g_{G_{*}}(x)\\right]\n$$\n\ninto a combination of linearly independent elements using Taylor expansion. In particular, let us denote $F\\left(x, \\beta_{1}\\right):=\\exp \\left(\\frac{\\beta_{1}^{*} x}{\\left(\\left\\|\\beta_{1}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}\\right)$, then we have\n\n$$\n\\begin{aligned}\nQ_{n}(x) & =\\sum_{j=1}^{k_{*}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[F\\left(x, \\beta_{1 i}^{n}\\right) h\\left(x, \\eta_{i}^{n}\\right)-F\\left(x, \\beta_{1 j}^{*}\\right) h\\left(x, \\eta_{j}^{*}\\right)\\right] \\\\\n& -\\sum_{j=1}^{k_{*}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[F\\left(x, \\beta_{1 i}^{n}\\right)-F\\left(x, \\beta_{1 j}^{*}\\right)\\right] g_{G_{n}}(x) \\\\\n& +\\sum_{j=1}^{k_{*}}\\left(\\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 j}^{*}\\right)\\right)\\left[F\\left(x, \\beta_{1 j}^{*}\\right) h\\left(x, \\eta_{j}^{*}\\right)-F\\left(x, \\beta_{1 j}^{*}\\right) g_{G_{n}}(x)\\right] \\\\\n& :=A_{n}(x)-B_{n}(x)+C_{n}(x)\n\\end{aligned}\n$$\n\nNext, we continue to separate the term $A_{n}(x)$ into two parts as\n\n$$\n\\begin{aligned}\nA_{n}(x):= & \\sum_{j \\in\\left[k_{*}\\right]: \\mid \\mathcal{A}_{j}=1} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[F\\left(x, \\beta_{1 i}^{n}\\right) h\\left(x, \\eta_{i}^{n}\\right)-F\\left(x, \\beta_{1 j}^{*}\\right) h\\left(x, \\eta_{j}^{*}\\right)\\right] \\\\\n& +\\sum_{j \\in\\left[k_{*}\\right]: \\mid \\mathcal{A}_{j} \\mid>1} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left[F\\left(x, \\beta_{1 i}^{n}\\right) h\\left(x, \\eta_{i}^{n}\\right)-F\\left(x, \\beta_{1 j}^{*}\\right) h\\left(x, \\eta_{j}^{*}\\right)\\right] \\\\\n:= & A_{n, 1}(x)+A_{n, 2}(x)\n\\end{aligned}\n$$\n\nSimilar to equation (29), by applying the first-order and the second-order Taylor expansions to $A_{n, 1}(x)$ and $A_{n, 2}(x)$, respectively, we have\n$A_{n, 1}(x)=\\sum_{j \\in\\left[k_{*}\\right]: \\mid \\mathcal{A}_{j}=1} \\sum_{|\\alpha|=1} \\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\alpha!}\\left(\\Delta \\beta_{1 i j}^{n}\\right)^{\\alpha_{1}}\\left(\\Delta \\eta_{i j}^{n}\\right)^{\\alpha_{2}} \\cdot \\frac{\\partial^{\\left|\\alpha_{1}\\right|} F}{\\partial \\beta_{1}^{\\alpha_{1}}}\\left(x, \\beta_{1 j}^{*}\\right) \\frac{\\partial^{\\left|\\alpha_{2}\\right|} h}{\\partial \\eta^{\\alpha_{2}}}\\left(x, \\eta_{j}^{*}\\right)+R_{1}(x)$,\n$A_{n, 2}(x)=\\sum_{j \\in\\left[k_{*}\\right]: \\mid \\mathcal{A}_{j} \\mid>1} \\sum_{|\\alpha|=1}^{2} \\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\alpha!}\\left(\\Delta \\beta_{1 i j}^{n}\\right)^{\\alpha_{1}}\\left(\\Delta \\eta_{i j}^{n}\\right)^{\\alpha_{2}} \\cdot \\frac{\\partial^{\\left|\\alpha_{1}\\right|} F}{\\partial \\beta_{1}^{\\alpha_{1}}}\\left(x, \\beta_{1 j}^{*}\\right) \\frac{\\partial^{\\left|\\alpha_{2}\\right|} h}{\\partial \\eta^{\\alpha_{2}}}\\left(x, \\eta_{j}^{*}\\right)+R_{2}(x)$,\nwhere $R_{i}(x)$ is a Taylor remainder such that $R_{i}(x) / \\mathcal{L}_{2 n} \\rightarrow 0$ as $n \\rightarrow \\infty$, for $i \\in\\{1,2\\}$. Analogously, we also get that $B_{n}(x)=B_{n, 1}(x)+B_{n, 2}(x)$ where\n\n$$\n\\begin{aligned}\nB_{n, 1}(x) & =\\sum_{j \\in\\left[k_{*}\\right]: \\mid \\mathcal{A}_{j}=1} \\sum_{|\\gamma|=1} \\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\gamma!}\\left(\\Delta \\beta_{1 i}^{n}\\right)^{\\gamma} \\cdot \\frac{\\partial^{|\\gamma|} F}{\\partial \\beta_{1}^{\\gamma}}\\left(x, \\beta_{1 j}^{*}\\right) g_{G_{n}}(x)+R_{3}(x) \\\\\nB_{n, 2}(x) & =\\sum_{j \\in\\left[k_{*}\\right]: \\mid \\mathcal{A}_{j} \\mid>1} \\sum_{|\\gamma|=1}^{2} \\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\gamma!}\\left(\\Delta \\beta_{1 i}^{n}\\right)^{\\gamma} \\cdot \\frac{\\partial^{|\\gamma|} F}{\\partial \\beta_{1}^{\\gamma}}\\left(x, \\beta_{1 j}^{*}\\right) g_{G_{n}}(x)+R_{4}(x)\n\\end{aligned}\n$$\n\nin which $R_{i}(x)$ is a Taylor remainder such that $R_{i}(x) / \\mathcal{L}_{2 n} \\rightarrow 0$ as $n \\rightarrow \\infty$, for $i \\in\\{3,4\\}$.\nAs a result, we deduce that\n\n$$\n\\begin{aligned}\nQ_{n}(x) & =\\sum_{j=1}^{k_{*}} \\sum_{|\\alpha|=0}^{2} T_{j, \\alpha_{1}, \\alpha_{2}}^{n} \\cdot \\frac{\\partial^{\\left|\\alpha_{1}\\right|} F}{\\partial \\beta_{1}^{\\alpha_{1}}}\\left(x, \\beta_{1 j}^{*}\\right) \\frac{\\partial^{\\left|\\alpha_{2}\\right|} h}{\\partial \\eta^{\\alpha_{2}}}\\left(x, \\eta_{j}^{*}\\right)+R_{1}(x)+R_{2}(x) \\\\\n& -\\sum_{j=1}^{k_{*}} \\sum_{|\\gamma|=0}^{2} S_{j, \\gamma}^{n} \\cdot \\frac{\\partial^{|\\gamma|} F}{\\partial \\beta_{1}^{\\gamma}}\\left(x, \\beta_{1 j}^{*}\\right) g_{G_{n}}(x)-R_{3}(x)-R_{4}(x)\n\\end{aligned}\n$$\n\nwhere we define\n\n$$\n\\begin{aligned}\nT_{j, \\alpha_{1}, \\alpha_{2}}^{n} & :=\\sum_{i \\in \\mathcal{A}_{j}} \\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\alpha!}\\left(\\Delta \\beta_{1 i j}^{n}\\right)^{\\alpha_{1}}\\left(\\Delta \\eta_{i j}^{n}\\right)^{\\alpha_{2}} \\\\\nS_{j, \\gamma}^{n} & :=\\sum_{i \\in \\mathcal{A}_{j}} \\frac{\\exp \\left(\\beta_{0 i}^{n}\\right)}{\\gamma!}\\left(\\Delta \\beta_{1 i j}^{n}\\right)^{\\gamma}\n\\end{aligned}\n$$\n\nfor any $\\left(\\alpha_{1}, \\alpha_{2}\\right) \\neq\\left(\\mathbf{0}_{d}, 0\\right)$ and $\\gamma \\neq \\mathbf{0}_{d}$. Otherwise, $T_{j, \\mathbf{0}_{d}, 0}^{n}=S_{j, \\mathbf{0}_{d}}^{n}:=\\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 j}^{*}\\right)$.\nStep 2: Non-vanishing coefficients. In this step, we show that not all the ratios $T_{j, \\alpha_{1}, \\alpha_{2}}^{n} / \\mathcal{L}_{2 n}$, and $S_{j, \\gamma}^{n} / \\mathcal{L}_{2 n}$ converge to zero. Indeed, assume by contrary that all of them converge to zero, i.e.\n\n$$\n\\frac{T_{j, \\alpha_{1}, \\alpha_{2}}^{n}}{\\mathcal{L}_{2 n}} \\rightarrow 0, \\quad \\frac{S_{j, \\gamma}^{n}}{\\mathcal{L}_{2 n}} \\rightarrow 0\n$$\n\nas $n \\rightarrow \\infty$. Then, it follows that\n\n- $\\frac{1}{\\mathcal{L}_{2 n}} \\cdot \\sum_{j=1}^{k_{*}}\\left|\\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)-\\exp \\left(\\beta_{0 j}^{*}\\right)\\right|=\\frac{1}{\\mathcal{L}_{2 n}} \\cdot \\sum_{j=1}^{k_{*}}\\left|S_{j, \\mathbf{0}_{d}}^{n}\\right| \\rightarrow 0$\n- $\\frac{1}{\\mathcal{L}_{2 n}} \\cdot \\sum_{j=1}^{k_{*}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\beta_{1 i j}^{n}\\right\\|_{1}=\\frac{1}{\\mathcal{L}_{2 n}} \\sum_{j=1}^{k_{*}} \\sum_{u=1}^{d_{1}}\\left|T_{j, e_{d_{1}, u}, \\mathbf{0}_{d}}^{n}\\right| \\rightarrow 0$;\n\n- $\\frac{1}{\\mathcal{L}_{2 n}} \\cdot \\sum_{j=1}^{k_{s}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\eta_{i j}^{n}\\right\\|_{1}=\\frac{1}{\\mathcal{L}_{2 n}} \\sum_{j=1}^{k_{s}} \\sum_{v=1}^{d_{2}}\\left|T_{j, \\mathbf{u}_{d}, e_{d_{2}, v}}^{n}\\right| \\rightarrow 0$\n- $\\frac{1}{\\mathcal{L}_{2 n}} \\cdot \\sum_{j=1}^{k_{s}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\beta_{1 i j}^{n}\\right\\|^{2}=\\frac{1}{\\mathcal{L}_{2 n}} \\sum_{j=1}^{k_{s}} \\sum_{u=1}^{d_{1}}\\left|T_{j, 2 e_{d_{1}, u}, \\mathbf{0}_{d}}^{n}\\right| \\rightarrow 0$\n- $\\frac{1}{\\mathcal{L}_{2 n}} \\cdot \\sum_{j=1}^{k_{s}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\eta_{i j}^{n}\\right\\|^{2}=\\frac{1}{\\mathcal{L}_{2 n}} \\sum_{j=1}^{k_{s}} \\sum_{v=1}^{d_{2}}\\left|T_{j, \\mathbf{u}_{d}, 2 e_{d_{2}, v}}^{n}\\right| \\rightarrow 0$.\n\nDue to the topological equivalence of the norm-1 and norm-2, we deduce that\n\n$$\n\\frac{1}{\\mathcal{L}_{2 n}} \\cdot \\sum_{j=1}^{k_{s}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\beta_{1 i j}^{n}\\right\\| \\rightarrow 0, \\quad \\frac{1}{\\mathcal{L}_{2 n}} \\cdot \\sum_{j=1}^{k_{s}} \\sum_{i \\in \\mathcal{A}_{j}} \\exp \\left(\\beta_{0 i}^{n}\\right)\\left\\|\\Delta \\eta_{i j}^{n}\\right\\| \\rightarrow 0\n$$\n\nThus, by taking the summation of the above limits, we obtain that $1=\\mathcal{L}_{2 n} / \\mathcal{L}_{2 n} \\rightarrow 0$ as $n \\rightarrow \\infty$, which is a contradiction. Consequently, at least one among the ratios $T_{j, \\alpha_{1}, \\alpha_{2}}^{n} / \\mathcal{L}_{2 n}$, and $S_{j, \\gamma}^{n} / \\mathcal{L}_{2 n}$ must not go to zero as $n \\rightarrow \\infty$.\nStep 3: Application of Fatou's lemma. In this step, we demonstrate a result opposed to that in Step 2 , i.e. the ratios $T_{j, \\alpha_{1}, \\alpha_{2}}^{n} / \\mathcal{L}_{2 n}$, and $S_{j, \\gamma}^{n} / \\mathcal{L}_{2 n}$ all converge to zero.\n\nIn particular, let us denote by $m_{n}$ the maximum of the absolute values of $T_{j, \\alpha_{1}, \\alpha_{2}}^{n} / \\mathcal{L}_{2 n}$, and $S_{j, \\gamma}^{n} / \\mathcal{L}_{2 n}$. Since at least one among those ratios must not approach zero as $n \\rightarrow \\infty$, we get that $1 / m_{n} \\nrightarrow \\infty$ as $n \\rightarrow \\infty$.\nRecall from the hypothesis in equation (34) that $\\left\\|g_{G_{n}}-g_{G_{*}}\\right\\|_{L^{2}(\\mu)} / \\mathcal{L}_{2 n} \\rightarrow 0$ as $n \\rightarrow \\infty$, which indicates that $\\left\\|g_{G_{n}}-g_{G_{*}}\\right\\|_{L^{1}(\\mu)} / \\mathcal{L}_{2 n} \\rightarrow 0$ due to the equivalence between $L^{1}(\\mu)$-norm and $L^{2}(\\mu)$ norm. By means of the Fatou's lemma, we have\n\n$$\n0=\\lim _{n \\rightarrow \\infty} \\frac{\\left\\|g_{G_{n}}-g_{G_{*}}\\right\\|_{L^{1}(\\mu)}}{m_{n} \\mathcal{L}_{2 n}} \\geq \\int \\liminf _{n \\rightarrow \\infty} \\frac{\\left|g_{G_{n}}(x)-g_{G_{*}}(x)\\right|}{m_{n} \\mathcal{L}_{2 n}} \\mathrm{~d} \\mu(x) \\geq 0\n$$\n\nThis result implies that $\\left[g_{G_{n}}(x)-g_{G_{*}}(x)\\right] /\\left[m_{n} \\mathcal{L}_{2 n}\\right] \\rightarrow 0$ for almost every $x$.\nLet us denote\n\n$$\n\\begin{aligned}\nT_{j, \\alpha_{1}, \\alpha_{2}}^{n} / m_{n} \\mathcal{L}_{2 n} & \\rightarrow t_{j, \\alpha_{1}, \\alpha_{2}} \\\\\nS_{j, \\gamma}^{n} / m_{n} \\mathcal{L}_{2 n} & \\rightarrow s_{j, \\gamma}\n\\end{aligned}\n$$\n\nwith a note that at least one among the limits $t_{j, \\alpha_{1}, \\alpha_{2}}, s_{j, \\gamma}$ is non-zero. Then, from the decomposition in equation (36), we deduce that\n\n$$\n\\sum_{j=1}^{k_{s}} \\sum_{|\\alpha|=0}^{2} t_{j, \\alpha_{1}, \\alpha_{2}} \\cdot \\frac{\\partial^{\\left|\\alpha_{1}\\right|} F}{\\partial \\beta_{1}^{\\alpha_{1}}}\\left(x, \\beta_{1 j}^{*}\\right) \\frac{\\partial^{\\left|\\alpha_{2}\\right|} h}{\\partial \\eta^{\\alpha_{2}}}\\left(x, \\eta_{j}^{*}\\right)-\\sum_{j=1}^{k_{s}} \\sum_{|\\gamma|=0}^{2} s_{j, \\gamma} \\cdot \\frac{\\partial^{|\\gamma|} F}{\\partial \\beta_{1}^{*}}\\left(x, \\beta_{1 j}^{*}\\right) g_{G_{n}}(x)=0\n$$\n\nfor almost every $x$. Note that the expert function $h(\\cdot, \\eta)$ satisfies the condition in Definition 1, then the above equation implies that $t_{j, \\alpha_{1}, \\alpha_{2}}=s_{j, \\gamma}=0$, for any $j \\in\\left[k_{*}\\right], \\alpha_{1} \\in \\mathbb{N}^{d_{1}}, \\alpha_{2} \\in \\mathbb{N}^{d_{2}}$ and $\\gamma \\in \\mathbb{N}^{d_{1}}$ such that $0 \\leq\\left|\\alpha_{1}\\right|+\\left|\\alpha_{2}\\right|,|\\gamma| \\leq 2$. This contradicts the fact that at least one among the limits $t_{j, \\alpha_{1}, \\alpha_{2}}, s_{j, \\gamma}$ is different from zero.\nHence, we obtain the local inequality in equation (33) and complete the proof.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 25,
      "text": "# D AUXILIARY RESULTS \n\nIn this appendix, we present three additional results to facilitate the proofs in Appendix B and Appendix C.\nProposition 1 (Identifiability). If $g_{G}(x)=g_{G_{*}}(x)$ holds true for almost every $x$, then it follows that $G \\equiv G^{\\prime}$.\n\nProof of Proposition 1. For almost every $x$, since $g_{G}(x)=g_{G_{*}}(x)$, then we have\n\n$$\n\\begin{aligned}\n& \\sum_{i=1}^{k} \\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}+\\beta_{0 i}\\right) \\cdot h\\left(x, \\eta_{i}\\right) \\\\\n& \\quad=\\sum_{i=1}^{k_{s}} \\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}+\\beta_{0 i}^{*}\\right) \\cdot h\\left(x, \\eta_{i}^{*}\\right)\n\\end{aligned}\n$$\n\nDue to the identifiability of the expert function $h(\\cdot, \\eta)$, the set $\\left\\{h\\left(x, \\eta_{i}^{\\prime}\\right): i \\in\\left[k^{\\prime}\\right]\\right\\}$, where $\\eta_{1}^{\\prime}, \\ldots, \\eta_{k^{\\prime}}^{\\prime}$ are pair-wise different vectors for some $k^{\\prime} \\in \\mathbb{N}$, is linearly independent for almost every $x$.\nAdditionally, note that if $k \\neq k_{*}$, then there exists some index $i \\in\\left[k_{*}\\right]$ such that $\\zeta_{i} \\neq \\eta_{j}^{*}$ for any $j \\in\\left[k_{*}\\right]$. This result implies that $\\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}+\\beta_{0 i}\\right)=0$ for almost every $x$, which is a contradiction. Thus, we must have $k=k_{*}$. As a result, it follows that\n\n$$\n\\left\\{\\operatorname{Softmax}\\left(\\left(\\beta_{1 i}\\right)^{\\top} x+\\beta_{0 i}\\right): i \\in[k]\\right\\}=\\left\\{\\operatorname{Softmax}\\left(\\left(\\beta_{1 i}^{*}\\right)^{\\top} x+\\beta_{0 i}^{*}\\right): i \\in\\left[k_{*}\\right]\\right\\}\n$$\n\nfor almost every $x$. WLOG, we may assume that\n\n$$\n\\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}+\\beta_{0 i}\\right)=\\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}+\\beta_{0 i}^{*}\\right)\n$$\n\nfor almost every $x$, for any $i \\in\\left[k_{*}\\right]$. As the Softmax function is invariant to translations, then the equation (38) indicates that\n\n$$\n\\begin{aligned}\n\\frac{\\left(\\beta_{1 i}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)} & =\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)} \\\\\n\\beta_{0 i} & =\\beta_{0 i}^{*}+v_{0}\n\\end{aligned}\n$$\n\nfor some $v_{0} \\in \\mathbb{R}$. The first equation implies that $\\beta_{1 i}=\\beta_{1 i}^{*}$, while the second equation together with the assumption $\\beta_{0 k}=\\beta_{0 k}^{*}=0$ lead to $\\beta_{0 i}=\\beta_{0 i}^{*}$ for any $i \\in\\left[k_{*}\\right]$.\nLet us consider $x \\in \\mathcal{X}_{\\ell}$, where $\\ell \\in[q]$ such that $\\left\\{\\ell_{1}, \\ldots, \\ell_{K}\\right\\}=\\{1, \\ldots, K\\}$. Then, the equation (37) can be rewritten as\n\n$$\n\\begin{aligned}\n\\sum_{i=1}^{k_{*}} \\exp \\left(\\beta_{0 i}\\right) \\exp ( & \\left.\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}\\right) h\\left(x, \\zeta_{i}\\right) \\\\\n& =\\sum_{i=1}^{k_{*}} \\exp \\left(\\beta_{0 i}^{*}\\right) \\exp \\left(\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}\\right) h\\left(x, \\eta_{i}^{*}\\right)\n\\end{aligned}\n$$\n\nfor almost every $x \\in \\mathcal{X}_{\\ell}$. Next, we denote $P_{1}, P_{2}, \\ldots, P_{m}$ as a partition of the index set $\\left[k_{*}\\right]$, where $m \\leq k$, such that $\\exp \\left(\\beta_{0 i}\\right)=\\exp \\left(\\beta_{0 i^{\\prime}}^{*}\\right)$ for any $i, i^{\\prime} \\in P_{j}$ and $j \\in\\left[k_{*}\\right]$. On the other hand, when $i$ and $i^{\\prime}$ do not belong to the same set $P_{j}$, we let $\\exp \\left(\\beta_{0 i}\\right) \\neq \\exp \\left(\\beta_{0 i^{\\prime}}\\right)$. Thus, we can represent equation (39) as\n\n$$\n\\begin{aligned}\n\\sum_{j=1}^{m} \\sum_{i \\in P_{j}} \\exp \\left(\\beta_{0 i}\\right) \\exp ( & \\left.\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}\\right) h\\left(x, \\zeta_{i}\\right) \\\\\n& =\\sum_{j=1}^{m} \\sum_{i \\in P_{j}} \\exp \\left(\\beta_{0 i}^{*}\\right) \\exp \\left(\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}^{*}\\right\\|+\\tau_{1}\\right) \\cdot\\left(\\|x\\|+\\tau_{2}\\right)}\\right) h\\left(x, \\eta_{i}^{*}\\right)\n\\end{aligned}\n$$\n\nfor almost every $x \\in \\mathcal{X}_{\\ell}$. Recall that we have $\\beta_{1 i}=\\beta_{1 i}^{*}$ and $\\beta_{0 i}=\\beta_{0 i}^{*}$, for any $i \\in\\left[k_{*}\\right]$, then the above result leads to\n\n$$\n\\left\\{\\eta_{i}: i \\in P_{j}\\right\\} \\equiv\\left\\{\\eta_{i}^{*}: i \\in P_{j}\\right\\}\n$$\n\nfor any $j \\in[m]$. As a consequence, we obtain that\n\n$$\nG=\\sum_{j=1}^{m} \\sum_{i \\in P_{j}} \\exp \\left(\\beta_{0 i}\\right) \\delta_{\\left(\\beta_{1 i}, \\eta_{i}\\right)}=\\sum_{j=1}^{m} \\sum_{i \\in P_{j}} \\exp \\left(\\beta_{0 i}\\right) \\delta_{\\left(\\beta_{1 i}^{*}, \\eta_{i}^{*}\\right)}=G_{*}\n$$\n\nHence, we reach the conclusion of this proposition.\nLemma 3. Let $F\\left(x, \\beta_{1}\\right):=\\exp \\left(\\frac{\\beta_{1}^{\\top} x}{\\left\\|\\beta_{1}\\right\\| \\cdot\\|x\\|}\\right)$. For any vector $\\beta_{1} \\in \\mathbb{R}^{d_{1}}$ and $t \\in \\mathbb{N}$, we have\n\n$$\n\\sum_{u_{1}, \\ldots, u_{t}=1}^{d_{1}} \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\beta_{1}^{\\left(u_{t}\\right)} \\cdot \\frac{\\partial^{t} F}{\\partial \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t}\\right)}}\\left(x, \\beta_{1}\\right)=0\n$$\n\nProof of Lemma 3. We will prove the above result by using the induction method. In particular, we first show that it holds for $t=1$. By taking the first derivative of $F$ w.r.t $\\beta_{1}$, we have\n\n$$\n\\frac{\\partial F}{\\partial \\beta_{1}}\\left(x, \\beta_{1}\\right)=\\frac{x \\cdot\\left\\|\\beta_{1}\\right\\| \\cdot\\|x\\|-\\frac{\\beta_{1}}{\\left\\|\\beta_{1}\\right\\|} \\cdot\\|x\\| \\cdot \\beta_{1}^{\\top} x}{\\left\\|\\beta_{1}\\right\\|^{2}\\|x\\|^{2}} \\cdot F\\left(x, \\beta_{1}\\right)\n$$\n\nThen, it follows that\n\n$$\n\\beta_{1}^{\\top} \\frac{\\partial F}{\\partial \\beta_{1}}\\left(x, \\beta_{1}\\right)=\\frac{\\beta_{1}^{\\top} x \\cdot\\left\\|\\beta_{1}\\right\\| \\cdot\\|x\\|-\\frac{\\beta_{1}^{\\top} \\beta_{1}}{\\left\\|\\beta_{1}\\right\\|} \\cdot\\|x\\| \\cdot \\beta_{1}^{\\top} x}{\\left\\|\\beta_{1}\\right\\|^{2}\\|x\\|^{2}}=0\n$$\n\nor equivalently,\n\n$$\n\\sum_{u_{t}=1}^{d_{1}} \\beta_{1}^{\\left(u_{1}\\right)} \\cdot \\frac{\\partial F}{\\partial \\beta_{1}^{\\left(u_{1}\\right)}}\\left(x, \\beta_{1}\\right)=0\n$$\n\nSubsequently, assume that the equation (40) holds for $t-1$, i.e.\n\n$$\n\\sum_{u_{1}, \\ldots, u_{t-1}=1}^{d_{1}} \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t-1}\\right)}}\\left(x, \\beta_{1}\\right)=0\n$$\n\nwe will demonstrate that it also holds for $t$. Note that the above left-hand side can be decomposed as\n\n$$\n\\begin{aligned}\n& \\sum_{u_{1}, \\ldots, u_{t-1}=1}^{d_{1}} \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t-1}\\right)}}\\left(x, \\beta_{1}\\right)=\\sum_{u_{1}, \\ldots, u_{t-1} \\neq u_{t}} \\beta_{1}^{\\left(u_{t}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{t}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t-1}\\right)}} \\\\\n& +\\binom{t-1}{1} \\sum_{u_{2}, \\ldots, u_{t-1} \\neq u_{t}} \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\beta_{1}^{\\left(u_{t}\\right)} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t-1}\\right)} \\partial \\beta_{1}^{\\left(u_{t}\\right)}}\\left(x, \\beta_{1}\\right) \\\\\n& +\\ldots \\\\\n& +\\binom{t-1}{t-2} \\sum_{u_{t-1} \\neq u_{t}} \\beta_{1}^{\\left(u_{t-1}\\right)}\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-2} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{t-1}\\right)} \\partial\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-2}}\\left(x, \\beta_{1}\\right)+\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-1} \\cdot \\frac{\\partial^{t-1} F}{\\partial\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-1}}\\left(x, \\beta_{1}\\right)\n\\end{aligned}\n$$\n\nwhere $u_{t}$ is some index in $[d]$. By taking the derivatives of both sides w.r.t $\\beta_{1}^{\\left(u_{t}\\right)}$, we get\n\n$$\n\\begin{aligned}\n& 0=\\sum_{u_{1}, \\ldots, u_{t-1} \\neq u_{t}} \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\cdot \\frac{\\partial^{t} F}{\\partial \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t}\\right)}} \\\\\n& +\\binom{t-1}{1} \\sum_{u_{2}, \\ldots, u_{t-1} \\neq u_{t}} \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t-1}\\right)} \\partial \\beta_{1}^{\\left(u_{t}\\right)}}\\left(x, \\beta_{1}\\right) \\\\\n& +\\binom{t-1}{1} \\sum_{u_{2}, \\ldots, u_{t-1} \\neq u_{t}} \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\beta_{1}^{\\left(u_{t}\\right)} \\cdot \\frac{\\partial^{t} F}{\\partial \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t-1}\\right)} \\partial\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{2}}\\left(x, \\beta_{1}\\right) \\\\\n& +\\ldots \\\\\n& +\\binom{t-1}{t-2}(t-2) \\sum_{u_{t-1} \\neq u_{t}} \\beta_{1}^{\\left(u_{t-1}\\right)}\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-3} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{t-1}\\right)} \\partial\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-2}}\\left(x, \\beta_{1}\\right) \\\\\n& +\\binom{t-1}{t-2} \\sum_{u_{t-1} \\neq u_{t}} \\beta_{1}^{\\left(u_{t-1}\\right)}\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-2} \\cdot \\frac{\\partial^{t} F}{\\partial \\beta_{1}^{\\left(u_{t-1}\\right)} \\partial\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-1}}\\left(x, \\beta_{1}\\right) \\\\\n& +(t-1)\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-2} \\cdot \\frac{\\partial^{t-1} F}{\\partial\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-1}}\\left(x, \\beta_{1}\\right)+\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t-1} \\cdot \\frac{\\partial^{t} F}{\\partial\\left(\\beta_{1}^{\\left(u_{t}\\right)}\\right)^{t}}\\left(x, \\beta_{1}\\right) \\\\\n& =\\sum_{u_{1}, \\ldots, u_{t-1}=1}^{d_{1}} \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\cdot \\frac{\\partial^{t} F}{\\partial \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t}\\right)}}\\left(x, \\beta_{1}\\right) \\\\\n& +(t-1) \\sum_{u_{2}, \\ldots, u_{t-1}=1}^{d_{1}} \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t}\\right)}}\\left(x, \\beta_{1}\\right)\n\\end{aligned}\n$$\n\nTherefore, it follows that\n\n$$\n\\begin{aligned}\n& \\sum_{u_{t}=1}^{d_{1}} \\beta_{1}^{\\left(u_{t}\\right)} \\cdot 0=\\sum_{u_{1}, \\ldots, u_{t}=1}^{d_{1}} \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\beta_{1}^{\\left(u_{t}\\right)} \\cdot \\frac{\\partial^{t} F}{\\partial \\beta_{1}^{\\left(u_{t}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t}\\right)}}\\left(x, \\beta_{1}\\right) \\\\\n&+(t-1) \\sum_{u_{2}, \\ldots, u_{t}=1}^{d_{1}} \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\beta_{1}^{\\left(u_{t}\\right)} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t}\\right)}}\\left(x, \\beta_{1}\\right) .\n\\end{aligned}\n$$\n\nIt is worth noting that\n\n$$\n\\begin{aligned}\n& \\sum_{u_{2}, \\ldots, u_{t}=1}^{d_{1}} \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\beta_{1}^{\\left(u_{t}\\right)} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{2}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t}\\right)}}\\left(x, \\beta_{1}\\right) \\\\\n& =\\sum_{u_{1}, \\ldots, u_{t-1}=1}^{d_{1}} \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\cdot \\frac{\\partial^{t-1} F}{\\partial \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t-1}\\right)}}\\left(x, \\beta_{1}\\right)=0\n\\end{aligned}\n$$\n\nConsequently, we deduce that\n\n$$\n\\sum_{u_{1}, \\ldots, u_{t}=1}^{d_{1}} \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\beta_{1}^{\\left(u_{t-1}\\right)} \\beta_{1}^{\\left(u_{t}\\right)} \\cdot \\frac{\\partial^{t} F}{\\partial \\beta_{1}^{\\left(u_{1}\\right)} \\ldots \\partial \\beta_{1}^{\\left(u_{t}\\right)}}\\left(x, \\beta_{1}\\right)=0\n$$\n\nHence, we reach the conclusion in equation (40).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 26,
      "text": "# E EXPERIMENTAL DETAILS \n\nIn this appendix, we provide the details for the numerical experiments on synthetic data, and the experiments with real data on language modeling conducted in Section 5.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 27,
      "text": "## E. 1 EXPERIMENTAL DETAILS FOR SYNTHETIC DATA\n\nModel details. We now provide the details for the model parameters in model\n\n$$\nf_{G_{*}}(x):=\\sum_{i=1}^{k_{i}} \\operatorname{Softmax}\\left(\\frac{\\left(\\beta_{1 i}^{*}\\right)^{\\top} x}{\\left(\\left\\|\\beta_{1 i}^{*}\\right\\|+\\tau\\right) \\cdot(\\|x\\|+\\tau)}+\\beta_{0 i}^{*}\\right) \\cdot \\phi\\left(\\left(a_{i}^{*}\\right)^{\\top} x+b_{i}^{*}\\right)\n$$\n\nThe variance of Gaussian noise is specified as $\\sigma^{2}=0.01$. For simplicity, the perturbations for both $\\|x\\|$ and $\\left\\|\\beta_{1 i}^{*}\\right\\|$ are considered identical, denoted by $\\tau_{1}=\\tau_{2}=\\tau$. The true parameters for the router, $\\left(\\beta_{1 i}^{*}, \\beta_{0 i}^{*}\\right) \\in \\mathbb{R}^{d} \\times \\mathbb{R}$, are drawn independently from an isotropic Gaussian distribution with zero mean and variance $\\sigma_{c}^{2}=0.01 / d$ for $1 \\leq i \\leq 6$, and otherwise are set to zero. Similarly, the true parameters of the experts, $\\left(a_{i}^{*}, b_{i}^{*}\\right) \\in \\mathbb{R}^{d} \\times \\mathbb{R}$, are drawn independently of an isotropic Gaussian distribution with zero mean and variance $\\sigma_{c}^{2}=1 / d$ for all experts. These parameters remain unchanged for all experiments.\nTraining procedure. For each sample size $n$, spanning from $10^{3}$ to $10^{5}$, we perform 20 experiments. In every experiment, the parameters initialization for the router's and experts' parameters are adjusted to be near the true parameters, minimizing potential instabilities from the optimization process. Subsequently, we execute SGD across 10 epochs, employing a learning rate of $\\eta=0.1$ to fit a model to the synthetic data. All the numerical experiments are conducted on a MacBook Air equipped with an M1 chip CPU.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 28,
      "text": "## E. 2 EXPERIMENTAL DETAILS FOR LANGUAGE MODELING TASK\n\nDatasets. We use the Enwik8 and Text8 datasets (Mahoney, 2011) for our character-level language modeling task. The Enwik8 dataset comprises 100 million bytes of unprocessed Wikipedia text, while the Text8 dataset contains 100 million processed Wikipedia characters. We further evaluate the word-level language modeling task on the Wikitext-103 dataset (Merity et al., 2016), which is the largest available word-level language modeling benchmark with long-term dependency. It contains\n\n103M training tokens from 28 K articles, with an average length of 3.6 K tokens per article, which allows us to test the ability of long-term dependency modeling.\nMetrics. In the main paper, we employ the Bit per character (BPC) (Graves, 2013) metric to assess the performance of character-level language modeling tasks. This metric measures the average number of bits needed to encode each character in the dataset. It is calculated as follows:\n\n$$\n\\operatorname{BPC}(\\mathrm{X})=-\\frac{1}{T} \\sum_{t=1}^{T} \\log _{2} \\hat{P}_{t}\\left(x_{t}\\right)\n$$\n\nwhere $T$ is the length of the input string $X, \\hat{P}_{t}$ is the approximate distribution and $x_{t}$ is the character in the input string at location $t$.\nEssentially, BPC quantifies the average number of bits required to encode each character in the text using the probability distribution predicted by the model. This concept works as follows: characters with high probability get a short bit sequence, while characters with low probability get a longer sequence. Then, the next character is read from the input and encoded using the bit sequence performance determined from the probability distribution. If the language model is good, the target character will have been predicted with high probability, so the bit sequence will be short. This means that a lower BPC indicates better compression, which in turn demonstrates the model's superior ability to predict the next character accurately.\n\nFor the word-level language modeling task on the Wikitext-103 dataset, we utilize Perplexity (PPL) (Jelinek et al., 1977) as our evaluation metric. It represents the exponentiated average negative log-likelihood of a sequence and demonstrates how well the model predicts the next word in a sequence. More specifically, if we have a tokenized sequence $X=\\left(x_{0}, x_{1}, \\ldots, x_{t}\\right)$, the perplexity of $X$ is:\n\n$$\n\\operatorname{PPL}(\\mathrm{X})=\\exp \\left\\{-\\frac{1}{t} \\sum_{i=1}^{t} \\log p_{\\theta}\\left(x_{i} \\mid x_{<i}\\right)\\right\\}\n$$\n\nwhere $p_{\\theta}\\left(x_{i} \\mid x_{<i}\\right)$ is the log-likelihood of the $i^{t h}$ token conditioned on the preceding tokens $x_{<i}$ according to our model. A lower perplexity score indicates better generalization performance.\n\nWe follow the implementation of (Pham et al., 2024) to use experts consisting of two linear layers: the first with weights of shape input_dim $\\times$ hidden_dim_experts and the second with weights of shape hidden_dim_experts $\\times$ input_dim, followed by ReLU activations and Dropout layer with drop rate $p=0.1$. This architecture ensures the output has a shape of input_dim $\\times$ input_dim while can flexibly reduce parameters compared to a single linear layer with weights of shape input_dim $\\times$ input_dim when the number of hidden dimensions of experts chosen is smaller than dimensions of the input. We consistently apply this architecture with our perturbed and vanilla cosine router across all datasets.\nTraining setup and hyperparameters. We consider two model configurations: the small and medium setups. The small setup has a total of 15 million parameters with 6 SMoE layers (Dryden \\& Hoefler, 2022), and each layer can learn spatial structure in the input domain and routing experts at a fine-grained level to utilize it. Similarly, the medium setup consists of 36 million parameters with 8 SMoE layers (Dryden \\& Hoefler, 2022). For SMoE layers in both configurations, we employ 16 experts with top-2 gating in both cosine and perturbed cosine routers. To mitigate the representation collapse issue, we adopt the method proposed by Chi et al. (2022). Specifically, this approach involves parameterizing the experts using lower-dimensional embeddings $\\boldsymbol{e}_{i} \\in \\mathbb{R}^{d_{e}}$ and applying a linear projection to map token vectors into this compact space, rather than the original high-dimensional hidden space $d$. Furthermore, we follow Chi et al. (2022) to set $d_{e}=8$, which is half the number of experts in our implementation for the language modeling task.\nDuring training, we use Adam optimizer (Kingma \\& Ba, 2017) with default parameters. We set the number of training steps to 60000 and 80000 for small and medium configurations, respectively. The results are averaged over three runs for fair comparisons.\nAll language modeling experiments are conducted on NVIDIA A100 GPUs. Training the small configuration of the Text8 and Enwik8 datasets takes 11 hours, whereas Wikitext-103 requires 5 hours. Training Text8 and Enwik8 for medium configurations takes 17 hours, while Wikitext-103 training takes 8 hours.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 29,
      "text": "# E. 3 Experimental Details for Domain Generalization \n\nExpert types. Similarly, following (Li et al., 2023), the experts utilized in the domain generalization experiments are two-layer feedforward networks, featuring a GELU activation and a Dropout layer with a drop rate of 0.1 positioned between the two linear layers. The dimensions of the two linear layers are input_dim $\\times$ hidden_dim_experts and hidden_dim_experts $\\times$ input_dim, respectively, where hidden_dim_experts $=$ input_dim $\\times 4$.\n\nTraining procedure. All DG experiments are run on NVIDIA A100 GPUs with 15,000 iterations. The training time on PACS, VLCS, OfficeHome, and TerraIncognita is 2 hours, while the training time for DomainNet is 7 hours.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 30,
      "text": "## F Additional EXPERIMENTS\n\nIn this appendix, we carry out additional experiments on the synthetic data to compare the sample efficiency of the perturbed cosine router to that of the cosine router and the linear router (Nguyen et al., 2024c) under the settings where the data are generated from the same regression framework.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 31,
      "text": "## F. 1 Cosine Router vs. Perturbed Cosine Router\n\nExperimental setup. We generate the data by first sampling $X_{i} \\sim \\operatorname{Uniform}\\left([-1,1]^{d}\\right)$ for $i=$ $1, \\ldots, n$. Then, we generate $Y_{i}$ according to the following model:\n\n$$\nY_{i}=s_{G_{*}}\\left(X_{i}\\right)+\\epsilon_{i}\n$$\n\nwhere the regression function $s_{G_{*}}(\\cdot)$ takes the form of a linear router MoE (Nguyen et al., 2024c):\n\n$$\n\\sum_{i=1}^{k_{n}} \\operatorname{Softmax}\\left(\\left(\\beta_{1 i}^{*}\\right)^{\\top} x+\\beta_{0 i}^{*}\\right) \\cdot \\operatorname{ReLU}\\left(\\left(a_{i}^{*}\\right)^{\\top} x+b_{i}^{*}\\right)\n$$\n\nAll other experimental details remain the same as specified in Section 5.1 and Appendix E.1.\nResults. We calculate the Voronoi losses and report the mean values for each sample size in Figure 2a. Error bars representing two standard deviations are also shown. In Figure 2a, the Voronoi losses associated with the cosine router vanish at the rate of $O\\left(n^{-0.10}\\right)$, which is very slow and roughly matches our theoretical results. Meanwhile, those associated with the perturbed cosine router converge to zero at significantly faster rate of $O\\left(n^{-0.45}\\right)$. This empirically shows that the perturbed cosine router is more sample efficient than the vanilla cosine router.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 32,
      "text": "## F. 2 Linear Router vs. Perturbed Cosine Router\n\nExperimental setup. From Table 1, we see that the perturbed cosine router outperforms the linear router when the experts are of polynomial forms $\\left(a^{\\top} x+b\\right)^{2}$. Thus, we will incorporate polynomial experts in both fitted models. Additionally, we generate the data by first sampling $X_{i} \\sim \\operatorname{Uniform}\\left([-1,1]^{d}\\right)$ for $i=1, \\ldots, n$. Then, we generate $Y_{i}$ according to the following model:\n\n$$\nY_{i}=s_{G_{*}}\\left(X_{i}\\right)+\\epsilon_{i}\n$$\n\nwhere the regression function $s_{G_{*}}(\\cdot)$ takes the form of a linear router MoE with polynomial experts (Nguyen et al., 2024c):\n\n$$\n\\sum_{i=1}^{k_{n}} \\operatorname{Softmax}\\left(\\left(\\beta_{1 i}^{*}\\right)^{\\top} x+\\beta_{0 i}^{*}\\right) \\cdot\\left(\\left(a_{i}^{*}\\right)^{\\top} x+b_{i}^{*}\\right)^{2}\n$$\n\nAll other experimental details remain the same as specified in Section 5.1 and Appendix E.1.\nResults. We calculate the Voronoi losses and report the mean values for each sample size in Figure 2b. Error bars representing two standard deviations are also shown. In Figure 2b, the Voronoi losses associated with the linear router vanish at a very slow rate of $O\\left(n^{-0.19}\\right)$. By contrast, the convergence rate of those associated with the perturbed cosine router are of order $O\\left(n^{-0.47}\\right)$, which are substantially faster. Thus, we can claim that the perturbed cosine router is more sample efficient than the linear router both theoretically and empirically.\n\n![img-2.jpeg](img-2.jpeg)\n\nFigure 2: Log-log scaled plots displaying the empirical convergence rates. Figure 2a depicts the empirical averages of the Voronoi losses when using the cosine router (green line) versus when using the perturbed cosine router (blue line). The red dash-dotted lines illustrate the fitted lines for determining the empirical convergence rates. Similarly, Figure 2b depicts the empirical averages of the Voronoi losses when using the linear router (green line) versus when using the perturbed cosine router (blue line). We use the same data samples for those experiments.",
      "tables": {},
      "images": {
        "img-2.jpeg": "data:image/jpeg;base64,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"
      }
    }
  ],
  "id": "2405.14131v3",
  "authors": [
    "Huy Nguyen",
    "Pedram Akbarian",
    "Trang Pham",
    "Trang Nguyen",
    "Shujian Zhang",
    "Nhat Ho"
  ],
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "abstract": "The cosine router in Mixture of Experts (MoE) has recently emerged as an\nattractive alternative to the conventional linear router. Indeed, the cosine\nrouter demonstrates favorable performance in image and language tasks and\nexhibits better ability to mitigate the representation collapse issue, which\noften leads to parameter redundancy and limited representation potentials.\nDespite its empirical success, a comprehensive analysis of the cosine router in\nMoE has been lacking. Considering the least square estimation of the cosine\nrouting MoE, we demonstrate that due to the intrinsic interaction of the model\nparameters in the cosine router via some partial differential equations,\nregardless of the structures of the experts, the estimation rates of experts\nand model parameters can be as slow as $\\mathcal{O}(1/\\log^{\\tau}(n))$ where\n$\\tau > 0$ is some constant and $n$ is the sample size. Surprisingly, these\npessimistic non-polynomial convergence rates can be circumvented by the widely\nused technique in practice to stabilize the cosine router -- simply adding\nnoises to the $\\ell^2$-norms in the cosine router, which we refer to as\n\\textit{perturbed cosine router}. Under the strongly identifiable settings of\nthe expert functions, we prove that the estimation rates for both the experts\nand model parameters under the perturbed cosine routing MoE are significantly\nimproved to polynomial rates. Finally, we conduct extensive simulation studies\nin both synthetic and real data settings to empirically validate our\ntheoretical results.",
  "updated": "2025-03-05T17:05:55Z",
  "published": "2024-05-23T03:11:07Z"
}