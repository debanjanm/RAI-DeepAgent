{"title": "Efficient estimation and data fusion under general semiparametric\n  restrictions on outcome mean functions", "sections": [{"section_id": 0, "text": "#### Abstract\n\nWe provide a novel characterization of semiparametric efficiency in a generic supervised learning setting where the outcome mean function - defined as the conditional expectation of the outcome of interest given the other observed variables - is restricted to lie in some known semiparametric function class. The primary motivation is causal inference where a researcher running a randomized controlled trial often has access to an auxiliary observational dataset that is confounded or otherwise biased for estimating causal effects. Prior work has imposed various bespoke assumptions on this bias in an attempt to improve precision via data fusion. We show how many of these assumptions can be formulated as restrictions on the outcome mean function in the concatenation of the experimental and observational datasets. Then our theory provides a unified framework to maximally leverage such restrictions for precision gain by constructing efficient estimators in all of these settings as well as in a wide range of others that future investigators might be interested in. For example, when the observational dataset is subject to outcome-mediated selection bias, we show our novel efficient estimator dominates an existing control variate approach both asymptotically and in numerical studies.", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1 Introduction\n\nConsider a generic supervised learning setup with independent and identically distributed (i.i.d.) observations $W_{i}=\\left(Y_{i}, R_{i}\\right)_{i=1}^{N}$, where the outcome $Y_{i} \\in \\mathbb{R}$ is scalar. Omitting the observation index $i$ for brevity, let the outcome mean function\n\n$$\nm(r)=\\mathbb{E}[Y \\mid R=r]\n$$\n\nbe the conditional expectation of the outcome given the covariates. We seek a novel characterization of the semiparametric efficiency bound - the smallest asymptotic variance attainable by any regular and asymptotically linear (RAL) estimator of any smooth (pathwise differentiable) and identified estimand $\\tau \\in \\mathbb{R}^{d}$ - in a class of semiparametric models $\\mathcal{P}_{\\mathcal{M}}$ that restrict the outcome mean function $m(\\cdot)$ to lie in a known collection $\\mathcal{M}$ of square integrable real-valued functions on the covariate space $\\mathcal{R}$. This facilitates the construction of efficient one-step estimators in the model $\\mathcal{P}_{\\mathcal{M}}$.\n\nA key motivation for this problem comes from data fusion problems in causal inference. The economic and ethical challenges of running a large randomized controlled trial (RCT) has spurred significant work on combining data from RCT's with much larger and easily obtained observational datasets to improve the precision of causal estimates. However, due to factors like unobserved confounding or selection, causal effects are generally not identifiable from observational data where treatment is not randomized. Then, naive approaches that simply combine observations from an RCT and an observational study into a single fused dataset will lead to biased causal estimates, even (indeed, most saliently) with a massive observational dataset (Bareinboim and Pearl, 2016).\n\nOn the other hand, without any assumptions about the biases in the observational dataset, one typically cannot use the outcomes in the observational data to improve the asymptotic precision of\n\ncausal estimates. Intuitively, this is because without any linkage between the outcome distributions in the two datasets, the outcomes in the observational dataset cannot possibly provide any statistical information about the outcome distribution in the RCT, which typically identifies causal parameters of interest. For example, the semiparametric efficiency bound for the average treatment effect $\\tau_{\\text {ret }}$ in the population in which the RCT was conducted can be attained by estimators such as the augmented inverse propensity weighting (AIPW) estimator (Robins et al., 1994; Tsiatis, 2006; Rothe, 2016) that only use observations from the RCT (Li et al., 2023b; Liao et al., 2023). Even if we wish to estimate a \"transported\" average treatment effect on the population defined by the covariate distribution in the observational dataset, the efficiency bound can be obtained by estimators using just the RCT observations and the covariates in the observational dataset. The outcomes in the observational dataset cannot provide further asymptotic variance reductions without using nonregular estimators (Dahabreh et al., 2019, 2020; Lee et al., 2023; Li et al., 2023a).\n\nMany authors have thus chosen another route: make structural assumptions linking the outcome distributions in the two datasets to enable precision gains. Through various examples in Section 1.1, we show that such assumptions often correspond precisely to imposing a model $\\mathcal{P}_{\\mathcal{M}}$ on the fused dataset, for an appropriate choice of $\\mathcal{M}$. In Section 2 we present our main theoretical result, Theorem 2, which characterizes semiparametric efficiency in a general model of the form $\\mathcal{P}_{\\mathcal{M}}$. We show how to use this result to construct efficient one-step estimators in Section 3. Such estimators depend on first-stage estimation of nuisance parameters including the outcome mean function $m(\\cdot)$. As well-established in the literature on double/debiased machine learning, under appropriate regularity conditions and cross-fitting, the estimators can attain the efficiency bound even if such nuisance functions are estimated at slower than parametric rates.\n\nIn Section 4, we provide specific efficiency bounds and estimators in two of the particular data fusion settings from the literature involving a linear confounding bias (see Example 4 below) and outcome-mediated selection bias (see Example 5). To our knowledge, efficient estimators have not previously been derived in either of these settings. We provide numerical simulations and a real data example based on the Tennessee Student Teacher Achievement Ratio (STAR) study in Sections 5 and 6, respectively, to illustrate the finite-sample performance of our novel estimators, which provably dominate the existing inefficient proposals asymptotically. We conclude with a brief discussion of related work in Section 7.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 1.1 Examples \n\nWe now provide several motivating examples from the literature of the semiparametric model $\\mathcal{P}_{\\mathcal{M}}$ introduced previously. Recall that each such model restricts the joint distribution of $W$ so that the outcome mean function $m(\\cdot)$ belongs to a particular known function collection $\\mathcal{M}$.\nExample 1 (Restricted moment model). If $\\mathcal{M}_{1}$ is a smooth finite-dimensional parametric class, i.e. $\\mathcal{M}_{1}=\\left\\{r \\mapsto \\mu(r ; \\beta), \\beta \\in \\mathbb{R}^{q}\\right\\}$, the model $\\mathcal{P}_{\\mathcal{M}_{1}}$ is known as the \"restricted moment model,\" and is a textbook motivating example of semiparametric theory (e.g., Ch. 4 of Tsiatis (2006)).\n\nThe remaining examples are data fusion settings in causal inference, where a model of the form $\\mathcal{P}_{\\mathcal{M}}$ applies to the fused observational and experimental dataset. The fused dataset has $R=(S, Z, X)$ for $S$ a binary indicator of which dataset the observation is taken from ( $S=1$ denotes experimental, $S=0$ denotes observational), $Z$ a binary indicator of treatment status, and $X \\in \\mathcal{X}$ a vector of observed covariates. Examples with more than two datasets and/or treatment statuses follow easily but are notationally cumbersome. For $(s, z) \\in\\{0,1\\}^{2}$ we write $m_{s z}(x)$ as a shorthand for $m(s, z, x)$. We also let $\\mathcal{H}_{V}$ denote the set of square integrable functions of any variable or set of variables $V$, which contains $\\mathcal{H}_{V}^{0}$, the set of such functions with mean zero under $P^{*}$, the true\n\ndistribution of $W$. For example, explicitly we have $\\mathcal{H}_{W}=\\left\\{f: \\mathcal{W} \\mapsto \\mathbb{R} \\mid \\mathbb{E}\\left[f^{2}(W)\\right]<\\infty\\right\\}$; unless otherwise noted, all expectations are to be taken with respect to $P^{*}$, i.e. $\\mathbb{E}[f(W)] \\equiv \\int f(w) d P^{*}(w)$. We recall that $\\mathcal{H}_{W}$ is a Hilbert space equipped with the inner product $\\langle f, g\\rangle=\\mathbb{E}[f(W) g(W)]$, which does not distinguish between functions equal almost-surely under $P^{*}$.\nExample 2 (Mean-exchangeable controls, Li et al. (2023b)). Suppose the observational dataset consists only of untreated subjects or \"controls\" (i.e., $\\operatorname{Pr}(Z=0 \\mid S=0)=1$ ) and assume that these controls are mean-exchangeable with those in the experimental dataset, i.e., $m_{10}(x)=$ $m_{00}(x) \\forall x \\in \\mathcal{X}$. This corresponds to the model $\\mathcal{P}_{\\mathcal{M}_{2}}$ where\n\n$$\n\\mathcal{M}_{2}=\\left\\{r \\mapsto z m_{1}(x)+(1-z) m_{0}(x) \\mid m_{0}, m_{1} \\in \\mathcal{H}_{X}\\right\\}\n$$\n\nExample 3 (Parametric confounding bias and CATE, Yang et al. (2024)). Assuming treatment is randomized in the experimental dataset, the difference $\\tau(x ; m)=m_{11}(x)-m_{10}(x)$ between the conditional average outcomes in the treated and control groups in the experimental dataset given $X=x$ is well known to have a causal interpretation in the Neyman-Rubin potential outcomes framework as the conditional average treatment effect (CATE). The analogous difference $m_{01}(x)-$ $m_{00}(x)$ in the observational dataset, on the other hand, may not have a causal interpretation due to unobserved confounders. Yang et al. (2024) consider a model where both the CATE $\\tau(x)$ and the differences in these differences $\\lambda(x ; m)=\\left(m_{01}(x)-m_{00}(x)\\right)-\\tau(x)$, called the confounding function, lie in smooth finite-dimensional parametric classes. This corresponds to the model $\\mathcal{P}_{\\mathcal{M}_{3}}$ for\n\n$$\n\\mathcal{M}_{3}=\\left\\{m \\in \\mathcal{H}_{R} \\mid \\tau(x ; m)=\\mu(x ; \\beta), \\lambda(x ; m)=\\phi(x ; \\theta), \\beta \\in \\mathbb{R}^{p}, \\theta \\in \\mathbb{R}^{q}\\right\\}\n$$\n\nExample 4 (Linear confounding bias, Kallus et al. (2018)). Continuing the notation of Example 3, we place no restrictions on the CATE $\\tau(x ; m)$ while assuming the confounding function is linear in a $q$-dimensional basis expansion $\\psi$. This is the model $\\mathcal{P}_{\\mathcal{M}_{4}}$ for\n\n$$\n\\mathcal{M}_{4}=\\left\\{m \\in \\mathcal{H}_{R} \\mid \\lambda(x ; m)=\\psi(x)^{\\top} \\theta, \\theta \\in \\mathbb{R}^{q}\\right\\}\n$$\n\nExample 5 (Outcome-mediated selection bias, Guo et al. (2022)). Suppose that $Y \\in\\{0,1\\}$ and that the observational dataset is subject to \"outcome-mediated selection bias.\" This means that the observational dataset is subject to selection that depends only on $Y$ (and not on $(Z, X)$ ). For example, \"controls\" with $Y=0$ are omitted in the observational dataset more often than \"cases\" with $Y=1$, and omission occurs independently of $(Z, X)$ given $Y$. Guo et al. (2022) show in their Proposition 4.1 that outcome-mediated selection bias implies\n\n$$\nO R_{1}(x)=O R_{0}(x), \\quad O R_{s}(x):=\\frac{m_{s 1}(x) /\\left(1-m_{s 1}(x)\\right)}{m_{s 0}(x) /\\left(1-m_{s 0}(x)\\right)}, \\quad s=0,1\n$$\n\nThus, outcome-mediated selection bias implies the model $\\mathcal{P}_{\\mathcal{M}_{5}}$ for\n\n$$\n\\mathcal{M}_{5}=\\left\\{m \\in \\mathcal{H}_{R} \\mid \\text { (4) holds, } \\epsilon<m(r)<1-\\epsilon \\forall r \\in \\mathcal{R}\\right\\}\n$$\n\nWe will show that our results can recover various semiparametric efficiency bounds and efficient estimators in the models of Examples 1, 2, and 3 that were previously derived in the cited works. The model in Example 4 was studied by Kallus et al. (2018) but for estimating the CATE rather than finite dimensional estimands. Turning to Example 5, Guo et al. (2022) propose a control variate approach to leverage (4) for variance reduction. In general, however, these estimators are inefficient, and do not come with clear guidance on how to choose control variates to minimize variance. Thus, we use our results to derive novel efficiency bounds and efficient estimators for various average treatment effects in the models $\\mathcal{P}_{\\mathcal{M}_{4}}$ and $\\mathcal{P}_{\\mathcal{M}_{5}}$ in Examples 4 and 5. The estimators are given explicitly in Section 4.", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 2 Semiparametric efficiency bounds \n\nWe return to the generic supervised learning setting of the introduction and present our main result characterizing semiparametric efficiency bounds for the models $\\mathcal{P}_{\\mathcal{M}}$. Formally, the model $\\mathcal{P}_{\\mathcal{M}}$ is a collection of distributions $P$ on $W$ with outcome mean functions in $\\mathcal{M}$ assumed to contain the true data-generating process $P^{*}$. The semiparametric efficiency bound for an a pathwise differentiable estimand $\\tau=\\tau(P)$ at its true value $\\tau^{*}=\\tau\\left(P^{*}\\right)$ is defined as the smallest asymptotic variance attainable by any RAL estimator of $\\tau$. The reader is encouraged to reference texts on semiparametric theory (Bickel et al., 1993; Van der Vaart, 2000; Tsiatis, 2006) for further background and technical details; we present the main concepts necessary for our discussion below.\n\nBy definition of asymptotic linearity, any RAL estimator of $\\tau \\in \\mathbb{R}^{d}$ must satisfy the asymptotic expansion\n\n$$\n\\sqrt{N}\\left(\\hat{\\tau}-\\tau^{*}\\right)=\\frac{1}{\\sqrt{N}} \\sum_{i=1}^{N} \\varphi\\left(W_{i} ; \\tau^{*}, \\eta^{*}\\right)+o_{p}(1)\n$$\n\nas $N \\rightarrow \\infty$ where $\\varphi\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right) \\in\\left(\\mathcal{H}_{W}^{0}\\right)^{d}$ is known as the influence function of $\\tau$. The notation $\\varphi\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$ emphasizes that in general, the influence function $\\varphi$ can depend on both the estimand $\\tau$ of interest and nuisance parameters $\\eta$ of interest, which are often infinite dimensional functionals, like the outcome mean function $m$, of the distribution $P$ in the model $\\mathcal{P}_{\\mathcal{M}}$. Regularity further restricts the space of possible influence functions of RAL estimators of $\\tau$ under the model $\\mathcal{P}_{\\mathcal{M}}$ within $\\mathcal{H}_{W}^{0}$, as discussed below. The efficient influence function $\\varphi_{\\text {eff }}$ is the unique influence function in this space with the smallest variance (with respect to the Loewner ordering when $d>1$ ). Then the semiparametric efficiency bound is $V_{\\text {eff }}=\\mathbb{E}\\left[\\varphi_{\\text {eff }}(W)^{\\otimes 2}\\right]$, where $v^{\\otimes 2}=v v^{\\top}$ for any vector $v$.\n\nTheorem 1 below is a standard result that characterizes the space of all influence functions of RAL estimators of any pathwise differentiable estimand $\\tau \\in \\mathbb{R}^{d}$ in the model $\\mathcal{P}_{\\mathcal{M}}$ as an affine translation of a space $\\left(\\mathcal{T}_{\\mathcal{M}}^{d}\\right)^{\\perp}$, the orthogonal complement of the semiparametric tangent space $\\mathcal{T}_{\\mathcal{M}}^{d}$ in $\\left(\\mathcal{H}_{W}^{0}\\right)^{d}$. The semiparametric tangent space $\\mathcal{T}_{\\mathcal{M}}^{d}$ is defined as the closure (with respect to $\\mathcal{H}_{W}^{0}$ ) of the set of all $d$-dimensional linear combinations of score functions of all smooth finite-dimensional parametric submodels $P_{\\gamma} \\subseteq \\mathcal{P}_{\\mathcal{M}}$. A parametric submodel $P_{\\gamma}$ of $\\mathcal{P}_{\\mathcal{M}}$ refers to a subset of distributions in $\\mathcal{P}_{\\mathcal{M}}$ indexed smoothly by a finite-dimensional parameter $\\gamma$ in an open neighborhood of 0 , with $P_{0}=P^{*}$. See Newey (1990) for the technical details on smoothness requirements for parametric submodels. The score function of a parametric submodel $P_{\\gamma}$ is the derivative of the log likelihood with respect to $\\gamma$, evaluated at the truth $\\gamma=0$.\n\nTheorem 1 (Theorem 4.3, Tsiatis (2006)). Suppose a RAL estimator of a pathwise differentiable estimand $\\tau \\in \\mathbb{R}^{d}$ in the model $\\mathcal{P}_{\\mathcal{M}}$ exists with influence function $\\varphi_{0}=\\varphi_{0}\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$. Further assume the semiparametric tangent space $\\mathcal{T}_{\\mathcal{M}}^{d}$ for the model $\\mathcal{P}_{\\mathcal{M}}$ is a closed linear subspace of $\\left(\\mathcal{H}_{W}^{0}\\right)^{d}$. Then:\n\n1. The influence function $\\varphi=\\varphi\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$ of any RAL estimator of $\\tau$ must satisfy $\\varphi-\\varphi_{0} \\in$ $\\left(\\mathcal{T}_{\\mathcal{M}}^{d}\\right)^{\\perp}$, where $\\left(\\mathcal{T}_{\\mathcal{M}}^{d}\\right)^{\\perp}$ is the orthogonal complement of $\\mathcal{T}_{\\mathcal{M}}^{d}$ in $\\left(\\mathcal{H}_{W}^{0}\\right)^{d}$.\n2. The efficient influence function is uniquely given by\n\n$$\n\\varphi_{\\mathrm{eff}}=\\Pi\\left(\\varphi_{0} ; \\mathcal{T}_{\\mathcal{M}}^{d}\\right)=\\varphi_{0}-\\underset{g \\in\\left(\\mathcal{T}_{\\mathcal{M}}^{d}\\right)^{\\perp}}{\\arg \\min } \\mathbb{E}\\left[\\left\\|\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)-g(W)\\right\\|^{2}\\right]\n$$\n\nwhere $\\Pi(f ; \\mathcal{S})$ denotes the projection of $f$ onto the subspace $\\mathcal{S} \\subseteq\\left(\\mathcal{H}_{W}^{0}\\right)^{d}$.\nTheorem 1 does not make use of the fact that $\\mathcal{P}_{\\mathcal{M}}$ is a model that specifically restricts the outcome mean function $m$. Our contribution, in Theorem 2 below, is to specify the semiparametric\n\ntangent space for a wide range of sets $\\mathcal{M}$ in terms of another space we call the outcome mean function tangent space $\\mathcal{S}_{\\mathcal{M}}$. Note it suffices to consider $d=1$ hereafter, since $\\mathcal{T}_{\\mathcal{M}}^{d}$ is just the $d$-fold Cartesian product of $\\mathcal{T}_{\\mathcal{M}}^{1} \\equiv \\mathcal{T}_{\\mathcal{M}}$.\nDefinition 1. An outcome mean function parametric submodel for $\\mathcal{M} \\subseteq \\mathcal{H}_{R}$ is any collection of mean functions $\\{m(\\cdot ; \\gamma) \\mid \\gamma \\in \\mathcal{U}\\}$ contained in $\\mathcal{M}$ and indexed by $\\gamma$ in an open neighborhood $U$ of $0 \\in \\mathbb{R}^{s}$ such that $m(r ; 0)=m^{*}(r)$ and $\\partial m(r ; \\gamma) / \\partial \\gamma$ exists for $P^{*}$-almost every $r \\in \\mathcal{R}$ and all $\\gamma \\in U$ with $\\partial m(\\cdot ; \\gamma) / \\partial \\gamma \\mid \\gamma=0 \\in\\left(\\mathcal{H}_{R}\\right)^{s}$. The outcome mean function tangent space $\\mathcal{S}_{\\mathcal{M}}$ for $\\mathcal{M}$ is then the closure (in the Hilbert space $\\mathcal{H}_{R}$ ) of the function collection\n\n$$\n\\left.r \\mapsto c^{\\top} \\frac{\\partial m(r ; \\gamma)}{\\partial \\gamma}\\right|_{\\gamma=0}\n$$\n\nindexed by $c \\in \\mathbb{R}^{s}$, positive integers $s \\geqslant 1$, and $s$-dimensional parametric submodels $\\{m(\\cdot ; \\gamma)\\}$.\nTheorem 2. Suppose $\\left\\{W_{i}=\\left(R_{i}, Y_{i}\\right)\\right\\}_{i=1}^{N}$ are i.i.d. from some distribution $P^{*} \\in \\mathcal{P}_{\\mathcal{M}}$ with $Y_{i} \\in \\mathbb{R}$ where $\\mathcal{M}$ is such that the corresponding outcome mean function tangent space $\\mathcal{S}_{\\mathcal{M}}^{1} \\equiv \\mathcal{S}_{\\mathcal{M}}$ is a linear subspace of $\\mathcal{H}_{R}$. Then $\\mathcal{T}_{\\mathcal{M}}^{1}$, the orthogonal complement in $\\mathcal{H}_{W}^{0}$ of the semiparametric tangent space $\\mathcal{T}_{\\mathcal{M}}$ for the model $\\mathcal{P}_{\\mathcal{M}}$, satisfies\n\n$$\n\\mathcal{T}_{\\mathcal{M}}^{\\perp}=\\left\\{w \\mapsto h(r)\\left(y-m^{*}(r)\\right) \\mid h \\in \\mathcal{S}_{\\mathcal{M}}^{\\perp}\\right\\}\n$$\n\nwhere $\\mathcal{S}_{\\mathcal{M}}^{\\perp}$ is the orthogonal complement of $\\mathcal{S}_{\\mathcal{M}}$ in $\\mathcal{H}_{R}$.\nProof. See Appendix A.2.\nPutting together Theorem 2 with (7) suggests the following three-step outline for computing the EIF of an estimand $\\tau$ in the model $\\mathcal{P}_{\\mathcal{M}}$ :\n\n1. Find an influence function $\\varphi_{0}$ of a RAL estimator of $\\tau$ in $\\mathcal{P}_{\\mathcal{M}}$\n2. Compute the space $\\mathcal{S}_{\\mathcal{M}}$\n3. Compute the orthogonal complement $\\mathcal{S}_{\\mathcal{M}}^{\\perp}$, which specifies $\\mathcal{T}_{\\mathcal{M}}^{\\perp}$ per Theorem 2, enabling computation of the projection (7).\n\nWe now provide some discussion for each of these steps, using them to compute EIF's in the outcome-mediated selection bias setting (the model $\\mathcal{P}_{\\mathcal{M}_{5}}$ ). In Appendix B, we similarly use this framework to derive EIF's in the model $\\mathcal{P}_{\\mathcal{M}_{4}}$ with linear confounding bias (the final result is given in Section 4). We also re-derive known EIF's from the literature in the models $\\mathcal{P}_{\\mathcal{M}_{1}}, \\mathcal{P}_{\\mathcal{M}_{2}}$, and $\\mathcal{P}_{\\mathcal{M}_{3}}$ in Appendix B.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 2.1 Initial influence function \n\nA natural choice for the initial influence function $\\varphi_{0}$ in step 1 above, so long as the estimand $\\tau$ is identified in the nonparametric model $\\mathcal{P}_{\\mathcal{H}_{R}}$ that does not place any structural restrictions on the data generating process, is the canonical gradient for the estimand $\\tau$. The canoncial gradient is the only possible influence function of a RAL estimator in $\\mathcal{P}_{\\mathcal{H}_{R}}$ (hence also the EIF in $\\mathcal{P}_{\\mathcal{H}_{R}}$ ). Since $\\mathcal{P}_{\\mathcal{M}} \\subseteq \\mathcal{P}_{\\mathcal{H}_{R}}$ for any $\\mathcal{M}$, the canonical gradient is also a possible influence function of a RAL estimator in $\\mathcal{P}_{\\mathcal{M}}$. The restriction that $m \\in \\mathcal{M}$, however, generally restricts the semiparametric tangent space $\\mathcal{T}_{\\mathcal{M}}$, thereby making the orthogonal complement $\\mathcal{T}_{\\mathcal{M}}^{\\perp}$ nontrivial which means that more influence functions are available in the model $\\mathcal{P}_{\\mathcal{M}}$, among which the minimal variance is\n\nTable 1: Some notation used throughout the paper for functionals in the data fusion examples. We use an asterisk superscript to indicate the value of the functional under the true data-generating process $P^{*}$.\n\n![table_0](table_0)\n\nattained by (7). The canonical gradient is readily available in the literature for many common estimands like average treatment effects and densities. When it is not, it can often be derived using various techniques like \"point mass contamination\" and calculus rules (Kennedy, 2024).\n\nIn Proposition 1 we provide the canonical gradients for three estimands $\\tau_{\\text {rct }}, \\tau_{\\text {obs }}$, and $\\tau_{\\text {tgt }}$\n\n$$\n\\tau_{\\mathrm{rct}}=\\mathbb{E}[\\tau(x ; m) \\mid S=1] ; \\quad \\tau_{\\mathrm{obs}}=\\mathbb{E}[\\tau(x ; m) \\mid S=0] ; \\quad \\tau_{\\mathrm{tgt}}=\\mathbb{E}[\\tau(x ; m)]\n$$\n\nin the data fusion setting where $R=(S, Z, X)$. Recall that $\\tau(x ; m)=m_{11}(x)-m_{10}(x)$ is the conditional average treatment effect when treatment is randomized (conditional on $X$ ) in the experiment. In that case, $\\tau_{\\text {rct }}$ identifies the average treatment effect for the population of subjects from which the experimental dataset is sampled. Under an additional mean-exchangeability assumption on the potential outcomes, Dahabreh et al. $(2019,2020)$ show that $\\tau_{\\text {obs }}$ and $\\tau_{\\text {tgt }}$ identify the average treatment effect in the population defined by the observational dataset and the fused dataset, respectively, as long as the RCT selection probability $p(x)=\\operatorname{Pr}(S=1 \\mid X=x)$ is bounded away from 0 . We do not discuss specific identification assumptions further, as our focus is on improving precision via restrictions on the outcome mean function $m$ for already identified estimands.\n\nProposition 1. The efficient influence functions $\\varphi_{0, \\mathrm{rct}}, \\varphi_{0, \\mathrm{obs}}$, and $\\varphi_{0, \\mathrm{tgt}}$ for the estimands $\\tau_{\\mathrm{rct}}$, $\\tau_{\\text {obs }}$, and $\\tau_{\\text {tgt }}$ defined in (9) at the true observed data distribution $P^{*}$ in the nonparametric model $\\mathcal{P}_{\\mathcal{H}_{R}}$ are given by\n\n$$\n\\begin{aligned}\n\\varphi_{0, \\mathrm{rct}}\\left(w ; \\tau_{\\mathrm{rct}}^{*}, \\eta^{*}\\right) & =\\frac{s\\left(\\Delta\\left(z, x, y ; \\eta^{*}\\right)+m_{11}^{*}(x)-m_{10}^{*}(x)-\\tau_{\\mathrm{rct}}^{*}\\right)}{\\rho^{*}} \\\\\n\\varphi_{0, \\mathrm{obs}}\\left(w ; \\tau_{\\mathrm{obs}}^{*}, \\eta^{*}\\right) & =\\frac{s\\left(1-p^{*}(x)\\right) \\Delta\\left(z, x, y ; \\eta^{*}\\right)}{p^{*}(x)\\left(1-\\rho^{*}\\right)}+\\frac{(1-s)\\left(m_{11}^{*}(x)-m_{10}^{*}(x)-\\tau_{\\mathrm{obs}}^{*}\\right)}{1-\\rho^{*}} \\\\\n\\varphi_{0, \\mathrm{tgt}}\\left(w ; \\tau_{\\mathrm{tgt}}^{*}, \\eta^{*}\\right) & =\\frac{s}{p^{*}(x)} \\Delta\\left(z, x, y ; \\eta^{*}\\right)+m_{11}^{*}(x)-m_{10}^{*}(x)-\\tau_{\\mathrm{tgt}}^{*}\n\\end{aligned}\n$$\n\nwhere the components of $\\eta^{*}=\\eta^{*}(\\cdot)=\\left(m^{*}(1, \\cdot, \\cdot), e^{*}(\\cdot), p^{*}(\\cdot)\\right)$ are defined in Table 1 and $\\Delta\\left(z, x, y ; \\eta^{*}\\right)$ is the inverse propensity weighted difference\n\n$$\n\\Delta\\left(z, x, y ; \\eta^{*}\\right)=\\frac{z\\left(y-m_{11}^{*}(x)\\right)}{e^{*}(x)}-\\frac{(1-z)\\left(y-m_{10}^{*}(x)\\right)}{1-e^{*}(x)}\n$$\n\nProof. Equation (10) is from Section 2.2 of Li et al. (2023b). Equations (11) and (12) are from Theorem 2.3 of Li et al. (2023a).\n\nAs noted in the introduction, the AIPW estimator of Robins et al. (1994), among many other popular estimators for $\\tau_{\\text {rct }}$, attains the influence function $\\varphi_{0, \\text { rct }}$ without using any observational\n\ndata. Reweighted variants of AIPW, referred to as augmented inverse propensity of sampling weighting (AIPSW) estimators by Colnet et al. (2024), attain the influence functions $\\varphi_{0, \\text { obs }}$ and $\\varphi_{0, \\text { tgt }}$ for $\\tau_{\\text {obs }}$ and $\\tau_{\\text {tgt }}$, respectively, using the experimental data along with the covariates (but not the outcomes) in the observational data.", "tables": {"table_0": "| Notation | Definition | Description |\n| :--: | :--: | :--: |\n| $p(x)$ | $\\operatorname{Pr}(S=1 \\mid X=x)$ | RCT selection probability function |\n| $e(x)$ | $\\operatorname{Pr}(Z=1 \\mid S=1, X=x)$ | RCT propensity score |\n| $q(x)$ | $\\operatorname{Pr}(Z=1 \\mid S=0, X=x)$ | Observational propensity score |\n| $\\rho$ | $\\operatorname{Pr}(S=1)$ | RCT sample size proportion |\n| $V(s, z, x)$ or $V_{s z}(x)$ | $\\operatorname{Var}(Y \\mid S=s, Z=z, X=x)$ | Outcome variance function |"}, "images": {}}, {"section_id": 5, "text": "# 2.2 The outcome mean function tangent space \n\nIf the set $\\mathcal{M}$ is a closed linear space, the outcome mean function tangent space $\\mathcal{S}_{\\mathcal{M}}$ is simply $\\mathcal{M}$.\nProposition 2. If $\\mathcal{M}$ is a closed linear subspace of $\\mathcal{H}_{R}$, then $\\mathcal{S}_{\\mathcal{M}}=\\mathcal{M}$.\nProof. First suppose $f \\in \\mathcal{S}_{\\mathcal{M}}$. Then there exists a sequence of nonrandom vectors $\\left\\{c_{i}\\right\\}_{i=1}^{N}$ and outcome mean function parametric submodels $\\left\\{M_{\\gamma}^{(i)}\\right\\}_{i=1}^{N}$ such that the functions\n\n$$\nf_{i}=\\left.c_{i}^{\\top} \\frac{\\partial M_{\\gamma}^{(i)}}{\\partial \\gamma}\\right|_{\\gamma=0}\n$$\n\nconverge in mean square to $f$. But for each $i$, letting $s_{i}$ be the dimension of $c_{i}$ we have\n\n$$\nc^{\\top} \\frac{\\partial M_{\\gamma}^{(i)}(\\cdot)}{\\partial \\gamma}\\left\\lvert\\, \\begin{aligned}\n& \\quad=\\lim _{h \\rightarrow 0} \\sum_{k=1}^{s_{i}} c_{k} \\frac{M_{h e_{k}}(\\cdot)-m^{*}(\\cdot)}{h}\n\\end{aligned}\\right.\n$$\n\nis a limit of linear combinations of functions in $\\mathcal{M}$, hence still in $\\mathcal{M}$ due to $\\mathcal{M}$ being closed and linear. Above, $e_{k}$ denotes the $k$-th standard basis vector in $\\mathbb{R}^{s_{i}}$. We conclude $\\mathcal{S}_{\\mathcal{M}} \\subseteq \\mathcal{M}$.\n\nTo show the reverse inclusion, note that for any $f \\in \\mathcal{M}$ we can write $f$ as the derivative of the one-dimensional parametric submodel $m(\\cdot ; \\gamma)=m^{*}(\\cdot)+\\gamma f(\\cdot)$, which is indeed a submodel as it satisfies $m(\\cdot ; \\gamma) \\in \\mathcal{M}$ for all $\\gamma$ due to $\\mathcal{M}$ being a linear space.\n\nIt is straightforward to verify that the collections $\\mathcal{M}_{2}$ and $\\mathcal{M}_{4}$ of (1) and 3 , respectively, are in fact closed linear spaces, so that $\\mathcal{S}_{\\mathcal{M}_{i}}=\\mathcal{M}_{i}$ for $i=2,4$. A linear restricted moment model $\\mathcal{M}=\\left\\{r \\mapsto \\psi(r)^{\\top} \\beta \\mid \\beta \\in \\mathbb{R}^{q}\\right\\}$, a special case of $\\mathcal{M}_{1}$ in Example 1, is also a closed linear space.\n\nIn other cases, computing the space $\\mathcal{S}_{\\mathcal{M}}$ may require some more work. However, Theorem 2 ensures that computing $\\mathcal{S}_{\\mathcal{M}}$ (and its orthogonal complement) can replace the work of directly deriving the space $\\mathcal{T}_{\\mathcal{M}}$. Such a direct derivation generally requires reasoning about integral restrictions on log likelihoods in parametric submodels. The proof of Theorem 2 essentially proceeds by working out such reasoning for general $\\mathcal{M}$ so that the user only needs to reason about the space $\\mathcal{S}_{\\mathcal{M}}$, which requires only considering derivatives of outcome mean function submodels.\n\nTo illustrate this, we work through a novel derivation of the outcome mean function tangent space $\\mathcal{S}_{\\mathcal{M}_{5}}$, where $\\mathcal{M}_{5}$ is the outcome mean function collection (5) in the outcome-mediated selection bias setting of Example 5.\nExample 5 (Continued). Let $\\ell(x)=\\log (x)-\\log (1-x)$ be the logistic link function, so that (4) can be written as\n\n$$\n\\ell\\left(m_{11}(x)\\right)=\\ell\\left(m_{10}(x)\\right)+\\ell\\left(m_{01}(x)\\right)-\\ell\\left(m_{00}(x)\\right)\n$$\n\nEssentially, the restriction (4) allows $m_{10}, m_{01}$, and $m_{00}$ to freely vary but then $m_{11}$ is a deterministic function of $\\left(m_{10}, m_{01}, m_{00}\\right)$. Thus, a generic outcome mean function parametric submodel for $\\mathcal{M}_{5}$ takes the form\n\n$$\n\\begin{aligned}\nm_{s z}(x ; \\gamma) & =s z f_{11}(x ; \\gamma)+s(1-z) f_{10}\\left(x ; \\gamma_{10}\\right)+(1-s) z f_{01}\\left(x ; \\gamma_{01}\\right)+(1-s)(1-z) f_{00}\\left(x ; \\gamma_{00}\\right) \\\\\nf_{11}(x ; \\gamma) & =\\ell^{-1}\\left[\\ell\\left(f_{10}\\left(x ; \\gamma_{10}\\right)\\right)+\\ell\\left(f_{01}\\left(x ; \\gamma_{01}\\right)\\right)-\\ell\\left(f_{00}\\left(x ; \\gamma_{00}\\right)\\right)\\right]\n\\end{aligned}\n$$\n\nfor $\\gamma=\\left(\\gamma_{10}^{\\top}, \\gamma_{01}^{\\top}, \\gamma_{00}^{\\top}\\right)^{\\top} \\in \\mathbb{R}^{s}$. For any $c=\\left(c_{10}^{\\top}, c_{01}^{\\top}, c_{00}^{\\top}\\right)^{\\top} \\in \\mathbb{R}^{r}$ partitioned as $\\gamma$, we compute\n\n$$\n\\begin{aligned}\n\\left.c^{\\top} \\frac{\\partial m(r ; \\gamma)}{\\partial \\gamma}\\right|_{\\gamma=0}= & {\\left[s(1-z)+s z \\kappa^{*}(x) \\ell^{\\prime}\\left(m_{10}^{*}(x)\\right)\\right] c_{10}^{\\top} f_{10}^{\\prime}(x ; 0) } \\\\\n& +\\left[(1-s) z+s z \\kappa^{*}(x) \\ell^{\\prime}\\left(m_{01}^{*}(x)\\right)\\right] c_{01}^{\\top} f_{01}^{\\prime}(x ; 0) \\\\\n& +\\left[(1-s)(1-z)-s z \\kappa^{*}(x) \\ell^{\\prime}\\left(m_{00}^{*}(x)\\right)\\right] c_{00}^{\\top} f_{00}^{\\prime}(x ; 0)\n\\end{aligned}\n$$\n\nwhere $f_{j}^{\\prime}(x ; a) \\equiv \\partial f_{j}(x ; \\alpha) /\\left.\\partial \\alpha\\right|_{\\alpha=a}$ for $j \\in\\{10,01,00\\}$ and $\\kappa^{*}(x)=\\left(\\ell^{-1}\\right)^{\\prime}\\left[\\ell\\left(m_{11}^{*}(x)\\right)\\right]=\\left(\\ell^{\\prime}\\left(m_{11}^{*}(x)\\right)\\right)^{-1}$. We note the preceding display is of the form\n$s(1-z) g_{1}(x)+(1-s) z g_{2}(x)+(1-s)(1-z) g_{3}(x)+s z \\kappa^{*}(x)\\left[\\ell^{\\prime}\\left(m_{10}^{*}(x)\\right) g_{1}(x)+\\ell^{\\prime}\\left(m_{01}^{*}(x)\\right) g_{2}(x)-\\ell^{\\prime}\\left(m_{00}^{*}(x)\\right) g_{3}(x)\\right]$ for some $g_{1}, g_{2}, g_{3} \\in \\mathcal{H}_{X}$ and hypothesize that any function of this form lies in $\\mathcal{S}_{\\mathcal{M}_{5}}$.\n\nTo show this, we fix arbitrary $g_{1}, g_{2}, g_{3} \\in \\mathcal{H}_{X}$ and note there exists an outcome mean function submodel $m(r ; \\gamma)$ of the form in (14) and (15) with\n\n$$\n\\begin{aligned}\nf_{10}\\left(x ; \\gamma_{10}\\right) & =m_{10}^{*}(x)+\\gamma_{10} g_{1}(x) \\\\\nf_{01}\\left(x ; \\gamma_{01}\\right) & =m_{01}^{*}(x)+\\gamma_{01} g_{2}(x) \\\\\nf_{00}\\left(x ; \\gamma_{00}\\right) & =m_{00}^{*}(x)+\\gamma_{00} g_{3}(x)\n\\end{aligned}\n$$\n\nwhere $\\gamma=\\left(\\gamma_{10}, \\gamma_{01}, \\gamma_{00}\\right) \\in \\mathbb{R}^{3}$. Then\n\n$$\n(1,1,1)^{\\top} \\frac{\\partial m_{s z}(x ; \\gamma)}{\\partial \\gamma}\\left.\\right|_{\\gamma=0}=s(1-z) g_{1}(x)+(1-s) z g_{2}(x)+(1-s)(1-z) g_{3}(x)+s z \\kappa^{*}(x) \\iota^{*}(x)\n$$\n\nwhich shows that indeed,\n\n$$\n\\begin{aligned}\n\\mathcal{S}_{\\mathcal{M}_{5}}= & \\left\\{r \\mapsto s(1-z) g_{1}(x)+(1-s) z g_{2}(x)+(1-s)(1-z) g_{3}(x)\\right. \\\\\n& \\left.\\quad+s z \\kappa^{*}(x)\\left[\\ell^{\\prime}\\left(m_{10}^{*}(x)\\right) g_{1}(x)+\\ell^{\\prime}\\left(m_{01}^{*}(x)\\right) g_{2}(x)-\\ell^{\\prime}\\left(m_{00}^{*}(x)\\right) g_{3}(x)\\right] \\mid g_{1}, g_{2}, g_{3} \\in \\mathcal{H}_{X}\\right\\}\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 6, "text": "# 2.3 Orthogonal complements and projections \n\nThe final step in deriving the EIF is to compute the orthogonal complement $\\mathcal{S}_{\\mathcal{M}}^{\\perp}$ of the outcome mean function tangent space $\\mathcal{S}_{\\mathcal{M}}$ along with the projection (7). A useful way to compute $\\mathcal{S}_{\\mathcal{M}}^{\\perp}$ is to characterize it as the set of all functions in $\\mathcal{H}_{R}$ whose projection onto $\\mathcal{S}_{\\mathcal{M}}$ is zero. We compute such projections, as well as the final projection (7), by using the law of iterated expectations to write the projection objective as a function of $X$ solely, and then minimize the objective using calculus. We perform both these steps for the collection $\\mathcal{M}_{5}$ below.\nExample 5 (Continued). By definition, the orthogonal complement $\\mathcal{S}_{\\mathcal{M}_{5}}^{\\perp}$ of the set $\\mathcal{S}_{\\mathcal{M}_{5}}$ in (16) consists of all functions $h \\in \\mathcal{H}_{R}$ for which the projection $\\pi\\left(h ; \\mathcal{S}_{\\mathcal{M}_{5}}\\right)$ is zero. By (16), an arbitrary function in $\\mathcal{S}_{\\mathcal{M}_{5}}$ takes the form\n\n$$\n\\begin{aligned}\n\\iota\\left(x ; g_{1}, g_{2}, g_{3}\\right) & =s(1-z) g_{1}(x)+(1-s) z g_{2}(x)+(1-s)(1-z) g_{3}(x)+s z v^{*}\\left(x ; g_{1}, g_{2}, g_{3}\\right) \\\\\nv^{*}\\left(x ; g_{1}, g_{2}, g_{3}\\right) & =\\kappa^{*}(x)\\left[\\ell^{\\prime}\\left(m_{10}^{*}(x)\\right) g_{1}(x)+\\ell^{\\prime}\\left(m_{01}^{*}(x)\\right) g_{2}(x)-\\ell^{\\prime}\\left(m_{00}^{*}(x)\\right) g_{3}(x)\\right]\n\\end{aligned}\n$$\n\nfor some $\\left(g_{1}, g_{2}, g_{3}\\right) \\in \\mathcal{H}_{X}^{3}$. Then for any $h \\in \\mathcal{H}_{R}$, we have $\\pi\\left(h ; \\mathcal{S}_{\\mathcal{M}_{5}}\\right)(x)=\\iota\\left(x ; h_{1}, h_{2}, h_{3}\\right)$ where $\\left(h_{1}, h_{2}, h_{3}\\right)$ minimize the projection objective\n\n$$\n\\begin{aligned}\n\\mathcal{O}\\left(g_{1}, g_{2}, g_{3}\\right)= & \\mathbb{E}\\left[\\left(h(R)-\\iota\\left(X ; g_{1}, g_{2}, g_{3}\\right)\\right)^{2}\\right] \\\\\n= & \\mathbb{E}\\left[p^{*}(X) e^{*}(X)\\left(h_{11}(X)-v^{*}\\left(X ; g_{1}, g_{2}, g_{3}\\right)\\right)^{2}\\right]+\\mathbb{E}\\left[p^{*}(X)\\left(1-e^{*}(X)\\right)\\left(h_{10}(X)-g_{1}(X)\\right)^{2}\\right] \\\\\n& +\\mathbb{E}\\left[\\left(1-p^{*}(X)\\right) q^{*}(X)\\left(h_{01}(X)-g_{2}(X)\\right)^{2}\\right]+\\mathbb{E}\\left[\\left(1-p^{*}(X)\\right)\\left(1-q^{*}(X)\\right)\\left(h_{00}(X)-g_{3}(X)\\right)^{2}\\right]\n\\end{aligned}\n$$\n\nover $\\left(g_{1}, g_{2}, g_{3}\\right) \\in \\mathcal{H}_{X}^{3}$, where the second equality follows from writing\n\n$$\nh(R)=S Z h_{11}(X)+S(1-Z) h_{10}(X)+(1-S) Z h_{01}(X)+(1-S)(1-Z) h_{00}(X)\n$$\n\nand conditioning on $X$, and we have used the notation from Table 1. With the objective now solely an expectation of a function of $X$, we can compute the optimizers $\\left(h_{1}, h_{2}, h_{3}\\right)$ simply by minimizing the argument of the expectation pointwise in $\\left(g_{1}, g_{2}, g_{3}\\right)$. This yields the first-order conditions\n\n$$\n\\begin{aligned}\n& 0=-p^{*}(x) e^{*}(x)\\left(h_{11}(x)-v^{*}\\left(x ; h_{1}, h_{2}, h_{3}\\right)\\right) \\kappa^{*}(x) \\ell^{\\prime}\\left(m_{10}^{*}(x)\\right)-p^{*}(x)\\left(1-e^{*}(x)\\right)\\left(h_{10}(x)-g_{1}(x)\\right) \\\\\n& 0=-p^{*}(x) e^{*}(x)\\left(h_{11}(x)-v^{*}\\left(x ; h_{1}, h_{2}, h_{3}\\right)\\right) \\kappa^{*}(x) \\ell^{\\prime}\\left(m_{01}^{*}(x)\\right)-\\left(1-p^{*}(x)\\right) q^{*}(x)\\left(h_{01}(x)-g_{2}(x)\\right) \\\\\n& 0=p^{*}(x) e^{*}(x)\\left(h_{11}(x)-v^{*}\\left(x ; h_{1}, h_{2}, h_{3}\\right)\\right) \\kappa^{*}(x) \\ell^{\\prime}\\left(m_{00}^{*}(x)\\right)-\\left(1-p^{*}(x)\\right)\\left(1-q^{*}(x)\\right)\\left(h_{00}(x)-g_{3}(x)\\right)\n\\end{aligned}\n$$\n\nfor all $x \\in \\mathcal{X}$. We conclude $\\mathcal{S}_{\\mathcal{M}_{5}}^{+}$is the set of all $h \\in \\mathcal{H}_{R}$ for which $h_{1}=h_{2}=h_{3}=0$ solves this system of equations. This is precisely the functions $h$ satisfying\n\n$$\n\\begin{aligned}\n\\left(1-e^{*}(x)\\right) h_{10}(x) & =-e^{*}(x) h_{11}(x) \\frac{\\ell^{\\prime}\\left(m_{10}^{*}(x)\\right)}{\\ell^{\\prime}\\left(m_{11}^{*}(x)\\right)} \\\\\n\\left(1-p^{*}(x)\\right) q^{*}(x) h_{01}(x) & =-p^{*}(x) e^{*}(x) h_{11}(x) \\frac{\\ell^{\\prime}\\left(m_{01}^{*}(x)\\right)}{\\ell^{\\prime}\\left(m_{11}^{*}(x)\\right)} \\\\\n\\left(1-p^{*}(x)\\right)\\left(1-q^{*}(x)\\right) h_{00}(x) & =p^{*}(x) e^{*}(x) h_{11}(x) \\frac{\\ell^{\\prime}\\left(m_{00}^{*}(x)\\right)}{\\ell^{\\prime}\\left(m_{11}^{*}(x)\\right)}\n\\end{aligned}\n$$\n\nfor all $x \\in \\mathcal{X}$. Noting that (17), (18), and (19) determine $h_{10}, h_{01}$, and $h_{00}$ in terms of $h_{11}$ and letting $\\zeta(x)$ denote $h_{11}(x) / \\ell^{\\prime}\\left(m_{11}^{*}(x)\\right)$, we can write this succinctly as\n\n$$\n\\mathcal{S}_{\\mathcal{M}_{5}}^{+}=\\left\\{f^{*}(r) \\zeta(x) \\mid \\zeta \\in \\mathcal{H}_{X}\\right\\}\n$$\n\nfor $f^{*}(r) \\equiv f\\left(r ; \\eta^{*}\\right)$ where\n\n$$\n\\begin{aligned}\nf(r ; \\eta)= & s z \\ell^{\\prime}\\left(m_{11}(x)\\right)-\\frac{s(1-z) e(x) \\ell^{\\prime}\\left(m_{10}(x)\\right)}{1-e(x)} \\\\\n& -\\frac{(1-s) z p(x) e(x) \\ell^{\\prime}\\left(m_{01}(x)\\right)}{(1-p(x)) q(x)}+\\frac{(1-s)(1-z) p(x) e(x) \\ell^{\\prime}\\left(m_{00}(x)\\right)}{(1-p(x))(1-q(x))}\n\\end{aligned}\n$$\n\nand the nuisance parameters are $\\eta(\\cdot)=(p(\\cdot), e(\\cdot), q(\\cdot), m(\\cdot))$. We conclude via (8) that\n\n$$\n\\mathcal{T}_{\\mathcal{M}_{5}}^{+}=\\left\\{w \\mapsto f^{*}(r) \\zeta(x)\\left(y-m^{*}(r)\\right) \\mid \\zeta \\in \\mathcal{H}_{X}\\right\\}\n$$\n\nFinally, to derive the EIF we perform the projection (7). This amounts to solving the optimization problem\n\n$$\n\\min _{\\zeta \\in \\mathcal{H}_{X}} \\mathbb{E}\\left[\\left(\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)-f^{*}(R) \\zeta(X)\\left(Y-m^{*}(R)\\right)\\right)^{2}\\right]\n$$\n\nSimilar to our derivation of the orthogonal complement $\\mathcal{S}_{\\mathcal{M}_{5}}^{+}$, we perform this minimization by re-writing the objective via conditioning on $X$ :\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left(\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)-f^{*}(R) \\zeta(X)\\left(Y-m^{*}(R)\\right)\\right)^{2}\\right] \\\\\n& \\quad=\\mathbb{E}\\left[\\zeta(X)^{2} \\mathbb{E}\\left[f^{*}(R)^{2}(Y-m^{*}(R))^{2} \\mid X\\right]-2 \\zeta(X) \\mathbb{E}\\left[\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right) f^{*}(R)\\left(Y-m^{*}(R)\\right) \\mid X\\right]\\right]+\\text { const. }\n\\end{aligned}\n$$\n\nMinimizing the argument of the expectation shows the solution to (22) is $\\zeta\\left(\\cdot ; \\eta^{*}\\right) \\equiv \\zeta\\left(\\cdot ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)$ where\n\n$$\n\\zeta(x ; \\eta) \\equiv \\zeta\\left(x ; \\eta, \\tau, \\varphi_{0}\\right)=\\frac{\\int \\varphi_{0}(w ; \\tau, \\eta) f(r ; \\eta)(y-m(r)) d P_{(S, Z, Y \\mid X)}(s, z, y ; x)}{\\int f(r ; \\eta)^{2}(y-m(r))^{2} d P_{(S, Z, Y \\mid X)}(s, z, y ; x)}\n$$\n\nand so by (7) the EIF is\n\n$$\n\\varphi_{\\mathrm{eff}}^{(5)}\\left(w ; \\tau^{*}, \\eta^{*}\\right)=\\varphi_{0}\\left(w ; \\tau^{*}, \\eta^{*}\\right)-f\\left(r ; \\eta^{*}\\right) \\zeta\\left(x ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)\\left(y-m^{*}(r)\\right)\n$$", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 2.4 Additional insights \n\nCommonly, the propensity score $e^{*}(x)$ in the experimental dataset is known, as it is subject to the control of the investigator. Hahn (1998) famously showed that knowledge of the propensity score does not change the semiparametric efficiency bound for estimating $\\tau_{\\mathrm{rct}}$ relative to the nonparametric model. This may not be true in general, however, for other estimands or for proper semiparametric models like $\\mathcal{P}_{\\mathcal{M}}$.\n\nWe can readily extend Theorem 2 to characterize the space of influence functions of RAL estimators in the model $\\tilde{\\mathcal{P}}_{\\mathcal{M}} \\subseteq \\mathcal{P}_{\\mathcal{M}}$ that imposes the restriction $e(x)=e^{*}(x)$ for some known function $e^{*}(\\cdot)$ in addition to the outcome mean function restriction that $m \\in \\mathcal{M}$. The main insight is to note that with $e(x)=\\operatorname{Pr}(Z=1 \\mid S=1, X=x)=\\mathbb{E}[Z \\mid S=1, X=x]$, a known experimental propensity score can be viewed as a restriction on the outcome mean function of the observations $(Z, S, X)$, where now $Z$ is viewed as the \"outcome\" and $(S, X)$ are the covariates. Such a restriction is variationally independent of any outcome mean function restriction on the original dataset (i.e., that $m \\in \\mathcal{M}$ ), which must pertain to the conditional distribution of $Y$ given $(Z, S, X)$. This allows us to recursively apply Theorem 2. Similar ideas could be used to characterize semiparametric efficiency when either instead of or in addition to a known experimental propensity score, the RCT selection probability $p^{*}(x)$ is known, as in a nested trial setup where RCT participants are directly sampled from a larger group by the investigator. A known selection probability is a restriction on the conditional mean of $S$ given $X$, which is variationally independent of any restrictions on the conditional distribution of $Z$ given $(S, X)$ (e.g., known experimental propensity score), or on the conditional distribution of $Y$ given $R$ (e.g., that $m \\in \\mathcal{M}$ ).\n\nCorollary 1. Suppose the conditions of Theorem 2 are satisfied for some outcome mean function collection $\\mathcal{M}$ in the causal inference setting where $R=(S, Z, X) \\in\\{0,1\\}^{2} \\times \\mathcal{X}$. Then $\\tilde{\\mathcal{T}}_{\\mathcal{M}}^{\\perp}$, the orthogonal complement in $\\mathcal{H}_{W}^{0}$ of the semiparametric tangent space $\\tilde{\\mathcal{T}}_{\\mathcal{M}}$ for the model $\\tilde{\\mathcal{P}}_{\\mathcal{M}}$ that imposes the restrictions that $m \\in \\mathcal{M}$ and $\\operatorname{Pr}(Z=1 \\mid S=1, X=x)=e^{*}(x)$ for known $e^{*} \\in \\mathcal{H}_{X}$, satisfies\n\n$$\n\\tilde{\\mathcal{T}}_{\\mathcal{M}}^{\\perp}=\\mathcal{T}_{\\mathcal{M}}^{\\perp} \\oplus\\left\\{w \\mapsto s h(x)\\left(z-e^{*}(x)\\right) \\mid h \\in \\mathcal{H}_{X}\\right\\}\n$$\n\nProof. See Appendix A.3.\nTheorem 2 can also provide useful insight into when certain modeling assumptions may not be helpful, in the sense that imposing such restrictions cannot improve (decrease) the semiparametric efficiency bound.\n\nCorollary 2. Suppose the experimental propensity score is constant, i.e. $e^{*}(x)=e^{*} \\in(0,1)$ for all $x \\in \\mathcal{X}$, and the variance functions in the RCT are homoskedastic, i.e.\n\n$$\nV^{*}(1, z, x)=\\operatorname{Var}(Y \\mid S=1, Z=z, X=x)=\\sigma_{z}^{2}, \\quad z \\in\\{0,1\\}, x \\in \\mathcal{X}\n$$\n\nfor some positive constants $\\sigma_{0}^{2}$ and $\\sigma_{1}^{2}$. Additionally, suppose $\\mathcal{M} \\subseteq \\mathcal{H}_{R}$ satisfies the conditions of Theorem 2 and the corresponding outcome mean tangent space $\\mathcal{S}_{\\mathcal{M}}$ contains all functions $m$ for which $m_{11}$ and $m_{10}$ are arbitrary constants and $m_{01}$ and $m_{00}$ are identically zero. Then $\\varphi_{0, \\text { rct }}$, the EIF for $\\tau_{\\mathrm{rct}}$ in the nonparametric model given in (10), is also the EIF for $\\tau_{\\mathrm{rct}}$ in both $\\mathcal{P}_{\\mathcal{M}}$ and $\\tilde{\\mathcal{P}}_{\\mathcal{M}}$.\n\nProof. See Appendix A.4.\nTo understand the implications of Corollary 2, consider the following restrictive set of modeling assumptions: the outcome mean functions $m_{01}$ and $m_{00}$ in the observational dataset are known exactly, while the outcome mean functions $m_{11}$ and $m_{10}$ in the RCT are linear in some known\n\nlow-dimensional basis functions $\\psi(x)$ containing an intercept. It is straightforward to show that the set $\\mathcal{M}$ of permissible outcome mean functions under these restrictions satisfies the conditions of Corollary 2. This shows that with homoskedastic outcomes and a constant propensity score $e^{*}$, even very stringent assumptions on the outcome mean functions do not enable any asymptotic improvement over the AIPW estimator for estimating $\\tau_{\\text {rct }}$. The same automatically applies for any set of strictly less stringent assumptions, such as the model $\\mathcal{P}_{\\mathcal{M}_{4}}$ (again assuming $\\psi(x)$ contains an intercept). Recall that the AIPW estimator neither uses any information from the observational dataset nor places any parametric or semiparametric assumptions on the outcome mean functions. Thus, Corollary 2 shows it is not useful in large samples to make such assumptions to try and leverage the observational data if we have a constant RCT propensity score and expect homoskedastic outcomes. Conversely, with heteroskedastic outcomes it is known that even without an additional observational dataset, parametric assumptions on the outcome mean functions can improve the efficiency bound for $\\tau_{\\text {rct }}$ (Tan, 2007), including when the propensity score is constant.", "tables": {}, "images": {}}, {"section_id": 8, "text": "# 3 Constructing one-step estimators \n\nSuppose $\\varphi=\\varphi\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$ is an influence function for an RAL estimator of $\\tau$ in the model $\\mathcal{P}_{\\mathcal{M}}$. Then techniques like one-step adjustment of an initial estimator $\\hat{\\tau}$, solving the estimating equation $N^{-1} \\sum_{i=1}^{N} \\varphi\\left(W_{i} ; \\tau, \\hat{\\eta}\\right)=0$ for $\\tau$ (where $\\hat{\\eta}$ is an initial estimate of the nuisance parameters $\\eta^{*}$ ), and targeted minimum loss estimation (Hines et al., 2022) are all well-studied options to construct an estimator with influence function $\\varphi$.\n\nOur focus is on providing precise conditions under which one-step adjustment, which most directly utilizes the structure of Section 2, yields an estimator that in fact achieves the efficiency bound in our models $\\mathcal{P}_{\\mathcal{M}}$ (i.e., has influence function equal to the EIF). We emphasize that while most attention has been placed on one-step adjustment of non-RAL estimators in nonparametric models to gain $N^{1 / 2}$ consistency, here we propose one-step adjustment of RAL estimators in our proper semiparametric models $\\mathcal{P}_{\\mathcal{M}}$, where there is typically than one possible influence function.\n\nLet $\\varphi_{0}=\\varphi_{0}\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$ be the influence function of an initial RAL estimator $\\hat{\\tau}$ in the semiparametric model $\\mathcal{P}_{\\mathcal{M}}$, as in Section 2.1. Expanding the definition of the nuisance parameters $\\eta^{*}$ if necessary, let $\\varphi=\\varphi\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$ be the desired influence function of an RAL estimator of $\\tau$ (in our context, generally the EIF). By Theorem 1, we can write $\\varphi=\\varphi_{0}-g$ where $g=g\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right) \\in \\mathcal{T}_{\\mathcal{M}}^{\\perp}$. Since $\\tau^{*}$ and $\\eta^{*}$ are not known, we need to plug in initial estimates $\\hat{\\tau}$ and $\\hat{\\eta}$. Ensuring this initial plug-in procedure does not affect the first-order asymptotics of the one-step estimator $\\hat{\\tau}_{\\text {os }}=\\hat{\\tau}-N^{-1} \\sum_{i=1}^{N} g\\left(W_{i} ; \\hat{\\tau}, \\hat{\\eta}\\right)$ requires that\n\n$$\n\\frac{1}{N} \\sum_{i=1}^{N} g\\left(W_{i} ; \\hat{\\tau}, \\hat{\\eta}\\right)-g\\left(W_{i} ; \\tau^{*}, \\eta^{*}\\right)=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nOne challenge with satisfying (25) is that the nuisance parameters $\\eta^{*}$ are typically infinite dimensional, and thus need to be estimated nonparametrically to avoid additional modeling assumptions. A popular approach to weaken the conditions needed for (25) to hold with nonparametric nuisance estimates is cross-fitting. Cross-fitting entails partitioning the observation indices $1, \\ldots, N$ into $K$ roughly equally-sized folds $\\mathcal{I}_{1}, \\ldots, \\mathcal{I}_{K}$ for some fixed $K \\geqslant 2$. Then for each fold $k=1, \\ldots, K$, we find estimates $\\hat{\\tau}^{(-k)}$ and $\\hat{\\eta}^{(-k)}$ of $\\tau^{*}$ and $\\eta^{*}$, respectively, using only the observations whose indices lie outside $\\mathcal{I}_{k}$. As we will see, orthogonality properties of the influence function $\\varphi$ enable the following cross-fit variant of (25) to hold even when the components of $\\hat{\\eta}^{(-k)}$ converge at nonparametric\n\nrates:\n\n$$\n\\frac{1}{N} \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{I}_{k}} g\\left(W_{i} ; \\hat{\\tau}^{(-k)}, \\hat{\\eta}^{(-k)}\\right)-g\\left(W_{i} ; \\tau^{*}, \\eta^{*}\\right)=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nSuch a result, with sufficient conditions stated precisely in Lemma 1 below, closely follows the literature on double machine learning (Zheng and van der Laan, 2011; Chernozhukov et al., 2018).\n\nWe assume cross-fitting into $K \\geqslant 2$ folds for the remainder of this section, and adopt the notation introduced in the previous paragraph in our formal results. First, we formalize the construction of our cross-fit one-step estimator $\\hat{\\tau}_{\\text {os }}$.\n\nTheorem 3. Let $\\hat{\\tau}_{0}$ be any RAL estimator of pathwise differentiable $\\tau \\in \\mathbb{R}$ in some semiparametric model with influence function $\\varphi_{0}=\\varphi_{0}\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$. Suppose (26) holds for some $g=g\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right) \\in \\mathcal{H}^{0}$ and cross-fit estimates $\\hat{\\tau}^{(-k)}$ and $\\hat{\\eta}^{(-k)}$ of $\\tau^{*}$ and $\\eta^{*}$, respectively, for $k=1, \\ldots, K$. Then\n\n$$\n\\hat{\\tau}_{\\mathrm{os}}=\\hat{\\tau}_{0}-\\frac{1}{N} \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{I}_{k}} g\\left(W_{i} ; \\hat{\\tau}^{(-k)}, \\hat{\\eta}^{(-k)}\\right)\n$$\n\nsatisfies the central limit theorem $\\sqrt{N}\\left(\\hat{\\tau}_{\\text {os }}-\\tau^{*}\\right) \\xrightarrow{d} \\mathcal{N}\\left(0, \\mathbb{E}\\left[\\varphi(W)^{2}\\right]\\right)$ whenever the model holds, for $\\varphi(\\cdot)=\\varphi\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)=\\varphi_{0}\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)-g\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$.\nProof. By the definition of $\\hat{\\tau}_{\\text {os }}$ in (27) we have\n\n$$\n\\begin{aligned}\n\\sqrt{N}\\left(\\hat{\\tau}_{\\mathrm{os}}-\\tau^{*}\\right) & =\\sqrt{N}\\left(\\hat{\\tau}_{0}-\\tau^{*}\\right)-\\frac{1}{\\sqrt{N}} \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{I}_{k}} g\\left(W_{i} ; \\hat{\\tau}^{(-k)}, \\hat{\\eta}^{(-k)}\\right) \\\\\n& =\\frac{1}{\\sqrt{N}} \\sum_{i=1}^{N} \\varphi_{0}\\left(W_{i} ; \\tau^{*}, \\eta^{*}\\right)-g\\left(W_{i} ; \\tau^{*}, \\eta^{*}\\right)+o_{p}(1)\n\\end{aligned}\n$$\n\nwhere by (26) and the definition of $\\varphi_{0}$, the second equality holds whenever the model holds. The result follows by the ordinary central limit theorem.\n\nTheorem 3 does not use any properties about $g$ - not even that $\\varphi=\\varphi_{0}-g$ is a valid influence function of an RAL estimator of $\\tau$. It also does not consider solely models of the form $\\mathcal{P}_{\\mathcal{M}}$. However, the geometry of the space of influence functions of RAL estimators in the model $\\mathcal{P}_{\\mathcal{M}}$ given by Theorem 1 ensures that the condition (26) can be feasibly satisfied when $\\varphi$ is in fact a valid influence function of a RAL estimator (i.e., that $g \\in \\mathcal{T}_{\\mathcal{M}}^{\\perp}$ ). Of course, this is necessarily the case when $\\varphi=\\varphi_{\\text {eff }}$, i.e., we are targeting the EIF.\n\nLemma 1. Suppose the function $g=g\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$ is any element of the space $\\mathcal{T}_{\\mathcal{M}}^{\\perp}$ for some collection of outcome mean functions $\\mathcal{M} \\subseteq \\mathcal{H}_{R}$ satisfying the conditions of Theorem 2, so that we can write\n\n$$\ng\\left(w ; \\tau^{*}, \\eta^{*}\\right)=\\left(y-m^{*}(r)\\right) h_{1}\\left(r ; \\tau^{*}, \\eta^{*}\\right)\n$$\n\nwhere $\\eta^{*}$ includes the mean function $m^{*}$ and $h_{1}^{*}(\\cdot)=h_{1}\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right) \\in \\mathcal{S}_{\\mathcal{M}}^{\\perp}$. Then equation (26) holds under the following conditions:\n\n- (Boundedness) There exists $C<\\infty$ for which $V^{*}(r) \\leqslant C$ for $P^{*}$-almost every $r$ and both $\\left\\|h_{1}^{*}\\right\\|_{\\infty, P^{*}} \\leqslant C$ and $\\left\\|\\hat{h}_{1}^{(-k)}\\right\\|_{\\infty, P^{*}} \\leqslant C$ where $\\|f\\|_{\\infty, P^{*}}=\\inf _{M} \\operatorname{Pr}(|f(R)| \\leqslant M)=1$\n\n- (Approximate tangency) With probability tending to 1, for each $k=1, \\ldots, K$ there exists $\\bar{R}^{(-k)}=\\bar{R}^{(-k)}(\\cdot) \\in \\mathcal{H}_{R}$ depending only on the observations outside fold $k$ such that the function $r \\mapsto \\hat{m}^{(-k)}(r)-m^{*}(r)-\\bar{R}^{(-k)}(r)$ lies in the outcome mean function tangent space $\\mathcal{S}_{\\mathcal{M}},\\left\\|\\bar{R}^{(-k)}\\right\\|_{2, P^{*}}=o_{p}(1)$, and the following rate holds:\n\n$$\n\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left|\\bar{R}^{(-k)}\\left(R_{i}\\right)\\right|=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\n- (Rate conditions) The following rate conditions hold for all $k=1, \\ldots, K$ :\n- (First order consistency) $\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}}+\\left\\|\\bar{h}_{1}^{(-k)}-h_{1}^{*}\\right\\|_{2, P^{*}}=o_{p}(1)$\n- (Product rate condition) $\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}} \\times\\left\\|\\bar{h}_{1}^{(-k)}-h_{1}^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 2}\\right)$\nwhere $\\|f\\|_{2, P^{*}}=\\left(\\int f^{2}(w) d P^{*}(w)\\right)^{1 / 2}$.\nIn the proof of Lemma 1 in Appendix A.5, we generalize Lemma 1 to cover models $\\overline{\\mathcal{P}}_{\\mathcal{M}}$ where the experimental propensity score is known (cf. Corollary 1). The main conditions in Lemma 1 that need to be verified for a given choice of $\\mathcal{M}$ are approximate tangency and the rate conditions. One can check that approximate tangency holds with $\\bar{R}^{(-k)}=0$ if $m^{*} \\in \\mathcal{M}$ and $\\hat{m}^{(-k)} \\in \\mathcal{M}$ for all folds $k$ with probability tending to 1 , and $\\mathcal{M}$ is a closed linear space. Otherwise, approximate tangency can be established (with generally nonzero remainders $\\bar{R}^{(-k)}$ ) via Taylor expansion in appropriately smooth models such as $\\mathcal{P}_{\\mathcal{M}_{5}}$, provided that the outcome mean function estimates $\\hat{m}^{(-k)}$ lie in $\\mathcal{M}$. The rate conditions required in Lemma 1 are standard in the double machine learning literature. Substantial work has established precise conditions under which they hold for particular nonparametric estimators (Chernozhukov et al., 2018; Benkeser and van der Laan, 2016; Schuler et al., 2024).", "tables": {}, "images": {}}, {"section_id": 9, "text": "# 4 Some specific novel efficient estimators \n\nWe are now ready to put everything together to specify novel one-step efficient estimators of the average treatment effects $\\tau_{\\text {rct }}, \\tau_{\\text {obs }}$, and $\\tau_{\\text {tgt }}$ in the models $\\mathcal{P}_{\\mathcal{M}_{4}}$ and $\\mathcal{P}_{\\mathcal{M}_{5}}$ that assume linear confounding bias and outcome-mediated selection bias, respectively.\n\nIn Appendix B.4, we show that the EIF $\\varphi_{\\text {eff }}^{(4)}$ in the linear confounding bias model $\\mathcal{P}_{\\mathcal{M}_{4}}$ in terms of any initial influence function $\\varphi_{0}=\\varphi_{0}\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$ of an RAL estimator of $\\tau$ is given by\n\n$$\n\\varphi_{\\text {eff }}^{(4)}\\left(w ; \\tau^{*}, \\eta^{*}\\right)=\\varphi_{0}\\left(w ; \\tau^{*}, \\eta^{*}\\right)-h\\left(r ; \\eta^{*}\\right) \\nu\\left(x ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)\\left(y-m^{*}(r)\\right)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\nh(r ; \\eta) & =\\frac{s z}{p(x) e(x)}-\\frac{s(1-z)}{p(x)(1-e(x))}-\\frac{(1-s) z}{(1-p(x)) q(x)}+\\frac{(1-s)(1-z)}{(1-p(x))(1-q(x))} \\\\\nI\\left(x ; \\eta, \\tau, \\varphi_{0}\\right) & =\\int \\varphi_{0}(w ; \\tau, \\eta)(y-m(r)) h(r ; \\eta) d P_{(S, Z, Y) \\mid X}(s, z, y ; x) \\\\\n\\nu\\left(x ; \\eta, \\tau, \\varphi_{0}\\right) & =p(x) e(x) \\frac{I\\left(x ; \\eta, \\tau, \\varphi_{0}\\right)+\\lambda\\left(\\eta, \\tau, \\varphi_{0}\\right)^{\\top} \\psi(x)}{\\Sigma(x ; \\eta)} \\\\\n\\Sigma(x ; \\eta) & =V_{11}(x)+\\frac{V_{10}(x) e(x)}{1-e(x)}+\\frac{V_{01}(x) p(x) e(x)}{(1-p(x)) q(x)}+\\frac{V_{00}(x) p(x) e(x)}{(1-p(x))(1-q(x))} \\\\\n\\lambda\\left(\\eta, \\tau, \\varphi_{0}\\right) & =-\\left(\\int \\frac{p(x) e(x)}{\\Sigma(x ; \\eta)} \\psi(x) \\psi(x)^{\\top} d P_{X}(x)\\right)^{-1} \\int \\frac{I\\left(x ; \\eta, \\tau, \\varphi_{0}\\right) p(x) e(x)}{\\Sigma(x ; \\eta)} \\psi(x) d P_{X}(x)\n\\end{aligned}\n$$\n\nfor $\\eta=\\eta(P)$ including the nuisance functions $(p(\\cdot), e(\\cdot), m(\\cdot), q(\\cdot), V(\\cdot))$ along with any other functionals that are needed to compute\n\n$$\nI\\left(x ; \\eta^{*}, \\tau^{*}, \\nu_{0}\\right)=\\mathbb{E}\\left[\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)\\left(Y-m^{*}(R)\\right) h\\left(R ; \\eta^{*}\\right) \\mid X=x\\right]\n$$\n\nApplying the construction (27) to the differences\n\n$$\ng(\\cdot ; \\tau, \\eta)=\\varphi_{\\mathrm{eff}}^{(4)}(\\cdot ; \\tau, \\eta)-\\varphi_{0}(\\cdot ; \\tau, \\eta)=h(r ; \\eta) \\nu(x ; \\tau, \\eta)(y-m(r))\n$$\n\ngives the efficient cross-fit one-step estimators\n\n$$\n\\hat{\\tau}_{\\text {eff }}^{(4)}=\\hat{\\tau}_{0}-\\frac{1}{N} \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{I}_{k}} h\\left(R_{i} ; \\hat{\\eta}^{(-k)}\\right) \\nu\\left(X_{i} ; \\hat{\\eta}^{(-k)}, \\hat{\\tau}_{0}, \\varphi_{0}\\right)\\left(Y_{i}-\\hat{m}^{(-k)}\\left(R_{i}\\right)\\right)\n$$\n\nwhere $\\hat{\\tau}_{0}$ is any initial RAL estimator with influence function $\\varphi_{0}$. Taking $\\varphi_{0}=\\varphi_{0, \\mathrm{rct}}, \\varphi_{0, \\mathrm{obs}}, \\varphi_{0, \\mathrm{tgt}}$ gives efficient estimators for the estimands $\\tau_{\\mathrm{rct}}, \\tau_{\\mathrm{obs}}, \\tau_{\\mathrm{tgt}}$, respectively. In that case, $\\hat{\\tau}_{0}$ can be a cross-fit $\\operatorname{AIP}(\\mathrm{S}) \\mathrm{W}$ estimator obtained by solving the estimating equations\n\n$$\nN^{-1} \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{I}_{k}} \\varphi_{0}\\left(W_{i} ; \\tau ; \\hat{\\eta}^{(-k)}\\right)=0\n$$\n\nfor $\\varphi_{0}=\\varphi_{0, \\mathrm{rct}}, \\varphi_{0, \\mathrm{obs}}, \\varphi_{0, \\mathrm{tgt}}$. We remind the reader that these $\\operatorname{AIP}(\\mathrm{S}) \\mathrm{W}$ estimators are efficient in the nonparametric model. For convenience in computing (35) for the estimands $\\tau_{\\mathrm{rct}}, \\tau_{\\mathrm{obs}}, \\tau_{\\mathrm{tgt}}$, note\n\n$$\n\\begin{aligned}\n& I\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{rct}}\\right)=\\frac{1}{\\rho}\\left(\\frac{V_{11}(x)}{e(x)}+\\frac{V_{10}(x)}{1-e(x)}\\right) \\\\\n& I\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{obs}}\\right)=\\frac{\\rho(1-p(x))}{p(x)(1-\\rho)} I\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{rct}}\\right), \\quad I\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{tgt}}\\right)=\\frac{\\rho}{p(x)} I\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{rct}}\\right)\n\\end{aligned}\n$$\n\nSimilarly, by (23) we can compute efficient cross-fit one-step estimators\n\n$$\n\\hat{\\tau}_{\\mathrm{eff}}^{(5)}=\\hat{\\tau}_{0}-\\frac{1}{N} \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{I}_{k}} f\\left(R_{i} ; \\hat{\\eta}^{(-k)}\\right) \\zeta\\left(X_{i} ; \\hat{\\eta}^{(-k)}, \\hat{\\tau}_{0}, \\varphi_{0}\\right)\\left(Y_{i}-\\hat{m}^{(-k)}\\left(R_{i}\\right)\\right)\n$$\n\nin the outcome-mediated selection bias model $\\mathcal{P}_{\\mathcal{M}_{\\mathrm{S}}}$, where $\\eta(\\cdot)=(p(\\cdot), e(\\cdot), m(\\cdot), q(\\cdot))$. For convenience in computing estimators for $\\tau_{\\mathrm{rct}}, \\tau_{\\mathrm{obs}}$, and $\\tau_{\\mathrm{tgt}}$, we write\n\n$$\n\\begin{aligned}\n& \\zeta\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{rct}}\\right)=\\frac{\\ell^{\\prime}\\left(m_{11}(x)\\right) V_{11}(x)+\\ell^{\\prime}\\left(m_{10}(x)\\right) \\frac{e(x)}{1-e(x)} V_{10}(x)}{\\rho e(x) D(x ; \\eta)} \\\\\n& \\zeta\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{obs}}\\right)=\\frac{\\rho(1-p(x))}{(1-\\rho) p(x)} \\zeta\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{rct}}\\right), \\quad \\zeta\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{tgt}}\\right)=\\frac{\\rho}{p(x)} \\zeta\\left(x ; \\eta, \\tau, \\varphi_{0, \\mathrm{rct}}\\right)\n\\end{aligned}\n$$\n\nwhere $V_{s z}(x)=m_{s z}(x)\\left(1-m_{s z}(x)\\right)$ for $(s, z) \\in\\{0,1\\}^{2}$ since $Y \\in\\{0,1\\}$ and\n\n$$\n\\begin{aligned}\nD(x ; \\eta)= & V_{11}(x)\\left(\\ell^{\\prime}\\left(m_{11}(x)\\right)\\right)^{2}+\\frac{e(x)}{1-e(x)} V_{10}(x)\\left(\\ell^{\\prime}\\left(m_{10}(x)\\right)\\right)^{2} \\\\\n& +\\frac{p(x) e(x)}{(1-p(x)) q(x)}\\left(\\ell^{\\prime}\\left(m_{01}(x)\\right)\\right)^{2} V_{01}(x)+\\left(\\ell^{\\prime}\\left(m_{00}(x)\\right)^{2} V_{00}(x) \\frac{p(x) e(x)}{(1-p(x))(1-q(x))}\\right.\n\\end{aligned}\n$$\n\nOf course, the efficiency of $\\hat{\\tau}_{\\text {eff }}^{(4)}$ and $\\hat{\\tau}_{\\text {eff }}^{(5)}$ requires that (26) holds. In Appendix C we provide specific primitive conditions that are sufficient for the conditions of Lemma 1 (and thus (26)) to hold. The main non-regularity requirement in these conditions is that the cross-fit outcome mean function estimates $\\hat{m}^{(-k)}$ lie in $\\mathcal{M}$.\n\nWe also argue in Appendix B. 5 that for the estimands $\\tau_{\\text {rct }}, \\tau_{\\text {obs }}$, and $\\tau_{\\text {tgt }}$, any efficient estimators in the model $\\mathcal{P}_{\\mathcal{M}}$ (for any $\\mathcal{M}$ satisfying the conditions of Theorem 2) is also efficient in the model $\\hat{\\mathcal{P}}_{\\mathcal{M}}$. In other words, knowledge of the experimental propensity score does not enable asymptotic variance reductions for $\\tau_{\\text {rct }}, \\tau_{\\text {obs }}$, and $\\tau_{\\text {tgt }}$ under any outcome mean function restrictions.", "tables": {}, "images": {}}, {"section_id": 10, "text": "# 5 Simulations \n\nWe study the finite-sample performance of our efficient estimators $\\hat{\\tau}_{\\text {eff }}^{(5)}$ from (37) for the estimands $\\tau_{\\text {rct }}, \\tau_{\\text {obs }}$, and $\\tau_{\\text {tgt }}$ under outcome-mediated selection bias in two numerical simulation scenarios. All code and data to reproduce these results and those of the next section can be found at https://github.com/hli90722/combining_experimental_observational. We compare performance with both baseline $\\operatorname{AIP}(\\mathrm{S}) \\mathrm{W}$ estimators and the control variate estimators proposed by Guo et al. (2022). Asymptotically, we know our efficient estimators must dominate the control variate estimators, which in turn dominate the $\\operatorname{AIP}(\\mathrm{S}) \\mathrm{W}$ estimators as shown in Guo et al. (2022).\n\nThe first simulation scenario is identical to that considered in Section 6 of Guo et al. (2022). We call it the \"discrete\" scenario, as it has two covariates $X_{1}$ and $X_{2}$ which are i.i.d. Bern(0.5), with treatment and binary outcomes generated according to logistic models. The second simulation setting is a \"continuous\" scenario where $X_{1}$ and $X_{2}$ are independent uniformly distributed random variables on $[-1,1]$, and there are interaction terms in the logistic models generating treatment and outcomes. See Appendix D for the exact data generating processes. In both scenarios, some of the observations are subject to strong outcome-mediated selection where cases $(Y=1)$ are included with probability 0.9 and controls $(Y=0)$ are included with probability 0.1 . These observations form a synthetic biased observational dataset of size $m=3000$. The remaining observations are not subject to selection and have a size $n_{\\text {rct }} \\in\\{300,600,1000,3000\\}$. This selection process is identical to that considered in Guo et al. (2022).\n\nIn both scenarios, the RCT propensity score $e$ is considered known; this does not change the efficiency bound, as stated at the end of the previous section. In the discrete scenario, the nuisance functions $m, p$, and $q$ are estimated by the appropriate sample averages with 1 and 2 added to the numerator and denominator, respectively. In the continuous scenario, we use multivariate adaptive regression splines (MARS; Friedman (1991)) in $X$ with logit link and final terms chosen via generalized cross validation as implemented by the earth package in the R language (Milborrow, 2011) to fit the regressions defining the nuisance functions $m_{10}, m_{01}, m_{00}, p$, and $q$. All MARS regressions are cross-fit with $K=5$ folds. For stability, we truncate each MARS prediction to the interval $[1 / \\sqrt{n}, 1-1 / \\sqrt{n}]$ for $n$ the number of observations used to fit the corresponding model. As a reminder, to ensure the efficiency of our one-step estimators, we must enforce (4). Hence we set\n\n$$\n\\hat{m}_{11}^{(-k)}(x)=\\ell^{-1}\\left(\\ell\\left(\\hat{m}_{10}^{(-k)}(x)\\right)+\\ell\\left(\\hat{m}_{01}^{(-k)}(x)\\right)-\\ell\\left(\\hat{m}_{00}^{(-k)}(x)\\right)\\right), \\quad k=1, \\ldots, K\n$$\n\nWe compute our baseline AIPSW estimators $\\hat{\\tau}_{\\text {ba }}$ by solving (36). Our efficient one-step estimators $\\hat{\\tau}_{\\text {eff }}^{(5)}$ are computed using (37) with $\\hat{\\tau}_{\\text {ba }}$ as the initial estimator $\\hat{\\tau}_{0}$. We also use $\\hat{\\tau}_{\\text {ba }}$ as the baseline estimator to construct the control variate estimators $\\hat{\\tau}_{\\mathrm{cv}}$ of Guo et al. (2022), and follow that paper's guidance for choosing control variate(s) and computing the adjustment term (see Appendix E for more details). Their guidance is not based on asymptotic variance considerations, and we note\n\nTable 2: The estimated relative efficiencies of the feasible (resp. oracle) efficient one step estimators $\\hat{\\tau}_{\\text {eff }}^{(5)}$ and the control variate estimators $\\hat{\\tau}_{\\mathrm{cv}}$ compared with the baseline $\\operatorname{AIP}(\\mathrm{S}) \\mathrm{W}$ estimators $\\hat{\\tau}_{\\mathrm{ba}}$ in the discrete simulation scenario as a function of the simulated RCT size $n_{\\mathrm{rct}}$. The parentheses give $95 \\%$ bootstrap confidence intervals for the relative efficiency, reflecting the uncertainty from 1,000 Monte Carlo simulations.\n\n![table_1](table_1)\n\nTable 3: Same as Table 2 but for the continuous simulation scenario.\n\n![table_2](table_2)\n\nthat using an efficient estimator eliminates the need to choose specific control variates. Finally, to evaluate the finite sample impacts of estimating the nuisance functions on estimator performance, we also consider oracle variants of all estimators that plug in the true nuisance functions $\\eta^{*}$.\n\nFrom Tables 2 and 3, we see the oracle one step estimator $\\hat{\\tau}_{\\text {eff }}^{(5)}$ has higher relative efficiency (i.e. lower MSE) than the corresponding oracle control variate estimator $\\hat{\\tau}_{\\text {cv }}$ for all three estimands, all RCT sizes, and both simulation scenarios (discrete and continuous). Relative efficiency for $\\hat{\\tau}_{\\text {cv }}$ and $\\hat{\\tau}_{\\text {eff }}^{(5)}$ is defined as the MSE of the baseline $\\hat{\\tau}_{\\text {ba }}$ divided by the MSE of the estimator. The computation is done separately for oracle variants of the estimators and feasible ones (i.e., those that estimate the nuisance functions as described above). We find that estimating the nuisance functions leads to a similar degree of performance deterioration between the oracle and feasible versions for all estimators in the discrete simulations. In the continuous scenario where the nuisance estimates are\n\nharder to compute, the oracle and feasible relative efficiencies of $\\hat{\\tau}_{\\mathrm{cv}}$ are quite similar. This suggests that the control variate estimator $\\hat{\\tau}_{\\mathrm{cv}}$ suffers a similar amount of performance degradation from nuisance estimation as the baseline estimator $\\hat{\\tau}_{\\text {ba }}$. By contrast, the one-step efficient estimator $\\hat{\\tau}_{\\text {eff }}^{(5)}$ exhibits a notable relative efficiency decline between the oracle and feasible variants, particularly with smaller sample sizes. This suggests that the one-step efficient estimator $\\hat{\\tau}_{\\text {eff }}^{(5)}$ is more sensitive to poor nuisance estimation in finite samples than the baseline and control variate estimators. Nevertheless, the feasible efficient estimator $\\hat{\\tau}_{\\text {eff }}$ has much lower MSE ( $10-40 \\%$ reduction) than the feasible control variate estimator $\\hat{\\tau}_{\\mathrm{cv}}$ across all estimands, RCT sizes, and scenarios. This suggests the asymptotic variance gap between the control variate and efficient estimators is quite large.", "tables": {"table_1": "| Estimand | $n_{\\text {rct }}$ | Feasible $\\hat{\\tau}_{\\text {cv }}$ | Feasible $\\hat{\\tau}_{\\text {eff }}^{(5)}$ | Oracle $\\hat{\\tau}_{\\text {cv }}$ | Oracle $\\hat{\\tau}_{\\text {eff }}^{(5)}$ |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| $\\tau_{\\text {rct }}$ | 300 | 2.37 (2.18, 2.54) | 3.23 (2.86, 3.56) | 2.19 (2.04, 2.33) | 3.50 (3.11, 3.85) |\n|  | 600 | 1.94 (1.77, 2.10) | 2.29 (2.05, 2.51) | 1.94 (1.76, 2.09) | 2.39 (2.15, 2.61) |\n|  | 1000 | 1.62 (1.50, 1.74) | 1.79 (1.64, 1.93) | 1.64 (1.51, 1.75) | 1.82 (1.67, 1.96) |\n|  | 3000 | 1.05 (0.96, 1.13) | 1.26 (1.18, 1.33) | 1.05 (0.96, 1.12) | 1.26 (1.19, 1.34) |\n| $\\tau_{\\text {obs }}$ | 300 | 2.73 (2.49, 2.95) | 3.89 (3.40, 4.30) | 2.48 (2.29, 2.66) | 4.34 (3.83, 4.80) |\n|  | 600 | 2.16 (1.96, 2.33) | 2.57 (2.29, 2.83) | 2.15 (1.95, 2.33) | 2.67 (2.39, 2.93) |\n|  | 1000 | 1.78 (1.64, 1.91) | 1.96 (1.79, 2.11) | 1.77 (1.64, 1.89) | 2.00 (1.83, 2.15) |\n|  | 3000 | 1.09 (1.02, 1.16) | 1.25 (1.17, 1.33) | 1.08 (1.00, 1.14) | 1.25 (1.18, 1.32) |\n| $\\tau_{\\text {tgt }}$ | 300 | 2.74 (2.50, 2.95) | 3.91 (3.42, 4.32) | 2.48 (2.29, 2.65) | 4.37 (3.85, 4.82) |\n|  | 600 | 2.16 (1.96, 2.34) | 2.58 (2.30, 2.84) | 2.15 (1.95, 2.33) | 2.69 (2.40, 2.95) |\n|  | 1000 | 1.77 (1.63, 1.90) | 1.95 (1.78, 2.11) | 1.76 (1.63, 1.89) | 1.99 (1.82, 2.14) |\n|  | 3000 | 1.08 (1.00, 1.15) | 1.27 (1.19, 1.35) | 1.07 (0.98, 1.14) | 1.27 (1.20, 1.35) |", "table_2": "| Estimand | $n_{\\text {rct }}$ | Feasible $\\hat{\\tau}_{\\text {cv }}$ | Feasible $\\hat{\\tau}_{\\text {eff }}^{(5)}$ | Oracle $\\hat{\\tau}_{\\text {cv }}$ | Oracle $\\hat{\\tau}_{\\text {eff }}^{(5)}$ |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| $\\tau_{\\text {rct }}$ | 300 | 1.49 (1.42, 1.55) | 2.47 (2.19, 2.74) | 1.50 (1.42, 1.57) | 3.28 (2.95, 3.58) |\n|  | 600 | 1.44 (1.38, 1.50) | 1.99 (1.77, 2.18) | 1.46 (1.40, 1.53) | 2.34 (2.12, 2.54) |\n|  | 1000 | 1.45 (1.38, 1.52) | 1.82 (1.63, 1.98) | 1.44 (1.37, 1.50) | 1.91 (1.75, 2.07) |\n|  | 3000 | 1.12 (1.10, 1.15) | 1.27 (1.18, 1.35) | 1.12 (1.10, 1.15) | 1.28 (1.20, 1.35) |\n| $\\tau_{\\text {obs }}$ | 300 | 1.50 (1.42, 1.57) | 2.61 (2.28, 2.90) | 1.58 (1.50, 1.66) | 4.81 (4.25, 5.29) |\n|  | 600 | 1.46 (1.38, 1.53) | 2.10 (1.85, 2.32) | 1.57 (1.48, 1.64) | 2.73 (2.46, 2.99) |\n|  | 1000 | 1.45 (1.39, 1.52) | 1.96 (1.74, 2.15) | 1.50 (1.43, 1.57)) | 2.15 (1.95, 2.33) |\n|  | 3000 | 1.10 (1.07, 1.12) | 1.24 (1.13, 1.33) | 1.10 (1.07, 1.12) | 1.28 (1.20, 1.35) |\n| $\\tau_{\\text {tgt }}$ | 300 | 1.51 (1.43, 1.58) | 2.66 (2.32, 2.96) | 1.58 (1.50, 1.66) | 4.81 (4.26, 5.30) |\n|  | 600 | 1.48 (1.41, 1.55) | 2.12 (1.88, 2.35) | 1.57 (1.49, 1.65) | 2.76 (2.48, 3.02) |\n|  | 1000 | 1.49 (1.42, 1.56) | 1.99 (1.77, 2.18) | 1.51 (1.44, 1.58) | 2.17 (1.97, 2.36) |\n|  | 3000 | 1.12 (1.10, 1.15) | 1.26 (1.16, 1.36) | 1.12 (1.09, 1.14) | 1.30 (1.22, 1.38) |"}, "images": {}}, {"section_id": 11, "text": "# 6 Data example \n\nWe now consider a real data example based on estimating $\\tau_{\\text {obs }}$ in data from the Tennessee Student Teacher Achievement Ratio (STAR) study. This is inspired by Kallus et al. (2018), who use the same dataset and the linear confounding bias assumption (3) to study treatment effect hetereogeneity. The Tennessee STAR study was a large RCT designed to investigate whether small class sizes improve test outcomes. For simplicity, we only consider three binary covariates: gender, race (white or non-white), and whether the student received free lunch in grade 1 . We also include birth date as a continuous covariate.\n\nFollowing Kallus et al. (2018), the outcome is the sum of listening, reading, and math scores (divided by 100) at the end of first grade. We randomly subsample 1405 students from rural or inner-city schools to be our RCT dataset, and generate an observational dataset (with different covariate distribution) by combining the observations from the remaining 1406 students from rural or inner-city schools with 1407 students from urban or suburban schools. We randomly set aside $30 \\%$ (1125) of the observations in the observational dataset as a \"validation set\" to represent the target population. In the remaining 1688 observations, we synthetically introduce confounding by reducing all outcomes above the median by the difference between the average outcome above the median and the average outcome below the median. The nuisance functions $m_{10}, m_{01}, m_{00}, p$, and $q$ are computed using MARS and cross-fitting $(K=5)$ in the same way as in the simulations of Section 5, except that for the outcome mean function estimates, we now use the identity link and do not truncate. The three binary predictors are encoded as 0 or 1 . We enforce (3) by setting\n\n$$\n\\hat{m}_{11}^{(-k)}(x)=\\hat{m}_{10}^{(-k)}(x)+\\hat{m}_{01}^{(-k)}(x)-\\hat{m}_{00}^{(-k)}(x)+\\psi(x)^{\\top} \\hat{\\theta}, \\quad k=1, \\ldots, K\n$$\n\nwhere $\\psi(x)$ consists of an intercept term and each of the covariates (additively) and\n\n$$\n\\hat{\\theta}=\\underset{\\theta}{\\arg \\min } \\frac{1}{N} \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{I}_{k}} S_{i}\\left(Y_{i}\\left[\\frac{Z_{i}}{e^{*}}-\\frac{1-Z_{i}}{1-e^{*}}\\right]-\\hat{m}_{01}^{(-k)}\\left(X_{i}\\right)-\\hat{m}_{00}^{(-k)}\\left(X_{i}\\right)-\\psi\\left(X_{i}\\right)^{\\top} \\theta\\right)^{2}\n$$\n\nfollowing Kallus et al. (2018), where $e^{*}$ is the known constant RCT propensity score. The astute reader might notice that $\\hat{\\theta}$ is not cross-fit so this makes $\\hat{m}_{11}^{(-k)}$ not cross-fit either. This does not violate the theoretical guarantees of efficiency of our estimators, however, since $\\theta \\mapsto \\psi(x)^{\\top} \\theta$ is a Donsker class. The variance functions $V_{11}, V_{10}, V_{01}, V_{00}$ are computed using a generalized linear model with logarithmic link predicting the squared residuals $\\left(Y_{i}-\\hat{m}^{(-k(i))}\\left(R_{i}\\right)\\right)^{2}$ from the covariates. This is similar to Yang et al. (2024). Finally, the estimators $\\hat{\\tau}_{\\text {eff }}^{(4)}$ and the baseline AIPSW estimator $\\hat{\\tau}_{\\text {ba }}$ are computed on a set of \"training observations\" consisting of a randomly sampled fraction $\\xi \\in\\{0.2,0.4,0.6,0.8,1\\}$ of our RCT dataset along with the entire confounded\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: The mean squared error (MSE), variance, and squared bias of our efficient one-step estimator $\\hat{\\tau}_{\\text {eff }}^{(4)}$ for $\\tau_{\\text {obs }}$ and the baseline AIPSW estimator $\\hat{\\tau}_{\\text {ba }}$ in the Tennessee STAR data example described in Section 6, as a function of the fraction $\\xi$ of the entire RCT used.\nobservational dataset of 1688 subjects. The one-step estimator $\\hat{\\tau}_{\\text {eff }}^{(4)}$ is derived assuming (3) holds with $\\hat{\\tau}_{\\text {ba }}$ as the initial estimator $\\hat{\\tau}_{0}$, with influence function $\\varphi_{0}$.\n\nTaking an AIPSW estimate of $\\tau_{\\text {obs }}$ computed via (36) using the validation set to be the true estimand $\\tau_{\\text {obs }}$ for evaluation purposes, we observe substantial precision gains from using the efficient one-step estimator $\\hat{\\tau}_{\\text {eff }}^{(4)}$ instead of the baseline AIPSW estimator $\\hat{\\tau}_{\\text {ba }}$ for all RCT sample sizes $\\xi$. The relative variance improvement from using the efficient one-step estimator appears to generally increase with $\\xi$. When the full RCT dataset is used $(\\xi=1)$, we observe a $75 \\%$ variance reduction in 1000 bootstrap replicates, though there is substantial uncertainty in that estimate (Table 4). Since this is a real data example, we do not know (3) to hold, yet assuming (3) (as $\\hat{\\tau}_{\\text {eff }}^{(4)}$ does) appears to introduce a negligible amount of bias into $\\hat{\\tau}_{\\text {eff }}^{(4)}$ (Fig. 1). In other words, the MSE reductions in Table 4 from using $\\hat{\\tau}_{\\text {eff }}$ can be attributed almost entirely to variance reduction, and are minimally offset by bias.", "tables": {}, "images": {"img-0.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 12, "text": "# 7 Discussion \n\nWe have provided a general semiparametric theory for supervised learning under generic restrictions on the outcome mean function. Many assumptions used for data fusion in the recent causal inference literature are covered by this theory, which we hope provides a valuable unifying perspective. Sometimes these assumptions have not been fully leveraged for maximal variance reduction, as\n\nTable 4: The relative efficiency of $\\hat{\\tau}_{\\text {eff }}$ compared to $\\hat{\\tau}_{\\text {ba }}$ for estimating $\\tau_{\\text {obs }}$ in the Tennessee STAR data example of Section 6, as a function of the fraction $\\xi$ of the RCT data used. Confidence intervals for the relative efficiency are computed using 1000 bootstrap replications.\n\n![table_3](table_3)\n\nillustrated by the inefficiency of control variate approaches in the outcome-mediated selection bias setting. This paper has sought to present a streamlined way to derive fully efficient estimators under a large class of restrictions on outcome mean functions, while also providing additional mathematical insights into the nature of efficiency bounds under such restrictions.\n\nOur setting does not handle all types of data fusion restrictions that have been of interest. For instance, some authors have imposed structure on quantile treatment effects (Athey et al., 2023) or shape restrictions on the density function of a continuous outcome (Li and Luedtke, 2023), which cannot be formulated as restrictions on the outcome mean function. Other recent work (Li and Luedtke, 2023; Li et al., 2025) has also aimed to provide general data fusion frameworks guided by semiparametric efficiency. These frameworks, however, do not cover many settings of interest that can be cast as restrictions on outcome mean functions. For instance, consider our data fusion setting with $R=(S, Z, X)$ and suppose $Y \\in\\{0,1\\}$, so that the outcome mean function in fact specifies the entire conditional distribution of $Y$ given $(S, Z, X)$. Then for the observational dataset to be used at all in estimating any parameter depending on the conditional distribution of $Y \\mid Z, X, S=1$ (i.e. the experimental outcome distribution), Li and Luedtke (2023) require an \"alignment\" assumption that amounts to $m(s, z, x)$ being independent of $s$. The \"weak alignment\" assumption of Li et al. (2025) weakens this to allow\n\n$$\n\\ell(m(1, z, x))=\\ell(m(0, z, x))+\\beta^{\\top} \\psi(z, x)\n$$\n\nfor some known low-dimensional basis expansion $\\psi$. If $x$ is continuous, however, this is not flexible enough to cover the outcome-mediated selection bias setting of Example 5, which is equivalent to $\\ell(m(1, z, x))-\\ell(m(0, z, x))=f(x), z=0,1$ for arbitrary $f \\in \\mathcal{H}_{X}$. Conversely, if $Y$ is continuous, the weak alignment assumption induces restrictions on higher moments of the conditional distribution of $Y$ given $R$ which cannot be expressed solely as restrictions on the outcome mean function.\n\nAsymptotic efficiency also has some limitations as an objective for precise inference in finite samples. Based on the simulations in Section 5, the one-step efficient estimators proposed in this work can have worse relative finite sample performance than asymptotics might suggest, which we attribute to the need to estimate additional nuisance functions relative to the baselines. Thus, we believe extensions of ideas from the nonparametric literature to improve nuisance estimation such as pooling the datasets (Gagnon-Bartsch et al., 2023; Huang et al., 2023; Liao et al., 2023) and Riesz regression (Klosin, 2021; Chernozhukov et al., 2022; Lee and Schuler, 2025) may be useful to extend to models with outcome mean function restrictions. Substantial recent work has also exhibited finite sample benefits to non-regular estimators combining experimental and observational datasets (Chen et al., 2021; Cheng and Cai, 2021; Yang et al., 2023; Dang et al., 2023; Lin et al., 2025; Rosenman et al., 2023). While such estimators are, in some sense, more robust in that they don't require assumptions on outcome mean functions, Oberst et al. (2023)\n\nshowed that many of these estimators will end up performing worse than the efficiency bound in the nonparametric model when the bias is larger than some constant multiple of $N^{-1 / 2}$ (Oberst et al., 2023). Further work is needed to more precisely understand when such estimators may be beneficial.", "tables": {"table_3": "| RCT fraction $\\xi$ | Relative efficiency ( $95 \\%$ CI) |\n| :--: | :--: |\n| 0.2 | $1.17(1.01,1.31)$ |\n| 0.4 | $2.40(1.65,2.97)$ |\n| 0.6 | $2.06(1.69,2.36)$ |\n| 0.8 | $2.48(1.72,3.03)$ |\n| 1.0 | $3.95(1.79,5.49)$ |"}, "images": {}}, {"section_id": 13, "text": "# Acknowledgments \n\nThe author is funded by a Stanford Interdisciplinary Graduate Fellowship (SIGF). The author thanks Art Owen and several anonymous reviewers for careful and insightful comments that greatly improved this manuscript. He is also grateful for extremely helpful feedback on this work from Tim Morrison, Stefan Wager, Shu Yang, and Jann Spiess.", "tables": {}, "images": {}}, {"section_id": 14, "text": "## A Proofs\n\nHere we collect the proofs of the results stated in the main text.", "tables": {}, "images": {}}, {"section_id": 15, "text": "## A. 1 Preliminaries\n\nFirst, we state and prove some useful elementary results in asymptotic statistics.\nLemma 2. Let $X_{n}$ be a sequence of random vectors and $\\left\\{\\mathcal{F}_{n}, n \\geqslant 1\\right\\}$ be a sequence of $\\sigma$-algebras such that $\\mathbb{E}\\left[\\left\\|X_{n}\\right\\| \\mid \\mathcal{F}_{n}\\right]=o_{p}(1)$. Then $X_{n}=o_{p}(1)$.\n\nProof of Lemma 2. Fixing $M>0$, we have $M \\mathbf{1}\\left(\\left\\|X_{n}\\right\\|>M\\right) \\leqslant\\left\\|X_{n}\\right\\|$ for all $n$. Taking conditional expectations given $\\mathcal{F}_{n}$ on both sides we have\n\n$$\nP\\left(\\left\\|X_{n}\\right\\|>M \\mid \\mathcal{F}_{n}\\right) \\leqslant M^{-1} \\mathbb{E}\\left[\\left\\|X_{n}\\right\\| \\mid \\mathcal{F}_{n}\\right]\n$$\n\nThus if $\\mathbb{E}\\left[\\left\\|X_{n}\\right\\| \\mid \\mathcal{F}_{n}\\right]=o_{p}(1)$ we have $\\operatorname{Pr}\\left(\\left\\|X_{n}\\right\\|>M \\mid \\mathcal{F}_{n}\\right)=o_{p}(1)$ as well. But $\\operatorname{Pr}\\left(\\left\\|X_{n}\\right\\|>M \\mid\\right.$ $\\mathcal{F}_{n}$ ) is uniformly bounded so its expectation converges to zero, i.e., $\\operatorname{Pr}\\left(\\left\\|X_{n}\\right\\|>M\\right)=o(1)$. Since $M>0$ was arbitrary we conclude that $X_{n}=o_{p}(1)$.\n\nLemma 3. Let $\\left\\{\\hat{f}_{n}\\right\\}_{n=1}^{\\infty},\\left\\{f_{n}\\right\\}_{n=1}^{\\infty},\\left\\{\\hat{g}_{n}\\right\\}_{n=1}^{\\infty}$, and $\\left\\{g_{n}\\right\\}_{n=1}^{\\infty}$ be sequences of $P$-square integrable functions with $\\left\\|\\hat{f}_{n}-f_{n}\\right\\|_{2, P}=o_{p}\\left(a_{n}\\right)$ and $\\left\\|\\hat{g}_{n}-g_{n}\\right\\|_{2, P}=o_{p}\\left(b_{n}\\right)$ for some sequences $a_{n} \\downarrow 0, b_{n} \\downarrow 0$. Further suppose that there exists $C<\\infty$ independent of $n$ for which $\\left\\|\\hat{f}_{n}\\right\\|_{\\infty}+\\left\\|g_{n}\\right\\|_{\\infty} \\leqslant C$ for all $n \\geqslant 1$. Then $\\left\\|\\hat{f}_{n} \\hat{g}_{n}-f_{n} g_{n}\\right\\|_{2, P}=o_{p}\\left(a_{n}\\right)+o_{p}\\left(b_{n}\\right)$.\n\nProof. By Minkowksi's inequality we have\n\n$$\n\\begin{aligned}\n\\left\\|\\hat{f}_{n} \\hat{g}_{n}-f_{n} g_{n}\\right\\|_{2, P} & \\leqslant\\left\\|\\left(\\hat{f}_{n}-f_{n}\\right) g_{n}\\right\\|_{2, P}+\\left\\|\\hat{f}_{n}\\left(\\hat{g}_{n}-g_{n}\\right)\\right\\|_{2, P} \\\\\n& \\leqslant C\\left\\|\\hat{f}_{n}-f_{n}\\right\\|_{2, P}+C\\left\\|\\hat{g}_{n}-g_{n}\\right\\|_{2, P} \\\\\n& =o_{p}\\left(a_{n}\\right)+o_{p}\\left(b_{n}\\right)\n\\end{aligned}\n$$\n\nLemma 4. Let $X_{1}, \\ldots, X_{n}$ be i.i.d. random variables taking values in a space $\\mathcal{X}$ with distribution $P$ and let $\\|f\\|_{2, P}=\\left(\\int f^{2}(x) d P(x)\\right)^{1 / 2}$. Suppose $\\hat{f}_{n}(\\cdot)$ is a sequence of random measurable functions\n\non $\\mathcal{X}$ independent of $X_{1}, \\ldots, X_{n}$ for which $\\left\\|\\hat{f}_{n}-f\\right\\|_{2, P}=o_{p}\\left(r_{n}\\right)$ as $n \\rightarrow \\infty$ for some sequence $\\left\\{r_{n}\\right\\}_{n=1}^{\\infty}$. Then\n\n$$\n\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\hat{f}_{n}\\left(X_{i}\\right)-f\\left(X_{i}\\right)\\right)^{2}}=o_{p}\\left(r_{n}\\right)\n$$\n\nProof. Define the quantity\n\n$$\nY_{n}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\hat{f}_{n}\\left(X_{i}\\right)-f\\left(X_{i}\\right)\\right)^{2}\n$$\n\nand note that\n\n$$\n\\mathbb{E}\\left(Y_{n} \\mid \\hat{f}_{n}\\right)=\\left\\|\\hat{f}_{n}-f\\right\\|_{2, P}^{2}=o_{p}\\left(r_{n}^{2}\\right)\n$$\n\nThen $Y_{n}=o_{p}\\left(r_{n}^{2}\\right)$ by Lemma 2, and the result follows.\nLemma 5. Suppose $\\left(A_{1}, B_{1}\\right),\\left(A_{2}, B_{2}\\right), \\ldots$ are random variables with $\\mathbb{E}\\left[A_{n}\\right]<\\infty$ for all $n \\geqslant 1$. Let $\\left\\{\\mathcal{F}_{n}, n \\geqslant 1\\right\\}$ be a sequence of $\\sigma$-algebras such that for each $n,\\left(A_{1}, \\ldots, A_{n}\\right)$ is measurable with respect to $\\mathcal{F}_{n}$ and $B_{1}, \\ldots, B_{n}$ are conditionally independent given $\\mathcal{F}_{n}$. Further assume that $\\mathbb{E}\\left[B_{i} \\mid \\mathcal{A}_{n}\\right]=0$ and that there exists $C<\\infty$ such that $\\mathbb{E}\\left[B_{i}^{2} \\mid \\mathcal{A}_{n}\\right] \\leqslant C$ for all $n \\geqslant 1,1 \\leqslant i \\leqslant n$. Finally suppose that $n^{-1} \\sum_{i=1}^{n} A_{i}^{2}=o_{p}\\left(r_{n}^{2}\\right)$ as $n \\rightarrow \\infty$ for some positive sequence $r_{n}$. Then\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} A_{i} B_{i}=o_{p}\\left(r_{n} / \\sqrt{n}\\right)\n$$\n\nProof. We have\n\n$$\n\\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} A_{i} B_{i} \\mid \\mathcal{A}_{n}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} A_{i} \\mathbb{E}\\left[B_{i} \\mid \\mathcal{A}_{n}\\right]=0\n$$\n\nand hence\n$\\mathbb{E}\\left[\\left(\\frac{1}{n} \\sum_{i=1}^{n} A_{i} B_{i}\\right)^{2} \\mid \\mathcal{A}_{n}\\right]=\\frac{1}{n^{2}} \\operatorname{Var}\\left(\\sum_{i=1}^{n} A_{i} B_{i} \\mid \\mathcal{A}_{n}\\right)=\\frac{1}{n^{2}} \\sum_{i=1}^{n} A_{i}^{2} \\mathbb{E}\\left[B_{i}^{2} \\mid \\mathcal{A}_{n}\\right] \\leqslant \\frac{C}{n^{2}} \\sum_{i=1}^{n} A_{i}^{2}=o_{p}\\left(n^{-1} r_{n}^{2}\\right)$\nIt follows by Lemma 2 that\n\n$$\n\\frac{1}{r_{n} \\sqrt{n}} \\sum_{i=1}^{n} A_{i} B_{i}=o_{p}(1)\n$$\n\nas desired.", "tables": {}, "images": {}}, {"section_id": 16, "text": "# A. 2 Proof of Theorem 2 \n\nOur argument extends many ideas from Section 4.5 of Tsiatis (2006) for the restricted moment model. As in that text, we impose the following mild technical conditions on the true datagenerating distribution $P^{*}$ and the model $\\mathcal{P}_{\\mathcal{M}}$ :\nAssumption 1. The moment generating function $\\zeta(t)=\\mathbb{E}[\\exp (t Y)]$ exists in a neighborhood of 0 .\nAssumption 2. There exist $A<\\infty$ and $\\delta>0$ for which $\\mathbb{E}\\left[\\left(\\left(Y-m^{*}(R)\\right)\\right)^{2} \\mathbf{1}\\left(\\left|\\left(Y-m^{*}(R)\\right)\\right| \\leqslant A\\right) \\mid\\right.$ $R] \\geqslant \\delta$ and $V^{*}(R) \\leqslant A$ with probability 1 .\n\nWe begin by characterizing the space $\\mathcal{T}_{\\mathcal{M}}$. An arbitrary parametric submodel $P_{\\gamma}$ for the model $\\mathcal{P}_{\\mathcal{M}}$ is a distribution with density\n\n$$\np(w ; \\gamma)=f\\left(y \\mid r ; \\gamma_{1}\\right) g\\left(r ; \\gamma_{2}\\right)\n$$\n\nwith respect to some dominating measure $\\mu \\times \\lambda$ on $\\mathbb{R} \\times\\left(\\{0,1\\}^{2} \\times \\mathcal{X}\\right)$; here $f$ denotes the conditional density of $Y$ given $R$ and $g$ is the density of $R$. Since the density (39) must correspond to a distribution in $\\mathcal{P}_{\\mathcal{M}}$, the function $r \\mapsto m\\left(r ; \\gamma_{1}\\right)$ lies in $\\mathcal{M}$ for all $\\gamma_{1}$ in a neighborhood of $0 \\in \\mathbb{R}^{q}$, for some $q \\geqslant 1$. Since the component $f\\left(y \\mid r ; \\gamma_{1}\\right)$ is variationally independent of $g\\left(r ; \\gamma_{2}\\right)$, it follows that we can write\n\n$$\n\\mathcal{T}_{\\mathcal{M}}=\\mathcal{T}_{1} \\oplus \\mathcal{T}_{2}\n$$\n\nwhere $\\mathcal{T}_{1}$ is the closure of the span of the score functions of the densities $f\\left(y \\mid r ; \\gamma_{1}\\right)$ over all submodels and $\\mathcal{T}_{2}$ is the closure of the span of the score functions of the densities $g\\left(r ; \\gamma_{2}\\right)$ over all submodels (see the discussion preceding Theorem 4.5 of Tsiatis (2006) or the proof of the main results of Li and Luedtke (2023) for a similar argument). Since the model $\\mathcal{P}_{\\mathcal{M}}$ places no restrictions on the distribution of $R$, it follows that $\\mathcal{T}_{2}=\\mathcal{H}_{R}^{0}$ by Theorem 4.5 of Tsiatis (2006).\n\nWe now claim that\n\n$$\n\\mathcal{T}_{1}=\\left\\{h \\in \\mathcal{H}^{0} \\mid r \\mapsto \\mathbb{E}[Y h(W) \\mid R=r] \\in \\mathcal{S}_{M}, \\mathbb{E}[h(W) \\mid R]=0\\right\\}\n$$\n\nThe space on the right-hand side of (40), which we call $\\mathcal{T}_{1}^{\\dagger}$, is evidently closed since $\\mathcal{S}_{\\mathcal{M}}$ is a closed subspace of $\\mathcal{H}_{R}$. Hence to show that $\\mathcal{T}_{1} \\subseteq \\mathcal{T}_{1}^{\\dagger}$, it suffices to show that any element in the span of the score function of an arbitrary parametric submodel $f\\left(y \\mid r ; \\gamma_{1}\\right)$ lies in $\\mathcal{T}_{1}^{\\dagger}$. Utilizing the notation\n\n$$\n\\frac{\\partial h(a)}{\\partial \\gamma}=\\left.\\left(\\frac{\\partial}{\\partial \\gamma_{1}} h(\\gamma), \\ldots, \\frac{\\partial}{\\partial \\gamma_{r}} h(\\gamma)\\right)^{\\top}\\right|_{\\gamma=a} \\in \\mathbb{R}^{q}\n$$\n\nfor an arbitrary smooth function $h: \\mathbb{R}^{q} \\rightarrow \\mathbb{R}$, such an element is given by\n\n$$\nh(w)=c^{\\top} \\frac{\\partial \\log f(y \\mid r ; 0)}{\\partial \\gamma_{1}}=c^{\\top} \\frac{\\frac{\\partial}{\\partial \\gamma_{1}} f(y \\mid r ; 0)}{f^{*}(y \\mid r)}\n$$\n\nBy the smoothness conditions for parametric submodels (see Newey (1990)), we know that\n\n$$\n\\frac{\\partial m(r ; 0)}{\\partial \\gamma_{1}}=\\int y \\frac{\\partial}{\\partial \\gamma_{1}} f(y \\mid r ; 0) d \\mu(y)\n$$\n\nfor all $r$ and is square integrable. Hence $r \\mapsto m\\left(r ; \\gamma_{1}\\right)$ is an outcome mean function parametric submodel for $\\mathcal{M}$ indexed by $\\gamma_{1}$. With\n\n$$\n\\mathbb{E}[Y h(W) \\mid R=r]=c^{\\top} \\int y \\frac{\\partial}{\\partial \\gamma_{1}} f(y \\mid r ; 0) d \\mu(y)=c_{1}^{\\top} \\frac{\\partial}{\\partial \\gamma_{1}} m(r ; 0)\n$$\n\nwe conclude that $r \\mapsto \\mathbb{E}[Y h(W) \\mid R=r] \\in \\mathcal{S}_{\\mathcal{M}}$ by the definition of $\\mathcal{S}_{\\mathcal{M}}$. Furthermore\n\n$$\n\\mathbb{E}\\left[h_{1}(W) \\mid R=r\\right]=c_{1}^{\\top} \\int \\frac{\\partial}{\\partial \\gamma_{1}} f(y \\mid r ; 0) d \\mu(y)=c_{1}^{\\top} \\frac{\\partial}{\\partial \\gamma_{1}} \\int f\\left(y \\mid r ; \\gamma_{1}\\right) d \\mu(y)=0\n$$\n\nso that $h \\in \\mathcal{T}_{1}^{\\dagger}$.\nNow, we show the reverse inclusion, i.e. $\\mathcal{T}_{1}^{\\dagger} \\subseteq \\mathcal{T}_{1}$. To that end we consider arbitrary $h_{1} \\in \\mathcal{T}_{1}^{\\dagger}$ so that $\\mathbb{E}\\left[h_{1}(W) \\mid R\\right]=0$ and $r \\mapsto \\int\\left(y-m^{*}(r)\\right) h_{1}(w) f^{*}(y \\mid r) d \\mu(y) \\in \\mathcal{S}_{\\mathcal{M}}$. Considering the latter\n\ncondition, by definition of $\\mathcal{S}_{\\mathcal{M}}$ there exists a sequence of $r_{n}$-dimensional outcome mean function parametric submodels $m^{(n)}\\left(r ; \\gamma_{1}^{(n)}\\right)$ of $\\mathcal{M}$ and constant vectors $c_{n} \\in \\mathbb{R}^{r_{n}}$ for which\n\n$$\n\\tilde{h}_{1}^{(n)}(r):=c_{n}^{\\top} \\frac{\\partial m^{(n)}(r ; 0)}{\\partial \\gamma_{1}^{(n)}}\n$$\n\nconverges to\n\n$$\n\\tilde{h}_{1}(r):=\\int\\left(y-m^{*}(r)\\right) h_{1}(w) f^{*}(y \\mid r) d \\mu(y)\n$$\n\nin mean square as $n \\rightarrow \\infty$. In fact, we can assume that each $\\tilde{h}_{1}^{(n)}$ is a bounded element of $\\mathcal{H}_{R}{ }^{1}$. Now define\n\n$$\n\\tilde{h}_{1}(w):=h_{1}(w)-\\frac{\\left(y-m^{*}(r)\\right) \\tilde{h}_{1}(r)}{V^{*}(r)}\n$$\n\nand note that\n\n$$\n\\int \\tilde{h}_{1}(w) f^{*}(y \\mid r) d \\mu(y)=\\int y \\tilde{h}_{1}(w) f^{*}(y \\mid r) d \\mu(y)=0\n$$\n\nConsider\n\n$$\n\\tilde{h}_{1}^{(n)}(w)=H^{(n)}(w)-\\int H^{(n)}\\left(r, y^{\\prime}\\right) f^{*}\\left(y^{\\prime} \\mid r\\right) d \\mu\\left(y^{\\prime}\\right)\n$$\n\nwhere\n\n$$\nH^{(n)}(w)=h_{1}^{(n)}(w)-\\frac{\\left(y-m^{*}(r)\\right) \\mathbf{1}\\left(\\left|\\left(y-m^{*}(r)\\right)\\right| \\leqslant A_{n}\\right) \\int\\left(y^{\\prime}-m^{*}(r)\\right) h_{1}^{(n)}\\left(r, y^{\\prime}\\right) f^{*}\\left(y^{\\prime} \\mid r\\right) d \\mu\\left(y^{\\prime}\\right)}{\\int\\left(y^{\\prime}-m^{*}(r)\\right)^{2} \\mathbf{1}\\left(\\left|y^{\\prime}-m^{*}(r)\\right| \\leqslant A_{n}\\right) f^{*}\\left(y^{\\prime} \\mid r\\right) d \\mu\\left(y^{\\prime}\\right)}\n$$\n\nfor $h_{1}^{(n)}(w)=h_{1}(w) \\mathbf{1}\\left(\\left|h_{1}(w)\\right| \\leqslant A_{n}\\right), A_{n}=\\max (A, n)$. Then one can verify $\\tilde{h}_{1}^{(n)}$ is a sequence of bounded functions converging in mean square to $\\bar{h}_{1}$ satisfying\n\n$$\n\\int \\tilde{h}_{1}^{(n)}(w) f^{*}(y \\mid r) d \\mu(y)=\\int y \\tilde{h}_{1}^{(n)}(w) f^{*}(y \\mid r) d \\mu(y)=0\n$$\n\n[^0]$$\n\\omega^{*}(r)=\\int y^{3} f^{*}(y \\mid r) d \\mu(y)-m^{*}(r) \\int y^{2} f^{*}(y \\mid r) d \\mu(y)\n$$\n\nThen consider the truncated submodel defined by\n\n$$\n\\hat{m}^{(n)}\\left(r ; \\gamma_{1}^{(n)}\\right):= \\begin{cases}m^{*}(r) & \\text { if } \\sup _{0<\\|a\\| \\leqslant b_{n}} \\frac{\\left|m^{(n)}(r ; a)-m^{*}(r)\\right|}{\\|a\\|}>a_{n} \\text { or }\\left|\\omega^{*}(r)\\right|>a_{n} \\\\ m^{(n)}\\left(r ; \\gamma_{1}^{(n)}\\right) & \\text { otherwise }\\end{cases}\n$$\n\nwhere $a_{n} \\uparrow \\infty$ and $b_{n} \\downarrow 0$ are chosen such that\n\n$$\n\\lim _{n \\rightarrow \\infty} \\mathbb{E}\\left[\\left(\\tilde{h}_{1}^{(n)}(R)\\right)^{2} \\mathbf{1}\\left(\\sup _{0<\\|a\\| \\leqslant b_{n}} \\frac{\\left|m^{(n)}(R ; a)-m^{*}(R)\\right|}{\\|a\\|}>a_{n} \\text { or }\\left|\\omega^{*}(R)\\right|>a_{n}\\right)\\right]=0\n$$\n\nSuch a sequence exists by dominated convergence since each $\\tilde{h}_{1}^{(n)}(R)$ is square integrable and the existence of $\\partial / \\partial \\gamma_{1}^{(n)} m^{(n)}\\left(r ; \\gamma_{1}^{(n)}\\right)$ for almost all $r$ ensures that the indicator\n\n$$\n\\mathbf{1}\\left(\\sup _{0<\\|a\\| \\leqslant \\tilde{b}} \\frac{\\left|m^{(n)}(R ; a)-m^{*}(R)\\right|}{\\|a\\|}>\\tilde{a} \\text { or }\\left|\\omega^{*}(R)\\right|>\\tilde{a}\\right)\n$$\n\nconverges to zero with probability 1 as $\\tilde{a} \\uparrow \\infty$ then $\\tilde{b} \\downarrow \\infty$. Then $c_{n}^{\\top} \\frac{\\partial \\hat{m}^{(n)}(r ; \\tilde{a})}{\\partial \\gamma_{1}^{(n)}}$ is bounded above in absolute value by $a_{n}\\left\\|c_{n}\\right\\|$ for all $r$ yet still converges to $\\tilde{h}_{1}$ in mean square. Note: the truncation by $\\omega^{*}(r)$ is not necessary here, but is useful for the next footnote.\n\n\n[^0]:    ${ }^{1}$ Given the outcome mean function parametric submodels $m^{(n)}\\left(r ; \\gamma_{1}^{(n)}\\right)$ from the main text, define\n\n    $$\n    \\omega^{*}(r)=\\int y^{3} f^{*}(y \\mid r) d \\mu(y)-m^{*}(r) \\int y^{2} f^{*}(y \\mid r) d \\mu(y)\n    $$\n\nfor all $n \\geqslant 1$. Now for each $n \\geqslant 1$, we can define the exponentially tilted density family\n\n$$\nf^{(n)}\\left(y \\mid r ; \\gamma_{0}, \\gamma_{1}^{(n)}\\right)=\\frac{f^{*}(y \\mid r)\\left(1+\\gamma_{0} \\bar{h}_{1}^{(n)}(w)\\right) \\exp \\left(c^{(n)}\\left(r ; \\gamma_{0}, \\gamma_{1}^{(n)}\\right) y\\right)}{\\int f^{*}\\left(y^{\\prime} \\mid r\\right)\\left(1+\\gamma_{0} \\bar{h}_{1}^{(n)}\\left(r, y^{\\prime}\\right)\\right) \\exp \\left(c^{(n)}\\left(r ; \\gamma_{0}, \\gamma_{1}^{(n)}\\right) y^{\\prime}\\right) d \\mu\\left(y^{\\prime}\\right)}\n$$\n\nwhere $c^{(n)}\\left(r ; \\gamma_{0}, \\gamma_{1}^{(n)}\\right)$ is chosen such that\n\n$$\n\\int y f^{(n)}\\left(y \\mid r ; \\gamma_{0}, \\gamma_{1}^{(n)}\\right) d \\mu(y)=m^{(n)}\\left(r ; \\gamma_{1}^{(n)}\\right)\n$$\n\nfor all $r$ and all $\\left(\\gamma_{0}, \\gamma_{1}^{(n)}\\right)$ in a neighborhood of zero ${ }^{2}$. Note that $f^{(n)}$ is indeed a valid density due to boundedness of $\\bar{h}_{1}^{(n)}$ ensuring $1+\\gamma_{0} \\bar{h}_{1}^{(n)}(w)>0$ for all $w$ and all $\\gamma_{0}$ sufficiently close to 0 , while equation (43) ensures the densities $f^{(n)}$ correspond to distributions with mean functions lying in $\\mathcal{P}_{\\mathcal{M}}$. Now using the quotient rule and noting we can take $c^{(n)}(r ; 0,0)=0$ for all $r$ by (42), we compute\n\n$$\n\\begin{aligned}\n& \\frac{\\partial f^{(n)}(y \\mid r, 0,0)}{\\partial \\gamma_{0}}=f^{*}(y \\mid r) \\bar{h}_{1}^{(n)}(w) \\\\\n& \\frac{\\partial f^{(n)}(y \\mid r ; 0,0)}{\\partial \\gamma_{1}^{(n)}}=\\left(y-m^{*}(r)\\right) f^{*}(y \\mid r) \\frac{\\partial c^{(n)}(r ; 0,0)}{\\partial \\gamma_{1}^{(n)}}\n\\end{aligned}\n$$\n\nwhich are square integrable by Assumption 1. So $f^{(n)}\\left(y \\mid r ; \\gamma^{(n)}\\right)$ is a parametric submodel for the conditional distribution of $Y$ given $R=r$ in the model $\\mathcal{P}_{\\mathcal{M}_{1}}$ with score\n\n$$\n\\frac{\\partial \\log f^{(n)}(w ; 0)}{\\partial \\gamma^{(n)}}=\\left(\\bar{h}_{1}^{(n)}(w),\\left(y-m^{*}(r)\\right) \\frac{\\partial c^{(n)}(r ; 0,0)}{\\partial\\left(\\gamma_{1}^{(n)}\\right)^{\\top}}\\right)^{\\top}\n$$\n\n[^0]$$\n\\iota\\left(c ; r, \\gamma_{0}\\right)=\\frac{\\int y f^{*}(y \\mid r)\\left(1+\\gamma_{0} \\bar{h}_{1}^{(n)}(w)\\right) \\exp (c y) d \\mu(y)}{\\int f^{*}(y \\mid r)\\left(1+\\gamma_{0} \\bar{h}_{1}^{(n)}(w)\\right) \\exp (c y) d \\mu(y)}\n$$\n\nBy the previous footnote we can assume there is some $a_{n}<\\infty$ and $b_{n}>0$ for which $m^{(n)}\\left(r ; \\gamma_{1}^{(n)}\\right)=m^{*}(r)$ for all $r$ such that $\\left|\\omega^{*}(r)\\right|>a_{n}$ or $\\sup _{0<\\|a\\| \\leqslant b_{n}} \\frac{\\left|m^{(n)}(R ; a)-m^{*}(R)\\right|}{\\|a\\|}>a_{n}$. Since $\\iota\\left(0 ; r, \\gamma_{0}\\right)=m^{*}(r)$ for any $\\left(r, \\gamma_{0}\\right)$ by (42), we can take $c^{(n)}\\left(r ; \\gamma_{0}, \\gamma_{1}^{(n)}\\right)=0$ whenever this condition on $r$ holds. When such conditions are not satisfied, we have $\\left|\\omega^{*}(r)\\right| \\leqslant a_{n}$ and also $\\sup _{0<\\|a\\| \\leqslant b_{n}} \\frac{\\left|m^{(n)}(R ; a)-m^{*}(R)\\right|}{\\|a\\|} \\leqslant a_{n}$. Then note that by Assumption 2 there exists some open interval $\\mathcal{C}$ containing 0 on which $\\frac{\\partial c}{\\partial c}\\left(c ; r, \\gamma_{0}\\right)$ and $\\frac{\\partial^{2}}{\\partial c^{2}}\\left(c ; r, \\gamma_{0}\\right)$ exist. With $\\omega^{*}(r)$ defined as in (41), we compute\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\iota\\left(0 ; r, \\gamma_{0}\\right)}{\\partial c} & =\\int y^{2} f^{*}(y \\mid r) d \\mu(y)-\\left(\\int y f^{*}(y \\mid r) d \\mu(y)\\right)^{2}=V^{*}(r) \\geqslant \\delta, \\quad \\forall r, \\gamma_{0} \\\\\n\\frac{\\partial^{2} \\iota\\left(0 ; r, \\gamma_{0}\\right)}{\\partial c^{2}} & =\\omega^{*}(r)\n\\end{aligned}\n$$\n\nThe condition $\\left|\\omega^{*}(r)\\right| \\leqslant a_{n}$ thus ensures there exists an interval $\\left(-\\tilde{c}_{n}, \\tilde{c}_{n}\\right) \\subseteq \\mathcal{C}$ on which $\\frac{\\partial \\iota\\left(c ; r, \\gamma_{0}\\right)}{\\partial c} \\geqslant \\delta / 2$ for all $\\left(r, \\gamma_{0}\\right)$. This implies that for all $\\left(r, \\gamma_{0}\\right)$ and $|\\epsilon|<\\frac{\\tilde{c}_{n} \\delta}{2}$, there exists $c \\in\\left(-\\tilde{c}_{n}, \\tilde{c}_{n}\\right)$ such that $\\iota\\left(c ; r, \\gamma_{0}\\right)=m^{*}(r)+\\epsilon$. But the condition $\\sup _{0<\\|a\\| \\leqslant b_{n}} \\frac{\\left|m^{(n)}(R ; a)-m^{*}(R)\\right|}{\\|a\\|} \\leqslant a_{n}$ ensures that whenever $\\left\\|\\gamma_{1}^{(n)}\\right\\| \\leqslant \\min \\left(b_{n}, \\frac{\\tilde{c}_{n} \\delta}{2} a_{n}\\right)$ we in fact have $\\left|m^{(n)}\\left(r ; \\gamma_{1}^{(n)}\\right)-m^{*}(r)\\right| \\leqslant \\frac{\\tilde{c}_{n} \\delta}{2}$, showing the existence of $c^{(n)}\\left(r ; \\gamma_{0}, \\gamma_{1}^{(n)}\\right)$ to satisfy (43) for all $r$ and $\\left(\\gamma_{0}, \\gamma_{1}^{(n)}\\right)$ in a neighborhood of 0 .\n\n\n[^0]:    ${ }^{2}$ To see that this is possible, define\n\nfor $\\gamma^{(n)}=\\left(\\gamma_{0}, \\gamma_{1}^{(n)}\\right)$. By taking the derivative of both sides of (43) with respect to $\\gamma_{1}^{(n)}$ at $\\gamma_{0}=0$, $\\gamma_{1}^{(n)}=0$, we see that\n\n$$\n\\frac{\\partial e^{(n)}(r ; 0 ; 0)}{\\partial \\gamma_{1}^{(n)}} V^{*}(r)=\\frac{\\partial m^{(n)}(r ; 0)}{\\partial \\gamma_{1}^{(n)}}\n$$\n\nso we can rewrite\n\n$$\n\\left(y-m^{*}(r)\\right) \\frac{\\partial e^{(n)}(r ; 0,0)}{\\partial \\gamma_{1}^{(n)}}=\\frac{\\left(y-m^{*}(r)\\right) \\frac{\\partial m^{(n)}(r ; 0)}{\\partial \\gamma_{1}^{(n)}}}{V^{*}(r)}\n$$\n\nHence\n\n$$\n\\left(1, c_{n}^{\\top}\\right)^{\\top} \\frac{\\partial \\log f^{(n)}(w ; 0)}{\\partial \\gamma^{(n)}}=\\bar{h}_{1}^{(n)}(w)+\\left(y-m^{*}(r)\\right) \\frac{c_{n}^{\\top} \\frac{\\partial m^{(n)}(r ; 0)}{\\partial \\gamma_{1}^{(n)}}}{V^{*}(r)}\n$$\n\nwhich we've ensured converge in mean square to\n\n$$\n\\bar{h}_{1}(w)+\\frac{\\left(y-m^{*}(r)\\right) \\bar{h}_{1}(r)}{V^{*}(r)}=h_{1}(w)\n$$\n\nas $n \\rightarrow \\infty$. This allows us to conclude $h_{1} \\in \\mathcal{T}_{1}$, as desired.\nHaving shown $\\mathcal{T}_{\\mathcal{M}}=\\mathcal{T}_{1} \\oplus \\mathcal{T}_{2}$ with $\\mathcal{T}_{1}$ as in (40) and $\\mathcal{T}_{2}=\\mathcal{H}_{R}^{0}$, we are now ready to prove (8). First suppose $g$ is an element of the right-hand side of (8), so that $g(w)=h(r)\\left(y-m^{*}(r)\\right)$ for some $h \\in \\mathcal{S}_{\\mathcal{M}}^{\\perp}$. Then for any $g_{1} \\in \\mathcal{T}_{1}$ we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[g(W) g_{1}(W)\\right] & =\\mathbb{E}\\left[h(R) \\mathbb{E}\\left[\\left(Y-m^{*}(R)\\right) g_{1}(W) \\mid R\\right]\\right] \\text { by conditioning on } R \\\\\n& =-\\mathbb{E}\\left[h(R) m^{*}(R) \\mathbb{E}\\left[g_{1}(W) \\mid R\\right]\\right] \\text { since } \\mathbb{E}\\left[Y g_{1}(W) \\mid R\\right] \\in \\mathcal{S}_{\\mathcal{M}} \\\\\n& =0 \\text { since } \\mathbb{E}\\left[g_{1}(W) \\mid R\\right]=0\n\\end{aligned}\n$$\n\nAdditionally, for any $g_{2} \\in \\mathcal{T}_{2}$ we have\n\n$$\n\\mathbb{E}\\left[g(W) g_{2}(R)\\right]=\\mathbb{E}\\left[h(R) g_{2}(R) \\mathbb{E}\\left[Y-m^{*}(R) \\mid R\\right]\\right]=0\n$$\n\nHence $g \\in \\mathcal{T}_{1}^{\\perp} \\cap \\mathcal{T}_{2}^{\\perp}=\\left(\\mathcal{T}_{1} \\oplus \\mathcal{T}_{2}\\right)^{\\perp}$, completing the inclusion \" $\\supseteq$ \" of (8).\nTo show the reverse inclusion, we now fix $g \\in\\left(\\mathcal{T}_{1} \\oplus \\mathcal{T}_{2}\\right)^{\\perp}$. Since $g \\in \\mathcal{T}_{2}^{\\perp}$ but $x \\mapsto \\mathbb{E}[g(W) \\mid R=$ $r] \\in \\mathcal{T}_{2}$, by conditioning on $R$ we have\n\n$$\n0=\\mathbb{E}[g(W) \\mathbb{E}[g(W) \\mid R]]=\\mathbb{E}\\left[(\\mathbb{E}[g(W) \\mid R])^{2}\\right]\n$$\n\nand so $\\mathbb{E}[g(W) \\mid R]=0$. Next, we define\n\n$$\n\\tilde{g}(w)=h(r)\\left(y-m^{*}(r)\\right), \\quad h(r)=\\frac{\\int\\left(y^{\\prime}-m^{*}(r)\\right) g\\left(r, y^{\\prime}\\right) f^{*}(y \\mid r) d \\mu(y)}{V^{*}(r)}\n$$\n\nso that $h(R) V^{*}(R)=\\mathbb{E}\\left[\\left(Y-m^{*}(R)\\right) g(W) \\mid R\\right]$. Notice that $g-\\tilde{g} \\in \\mathcal{T}_{1}$, since\n\n$$\n\\mathbb{E}[\\tilde{g}(W) \\mid R]=h(R) \\mathbb{E}\\left[Y-m^{*}(R) \\mid R\\right]=0=\\mathbb{E}[g(W) \\mid R]\n$$\n\nwhile\n\n$$\n\\begin{aligned}\n\\mathbb{E}[Y(g(W)-\\tilde{g}(W)) \\mid R] & =\\mathbb{E}[Y g(W) \\mid R]-\\mathbb{E}\\left[Y h(R)\\left(Y-m^{*}(R)\\right) \\mid R\\right] \\\\\n& =h(R) V^{*}(R)-\\mathbb{E}\\left[\\left(Y-m^{*}(R)\\right)^{2} h(R) \\mid R\\right] \\\\\n& =0 \\in \\mathcal{S}_{\\mathcal{M}}\n\\end{aligned}\n$$\n\nwhere the second equality uses the fact that $\\mathbb{E}\\left[h(R)\\left(Y-m^{*}(R)\\right) \\mid R\\right]=\\mathbb{E}[h(R) g(W) \\mid R]=0$ for all $h \\in \\mathcal{H}_{R}$. Since $g \\in \\mathcal{T}_{1}^{\\perp}$ we must have\n\n$$\n\\mathbb{E}[g(W)(g(W)-\\tilde{g}(W))]=0\n$$\n\nIt follows that\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[(g(W)-\\tilde{g}(W))^{2}\\right] & =\\mathbb{E}\\left[\\tilde{g}^{2}(W)\\right]-\\mathbb{E}[g(W) \\tilde{g}(W)] \\\\\n& =\\mathbb{E}\\left[\\frac{\\left(\\mathbb{E}\\left[\\left(Y-m^{*}(R)\\right) g(W) \\mid R\\right]\\right)^{2}\\left(Y-m^{*}(R)\\right)^{2}}{\\left(V^{*}(R)\\right)^{2}}\\right] \\\\\n& -\\mathbb{E}\\left[\\frac{\\mathbb{E}\\left[g(W)\\left(Y-m^{*}(R)\\right) \\mid R\\right] g(W)\\left(Y-m^{*}(R)\\right)}{\\left.V^{*}(R)\\right.}\\right] \\\\\n& =0\n\\end{aligned}\n$$\n\nHence $g(w)=\\tilde{g}(w)$. Finally we note that $h \\in \\mathcal{S}_{\\mathcal{M}}^{\\perp}$ since for any $h_{1} \\in \\mathcal{S}_{\\mathcal{M}}$ we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[h_{1}(R) h(R)\\right] & =\\mathbb{E}\\left[\\frac{h_{1}(R) \\mathbb{E}\\left[\\left(Y-m^{*}(R)\\right) g(W) \\mid R\\right]}{V^{*}(R)}\\right] \\\\\n& =\\mathbb{E}\\left[\\frac{\\left(Y-m^{*}(R)\\right) g(W) h_{1}(R)}{V^{*}(R)}\\right] \\\\\n& =0\n\\end{aligned}\n$$\n\nwhere the last equality follows since $g \\in \\mathcal{T}_{1}^{\\perp}$ but\n\n$$\n\\tilde{g}(w)=\\frac{\\left(y-m^{*}(r)\\right) h_{1}(r)}{V^{*}(r)}\n$$\n\nsatisfies $\\mathbb{E}[\\tilde{g}(W) \\mid R]=0$ and $\\mathbb{E}[Y \\tilde{g}(W) \\mid R]=h_{1}(R)$ so that $\\tilde{g} \\in \\mathcal{T}_{1}$ by definition.", "tables": {}, "images": {}}, {"section_id": 17, "text": "# A. 3 Proof of Corollary 1 \n\nWe write $\\mathcal{H}_{W}^{0}$ as the direct sum of orthogonal subspaces $\\mathcal{T}_{1} \\oplus \\mathcal{T}_{2}$ where\n\n$$\n\\mathcal{T}_{1}=\\left\\{f \\in \\mathcal{H}_{W}^{0} \\mid \\mathbb{E}[f(W) \\mid R]=0\\right\\}, \\quad \\mathcal{T}_{2}=\\mathcal{H}_{R}^{0}\n$$\n\nSince the model $\\mathcal{P}_{\\mathcal{M}}$ places no restrictions on the distribution of $R$, it follows that its semiparametric tangent space $\\mathcal{T}_{\\mathcal{M}}=\\overline{\\mathcal{T}_{1}} \\oplus \\mathcal{T}_{2}$ for some $\\overline{\\mathcal{T}_{1}} \\subseteq \\mathcal{T}_{1}$ (this is also easy to see directly from the fact that $\\mathcal{T}_{\\mathcal{M}}^{\\perp} \\subseteq \\mathcal{T}_{1}$ by (8)). Conversely, the experimental propensity score being known places no restriction on the conditional distribution of $Y$ given $R$. Thus, we can write $\\overline{\\mathcal{T}}_{\\mathcal{M}}=\\overline{\\mathcal{T}_{1}} \\oplus \\overline{\\mathcal{T}}_{2}$ where $\\overline{\\mathcal{T}}_{2}$ is the semiparametric tangent space for the model $\\mathcal{P}_{\\mathcal{N}}$ on the distribution of $R=(Z, S, X)$, viewing $Z$ as the outcome and $(S, X)$ the covariates and letting $\\mathcal{N}=\\left\\{(s, x) \\mapsto s e^{*}(x)+(1-s) h(x) \\mid h \\in \\mathcal{H}_{X}\\right\\}$ so that the restriction that the mapping $(s, x) \\mapsto \\mathbb{E}[Z \\mid S=s, X=x] \\in \\mathcal{N}$ amounts precisely to the experimental propensity score being known and equal to $e^{*}$. By (8) we have\n\n$$\n\\overline{\\mathcal{T}}_{2}^{\\perp}=\\left\\{\\left(z-e^{*}(x)\\right) h(x) \\mid h \\in \\mathcal{S}_{\\mathcal{N}}^{\\perp}\\right\\}\n$$\n\nwhere $\\mathcal{S}_{\\mathcal{N}}^{\\perp}$ is the orthogonal complement of $\\mathcal{S}_{\\mathcal{N}}$ in $\\mathcal{H}_{S X}$ and $\\overline{\\mathcal{T}}_{2}^{\\perp}$ denotes the orthogonal complement of $\\overline{\\mathcal{T}}_{2}$ in $\\overline{\\mathcal{T}}_{2}=\\mathcal{H}_{R}^{0}$, (not the orthogonal complement in $\\mathcal{H}_{W}^{0}$ ). We conclude\n\n$$\n\\overline{\\mathcal{T}}_{\\mathcal{M}}^{\\perp}=\\mathcal{T}_{\\mathcal{M}}^{\\perp} \\oplus \\overline{\\mathcal{T}}_{2}^{\\perp}\n$$\n\nThus, to prove (24) it suffices to show that\n\n$$\n\\mathcal{S}_{\\mathcal{N}}^{\\perp}=\\left\\{(s, x) \\mapsto s h(x) \\mid h \\in \\mathcal{H}_{X}\\right\\}\n$$\n\nTo that end, we first show that $\\mathcal{S}_{\\mathcal{N}}=\\left\\{(s, x) \\mapsto(1-s) h(x) \\mid h \\in \\mathcal{H}_{X}\\right\\}$. To do so, note that the RHS is a closed subset of $\\mathcal{H}_{S X}$, and that an arbitrary outcome mean function parametric submodel of $\\mathcal{N}$ takes the form\n\n$$\nm(s, x ; \\gamma)=s e^{*}(x)+(1-s) h(x ; \\gamma)\n$$\n\nwhere $h(x ; 0)=r^{*}(x)$. For any $c$ with the same length as $\\gamma$ we have\n\n$$\nc^{\\gamma} \\frac{\\partial m(s, x ; 0)}{\\partial \\gamma}=(1-s) c^{\\gamma} \\frac{\\partial h(x ; 0)}{\\partial \\gamma}\n$$\n\nwhich clearly lies in the set $\\{(s, x) \\mapsto(1-s) h(x) \\mid h \\in \\mathcal{H}_{X}\\}$. Hence $\\mathcal{S}_{\\mathcal{N}}$ is contained inside this set. Conversely, given an arbitrary element $g$ given by $g(s, x)=(1-s) h(x)$ in this set, we can consider the univariate submodels\n\n$$\nm_{n}(s, x ; \\gamma)=s e^{*}(x)+(1-s)\\left(q^{*}(x)+\\gamma h_{n}(x)\\right)\n$$\n\nfor a sequence of bounded functions $h_{n} \\in \\mathcal{H}_{X}$ converging in mean square to $h$. Note the validity of each of these submodels for $\\gamma$ in a neighborhood of zero uses the fact that $h_{n}$ is bounded and $\\delta<q^{*}(x)<1-\\delta$ for all $x$ and some $\\delta>0$. With $\\partial m_{n}(s, x ; \\gamma) /\\left.\\partial \\gamma\\right|_{\\gamma=0}$ converging to $g(s, x)$ in mean square, we conclude $g \\in \\mathcal{S}_{\\mathcal{N}}$, and indeed $\\mathcal{S}_{\\mathcal{N}}=\\left\\{(s, x) \\mapsto(1-s) h(x) \\mid h \\in \\mathcal{H}_{X}\\right\\}$. Given this, it is then straightforward to verify that\n\n$$\n\\mathcal{S}_{\\mathcal{N}}^{\\perp}=\\left\\{(s, x) \\mapsto s h(x) \\mid h \\in \\mathcal{H}_{X}\\right\\}\n$$\n\nupon recalling that $s(1-s)=0$.", "tables": {}, "images": {}}, {"section_id": 18, "text": "# A. 4 Proof of Corollary 2 \n\nIt suffices to show that $\\varphi_{0, \\text { rct }}$ is the EIF for $\\tau_{\\text {rct }}$ in the model $\\tilde{\\mathcal{P}}_{\\mathcal{M}}$, since $\\tilde{\\mathcal{P}}_{\\mathcal{M}} \\subseteq \\mathcal{P}_{\\mathcal{M}} \\subseteq \\mathcal{P}_{\\mathcal{H}_{R}}$ and hence the semiparametric bound for $\\mathcal{P}_{\\mathcal{M}}$ must be sandwiched between the bounds for $\\tilde{\\mathcal{P}}_{\\mathcal{M}}$ and the nonparametric model $\\mathcal{P}_{\\mathcal{H}_{R}}$. To do so, by Theorem 1 it suffices to show that $\\varphi_{0, \\text { rct }} \\in\\left(\\tilde{\\mathcal{T}}_{\\mathcal{M}}^{\\perp}\\right)^{\\perp}=\\tilde{\\mathcal{T}}_{\\mathcal{M}}$.\n\nFix an arbitrary element $g \\in \\tilde{\\mathcal{T}}_{\\mathcal{M}}^{\\perp}$. By Theorem 2, we can write $g(w)=h(r)\\left(y-m^{*}(r)\\right)+$ $s \\tilde{h}(x)\\left(z-e^{*}(x)\\right)$ for some $h \\in \\mathcal{S}_{\\mathcal{M}}^{\\perp}$ and $\\tilde{h} \\in \\mathcal{H}_{X}$. For each $f \\in \\mathcal{H}_{R}$, with $\\Delta(z, x, y ; \\eta)$ as in Proposition 1, we define\n\n$$\n\\begin{aligned}\n\\phi(r ; f) & =h(r) \\mathbb{E}\\left[\\left(\\left(\\rho^{*}\\right)^{-1} S \\Delta\\left(Z, X, Y ; \\eta^{*}\\right)+f(R)\\right)\\left(Y-m^{*}(R)\\right) \\mid R=r\\right] \\\\\n& =h(r)\\left(\\frac{s z}{\\rho^{*} e^{*}} \\mathbb{E}\\left[\\left(Y-m_{11}^{*}(X)\\right)^{2} \\mid R\\right]-\\frac{s(1-z)}{\\rho^{*}\\left(1-e^{*}\\right)} \\mathbb{E}\\left[\\left(Y-m_{10}^{*}(X)\\right)^{2} \\mid R\\right]\\right) \\\\\n& =h(r)\\left(\\rho^{*}\\right)^{-1}\\left(c_{1} s z-c_{0} s(1-z)\\right)\n\\end{aligned}\n$$\n\nwhere $c_{1}=\\sigma_{1}^{2} / e^{*}$ and $c_{0}=\\sigma_{0}^{2} /\\left(1-e^{*}\\right)$, and the second equality uses the fact that $f(r) \\mathbb{E}[Y-$ $\\left.m^{*}(R) \\mid R\\right]=0$. Note $\\phi(r ; f)$ is independent of $f \\in \\mathcal{H}_{R}$ and by the assumption of the theorem, $r \\mapsto\\left(\\rho^{*}\\right)^{-1}\\left(c_{1} s z-c_{0} s(1-z)\\right) \\in \\mathcal{S}_{\\mathcal{M}}$. With $h \\in \\mathcal{S}_{\\mathcal{M}}^{\\perp}$ we conclude that\n\n$$\n\\mathbb{E}[\\phi(R ; f)]=0, \\quad \\forall f \\in \\mathcal{H}_{R}\n$$\n\nNext, we note that for any $\\bar{g} \\in \\mathcal{H}_{X}$ we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[S \\bar{h}(X)\\left(Z-e^{*}\\right)\\left(\\Delta(Z, X, Y ; \\eta^{*}\\right)+\\bar{g}(X)\\right)\\right] & =\\mathbb{E}\\left[S \\bar{h}(X)\\left(Z-e^{*}\\right) \\mathbb{E}\\left(\\Delta(Z, X, Y ; \\eta^{*}\\right) \\mid S=1, Z, X\\right)\\right] \\\\\n& +\\mathbb{E}\\left[S \\bar{h}(X) \\bar{g}(X) \\mathbb{E}\\left(Z-e^{*} \\mid S=1, X\\right)\\right] \\\\\n& =0\n\\end{aligned}\n$$\n\nFinally, we put together (46) and (47) to show that $\\varphi_{0, \\text { rct }}$ must be orthogonal to our arbitrary chosen function $g \\in \\tilde{\\mathcal{F}}_{\\mathcal{M}}^{\\perp}$. By iterated expectations, defining $f(r)=s\\left(m_{11}^{*}(x)-m_{10}^{*}(x)-\\tau_{\\text {rct }}^{*}\\right) / \\rho^{*}$ we compute\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\varphi_{0, r c t}\\left(W ; \\tau_{r c t}^{*}, \\eta^{*}\\right) g(W)\\right]= & \\mathbb{E}\\left[\\varphi_{0, \\mathrm{rct}}\\left(W ; \\tau_{\\mathrm{rct}}^{*} ; \\eta^{*}\\right)\\left(h(R)\\left(Y-m^{*}(R)\\right)+S \\bar{h}(X)\\left(Z-e^{*}\\right)\\right)\\right] \\\\\n= & \\mathbb{E}\\left[h(R) \\mathbb{E}\\left(\\varphi_{0, \\mathrm{rct}}\\left(W ; \\tau_{\\mathrm{rct}}^{*} ; \\eta^{*}\\right)\\left(Y-m^{*}(R)\\right) \\mid R\\right]\\right. \\\\\n& \\left.+\\left(\\rho^{*}\\right)^{-1} \\mathbb{E}\\left[S \\bar{h}(X)\\left(Z-e^{*}\\right)\\left(\\Delta(Z, X, Y ; \\eta^{*}\\right)+m_{11}^{*}(X)-m_{10}^{*}(X)-\\tau_{\\mathrm{rct}}^{*}\\right)\\right] \\\\\n= & \\mathbb{E}[\\phi(S, Z, X ; f)]+0 \\text { by (47) with } \\bar{g}(X)=m_{11}^{*}(X)-m_{10}^{*}(X)-\\tau_{\\mathrm{rct}}^{*} \\\\\n= & 0 \\text { by }(46)\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 19, "text": "# A. 5 Proof of Lemma 1 \n\nAs indicated in the main text, here we prove a stronger version of Lemma 1 that is applicable to models of the form $\\tilde{\\mathcal{P}}_{\\mathcal{M}}$ where the experimental propensity score is known, as considered in Corollary 1.\n\nLemma. Suppose the function $g=g\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)$ is any element of the space $\\tilde{\\mathcal{F}}_{\\mathcal{M}}^{\\perp}$ for some collection of outcome mean functions $\\mathcal{M} \\subseteq \\mathcal{H}_{R}$ satisfying the conditions of Corollary 1, so that we can write\n\n$$\ng\\left(w ; \\tau^{*}, \\eta^{*}\\right)=\\left(y-m^{*}(r)\\right) h_{1}\\left(r ; \\tau^{*}, \\eta^{*}\\right)+s\\left(z-e^{*}(x)\\right) h_{2}\\left(x ; \\tau^{*}, \\eta^{*}\\right)\n$$\n\nwhere $\\eta^{*}$ includes the mean function $m^{*}, h_{1}^{*}(\\cdot)=h_{1}\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right) \\in \\mathcal{S}_{\\mathcal{M}}^{\\perp}$, and $h_{2}^{*}(\\cdot)=h_{2}\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right) \\in \\mathcal{H}_{X}$. Then equation (26) holds under the following conditions:\n\n- (Boundedness) There exists $C<\\infty$ for which $V^{*}(r) \\leqslant C$ for $P^{*}$-almost every $r$ and both $\\left\\|h_{i}^{*}\\right\\|_{\\infty, P^{*}} \\leqslant C$ and $\\left\\|\\left|\\hat{h}_{i}^{(-k)}\\right|\\right\\|_{\\infty, P^{*}} \\leqslant C$ for $i=1,2$\n- (Approximate tangency) With probability tending to 1, for each $k=1, \\ldots, K$ there exists $\\hat{R}^{(-k)}=\\hat{R}^{(-k)}(\\cdot) \\in \\mathcal{H}_{R}$ depending only on the observations outside fold $k$ such that the function $r \\mapsto \\hat{m}^{(-k)}(r)-m^{*}(r)-\\hat{R}^{(-k)}(r)$ lies in the outcome mean function tangent space $\\mathcal{S}_{\\mathcal{M}},\\left\\|\\hat{R}^{(-k)}\\right\\|_{2, P^{*}}=o_{p}(1)$, and the following rate holds:\n\n$$\n\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left|\\hat{R}^{(-k)}\\left(R_{i}\\right)\\right|=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\n- (Rate conditions) The following rate conditions hold for all $k=1, \\ldots, K$ :\n- (First order consistency) $\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}}+\\left\\|\\hat{h}_{1}^{(-k)}-h_{1}^{*}\\right\\|_{2, P^{*}}+\\left\\|\\hat{h}_{2}^{(-k)}-h_{2}^{*}\\right\\|_{2, P^{*}}=o_{p}(1)$\n- (Product rate condition) $\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}} \\times\\left\\|\\hat{h}_{1}^{(-k)}-h_{1}^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 2}\\right)$.\n\nLemma 1 follows immediately from the modified Lemma above since $\\tilde{\\mathcal{T}}_{\\mathcal{M}}^{\\perp} \\supseteq \\mathcal{T}_{\\mathcal{M}}^{\\perp}$, and for any $g \\in \\mathcal{T}_{\\mathcal{M}}^{\\perp}$ we can apply the modified Lemma with $h_{2}=h_{2}^{(-k)}=0$.\n\nWriting\n\n$$\n\\frac{1}{N} \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{I}_{k}} g\\left(W_{i} ; \\hat{\\tau}^{(-k)}, \\hat{\\eta}^{(-k)}\\right)-g\\left(W_{i} ; \\tau^{*}, \\eta^{*}\\right)=\\sum_{k=1}^{K} \\frac{\\left|\\mathcal{I}_{k}\\right|}{N} \\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} g\\left(W_{i} ; \\hat{\\tau}^{(-k)}, \\hat{\\eta}^{(-k)}\\right)-g\\left(W_{i} ; \\tau^{*}, \\eta^{*}\\right)\n$$\n\nwe see it suffices to fix $k \\in\\{1, \\ldots, K\\}$ and show\n\n$$\n\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} g\\left(W_{i} ; \\hat{\\tau}^{(-k)}, \\hat{\\eta}^{(-k)}\\right)-g\\left(W_{i} ; \\tau^{*}, \\eta^{*}\\right)=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nWe have\n\n$$\n\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} g\\left(W_{i} ; \\hat{\\tau}^{(-k)}, \\hat{\\eta}^{(-k)}\\right)-g\\left(W_{i} ; \\tau^{*}, \\eta^{*}\\right)=a_{k}+b_{k}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\na_{k} & =\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left(Y_{i}-\\hat{m}^{(-k)}\\left(R_{i}\\right)\\right) \\hat{h}_{1}^{(-k)}\\left(R_{i}\\right)-\\left(Y_{i}-m^{*}\\left(R_{i}\\right)\\right) h_{1}^{*}\\left(R_{i}\\right) \\\\\nb_{k} & =\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} S_{i}\\left(Z_{i}-e^{*}\\left(X_{i}\\right)\\right)\\left(\\hat{h}_{2}^{(-k)}\\left(X_{i}\\right)-h_{2}^{*}\\left(X_{i}\\right)\\right)\n\\end{aligned}\n$$\n\nWe further expand\n\n$$\na_{k}=a_{1 k}+a_{2 k}+a_{3 k}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\na_{1 k} & =\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left(m^{*}\\left(R_{i}\\right)-\\hat{m}^{(-k)}\\left(R_{i}\\right)\\right) h_{1}^{*}\\left(R_{i}\\right) \\\\\na_{2 k} & =\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left(Y_{i}-m^{*}\\left(R_{i}\\right)\\right)\\left(\\hat{h}_{1}^{(-k)}\\left(R_{i}\\right)-h_{1}^{*}\\left(R_{i}\\right)\\right) \\\\\na_{3 k} & =\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left(m^{*}\\left(R_{i}\\right)-\\hat{m}^{(-k)}\\left(R_{i}\\right)\\right)\\left(\\hat{h}_{1}^{(-k)}\\left(R_{i}\\right)-h_{1}^{*}\\left(R_{i}\\right)\\right)\n\\end{aligned}\n$$\n\nFirst, we show $a_{1 k}=o_{p}\\left(N^{-1 / 2}\\right)$. Note that\n\n$$\n\\left|\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} \\hat{R}^{(-k)}\\left(R_{i}\\right) h_{1}^{*}\\left(R_{i}\\right)\\right| \\leqslant C \\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left|\\hat{R}^{(-k)}\\left(R_{i}\\right)\\right|=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nby (48) and the boundedness condition $\\left\\|h_{1}^{*}\\right\\|_{\\infty, P^{*}} \\leqslant C$. Hence $a_{1 k}=\\tilde{a}_{1 k}+o_{p}\\left(N^{-1 / 2}\\right)$ where\n\n$$\n\\tilde{a}_{1 k}=\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left(\\hat{R}^{(-k)}\\left(R_{i}\\right)+m^{*}\\left(R_{i}\\right)-\\hat{m}^{(-k)}\\left(R_{i}\\right)\\right) h_{1}^{*}\\left(R_{i}\\right)\n$$\n\nFurthermore, whenever $\\hat{R}^{(-k)}$ exists, we have that\n\n$$\n\\mathbb{E}\\left[\\left(\\hat{R}^{(-k)}\\left(R_{i}\\right)+m^{*}\\left(R_{i}\\right)-\\hat{m}^{(-k)}\\left(R_{i}\\right)\\right) h_{1}^{*}\\left(R_{i}\\right) \\mid \\mathcal{F}_{N}^{(-k)}\\right]=0\n$$\n\nfor each $i \\in \\mathcal{I}_{k}$; here $\\mathcal{F}_{N}^{(-k)}$ is the $\\sigma$-algebra generated by the observations outside fold $k$, and the preceding display holds since $\\hat{R}^{(-k)}+m^{*}-m^{(-k)} \\in \\mathcal{S}_{\\mathcal{M}}$ (recall $\\mathcal{S}_{\\mathcal{M}}$ is assumed to be a linear space, hence it is closed under multiplication by negative 1) but $h_{1}^{*} \\in \\mathcal{S}_{\\mathcal{M}}^{*}$. With the summands in $\\hat{a}_{1 k}$ i.i.d. across $i \\in \\mathcal{I}_{k}$ conditional on $\\mathcal{F}_{N}^{(-k)}$, we compute\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\hat{a}_{1 k}^{2} \\mid \\mathcal{F}_{N}^{(-k)}\\right] & =\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\int\\left(\\hat{R}^{(-k)}(r)+m^{*}(r)-\\hat{m}^{(-k)}(r)\\right)^{2}\\left(h_{1}^{*}(r)\\right)^{2} d P^{*}(w) \\\\\n& \\leqslant \\frac{C^{2}}{\\left|\\mathcal{I}_{k}\\right|}\\left\\|\\hat{R}^{(-k)}+m^{*}-\\hat{m}^{(-k)}\\right\\|_{2, P^{*}}^{2} \\text { by boundedness of } h_{1}^{*} \\\\\n& =o_{p}\\left(N^{-1}\\right)\n\\end{aligned}\n$$\n\nsince $\\left|\\mathcal{I}_{k}\\right|^{-1}=O\\left(N^{-1}\\right)$ while\n\n$$\n\\|\\hat{R}^{(-k)}+m^{*}-\\hat{m}^{(-k)}\\|_{2, P^{*}} \\leqslant\\left\\|\\hat{R}^{(-k)}\\right\\|_{2, P^{*}}+\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}}=o_{p}(1)\n$$\n\nby Minkowski's inequality and the rate conditions $\\left\\|\\hat{R}^{(-k)}\\right\\|_{2, P^{*}}=o_{p}(1)$ and $\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}}=$ $o_{p}(1)$. We conclude by Lemma 2 that $\\hat{a}_{1 k}=o_{p}\\left(N^{-1 / 2}\\right)$ and hence $a_{1 k}=o_{p}\\left(N^{-1 / 2}\\right)$ as well.\n\nNext, we show $a_{2 k}=o_{p}\\left(N^{-1 / 2}\\right)$. Note that $\\mathbb{E}\\left[Y_{i}-m^{*}\\left(R_{i}\\right) \\mid \\mathcal{F}_{N}^{(-k)}, \\mathcal{F}_{R}^{(k)}\\right]=0$ for all $i \\in$ $\\mathcal{I}_{k}$ where $\\mathcal{F}_{R}^{(k)}$ is the $\\sigma$-algebra generated by $\\left(R_{i}\\right)_{i \\in \\mathcal{I}_{k}}$. Additionally we have that the collection $\\left\\{Y_{i}-m^{*}\\left(R_{i}\\right)\\right\\}_{i \\in \\mathcal{I}_{k}}$ is conditionally independent given the $\\sigma$-algebra generated by $\\mathcal{F}_{N}^{(-k)}$ and $\\mathcal{F}_{R}^{(k)}$, and that $\\hat{h}_{1}^{(-k)}\\left(R_{i}\\right)-h_{1}^{*}\\left(R_{i}\\right)$ is measurable with respect to this $\\sigma$-algebra. Finally we have $\\mathbb{E}\\left[\\left(Y_{i}-m^{*}\\left(R_{i}\\right)\\right)^{2} \\mid \\mathcal{F}_{N}^{(-k)}, \\mathcal{F}_{R}^{(k)}\\right]=V^{*}\\left(R_{i}\\right) \\leqslant C$ by the boundedness condition. Thus with\n\n$$\n\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left(\\hat{h}_{1}^{(-k)}\\left(R_{i}\\right)-h_{1}^{*}\\left(R_{i}\\right)\\right)^{2}=o_{p}(1)\n$$\n\nby the rate conditions for the first component and Lemma 4, we conclude by Lemma 5 that $a_{2 k}=o_{p}\\left(N^{-1 / 2}\\right)$.\n\nTo show that $a_{3 k}=o_{p}\\left(N^{-1 / 2}\\right)$, simply note that\n\n$$\n\\begin{aligned}\n\\left|a_{3 k}\\right| & \\leqslant\\left(\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left(m^{*}\\left(R_{i}\\right)-\\hat{m}^{(-k)}\\left(R_{i}\\right)\\right)^{2}\\right)^{1 / 2} \\times\\left(\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left(\\hat{h}_{1}^{(-k)}\\left(R_{i}\\right)-h_{1}^{*}\\left(R_{i}\\right)\\right)^{2}\\right)^{1 / 2} \\\\\n& =o_{p}\\left(N^{-1 / 2}\\right)\n\\end{aligned}\n$$\n\nby Cauchy-Schwarz and the product rate condition $\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}}\\left\\|\\hat{h}_{1}^{(-k)}-h_{1}^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 2}\\right)$, in view of Lemma 4.\n\nFinally, let $\\mathcal{F}_{X}^{(k)}$ be the $\\sigma$-algebra generated by the observations $\\left\\{X_{i}\\right\\}_{i \\in \\mathcal{I}_{k}}$, so that\n\n$$\n\\mathbb{E}\\left[S_{i}\\left(Z_{i}-e^{*}\\left(X_{i}\\right)\\right) \\mid \\mathcal{F}_{N}^{(-k)}, \\mathcal{F}_{X}^{(k)}\\right]=0, \\quad \\forall i \\in \\mathcal{I}_{k}\n$$\n\nThe collection $\\left\\{S_{i}\\left(Z_{i}-e^{*}\\left(X_{i}\\right)\\right)\\right\\}_{i \\in \\mathcal{I}_{k}}$ is conditionally independent given the $\\sigma$-algebra generated by $\\mathcal{F}_{N}^{(-k)}$ and $\\mathcal{F}_{X}^{(k)}$, and $\\hat{h}_{2}^{(-k)}\\left(X_{i}\\right)-h_{2}^{*}\\left(X_{i}\\right)$ is measurable with respect to this $\\sigma$-algebra. Since\n\n$$\n\\mathbb{E}\\left[\\left(S_{i}\\left(Z_{i}-e^{*}\\left(X_{i}\\right)\\right)\\right)^{2} \\mid \\mathcal{F}_{N}^{(-k)}, \\mathcal{F}_{X}^{(k)}\\right] \\leqslant 1, \\quad \\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left(\\hat{h}_{2}^{(-k)}\\left(X_{i}\\right)-h_{2}^{*}\\left(X_{i}\\right)\\right)^{2}=o_{p}(1)\n$$\n\nby the rate conditions for the second component and Lemma 4, we conclude by Lemma 5 that $b_{k}=o_{p}\\left(N^{-1 / 2}\\right)$.", "tables": {}, "images": {}}, {"section_id": 20, "text": "# B Additional derivations of efficient influence functions \n\nHere we work through the three-step outline presented following the statement of Theorem 2 in the main text to derive EIF's in the models $\\mathcal{P}_{\\mathcal{M}_{1}}, \\mathcal{P}_{\\mathcal{M}_{2}}, \\mathcal{P}_{\\mathcal{M}_{3}}$, and $\\mathcal{P}_{\\mathcal{M}_{4}}$ corresponding to examples $1,2,3$, and 4 .", "tables": {}, "images": {}}, {"section_id": 21, "text": "## B. 1 Restricted moment model\n\nIn the restricted moment model $\\mathcal{P}_{\\mathcal{M}_{1}}$ of Example 1, under suitable regularity conditions we have by standard $Z$-estimation theory that the solution to\n\n$$\n\\frac{1}{N} \\sum_{i=1}^{N} a\\left(R_{i}\\right)\\left(Y_{i}-\\mu\\left(R_{i} ; \\beta\\right)\\right)=0\n$$\n\nis a RAL estimator for $\\beta \\in \\mathbb{R}^{q}$ for any $a \\in \\mathcal{H}_{R}^{q}$. The corresponding influence function is\n\n$$\n\\varphi_{0}\\left(w ; \\beta^{*}, \\eta^{*}\\right)=\\mathbb{E}\\left[a(r) D\\left(r ; \\beta^{*}\\right)^{\\top}\\right]^{-1} a(r)(y-\\mu(r ; \\beta))\n$$\n\nwhere $D(r ; b)=\\mu_{\\beta}(r ; b) \\equiv \\partial \\mu(r ; \\beta) /\\left.\\partial \\beta\\right|_{\\beta=b} \\in \\mathbb{R}^{q}$ (cf. Chapter 5 of Van der Vaart (2000)). In the case that $\\mu(r ; \\beta)=\\psi(r)^{\\top} \\beta$ is a linear model, taking $a(r)=\\psi(r)$ gives the ordinary least squares estimator. In any case, we can use $\\varphi_{0}$ as above for our initial influence function of an RAL estimator of $\\beta$, which we aim to project via (7) to derive the EIF for $\\beta$ in the model $\\mathcal{P}_{\\mathcal{M}_{1}}$.\n\nTo do so, we first compute the semiparametric tangent space $\\mathcal{S}_{\\mathcal{M}_{1}}$, similar to the example in Section 2.2. An arbitrary outcome mean function parametric submodel for $\\mathcal{M}_{1}$ evidently takes the form\n\n$$\nm(r ; \\gamma)=\\mu(r ; \\alpha(\\gamma))\n$$\n\nfor some smooth $\\alpha: \\mathbb{R}^{s} \\rightarrow \\mathbb{R}^{q}$ defined on a neighborhood of $0 \\in \\mathbb{R}^{s}$ with $\\alpha(0)=\\beta^{*}$. Then by the chain rule\n\n$$\nm_{\\gamma}(r ; 0)=J_{\\alpha}(0)^{\\top} D\\left(r ; \\beta^{*}\\right)\n$$\n\nfor $J_{\\alpha}(0) \\in \\mathbb{R}^{q \\times s}$ a Jacobian matrix. For any $c_{0} \\in \\mathbb{R}^{s}, c_{0}^{\\top} m_{\\gamma}(r ; 0)$ is of the form $r \\mapsto c^{\\top} D\\left(r ; \\beta^{*}\\right)$ for some $c \\in \\mathbb{R}^{q}$, which is evidently closed. Conversely, any function of this form clearly lies in the span of the derivative of the simple $q$-dimensional submodel $m(r ; \\gamma)=\\mu\\left(r ; \\beta^{*}+\\gamma\\right)$. It follows that\n\n$$\n\\mathcal{S}_{\\mathcal{M}_{1}}=\\left\\{r \\mapsto c^{\\top} D\\left(r ; \\beta^{*}\\right) \\mid c \\in \\mathbb{R}^{q}\\right\\}\n$$\n\nWe note in passing that for the linear model $\\mu(r ; \\beta)=\\psi(r)^{\\top} \\beta$ we can verify that indeed $\\mathcal{S}_{\\mathcal{M}_{1}}=\\mathcal{M}_{1}$, as shown by Proposition 2.\n\nTo derive the orthogonal complement $\\mathcal{S}_{\\mathcal{M}_{1}}^{\\perp}$, as suggested in Section 2.3 we solve for the set of functions $h \\in \\mathcal{H}_{R}$ for which the projection $g=\\Pi\\left(h ; \\mathcal{S}_{\\mathcal{M}_{1}}\\right)$ is equal to 0 . We have $g(r)=c_{h}^{\\top} D\\left(r ; \\beta^{*}\\right)$ where\n\n$$\nc_{h}=\\underset{c \\in \\mathbb{R}^{q}}{\\arg \\min } \\mathbb{E}\\left[\\left(h(R)-c^{\\top} D\\left(R ; \\beta^{*}\\right)\\right)^{2}\\right]\n$$\n\nTaking a derivative gives\n\n$$\nc_{h}=\\left(\\mathbb{E}\\left[D\\left(R ; \\beta^{*}\\right) D\\left(R ; \\beta^{*}\\right)^{\\top}\\right]\\right)^{-1} \\mathbb{E}[h(R) D\\left(R ; \\beta^{*}\\right)]\n$$\n\nso $g=0 \\Longleftrightarrow c_{h}=0 \\Longleftrightarrow \\mathbb{E}\\left[h(R) D\\left(R ; \\beta^{*}\\right)=0\\right]$. It follows that\n\n$$\n\\mathcal{S}_{\\mathcal{M}_{1}}^{\\perp}=\\left\\{h \\in \\mathcal{H}_{R} \\mid \\mathbb{E}\\left[h(R) D\\left(R ; \\beta^{*}\\right)\\right]=0\\right\\}\n$$\n\nand (by Theorem 2) that\n\n$$\n\\mathcal{T}_{\\mathcal{M}_{1}}^{\\perp}=\\left\\{w \\mapsto h(r)\\left(y-\\mu\\left(r ; \\beta^{*}\\right)\\right) \\mid \\mathbb{E}\\left[h(R) D\\left(R ; \\beta^{*}\\right)\\right]=0\\right\\}\n$$\n\nFinally, we compute the projection $g=\\Pi\\left(\\varphi_{0} ;\\left(\\mathcal{T}_{\\mathcal{M}_{1}}^{\\perp}\\right)^{q}\\right)$ to get the EIF via (7). By definition, $g(w)=\\left(y-\\mu\\left(r ; \\beta^{*}\\right)\\right) h_{0}(r)$ where\n\n$$\n\\begin{aligned}\nh_{0} & =\\underset{h \\in \\mathcal{H}_{R}^{q}: \\mathbb{E}\\left[h(R) D\\left(R ; \\beta^{*}\\right)^{\\top}\\right]=0}{\\arg \\min } \\mathbb{E}\\left[\\left(Y-\\mu\\left(R ; \\beta^{*}\\right)\\right)^{2} \\| A^{-1} a(R)-h(X) \\|^{2}\\right] \\\\\nA & =\\mathbb{E}\\left[a(R) D\\left(R ; \\beta^{*}\\right)^{\\top}\\right]\n\\end{aligned}\n$$\n\nTo compute this projection, we handle the constraint $\\mathbb{E}\\left[h(R) D\\left(R ; \\beta^{*}\\right)^{\\top}\\right]=0$ by introducing a Lagrange multiplier $\\Lambda \\in \\mathbb{R}^{q \\times q}$. Then we minimize the Lagrangian by conditioning on $R$ and minimizing pointwise:\n\n$$\n\\begin{aligned}\n& \\underset{h \\in \\mathcal{H}_{R}^{q}}{\\arg \\min } \\mathbb{E}\\left[\\left(Y-\\mu\\left(R ; \\beta^{*}\\right)\\right)^{2} \\| A^{-1} a(R)-h(R) \\|^{2}\\right]-2 \\operatorname{tr}\\left(\\Lambda^{\\top} \\mathbb{E}\\left[h(R) D\\left(R ; \\beta^{*}\\right)^{\\top}\\right]\\right) \\\\\n& \\quad=\\underset{h \\in \\mathcal{H}_{R}^{q}}{\\arg \\min } \\mathbb{E}\\left[V^{*}(R) \\| h(R) \\|^{2}-2\\left(V^{*}(R) a(R)^{\\top} A^{-1}+D\\left(R ; \\beta^{*}\\right)^{\\top} \\Lambda^{\\top}\\right) h(R)\\right]\n\\end{aligned}\n$$\n\nMinimizing the argument to the expectation pointwise gives\n\n$$\nh_{0}(r)=A^{-1} a(r)+\\frac{1}{V^{*}(r)} \\Lambda D\\left(r ; \\beta^{*}\\right)\n$$\n\nWe solve for the Lagrange multiplier $\\Lambda$ by plugging the preceding display into the matrix equations $\\mathbb{E}\\left[h_{0}(R) D\\left(R ; \\beta^{*}\\right)^{\\top}\\right]=0$, giving\n\n$$\n\\Lambda=-\\left(\\mathbb{E}\\left[\\frac{1}{V^{*}(R)} D\\left(R ; \\beta^{*}\\right) D\\left(R ; \\beta^{*}\\right)^{\\top}\\right]\\right)^{-1}\n$$\n\nThen the EIF is\n\n$$\n\\begin{aligned}\n\\varphi_{\\text {eff }}\\left(w ; \\beta^{*}, \\eta^{*}\\right) & =\\varphi_{0}\\left(w ; \\beta^{*}, \\eta^{*}\\right)-h_{0}(r)\\left(y-\\mu\\left(r ; \\beta^{*}\\right)\\right) \\\\\n& =\\frac{y-\\mu\\left(r ; \\beta^{*}\\right)}{V^{*}(r)}\\left(\\mathbb{E}\\left[\\frac{1}{V^{*}(R)} D\\left(R ; \\beta^{*}\\right) D\\left(R ; \\beta^{*}\\right)^{\\top}\\right]\\right)^{-1} D\\left(r ; \\beta^{*}\\right)\n\\end{aligned}\n$$\n\nwhich matches equation (4.55) of Tsiatis (2006).", "tables": {}, "images": {}}, {"section_id": 22, "text": "# B. 2 Mean-exchangeable controls \n\nWe derive EIF's in the model $\\mathcal{P}_{\\mathcal{M}_{2}}$. We can use $\\varphi_{0, \\text { obs }}$ and $\\varphi_{0, \\text { tgt }}$ as the initial influence functions $\\varphi_{0}$. By Proposition 2, we know $\\mathcal{S}_{\\mathcal{M}_{2}}=\\mathcal{M}_{2}$. It only remains to compute $\\mathcal{S}_{\\mathcal{M}_{2}}^{\\perp}$ and the projection (7).\n\nAs in the derivation of $\\mathcal{S}_{\\mathcal{M}_{2}}^{\\perp}$ in the main text, we characterize $\\mathcal{S}_{\\mathcal{M}_{2}}^{\\perp}$ as the set of all functions $h \\in$ $\\mathcal{H}_{R}$ for which $g=\\Pi\\left(h ; \\mathcal{S}_{\\mathcal{M}_{2}}\\right)=0$. To that end we write $h(r)=z h_{11}(x)+(1-s) h_{01}(x)+s(1-z) h_{10}(x)$ and\n\n$$\ng=z g_{1}(x)+(1-z) g_{0}(x)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\left(g_{0}, g_{1}\\right)= & \\underset{\\left(f_{0}, f_{1}\\right) \\in \\mathcal{H}_{X}^{2}}{\\arg \\min } \\mathbb{E}\\left[\\left(h(R)-Z f_{1}(X)-(1-Z) f_{0}(X)\\right)^{2}\\right] \\\\\n= & \\underset{\\left(f_{0}, f_{1}\\right) \\in \\mathcal{H}_{X}^{2}}{\\arg \\min } \\mathbb{E}\\left[Z\\left(h_{11}(X)-f_{1}(X)\\right)^{2}\\right] \\\\\n& +\\mathbb{E}\\left[(1-S)\\left(h_{01}(X)-f_{0}(X)\\right)^{2}+S(1-Z)\\left(h_{10}(X)-f_{0}(X)\\right)^{2}\\right] \\\\\n= & \\underset{\\left(f_{0}, f_{1}\\right) \\in \\mathcal{H}_{X}^{2}}{\\arg \\min } \\mathbb{E}\\left[p^{*}(X) e^{*}(X)\\left(h_{11}(X)-f_{1}(X)\\right)^{2}+\\left(1-p^{*}(X)\\right)\\left(h_{01}(X)-f_{0}(X)\\right)^{2}\\right] \\\\\n& +\\mathbb{E}\\left[p^{*}(X)\\left(1-e^{*}(X)\\right)\\left(h_{10}(X)-f_{0}(X)\\right)^{2}\\right]\n\\end{aligned}\n$$\n\nThe final equality follows by conditioning on $X$, and expresses the objective as the expectation of a function of $X$. Without any restrictions on $f_{0}$ and $f_{1}$ we can perform the minimization by minimizing the argument of the expectation pointwise in $x$. Writing the first order conditions, we find they are satisfied by $f_{0}=f_{1}=0$ if and only if\n\n$$\n\\begin{aligned}\np^{*}(x) e^{*}(x) h_{11}(x) & =0 \\\\\np^{*}(x)\\left(1-e^{*}(x)\\right) h_{10}(x)+\\left(1-p^{*}(x)\\right) h_{00}(x) & =0\n\\end{aligned}\n$$\n\nfor all $x \\in \\mathcal{X}$. Hence $h \\in \\mathcal{S}_{\\mathcal{M}_{2}}^{\\perp}$ if and only if $h_{11}(x)=0$ and $h_{10}(x)=-\\frac{1-p^{*}(x)}{p^{*}(x)\\left(1-e^{*}(x)\\right)} h_{00}(x)$, with $h_{00} \\in \\mathcal{H}_{X}$ free to vary. In other words\n\n$$\n\\mathcal{S}_{\\mathcal{M}_{2}}^{\\perp}=\\left\\{r \\mapsto\\left[(1-s)-s(1-z) \\frac{1-p^{*}(x)}{p^{*}(x)\\left(1-e^{*}(x)\\right)}\\right] \\zeta(x) \\mid \\zeta \\in \\mathcal{H}_{X}\\right\\}\n$$\n\nand so by Theorem 2\n\n$$\n\\mathcal{T}_{\\mathcal{M}_{2}}^{\\perp}=\\left\\{w \\mapsto\\left[(1-s)-s(1-z) \\frac{1-p^{*}(x)}{p^{*}(x)\\left(1-e^{*}(x)\\right)}\\right] \\zeta(x)\\left(y-m^{*}(r)\\right) \\mid \\zeta \\in \\mathcal{H}_{X}\\right\\}\n$$\n\nNow we compute the projection $g=\\Pi\\left(\\varphi_{0} ; \\mathcal{T}_{\\mathcal{M}_{2}}^{\\perp}\\right)$ from (7) in terms of $\\varphi_{0}$. We write\n\n$$\ng(w)=\\left[(1-s)-s(1-z) \\frac{1-p^{*}(x)}{p^{*}(x)\\left(1-e^{*}(x)\\right)}\\right] \\zeta(x)\\left(y-m^{*}(r)\\right)\n$$\n\nwhere\n\n$$\n\\zeta=\\underset{h \\in \\mathcal{H}_{X}}{\\arg \\min } \\mathbb{E}\\left[\\left(\\varphi_{0, \\text { obs }}(W)-\\left[(1-S)-S(1-Z) \\frac{1-p^{*}(X)}{p^{*}(X)\\left(1-e^{*}(X)\\right)}\\right] h(X)\\left(Y-m^{*}(R)\\right)\\right)^{2}\\right]\n$$\n\nBy our usual approach of rewriting the objective as the expectation of a function of $X$ by iterated expectations and then minimizing the argument of this expectation pointwise, we get\n\n$$\n\\zeta(x)=\\frac{\\mathbb{E}\\left[\\varphi_{0}(W)\\left((1-S)-S(1-Z) \\frac{1-p^{*}(X)}{p^{*}(X)\\left(1-e^{*}(X)\\right)}\\right)\\left(Y-m^{*}(R)\\right) \\mid X=x\\right]}{\\left(1-p^{*}(x)\\right) V_{00}^{*}(x)+\\frac{\\left(1-p^{*}(x)\\right)^{2}}{p^{*}(x)\\left(1-e^{*}(x)\\right)} V_{10}^{*}(x)}\n$$\n\nFor $\\varphi_{0}=\\varphi_{0, \\text { obs }}$ we observe\n\n$$\n\\mathbb{E}\\left[\\varphi_{0}\\left(W ; \\tau_{\\text {obs }}^{*}, \\eta^{*}\\right)(1-S)\\left(Y-m^{*}(R)\\right) \\mid X\\right]=0\n$$\n\nwhile\n\n$$\n\\mathbb{E}\\left[-\\varphi_{0}\\left(W ; \\tau_{\\mathrm{obs}}^{*}, \\eta^{*}\\right) S(1-Z) \\frac{1-p^{*}(X)}{p^{*}(X)\\left(1-e^{*}(X)\\right)}\\left(Y-m^{*}(R)\\right) \\mid X\\right]=\\frac{\\left(1-p^{*}(X)\\right)^{2} V_{10}^{*}(X)}{p^{*}(X)\\left(1-e^{*}(X)\\right)\\left(1-\\rho^{*}\\right)}\n$$\n\nHence\n\n$$\n\\zeta(x)=\\frac{\\left(1-p^{*}(x)\\right) V_{10}^{*}(x)}{\\left(1-\\rho^{*}\\right)\\left[V_{00}^{*}(x) p^{*}(x)\\left(1-e^{*}(x)\\right)+V_{10}^{*}(x)\\left(1-p^{*}(x)\\right)\\right]}\n$$\n\nand then the EIF for $\\tau_{\\text {obs }}$ is\n\n$$\n\\varphi_{\\mathrm{eff}}\\left(w ; \\tau_{\\mathrm{obs}}^{*}, \\eta^{*}\\right)=\\varphi_{0, \\mathrm{obs}}\\left(w ; \\tau_{\\mathrm{obs}}^{*}, \\eta^{*}\\right)-\\left[(1-s)-s(1-z) \\frac{1-p^{*}(x)}{p^{*}(x)\\left(1-e^{*}(x)\\right)}\\right] \\zeta(x)\\left(y-m^{*}(z, x)\\right)\n$$\n\nwhich after algebraic simplification matches the expression given in Proposition 3 of Li et al. (2023b).\n\nRepeating the calculation in the preceding paragraph with $\\varphi_{0}=\\varphi_{0, \\text { tgt }}$ gives the efficiency bound for $\\tau_{\\text {tgt }}$, also given in Proposition 3 of Li et al. (2023b).", "tables": {}, "images": {}}, {"section_id": 23, "text": "# B. 3 Parametric confounding bias and CATE \n\nFollowing our three-step outline, we derive EIF's in the model $\\mathcal{P}_{\\mathcal{M}_{3}}$ in terms of an arbitrary influence function $\\varphi_{0}=\\varphi_{0}(\\cdot, \\tau, \\eta)$ for an RAL estimator of some generic pathwise-differentiable estimand $\\tau$. We later specialize to the estimand $\\tau_{\\text {obs }}$ using $\\varphi_{0}=\\varphi_{0, \\text { obs }}$, which verifies Theorem 4 of Yang et al. (2024).\n\nWe begin by deriving the outcome mean function tangent space $\\mathcal{S}_{\\mathcal{M}_{3}}$. The method to do so is similar to that of Appendix B.1. In words, the model $\\mathcal{P}_{\\mathcal{M}_{3}}$ allows the control level $(z=0)$ of the outcome mean function to be an arbitrary function of $(s, x)$. Then given the control level, the treatment level differs by $\\mu(x ; \\beta)$ from the control level when $s=1$, and by $\\mu(x ; \\beta)+\\phi(x ; \\theta)$ from the control level when $s=0$. Formalizing this mathematically, this means that an arbitrary outcome mean function parametric submodel of $\\mathcal{M}_{3}$ must take the form\n\n$$\nm(r ; \\gamma)=f_{0}\\left(s, x ; \\gamma_{0}\\right)+z\\left[\\mu\\left(x ; \\beta\\left(\\gamma_{1}\\right)\\right)+(1-s) \\phi\\left(x ; \\theta\\left(\\gamma_{2}\\right)\\right)\\right]\n$$\n\nfor some smooth mappings $\\beta: \\mathbb{R}^{s_{1}} \\rightarrow \\mathbb{R}^{p}$ and $\\theta: \\mathbb{R}^{s_{2}} \\rightarrow \\mathbb{R}^{q}$, where $\\gamma=\\left(\\gamma_{0}^{\\top}, \\gamma_{1}^{\\top}, \\gamma_{2}^{\\top}\\right)^{\\top}$ and $\\gamma_{i} \\in \\mathbb{R}^{s_{i}}$, $i=0,1,2$. Then using the subscript notation for (partial) derivatives from Example 5 in Section 2 of the main text, for arbitrary $c=\\left(c_{0}^{\\top}, c_{1}^{\\top}, c_{2}^{\\top}\\right)^{\\top}$ partitioned in the same way as $\\gamma$, we have\n\n$$\nc^{\\top} m_{\\gamma}(r ; 0)=c_{0}^{\\top} f_{0}^{\\prime}(s, x ; 0)+z c_{1}^{\\top} J_{\\beta}(0)^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)+z(1-s) c_{2}^{\\top} J_{\\theta}(0)^{\\top} D_{\\phi}\\left(x ; \\theta^{*}\\right)\n$$\n\nwhere $J_{\\beta}(0) \\in \\mathbb{R}^{p \\times s_{1}}$ and $J_{\\theta}(0) \\in \\mathbb{R}^{q \\times s_{2}}$ are the Jacobian matrices for the mappings $\\beta$ and $\\theta$ at 0 , and\n\n$$\nD_{\\mu}\\left(x ; \\beta^{*}\\right)=\\left.\\frac{\\partial \\mu(x ; \\beta)}{\\partial \\beta}\\right|_{\\beta=\\beta^{*}}, \\quad D_{\\phi}\\left(x ; \\theta^{*}\\right)=\\left.\\frac{\\partial \\phi(x ; \\theta)}{\\partial \\theta}\\right|_{\\theta=\\theta^{*}}\n$$\n\nThis is of the form\n\n$$\nf(s, x)+z\\left[c_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)+(1-s) c_{2}^{\\top} D_{\\phi}\\left(x ; \\theta^{*}\\right)\\right]\n$$\n\nfor some $f \\in \\mathcal{H}_{S X}, c_{1} \\in \\mathbb{R}^{p}, c_{2} \\in \\mathbb{R}^{q}$ (note we have overloaded the notation for $c_{1}$ and $c_{2}$ ). Conversely, we now show that any function of the form (50), for some $f \\in \\mathcal{H}_{S X}, c_{1} \\in \\mathbb{R}^{p}, c_{2} \\in \\mathbb{R}^{q}$, lies in $\\mathcal{S}_{\\mathcal{M}_{3}}$. To that end, consider the submodel of the form (49) with $s_{0}=1, s_{1}=p, s_{2}=q$ and $f(s, x)=m^{*}(s, 0, x)+\\gamma_{0} f(s, x), \\beta\\left(\\gamma_{1}\\right)=\\beta^{*}+\\gamma_{1}$, and $\\theta\\left(\\gamma_{2}\\right)=\\theta^{*}+\\gamma_{2}$. We observe that our function\n\ngiven by (50) is equivalent to $c^{\\top} m_{\\gamma}(r ; 0)$ for $c=\\left(1, c_{1}^{\\top}, c_{2}^{\\top}\\right)$, and thus lies in $\\mathcal{S}_{\\mathcal{M}_{3}}$ by definition. We conclude that indeed,\n\n$$\n\\mathcal{S}_{\\mathcal{M}_{3}}=\\left\\{r \\mapsto f(s, x)+z\\left[c_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)+(1-s) c_{2}^{\\top} D_{\\phi}\\left(x ; \\theta^{*}\\right)\\right] \\mid f \\in \\mathcal{H}_{S X}, c_{1} \\in \\mathbb{R}^{p}, c_{2} \\in \\mathbb{R}^{q}\\right\\}\n$$\n\nIt remains to compute the orthogonal complement $\\mathcal{S}_{\\mathcal{M}_{3}}^{\\perp}$ and then the projection (7). As in the other examples, we compute $\\mathcal{S}_{\\mathcal{M}_{3}}^{\\perp}$ as the set of all functions $h \\in \\mathcal{H}_{R}$ for which $g=\\Pi\\left(h ; \\mathcal{S}_{\\mathcal{M}_{3}}\\right)=0$. Given $h$ we write\n\n$$\ng(r)=f_{0}(s, x)+z\\left[d_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)+(1-s) d_{2}^{\\top} D_{\\phi}\\left(x ; \\theta^{*}\\right)\\right]\n$$\n\nwhere\n\n$$\n\\left(f_{0}, d_{1}, d_{2}\\right)=\\underset{f \\in \\mathcal{H}_{S X}, c_{1} \\in \\mathbb{R}^{p}, c_{2} \\in \\mathbb{R}^{q}}{\\arg \\min } \\mathbb{E}\\left[\\left(h(R)-f(S, X)-Z\\left[c_{1}^{\\top} D_{\\mu}\\left(X ; \\beta^{*}\\right)+(1-S) c_{2}^{\\top} D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right]\\right)^{2}\\right]\n$$\n\nAs we have seen, we can solve this minimization by rewriting the objective as the expectation of a function of solely $X$ via iterated expectations:\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left(h(R)-f(S, X)-Z\\left[c_{1}^{\\top} D_{\\mu}\\left(X ; \\beta^{*}\\right)+(1-S) c_{2}^{\\top} D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right]\\right)^{2}\\right] \\\\\n& \\quad=\\mathbb{E}\\left[p^{*}(X) e^{*}(X)\\left(h_{11}(X)-f(1, X)-c_{1}^{\\top} D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right)^{2}\\right]+\\mathbb{E}\\left[p^{*}(X)\\left(1-e^{*}(X)\\right)\\left(h_{10}(X)-f(1, X)\\right)^{2}\\right] \\\\\n& \\quad+\\mathbb{E}\\left[\\left(1-p^{*}(X)\\right) q^{*}(X)\\left(h_{01}(X)-f(0, X)-c_{1}^{\\top} D_{\\mu}\\left(X ; \\beta^{*}\\right)-c_{2}^{\\top} D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right)^{2}\\right] \\\\\n& \\quad+\\mathbb{E}\\left[\\left(1-p^{*}(X)\\right)\\left(1-q^{*}(X)\\right)\\left(h_{00}(X)-f(0, X)\\right)^{2}\\right]\n\\end{aligned}\n$$\n\nFor fixed $c_{1}$ and $c_{2}$, the minimizer of this objective in $f \\in \\mathcal{H}_{S X}$ takes the form $f(s, x)=s f(1, x)+$ $(1-s) f(0, x)$ where $f(1, x)$ and $f(0, x)$ globally minimize the argument to the expectation, i.e. they satisfy the first-order conditions\n\n$$\n\\begin{aligned}\n0= & p^{*}(x) e^{*}(x)\\left(h_{11}(x)-f(1, x)-c_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)\\right)+p^{*}(x)\\left(1-e^{*}(x)\\right)\\left(h_{10}(x)-f(1, x)\\right)^{2} \\\\\n0= & \\left(1-p^{*}(x)\\right) q^{*}(x)\\left(h_{01}(x)-f(0, x)-c_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)-c_{2}^{\\top} D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right) \\\\\n& +\\left(1-p^{*}(x)\\right)\\left(1-q^{*}(x)\\right)\\left(h_{00}(x)-f(0, x)\\right)\n\\end{aligned}\n$$\n\nConversely for any fixed $f \\in \\mathcal{S}_{S X}$, taking the derivative of the objective shows that the minimizers $c_{1} \\in \\mathbb{R}^{p}$ and $c_{2} \\in \\mathbb{R}^{q}$ must satisfy the first order conditions\n\n$$\n\\begin{aligned}\n0= & \\mathbb{E}\\left[p^{*}(X) e^{*}(X)\\left(h_{11}(X)-f_{1}(X)-c_{1}^{\\top} D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right) D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right] \\\\\n& +\\mathbb{E}\\left[\\left(1-p^{*}(X)\\right) q^{*}(X)\\left(h_{01}(X)-f(0, X)-c_{1}^{\\top} D_{\\mu}\\left(X ; \\beta^{*}\\right)-c_{2}^{\\top} D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right) D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right] \\\\\n0= & \\mathbb{E}\\left[\\left(1-p^{*}(X)\\right) q^{*}(X)\\left(h_{01}(X)-f(0, X)-c_{1}^{\\top} D_{\\mu}\\left(X ; \\beta^{*}\\right)-c_{2}^{\\top} D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right) D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right]\n\\end{aligned}\n$$\n\nThe four first-order conditions are satisfied with $f(0, x)=f(1, x)=0$ and $c_{1}=c_{2}=0$ if and only if\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left[p^{*}(X) e^{*}(X) h_{11}(X)+\\left(1-p^{*}(X)\\right) q^{*}(X) h_{01}(X)\\right] D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right] & =0 \\\\\n\\mathbb{E}\\left[\\left(1-p^{*}(X)\\right) q^{*}(X) h_{01}(X) D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right] & =0 \\\\\ne^{*}(x) h_{11}(x)+\\left(1-e^{*}(x)\\right) h_{10}(x) & =0 \\\\\nq^{*}(x) h_{01}(x)+\\left(1-q^{*}(x)\\right) h_{00}(x) & =0\n\\end{aligned}\n$$\n\nThus $\\mathcal{S}_{\\mathcal{M}_{3}}^{\\perp}$ is the set of all functions $h$ satisfying (52), (53), (54), and (55). Recognizing that (54) and (55) determine $h_{00}$ and $h_{10}$ in terms of $h_{11}$ and $h_{01}$, respectively, which are free to vary other\n\nthan the moment constraints given by (52) and (53), we can compactly write $\\mathcal{S}_{\\mathcal{M}_{3}}^{j}$ as the set of all mappings\n\n$$\nr \\mapsto f(s, x)\\left[z-(1-z)\\left(\\frac{s e^{*}(x)}{1-e^{*}(x)}+\\frac{(1-s) q^{*}(x)}{1-q^{*}(x)}\\right)\\right]\n$$\n\nsuch that $f \\in \\mathcal{H}_{S X}$ with\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left(p^{*}(X) e^{*}(X) f(1, X)+\\left(1-p^{*}(X)\\right) q^{*}(X) f(0, X)\\right) D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right] & =0 \\quad \\text { and } \\\\\n\\mathbb{E}\\left[\\left(1-p^{*}(X)\\right) q^{*}(X) f(1, X) D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right] & =0\n\\end{aligned}\n$$\n\nFinally, we compute the projection (7). Given the initial influence function $\\varphi_{0}$, we have by the form of $\\mathcal{S}_{\\mathcal{M}_{3}}^{j}$ derived in the previous paragraph and Theorem 2 that\n\n$$\n\\pi\\left(\\varphi_{0} ; \\mathcal{T}_{\\mathcal{M}_{3}}^{\\perp}\\right)=w \\mapsto\\left(s \\Gamma_{1}^{*}(x)+(1-s) \\Gamma_{0}^{*}(x)\\right)\\left(z-(1-z)\\left[\\frac{s e^{*}(x)}{1-e^{*}(x)}+\\frac{(1-s) q^{*}(x)}{1-q^{*}(x)}\\right]\\right)\\left(y-m^{*}(r)\\right)\n$$\n\nwhere $\\Gamma_{0}^{*}$ and $\\Gamma_{1}^{*}$ solve the optimization problem\n\n$$\n\\min _{\\Gamma_{0} \\in \\mathcal{H}_{X}, \\Gamma_{1} \\in \\mathcal{H}_{X}} \\mathbb{E}\\left[\\left(\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)-\\left[S \\Gamma_{1}(X)+(1-S) \\Gamma_{0}(X)\\right] \\nu^{*}(R)\\right)^{2}\\right]\n$$\n\ns.t. $\\mathbb{E}\\left[\\left[p^{*}(X) e^{*}(X) \\Gamma_{1}(X)+\\left(1-p^{*}(X)\\right) q^{*}(X) \\Gamma_{0}(X)\\right] D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right]=0, \\quad$ and $\\mathbb{E}\\left[\\left(1-p^{*}(X)\\right) q^{*}(X) \\Gamma_{0}(X) D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right]=0$\nwith\n\n$$\n\\nu^{*}(r)=\\nu\\left(r ; \\eta^{*}\\right) \\text { for } \\nu(r ; \\eta)=\\left[z-(1-z)\\left(\\frac{s e^{*}(x)}{1-e^{*}(x)}+\\frac{(1-s) q^{*}(x)}{1-q^{*}(x)}\\right)\\right]\\left(y-m^{*}(r)\\right)\n$$\n\nOnce again, we rewrite the objective by conditioning on $X$ :\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left(\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)-\\left(S \\Gamma_{1}(X)+(1-S) \\Gamma_{0}(X)\\right) \\nu^{*}(R)\\right)^{2}\\right] \\\\\n& =\\mathbb{E}\\left[\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)^{2} \\mid X\\right]+\\mathbb{E}\\left[\\Gamma_{1}^{2}(X) p^{*}(X) e^{*}(X)\\left(V_{11}^{*}(X)+\\frac{e^{*}(X)}{1-e^{*}(X)} V_{10}^{*}(X)\\right)\\right] \\\\\n& \\quad+\\mathbb{E}\\left[\\Gamma_{0}^{2}(X)\\left(1-p^{*}(X)\\right) q^{*}(X)\\left(V_{01}^{*}(X)+\\frac{q^{*}(X)}{1-q^{*}(X)} V_{00}^{*}(X)\\right)\\right]-2 \\mathbb{E}\\left[\\mathcal{J}\\left(X ; \\Gamma_{0}, \\Gamma_{1}, \\eta, \\tau, \\varphi_{0}\\right)\\right]\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\mathcal{J}\\left(x ; \\Gamma_{0}, \\Gamma_{1}, \\eta, \\tau, \\varphi_{0}\\right)=\\mathbb{E}\\left[\\left(S \\Gamma_{1}(X)+(1-S) \\Gamma_{0}(X)\\right) \\nu^{*}(R) \\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right) \\mid X=x\\right]\n$$\n\nTo deal with the two moment constraints, we introduce Lagrange multipliers $\\Lambda_{1} \\in \\mathbb{R}^{p}$ and $\\Lambda_{2} \\in \\mathbb{R}^{q}$, similar to Appendix B.1. Dropping the constant $\\mathbb{E}\\left[\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)^{2} \\mid X=x\\right]$ term from the objective, the Lagrangian is $\\mathbb{E}[\\mathcal{L}(X)]$ where\n\n$$\n\\begin{aligned}\n\\mathcal{L}(x)= & \\Gamma_{1}^{2}(x) p^{x}(x) e^{x}(x)\\left(V_{11}^{*}(x)+\\frac{e^{*}(x)}{1-e^{*}(x)} V_{10}^{*}(x)\\right) \\\\\n& +\\Gamma_{0}^{2}(x)\\left(1-p^{*}(x)\\right) q^{*}(x)\\left(V_{01}^{*}(x)+\\frac{q^{*}(x)}{1-q^{*}(x)} V_{00}^{*}(x)\\right) \\\\\n& -2 \\mathcal{J}\\left(x ; \\Gamma_{0}, \\Gamma_{1}, \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right) \\\\\n& -2 \\Lambda_{1}^{\\top}\\left[p^{*}(x) e^{*}(x) \\Gamma_{1}(x)+\\left(1-p^{*}(x)\\right) q^{*}(x) \\Gamma_{0}(x)\\right] D_{\\mu}\\left(x ; \\beta^{*}\\right) \\\\\n& -2 \\Lambda_{2}^{\\top}\\left[\\left(1-p^{*}(x)\\right) q^{*}(x) \\Gamma_{0}(x) D_{\\phi}\\left(x ; \\theta^{*}\\right)\\right]\n\\end{aligned}\n$$\n\nWe can minimize the Lagrangian over $\\Gamma_{0} \\in \\mathcal{H}_{X}$ and $\\Gamma_{1} \\in \\mathcal{H}_{X}$ by minimizing $\\mathcal{L}(x)$ pointwise, giving\n\n$$\n\\begin{aligned}\n\\Gamma_{0}^{*}(x)= & \\left(1-q^{*}(x)\\right)\\left(\\frac{\\Lambda_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)+\\Lambda_{2}^{\\top} D_{\\phi}\\left(x ; \\theta^{*}\\right)}{\\left(1-q^{*}(x)\\right) V_{01}^{*}(x)+q^{*}(x) V_{00}^{*}(x)}\\right) \\\\\n& +\\frac{\\left(1-q^{*}(x)\\right) \\mathcal{J}_{0}\\left(x ; \\eta, \\tau, \\varphi_{0}\\right)}{\\left(1-p^{*}(x)\\right) q^{*}(x)\\left[\\left(1-q^{*}(x)\\right) V_{01}^{*}(x)+q^{*}(x) V_{00}^{*}(x)\\right]} \\\\\n\\Gamma_{1}^{*}(x)= & \\left(1-e^{*}(x)\\right)\\left(\\frac{\\Lambda_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)}{\\left(1-e^{*}(x)\\right) V_{11}^{*}(x)+e^{*}(x) V_{10}^{*}(x)}\\right) \\\\\n& +\\frac{\\left(1-e^{*}(x)\\right) \\mathcal{J}_{1}\\left(x ; \\eta, \\tau, \\varphi_{0}\\right)}{p^{*}(x) e^{*}(x)\\left[\\left(1-e^{*}(x)\\right) V_{11}^{*}(x)+e^{*}(x) V_{10}^{*}(x)\\right]}\n\\end{aligned}\n$$\n\nwhere the Lagrange multipliers can be solved for by plugging back into the moment constraints and\n\n$$\n\\begin{aligned}\n& \\mathcal{J}_{1}\\left(x ; \\eta, \\tau, \\varphi_{0}\\right)=\\mathbb{E}\\left[S\\left(Z-(1-Z) \\frac{e^{*}(X)}{1-e^{*}(X)}\\right)\\left(Y-m^{*}(R)\\right) \\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right) \\mid X=x\\right] \\\\\n& \\mathcal{J}_{0}\\left(x ; \\eta, \\tau, \\varphi_{0}\\right)=\\mathbb{E}\\left[(1-S)\\left(Z-(1-Z) \\frac{q^{*}(X)}{1-q^{*}(X)}\\right)\\left(Y-m^{*}(R)\\right) \\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right) \\mid X=x\\right]\n\\end{aligned}\n$$\n\nwhich come from writing\n\n$$\n\\mathcal{J}\\left(x ; \\eta, \\tau, \\varphi_{0}\\right)=\\mathcal{J}_{1}\\left(x ; \\eta, \\tau, \\varphi_{0}\\right) \\Gamma_{1}(x)+\\mathcal{J}_{1}\\left(x ; \\eta, \\tau, \\varphi_{0}\\right) \\Gamma_{0}(x)\n$$\n\nThis shows that $\\Lambda_{1}$ and $\\Lambda_{2}$ solve the linear matrix system\n\n$$\n\\begin{aligned}\nA_{\\mu \\mu} \\Lambda_{1}+A_{\\mu \\phi} \\Lambda_{2} & =-b_{\\mu} \\\\\nA_{\\mu \\phi}^{\\top}+A_{\\phi \\phi} \\Lambda_{2} & =-b_{\\phi}\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\nA_{\\mu \\mu}= & \\mathbb{E}\\left[\\frac{p^{*}(X) e^{*}(X)\\left(1-e^{*}(X)\\right)}{\\left(1-e^{*}(X)\\right) V_{11}^{*}(X)+e^{*}(X) V_{10}^{*}(X)} D_{\\mu}\\left(X ; \\beta^{*}\\right) D_{\\mu}\\left(X ; \\beta^{*}\\right)^{\\top}\\right] \\\\\n& +\\mathbb{E}\\left[\\frac{\\left(1-p^{*}(X)\\right) q^{*}(X)\\left(1-q^{*}(X)\\right)}{\\left(1-q^{*}(X)\\right) V_{01}^{*}(X)+q^{*}(X) V_{00}^{*}(X)} D_{\\mu}\\left(X ; \\beta^{*}\\right) D_{\\mu}\\left(X ; \\beta^{*}\\right)^{\\top}\\right] \\in \\mathbb{R}^{p \\times p} \\\\\nA_{\\mu \\phi}= & \\mathbb{E}\\left[\\frac{\\left(1-p^{*}(X)\\right) q^{*}(X)\\left(1-q^{*}(X)\\right)}{\\left(1-q^{*}(X)\\right) V_{01}^{*}(X)+q^{*}(X) V_{00}^{*}(X)} D_{\\mu}\\left(X ; \\beta^{*}\\right) D_{\\phi}\\left(X ; \\theta^{*}\\right)^{\\top}\\right] \\in \\mathbb{R}^{p \\times q} \\\\\nA_{\\phi \\phi}= & \\mathbb{E}\\left[\\frac{\\left(1-p^{*}(X)\\right) q^{*}(X)\\left(1-q^{*}(X)\\right)}{\\left(1-q^{*}(X)\\right) V_{01}^{*}(X)+q^{*}(X) V_{00}^{*}(X)} D_{\\phi}\\left(X ; \\theta^{*}\\right) D_{\\phi}\\left(X ; \\theta^{*}\\right)^{\\top}\\right] \\in \\mathbb{R}^{q \\times q} \\\\\nb_{\\mu}= & \\mathbb{E}\\left[\\frac{\\left(1-e^{*}(X)\\right) \\mathcal{J}_{1}\\left(X ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)}{\\left(1-e^{*}(X)\\right) V_{11}^{*}(X)+e^{*}(X) V_{10}^{*}(X)} D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right] \\\\\n& +\\mathbb{E}\\left[\\frac{\\left(1-q^{*}(X)\\right) \\mathcal{J}_{0}\\left(X ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)}{\\left(1-q^{*}(X)\\right) V_{01}^{*}(X)+q^{*}(X) V_{00}^{*}(X)} D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right] \\in \\mathbb{R}^{p} \\\\\nb_{\\phi}= & \\mathbb{E}\\left[\\frac{\\left(1-q^{*}(X)\\right) \\mathcal{J}_{0}\\left(X ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)}{\\left(1-q^{*}(X)\\right) V_{01}^{*}(X)+q^{*}(X) V_{00}^{*}(X)} D_{\\phi}\\left(X ; \\theta^{*}\\right)\\right] \\in \\mathbb{R}^{q}\n\\end{aligned}\n$$\n\nTaking $\\varphi_{0}=\\varphi_{0, \\text { obs }}$ to give a more explicit EIF for $\\tau_{\\text {obs }}$, we can easily compute\n\n$$\n\\begin{aligned}\n& \\mathcal{J}_{0}\\left(x ; \\eta, \\tau, \\varphi_{0, \\text { obs }}\\right)=0 \\\\\n& \\mathcal{J}_{1}\\left(x ; \\eta, \\tau, \\varphi_{0, \\text { obs }}\\right)=\\frac{1-p^{*}(x)}{1-\\rho^{*}}\\left(V_{11}^{*}(x)+\\frac{e^{*}(x)}{1-e^{*}(x)} V_{10}^{*}(x)\\right)\n\\end{aligned}\n$$\n\nThen some of the other expressions simplify:\n\n$$\n\\begin{aligned}\nb_{\\phi} & =0 \\\\\nb_{\\mu} & =\\mathbb{E}\\left[\\frac{1-p^{*}(X)}{1-\\rho^{*}} D_{\\mu}\\left(X ; \\beta^{*}\\right)\\right]=\\mathbb{E}\\left[D_{\\mu}\\left(X ; \\beta^{*}\\right) \\mid S=0\\right] \\\\\n\\Gamma_{0}^{*}(x) & =\\left(1-q^{*}(x)\\right)\\left(\\frac{\\Lambda_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)+\\Lambda_{2}^{\\top} D_{\\phi}\\left(x ; \\theta^{*}\\right)}{\\left(1-q^{*}(x)\\right) V_{01}^{*}(x)+q^{*}(x) V_{00}^{*}(x)}\\right) \\\\\n\\Gamma_{1}^{*}(x) & =\\left(1-e^{*}(x)\\right)\\left(\\frac{\\Lambda_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)}{\\left(1-e^{*}(x)\\right) V_{11}^{*}(x)+e^{*}(x) V_{10}^{*}(x)}\\right)+\\frac{\\left(1-p^{*}(x)\\right)}{\\left(1-\\rho^{*}\\right) p^{*}(x) e^{*}(x)}\n\\end{aligned}\n$$\n\nSolving the matrix equations for the Lagrange multipliers then gives\n\n$$\n\\begin{aligned}\n\\Lambda_{1} & =-\\left(A_{\\mu \\mu}-A_{\\mu \\phi} A_{\\phi \\phi}^{-1} A_{\\mu \\phi}^{\\top}\\right)^{-1} b_{\\mu} \\\\\n\\Lambda_{2} & =-A_{\\phi \\phi}^{-1} A_{\\mu \\mu}^{\\top} \\Lambda_{1}=A_{\\phi \\phi}^{-1} A_{\\mu \\mu}^{\\top}\\left(A_{\\mu \\mu}-A_{\\mu \\phi} A_{\\phi \\phi}^{-1} A_{\\mu \\phi}^{\\top}\\right)^{-1} b_{\\mu}\n\\end{aligned}\n$$\n\nPutting everything together, the EIF for $\\tau_{\\text {obs }}$ in the model $\\mathcal{P}_{\\mathcal{M}_{3}}$ is given by\n\n$$\n\\begin{aligned}\n& \\varphi_{\\text {eff,obs }}^{(3)}\\left(w ; \\tau^{*}, \\eta^{*}\\right)=\\varphi_{0, \\text { obs }}\\left(w ; \\tau^{*}, \\eta^{*}\\right)- \\\\\n& \\quad\\left(s \\Gamma_{1}^{*}(x)+(1-s) \\Gamma_{0}^{*}(x)\\right)\\left(z-(1-z)\\left[\\frac{s e^{*}(x)}{1-e^{*}(x)}+\\frac{(1-s) q^{*}(x)}{1-q^{*}(x)}\\right]\\right)\\left(y-m^{*}(r)\\right) \\\\\n& =\\frac{1-s}{1-\\rho^{*}}\\left(m_{11}^{*}(x)-m_{10}^{*}(x)-\\tau_{\\text {obs }}^{*}\\right)+s \\Delta\\left(z, x, y ; \\eta^{*}\\right)\\left(\\frac{1-p^{*}(x)}{p^{*}(x)\\left(1-\\rho^{*}\\right)}-e^{*}(x) \\Gamma_{1}^{*}(x)\\right) \\\\\n& \\quad-(1-s) \\Gamma_{0}^{*}(x)\\left(z-(1-z) \\frac{q^{*}(x)}{1-q^{*}(x)}\\right)\\left(y-m^{*}(r)\\right)\n\\end{aligned}\n$$\n\nwhere $\\Gamma_{0}^{*}$ and $\\Gamma_{1}^{*}$ are given by (67) and (68), respectively, for $\\Lambda_{1}$ and $\\Lambda_{2}$ as in (69) and (70).\nWe can now verify this matches the result in Theorem 4 of Yang et al. (2024). While that Theorem is stated as giving the efficient score of $\\tau_{\\text {obs }}$ rather than the efficient influence function of $\\tau_{\\text {obs }}$, this is incorrect according to S. Yang (personal communication, April 19, 2025). We correct the theorem below; essentially we must replace the efficient score $s_{\\beta}\\left(w ; \\eta^{*}\\right)$ for $\\beta$ with the efficient influence function $\\varphi_{\\beta}\\left(w ; \\eta^{*}\\right)$ for $\\beta$.\nTheorem (Theorem 4, Yang et al. (2024), corrected). Suppose Assumptions 1-3 hold. Then the EIF of $\\tau_{\\text {obs }}$ at the truth $\\tau_{\\text {obs }}^{*}$ is\n\n$$\n\\bar{\\varphi}_{\\text {eff,obs }}^{(3)}\\left(w ; \\tau_{\\text {obs }}^{*}, \\eta^{*}\\right)=\\frac{1-s}{1-\\rho^{*}}\\left(m_{11}^{*}(x)-m_{10}^{*}(x)-\\tau_{\\text {obs }}^{*}\\right)+\\mathbb{E}\\left[D_{\\mu}\\left(X ; \\beta^{*}\\right)^{\\top} \\mid S=0\\right] \\varphi_{\\beta}\\left(w ; \\eta^{*}\\right)\n$$\n\nwhere $\\varphi_{\\beta}\\left(w ; \\eta^{*}\\right)$ is the first $p$ components of\n\n$$\n\\varphi_{\\lambda}\\left(w ; \\eta^{*}\\right)=\\left(\\mathbb{E}\\left[s_{\\lambda}\\left(W ; \\eta^{*}\\right) s_{\\lambda}\\left(W ; \\eta^{*}\\right)^{\\top}\\right]\\right)^{-1} s_{\\lambda}\\left(w ; \\eta^{*}\\right)\n$$\n\nfor\n\n$$\n\\begin{aligned}\ns_{\\lambda}\\left(w ; \\eta^{*}\\right) & =\\left(Z-\\mathbb{E}\\left[Z \\omega^{*}(R) \\mid S, X\\right] \\mathbb{E}\\left[\\omega^{*}(R) \\mid S, X\\right]^{-1}\\right) \\omega^{*}(R)\\left(Y-m^{*}(R)\\right)\\left[\\begin{array}{c}\nD_{\\mu}\\left(x ; \\beta^{*}\\right) \\\\\nD_{\\phi}\\left(x ; \\theta^{*}\\right)\n\\end{array}\\right] \\\\\n\\omega^{*}(r) & =\\frac{1}{V^{*}(r)}\n\\end{aligned}\n$$\n\nthe efficient score for $\\lambda=(\\beta, \\theta)$, as given explicitly by Theorem 1 of Yang et al. (2024).\n\nShowing that $\\varphi_{\\text {eff,obs }}^{(3)}=\\widetilde{\\varphi}_{\\text {eff,obs }}^{(3)}$ requires some algebra which we outline below. First we compute\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\omega^{*}(R) \\mid S, X\\right] & =S\\left(\\frac{e^{*}(X)}{V_{11}^{*}(X)}+\\frac{1-e^{*}(X)}{V_{10}^{*}(X)}\\right)+(1-S)\\left(\\frac{q^{*}(X)}{V_{01}^{*}(X)}+\\frac{1-q^{*}(X)}{V_{00}^{*}(X)}\\right), \\quad \\text { and } \\\\\n\\mathbb{E}\\left[Z \\omega^{*}(R) \\mid S, X\\right] & =S\\left(\\frac{e^{*}(X)}{V_{11}^{*}(X)}\\right)+(1-S)\\left(\\frac{q^{*}(X)}{V_{01}^{*}(X)}\\right)\n\\end{aligned}\n$$\n\nThen\n\n$$\n\\begin{aligned}\n\\Omega^{*}(R):= & \\left(Z-\\mathbb{E}\\left[Z \\omega^{*}(R) \\mid S, X\\right] \\mathbb{E}\\left[\\omega^{*}(R) \\mid S, X\\right]^{-1}\\right) \\omega^{*}(R) \\\\\n& =S \\frac{Z\\left(1-e^{*}(X)\\right)-(1-Z) e^{*}(X)}{V_{10}^{*}(X) e^{*}(X)+V_{11}^{*}(X)\\left(1-e^{*}(X)\\right)}+(1-S) \\frac{Z\\left(1-q^{*}(X)\\right)-(1-Z) q^{*}(X)}{V_{00}^{*}(X) q^{*}(X)+V_{01}^{*}(X)\\left(1-q^{*}(X)\\right)}\n\\end{aligned}\n$$\n\nThis allows us to compute\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[s_{\\lambda}\\left(W ; \\eta^{*}\\right) s_{\\lambda}\\left(W ; \\eta^{*}\\right)^{\\top}\\right] \\\\\n& \\quad=\\mathbb{E}\\left(\\mathbb{E}\\left[\\left(\\Omega^{*}(R)\\right)^{2}\\left(Y-m^{*}(R)\\right)^{2} \\mid X\\right]\\left[\\begin{array}{ll}\nD_{\\mu}\\left(X ; \\beta^{*}\\right) D_{\\mu}\\left(X ; \\beta^{*}\\right)^{\\top} & D_{\\mu}\\left(X ; \\beta^{*}\\right) D_{\\phi}\\left(X ; \\theta^{*}\\right)^{\\top} \\\\\nD_{\\phi}\\left(X ; \\theta^{*}\\right) D_{\\mu}\\left(X ; \\beta^{*}\\right)^{\\top} & D_{\\phi}\\left(X ; \\theta^{*}\\right) D_{\\phi}\\left(X ; \\theta^{*}\\right)^{\\top}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nwith\n$\\mathbb{E}\\left[\\left(\\Omega^{*}(R)\\right)^{2}\\left(Y-m^{*}(R)\\right)^{2} \\mid X\\right]=\\frac{p^{*}(X) e^{*}(X)\\left(1-e^{*}(X)\\right)}{V_{10}^{*}(X) e^{*}(X)+V_{11}^{*}(X)\\left(1-e^{*}(X)\\right)}+\\frac{\\left(1-p^{*}(X)\\right) q^{*}(X)\\left(1-q^{*}(X)\\right)}{\\left(V_{00}^{*}(X) q^{*}(X)+V_{01}^{*}(X)\\left(1-q^{*}(X)\\right)\\right.}$.\nHence\n\n$$\n\\mathbb{E}\\left[s_{\\lambda}\\left(W ; \\eta^{*}\\right) s_{\\lambda}\\left(W ; \\eta^{*}\\right)^{\\top}\\right]=\\left[\\begin{array}{ll}\nA_{\\mu \\mu} & A_{\\mu \\phi} \\\\\nA_{\\mu \\phi}^{\\top} & A_{\\phi \\phi}\n\\end{array}\\right]\n$$\n\nwhere $A_{\\mu \\mu}, A_{\\mu \\phi}$, and $A_{\\phi \\phi}$ are as in (58), (59), and (60). So\n\n$$\n\\varphi_{\\lambda}\\left(w ; \\eta^{*}\\right)=\\left[\\begin{array}{ll}\nA_{\\mu \\mu} & A_{\\mu \\phi} \\\\\nA_{\\mu \\phi}^{\\top} & A_{\\phi \\phi}\n\\end{array}\\right]^{-1} s_{\\lambda}\\left(w ; \\eta^{*}\\right)\n$$\n\nand by block matrix inversion formulas\n\n$$\n\\varphi_{\\beta}\\left(w ; \\eta^{*}\\right)=\\left[\\left(A_{\\mu \\mu}-A_{\\mu \\phi} A_{\\phi \\phi}^{-1} A_{\\mu \\phi}^{\\top}\\right)^{-1}-\\left(A_{\\mu \\mu}-A_{\\mu \\phi} A_{\\phi \\phi}^{-1} A_{\\mu \\phi}^{\\top}\\right)^{-1} A_{\\mu \\phi} A_{\\phi \\phi}^{-1}\\right] s_{\\lambda}\\left(w ; \\eta^{*}\\right)\n$$\n\nThen by (72), (69), and (70) we can write\n\n$$\n\\widetilde{\\varphi}_{\\text {eff,obs }}^{(3)}\\left(w ; \\tau^{*}, \\eta^{*}\\right)=\\frac{1-s}{1-\\rho^{*}}\\left(m_{11}^{*}(x)-m_{10}^{*}(x)-\\tau_{\\mathrm{obs}}^{*}\\right)-\\left[\\begin{array}{ll}\n\\Lambda_{1}^{\\top} & \\Lambda_{2}^{\\top}\n\\end{array}\\right] s_{\\lambda}\\left(w ; \\eta^{*}\\right)\n$$\n\nUsing (73) we compute\n\n$$\n\\begin{aligned}\n{\\left[\\begin{array}{ll}\n\\Lambda_{1}^{\\top} & \\Lambda_{2}^{\\top}\n\\end{array}\\right] } & s_{\\lambda}\\left(w ; \\eta^{*}\\right) \\\\\n= & \\Omega^{*}(r)\\left[y-m^{*}(r)\\right]\\left[\\Lambda_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)+\\Lambda_{2}^{\\top} D_{\\phi}\\left(x ; \\theta^{*}\\right)\\right] \\\\\n= & s \\Delta\\left(z, x, y ; \\eta^{*}\\right) \\frac{e^{*}(x)\\left(1-e^{*}(x)\\right)}{V_{10}^{*}(x) e^{*}(x)+V_{11}^{*}(x)\\left(1-e^{*}(x)\\right)}\\left[\\Lambda_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)+\\Lambda_{2}^{\\top} D_{\\phi}\\left(x ; \\theta^{*}\\right)\\right] \\\\\n& +(1-s) \\frac{z\\left(1-q^{*}(x)\\right)-(1-z) q^{*}(x)}{V_{00}^{*}(x) q^{*}(x)+V_{01}^{*}(x)\\left(1-q^{*}(x)\\right)}\\left(y-m^{*}(r)\\right)\\left[\\Lambda_{1}^{\\top} D_{\\mu}\\left(x ; \\beta^{*}\\right)+\\Lambda_{2}^{\\top} D_{\\phi}\\left(x ; \\theta^{*}\\right)\\right] \\\\\n= & s \\Delta\\left(z, x, y ; \\eta^{*}\\right)\\left[e^{*}(x) \\Gamma_{1}^{*}(x)-\\frac{1-p^{*}(x)}{p^{*}(x)\\left(1-\\rho^{*}\\right)}\\right] \\\\\n& +(1-s) \\Gamma_{0}^{*}(x)\\left(z-(1-z) \\frac{q^{*}(x)}{1-q^{*}(x)}\\right)\\left(y-m^{*}(r)\\right)\n\\end{aligned}\n$$\n\nwhere the last equality follows by (67) and (68). Plugging into (74) verifies that indeed, $\\varphi_{\\text {eff,obs }}^{(3)}=$ $\\tilde{\\varphi}_{\\text {eff,obs }}^{(3)}$.", "tables": {}, "images": {}}, {"section_id": 24, "text": "# B. 4 Linear confounding bias \n\nHere we derive the EIF for an arbitrary scalar estimand $\\tau$, which can be estimated by an RAL estimator with influence function $\\varphi_{0}$, in the linear confounding bias model $\\mathcal{P}_{\\mathcal{M}_{4}}$.\n\nAs argued in the main text, $\\mathcal{M}_{4}$ is a linear space so $\\mathcal{S}_{\\mathcal{M}_{4}}=\\mathcal{M}_{4}$ by Proposition 2. The remainder of the EIF derivation closely follows the outcome-mediated selection bias example in Section 2.3. As in that example, we note $\\mathcal{S}_{\\mathcal{M}_{4}}^{\\dot{\\tau}}$ is precisely set of all functions $f \\in \\mathcal{H}_{R}$ for which the projection $g=\\Pi\\left(f ; \\mathcal{S}_{\\mathcal{M}_{4}}\\right)$ is zero. Given $\\hat{f}$, by the form of $\\mathcal{S}_{\\mathcal{M}_{4}}=\\mathcal{M}_{4}$ we can write\n\n$$\ng(r)=s(1-z) g_{1}(x)+(1-s) z g_{2}(x)+(1-s)(1-z) g_{3}(x)+s z \\kappa\\left(x ; \\beta, g_{1}, g_{2}, g_{3}\\right)\n$$\n\nwhere $g_{1}, g_{2}, g_{3}$ in $\\mathcal{H}_{X}$ and $\\beta \\in \\mathbb{R}^{q}$ minimize the objective\n\n$$\n\\begin{aligned}\n\\mathcal{O}\\left(g_{1}, g_{2}, g_{3}, \\beta\\right)= & \\mathbb{E}\\left[\\left(f(R)-S(1-Z) g_{1}(X)-(1-S) Z g_{2}(X)-(1-S)(1-Z) g_{3}(X)-S Z \\kappa\\left(X ; \\beta, g_{1}, g_{2}, g_{3}\\right)\\right)^{2}\\right] \\\\\n= & \\mathbb{E}\\left[p^{*}(X) e^{*}(X)\\left(f_{11}(X)-\\kappa\\left(X ; \\beta, g_{1}, g_{2}, g_{3}\\right)\\right)^{2}+p^{*}(X)\\left(1-e^{*}(X)\\right)\\left(f_{10}(X)-g_{1}(X)\\right)^{2}\\right] \\\\\n& +\\mathbb{E}\\left[\\left(1-p^{*}(X)\\right) r^{*}(X)\\left(f_{01}(X)-g_{2}(X)\\right)^{2}+\\left(1-p^{*}(X)\\right)\\left(1-r^{*}(X)\\right)\\left(f_{00}(X)-g_{3}(X)\\right)^{2}\\right]\n\\end{aligned}\n$$\n\nover $\\mathcal{H}_{X}^{3} \\times \\mathbb{R}^{q}$, for\n\n$$\n\\kappa\\left(x ; \\beta, g_{1}, g_{2}, g_{3}\\right)=g_{1}(x)+g_{2}(x)-g_{3}(x)+\\psi(x)^{\\top} \\beta\n$$\n\nThe second equality follows from writing\n\n$$\nf(R)=S Z f_{11}(X)+S(1-Z) f_{10}(X)+(1-S) Z f_{01}(X)+(1-S)(1-Z) f_{00}(X)\n$$\n\nand conditioning on $X$. With the objective now an expectation of a function of $X$, for fixed $\\beta$ we can minimize over $g_{1}, g_{2}$, and $g_{3}$ pointwise to obtain the first-order conditions\n\n$$\n\\begin{aligned}\n& 0=-p^{*}(x) e^{*}(x)\\left(f_{11}(x)-\\kappa\\left(x ; \\beta, g_{1}, g_{2}, g_{3}\\right)\\right)-p^{*}(x)\\left(1-e^{*}(x)\\right)\\left(f_{10}(x)-g_{1}(x)\\right) \\\\\n& 0=-p^{*}(x) e^{*}(x)\\left(f_{11}(x)-\\kappa\\left(x ; \\beta, g_{1}, g_{2}, g_{3}\\right)\\right)-\\left(1-p^{*}(x)\\right) r^{*}(x)\\left(f_{01}(x)-g_{2}(x)\\right) \\\\\n& 0=p^{*}(x) e^{*}(x)\\left(f_{11}(x)-\\kappa\\left(x ; \\beta, g_{1}, g_{2}, g_{3}\\right)\\right)-\\left(1-p^{*}(x)\\right)\\left(1-r^{*}(x)\\right)\\left(f_{00}(x)-g_{3}(x)\\right)\n\\end{aligned}\n$$\n\nfor all $x \\in \\mathcal{X}$. Further, setting the partial derivative of the objective with respect to $\\beta$ equal to zero yields the additional first-order condition\n\n$$\n\\mathbb{E}\\left[p^{*}(X) e^{*}(X)\\left(f_{11}(X)-\\kappa\\left(X ; \\beta, g_{1}, g_{2}, g_{3}\\right)\\right) \\psi(X)\\right]=0\n$$\n\nRearranging, we see these first-order conditions are satisfied by $g=0$ (i.e. $g_{1}=g_{2}=g_{3}=0, \\beta=0$ ) if and only if\n\n$$\n\\begin{aligned}\n\\left(1-e^{*}(x)\\right) f_{10}(x) & =-e^{*}(x) f_{11}(x) \\\\\n\\left(1-p^{*}(x)\\right) r^{*}(x) f_{01}(x) & =-p^{*}(x) e^{*}(x) f_{11}(x) \\\\\n\\left(1-p^{*}(x)\\right)\\left(1-r^{*}(x)\\right) f_{00}(x) & =p^{*}(x) e^{*}(x) f_{11}(x) \\\\\n0 & =\\mathbb{E}\\left[p^{*}(X) e^{*}(X) f_{11}(X) \\psi(X)\\right]\n\\end{aligned}\n$$\n\nfor all $x \\in \\mathcal{X}$. So $\\mathcal{S}_{\\mathcal{M}_{4}}$ is the set of all functions $f \\in \\mathcal{H}_{R}$ satisfying these equations, and by Theorem 2\n\n$$\n\\mathcal{T}_{\\mathcal{M}_{4}}^{\\perp}=\\left\\{w \\mapsto f(r)\\left(y-m^{*}(r)\\right) \\mid f \\in \\mathcal{H}_{R} \\text { satisfies (75), (76), (77), and (78). }\\right.\n$$\n\nNoting that (75), (76), and (77) determine $f_{10}, f_{01}$, and $f_{00}$ in terms of $f_{11}$ and letting $\\nu(x)=$ $p^{*}(x) e^{*}(x) f_{11}(x)$, we can write this succinctly as\n\n$$\n\\mathcal{T}_{\\mathcal{M}_{4}}^{\\perp}=\\left\\{w \\mapsto h\\left(r ; \\eta^{*}\\right) \\nu(x)\\left(y-m^{*}(r)\\right) \\mid \\nu \\in \\mathcal{H}_{X}, \\mathbb{E}[\\nu(X) \\psi(X)]=0\\right\\}\n$$\n\nwhere $h(r ; \\eta)$ is as in (30).\nTo compute the projection (7), as in the other examples we condition on $X$ to write the objective as the expectation of a function of $X$ and minimize this function pointwise. Unlike in the projection for computing $\\varphi_{\\text {eff }}^{(5)}$ in the main text, but like the computation in Appendix B. 1 for the EIF in the restricted moment model, we deal with the constraints using Lagrange multipliers. By (8), an arbitrary element $g \\in \\mathcal{T}_{\\mathcal{M}_{4}}^{\\perp}$ takes the form\n\n$$\ng(w)=h\\left(r ; \\eta^{*}\\right) \\nu(x)\\left(y-m^{*}(r)\\right)\n$$\n\nfor some $\\nu \\in \\mathcal{H}_{X}$ with $\\mathbb{E}[\\nu(X) \\psi(X)]=0$. Then the EIF is $\\varphi_{\\text {eff }}^{(4)}=\\varphi_{0}-g_{\\text {eff }}^{(4)}$ where\n\n$$\ng_{\\mathrm{eff}}^{(4)}(w)=h\\left(r ; \\eta^{*}\\right) \\nu^{*}\\left(x ; \\varphi_{0}\\right)\\left(y-m^{*}(r)\\right)\n$$\n\nfor $\\nu^{*}\\left(\\cdot ; \\varphi_{0}\\right)$ solving the optimization problem\n\n$$\n\\begin{aligned}\n& \\min . \\mathbb{E}\\left[\\left(\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)-h\\left(R ; \\eta^{*}\\right) \\nu(X)\\left(Y-m^{*}(R)\\right)\\right)^{2}\\right] \\text { over } \\nu \\in \\mathcal{H}_{X} \\\\\n& \\text { s.t. } \\mathbb{E}[\\nu(X) \\psi(X)]=0\n\\end{aligned}\n$$\n\nWe eliminate the constraint by introducing Lagrange multipliers $\\lambda \\in \\mathbb{R}^{q}$, and then rewrite the Lagrangian by conditioning on $X$ :\n\n$$\n\\begin{aligned}\n\\min . & \\mathbb{E}[\\tilde{g}(X ; \\nu)] \\text { over } \\nu \\in \\mathcal{H}_{X} \\text { where } \\\\\n\\tilde{g}(X ; \\nu) & =\\mathbb{E}\\left[\\left(\\varphi_{0}\\left(W ; \\tau^{*}, \\eta^{*}\\right)-h\\left(R ; \\eta^{*}\\right) \\nu(X)\\left(Y-m^{*}(R)\\right)\\right)^{2} \\mid X\\right] \\\\\n& =\\nu(X)^{2} \\mathbb{E}\\left[h^{2}\\left(R ; \\eta^{*}\\right)\\left(Y-m^{*}(R)\\right)^{2} \\mid X\\right]-2 \\nu(X) I\\left(X ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)-2 \\nu(X) \\lambda^{\\top} \\psi(X)\n\\end{aligned}\n$$\n\nSince $\\tilde{g}(x ; \\nu)$ is quadratic in $\\nu$, the minimizer is\n\n$$\n\\nu^{*}(x)=\\frac{I\\left(x ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)+\\lambda^{\\top} \\psi(x)}{\\Sigma\\left(x ; \\eta^{*}\\right)}\n$$\n\nwhere $I\\left(x ; \\eta, \\tau, \\varphi_{0}\\right)$ is given by (31). This matches the formula for $\\nu\\left(x ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)$ in (32), upon noting that\n\n$$\np^{*}(x) e^{*}(x) \\mathbb{E}\\left[h^{2}\\left(R ; \\eta^{*}\\right)\\left(Y-m^{*}(R)\\right)^{2} \\mid X=x\\right]=\\Sigma\\left(x ; \\eta^{*}\\right)\n$$\n\nfor $\\Sigma(x ; \\eta)$ as in (33). We solve for the Lagrange multiplier $\\lambda$ using the constraint\n\n$$\n\\mathbb{E}\\left[\\nu^{*}\\left(X ; \\nu_{0}\\right) \\psi(X)\\right]=0\n$$\n\nwhich gives $\\lambda=\\lambda\\left(\\eta^{*}, \\tau^{*}, \\eta_{0}\\right)$ as in (34). Thus\n\n$$\ng_{\\mathrm{eff}}^{(4)}(w)=h(r ; \\eta) \\nu\\left(x ; \\eta^{*}, \\tau^{*}, \\varphi_{0}\\right)\\left(y-m^{*}(r)\\right)\n$$\n\nwhich shows (29).", "tables": {}, "images": {}}, {"section_id": 25, "text": "# B. 5 Known RCT propensity scores \n\nWe characterize the EIF $\\tilde{\\varphi}_{\\text {eff }}$ for any pathwise differentiable estimand $\\tau \\in \\mathbb{R}$ in the model $\\tilde{\\mathcal{P}}_{\\mathcal{M}}$ in terms of the EIF $\\varphi_{\\text {eff }}$ for the analogous model $\\mathcal{P}_{\\mathcal{M}}$ where the RCT propensity score $e^{*}$ is unknown. Our result will work for any mean function collection $\\mathcal{M}$ satisfying the conditions of Corollary 1.\n\nIt is easy to see that the space\n\n$$\n\\tilde{\\mathcal{U}}=\\left\\{w \\mapsto s h(x)\\left(z-e^{*}(x)\\right) \\mid h \\in \\mathcal{H}_{X}\\right\\}\n$$\n\nis orthogonal to $\\mathcal{T}_{\\mathcal{M}}^{\\perp}$. Hence\n\n$$\n\\tilde{\\varphi}_{\\mathrm{eff}}=\\varphi_{0}-\\Pi\\left(\\varphi_{0} ; \\tilde{\\mathcal{T}}_{\\mathcal{M}}^{\\perp}\\right)=\\varphi_{0}-\\Pi\\left(\\varphi_{0} ; \\mathcal{T}_{\\mathcal{M}}^{\\perp}\\right)-\\Pi\\left(\\varphi_{0} ; \\tilde{\\mathcal{U}}\\right)=\\varphi_{\\mathrm{eff}}-\\Pi\\left(\\varphi_{0} ; \\tilde{\\mathcal{U}}\\right)\n$$\n\nwhere as in the main text, $\\varphi_{0}$ is the influence function of any initial RAL estimator. Thus it suffices to compute $\\Pi\\left(\\varphi_{0} ; \\tilde{\\mathcal{U}}\\right)$. By definition\n\n$$\n\\Pi\\left(\\varphi_{0} ; \\tilde{\\mathcal{U}}\\right)(w)=s \\nu^{*}(x)\\left(z-e^{*}(x)\\right)\n$$\n\nwhere\n\n$$\n\\nu^{*}=\\underset{h \\in \\mathcal{H}_{X}}{\\arg \\min } \\mathbb{E}\\left[\\left(\\varphi_{0}(W)-S h(X)\\left(Z-e^{*}(X)\\right)\\right)^{2}\\right]\n$$\n\nAs usual, we expand the right-hand side and condition on $X$ :\n\n$$\n\\begin{aligned}\n& \\underset{h \\in \\mathcal{H}_{X}}{\\arg \\min } \\mathbb{E}\\left[\\left(\\varphi_{0}(W)-S h(X)\\left(Z-e^{*}(X)\\right)\\right)^{2}\\right] \\\\\n& \\quad=\\underset{h \\in \\mathcal{H}_{X}}{\\arg \\min } \\mathbb{E}\\left[\\mathbb{E}\\left[S\\left(Z-e^{*}(X)\\right)^{2} \\mid X\\right] h(X)^{2}-2 \\mathbb{E}\\left[S\\left(Z-e^{*}(X)\\right) \\varphi_{0}(W) \\mid X\\right] h(X)\\right]\n\\end{aligned}\n$$\n\nWe compute\n\n$$\n\\mathbb{E}\\left[S\\left(Z-e^{*}(X)\\right)^{2} \\mid X\\right]=p^{*}(X) e^{*}(X)\\left(1-e^{*}(X)\\right)\n$$\n\nand so by minimizing pointwise we conclude\n\n$$\n\\nu^{*}(x)=\\frac{\\mathbb{E}\\left[S\\left(Z-e^{*}(X)\\right) \\varphi_{0}(W) \\mid X=x\\right]}{p^{*}(x) e^{*}(x)\\left(1-e^{*}(x)\\right)}\n$$\n\nFor $\\varphi_{0} \\in\\left\\{\\varphi_{0, \\text { rct }}, \\varphi_{0, \\text { obs }}, \\varphi_{0, \\text { tgt }}\\right\\}$ we have\n\n$$\n\\mathbb{E}\\left[S\\left(Z-e^{*}(X)\\right) \\varphi_{0}(W) \\mid X\\right]=0\n$$\n\nand hence $\\nu^{*}(x)=0$. Thus, knowing the RCT propensity score does not improve the semiparametric efficiency bound for estimating $\\tau_{\\text {rct }}, \\tau_{\\text {obs }}$, or $\\tau_{\\text {tgt }}$ in any semiparametric model restricting the outcome mean function. In particular, the EIF's $\\varphi_{\\text {eff }}^{(k)}, k=1,2,3,4,5$ in the models $\\mathcal{P}_{\\mathcal{M}_{k}}$ for these estimands are also the EIF's in the corresponding models $\\tilde{\\mathcal{P}}_{\\mathcal{M}_{k}}$.", "tables": {}, "images": {}}, {"section_id": 26, "text": "## C Conditions for efficiency\n\nIn this section, we provide specific primitive conditions under which the cross-fit one-step estimators $\\hat{\\tau}_{\\text {eff }}^{(4)}$ and $\\hat{\\tau}_{\\text {eff }}^{(5)}$ defined in Section 4 indeed attain the efficiency bound, by proving that these conditions imply those of the generalized Lemma in Section A.5. One condition is the following overlap assumption; a version of overlap is needed to even identify causal estimands in the nonparametric model.\n\nAssumption 3 (Overlap). There exists $\\delta>0$ such that $e^{*}(x) \\in[\\delta, 1-\\delta]$ whenever $p^{*}(x)>0$, $r^{*}(x) \\in[\\delta, 1-\\delta]$, and $p^{*}(x) \\in\\{0\\} \\cup[\\delta, 1-\\delta]$ for all $x \\in \\mathcal{X}$.\nRemark 1. The condition on $p^{*}(x)$ in Assumption 3 presumes that the support of the covariates in the RCT is contained within the support of the covariates of the observational dataset. Unlike typical analyses, we allow this inclusion to be strict since assumptions like linear confounding bias or outcome mediated selection permit extrapolation of the CATE beyond the support of the covariates in the RCT.\n\nWe also need the nuisance estimates to satisfy a similar overlap condition, as well as $o_{p}\\left(N^{-1 / 4}\\right)$ root-mean-square convergence rates, typical in the double machine learning literature.\nAssumption 4. Cross-fit estimates of the nuisance functions $e^{*}, m_{10}^{*}, m_{01}^{*}, m_{00}^{*}, r^{*}$, and $p^{*}$ satisfy Assumption 3 along with the following for each $k=1, \\ldots, K$ :\n\n- (Convergence of infinite-dimensional nuisance estimates)\n\n$$\n\\begin{gathered}\n\\left\\|\\hat{e}^{(-k)}-e^{*}\\right\\|_{2, P^{*}}+\\left\\|\\hat{m}_{s z}^{(-k)}-m_{s z}^{*}\\right\\|_{2, P^{*}}+\\left\\|\\hat{r}^{(-k)}-r^{*}\\right\\|_{2, P^{*}}+\\left\\|\\hat{p}^{(-k)}-p^{*}\\right\\|_{2, P^{*}} \\\\\n=o_{p}\\left(N^{-1 / 4}\\right), \\quad(s, z) \\neq(1,1)\n\\end{gathered}\n$$\n\n- (Overlap in infinite-dimensional nuisance estimates) With probability tending to 1 as $N \\rightarrow \\infty$, the statement of Assumption 3 holds with $e^{*}, r^{*}$, and $p^{*}$ replaced by their estimates $\\hat{e}^{(-k)}$, $\\hat{r}^{(-k)}$, and $\\hat{p}^{(-k)}$, respectively.", "tables": {}, "images": {}}, {"section_id": 27, "text": "# C. 1 Linear confounding bias \n\nProposition. Suppose Assumption 3 holds and $m^{*} \\in \\mathcal{M}_{4}$. Consider a function\n\n$$\ng(w ; \\eta)=h(r ; \\eta) \\zeta(x ; \\eta)(y-m(r))+s\\left(z-e^{*}(x)\\right) h_{2}(x ; \\eta)\n$$\n\nwhere $\\eta$ includes $(p(\\cdot), e(\\cdot), m(\\cdot), q(\\cdot), V(\\cdot)), h(r ; \\eta)$ is as in (30), $h_{2}\\left(\\cdot ; \\eta^{*}\\right) \\in \\mathcal{H}_{X}$, and $\\zeta^{*}(x)=$ $\\zeta\\left(x ; \\eta^{*}\\right)$ satisfies $\\mathbb{E}\\left[\\zeta^{*}(X) \\psi(X)\\right]=0$ so that $g^{*}(\\cdot)=g\\left(\\cdot ; \\eta^{*}\\right) \\in \\widetilde{\\mathcal{T}}_{\\mathcal{M}_{4}}^{\\perp}$ by (79) and Corollary 1. Further suppose that there are cross-fit estimates $\\hat{\\eta}^{(-k)}$ of $\\eta^{*}$ satisfying Assumption 4 with\n\n$$\n\\hat{m}_{11}^{(-k)}(x)=\\hat{m}_{10}^{(-k)}(x)+\\hat{m}_{01}^{(-k)}(x)-\\hat{m}_{00}^{(-k)}(x)+\\psi(x)^{\\top} \\hat{\\theta}^{(-k)}, \\quad k=1, \\ldots, K\n$$\n\nwhere $\\left\\|\\hat{\\theta}^{(-k)}-\\theta^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$. Finally, assume that the rate conditions $\\| \\zeta\\left(\\cdot ; \\hat{\\eta}^{(-k)}\\right)-$ $\\zeta\\left(\\cdot ; \\eta^{*}\\right) \\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$ and $\\left\\|h_{2}\\left(\\cdot ; \\hat{\\eta}^{(-k)}\\right)-h_{2}\\left(\\cdot ; \\eta^{*}\\right)\\right\\|_{2, P^{*}}=o_{p}(1)$ hold, and additionally that $V^{*}(s, z, x) \\leqslant C$ and $\\max \\left(\\left|\\zeta\\left(x ; \\eta^{*}\\right)\\right|,\\left|\\zeta\\left(x, \\hat{\\eta}^{(-k)}\\right)\\right|\\right) \\leqslant C$ for all $(s, z) \\in\\{0,1\\}^{2}, x \\in \\mathcal{X}$, and folds $k=1, \\ldots, K$. Then the Lemma in Section A. 5 holds with $\\mathcal{M}=\\mathcal{M}_{4}, g\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)=g\\left(\\cdot ; \\eta^{*}\\right)$, and $g\\left(\\cdot ; \\hat{\\tau}^{(-k)}, \\hat{\\eta}^{(-k)}\\right)=g\\left(\\cdot ; \\hat{\\eta}^{(-k)}\\right)$.\nProof. We must show the boundedness, approximate tangency, and rate conditions in the Lemma of Appendix A. 5 with\n\n$$\n\\begin{aligned}\n& h_{1}^{*}(r)=h\\left(r ; \\eta^{*}\\right) \\zeta\\left(x ; \\eta^{*}\\right), \\quad \\hat{h}_{1}^{(-k)}(r)=h\\left(r ; \\hat{\\eta}^{(-k)}\\right) \\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right) \\\\\n& h_{2}^{*}(x)=h_{2}\\left(x ; \\eta^{*}\\right), \\quad \\hat{h}_{2}^{(-k)}(x)=h_{2}\\left(x ; \\hat{\\eta}^{(-k)}\\right)\n\\end{aligned}\n$$\n\nBoundedness: We have $V^{*}(s, z, x) \\leqslant C$ for $P^{*}$-almost all $(s, z, x)$ by assumption. Furthermore by the overlap condition (Assumption 3) it is clear that $h\\left(r ; \\eta^{*}\\right)$ is uniformly upper bounded for $P^{*}$-almost all $r$, a qualifier we omit hereafter in all inequalities and equalities in this proof, in\n\nabsolute value by $C \\delta^{-2}$. Similarly, by the overlap in infinite-dimensional nuisance estimates condition in Assumption 4, we have $h\\left(r ; \\hat{\\eta}^{(-k)}\\right)$is uniformly upper bounded in absolute value by $C \\delta^{-2}$ for all $k=1, \\ldots, K$. With $\\max \\left(\\left|\\zeta\\left(x ; \\eta^{*}\\right)\\right|,\\left|\\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right)\\right|\\right) \\leqslant C$ for all $k$, we conclude that $\\max \\left(\\left\\|h_{1}^{*}\\right\\|_{\\infty, P^{*}},\\left\\|\\hat{h}_{1}^{(-k)}\\right\\|_{\\infty, P^{*}}\\right) \\leqslant C^{2} \\delta^{-2}$.\nApproximate tangency: Approximate tangency holds trivially with $\\hat{R}^{(-k)}=0$ in view of the observation that $\\hat{m}^{(-k)} \\in \\mathcal{M}_{4}=\\mathcal{S}_{\\mathcal{M}_{4}}$ for all $k$ by (82), so that $\\hat{m}^{(-k)}-m \\in \\mathcal{S}_{\\mathcal{M}_{4}}$ as well since $\\mathcal{S}_{\\mathcal{M}_{4}}$ is a linear space.\nRate conditions: Fixing $k \\in\\{1, \\ldots, K\\}$, we have $\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$ by (80) and (82) in light of the assumption $\\left\\|\\hat{\\theta}^{(-k)}-\\theta^{*}\\right\\|=o_{p}\\left(N^{-1 / 4}\\right)$. With $\\left\\|\\hat{h}_{2}^{(-k)}-h_{2}^{*}\\right\\|_{2, P^{*}}=o_{p}(1)$ by assumption, to show the rate conditions it suffices to show that $\\left\\|\\hat{h}_{1}^{(-k)}-h_{1}^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$. To that end, we write\n\n$$\n\\hat{h}_{1}^{(-k)}(r)-h_{1}^{*}(r)=h\\left(r ; \\hat{\\eta}^{(-k)}\\right) \\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right)-h\\left(r ; \\eta^{*}\\right) \\zeta\\left(x ; \\eta^{*}\\right)\n$$\n\nand note that since $\\left|h\\left(r ; \\eta^{*}\\right)\\right| \\leqslant C$ as shown above and we have $\\max \\left(\\left|\\zeta\\left(x ; \\eta^{*}\\right)\\right|,\\left|\\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right)\\right|\\right) \\leqslant C$ and $\\left\\|\\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right)-\\zeta\\left(x ; \\eta^{*}\\right)\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$ by assumption, it suffices by Lemma 3 to show that\n\n$$\n\\int\\left(h\\left(r ; \\hat{\\eta}^{(-k)}\\right)-h\\left(r ; \\eta^{*}\\right)\\right)^{2} d P^{*}(w)=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nWe compute\n\n$$\n\\begin{aligned}\n& \\left(h\\left(r ; \\hat{\\eta}^{(-k)}\\right)-h\\left(r ; \\eta^{*}\\right)\\right)^{2} \\\\\n& =s z\\left(\\frac{p^{*}(x) e^{*}(x)-\\hat{p}^{(-k)}(x) \\hat{e}^{(-k)}(x)}{\\hat{p}^{(-k)}(x) \\hat{e}^{(-k)}(x) p^{*}(x) e^{*}(x)}\\right)^{2}+s(1-z)\\left(\\frac{p^{*}(x)\\left(1-e^{*}(x)\\right)-\\hat{p}^{(-k)}(x)\\left(1-\\hat{e}^{(-k)}(x)\\right)}{\\hat{p}^{(-k)}(x)\\left(1-\\hat{e}^{(-k)}(x)\\right) p^{*}(x)\\left(1-e^{*}(x)\\right)}\\right)^{2} \\\\\n& \\quad+(1-s) z\\left(\\frac{\\left(1-p^{*}(x)\\right) r^{*}(x)\\left(1-\\hat{p}^{(-k)}(x)\\right)\\left(\\hat{r}^{(-k)}(x)\\right)}{\\left(1-\\hat{p}^{(-k)}(x)\\right) \\hat{r}^{(-k)}(x)\\left(1-p^{*}(x)\\right) r^{*}(x)}\\right)^{2} \\\\\n& \\quad+(1-s)(1-z)\\left(\\frac{\\left(1-p^{*}(x)\\right)\\left(1-r^{*}(x)\\right)-\\left(1-\\hat{p}^{(-k)}(x)\\right)\\left(1-\\hat{r}^{(-k)}(x)\\right)}{\\left(1-\\hat{p}^{(-k)}(x)\\right)\\left(1-\\hat{r}^{(-k)}(x)\\right)\\left(1-p^{*}(x)\\right)\\left(1-r^{*}(x)\\right)}\\right)^{2}\n\\end{aligned}\n$$\n\nIn light of (80), we apply Lemma 3 four times to show that\n\n$$\n\\begin{aligned}\n\\int\\left(\\hat{p}^{(-k)}(x) \\hat{e}^{(-k)}(x)-p^{*}(x) e^{*}(x)\\right)^{2} d P^{*}(w) & =o_{p}\\left(N^{-1 / 2}\\right) \\\\\n\\int\\left(\\hat{p}^{(-k)}(x)\\left(1-\\hat{e}^{(-k)}(x)\\right)-p^{*}(x)\\left(1-e^{*}(x)\\right)\\right)^{2} d P^{*}(w) & =o_{p}\\left(N^{-1 / 2}\\right) \\\\\n\\int\\left(\\left(1-\\hat{p}^{(-k)}(x)\\right) \\hat{r}^{(-k)}(x)-\\left(1-p^{*}(x)\\right) r^{*}(x)\\right)^{2} d P^{*}(w) & =o_{p}\\left(N^{-1 / 2}\\right) \\\\\n\\int\\left(\\left(1-\\hat{p}^{(-k)}(x)\\right)\\left(1-\\hat{r}^{(-k)}(x)\\right)-\\left(1-p^{*}(x)\\right)\\left(1-r^{*}(x)\\right)\\right)^{2} d P^{*}(w) & =o_{p}\\left(N^{-1 / 2}\\right)\n\\end{aligned}\n$$\n\nThen by Assumption 3 and overlap in infinite-dimensional nuisance estimates, we conclude that indeed (83) holds.", "tables": {}, "images": {}}, {"section_id": 28, "text": "# C. 2 Outcome-mediated selection bias \n\nProposition. Suppose Assumption 3 holds and $m^{*} \\in \\mathcal{M}_{5}$. Consider a function\n\n$$\ng(w ; \\eta)=f(r ; \\eta) \\zeta(x ; \\eta)(y-m(r))+s\\left(z-e^{*}(x)\\right) h_{2}(x ; \\eta)\n$$\n\nwhere $\\eta$ includes $(p(\\cdot), e(\\cdot), m(\\cdot), q(\\cdot)), f(r ; \\eta)$ is as in (20), $h_{2}\\left(\\cdot ; \\eta^{*}\\right) \\in \\mathcal{H}_{X}$, and $\\zeta^{*}(\\cdot)=\\zeta\\left(\\cdot ; \\eta^{*}\\right) \\in$ $\\mathcal{H}_{X}$ so that $g^{*}(\\cdot)=g\\left(\\cdot ; \\eta^{*}\\right) \\in \\overline{\\mathcal{T}_{\\mathcal{M}_{5}}^{\\perp}}$ by (21) and Corollary 1. Further suppose that there are cross-fit estimates $\\hat{\\eta}^{(-k)}$ of $\\eta^{*}$ satisfying Assumption 4 with\n\n$$\n\\hat{m}_{11}^{(-k)}(x)=\\ell^{-1}\\left(\\ell\\left(\\hat{m}_{10}^{(-k)}(x)\\right)+\\ell\\left(\\hat{m}_{01}^{(-k)}(x)\\right)-\\ell\\left(\\hat{m}_{00}^{(-k)}(x)\\right)\\right), \\quad k=1, \\ldots, K\n$$\n\nFinally, assume that the rate conditions $\\left\\|\\zeta\\left(\\cdot ; \\hat{\\eta}^{(-k)}\\right)-\\zeta\\left(\\cdot ; \\eta^{*}\\right)\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$ and $\\left\\|h_{2}\\left(\\cdot ; \\hat{\\eta}^{(-k)}\\right)-\\right.$ $\\left.h_{2}\\left(\\cdot ; \\eta^{*}\\right)\\right\\|_{2, P^{*}}=o_{p}(1)$ hold, and additionally that $\\max \\left(\\left|\\zeta\\left(x ; \\eta^{*}\\right)\\right|,\\left|\\zeta\\left(x, \\hat{\\eta}^{(-k)}\\right)\\right|\\right) \\leqslant C$ for all $(s, z) \\in$ $\\{0,1\\}^{2}, x \\in \\mathcal{X}$, and folds $k=1, \\ldots, K$. Then the Lemma in Section A. 5 holds with $\\mathcal{M}=\\mathcal{M}_{5}$, $g\\left(\\cdot ; \\tau^{*}, \\eta^{*}\\right)=g\\left(\\cdot ; \\eta^{*}\\right)$, and $g\\left(\\cdot ; \\hat{\\tau}^{(-k)}, \\hat{\\eta}^{(-k)}\\right)=g\\left(\\cdot ; \\hat{\\eta}^{(-k)}\\right)$.\n\nProof. The proof is similar in structure to the argument in Appendix C.1. We must show the boundedness, approximate tangency, and rate conditions in the Lemma of Appendix A. 5 with\n\n$$\n\\begin{aligned}\n& h_{1}^{*}(r)=f\\left(r ; \\eta^{*}\\right) \\zeta\\left(x ; \\eta^{*}\\right), \\quad \\hat{h}_{1}^{(-k)}(r)=f\\left(r ; \\hat{\\eta}^{(-k)}\\right) \\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right) \\\\\n& h_{2}^{*}(x)=h_{2}\\left(x ; \\eta^{*}\\right), \\quad \\hat{h}_{2}^{(-k)}(x)=h_{2}\\left(x ; \\hat{\\eta}^{(-k)}\\right)\n\\end{aligned}\n$$\n\nBoundedness: We have $V^{*}(s, z, x) \\leqslant 1$ trivially since $Y \\in[0,1]$. Furthermore by the overlap condition (Assumption 3) it is clear that $f\\left(r ; \\eta^{*}\\right)$ is uniformly upper bounded for $P^{*}$-almost all $r$, a qualifier we omit hereafter in all inequalities and equalities in this proof, in absolute value by $C \\delta^{-2}$. Similarly, by the overlap in infinite-dimensional nuisance estimates condition in Assumption 4, we have $f\\left(r ; \\hat{\\eta}^{(-k)}\\right)$ is uniformly upper bounded in absolute value by $C \\delta^{-2}$ for all $k=1, \\ldots, K$. With $\\max \\left(\\left|\\zeta\\left(x ; \\eta^{*}\\right)\\right|,\\left|\\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right)\\right|\\right) \\leqslant C$ for all $k$, we conclude that $\\max \\left(\\left\\|h_{1}^{*}\\right\\|_{\\infty, P^{*}},\\left\\|\\hat{h}_{1}^{(-k)}\\right\\|_{\\infty, P^{*}}\\right) \\leqslant$ $C^{2} \\delta^{-2}$.\n\nApproximate tangency: Fix $k \\in\\{1, \\ldots, K\\}$ and consider the one-dimensional outcome mean function parametric submodel\n\n$$\n\\begin{aligned}\n& \\tilde{m}(s, z, x ; t)=m^{*}(s, z, x)+t\\left(\\hat{m}^{(-k)}(s, z, x)-m^{*}(s, z, x)\\right), \\quad(s, z) \\neq(1,1) \\\\\n& \\tilde{m}(1,1, x ; t)=\\ell^{-1}(\\ell(\\tilde{m}(1,0, x ; t))+\\ell(\\tilde{m}(0,1, x ; t))-\\ell(\\tilde{m}(0,0, x ; t)))\n\\end{aligned}\n$$\n\nindexed by $t$ in an open subset of $\\mathbb{R}$ containing $[0,1]$, where we note that $\\tilde{m}(s, z, x ; 1)=\\hat{m}^{(-k)}(s, z, x)$ for all $(s, z)$ (including for $(s, z)=(1,1)$ by (84)). The overlap and nuisance in overlap conditions ensure that this is a well-defined submodel in the sense that it does not violate the restriction $\\epsilon<m<1-\\epsilon$ in the definition of $\\mathcal{M}_{5}$ in (5).\n\nBy Taylor's theorem, for each $x$ there exists $\\tilde{t}=\\tilde{t}(x) \\in[0,1]$ for which\n\n$$\n\\begin{aligned}\n\\hat{m}^{(-k)}(1,1, x)-m^{*}(1,1, x) & =\\tilde{m}(1,1, x ; 1)-\\tilde{m}(1,1, x ; 0) \\\\\n& =\\left.\\frac{\\partial \\tilde{m}(1,1, x ; t)}{\\partial t}\\right|_{t=0}+\\left.\\frac{1}{2} \\frac{\\partial^{2} \\tilde{m}(1,1, x ; t)}{\\partial t^{2}}\\right|_{t=\\tilde{t}(x)}\n\\end{aligned}\n$$\n\nwhich implies that for all $(s, z, x)$ we have\n\n$$\n\\hat{m}^{(-k)}(s, z, x)-m^{*}(s, z, x)-\\left.\\hat{R}^{(-k)}(s, z, x)=\\frac{\\partial \\tilde{m}(s, z, x ; t)}{\\partial t}\\right|_{t=0}\n$$\n\nfor\n\n$$\n\\hat{R}^{(-k)}(s, z, x)=\\left.\\frac{s z}{2} \\frac{\\partial^{2} \\tilde{m}(1,1, x ; t)}{\\partial t^{2}}\\right|_{t=\\tilde{t}(x)}\n$$\n\nsince evidently\n\n$$\n\\frac{\\partial \\tilde{m}(s, z, x ; t)}{\\partial t}=m^{(-k)}(s, z, x)-m^{*}(s, z, x)\n$$\n\nfor $(s, z) \\neq(1,1)$. Hence $\\hat{m}^{(-k)}-m^{*}-\\hat{R}^{(-k)} \\in \\mathcal{S}_{\\mathcal{M}_{1}}$ and it remains to show the necessary rate conditions on $\\hat{R}^{(-k)}$.\n\nBy the chain rule\n\n$$\n\\frac{\\partial \\tilde{m}(1,1, x ; t)}{\\partial t}=\\frac{\\sum_{(s, z) \\neq(1,1)} \\ell^{\\prime}(\\tilde{m}(s, z, x ; t))\\left(\\hat{m}_{s z}^{(-k)}(x)-m_{s z}^{*}(x)\\right)}{\\ell^{\\prime}(\\tilde{m}(1,1, x ; t))}\n$$\n\nand then by the quotient rule\n\n$$\n\\frac{\\partial^{2} \\tilde{m}(1,1, x ; t)}{\\partial t^{2}}=\\frac{\\sum_{(s, z) \\neq(1,1)} \\ell^{\\prime \\prime}(\\tilde{m}(s, z, x ; t))\\left(\\hat{m}_{s z}^{(-k)}(x)-m_{s z}^{*}(x)\\right)^{2}-\\ell^{\\prime \\prime}(\\tilde{m}(1,1, x ; t))\\left(\\frac{\\partial \\tilde{m}(1,1, x ; t)}{\\partial t}\\right)^{2}}{\\ell^{\\prime}(\\tilde{m}(1,1, x ; t))}\n$$\n\nWe also note that for each $(s, z),\\left(s^{\\prime}, z^{\\prime}\\right) \\in\\{0,1\\}^{2}$, we have\n\n$$\n\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} S_{i} Z_{i}\\left|\\hat{m}_{s z}^{(-k)}\\left(X_{i}\\right)-m_{s z}^{*}\\left(X_{i}\\right)\\right|\\left|\\hat{m}_{s^{\\prime} z^{\\prime}}^{(-k)}\\left(X_{i}\\right)-m_{s^{\\prime} z^{\\prime}}^{*}\\left(X_{i}\\right)\\right|=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nby the Cauchy-Schwarz inequality, the rate condition $\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$, and Lemma 4.\nNote $\\ell^{\\prime}$ and $\\ell^{\\prime \\prime}$ are uniformly bounded, i.e. there exist $0<c<C<\\infty$ for which\n\n$$\nc \\leqslant \\ell^{\\prime}(\\tilde{m}(s, z, x ; t)) \\leqslant C, \\quad\\left|\\ell^{\\prime \\prime}(\\tilde{m}(s, z, x ; t))\\right| \\leqslant C\n$$\n\nfor all $(s, z, x), t \\in[0,1]$. Hence\n\n$$\n\\begin{aligned}\n& \\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}}\\left|\\hat{R}^{(-k)}\\left(R_{i}\\right)\\right| \\\\\n& \\quad \\leqslant \\frac{C}{2 c\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} S_{i} Z_{i}\\left[\\sup _{t \\in[0,1]}\\left(\\frac{\\partial \\tilde{m}\\left(1,1, X_{i} ; t\\right)}{\\partial t}\\right)^{2}+\\sum_{(s, z) \\neq(1,1)}\\left(\\hat{m}_{s z}^{(-k)}\\left(X_{i}\\right)-m_{s z}^{*}\\left(X_{i}\\right)\\right)^{2}\\right]\n\\end{aligned}\n$$\n\nBut\n\n$$\n\\begin{aligned}\n& \\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} S_{i} Z_{i} \\sup _{t \\in[0,1]}\\left(\\frac{\\partial \\tilde{m}\\left(1,1, X_{i} ; t\\right)}{\\partial t}\\right)^{2} \\\\\n& \\quad \\leqslant \\frac{1}{c^{2}\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} S_{i} Z_{i} \\sup _{t \\in[0,1]}\\left(\\sum_{(s, z) \\neq(1,1)} \\ell^{\\prime}\\left(\\tilde{m}\\left(s, z, X_{i} ; t\\right)\\right)\\left(\\hat{m}_{s z}^{(-k)}\\left(X_{i}\\right)-m_{s z}^{*}\\left(X_{i}\\right)\\right)\\right)^{2} \\\\\n& =\\frac{1}{c^{2}\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} S_{i} Z_{i} \\sup _{t \\in[0,1]} \\sum_{(s, z) \\neq(1,1)} \\sum_{\\left(s^{\\prime}, z^{\\prime}\\right) \\neq(1,1)} \\ell^{\\prime}\\left(\\tilde{m}\\left(s, z, X_{i} ; t\\right)\\right) \\ell^{\\prime}\\left(\\tilde{m}\\left(s^{\\prime}, z^{\\prime}, X_{i} ; t\\right)\\right)\\left(\\hat{m}_{s z}^{(-k)}\\left(X_{i}\\right)-m_{s z}^{*}\\left(X_{i}\\right)\\right) \\times \\\\\n& \\left(\\hat{m}_{s^{\\prime} z^{\\prime}}^{(-k)}\\left(X_{i}\\right)-m_{s^{\\prime} z^{\\prime}}^{*}\\left(X_{i}\\right)\\right) \\\\\n& \\leqslant \\frac{C^{2}}{c^{2}\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} \\sum_{(s, z) \\neq(1,1)} \\sum_{\\left(s^{\\prime}, z^{\\prime}\\right) \\neq(1,1)} S_{i} Z_{i}\\left|\\hat{m}_{s z}^{(-k)}\\left(X_{i}\\right)-m_{s z}^{*}\\left(X_{i}\\right)\\right|\\left|\\hat{m}_{s^{\\prime} z^{\\prime}}^{(-k)}\\left(X_{i}\\right)-m_{s^{\\prime} z^{\\prime}}^{*}\\left(X_{i}\\right)\\right| \\\\\n& =o_{p}\\left(N^{-1 / 2}\\right) \\text { by (85) }\n\\end{aligned}\n$$\n\nwhile\n\n$$\n\\frac{1}{\\left|\\mathcal{I}_{k}\\right|} \\sum_{i \\in \\mathcal{I}_{k}} \\sum_{(s, z) \\neq(1,1)} S_{i} Z_{i}\\left(\\hat{m}_{s z}^{(-k)}\\left(X_{i}\\right)-m_{s z}^{*}\\left(X_{i}\\right)\\right)^{2}=o_{p}\\left(N^{-1 / 2}\\right) \\text { by (85). }\n$$\n\nThis completes the proof of the required rate conditions on $\\hat{R}^{(-k)}$.\nRate conditions: Fixing $k \\in\\{1, \\ldots, K\\}$, we have $\\left\\|\\hat{m}^{(-k)}-m^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$ by (80) and (84). With $\\left\\|\\hat{h}_{2}^{(-k)}-h_{2}^{*}\\right\\|_{2, P^{*}}=o_{p}(1)$ by assumption, to show the rate conditions it suffices to show that $\\left\\|\\hat{h}_{1}^{(-k)}-h_{1}^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$. To that end, we write\n\n$$\n\\hat{h}_{1}^{(-k)}(r)-h_{1}^{*}(r)=f\\left(r ; \\hat{\\eta}^{(-k)}\\right) \\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right)-f\\left(r ; \\eta^{*}\\right) \\zeta\\left(x ; \\eta^{*}\\right)\n$$\n\nand note that since $\\left|f\\left(r ; \\eta^{*}\\right)\\right| \\leqslant C$ as shown above and we have $\\max \\left(\\left|\\zeta\\left(x ; \\eta^{*}\\right)\\right|,\\left|\\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right)\\right|\\right) \\leqslant C$ and $\\left\\|\\zeta\\left(x ; \\hat{\\eta}^{(-k)}\\right)-\\zeta\\left(x ; \\eta^{*}\\right)\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right)$ by assumption, it suffices by Lemma 3 to show that\n\n$$\n\\int\\left(f\\left(r ; \\hat{\\eta}^{(-k)}\\right)-f\\left(r ; \\eta^{*}\\right)\\right)^{2} d P^{*}(w)=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nWe compute\n\n$$\n\\begin{aligned}\n& \\left(f\\left(s, z, x ; \\hat{\\eta}^{(-k)}\\right)-f\\left(s, z, x ; \\eta^{*}\\right)\\right)^{2} \\\\\n& =s z\\left(\\ell\\left(\\hat{m}_{11}^{(-k)}(x)\\right)-\\ell\\left(m_{11}^{*}(x)\\right)\\right)^{2} \\\\\n& \\quad+s(1-z)\\left(\\frac{\\hat{e}^{(-k)}(x) \\ell^{\\prime}\\left(\\hat{m}_{10}^{(-k)}(x)\\right)}{1-\\hat{e}^{(-k)}(x)}-\\frac{e^{*}(x) \\ell^{\\prime}\\left(m_{10}^{*}(x)\\right)}{1-e^{*}(x)}\\right)^{2} \\\\\n& \\quad+(1-s) z\\left(\\frac{\\hat{p}^{(-k)}(x) \\hat{e}^{(-k)}(x) \\ell^{\\prime}\\left(\\hat{m}_{01}^{(-k)}(x)\\right)}{\\left(1-\\hat{p}^{(-k)}(x)\\right) \\hat{q}^{(-k)}(x)}-\\frac{p^{*}(x) e^{*}(x) \\ell^{\\prime}\\left(m_{01}^{*}(x)\\right)}{\\left(1-p^{*}(x)\\right) q^{*}(x)}\\right)^{2} \\\\\n& \\quad+(1-s)(1-z)\\left(\\frac{\\hat{p}^{(-k)}(x) \\hat{e}^{(-k)}(x) \\ell^{\\prime}\\left(\\hat{m}_{00}^{(-k)}(x)\\right)}{\\left(1-\\hat{p}^{(-k)}(x)\\right)\\left(1-\\hat{q}^{(-k)}(x)\\right)}-\\frac{p^{*}(x) e^{*}(x) \\ell^{\\prime}\\left(m_{00}^{*}(x)\\right)}{\\left(1-p^{*}(x)\\right)\\left(1-q^{*}(x)\\right)}\\right)^{2}\n\\end{aligned}\n$$\n\nBy (80) and (86)\n\n$$\n\\left\\|\\ell^{\\prime} \\circ \\hat{m}_{s s}^{(-k)}-\\ell^{\\prime} \\circ m^{*}\\right\\|_{2, P^{*}}=o_{p}\\left(N^{-1 / 4}\\right), \\quad \\forall(s, z) \\in\\{0,1\\}^{2}\n$$\n\nTaking $(s, z)=(1,1)$ above shows\n\n$$\n\\int s z\\left(\\ell\\left(\\hat{m}_{11}^{(-k)}(x)\\right)-\\ell\\left(m_{11}^{*}(x)\\right)\\right)^{2} d P^{*}(w)=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nFor the remaining three terms, by Assumption 3, overlap in infinite-dimensional nuisance estimates, and (80), we know by Lemma 3 that\n\n$$\n\\int\\left(\\frac{\\hat{e}^{(-k)}(x)}{1-\\hat{e}^{(-k)}(x)}-\\frac{e^{*}(x)}{1-e^{*}(x)}\\right)^{2} d P^{*}(w)=\\int\\left(\\frac{\\hat{e}^{(-k)}(x)-e^{*}(x)}{\\left(1-e^{*}(x)\\right)\\left(1-\\hat{e}^{(-k)}(x)\\right)}\\right)^{2}=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nSimilarly\n\n$$\n\\int\\left(\\frac{\\hat{p}^{(-k)}(x)}{1-\\hat{p}^{(-k)}(x)}-\\frac{p^{*}(x)}{1-p^{*}(x)}\\right)^{2} d P^{*}(w)=\\int\\left(\\frac{\\hat{p}^{(-k)}(x)-p^{*}(x)}{\\left(1-p^{*}(x)\\right)\\left(1-\\hat{p}^{(-k)}(x)\\right)}\\right)^{2} d P^{*}(w)=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nand\n\n$$\n\\int\\left(\\frac{1}{1-\\hat{r}^{(-k)}(x)}-\\frac{1}{1-r^{*}(x)}\\right)^{2} d P^{*}(w)+\\int\\left(\\frac{1}{\\hat{r}^{(-k)}(x)}-\\frac{1}{r^{*}(x)}\\right)^{2} d P^{*}(w)=o_{p}\\left(N^{-1 / 2}\\right)\n$$\n\nThen applying Lemma 3 recursively we conclude\n\n$$\n\\begin{gathered}\n\\int s(1-z)\\left(\\frac{\\hat{e}^{(-k)}(x) \\ell^{\\prime}\\left(\\hat{m}_{10}^{(-k)}(x)\\right)}{1-\\hat{e}^{(-k)}(x)}-\\frac{e^{*}(x) \\ell^{\\prime}\\left(m_{10}^{*}(x)\\right)}{1-e^{*}(x)}\\right)^{2} d P^{*}(w)=o_{p}\\left(N^{-1 / 2}\\right) \\\\\n\\int(1-s) z\\left(\\frac{\\hat{p}^{(-k)}(x) \\hat{e}^{(-k)}(x) \\ell^{\\prime}\\left(\\hat{m}_{01}^{(-k)}(x)\\right)}{\\left(1-\\hat{p}^{(-k)}(x)\\right) \\hat{r}^{(-k)}(x)}-\\frac{p^{*}(x) e^{*}(x) \\ell^{\\prime}\\left(m_{01}^{*}(x)\\right)}{\\left(1-p^{*}(x)\\right) r^{*}(x)}\\right)^{2} d P^{*}(w)=o_{p}\\left(N^{-1 / 2}\\right), \\quad \\text { and } \\\\\n\\int(1-s)(1-z)\\left(\\frac{\\hat{p}^{(-k)}(x) \\hat{e}^{(-k)}(x) \\ell^{\\prime}\\left(\\hat{m}_{00}^{(-k)}(x)\\right)}{\\left(1-\\hat{p}^{(-k)}(x)\\right)\\left(1-\\hat{r}^{(-k)}(x)\\right)}-\\frac{p^{*}(x) e^{*}(x) \\ell^{\\prime}\\left(m_{00}^{*}(x)\\right)}{\\left(1-p^{*}(x)\\right)\\left(1-r^{*}(x)\\right)}\\right)^{2} d P^{*}(w)=o_{p}\\left(N^{-1 / 2}\\right)\n\\end{gathered}\n$$\n\nas desired.", "tables": {}, "images": {}}, {"section_id": 29, "text": "# D Details of numerical simulations in Section 5 \n\nHere we specify the full data generating processes for the numerical simulations in Section 5.\nFor the simulations in Section 5, in the discrete scenario, we emulate the data generating process in Section 5 of Guo et al. (2022):\n\n$$\n\\begin{aligned}\nX= & \\left(X_{1}, X_{2}\\right) \\sim \\operatorname{Bern}(0.5) \\times \\operatorname{Bern}(0.5) \\\\\n& Z \\mid X \\sim \\operatorname{Bern}\\left(X_{1}-X_{2}\\right) \\\\\n& Y \\mid X, Z \\sim \\operatorname{Bern}\\left(\\operatorname{expit}\\left(-0.5+Z+(1-2 Z)\\left(X_{1}-X_{2}\\right)\\right)\\right)\n\\end{aligned}\n$$\n\nFor the continuous scenario, we have\n\n$$\n\\begin{aligned}\nX=\\left(X_{1}, X_{2}\\right) & \\sim \\mathbb{U}(-1,1) \\times \\mathbb{U}(-1,1) \\\\\nZ \\mid X & \\sim \\operatorname{Bern}\\left(X_{1}-X_{2}\\right) \\\\\nY \\mid X, Z & \\sim \\operatorname{Bern}\\left(\\operatorname{expit}\\left(-0.5+Z+(1-2 Z)\\left(X_{1}-X_{2}\\right)+\\left(1.5 Z-1\\right) X_{1} X_{2}\\right)\\right)\n\\end{aligned}\n$$\n\nTo generate the simulated observational dataset, we draw observations from these DGP's with the selection process described in Section 5 until 3000 selected observations are obtained. Then additional observations from the above DGP's are generated without selection to form the RCT.", "tables": {}, "images": {}}, {"section_id": 30, "text": "# E Implementation detail for control variate estimator \n\nAs indicated in the main text, our implementation of the control variate estimator $\\hat{\\tau}_{\\mathrm{cv}}$ follows the guidance of Guo et al. (2022). In the discrete scenario simulation, the differences $\\left\\{O R_{1}(x)-\\right.$ $\\left.O R_{0}(x) \\mid x \\in\\{0,1\\}^{2}\\right\\}$ are used as four control variates. In the continuous scenario simulation and the Spambase data example, we use the average of $\\log \\left(O R_{1}(x)\\right)-\\log \\left(O R_{0}(x)\\right)$ over 50 randomly chosen distinct values of the covariates $x$ in the combined RCT and observational datasets as a single control variate. As the odds ratios are determinstic transformations of the outcome mean function $m$, we can estimate all of these control variates via deterministic transformations of an outcome mean function estimator $\\hat{m}$. We use the same model for $\\hat{m}$ as the cross-fit outcome mean function estimates $\\hat{m}^{(-k)}$ used for computing the baseline estimator $\\hat{\\tau}_{\\text {ba }}$ (i.e. the $+1 /+2$ estimator in the discrete simulation scenario, MARS in the continuous simulation scenario ) except that we refit the model to the entire dataset (i.e. avoiding cross-fitting) as cross-fitting is not known to improve estimation of mean functions at a single point.\n\nWe can write\n\n$$\n\\hat{\\tau}_{\\mathrm{cv}}=\\hat{\\tau}_{\\mathrm{ba}}-\\hat{\\Gamma} \\hat{\\lambda}\n$$\n\nwhere $\\hat{\\tau}_{\\text {ba }}$ is the initial baseline estimator (AIPW or AIPSW in all simulations and the data example), $\\hat{\\lambda}$ is the estimate of the control variate(s) viewed as a vector, and $\\hat{\\Gamma}$ is an estimate of the optimal adjustment factor $\\operatorname{cov}\\left(\\hat{\\lambda}, \\hat{\\tau}_{\\text {ba }}\\right) \\operatorname{cov}(\\hat{\\lambda})^{-1}$. The same adjustment $\\hat{\\Gamma}$ is used for all Monte Carlo simulations in each simulation scenario. This adjustment is computed by calculating $\\hat{\\tau}_{\\text {ba }}$ and $\\hat{\\lambda}$ in each of $B=1000$ Monte Carlo replicates of the data-generating process of the scenario, independent of the Monte Carlo replicates from which we report results, and then extracting the appropriate components of the sample covariance matrix. Similarly, for the Spambase data example, we compute $\\hat{\\Gamma}$ using the sample covariance matrix of estimates $\\hat{\\tau}_{\\text {ba }}$ and $\\hat{\\lambda}$ computed on $B=1000$ bootstrapped replications of the dataset, independent of the bootstrap replications on which we report results. This is exactly what is done by Guo et al. (2022). We remark that in practice, an investigator will not have access to additional, independent data on which to compute a regression adjustment independently of the data, and even if they did, they would want to use it for causal estimation to gain full efficiency. Thus, we believe a more accurate estimate of the performance of $\\hat{\\tau}_{\\mathrm{cv}}$ would be reflected by computing a different adjustment factor $\\hat{\\Gamma}$ each time $\\hat{\\tau}_{\\mathrm{cv}}$ is computed. We presume this is not done in Guo et al. (2022) due to the computation time required (each computation of $\\hat{\\tau}_{\\text {ba }}$ requires estimating nuisance functions, so estimating separate adjustments $\\hat{\\Gamma}$ would require fitting $B$ times as many nuisance function estimates).", "tables": {}, "images": {}}, {"section_id": 31, "text": "# References \n\nS. Athey, P. J. Bickel, A. Chen, G. W. Imbens, and M. Pollmann. Semi-parametric estimation of treatment effects in randomised experiments. Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(5):1615-1638, 2023.\nE. Bareinboim and J. Pearl. Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences, 113(27):7345-7352, 2016.\nD. Benkeser and M. van der Laan. The highly adaptive lasso estimator. In 2016 IEEE international conference on data science and advanced analytics (DSAA), pages 689-696. IEEE, 2016.\nP. J. Bickel, C. A. Klaassen, Y. Ritov, and J. A. Wellner. Efficient and Adaptive Estimation for Semiparametric Models, volume 4. Springer, 1993.\nS. Chen, B. Zhang, and T. Ye. Minimax rates and adaptivity in combining experimental and observational data, Sept. 2021. URL http://arxiv.org/abs/2109.10522. arXiv:2109.10522 [math, stat].\nD. Cheng and T. Cai. Adaptive combination of randomized and observational data, Nov. 2021. URL http://arxiv.org/abs/2111.15012. arXiv:2111.15012 [stat].\nV. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1-C68, 2018.\nV. Chernozhukov, W. K. Newey, and R. Singh. Automatic debiased machine learning of causal and structural effects. Econometrica, 90(3):967-1027, 2022.\nB. Colnet, I. Mayer, G. Chen, A. Dieng, R. Li, G. Varoquaux, J.-P. Vert, J. Josse, and S. Yang. Causal inference methods for combining randomized trials and observational studies: a review. Statistical Science, 39(1):165-191, 2024.\nI. J. Dahabreh, S. E. Robertson, E. J. Tchetgen, E. A. Stuart, and M. A. Hern\u00e1n. Generalizing causal inferences from individuals in randomized trials to all trial-eligible individuals. Biometrics, $75(2): 685-694,2019$.\nI. J. Dahabreh, S. E. Robertson, J. A. Steingrimsson, E. A. Stuart, and M. A. Hern\u00e1n. Extending inferences from a randomized trial to a new target population. Statistics in Medicine, 39(14): 1999-2014, 2020.\nL. E. Dang, J. M. Tarp, T. J. Abrahamsen, K. Kvist, J. B. Buse, M. Petersen, and M. van der Laan. A cross-validated targeted maximum likelihood estimator for data-adaptive experiment selection applied to the augmentation of RCT control arms with external data, Feb. 2023. URL http://arxiv.org/abs/2210.05802. arXiv:2210.05802 [stat].\nJ. H. Friedman. Multivariate adaptive regression splines. The Annals of Statistics, 19(1):1-67, 1991.\nJ. A. Gagnon-Bartsch, A. C. Sales, E. Wu, A. F. Botelho, J. A. Erickson, L. W. Miratrix, and N. T. Heffernan. Precise unbiased estimation in randomized experiments using auxiliary observational data. Journal of Causal Inference, 11(1), Jan. 2023. Publisher: De Gruyter.\n\nW. Guo, S. L. Wang, P. Ding, Y. Wang, and M. Jordan. Multi-source causal inference using control variates under outcome selection bias. Transactions on Machine Learning Research, 2022.\nJ. Hahn. On the role of the propensity score in efficient semiparametric estimation of average treatment effects. Econometrica, pages 315-331, 1998.\nO. Hines, O. Dukes, K. Diaz-Ordaz, and S. Vansteelandt. Demystifying statistical learning based on efficient influence functions. The American Statistician, 76(3):292-304, 2022.\nM. Huang, N. Egami, E. Hartman, and L. Miratrix. Leveraging population outcomes to improve the generalization of experimental results: Application to the JTPA study. The Annals of Applied Statistics, 17(3), 2023.\nN. Kallus, A. M. Puli, and U. Shalit. Removing hidden confounding by experimental grounding. In Advances in Neural Information Processing Systems, volume 31, 2018.\nE. H. Kennedy. Semiparametric doubly robust targeted double machine learning: a review. Handbook of Statistical Methods for Precision Medicine, pages 207-236, 2024.\nS. Klosin. Automatic double machine learning for continuous treatment effects. arXiv preprint arXiv:2104.10334, 2021.\nD. Lee, S. Yang, L. Dong, X. Wang, D. Zeng, and J. Cai. Improving trial generalizability using observational studies. Biometrics, 79(2):1213-1225, 2023.\nK. J. Lee and A. Schuler. RieszBoost: Gradient boosting for Riesz regression. arXiv preprint arXiv:2501.04871, 2025. URL https://arxiv.org/abs/2501.04871. arxiv:2501.04871[stat].\nF. Li, H. Hong, and E. A. Stuart. A note on semiparametric efficient generalization of causal effects from randomized trials to target populations. Communications in Statistics - Theory and Methods, 52(16):5767-5798, 2023a.\nS. Li and A. Luedtke. Efficient estimation under data fusion. Biometrika, 110(4):1041-1054, 2023.\nS. Li, P. B. Gilbert, R. Duan, and A. Luedtke. Data fusion using weakly aligned sources. Journal of the American Statistical Association, pages 1-28, 2025.\nX. Li, W. Miao, F. Lu, and X.-H. Zhou. Improving efficiency of inference in clinical trials with external control data. Biometrics, 79(1):394-403, 2023b.\nL. D. Liao, E. Hejbjerre-Frandsen, A. E. Hubbard, and A. Schuler. Transfer learning with efficient estimators to optimally leverage historical data in analysis of randomized trials, Nov. 2023. URL http://arxiv.org/abs/2305.19180. arXiv:2305.19180 [stat].\nX. Lin, J. M. Tarp, and R. J. Evans. Combining experimental and observational data through a power likelihood. Biometrics, 81(1), 2025.\nS. Milborrow. earth: Multivariate Adaptive Regression Splines, 2011. URL http://CRAN. R-project.org/package=earth. R package.\nW. K. Newey. Semiparametric efficiency bounds. Journal of Applied Econometrics, 5(2):99-135, 1990 .\n\nM. Oberst, A. D'Amour, M. Chen, Y. Wang, D. Sontag, and S. Yadlowsky. Understanding the risks and rewards of combining unbiased and possibly biased estimators, with applications to causal inference, 2023. URL http://arxiv.org/abs/2205.10467. arXiv:2205.10467 [stat].\nJ. M. Robins, A. Rotnitzky, and L. P. Zhao. Estimation of regression coefficients when some regressors are not always observed. Journal of the American Statistical Association, 89(427): 846-866, 1994.\nE. T. Rosenman, G. Basse, A. B. Owen, and M. Baiocchi. Combining observational and experimental datasets using shrinkage estimators. Biometrics, 79(4):2961-2973, 2023.\nC. Rothe. The value of knowing the propensity score for estimating average treatment effects. SSRN Electronic Journal, 2016. URL https://www.ssrn.com/abstract=2797560.\nA. Schuler, A. Hagemeister, and M. van der Laan. Highly adaptive ridge. arXiv preprint arXiv:2410.02680, 2024. URL https://arxiv.org/abs/2410.02680. arXiv:2410.02680 [stat].\nZ. Tan. Comment: Understanding OR, PS and DR. Statistical Science, 22(4), Nov. 2007.\nA. A. Tsiatis. Semiparametric Theory and Missing Data. Springer, 2006.\nA. W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge University Press, 2000.\nS. Yang, C. Gao, D. Zeng, and X. Wang. Elastic integrative analysis of randomised trial and real-world data for treatment heterogeneity estimation. Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(3):575-596, 2023.\nS. Yang, S. Liu, D. Zeng, and X. Wang. Data fusion methods for the heterogeneity of treatment effect and confounding function, 2024. URL https://arxiv.org/abs/2007.12922. arXiv:2007.12922 [stat].\nW. Zheng and M. J. van der Laan. Cross-validated targeted minimum-loss-based estimation. In Targeted Learning, pages 459-474. Springer New York, New York, NY, 2011.", "tables": {}, "images": {}}], "id": "2406.06941v3", "authors": ["Harrison H. Li"], "categories": ["stat.ME", "math.ST", "stat.TH"], "abstract": "We provide a novel characterization of semiparametric efficiency in a generic\nsupervised learning setting where the outcome mean function -- defined as the\nconditional expectation of the outcome of interest given the other observed\nvariables -- is restricted to lie in some known semiparametric function class.\nThe primary motivation is causal inference where a researcher running a\nrandomized controlled trial often has access to an auxiliary observational\ndataset that is confounded or otherwise biased for estimating causal effects.\nPrior work has imposed various bespoke assumptions on this bias in an attempt\nto improve precision via data fusion. We show how many of these assumptions can\nbe formulated as restrictions on the outcome mean function in the concatenation\nof the experimental and observational datasets. Then our theory provides a\nunified framework to maximally leverage such restrictions for precision gain by\nconstructing efficient estimators in all of these settings as well as in a wide\nrange of others that future investigators might be interested in. For example,\nwhen the observational dataset is subject to outcome-mediated selection bias,\nwe show our novel efficient estimator dominates an existing control variate\napproach both asymptotically and in numerical studies.", "updated": "2025-04-21T00:07:13Z", "published": "2024-06-11T04:49:04Z"}