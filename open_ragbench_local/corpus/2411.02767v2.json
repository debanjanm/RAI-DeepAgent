{"title": "A Convex Relaxation Approach to Generalization Analysis for Parallel\n  Positively Homogeneous Networks", "sections": [{"section_id": 0, "text": "## Abstract\n\nWe propose a general framework for deriving generalization bounds for parallel positively homogeneous neural networks-a class of neural networks whose input-output map decomposes as the sum of positively homogeneous maps. Examples of such networks include matrix factorization and sensing, single-layer multi-head attention mechanisms, tensor factorization, deep linear and ReLU networks, and more. Our general framework is based on linking the non-convex empirical risk minimization (ERM) problem to a closely related convex optimization problem over prediction functions, which provides a global, achievable lower-bound to the ERM problem. We exploit this convex lower-bound to perform generalization analysis in the convex space while controlling the discrepancy between the convex model and its non-convex counterpart. We apply our general framework to a wide variety of models ranging from low-rank matrix sensing, to structured matrix sensing, twolayer linear networks, two-layer ReLU networks, and single-layer multi-head attention mechanisms, achieving generalization bounds with a sample complexity that scales almost linearly with the network width.", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1 INTRODUCTION\n\nDespite significant recent advances in the analysis of deep neural networks (DNNs), key gaps persist in es-\n\n[^0]", "tables": {}, "images": {}}, {"section_id": 2, "text": "## Benjamin D. Haeffele\n\nUniversity of Pennsylvania", "tables": {}, "images": {}}, {"section_id": 3, "text": "## Ren\u00e9 Vidal\n\nUniversity of Pennsylvania\n\n\n[^0]:    Proceedings of the $28^{\\text {th }}$ International Conference on Artificial Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume 258. Copyright 2025 by the author(s).\n\nsis space (Neyshabur et al., 2017). As a result, the effective hypothesis space is much smaller than what classical bounds account for based on the expressivity of the model alone. Consequently, modern bounds focus on data-dependent, non-uniform approaches. For instance, margin bounds (Neyshabur et al., 2018; Golowich et al., 2018; Barron and Klusowski, 2019) provide specific generalization error bounds for DNNs trained to minimize max-margin type loss functions for classification tasks. Another line of research (Dziugaite and Roy, 2017; Arora et al., 2018; Banerjee et al., 2020) exploits the sensitivity of the non-convex landscapes around learned weights; however, this approach requires the estimation of hard quantities like expected sharpness and KL divergence and questions remain regarding the extent to which quantities such as sharpness explain network generalization (Wen et al., 2023; Andriushchenko et al., 2023).\n\nRecent work has observed that optimization methods such as SGD, even without explicit regularization, tend to yield solutions that generalize well, a notion known as implicit bias (Gunasekar et al., 2017, 2018a,b; Soudry et al., 2018; Li et al., 2020; HaoChen et al., 2021; Vardi, 2023). This stands in contrast to classical theory, which suggests that explicit regularization is necessary to avoid overfitting. For example, DNNs have been shown to converge toward maximummargin solutions in classification tasks (Soudry et al., 2018), while solutions in regression tasks often exhibit low-rank structures (Li et al., 2020) that generalize well. Although these analyses provide valuable insights, they are generally limited to specific objectives and types of neural network architectures.\n\nA key challenge in understanding the generalization properties of DNNs is their non-convex landscape. Indeed, convex landscapes are better understood, and numerous generalization bounds have already been derived (Shalev-Shwartz et al., 2009; Lugosi and Neu, 2022). We argue that bridging the gap between nonconvex and convex landscapes could provide a pathway to understanding generalization better. Our key contribution is to propose a new generalization analysis framework for DNNs based on linking their non-convex landscape to a convex one. Our framework builds upon Haeffele and Vidal (2017) and Vidal et al. (2022), who connected certain non-convex optimization problems to closely related convex ones. However, their work focuses on characterizing the optimization properties of such problems and does not consider generalization.\n\nPaper contributions. In this work, we use the idea of analyzing non-convex problems via a closely related convex problem to derive generalization bounds for a broad family of learning models, which take the form of sums of (slightly generalized) positively homogeneous\nfunctions whose parameters are regularized by sums of positively homogeneous functions of the same degree. This allows for a reinterpretation of the (empirical and expected) non-convex optimization problems as closely related to carefully constructed convex problems. We then apply concentration of measure techniques to the convexified version under reasonable data distributions and show that this also implies the concentration of the non-convex problem of interest. More specifically, we extend the finite-dimensional framework of Haeffele and Vidal (2017) and Vidal et al. (2022) to its infinite-dimensional counterpart, which allows us to derive generalization guarantees from a novel viewpoint by exploiting the connection between our problem of interest and a closely related convex problem. We note that other prior work (Bach, 2017) has also considered similar relationships between convex and non-convex problems for establishing generalization results. However, the generalization guarantees in Bach (2017) largely rely on Rademacher complexities, which results in a sample complexity that grows quadratically with the network width. In contrast, we exploit the relationship between the convex and nonconvex problems more directly, which allows us to derive bounds with an improved sample complexity.\n\nTo be more precise, our main results can be stated informally as follows. Let $N$ be the number of data points, $R$ be the number of positively homogeneous functions (or the width of the network) whose predictions are summed together to form the output, and $\\operatorname{dim}(\\mathcal{W})$ be the dimension of the parameters in one of the functions. When $N \\gtrsim \\tilde{\\mathcal{O}}\\left(R \\times \\operatorname{dim}(\\mathcal{W})\\right.$ ), we show that the generalization error can be bounded with high probability by two terms: the first term, dubbed the optimization error, which vanishes at a globally optimal solution, and the second term, dubbed the statistical error, which depends on the ratio $\\frac{R \\times \\operatorname{dim}(\\mathcal{W})}{N}$, and hence vanishes only asymptotically.\n\nOur results apply to a wide range of signal processing and DNN problems. The derived bounds achieve near state-of-the-art sample complexity for non-convex low-rank matrix sensing that match the lower bound provided by Cand\u00e8s and Plan (2011) for convex lowrank matrix sensing. By applying these general results to two-layer linear (and ReLU) neural networks with weight decay and multi-head attention models, a key component of transformer architecture (Vaswani et al., 2017), we obtain novel generalization bounds with \"tight\" ${ }^{1}$ sample complexities for both problems.\n\n[^0]\n[^0]:    ${ }^{1}$ Our notion of \"tight\" bounds corresponds to cases where the sample complexity scales linearly or nearly linearly (up to logarithmic factors) with the number of model parameters.\n\nOutline. The remainder of this paper is organized as follows. In $\\S 2$, we formulate the learning problem and introduce our approach. In $\\S 3$, we explore how learning problems can be bounded via convex surrogates. In $\\S 4$, we present the statistical bounds through the master theorem that provides generalization error bounds. In $\\S 5$, we apply the master theorem to various problems in signal processing and DNNs and compare our derived sample complexities with those in the existing literature. The supplementary material contains detailed proofs of the mathematical statements, validations of our framework's assumptions through simulations, and an additional survey of related works.\nNotation. For two random variables $(Z, W)$, drawn from a joint distribution $q$, we define $\\langle Z, W\\rangle_{q}=$ $\\mathbb{E}[\\langle Z, W\\rangle]$, where the expectation is with respect to the joint probability distribution $q$. For a generic function $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$, we denote $\\|f\\|_{L i p}$ as its Lipschitz constant; i.e., the smallest number $L_{f}$ such that $|f(x)-f(y)| \\leq L_{f}\\|x-y\\|$. A function $f$ is said to be integrable with respect to measure $q$, i.e., $f \\in L^{2}(q)$, if $\\left(\\int_{x \\in \\mathcal{X}}\\|f(x)\\|^{2} d q(x)\\right)^{1 / 2}<\\infty$. The inequality $f(x) \\gtrsim g(x)$, means that there exists a constant $c>0$ such that $f(x) \\geq c g(x)$. We define the ReLU function as $[x]_{+}=\\max (x, 0)$. For the matrix $U$ the variable $\\mathbf{u}_{j}$ corresponds to $j$ th column of $U$.", "tables": {}, "images": {}}, {"section_id": 4, "text": "## 2 PROBLEM FORMULATION\n\nGiven a realization of a pair $(X, Y) \\in \\mathcal{X} \\times \\mathcal{Y}$ from a distribution $\\mu$ with $\\mathcal{X} \\subset \\mathbb{R}^{n_{X}}, \\mathcal{Y} \\subset \\mathbb{R}^{n_{Y}}$, we consider a (non)parametric regression problem of the form $Y=g(X, \\epsilon)$, where $\\epsilon$ is a source of additional noise (typically independent from $X$ ). We are interested in approximating $g$ by the sum of $r$ prediction functions, $\\phi: \\mathcal{W} \\times \\mathbb{R}^{n_{X}} \\rightarrow \\mathbb{R}^{n_{Y}}$, parameterized by $W \\in \\mathcal{W}$, i.e.,\n\n$$\n\\hat{Y}=\\sum_{j=1}^{r} \\phi\\left(W_{j}\\right)(X)=\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)\n$$\n\nWe will additionally refer to $\\phi(W)(X)$ as the factor map/sub-network depending on the specific problem.\nOur goal is to learn the parameters $\\left\\{W_{j}\\right\\}^{2}$ that minimize the regularized population risk defined as\n\n$$\n\\begin{aligned}\n& \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right):=\\underbrace{\\mathbb{E}_{(X, Y)}\\left[\\ell\\left(Y, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)(X)\\right)\\right]}_{=: \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)\\right)_{\\mu}} \\\\\n& +\\lambda \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)\n\\end{aligned}\n$$\n\nwhere $Y=g(X, \\epsilon)$ is the target random variable, $\\ell(\\cdot, \\cdot)$ is the loss function, typically convex in the second argument, and $\\Theta_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)$ is an explicit regularization\n\n[^0]function which helps find structured parameters, such as minimum norm or sparse solutions. Specifically, the regularization term $\\Theta_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)$ is defined as\n\n$$\n\\Theta_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right):=\\sum_{j=1}^{r} \\theta\\left(W_{j}\\right)\n$$\n\nwhere $\\theta: \\mathcal{W} \\rightarrow \\mathbb{R}^{+}$is a regularization term for each factor map, and $\\lambda \\in \\mathbb{R}^{+}$is a regularization hyperparameter that controls the trade-off between loss reduction and inducing structure.\n\nNotice that we will minimize the population risk $\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)$ over both $r$ and $\\left\\{W_{j}\\right\\}_{j=1}^{r}$. More explicitly, we will allow for problems where, in addition to optimizing over the model parameters, one also optimizes over the number of prediction functions $r$ (e.g., the network width) during training. However, our results will also apply to a value of $r$ that is fixed $a$ priori.\nEstimating $\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)$ directly is challenging due to (i) the lack of access to the distribution $\\mu$, (ii) the fact that $\\left(\\left\\{W_{j}\\right\\}\\right)$ (and potentially the number $r$ ) are random variables dependent on the training data $\\left\\{\\left(X_{i}, Y_{i}\\right)\\right\\}$, and (iii) the non-linearity and potential non-convexity of $\\mathrm{NC}_{\\mu}$. We address the first point (as is standard) via empirical minimization of $\\mathrm{NC}_{\\mu}(\\cdot)$ using the empirical risk (or training error) defined via:\n\n$$\n\\begin{aligned}\n& \\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right):=\\underbrace{\\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(Y_{i}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)\\left(X_{i}\\right)\\right)}_{=: \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)\\right)_{\\mu_{N}}} \\\\\n& \\quad+\\lambda \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)\n\\end{aligned}\n$$\n\nwhere $\\mu_{N}$ denotes the empirical distribution of the samples $\\left\\{X_{i}, Y_{i}\\right\\}_{i=1}^{N}$. We define empirical risk minimization (ERM) via the $\\arg \\min$ of $\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)$. For concreteness, recall we also allow for the minimization over $r$ (provided $r$ is bounded above by some quantity independent of the data), though our results hold for any fixed $r$.\nNote that if we minimize the objective $\\mathrm{NC}_{\\mu_{N}}(\\cdot)$, there is no guarantee that we will also minimize $\\mathrm{NC}_{\\mu}(\\cdot)$. This discrepancy is quantified by the Generalization Error:\n\n$$\n\\begin{aligned}\n& \\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\\\\n& =\\left|\\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)\\right)_{\\mu}-\\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)\\right)_{\\mu_{N}}\\right|\n\\end{aligned}\n$$\n\nNote that the regularization terms containing $\\Theta_{r}$ are the same between the two objectives, giving the typical difference between the empirical and population losses.\nIn this work, we compute an upper bound for the generalization error at any stationary point of the empirical problem, $\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)$, under certain technical assumptions. To build our main results, we relate these\n\n\n[^0]:    ${ }^{2}$ We occasionally notate $\\left\\{W_{j}\\right\\}_{i=1}^{r}$ as $\\left\\{W_{j}\\right\\}$ for brevity of notation, but the dependence on $r$ is always implied.\n\nnon-convex objectives $\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)$ and $\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)$ to closely related convex objectives in the prediction space, respectively $\\mathrm{C}_{\\mu}\\left(f_{\\mu}\\right)$ and $\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}\\right)$, whose definitions will be introduced in $\\S 3$. This allows us to decompose the generalization error in (5) as:\n\n$$\n\\begin{aligned}\n& \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)=\\underbrace{\\left[\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{C}_{\\mu}\\left(f_{\\mu}\\right)\\right]}_{\\text {Population Gap }} \\\\\n& -\\underbrace{\\left[\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}\\right)\\right]}_{\\text {Empirical Gap }}+\\underbrace{\\left[\\mathrm{C}_{\\mu}\\left(f_{\\mu}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}\\right)\\right]}_{\\text {Convex Generalization Gap }}\n\\end{aligned}\n$$\n\nOur Theorem 1 bounds the Empirical Gap and the Population Gap. With these bounds, we then apply concentration techniques to bound the Convex Generalization Gap and obtain our main Theorem 2 which gives bounds for the generalization error in (5).", "tables": {}, "images": {}}, {"section_id": 5, "text": "## 3 CONVEX BOUNDS FOR LEARNING\n\nIn this section, we present bounds for the Empirical Gap and Population Gap through Theorem 1, linking our learning problem of interest to functions that are convex in the space of prediction functions. To begin, we state several requirements for our framework.\nAssumption 1 (Regularization). The regularization function $\\theta$ is positive semidefinite; i.e, $\\theta(0)=0$ and $\\theta(W) \\geq 0, \\forall W \\in \\mathcal{W}$\n\nThis is a mild assumption; it only ensures we do not impose negative regularization on the parameters $\\left\\{W_{j}\\right\\}$. Our next assumption is our main functional assumption on $\\phi$ and $\\theta$.\nAssumption 2 (Balanced Homogeneity of $\\phi$ and $\\theta$ ). The factor map $\\phi$ and the regularization map $\\theta$ can be scaled equally by non-negative scaling of (a subset of) the parameters. Formally, we assume that there exists sub-parameter spaces $(\\mathcal{K}, \\mathcal{H})$ from the parameter space $\\mathcal{W}$ such that $\\mathcal{K} \\times \\mathcal{H}=\\mathcal{W}, \\forall(\\mathbf{k}, \\mathbf{h}) \\in(\\mathcal{K}, \\mathcal{H})$, and $\\beta \\geq 0$ we have $\\phi((\\beta \\mathbf{k}, \\mathbf{h}))=\\beta^{p} \\phi((\\mathbf{k}, \\mathbf{h}))$ and $\\theta((\\beta \\mathbf{k}, \\mathbf{h}))=\\beta^{p} \\theta((\\mathbf{k}, \\mathbf{h}))$ for some $p>0$. Further, we assume that for bounded input $X$ the set $\\{\\phi(W)(X)$ : $\\forall W \\in \\mathcal{W}$ s.t. $\\theta(W) \\leq 1\\}$ is bounded.\n\nThis is a slight generalization of positive homogeneity, which only requires positive homogeneity in a subset of parameters, provided the image of the factor map for parameters with $\\theta(W) \\leq 1$ is bounded ${ }^{3}$.\n\n[^0]Our next assumption concerns the loss function $\\ell$.\nAssumption 3 (Convex Loss). The loss $\\ell(Y, \\hat{Y})$ is second-order differentiable (written $\\ell \\in \\mathcal{C}^{2}$ ), and $L$ smooth w.r.t. $\\hat{Y}$, i.e, for any $Y, \\hat{Y} \\in \\mathbb{R}^{n_{Y}}$\n\n$$\n0 \\preceq \\nabla_{\\hat{Y}}^{2} \\ell(Y, \\hat{Y}) \\preceq L I_{n_{Y}}\n$$\n\nAdditionally, the gradient of the loss is bi-Lipschitz smooth; that is, for all $Y_{1}, Y_{2}, \\hat{Y}_{1}, \\hat{Y}_{2} \\in \\mathbb{R}^{n_{Y}}$\n\n$$\n\\begin{aligned}\n\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(Y_{2}, \\hat{Y}_{2}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(Y_{1}, \\hat{Y}_{1}\\right)\\right\\| \\leq & L\\left[\\left\\|Y_{2}-Y_{1}\\right\\|_{2}\\right. \\\\\n& \\left.+\\left\\|\\hat{Y}_{2}-\\hat{Y}_{1}\\right\\|_{2}\\right]\n\\end{aligned}\n$$\n\nand the loss is constant if both the arguments are the same, i.e., for all $Y_{1}, Y_{2} \\in \\mathbb{R}^{n_{Y}}, \\ell\\left(Y_{1}, Y_{1}\\right)=\\ell\\left(Y_{2}, Y_{2}\\right)$.\n\nThis ensures that the loss function is convex and smooth. Furthermore, if the loss is $\\alpha$-strongly convex, i.e, $0 \\prec \\alpha I_{n_{Y}} \\preceq \\nabla_{\\hat{Y}}^{2} \\ell(Y, \\hat{Y}) \\preceq L I_{n_{Y}}$ we have derived tighter results (see the Appendix).\n\nWe define the induced regularization function as\n\n$$\n\\begin{aligned}\n\\Omega(f) & :=\\inf _{r,\\left\\{W_{j}\\right\\}} \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\\\\n\\text { s.t. } f(X) & =\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X) ; \\quad \\forall X \\in \\mathcal{X}\n\\end{aligned}\n$$\n\nwith the function taking value infinity if $f(X)$ cannot be realized for some choice of the parameters $\\left(r,\\left\\{W_{j}\\right\\}_{j=1}^{r}\\right)$. Using similar arguments as in Haeffele and Vidal (2015) it can be shown that under assumptions $1-2$, the function $\\Omega(f)$ is convex in the space of prediction functions; see Proposition 1 in the Appendix. Moreover, by Assumption 3, the loss function is convex with respect to the model predictions, which allows us to define the following two convex optimization problems over the space of prediction functions:\n\n$$\n\\mathrm{C}_{\\mu}(f):=\\mathbb{E}_{(X, Y)}[\\ell(Y, f(X))]+\\lambda \\Omega(f)\n$$\n\nwhere $f \\in L^{2}(\\mu)$, and\n\n$$\n\\mathrm{C}_{\\mu_{N}}(f):=\\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(Y_{i}, f\\left(X_{i}\\right)\\right)+\\lambda \\Omega(f)\n$$\n\nwhere $f \\in L^{2}\\left(\\mu_{N}\\right)$.\nFrom the definition of $\\Omega(f)$ we have that $\\mathrm{C}_{\\mu}$ and $\\mathrm{C}_{\\mu_{N}}$ are always lower bounds of $\\mathrm{NC}_{\\mu}$ and $\\mathrm{NC}_{\\mu_{N}}$, respectively, for any $\\left(f,\\left\\{W_{j}\\right\\}\\right)$ such that $f(X)=$ $\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)$, which becomes a tight bound for any parametrization $\\left(\\left\\{W_{j}\\right\\}\\right)$ of $f$ which achieves the infimum. As a result, we can relate solutions of the nonconvex problems to the corresponding convex problem via tools from convex analysis, as we establish in the following result.\n\n\n[^0]:    ${ }^{3}$ For example, $\\phi(v)(X)$ can take the form $v^{a} g(X)$ for $a \\geq 0$, and $\\theta(v)=|v|^{a}$, where $g: \\mathbb{R}^{n_{p}} \\rightarrow \\mathbb{R}$ is some fixed function. More generally, we can choose $\\phi\\left(v_{1}, v_{2}\\right)=$ $v_{1}^{a} g_{v_{2}}(X)$ and $\\theta\\left(v_{1}, v_{2}\\right)=\\left|v_{1}\\right|^{a}+\\delta_{v_{2}}\\left(v_{2}\\right)$, where $g_{v_{2}}$ : $\\mathbb{R}^{n_{p}} \\rightarrow \\mathbb{R}$ is a function parameterized by $v_{2} \\in \\mathcal{V}_{2}$ and has a bounded range for bounded inputs.\n\nTheorem 1 (Convex Bounds for Learning). Under assumptions 1-3, let $f_{\\mu_{N}}^{*}\\left(\\right.$ or $\\left.f_{\\mu}^{*}\\right)$ be the global minimizer for $\\mathrm{C}_{\\mu_{N}}(\\cdot)$ (or $\\left.\\mathrm{C}_{\\mu}(\\cdot)\\right)$. For any stationary points $\\left(r,\\left\\{W_{j}\\right\\}\\right)$ of the function $\\mathrm{NC}_{\\mu_{N}}(\\cdot)$ and any $f \\in L^{2}(\\mu) \\cap$ $L^{2}\\left(\\mu_{N}\\right)$ the following are true:\n\n1. Empirical optimality gap:\n\n$$\n\\begin{aligned}\n& \\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}^{*}\\right) \\leq \\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\mathrm{C}_{\\mu_{N}}(f) \\\\\n& \\quad+\\lambda \\Omega(f)\\left[\\Omega_{\\mu_{N}}^{o}\\left(-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]\n\\end{aligned}\n$$\n\n2. Population optimality gap:\n\n$$\n\\begin{aligned}\n\\mathrm{C}_{\\mu}\\left(f_{\\mu}^{*}\\right) & \\leq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\mathrm{C}_{\\mu}(f) \\\\\n+\\lambda & \\left.\\Omega(f)\\left[\\Omega_{\\mu}^{o}\\left(-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]\\right] \\\\\n& +\\left[\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}\\right. \\\\\n& \\left.-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}\\right]\n\\end{aligned}\n$$\n\nwhere $\\Omega_{q}^{o}(\\cdot)$ is referred to as polar in the measure $q$ defined as\n\n$$\n\\Omega_{q}^{o}(g):=\\sup _{\\theta(W) \\leq 1}\\langle g, \\phi(W)\\rangle_{q}\n$$\n\nReaders are referred to Appendix A for the proof with extensions to strongly convex functions.\n\nThe population optimality gap is obtained by an infinite-dimensional extension of Proposition 3 in Haeffele and Vidal (2020). The additional term in the population optimality gap (13) arises from the fact that the stationary points of $\\mathrm{ERM}, \\mathrm{NC}_{\\mu_{N}}(\\cdot)$, are not necessarily the same as those of $\\mathrm{NC}_{\\mu}(\\cdot)$.\nFrom equation (5) the goal is to bound the difference between the original non-convex formulations $\\mathrm{NC}_{\\mu_{N}}$ and $\\mathrm{NC}_{\\mu}$. By Theorem 1, we established the optimality gaps for both empirical and population non-convex optimization problems, and by computing the difference between equation (12) and (13), with algebraic manipulation we arrive at the following quantities:\n\n- Convex Generalization Gap: The convex generalization gap is defined as $\\left|C_{\\mu}(f)-C_{\\mu_{N}}(f)\\right|$.\n- Polar Gap: By virtue of the fact that the loss functions each contain the respective polars, we define the Polar Gap as the quantity $\\left|\\Omega_{\\mu}^{o}\\left(\\nabla_{\\bar{Y}} \\ell(g, f)\\right)-\\right.$ $\\Omega_{\\mu_{N}}^{o}\\left(\\nabla_{\\bar{Y}} \\ell(g, f)\\right)$\n- Equilibria Gap: We define the Equilibria Gap via $\\left|\\left\\langle\\nabla_{\\bar{Y}} \\ell(g, f), f\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\bar{Y}} \\ell(g, f), f\\right\\rangle_{\\mu}\\right|$.\n- Norm Gap: The final remaining quantity is defined via $\\left\\|\\left\\|f_{\\mu}^{*}-f\\right\\|_{\\mu_{N}}^{2}-\\right\\| f_{\\mu}^{*}-f\\left\\|_{\\mu}^{2}\\right\\|$. This quantity applies only to strongly convex functions (see the Appendix).\n\nA major technical contribution of this paper is to demonstrate that each of these quantities uniformly concentrates at a rate equal to or smaller than the \"statistical error\" under certain realistic assumptions that are discussed in $\\S 4$. The only remaining term from Theorem 1 is the quantity $\\Omega\\left(f_{\\mu}\\right)\\left[\\Omega_{\\mu_{N}}^{o}(\\cdot)-1\\right]$, which bounds the sub-optimality (in objective value) of the current stationary point for the empirical optimization problem. This term approaches zero at the global optimum of $\\mathrm{NC}_{\\mu_{N}}$ (see $\\S \\mathrm{A}$ in the Appendix).", "tables": {}, "images": {}}, {"section_id": 6, "text": "## 4 STATISTICAL BOUNDS\n\nIn Theorem 1, we established bounds for the Empirical Gap and Population Gap. Building on these results, we identified key quantities such as the Convex Generalization Gap, Polar Gap, Equilibrium Gap, and Norm Gap, all of which can be controlled under certain general conditions (Assumptions 1-6, along with Assumption $7^{\\prime}$ from the Appendix) that we state momentarily. In this section, we present Theorem 2, which consolidates these bounds to derive our main generalization error bound. For clarity and to minimize technical complexity, we present Theorem 2 with Assumption 7 , which a stronger version of Assumption $7^{\\prime}$.\nTo begin, we state our additional assumptions. We assume that $\\phi$ is Lipschitz.\nAssumption 4 (Lipschitz Continuity of $\\phi$ ). Let $\\mathcal{B}$ be some compact subset of $\\mathcal{W}$, and denote\n\n$$\n\\mathcal{F}_{\\theta}:=\\{W: \\theta(W) \\leq 1\\} \\cap \\mathcal{B} \\subseteq \\mathbb{B}\\left(r_{\\theta}\\right)\n$$\n\nwhere $\\mathbb{B}\\left(r_{\\theta}\\right)$ is the $L_{2}$ ball with radius $r_{\\theta} .{ }^{4}$ The factor map $\\phi$ is Lipschitz continuous with respective to inputs for any choice of parameters $W \\in \\mathcal{F}_{\\theta}$, i.e,\n\n$$\nL_{\\phi}:=\\sup _{W \\in \\mathcal{F}_{\\theta}}\\|\\phi(W)\\|_{L i p}<\\infty\n$$\n\nOur next assumption imposes tail conditions on the random variables $(X, Y)$.\nAssumption 5 (Data Model). The input data $X \\in$ $\\mathbb{R}^{n_{X}}$ is drawn from the 1-Lipschitz concentrated subGaussian distribution with a proxy variance $\\sigma_{X}^{2} / n_{X}$;\n\n[^0]\n[^0]:    ${ }^{4}$ The radius $r_{\\theta}$ can depend on the dimension of $W$. For instance, suppose $W \\in \\mathbb{R}^{n}$ and $\\theta(W)=\\|W\\|_{1}$, as $\\|W\\|_{1} \\leq \\sqrt{n}\\|W\\|_{2}$, then $r_{\\theta}$ must be at least $\\sqrt{n}$. On another instance, suppose $W=\\left(\\mathbf{u} \\in \\mathbb{R}^{m}, \\mathbf{v} \\in \\mathbb{R}^{n}\\right)$, and $\\theta(W)=\\|\\mathbf{u}\\|_{2}\\|\\mathbf{v}\\|_{2}$; this requires $r_{\\theta}$ to be at least $1 / 2$.\n\ni.e., for any 1-Lipschitz continuous function, $h$ : $\\mathbb{R}^{n_{X}} \\rightarrow \\mathbb{R}$ there exists $c>0$ such that\n\n$$\nP\\left(\\left|h(X)-\\mathbb{E}_{X}[h(X)]\\right| \\geq \\epsilon\\right) \\leq c \\exp \\left(-\\frac{n_{X} \\epsilon^{2}}{2 \\sigma_{X}^{2}}\\right)\n$$\n\nThe target function $Y$ takes the form $Y=g(X, \\epsilon)$, where $g \\in L^{2}(\\mu)$ is bi-Lipschitz in $X$ and $\\epsilon$; that is,\n\n$$\n\\begin{aligned}\n\\left\\|g\\left(X_{2}, \\epsilon_{2}\\right)-g\\left(X_{1}, \\epsilon_{1}\\right)\\right\\|_{2} \\leq\\|g\\|_{L i p} & {\\left[\\left\\|X_{2}-X_{1}\\right\\|_{2}\\right.} \\\\\n& \\left.+\\left\\|\\epsilon_{2}-\\epsilon_{1}\\right\\|_{2}\\right]\n\\end{aligned}\n$$\n\nand $\\epsilon \\sim \\mathcal{N}\\left(0,\\left(\\sigma_{Y \\mid X}^{2} / n_{E}\\right) I\\right)$ in $\\mathbb{R}^{n_{E}}$.\nWe note that the above assumption is mild. While extending our framework to heavy-tailed distributions are likely possible; it would require a more intricate analysis and may result in worse error rates and larger sample complexities.\nOur next assumption concerns the possible functions learned via empirical risk minimization.\nAssumption 6 (Hypothesis class). Stationary points of $N C_{\\mu_{N}}(\\cdot)$ have bounded regularization and bounded width, $r \\leq R$, almost surely. The input-output map, $\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)$ has Lipschitz constant at most $\\gamma$, and the parameters are bounded. Let $\\mathcal{B}_{R} \\subseteq \\mathcal{W}^{R}$ be some compact set; then the hypothesis class is defined as\n\n$$\n\\mathcal{F}_{\\mathcal{W}}:=\\left\\{\\left\\{W_{j}\\right\\}_{j=1}^{r}:\\left\\|\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{L i p} \\leq \\gamma\\right\\} \\cap \\mathcal{B}_{R}\n$$\n\nIn words, the set of maps learned through ERM are essentially Lipschitz in the parameters $\\left\\{W_{j}\\right\\}$, and, furthermore, the $\\left\\{W_{j}\\right\\}$ are bounded (almost surely). Moreover, the assumption that $r \\leq R$ ensures that at most $R$ individual functions $\\left\\{W_{j}\\right\\}$ are needed, which implicitly imposes a \"low-complexity\" constraint on the learned function. Finally, note that we assume that $\\gamma$ does not depend on the width of the network. In practice, our empirical observations show that the Lipschitz constant does not increase with width, making it a realistic assumption. For further details, refer to the numerical simulations in $\\S \\mathrm{E}$ of Appendix.\nOur general master theorem, Theorem 4 in the Appendix, requires only Assumptions 1-6 and 7' (in the Appendix). For the sake of notational brevity, we state our main results with the slightly stronger Assumption 7 instead of Assumption 7'.\n\nAssumption 7 (Boundedness). For all $(X, Y) \\in \\mathcal{X} \\times$ $\\mathcal{Y}$, and $\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}$, the predictions, and gradients are bounded; i.e,\n\n$$\n\\left\\|\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)\\right\\| \\leq B_{\\Phi},\\left\\|\\nabla_{\\tilde{Y}} \\ell\\left(Y, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)\\right)\\right\\| \\leq B_{\\ell}\n$$\n\nFurther, for any $(X, Y) \\in \\mathcal{X} \\times \\mathcal{Y}$, for any $\\left\\{W_{j}\\right\\},\\left\\{\\tilde{W}_{j}^{\\prime}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}, W, \\tilde{W} \\in \\mathcal{F}_{\\theta}$, the network, $\\phi$ and $\\Phi_{r}$, are Lipschitz in the parameters; i.e,\n\n$$\n\\begin{aligned}\n& \\left\\|\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)-\\Phi_{r}\\left(\\left\\{\\tilde{W}_{j}\\right\\}\\right)(X)\\right\\|_{2} \\\\\n& \\leq \\tilde{L}_{\\Phi} \\max _{j}\\left\\|W_{j}-\\tilde{W}_{j}\\right\\|_{2}, \\text { and } \\\\\n& \\|\\phi(W)(X)-\\phi(\\tilde{W})(X)\\|_{2} \\leq \\tilde{L}_{\\phi}\\|W-\\tilde{W}\\|_{2} .\n\\end{aligned}\n$$\n\nAssumption 7 ensures that predictions and its gradients are bounded while the network being Lipschitz continuous on the parameter space for any inputs. Assumption 7 implicitly indicates that either the data points are uniformly bounded or the search space for the parameters is of small dimension, which can restrict the potential applications. However, as we demonstrate in the more general version (Theorem 4) in the Appendix, it suffices that the conditions above hold only for some convex set $\\mathcal{C}$, though this extension requires significantly more notation and discussion, so we do not include it here.\nTheorem 2 (Master Theorem). Suppose Assumptions $1-7$ hold. Let $\\delta \\in(0,1]$ be fixed, and let $f_{\\mu}^{*}$ be the global optimum of $\\mathrm{C}_{\\mu}$. Suppose that $\\gamma \\geq \\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}$, and define\n\n$$\n\\begin{aligned}\n& \\epsilon_{1}=16 \\gamma^{2} \\sigma_{X}^{2} \\max \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{L i p}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\} \\\\\n& \\epsilon_{2}=4 \\tilde{L}_{\\Phi} B_{\\Phi} \\max \\left\\{1,2 L+2 B_{\\ell} / B_{\\Phi}\\right. \\\\\n& \\left.8 \\Omega\\left(f_{\\mu}^{*}\\right)\\left(B_{\\ell} \\tilde{L}_{\\phi}\\right) /\\left(\\tilde{L}_{\\Phi} B_{\\Phi}\\right), 8 L \\Omega\\left(f_{\\mu}^{*}\\right)\\right\\}\n\\end{aligned}\n$$\n\nLet $\\left\\{W_{j}\\right\\}$ denote any stationary point of $\\mathrm{NC}_{\\mu_{N}}(\\cdot)$. Then with probability at least $1-\\delta$, it holds that\n\n$$\n\\begin{aligned}\n& \\frac{1}{n_{Y}}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\lesssim \\\\\n& \\quad \\underbrace{\\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)}_{\\text {Optimization Error }} \\underbrace{\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu_{N}}\\right)-1\\right]}_{\\text {Optimization Error }} \\\\\n& +\\underbrace{\\epsilon_{1} \\sqrt{\\frac{R \\cdot \\operatorname{dim}(\\mathcal{W}) \\log \\left(\\frac{\\gamma \\epsilon_{2} \\epsilon_{2}}{L_{\\phi}}\\right) \\log (N)+\\log \\left(\\frac{1}{2}\\right)}{N}}}_{\\text {Statistical Error }}\n\\end{aligned}\n$$\n\nRemarks: The generalization error is upper bounded by two terms:\n\n- the Optimization Error, which quantifies the distance to the globally optimal solution, and\n- the Statistical Error, or the intrinsic error that depends on the sample complexity and the noise.\nThe optimization error diminishes as we approach a global optimum of the ERM problem $\\mathrm{NC}_{\\mu_{N}}$ and vanishes at a global optimum, whereas the statistical error diminishes as the sample size increases relative to\n\nthe intrinsic dimension, i.e., when $N \\gtrsim R \\times \\operatorname{dim}(\\mathcal{W})$ (ignoring logarithmic factors). By a naive counting argument, there are $R \\times \\operatorname{dim}(\\mathcal{W})$ many parameters in the underlying network. As we will see in subsequent sections, this sample complexity turns out to be optimal or nearly optimal for a number of reasonable statistical settings. The implicit constants appearing in the result are universal and are not problem dependent.", "tables": {}, "images": {}}, {"section_id": 7, "text": "## 5 APPLICATIONS\n\nIn this section, we present applications of the Theorem 2 for low-rank matrix sensing, two-layer ReLU neural networks, and single-layer multi-head attention. To apply Theorem 2, we must compute the problemspecific quantities $\\Omega\\left(f_{\\kappa}^{*}\\right), \\Omega_{\\mu_{N}}^{\\circ}(\\cdot), L,\\|g\\|_{\\text {Lip }}, \\sigma_{X}, \\sigma_{Y \\mid X}$, $\\epsilon_{1}, \\epsilon_{2}, r_{\\theta}, \\gamma, L_{\\phi}$. For each application, we have estimated these quantities, with further details provided in the proofs located in Appendix C.1, C.4, and C.5, respectively. We summarize and compare the obtained sample complexities for the various applications with their state-of-the-art bounds in Table 1. The additional applications to structured matrix sensing and two-layer linear neural networks can be found in Appendix C. 2 and Appendix C.3, respectively.\nLow-rank matrix sensing: We first consider lowrank matrix sensing (Cand\u00e8s and Plan, 2011), which is a well-studied problem in the signal processing and statistics literature. Given a few linear measurements of an unknown low-rank matrix, the goal is to estimate the low-rank matrix in the presence of noise. One potential strategy is to define a convex program via nuclear-norm regularization (Cand\u00e8s and Recht, 2009). While recovery guarantees for this convex program are well-studied, solving it is a computationally intensive procedure involving computing a full singular value decomposition at each iteration. To address this issue, several authors have considered a non-convex variant that reparameterizes the low-rank matrix into its underlying left and right factors, which is known as the Burer-Monteiro factorization (Burer and Monteiro, 2003). While the new optimization problem runs faster in practice, it is also non-convex, and its properties can be difficult to analyze theoretically. Corollary (1) provides the bounds on the generalization error for this non-convex program.\n\nCorollary 1 (Low-Rank Matrix Sensing). Consider the true model for $(X, y)$, where $X \\in \\mathbb{R}^{m \\times n}$ is a random matrix with i.i.d. entries $X_{l k} \\sim \\mathcal{N}\\left(0, \\frac{1}{m \\kappa}\\right)$ and $y=\\left\\langle M^{*}, X\\right\\rangle+\\epsilon$, where $M^{*} \\in \\mathbb{R}^{m \\times n}$ and $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)$ is independent from $X$. For all $i \\in[N]$, let $\\left(X_{i}, y_{i}\\right)$ be i.i.d. samples from this true model. Consider the estimator $\\hat{y}=\\left\\langle U V^{T}, X\\right\\rangle$, where $U \\in \\mathbb{R}^{m \\times R}$ and $V \\in \\mathbb{R}^{n \\times R}$. Let $\\delta \\in(0,1]$ be fixed. Define the non-\nconvex problem\n\n$$\n\\begin{aligned}\n& \\mathrm{NC}_{\\mu_{N}}^{\\mathrm{MS}}((U, V)):=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left(y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right)^{2} \\\\\n& +\\lambda \\sum_{j=1}^{R}\\left\\|\\mathbf{u}_{j}\\right\\|_{2}\\left\\|\\mathbf{v}_{j}\\right\\|_{2}\n\\end{aligned}\n$$\n\nand define $\\mathrm{NC}_{\\mu}^{M S}((U, V))$ similarly with the sum over $i$ replaced by expectation taken over $(X, y)$.\nLet $(\\hat{U}, \\hat{V})$ be a stationary point of $\\mathrm{NC}_{\\mu_{N}}^{\\mathrm{MS}}(\\cdot)$. Suppose there exists $C_{U V}, B_{u}, B_{v}>0$ such that $\\left\\|\\hat{U} \\hat{V}^{T}\\right\\|_{2} \\leq$ $C_{U V}\\left\\|M^{*}\\right\\|_{*}$, and for all $j \\in[R],\\left\\|\\hat{\\mathbf{u}}_{j}\\right\\|_{2} \\leq B_{u},\\left\\|\\hat{\\mathbf{v}}_{j}\\right\\|_{2} \\leq$ $B_{v}$. Then with probability at least $1-\\delta$, it holds that\n\n$$\n\\begin{aligned}\n& \\left|\\mathrm{NC}_{\\mu}^{\\mathrm{MS}}((\\hat{U}, \\hat{V}))-\\mathrm{NC}_{\\mu_{N}}^{\\mathrm{MS}}((\\hat{U}, \\hat{V}))\\right| \\lesssim \\\\\n& \\left\\|M^{*}\\right\\|_{*}\\left[\\left\\|\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\left\\langle\\hat{U} \\hat{V}^{T}, X_{i}\\right\\rangle\\right) X_{i}\\right\\|_{2}-\\lambda\\right] \\\\\n& +C_{U V}^{2}\\left\\|M^{*}\\right\\|_{*}^{2} \\times \\\\\n& \\sqrt{\\frac{R \\log \\left(R\\left(C_{U V}+B_{u} B_{v}\\right)\\right)(m+n) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{aligned}\n$$\n\nRemarks: Observe that at a global minimum, the right side tends to zero when $R(m+n) / N \\rightarrow 0$, ignoring logarithmic terms. Existing literature on nonconvex noisy low-rank matrix sensing typically requires knowledge of true $\\operatorname{rank}\\left(M^{*}\\right)=R^{*}$, and the state-of-the-art sample complexity for this setting is of order $R^{*}(m+n)$ in the un-regularized setting (St\u00f6ger and Zhu, 2024). In contrast, Corollary 1 does not require knowledge of the true rank. However, if the estimated $\\operatorname{rank} R$ is too small $\\left(R<R^{*}\\right)$, then the optimization error still persists. In contrast, if $\\left(R \\geq R^{*}\\right)$ then optimization error can vanish subject to the ability of the algorithm utilized to reach stationary points, Haeffele and Vidal (2015) provides such guarantees.\nTwo-layer ReLU Networks: Next, we move on to two-layer ReLU networks, which introduce an additional nonlinearity with respect to the inputs. ReLU networks are widely used and proven to be universal approximators (Huang, 2020). Prior work on generalization analysis for ReLU networks is based on classical measures, such as Rademacher complexity (Bartlett et al., 2019). The following result circumvents the difficulty in the estimate of such classical measures.\nCorollary 2 (Two-Layer ReLU Neural Network). Consider the true model for $(\\mathbf{x}, \\mathbf{y})$, where $\\mathbf{x} \\sim$ $\\mathcal{N}\\left(0,(1 / n) I_{n}\\right) \\in \\mathbb{R}^{n}, \\mathbf{y}=U^{*}\\left[V^{* T} \\mathbf{x}\\right]_{+}+\\epsilon$, where $U^{*} \\in$ $\\mathbb{R}^{m \\times R^{*}}, V^{*} \\in \\mathbb{R}^{n \\times R^{*}}$, and $\\epsilon \\sim \\mathcal{N}\\left(0,\\left(\\sigma^{2} / m\\right) I_{m}\\right) \\in \\mathbb{R}^{m}$ independent from $\\mathbf{x}$. For all $i \\in[N]$, let $\\left(\\mathbf{x}_{i}, \\mathbf{y}_{i}\\right)$ be i.i.d. samples from this true model. Consider the estimator $\\hat{\\mathbf{y}}=U\\left[V^{T} \\mathbf{x}\\right]_{+}$, where $U \\in \\mathbb{R}^{m \\times R}, V \\in \\mathbb{R}^{n \\times R}$.\n\nTable 1: Comparisons with the state-of-the-art sample complexities. $N$ represents the number of data points.\n\n![table_0](table_0)\n\nLet $\\delta \\in(0,1]$ be fixed. Define the non-convex problem\n\n$$\n\\begin{aligned}\n\\mathbb{N C}_{\\mu_{N}}^{\\operatorname{ReLU}}((U, V)) & :=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-U\\left[V^{T} \\mathbf{x}_{i}\\right]_{+}\\right\\|_{+}^{2} \\\\\n& +\\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right)\n\\end{aligned}\n$$\n\nand define $\\mathbb{N C}_{\\mu}^{\\operatorname{ReLU}}((U, V))$ similarly with the sum over $i$ replaced by expectation taken over $(\\mathbf{x}, \\mathbf{y})$.\nLet $(\\hat{U}, \\hat{V})$ be a stationary point of $\\mathbb{N C}_{\\mu_{N}}^{\\operatorname{ReLU}}(\\cdot)$. Suppose there exists $C_{U V}, B_{u}, B_{v}>0$ such that $\\left\\|\\hat{U} \\hat{V}^{T}\\right\\|_{2} \\leq$ $C_{U V}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]$, and for all $j \\in[R],\\left\\|\\hat{\\mathbf{u}}_{j}\\right\\|_{2} \\leq$ $B_{u},\\left\\|\\hat{\\mathbf{v}}_{j}\\right\\|_{2} \\leq B_{v}$. Then with probability at least $1-\\delta$, it holds that\n\n$$\n\\begin{aligned}\n& \\frac{1}{m}\\left[\\mathbb{N C}_{\\mu}^{\\operatorname{ReLU}}((\\hat{U}, \\hat{V}))-\\mathbb{N C}_{\\mu_{N}}^{\\operatorname{ReLU}}((\\hat{U}, \\hat{V}))\\right] \\lesssim \\\\\n& \\frac{1}{2 m}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\\left[\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-\\hat{\\mathbf{y}}_{i}\\right\\|_{2}\\left\\|\\mathbf{x}_{i}\\right\\|_{2}-\\lambda\\right] \\\\\n& +C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right] \\times \\\\\n& {\\left[\\frac{R(m+n) \\log \\left(R(m+n)\\left(C_{U V}+B_{u}^{2}+B_{v}^{2}\\right)\\right) \\log (N)}{N}\\right.} \\\\\n& \\left.+\\frac{\\log (1 / \\delta)}{N}\\right]^{1 / 2}\n\\end{aligned}\n$$\n\nRemarks: Analogous to matrix sensing, when $R(m+$ $n) / N \\rightarrow 0$, the right side tends to zero at global optimality (ignoring logarithmic terms). Furthermore, Corollary 2 recovers the state-of-the-art result by Bartlett et al. (2019).\nTransformers: Finally, we move on to our last application (though of course, the applications are in fact myriad in principle) to a single layer multi-head attention, which are backbones for transformer-style architecture (Vaswani et al., 2017). In practice, transformers are shown to have remarkable generalization capabilities (Zhou et al., 2024). However, there is a lack of intensive theoretical analysis for this architecture. Few attempts on estimating the capacities of the attention mechanisms have been made in Edelman et al. (2022) and Trauger and Tewari (2024), among others. For our analysis, we consider the case where the output of the\nmodel is one particular token within the input (e.g., transformers use a dedicated class token for the output initialized as a constant vector). The output for one attention head is modeled as $V X \\sigma\\left((K X)^{T} Q \\mathbf{x}_{\\text {out }}\\right)$ where $\\mathbf{x}_{\\text {out }}$ is the column of $X$ corresponding to the transformer output. We then reparameterize $K^{T} Q \\mathbf{x}_{\\text {out }}=\\mathbf{z}$ and present the following result.\nCorollary 3 (Transformers). Consider the true model for $(X, \\mathbf{y})$, where $X \\in \\mathbb{R}^{n \\times T}$ is a random matrix with i.i.d. entries $X_{l k} \\sim \\mathcal{N}(0,1 /(n T))$ and $\\mathbf{y}=$ $A^{*} X \\mathbf{b}^{*}+\\epsilon$, where $A^{*} \\in \\mathbb{R}^{m \\times n}, \\mathbf{b}^{*} \\in \\mathbb{S}^{T-1}$ and $\\epsilon \\sim$ $\\mathcal{N}\\left(0,\\left(\\sigma^{2} / m\\right) I_{m}\\right)$ is independent from $X$. For all $i \\in$ $[N]$, let $\\left(X_{i}, \\mathbf{y}_{i}\\right)$ be i.i.d. samples from this true model. Consider the estimator $\\hat{\\mathbf{y}}=\\sum_{j=1}^{R} V_{j} X \\sigma\\left(X^{T} \\mathbf{z}_{j}\\right), V_{j} \\in$ $\\mathbb{R}^{n}, \\mathbf{z}_{j} \\in \\mathbb{R}^{n}$. Let $\\delta \\in(0,1]$ be fixed. Define the nonconvex problem\n\n$$\n\\begin{aligned}\n& \\mathbb{N C}_{\\mu_{N}}^{\\mathrm{TF}}\\left(\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}\\right):=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-\\sum_{j=1}^{R} V_{j} X_{i} \\sigma_{t}\\left(X_{i}^{T} \\mathbf{z}_{j}\\right)\\right\\|_{2}^{2} \\\\\n& \\quad+\\lambda \\sum_{j=1}^{R}\\left[\\left\\|V_{j}\\right\\|_{F}+\\delta_{\\left\\{\\mathbf{z}:\\|\\mathbf{z}\\|_{2} \\leq 1\\right\\}}\\left(\\mathbf{z}_{j}\\right)\\right]\n\\end{aligned}\n$$\n\nwhere, $\\sigma_{t}(\\cdot)$ is softmax function with temperature $t$, for $k \\in[T]$ defined $\\sigma_{t}(\\mathbf{u})_{k}:=\\exp \\left(t u_{k}\\right) / \\sum_{l=1}^{T} \\exp \\left(t u_{l}\\right)$ and define $\\mathbb{N C}_{\\mu}^{\\mathrm{TF}}\\left(\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}\\right)$ similarly with the sum over $i$ replaced by expectation taken over $(X, \\mathbf{y})$.\nLet $\\left\\{\\left(\\hat{V}_{j}, \\hat{\\mathbf{z}}_{j}\\right)\\right\\}$ be a stationary point of $\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{TF}}(\\cdot)$. Suppose there exists $C_{V}, B_{V}>0$ such that $\\sum_{j=1}^{R}\\left\\|\\hat{V}_{j}\\right\\|_{F} \\leq$ $C_{V}\\left\\|A^{*}\\right\\|_{F}$, and for all $j \\in[R],\\left\\|\\hat{V}_{j}\\right\\|_{F} \\leq B_{V}$. Then with probability at least $1-\\delta$ it holds that\n\n$$\n\\begin{aligned}\n& \\frac{1}{m}\\left|\\mathbb{N C}_{\\mu}^{\\mathrm{TF}}\\left(\\left\\{\\left(\\hat{V}_{j}, \\hat{\\mathbf{z}}_{j}\\right)\\right\\}\\right)-\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{TF}}\\left(\\left\\{\\left(\\hat{V}_{j}, \\hat{\\mathbf{z}}_{j}\\right)\\right\\}\\right)\\right| \\lesssim \\\\\n& \\frac{1}{2 m}\\left\\|A^{*}\\right\\|_{F}\\left[\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-\\hat{\\mathbf{y}}_{i}\\right\\|_{2}\\left\\|X_{i}\\right\\|_{2}-\\lambda\\right] \\\\\n& +C_{V}^{2}\\left\\|A^{*}\\right\\|_{F}^{2} \\times \\\\\n& \\sqrt{\\frac{R(m+n) \\log \\left(R(m+n)\\left(C_{V}+B_{V}\\right)\\right) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{aligned}\n$$\n\nRemarks: The dependence on $\\mathbf{b}^{*}$ is not explicitly reflected in Equation (31) because the ground truth\n\nmodel is bilinear. Consequently, assuming $\\mathbf{b}^{*}$ is unitnorm without loss of generality, as its norm can be absorbed into $A^{*}$. Thus, the dependence on $\\mathbf{b}^{*}$ is implicitly captured by the norm of $A^{*}$ in Equation (31). As in the previous two applications, we can achieve consistency at global optimality when $N \\gtrsim R(m+n)$, ignoring logarithmic terms. Note that the sample complexity has no dependency on the number of tokens, $T$, which suggests an explanation for the success behind the prediction capabilities of transformers for longer length inputs (Zhou et al., 2024). Our sample complexity matches the state-of-the-art bounds on the transformers by Trauger and Tewari (2024).", "tables": {"table_0": "| Application | Our work, $N \\gtrsim$ | State-of-the-art, $N \\gtrsim$ |\n| :--: | :--: | :--: |\n| Low rank matrix sensing | $\\mathcal{O}(R(m+n))$ | $R^{*}(m+n)$, (St\u00f6ger and Zhu, 2024) (no regularization) |\n| Structured matrix sensing |  |  |\n| 2-Layer linear NN |  | $R(m+n)$ (Kakade et al., 2008) (bounded data-points) |\n| 2-Layer ReLU NN |  | $R(m+n) \\log (R(m+n))$, (Bartlett et al., 2019) |\n| Multi-head attention |  | $R(m+n)$, (Trauger and Tewari, 2024) (bounded data-points) |"}, "images": {}}, {"section_id": 8, "text": "## 6 CONCLUSIONS\n\nIn this work, we provide generalization bounds for nonconvex problems of the form of sums of (slightly generalized) positively homogeneous functions with a general objective. Our bounds provide sample complexities that are near-optimal and applicable to various problems, such as low-rank matrix sensing, two-layer neural networks, and single-layer multi-head attention. The sample complexity of our bounds grows almost linear with the total number of parameters in the model, and for matrix sensing, this sample complexity is optimal, as demonstrated in Cand\u00e8s and Plan (2011). Our proofs are based on analyzing closely related convex programs in the prediction space; this perspective enabled us to provide near-optimal sample complexities due to existing results on generalization properties for convex functions. In future work, it would be interesting to sharpen the dependence of our bounds on all the relevant parameters and apply our techniques to other machine learning problems.", "tables": {}, "images": {}}, {"section_id": 9, "text": "## Acknowledgments\n\nUKRT gratefully acknowledges Pratik Chaudhari, Hancheng Min, Kyle Poe and Ziqing Xu for their valuable discussions and constructive feedback. His research was supported by the Leggett Family Fellowship and the Dean's Fellowship programs. Other authors acknowledge the support of the Research Collaboration on the Mathematical and Scientific Foundations of Deep Learning (NSF grant 2031985 and Simons Foundation grant 814201).", "tables": {}, "images": {}}, {"section_id": 10, "text": "## References\n\nAllen-Zhu, Z., Li, Y., and Liang, Y. (2019). Learning and generalization in overparameterized neural networks, going beyond two layers. Advances in neural information processing systems, 32.\nAndriushchenko, M., Croce, F., M\u00fcller, M., Hein, M., and Flammarion, N. (2023). A modern look at the\nrelationship between sharpness and generalization. International Conference on Learning Representations (ICLR).\n\nArora, S., Cohen, N., Golowich, N., and Hu, W. (2019). A convergence analysis of gradient descent for deep linear neural networks. In International Conference on Learning Representations.\n\nArora, S., Ge, R., Neyshabur, B., and Zhang, Y. (2018). Stronger generalization bounds for deep nets via a compression approach. In International conference on machine learning, pages 254-263. PMLR.\n\nBach, F. (2013). Convex relaxations of structured matrix factorizations. arXiv preprint arXiv:1309.311.\nBach, F. (2017). Breaking the curse of dimensionality with convex neural networks. Journal of Machine Learning Research, 18(19):1-53.\nBanerjee, A., Chen, T., and Zhou, Y. (2020). Derandomized pac-bayes margin bounds: Applications to non-convex and non-smooth predictors. arXiv preprint arXiv:2002.09956.\n\nBarron, A. R. and Klusowski, J. M. (2019). Complexity, statistical risk, and metric entropy of deep nets using total path variation. arXiv preprint arXiv:1902.00800.\n\nBartlett, P. L., Harvey, N., Liaw, C., and Mehrabian, A. (2019). Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research, 20(63):1-17.\n\nBartlett, P. L. and Mendelson, S. (2001). Rademacher and gaussian complexities: Risk bounds and structural results. In Helmbold, D. and Williamson, B., editors, Computational Learning Theory, volume 2111, pages 224-240. Springer Berlin Heidelberg. Series Title: Lecture Notes in Computer Science.\n\nBordelon, B., Chaudhry, H. T., and Pehlevan, C. (2024). Infinite limits of multi-head transformer dynamics. arXiv preprint arXiv:2405.15712.\n\nBurer, S. and Monteiro, R. D. (2003). A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329-357.\n\nCand\u00e8s, E. J. and Plan, Y. (2011). Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57(4):23422359.\n\nCand\u00e8s, E. J. and Recht, B. (2009). Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6):717-772.\n\nChandrasekaran, V., Recht, B., Parrilo, P. A., and Willsky, A. S. (2012). The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805-849.\n\nDeora, P., Ghaderi, R., Taheri, H., and Thrampoulidis, C. (2024). On the optimization and generalization of multi-head attention. Transactions on Machine Learning Research.\n\nDziugaite, G. K. and Roy, D. M. (2017). Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Conference on Uncertainty in Artificial Intelligence.\n\nEdelman, B. L., Goel, S., Kakade, S., and Zhang, C. (2022). Inductive biases and variable creation in selfattention mechanisms. In International Conference on Machine Learning, pages 5793-5831. PMLR.\n\nFeldman, V. and Vondrak, J. (2019). High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Conference on Learning Theory, pages 1270-1279. PMLR.\n\nGe, R., Jin, C., and Zheng, Y. (2017). No spurious local minima in nonconvex low rank problems: a unified geometric analysis. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, pages 1233-1242. JMLR.org.\n\nGiampouras, P., Vidal, R., Rontogiannis, A., and Haeffele, B. D. (2020). A novel variational form of the schatten-p quasi-norm. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS '20, Red Hook, NY, USA. Curran Associates Inc.\n\nGolowich, N., Rakhlin, A., and Shamir, O. (2018). Size-independent sample complexity of neural networks. In Conference On Learning Theory, pages 297-299. PMLR.\n\nGunasekar, S., Lee, J., Soudry, D., and Srebro, N. (2018a). Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832-1841. PMLR.\n\nGunasekar, S., Lee, J. D., Soudry, D., and Srebro, N. (2018b). Implicit bias of gradient descent on linear convolutional networks. Advances in neural information processing systems, 31.\n\nGunasekar, S., Woodworth, B. E., Bhojanapalli, S., Neyshabur, B., and Srebro, N. (2017). Implicit regularization in matrix factorization. Advances in neural information processing systems, 30.\n\nHaeffele, B. D. and Vidal, R. (2015). Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540.\n\nHaeffele, B. D. and Vidal, R. (2017). Global optimality in neural network training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7331-7339.\n\nHaeffele, B. D. and Vidal, R. (2020). Structured low-rank matrix factorization: Global optimality, algorithms, and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(6):1468-1482.\n\nHaoChen, J. Z., Wei, C., Lee, J., and Ma, T. (2021). Shape matters: Understanding the implicit bias of the noise covariance. In Conference on Learning Theory, pages 2315-2357. PMLR.\n\nHendrickx, J. and Olshevsky, A. (2010). Matrix pnorms are np-hard to approximate if $p \\neq 1,2, \\infty$. SIAM Journal on Matrix Analysis and Applications, 31(5):2802.\n\nHuang, C. (2020). ReLU networks are universal approximators via piecewise linear or constant functions. Neural Computation, 32(11):2249-2278.\n\nImaizumi, M. and Schmidt-Hieber, J. (2023). On generalization bounds for deep networks based on loss surface implicit regularization. IEEE Transactions on Information Theory, 69(2):1203-1223.\n\nJia, X., Wang, H., Peng, J., Feng, X., and Meng, D. (2023). Preconditioning matters: Fast global convergence of non-convex matrix factorization via scaled gradient descent. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S., editors, Advances in Neural Information Processing Systems, volume 36, pages 76202-76213. Curran Associates, Inc.\n\nJin, J., Li, Z., Lyu, K., Du, S. S., and Lee, J. D. (2023). Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing. In Proceedings of the 40th International Conference on Machine Learning, pages 15200-15238. PMLR. ISSN: 2640-3498.\n\nKakade, S. M., Sridharan, K., and Tewari, A. (2008). On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc.\n\nKoltchinskii, V., Lounici, K., and Tsybakov, A. B. (2011). Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39(5):2302-2329. Publisher: Institute of Mathematical Statistics.\n\nLi, G. and Wei, Y. (2023). A non-asymptotic framework for approximate message passing in spiked models. arXiv preprint arXiv:2208.03313.\n\nLi, Y., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. (2023). Transformers as algorithms: Generalization and stability in in-context learning. In Proceedings of the 40th International Conference on Machine Learning, pages 19565-19594. PMLR.\nLi, Z., Luo, Y., and Lyu, K. (2020). Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. In International Conference on Learning Representations.\nLugosi, G. and Neu, G. (2022). Generalization bounds via convex analysis. In Conference on Learning Theory, pages 3524-3546. PMLR.\nMa, C., Wang, K., Chi, Y., and Chen, Y. (2020). Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and blind deconvolution. Foundations of Computational Mathematics, 20(3):451-632.\nMcAllester, D. A. (1999). PAC-bayesian model averaging. In Proceedings of the twelfth annual conference on Computational learning theory, pages 164-170. ACM.\n\nMuthukumar, R. and Sulam, J. (2023). Sparsity-aware generalization theory for deep neural networks. In Proceedings of Thirty Sixth Conference on Learning Theory, pages 5311-5342. PMLR. ISSN: 2640-3498.\nNegahban, S. and Wainwright, M. J. (2011). Estimation of (near) low-rank matrices with noise and high-dimensional scaling. The Annals of Statistics, 39(2):1069 - 1097.\nNeyshabur, B., Bhojanapalli, S., Mcallester, D., and Srebro, N. (2017). Exploring generalization in deep learning. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\nNeyshabur, B., Bhojanapalli, S., and Srebro, N. (2018). A pac-bayesian approach to spectrallynormalized margin bounds for neural networks. In International Conference on Learning Representations.\nNichani, E., Damian, A., and Lee, J. D. (2024). How transformers learn causal structure with gradient descent. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F., editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 38018-38070. PMLR.\nOymak, S. and Soltanolkotabi, M. (2019). Overparameterized nonlinear learning: Gradient descent takes the shortest path? In Proceedings of the 36th International Conference on Machine Learning, pages 4951-4960. PMLR. ISSN: 2640-3498.\n\nRecht, B., Xu, W., and Hassibi, B. (2008). Necessary and sufficient conditions for success of the nuclear norm heuristic for rank minimization. In 2008 47th IEEE Conference on Decision and Control. IEEE.\nReddy, T. U. K. and Vidyasagar, M. (2023). Convergence of momentum-based heavy ball method with batch updating and/or approximate gradients. In 2023 Ninth Indian Control Conference (ICC), pages $182-187$.\nRockafellar, R. T. (1970). Convex Analysis. Princeton University Press, Princeton.\nShalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K. (2009). Stochastic convex optimization. In COLT, volume 2, page 5.\nSingh, S. S. (2023). Analyzing transformer dynamics as movement through embedding space. arXiv preprint arXiv:2308.10874.\nSoudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and Srebro, N. (2018). The implicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70):1-57.\nSt\u00f6ger, D. and Soltanolkotabi, M. (2021). Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. Advances in Neural Information Processing Systems, $34: 23831-23843$.\nSt\u00f6ger, D. and Zhu, Y. (2024). Non-convex matrix sensing: Breaking the quadratic rank barrier in the sample complexity. arXiv preprint arXiv:2408.13276.\nTeam, G. (2024). Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.\nTian, Y., Wang, Y., Chen, B., and Du, S. S. (2023). Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36:71911-71947.\nTrauger, J. and Tewari, A. (2024). Sequence length independent norm-based generalization bounds for transformers. In International Conference on Artificial Intelligence and Statistics, pages 1405-1413. PMLR.\nVapnik, V. N. (2000). The Nature of Statistical Learning Theory. Springer.\nVapnik, V. N. and Chervonenkis, A. Y. (1968). The uniform convergence of frequencies of the appearance of events to their probabilities. Doklady Akademii Nauk SSSR, 181(4):781-783.\nVardi, G. (2023). On the implicit bias in deep-learning algorithms. Commun. ACM, 66(6):86-93.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\nVershynin, R. (2018). High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.\nVidal, R., Zhu, Z., and Haeffele, B. D. (2022). Optimization Landscape of Neural Networks, page 200-228. Cambridge University Press.\nWen, K., Li, Z., and Ma, T. (2023). Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. In Thirty-seventh Conference on Neural Information Processing Systems.\nYang, Y., Wipf, D. P., et al. (2022). Transformers from an optimization perspective. Advances in Neural Information Processing Systems, 35:36958-36971.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021). Understanding deep learning (still) requires rethinking generalization. Commun. $A C M, 64(3): 107-115$.\nZhang, R., Frei, S., and Bartlett, P. L. (2024). Trained transformers learn linear models in-context. Journal of Machine Learning Research, 25(49):1-55.\nZhang, Z., Fang, J., Lin, J., Zhao, S., Xiao, F., and Wen, J. (2020). Improved upper bound on the complementary error function. Electronics Letters, $56(13): 663-665$.\nZhou, Y., Alon, U., Chen, X., Wang, X., Agarwal, R., and Zhou, D. (2024). Transformers can achieve length generalization but not robustly. In International Conference on Learning Representations.", "tables": {}, "images": {}}, {"section_id": 11, "text": "## Checklist\n\n1. For all models and algorithms presented, check if you include:\n(a) A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes/No/Not Applicable]\n(b) An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes/No/Not Applicable]\n(c) (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Yes/No/Not Applica-\nble]\n2. For any theoretical claim, check if you include:\n(a) Statements of the full set of assumptions of all theoretical results. [Yes/No/Not Applicable]\n(b) Complete proofs of all theoretical results. [Yes/No/Not Applicable]\n(c) Clear explanations of any assumptions. [Yes/No/Not Applicable]\n3. For all figures and tables that present empirical results, check if you include:\n(a) The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes/No/Not Applicable]\n(b) All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes/No/Not Applicable]\n(c) A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes/No/Not Applicable]\n(d) A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Yes/No/Not Applicable]\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:\n(a) Citations of the creator If your work uses existing assets. [Yes/No/Not Applicable]\n(b) The license information of the assets, if applicable. [Yes/No/Not Applicable]\n(c) New assets either in the supplemental material or as a URL, if applicable. [Yes/No/Not Applicable]\n(d) Information about consent from data providers/curators. [Yes/No/Not Applicable]\n(e) Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Yes/No/Not Applicable]\n5. If you used crowdsourcing or conducted research with human subjects, check if you include:\n(a) The full text of instructions given to participants and screenshots. [Yes/No/Not Applicable]\n(b) Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Yes/No/Not Applicable]\n(c) The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Yes/No/Not Applicable]", "tables": {}, "images": {}}, {"section_id": 12, "text": "# A Convex Relaxation Approach to Generalization Analysis for Parallel Positively Homogeneous Networks: Supplementary Materials \n\nIn this supplementary material, we provide a detailed discussion of the rigorous technical aspects omitted from the main text. Additionally, we present a comprehensive review of related works and include a few numerical experiments. Below is the table of contents for this appendix/supplementary material.", "tables": {}, "images": {}}, {"section_id": 13, "text": "## Contents\n\n1 INTRODUCTION ..... 1\n2 PROBLEM FORMULATION ..... 3\n3 CONVEX BOUNDS FOR LEARNING ..... 4\n4 STATISTICAL BOUNDS ..... 5\n5 APPLICATIONS ..... 7\n6 CONCLUSIONS ..... 9\nA CONVEX BOUNDS FOR LEARNING ..... 14\nA. 1 Induced Regularizer in Convex Space ..... 14\nA. 2 Proof of Theorem 1 ..... 15\nB STATISTICAL BOUNDS ..... 20\nB. 1 Computing Function Class Capacities ..... 21\nB. 2 Proof of Theorem 2 ..... 22\nC APPLICATIONS ..... 28\nC. 1 Low-Rank Matrix Sensing ..... 28\nC. 2 Structured Matrix Sensing ..... 34\nC. 3 Two-Layer Linear NN ..... 36\nC. 4 Two-Layer ReLU NN ..... 41\nC. 5 Multi-head Attention ..... 47\nD GOODS EVENTS ..... 54\nD. 1 Concentration of Norms ..... 54\nD. 2 Concentration of Convex functions ..... 55\nD. 3 Concentration of Equilibria ..... 58\nD. 4 Concentration of Polar ..... 60\nE NUMERICAL EXPERIMENTS ..... 63\nF OTHER RELATED WORKS ..... 63", "tables": {}, "images": {}}, {"section_id": 14, "text": "# G PRELIMINARIES \n\nG. 1 Convex Functions\nG. 2 Concentration of Measure", "tables": {}, "images": {}}, {"section_id": 15, "text": "## A CONVEX BOUNDS FOR LEARNING\n\nIn this section, we discuss the proof for Theorem 1 that establishes the optimality gaps in the empirical and population landscapes. First, we analyze the convexity of the induced regularizer, $\\Omega(\\cdot)$ and properties of the stationary points in non-convex landscape. These are the key components of our proof for Theorem 1. We state a more general version of Assumption 2 by having the flexibility of the loss being strongly convex to derive tighter results.\nAssumption 2' (Convex Loss). The loss $\\ell(Y, \\hat{Y})$ is second-order differentiable (written $\\ell \\in \\mathcal{C}^{2}$ ), $\\alpha$-strong and L-smooth w.r.t. $\\hat{Y}$, i.e, for any $Y, \\hat{Y} \\in \\mathbb{R}^{n_{Y}}$\n\n$$\n0 \\preceq \\alpha I_{n_{Y}} \\preceq \\nabla_{\\hat{Y}}^{2} \\ell(Y, \\hat{Y}) \\preceq L I_{n_{Y}}\n$$\n\nAdditionally, the gradient of the loss is bi-Lipschitz; that is, for all $Y_{1}, Y_{2}, \\hat{Y}_{1}, \\hat{Y}_{2} \\in \\mathbb{R}^{n_{Y}}$\n\n$$\n\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(Y_{2}, \\hat{Y}_{2}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(Y_{1}, \\hat{Y}_{1}\\right)\\right\\| \\leq L\\left[\\left\\|Y_{2}-Y_{1}\\right\\|_{2}+\\left\\|\\hat{Y}_{2}-\\hat{Y}_{1}\\right\\|_{2}\\right]\n$$\n\nand the loss is constant if both the arguments are the same, i.e., for all $Y_{1}, Y_{2} \\in \\mathbb{R}^{n_{Y}}, \\ell\\left(Y_{1}, Y_{1}\\right)=\\ell\\left(Y_{2}, Y_{2}\\right)$.\nNote that we allow $\\alpha=0$, in which case we recover Assumption 2.", "tables": {}, "images": {}}, {"section_id": 16, "text": "## A. 1 Induced Regularizer in Convex Space\n\nFirst, we show that the induced regularizer is convex in the function spaces through Proposition 1.\nProposition 1 (Convexity of induced regularizer). Suppose assumptions 1-2' hold. Then $\\Omega(f)$ is convex in $f$ in the space of functions $\\mathbb{R}^{n_{X}} \\rightarrow \\mathbb{R}^{n_{Y}}$.\n\nProof. This proof is infinite dimensional extension of Haeffele and Vidal (2015). Recall the definition of induced regularizer:\n\n$$\n\\Omega(f):=\\inf _{r,\\left\\{W_{j}\\right\\}} \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\text { such that } f(X)=\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) ; \\forall X \\in \\mathcal{X}\n$$\n\nDefine the function class\n\n$$\n\\mathcal{F}_{\\Phi}:=\\left\\{\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right): r \\in \\mathbb{N}, W_{j} \\in \\mathcal{W}\\right\\}\n$$\n\nBy definition if $f \\notin \\mathcal{F}_{\\Phi}$ then $\\Omega(f)$ evaluates to infinity. Now suppose that $\\beta \\geq 0$ and for any $f \\in \\mathcal{F}_{\\Phi}$,\n\n$$\n\\Omega(\\beta f)=\\inf _{r,\\left\\{W_{j}\\right\\}} \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\text { such that } \\beta f(X)=\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) ; \\forall X \\in \\mathcal{X}\n$$\n\nNow by Assumption 2', there exists $\\hat{\\beta}$ such that $\\beta \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)=\\Phi_{r}\\left(\\left\\{\\hat{\\beta} W_{j}\\right\\}\\right)$, and $\\beta \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)=\\Theta_{r}\\left(\\left\\{\\hat{\\beta} W_{j}\\right\\}\\right)$ (throughout note that this scaling is applied only to the $\\mathcal{W}_{p}$ subset of parameters from Assumption 2, but we do not notate this explicitly for brevity of notation). Now we perform a change of variables in the induced regularizer, obtaining\n\n$$\n\\Omega(\\beta f)=\\inf _{r,\\left\\{\\hat{\\beta} W_{j}\\right\\}} \\Theta_{r}\\left(\\left\\{\\hat{\\beta} W_{j}\\right\\}\\right) \\text { such that } \\beta f(X)=\\Phi_{r}\\left(\\left\\{\\hat{\\beta} W_{j}\\right\\}\\right) ; \\forall X \\in \\mathcal{X}\n$$\n\nThen we have that\n\n$$\n\\Omega(\\beta f)=\\inf _{r,\\left\\{W_{j}\\right\\}} \\beta \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\text { such that } \\beta f(X)=\\beta \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) ; \\forall X \\in \\mathcal{X}=\\beta \\Omega(f)\n$$\n\nWe have established that the function $\\Omega(\\cdot)$ is 1-degree homogeneous. Now we prove that the function $\\Omega(\\cdot)$ is sub-additive. Choose any $f_{1}, f_{2} \\in \\mathcal{F}_{\\Phi}$, because the case when either of them is not in $\\mathcal{F}_{\\Phi}$ is trivially sub-additive. Recall\n\n$$\n\\begin{aligned}\n\\Omega\\left(f_{1}\\right) & =\\inf _{r,\\left\\{W_{j}\\right\\}} \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\text { such that } f_{1}(X)=\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) ; \\forall X \\in \\mathcal{X} \\\\\n\\Omega\\left(f_{2}\\right) & =\\inf _{r,\\left\\{W_{j}\\right\\}} \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\text { such that } f_{2}(X)=\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) ; \\forall X \\in \\mathcal{X} \\\\\n\\Omega\\left(f_{1}+f_{2}\\right) & =\\inf _{r,\\left\\{W_{j}\\right\\}} \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\text { such that } f_{1}(X)+f_{2}(X)=\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) ; \\forall X \\in \\mathcal{X}\n\\end{aligned}\n$$\n\nFor any $\\epsilon>0$ let $\\left(r_{1},\\left\\{W_{j}^{1}\\right\\}\\right)$ and $\\left(r_{2},\\left\\{W_{j}^{2}\\right\\}\\right)$ be parameters which come within $\\epsilon$ of the infimum in the optimization problems for $\\Omega\\left(f_{1}\\right)$ and $\\Omega\\left(f_{2}\\right)$ respectively. Then note that\n\n$$\n\\Omega\\left(f_{1}+f_{2}\\right) \\leq \\Theta_{r_{1}}\\left(\\left\\{W_{j}^{1}\\right\\}\\right)+\\Theta_{r_{2}}\\left(\\left\\{W_{j}^{2}\\right\\}\\right) \\leq \\Omega\\left(f_{1}\\right)+\\Omega\\left(f_{2}\\right)+2 \\epsilon\n$$\n\nLetting $\\epsilon \\rightarrow 0$ gives that $\\Omega\\left(f_{1}+f_{2}\\right) \\leq \\Omega\\left(f_{1}\\right)+\\Omega\\left(f_{2}\\right)$. Thus, as $\\Omega(\\cdot)$ is both positively homogenous with degree one and sub-additive, it is convex.\n\nFrom the above proposition we have that $\\Omega(\\cdot)$ is a convex function, therefore we have that $C(\\cdot)$ is indeed a convex function in the prediction functions space. Our results primarily depend upon the optimal regularization of the globally optimal solution of a convex function, $C .(\\cdot)$. As we operate in the space of functions, it is very unlikely that we have the knowledge of the global optima. Nevertheless, by exploiting the convexity of $C .(\\cdot)$ we can upper bound the optimal regularization. Proposition 2 establishes the upper bound for the optimal regularization for regression loss.\nProposition 2. Consider $\\ell\\left(Y_{1}, Y_{2}\\right)=\\frac{1}{2}\\left\\|Y_{1}-Y_{2}\\right\\|_{2}^{2},\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathrm{W}}$. Suppose $X \\sim \\mu, \\epsilon$ is random variable such that $\\mathbb{E}[\\epsilon]=0$ and independent from $x$. Let $Y=\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)+\\epsilon$, and suppose $f_{\\mu}^{*}$ is the global optimal solution of $C_{\\mu}(\\cdot)$. Then we have\n\n$$\n\\Omega\\left(\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right) \\geq \\Omega\\left(f_{\\mu}^{*}\\right)\n$$\n\nProof. As $f_{\\mu}^{*}$ is the global optimal solution, we have that\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\frac{1}{2}\\left\\|\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)+\\epsilon-f_{\\mu}^{*}(X)\\right\\|_{2}^{2}\\right]+\\lambda \\Omega\\left(f_{\\mu}\\right) \\leq & \\mathbb{E}\\left[\\frac{1}{2}\\left\\|\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)+\\epsilon-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)\\right\\|_{2}^{2}\\right] \\\\\n& +\\lambda \\Omega\\left(\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\n\\end{aligned}\n$$\n\nNow, by re-arranging the terms we obtain\n\n$$\n\\mathbb{E}\\left[\\frac{1}{2}\\left\\langle\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)-f_{\\mu}^{*}(X), \\epsilon\\right\\rangle\\right]+\\lambda \\Omega\\left(f_{\\mu}\\right) \\leq \\lambda \\Omega\\left(\\Phi_{r^{*}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\n$$\n\nAs $\\epsilon$ is independent of $X$,\n\n$$\n\\frac{1}{2}\\left\\langle\\mathbb{E}_{X}\\left[\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)-f_{\\mu}^{*}(X)\\right], \\mathbb{E}_{\\epsilon}[\\epsilon]\\right\\rangle+\\lambda \\Omega\\left(f_{\\mu}\\right) \\leq \\lambda \\Omega\\left(\\Phi_{r^{*}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\n$$\n\nThen we have\n\n$$\n\\Omega\\left(f_{\\mu}\\right) \\leq \\Omega\\left(\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\n$$", "tables": {}, "images": {}}, {"section_id": 17, "text": "# A. 2 Proof of Theorem 1 \n\nOptimization algorithms used to optimize DNNs try to find the set of parameters that are first-order optimal. However, we do not have a guarantee that these points are saddle/local minima/global minima. In proposition 3 , we provide properties that any first-order optimal satisfies for positively homogeneous networks.\nProposition 3 (Stationary Points). Under assumption 2, if $\\left\\{W_{j}\\right\\}$ are stationary points of $\\mathrm{NC}_{\\mu}(\\cdot)$, then for all $j \\in[r]$,\n\n$$\n\\left\\langle-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W_{j}\\right)\\right\\rangle=\\theta\\left(W_{j}\\right)\n$$\n\nProof. This proof is similar to that of Proposition 2 in Haeffele and Vidal (2020) but applied to a general class of (slightly) positively homogeneous functions (see assumption 2).\nFrom assumption 2, there exists a subset of parameters where both $\\theta$ and $\\phi$ are positively homogeneous. Let $\\mathbf{w}_{i}$ be the subset of parameters in $\\mathcal{W}_{p}$ from assumption 2 . Then we have\n\n$$\n\\left\\langle\\mathbf{w}_{i}, \\partial_{\\mathbf{w}_{i}} \\theta\\left(\\mathbf{w}_{1}, \\ldots, \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right)\\right\\rangle=\\lim _{\\epsilon \\rightarrow 0}\\left[\\frac{\\theta\\left(\\mathbf{w}_{1}, \\ldots,\\left(1+\\epsilon\\right) \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right)}{\\epsilon}-\\frac{\\theta\\left(\\mathbf{w}_{1}, \\ldots, \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right)}{\\epsilon}\\right]\n$$\n\nLet $p_{i}$ be the homogeneous degree of the parameters $\\mathbf{w}_{i}$. Note that $\\partial_{\\mathbf{w}_{i}} \\theta\\left(\\mathbf{w}_{1}, \\ldots, \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right) \\in \\mathbb{R}^{d i m\\left(\\mathbf{w}_{i}\\right) \\times 1}$, $\\partial_{\\mathbf{w}_{i}} \\phi\\left(\\mathbf{w}_{1}, \\ldots, \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right) \\in \\mathbb{R}^{d i m\\left(\\mathbf{w}_{i}\\right) \\times n_{V}}$. Then\n\n$$\n\\begin{aligned}\n\\left\\langle\\mathbf{w}_{i}, \\partial_{\\mathbf{w}_{i}} \\theta\\left(\\mathbf{w}_{1}, \\ldots, \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right)\\right\\rangle & =\\theta\\left(\\mathbf{w}_{1}, \\ldots, \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right) \\lim _{\\epsilon \\rightarrow 0} \\frac{(1+\\epsilon)^{p_{i}}-1}{\\epsilon} \\\\\n& =p_{i} \\theta\\left(\\mathbf{w}_{1}, \\ldots, \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right)\n\\end{aligned}\n$$\n\nSimilarly, following a similar argument for $\\phi$ we obtain\n\n$$\n\\left\\langle\\partial_{\\mathbf{w}_{i}} \\phi\\left(\\mathbf{w}_{1}, \\ldots, \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right), \\mathbf{w}_{i}\\right\\rangle=p_{i} \\phi\\left(\\mathbf{w}_{1}, \\ldots, \\mathbf{w}_{i}, \\ldots, \\mathbf{w}_{n}\\right)\n$$\n\nAs $W_{j}$ are the stationary points we have that\n\n$$\n0 \\in \\partial_{W_{j}} \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu}+\\lambda \\partial_{W_{j}} \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\n$$\n\nSince $\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)=\\sum_{j=1}^{r} \\phi\\left(W_{j}\\right)$, we have that $\\partial_{W_{j}} \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)=\\partial_{W_{j}} \\phi\\left(W_{j}\\right)$. Similarly, $\\partial_{W_{j}} \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)=\\partial_{W_{j}} \\theta\\left(W_{j}\\right)$ holds true. Consequently,\n\n$$\n0 \\in \\partial_{W_{j}} \\phi\\left(W_{j}\\right) \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu}+\\lambda \\partial_{W_{j}} \\theta\\left(W_{j}\\right)\n$$\n\nLetting $W_{j}=\\left[\\begin{array}{lll}\\mathbf{w}_{1} & \\ldots & \\mathbf{w}_{n}\\end{array}\\right]$, for all $\\mathbf{w}_{i}$ it holds that\n\n$$\n0 \\in \\partial_{\\mathbf{w}_{i}} \\phi\\left(W_{j}\\right) \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu}+\\lambda \\partial_{\\mathbf{w}_{i}} \\theta\\left(W_{j}\\right)\n$$\n\nTaking the inner product of the above equation with $\\mathbf{w}_{i}$, when $p_{i} \\neq 0$ we have that\n\n$$\n0 \\in \\mathbf{w}_{i}{ }^{T} \\partial_{\\mathbf{w}_{i}} \\phi\\left(W_{j}\\right) \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu}+\\lambda \\mathbf{w}_{i}{ }^{T} \\partial_{\\mathbf{w}_{i}} \\theta\\left(W_{j}\\right)\n$$\n\nFrom (51) we have that\n\n$$\n0=p_{i} \\phi\\left(W_{j}\\right)^{T} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu}+\\lambda p_{i} \\theta\\left(W_{j}\\right)\n$$\n\nRearranging, we obtain\n\n$$\n\\left\\langle-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W_{j}\\right)\\right\\rangle=\\theta\\left(W_{j}\\right)\n$$\n\nwhich holds for all $j \\in[r]$.\nProposition 3 establishes that at any stationary point, the inner product between the prediction errors and the predictions equates to the the current regularization. Next, we exploit this property of stationary points that enable us to tie the non-convex landscape to its convex counterpart. Lemma 1 establishes the difference between the non-convex and convex objective values at stationary points.\nLemma 1 (Optimality Gap). Let $\\ell(\\cdot, \\cdot)$ denote any L-smooth, and $\\alpha$-strongly convex loss function, let $q$ be some measure, and suppose that $\\left\\{W_{j}\\right\\}$ is a stationary point of $\\mathrm{NC}_{q}\\left(\\left\\{W_{j}\\right\\}\\right)$. Let $f_{q}^{*}$ denote the global minimizer of $\\mathrm{C}_{q}(\\cdot)$. Then for any $f \\in L^{2}(q)$, we have that\n\n$$\n\\mathrm{C}_{q}\\left(f_{q}^{*}\\right) \\leq \\mathrm{NC}_{q}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\mathrm{C}_{q}(f)+\\lambda \\Omega_{q}(f)\\left[\\Omega_{q}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{q}^{2}\n$$\n\nProof. The loss $\\ell(Y, \\hat{Y})$ is $(L, \\lambda)$-convex in $\\hat{Y}$. Therefore, for any functions $g: \\mathcal{X} \\times E \\rightarrow \\mathcal{Y}$, and $f_{1}, f_{2} \\in L^{2}(q)$ we have that\n\n$$\n\\begin{aligned}\n\\ell(g(X, \\epsilon), f(X)) & \\geq \\ell\\left(g(X, \\epsilon), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\})(X)\\right) \\\\\n& +\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g(X, \\epsilon), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\})(X)\\right), f(X)-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\})(X)\\right\\rangle_{\\mathcal{Y}}\\right. \\\\\n& \\left.+\\frac{\\alpha}{2}\\|f(X)-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\})(X)\\right\\|_{y}^{2}\n\\end{aligned}\n$$\n\nTaking expectations of both sides with respect to the probability measure $q$, we have that\n\n$$\n\\ell(g, f)_{q} \\geq \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{q}+\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{q}+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{q}^{2}\n$$\n\nAs the $\\left\\{W_{j}\\right\\}$ are the stationary points of $\\mathrm{NC}_{q}\\left(\\left\\{W_{j}\\right\\}\\right)$ from Proposition 3 we have that for all $j \\in[r]$\n\n$$\n\\left\\langle-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W_{j}\\right)\\right\\rangle_{q}=\\theta\\left(W_{j}\\right)\n$$\n\nSumming the above identity up overall $j$, it holds that\n\n$$\n\\left\\langle-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{q}=\\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\n$$\n\nTherefore, plugging this identity into the inequality (63), we have that\n\n$$\n\\ell(g, f)_{q} \\geq \\underbrace{\\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{q}+\\lambda \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)}_{\\mathrm{NC}_{q}\\left(\\left\\{W_{j}\\right\\}\\right)}+\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), f\\right\\rangle_{q}+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{q}^{2}\n$$\n\nwhich implies that\n\n$$\n\\ell(g, f)_{q}+\\lambda\\left\\langle-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), f\\right\\rangle_{q} \\geq N C_{q}\\left(\\left\\{W_{j}\\right\\}\\right)+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{q}^{2}\n$$\n\nWe have established from Proposition 1 that $\\Omega$ is a convex function. As a well-known result from convex analysis (see Proposition 5) we have that for any convex function $\\Omega$ and any $f, g \\in L^{2}(q)$, it holds that $\\langle f, g\\rangle_{q} \\leq$ $\\Omega_{q}(f) \\Omega_{q}^{\\circ}(g)$. Consequently,\n\n$$\n\\ell(g, f)_{q}+\\lambda \\Omega_{q}(f) \\Omega_{q}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) \\geq N C_{q}\\left(\\left\\{W_{j}\\right\\}\\right)+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{q}^{2}\n$$\n\nTherefore, rearranging,\n\n$$\n\\underbrace{\\ell(g, f)_{q}+\\lambda \\Omega_{q}(f)}_{\\mathrm{C}_{q}(f)}+\\lambda \\Omega_{q}(f)\\left[\\Omega_{q}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\geq N C_{q}\\left(\\left\\{W_{j}\\right\\}\\right)+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{q}^{2}\n$$\n\nand, as a result,\n\n$$\n\\mathrm{NC}_{q}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\mathrm{C}_{q}(f)+\\lambda \\Omega_{q}(f)\\left[\\Omega_{q}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{q}^{2}\n$$\n\nLet $f_{q}^{*}=\\arg \\min _{f} \\mathrm{C}_{q}(f)$, and $\\left(r^{*},\\left\\{W_{j}^{*}\\right\\}\\right)=\\arg \\min _{r,\\left\\{W_{j}\\right\\}} N C_{q}\\left(\\left\\{W_{j}\\right\\}\\right)$. Since $f_{q}^{*}$ is the minimizer of $C_{q}(f)$, it holds that\n\n$$\n\\mathrm{C}_{q}\\left(f_{q}^{*}\\right) \\leq \\mathrm{C}_{q}\\left(\\Phi_{r^{*}}\\left(\\left\\{W_{j}^{*}\\right\\}\\right)\\right)=\\ell\\left(g, \\Phi_{r^{*}}\\left(\\left\\{W_{j}^{*}\\right\\}\\right)\\right)_{q}+\\lambda \\Omega_{q}\\left(\\Phi_{r^{*}}\\left(\\left\\{W_{j}^{*}\\right\\}\\right)\\right)\n$$\n\nTherefore, we obtain\n\n$$\n\\begin{aligned}\n\\mathrm{C}_{q}\\left(f_{q}^{*}\\right) & \\leq \\ell\\left(g, \\Phi_{r^{*}}\\left(\\left\\{W_{j}^{*}\\right\\}\\right)\\right)_{q}+\\lambda \\Omega_{q}\\left(\\Phi_{r^{*}}\\left(\\left\\{W_{j}^{*}\\right\\}\\right)\\right) \\\\\n& \\leq \\ell\\left(g, \\Phi_{r^{*}}\\left(\\left\\{W_{j}^{*}\\right\\}\\right)\\right)_{q}+\\lambda \\Theta_{r^{*}}\\left(\\left\\{W_{j}^{*}\\right\\}\\right) \\\\\n& =\\mathrm{NC}_{q}\\left(\\left\\{W_{j}^{*}\\right\\}\\right) \\\\\n& \\leq \\mathrm{NC}_{q}\\left(\\left\\{W_{j}\\right\\}\\right)\n\\end{aligned}\n$$\n\nTherefore, combining Equations (75) and (70), we obtain the bound\n\n$$\n\\mathrm{C}_{q}\\left(f_{q}^{*}\\right) \\leq \\mathrm{NC}_{q}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\mathrm{C}_{q}(f)+\\lambda \\Omega_{q}(f)\\left[\\Omega_{q}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{q}^{2}\n$$\n\nLemma 1 has established that the non-convex objective, $\\mathrm{NC}_{q}(\\cdot)$ is both upper and lower bounded by the convex function, $\\mathrm{C}_{q}(\\cdot)$. Now, we utilize this result to compute the empirical gap with the measure, $\\mu_{N}$ for the stationary points obtained from the ERM. On these stationary points, we bound the optimality gap by changing the measure to $\\mu$, i.e., the behavior of ERM's first-order points on population landscape.\nTheorem 3 (Global Optimality). Under assumptions 1, 2', 3. Let $f_{\\mu_{N}}^{*}$ (or $f_{\\mu}^{*}$ ) be the global minimizer for $\\mathrm{C}_{\\mu_{N}}(\\cdot)$ (or $\\left.\\mathrm{C}_{\\mu}(\\cdot)\\right)$. For any stationary points, $\\left(r,\\left\\{W_{j}\\right\\}\\right)$ of the function $\\mathrm{NC}_{\\mu_{N}}(\\cdot)$ and any $f \\in L^{2}(\\mu) \\cap L^{2}\\left(\\mu_{N}\\right)$ the following items are true:\n\n1. Empirical optimality gap:\n\n$$\n\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}^{*}\\right) \\leq \\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\mathrm{C}_{\\mu_{N}}(f)+\\lambda \\Omega(f)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2}\n$$\n\n2. Population optimality gap:\n\n$$\n\\begin{aligned}\n& \\mathrm{C}_{\\mu}\\left(f_{\\mu}^{*}\\right) \\leq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\mathrm{C}_{\\mu}(f)+\\lambda \\Omega(f)\\left[\\Omega_{\\mu}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2} \\\\\n& \\quad+\\left[\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}\\right]\n\\end{aligned}\n$$\n\nwhere $\\Omega_{q}^{\\circ}(\\cdot)$ is the polar in the measure $q$ defined as\n\n$$\n\\Omega_{q}^{\\circ}(g):=\\sup _{\\theta(W) \\leq 1}\\langle g, \\phi(W)\\rangle_{q}\n$$\n\nRemarks: Setting $f=f_{\\mu_{N}}^{*}$ in Equation 77 and taking $\\left(r,\\left\\{W_{j}\\right\\}\\right)$ to be any stationary point of $\\mathrm{NC}_{\\mu_{N}}(\\cdot)$ gives a means to verify if $\\left\\{W_{j}\\right\\}$ is a globally optimal solution. We see that it suffices to check if $\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)$ is a first-order stationary point of $\\mathrm{C}_{\\mu_{N}}(\\cdot)$, which is a necessary condition for a local minimum of convex functions.\nFrom convex analysis, if a function $f \\in L^{2}\\left(\\mu_{N}\\right)$ is a first-order solution of $\\mathrm{C}_{\\mu_{N}}$ then we have that 0 belongs to the sub-gradient of $\\mathrm{C}_{\\mu}(\\cdot)$ at $f$. As the loss $\\ell$ is first-order differentiable (by Assumption 3 or $2^{\\prime}$ ) we have that\n\n$$\n0 \\in \\partial \\mathrm{C}_{\\mu_{N}}(f) \\Longleftrightarrow-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell(g, f)_{\\mu_{N}} \\in \\partial \\Omega(f)\n$$\n\nwhere $\\partial \\mathrm{C}_{\\mu_{N}}(f)$ denotes the subgradient of C (viewed as a function of $f$ ). The above condition for $f$ can also be verified by a dual notion known as the polar condition, Definition 6 (Rockafellar, 1970). The sub-gradient of a convex function can be defined through the notion of it's polar via\n\n$$\n\\partial \\Omega_{\\mu_{N}}(f)=\\left\\{g \\in L_{2}\\left(\\mu_{N}\\right):\\langle g, f\\rangle_{\\mu_{N}}=\\Omega_{\\mu_{N}}(f), \\Omega_{\\mu_{N}}^{\\circ}(g) \\leq 1\\right\\}\n$$\n\nFrom Lemma 1 in the supplement of Haeffele and Vidal (2017) the following statements are equivalent:\n\n1. $\\left\\{W_{j}\\right\\}$ is an optimal factorization of $f$; i.e, $\\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)=\\Omega_{\\mu_{N}}(f)$.\n2. $\\exists h \\in L^{2}\\left(\\mu_{N}\\right)$ such that $\\Omega_{\\mu_{N}}^{\\circ}(h) \\leq 1$ and $\\left\\langle h, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}=\\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)$.\n3. $\\exists h \\in L^{2}\\left(\\mu_{N}\\right)$ such that $\\Omega_{\\mu_{N}}^{\\circ}(h) \\leq 1$ and $\\left\\langle h, \\phi\\left(W_{j}\\right)\\right\\rangle_{\\mu_{N}}=\\theta\\left(W_{j}\\right) ; \\forall i \\in[r]$.\n\nFurther, if (2) or (3) above is satisfied then we have that $h \\in \\partial \\Omega_{\\mu_{N}}(f)$. From Proposition 3 we have that for any stationary point $\\left(r,\\left\\{W_{j}\\right\\}\\right)$ of $\\mathrm{NC}_{\\mu_{N}}$,\n\n$$\n\\left\\langle-\\frac{1}{\\lambda} \\nabla \\ell_{\\mu_{N}}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}=\\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\n$$\n\nConsequently, to check if a stationary point is globally optimal, it then suffices to check whether the polar condition $\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu_{N}}\\right) \\leq 1$ holds at the stationary point, $\\left(r,\\left\\{W_{j}\\right\\}\\right)$. In the case when the polar condition holds true, the upper bound evaluates to $\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu}^{*}\\right)$ matching the lower bound of $\\mathrm{NC}_{\\mu_{N}}(\\cdot)$, which in turn implies global optimality.\nThen, we can claim the following:\nAt a stationary point $\\left\\{W_{j}\\right\\}$, if $\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu_{N}}\\right) \\leq 1$, then $\\left\\{W_{j}\\right\\}$ is globally optimal.\nNow we prove Theorem 1.\nProof. The proof sketch is similar to Proposition 4 from Haeffele and Vidal (2020). Equation (12) can be obtained from the Lemma 1, for any stationary points, $\\left(r,\\left\\{W_{j}\\right\\}\\right)$ of $\\mathrm{NC}_{\\mu_{N}}(\\cdot)$.\nSince, $f \\in L^{2}(\\mu) \\cap L^{2}\\left(\\mu_{N}\\right) \\subseteq L^{2}\\left(\\mu_{n}\\right)$, and the parameters satisfy the equality in Lemma 1, we can conclude that Equation (12) holds. The local minima of $\\mathrm{NC}_{\\mu_{N}}(\\cdot)$ need not be local minima of $\\mathrm{NC}_{\\mu}(\\cdot)$, therefore we shall obtain an discrepency term. From the fact that $\\ell$ is a $\\alpha$-strongly convex function we have the inequality\n\n$$\n\\ell(g, f)_{\\mu} \\geq \\ell_{\\mu}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)+\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\n$$\n\nAdding $\\lambda \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)$ on both sides we obtain the inequality\n\n$$\n\\begin{aligned}\n\\ell(g, f)_{\\mu}+\\lambda \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\geq \\ell_{\\mu} & \\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)+\\lambda \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\\\\n& +\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\n\\end{aligned}\n$$\n\nNow replacing the first term term on the side with $\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)$ we obtain\n\n$$\n\\ell(g, f)_{\\mu}+\\lambda \\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\geq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)+\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\n$$\n\nFrom Proposition 3 we have that for stationary points $\\left\\{W_{j}\\right\\}$, it holds that $\\Theta_{r}\\left(\\left\\{W_{j}\\right\\}\\right)=$ $\\left\\langle-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell_{\\mu_{N}}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}$. Therefore, by plugging this into the inequality above, we obtain that\n\n$$\n\\begin{aligned}\n\\ell(g, f)_{\\mu}+ & \\lambda\\left\\langle-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell_{\\mu_{N}}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu_{N}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle \\\\\n& \\geq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)+\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\n\\end{aligned}\n$$\n\nRearranging the terms we have\n\n$$\n\\begin{aligned}\n\\ell(g, f)_{\\mu}+ & \\lambda\\left\\langle-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), f\\right\\rangle_{\\mu} \\\\\n& +\\left\\langle\\nabla_{\\bar{Y}} \\ell_{\\mu}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\bar{Y}} \\ell_{\\mu_{N}}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}} \\\\\n& \\geq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\n\\end{aligned}\n$$\n\nNext, the following inequality always holds:\n\n$$\n\\left\\langle f,-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right\\rangle_{\\mu} \\leq \\Omega_{\\mu}(f) \\Omega_{\\mu}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)\n$$\n\nRearranging (98) and plugging in (92), we obtain the inequality\n\n$$\n\\begin{aligned}\n\\ell(g, f)_{\\mu}+ & \\lambda \\Omega_{\\mu}(f) \\Omega_{\\mu}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) \\\\\n& +\\left\\langle\\nabla_{\\bar{Y}} \\ell_{\\mu}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\bar{Y}} \\ell_{\\mu_{N}}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}} \\\\\n& \\geq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\n\\end{aligned}\n$$\n\nWe add and subtract $\\Omega(f)$ to obtain\n\n$$\n\\begin{aligned}\n\\ell(g, f)_{\\mu}+\\lambda \\Omega_{\\mu}(f)+ & \\lambda \\Omega_{\\mu}(f)\\left[\\Omega_{\\mu}^{*}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& +\\left\\langle\\nabla_{\\tilde{Y}} \\ell_{\\mu}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell_{\\mu_{N}}\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}} \\\\\n& \\geq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)+\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\n\\end{aligned}\n$$\n\nRearranging the right most term and using the definition of $C_{\\mu}(f)$ we obtain,\n\n$$\n\\begin{aligned}\n\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\mathrm{C}_{\\mu}(f)+ & \\lambda \\Omega_{\\mu}(f)\\left[\\Omega_{\\mu}^{*}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2} \\\\\n& +\\left[\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu_{N}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)_{\\mu_{N}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}\\right]\n\\end{aligned}\n$$\n\nThis yields the right hand side of (13). As for the left hand side, by definition we have that for any $\\left(r,\\left\\{W_{j}\\right\\}\\right)$, $\\mathrm{C}_{\\mu}\\left(f_{\\mu}^{*}\\right) \\leq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)$. This completes the proof.\n\nTheorem 1 provides the behavior of ERM solutions in the population landscape. This paves a path to bound the empirical and population objectives at these stationary points.", "tables": {}, "images": {}}, {"section_id": 18, "text": "# B STATISTICAL BOUNDS \n\nThis section provides a more general version of Theorem 2 that does not need Assumption 7 to hold uniformly for all the data points, $(X, Y)$. Rather, we relax the assumption to the following.\nAssumption 7' (Probabilistic boundedness). There exists a convex set, $\\mathcal{C} \\subseteq \\mathbb{R}^{n_{X}} \\times \\mathbb{R}^{n_{Y}}$ such that\n\n$$\nP\\left(\\cap_{i=1}^{N}\\left(X_{i}, \\epsilon_{i}\\right) \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}\n$$\n\nFor all $(X, \\epsilon) \\in \\mathcal{C}$, and $\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}$ the predictions and gradients are bounded; i.e.,\n\n$$\n\\left\\|\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)\\right\\| \\leq B_{\\Phi},\\left\\|\\nabla_{\\tilde{Y}} \\ell\\left(g(X, \\epsilon), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)\\right\\| \\leq B_{\\ell}\\right.\n$$\n\nFurther, for any $\\left\\{W_{j}\\right\\},\\left\\{\\tilde{W}_{j}^{\\prime}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}, W, \\tilde{W} \\in \\mathcal{F}_{\\theta},(X, \\epsilon) \\in \\mathcal{C}$, the network $\\phi$ and $\\Phi_{r}$ are Lipschitz in the parameters; i.e,\n\n$$\n\\left\\|\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)(X)-\\Phi_{r}\\left(\\left\\{\\tilde{W}_{j}\\right\\}\\right)(X)\\right\\| \\leq \\tilde{L}_{\\Phi} \\max _{j}\\left\\|W_{j}-\\tilde{W}_{j}\\right\\|_{2}\n$$\n\nand\n\n$$\n\\|\\phi(W)(X)-\\phi(\\tilde{W})(X)\\| \\leq \\tilde{L}_{\\phi}\\|W-\\tilde{W}\\|_{2}\n$$\n\nAdditionally, define the quantity\n\n$$\n\\begin{aligned}\nB(\\mathcal{C}) & :=\\left[(1+\\alpha) \\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right|\\right. \\\\\n& \\left.+\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}, W^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu}\\right|\\right. \\\\\n& \\left.+\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}\\right|\\right]\n\\end{aligned}\n$$\n\nwhere $\\mathcal{P}_{\\mathcal{C}}(\\cdot)$ is the Euclidean projection to the set $\\mathcal{C}$.\nComparison with Assumption 7: Unlike in Assumption 7, we do not require the equations (102), (103), and (104) to hold for all the inputs. However, we relax this restriction by assuming that there exists a convex set, $\\mathcal{C}$ which consists of the data points with probability at least $1-\\delta_{\\mathcal{C}}$. For well-behaved probability distributions like sub-Gaussian distributions (see Assumption 2'), such a convex exists with very high probability; i.e., very small $\\delta_{\\mathcal{C}}$.\nNow we state the general master theorem that relies on Assumptions 1, 2', 3, 4, 5, 6 and 7' (but not on Assumption 7).\n\nTheorem 4 (General Master Theorem). Suppose Assumptions 1, 2', 3, 4, 5, 6 and 7' hold. Let $\\delta \\in(0,1]$ be fixed, and let $f_{\\mu}^{*}$ be the global optimum of $\\mathrm{C}_{\\mu}$. Suppose that $\\gamma \\geq \\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}$, and define\n\n$$\n\\begin{aligned}\n& \\epsilon_{1}=16 \\gamma^{2} \\sigma_{X}^{2} \\max \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{L_{\\Phi}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\} \\\\\n& \\epsilon_{2}=4 \\tilde{L}_{\\Phi} B_{\\Phi} \\max \\left\\{1,2 L+\\frac{2 B_{\\ell}}{B_{\\Phi}}, 8 \\Omega\\left(f_{\\mu}^{*}\\right) \\frac{B_{\\ell} \\tilde{L}_{\\phi}}{\\tilde{L}_{\\Phi} B_{\\Phi}}, 8 L \\Omega\\left(f_{\\mu}^{*}\\right)\\right\\}\n\\end{aligned}\n$$\n\nLet $\\left\\{W_{j}\\right\\}$ denote any stationary point of $\\mathrm{NC}_{\\mu_{N}}(\\cdot)$. Then with probability at least $1-\\left(\\delta+\\delta_{\\mathcal{C}}\\right)$, it holds that\n\n$$\n\\begin{aligned}\n& \\frac{1}{n_{Y}}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\\\\n& \\quad \\lesssim \\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{\\tau}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2 n_{Y}}\\left\\|f_{\\mu}^{*}-\\Phi_{\\tau}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2} \\\\\n& \\quad+\\frac{B(\\mathcal{C})}{n_{Y}}+(1+\\alpha) \\epsilon_{1} \\sqrt{\\frac{R \\cdot \\operatorname{dim}(\\mathcal{W}) \\log \\left(\\gamma \\epsilon_{2} r_{\\theta} / L_{\\phi}\\right) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{aligned}\n$$\n\nAdditional Remarks: In addition to the discussion in Section 4, the general version mentioned above (i) takes into account unbounded sub-Gaussian distributions, and (ii) imposes a weaker notion of Lipschitz continuity on the parameters. For sub-Gaussian inputs, one may choose the convex set $\\mathcal{C}$ to be a ball with radius $\\mathbb{B}(g)$. As we grow $g$, the term $B(\\mathcal{C})$ decays exponentially, and $\\epsilon_{2}$ only grows in the order of polynomial. This fast decay allows us to keep the statistical error under control while pertaining to the optimal sample complexity. Theorem 4 reduces to Theorem 2 by setting $\\alpha=0$ and $\\mathcal{C}=\\operatorname{conv}(\\mathcal{X})$. Under this choice, Assumption 7' coincides with Assumption 7.\nWe discuss the proof in section B.2. Before diving into the proof, we discuss a few preliminaries on the covering number essential to estimate the capacity of the hypotheses class.", "tables": {}, "images": {}}, {"section_id": 19, "text": "# B. 1 Computing Function Class Capacities \n\nLemma 2 (Covering number of $\\mathcal{F}_{\\mathcal{W}}$ ). Under assumption 6, and 7' the $\\nu$-net covering number of the set $\\mathcal{F}_{\\mathcal{W}}$ on the metric, $\\|\\cdot\\|_{\\infty, d}$ is upper bounded via\n\n$$\n\\mathcal{C}_{\\mathcal{F}_{\\mathcal{W}}}(\\nu) \\leq\\left(\\mathcal{C}_{\\theta}\\left(L_{\\phi} \\nu / \\gamma\\right)\\right)^{R}\n$$\n\nwhere $\\mathcal{C}_{\\theta}(\\nu):=\\mathcal{N}(\\{W: \\theta(W) \\leq 1\\}, d(., .), \\nu)$.\nProof. Recall that\n\n$$\n\\begin{aligned}\n\\mathcal{C}_{\\mathcal{F}_{\\mathcal{W}}}(\\nu) & :=\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}}, \\max _{i} d(., .), \\nu\\right) \\\\\n\\mathcal{C}_{\\theta}(\\nu) & :=\\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(., .), \\nu\\right)\n\\end{aligned}\n$$\n\nBy the definition of $\\nu$ covering number,\n\n$$\n\\begin{aligned}\n\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}},\\|\\cdot\\|_{\\infty, d}, \\nu\\right) & :=\\inf \\left|\\left\\{\\left\\{W_{j}^{0}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}: \\forall\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|\\left\\{W_{j}\\right\\}-\\left\\{W_{j}^{0}\\right\\}\\right\\|_{\\infty, d}=\\max _{j} d\\left(W_{j}, W_{j}^{0}\\right) \\leq \\nu\\right\\}\\right| \\\\\n\\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(\\cdot, \\cdot), \\nu\\right) & :=\\inf \\left|\\left\\{W^{0} \\in \\mathcal{F}_{\\theta}: \\forall W \\in \\mathcal{F}_{\\theta}: d\\left(W, W^{0}\\right) \\leq \\nu\\right\\}\\right|\n\\end{aligned}\n$$\n\nTherefore we can upper bound $\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}},\\|\\cdot\\|_{\\infty, d}, \\nu\\right)$ witht he product of $\\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(\\cdot, \\cdot), \\nu\\right) R$ times. We have\n\n$$\n\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}},\\|\\cdot\\|_{\\infty, d}, \\nu\\right) \\leq\\left[\\mathcal{N}\\left(\\frac{\\gamma}{L_{\\phi}} \\mathcal{F}_{\\theta}, d(., .), \\nu\\right)\\right]^{R}\n$$\n\nRewriting the above for appropriately chosen $\\nu$ we get\n\n$$\n\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}},\\|\\cdot\\|_{\\infty, d}, \\nu\\right) \\leq\\left[\\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(., .), \\frac{L_{\\phi} \\nu}{\\gamma}\\right)\\right]^{R}\n$$\n\nThis concludes our proof.\n\nLemma 3 (Bounding covering number). Consider a metric space, $\\left(\\mathcal{W} \\subseteq \\mathbb{R}^{n},\\|\\cdot\\|_{2}\\right)$ and a compact set, $\\mathcal{F}_{\\mathcal{W}} \\subseteq \\mathcal{W}$. Suppose that there exist $r<\\infty$ such that $\\mathcal{F}_{\\mathcal{W}} \\subseteq \\mathbb{B}(r)$. Then we have\n\n$$\n\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}},\\|\\cdot\\|_{2}, \\nu\\right) \\leq\\left(1+\\frac{2 r}{\\nu}\\right)^{n}\n$$\n\nProof. We have that $\\mathcal{F}_{\\mathcal{W}} \\subseteq \\mathbb{B}(r)$. By monotonicity of covering numbers, we have that\n\n$$\n\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}},\\|\\cdot\\|_{2}, \\nu\\right) \\leq \\mathcal{N}(\\mathbb{B}(r),\\|\\cdot\\|_{2}, \\nu)\n$$\n\nFrom Corollary 4.2.13 in Vershynin (2018) we have that,\n\n$$\n\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}},\\|\\cdot\\|_{2}, \\nu\\right) \\leq \\mathcal{N}(\\mathbb{B}(r),\\|\\cdot\\|_{2}, \\nu) \\leq\\left(1+\\frac{2 r}{\\nu}\\right)^{n}\n$$", "tables": {}, "images": {}}, {"section_id": 20, "text": "# B. 2 Proof of Theorem 2 \n\nThis section discusses the proof of Theorem 4. We extensively use concentration results from Section D that are preliminaries for the upcoming technical details.\n\nProof. First, we recall the definition of generalization error:\n\n$$\n\\text { Generalization Error }:=\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right|\n$$\n\nWe can bound the above from the optimality gaps obtained in Theorem 1 via the following decomposition:\n\n$$\n\\begin{aligned}\n\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)= & \\underbrace{\\left[\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{C}_{\\mu}\\left(f_{\\mu}\\right)\\right]}_{\\text {Population Gap }}-\\underbrace{\\left[\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}\\right)\\right]}_{\\text {Empirical Gap }} \\\\\n& +\\underbrace{\\left[\\mathrm{C}_{\\mu}\\left(f_{\\mu}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}\\right)\\right]}_{\\text {Convex Gap }} .\n\\end{aligned}\n$$\n\nFrom Theorem 1 we have that for any $f_{\\mu}, f_{\\mu_{N}} \\in L^{2}(\\mu) \\cap L^{2}\\left(\\mu_{N}\\right)$ and stationary points $\\left(r,\\left\\{W_{j}\\right\\}\\right)$, the empirical gap is bounded by\n\n$$\n\\begin{aligned}\n\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}^{*}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}\\right) & \\leq \\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}\\right) \\\\\n& \\leq \\lambda \\Omega\\left(f_{\\mu_{N}}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f_{\\mu_{N}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2}\n\\end{aligned}\n$$\n\nand the population gap is bounded by\n\n$$\n\\begin{aligned}\n\\mathrm{C}_{\\mu}\\left(f_{\\mu}^{*}\\right)-\\mathrm{C}_{\\mu}\\left(f_{\\mu}\\right) \\leq & \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{C}_{\\mu}\\left(f_{\\mu}\\right) \\\\\n\\leq & \\lambda \\Omega\\left(f_{\\mu}\\right)\\left[\\Omega_{\\mu}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f_{\\mu}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2} \\\\\n& +\\left[\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}\\right]\n\\end{aligned}\n$$\n\nFor any $f_{\\mu}, f_{\\mu_{N}}$, subtracting the above two equations we obtain\n\n$$\n\\begin{aligned}\n& \\mathrm{C}_{\\mu}\\left(f_{\\mu}^{*}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}\\right)-\\lambda \\Omega\\left(f_{\\mu_{N}}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]+\\frac{\\alpha}{2}\\left\\|f_{\\mu_{N}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2} \\\\\n& \\leq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right) \\\\\n& \\leq \\lambda \\Omega\\left(f_{\\mu}\\right)\\left[\\Omega_{\\mu}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f_{\\mu}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2} \\\\\n& \\quad+\\left[\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}\\right] \\\\\n& \\quad+\\left[\\mathrm{C}_{\\mu}\\left(f_{\\mu}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}^{*}\\right)\\right] \\text {. }\n\\end{aligned}\n$$\n\nBy choosing $f_{\\mu}=f_{\\mu_{N}}=f_{\\mu}^{*}$ (as $f_{\\mu}^{*} \\in L^{2}(\\mu) \\cap L^{2}\\left(\\mu_{N}\\right)$ ) and noting that $f_{\\mu}^{*}$ is not a random variable unlike $f_{\\mu_{N}}^{*}$ (which depends on the data points) we get\n\n$$\n\\begin{aligned}\nC_{\\mu}\\left(f_{\\mu}^{*}\\right)- & C_{\\mu_{N}}\\left(f_{\\mu}^{*}\\right)-\\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]+\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2} \\\\\n\\leq & \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right) \\\\\n\\leq & \\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2} \\\\\n& +\\left[\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}\\right] \\\\\n& +\\left[\\mathrm{C}_{\\mu}\\left(f_{\\mu}^{*}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}^{*}\\right)\\right]\n\\end{aligned}\n$$\n\nSince $f_{\\mu}^{*}$ is the global minimizer of $C_{\\mu}(\\cdot)$, it always holds that $\\mathrm{C}_{\\mu}\\left(f_{\\mu}^{*}\\right) \\leq C_{\\mu}\\left(f_{\\mu_{N}}^{*}\\right)$. We use this fact to upper bound the right side term, upon which we obtain the bound\n\n$$\n\\begin{aligned}\n\\mathrm{C}_{\\mu}\\left(f_{\\mu}^{*}\\right)- & \\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu}^{*}\\right)-\\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]+\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2} \\\\\n\\leq & \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right) \\\\\n\\leq & \\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2} \\\\\n& +\\left[\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}\\right] \\\\\n& +\\left[\\mathrm{C}_{\\mu}\\left(f_{\\mu_{N}}^{*}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}^{*}\\right)\\right]\n\\end{aligned}\n$$\n\nNow we add and subtract $\\Omega_{\\mu}^{\\circ}(\\cdot)^{5}$ and $\\frac{\\alpha}{2}\\left\\|f-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}$ on the right side. We then have that\n\n$$\n\\begin{aligned}\n\\underbrace{\\mathrm{C}_{\\mu}\\left(f_{\\mu}^{*}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu}^{*}\\right)}_{=: T_{1}} & -\\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& +\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2} \\\\\n\\leq & \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right) \\\\\n\\leq & \\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2} \\\\\n& +\\frac{\\alpha}{2}\\left[\\underbrace{\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}}_{=: T_{2}}\\right] \\\\\n& +\\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\underbrace{\\left.\\Omega_{\\mu}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)}_{=: T_{3}}\\right] \\\\\n& +\\left[\\underbrace{\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu_{N}}}_{=: T_{5}}\\right] \\\\\n& +\\left[\\underbrace{\\left.\\mathrm{C}_{\\mu}\\left(f_{\\mu_{N}}^{*}\\right)-\\mathrm{C}_{\\mu_{N}}\\left(f_{\\mu_{N}}^{*}\\right)\\right]}_{=: T_{5}}\\right] .\n\\end{aligned}\n$$\n\nNow we apply uniform concentration on the quantities $T_{1}, T_{2}, T_{3}, T_{4}$, and $T_{5}$ to get bound the statistical error terms.\n\nFrom assumption $7^{\\prime}$ we assume that $\\mathcal{C}$ is some convex set in $\\mathbb{R}^{n_{X}} \\times \\mathbb{R}^{n_{Y}}$ such that the following hold true:\n\n[^0]\n[^0]:    ${ }^{5}$ We are ignoring the input arguments for brevity.\n\n1. For any i.i.d. samples $\\left\\{X_{i}, \\epsilon_{i}\\right\\}$ the $P\\left(\\bigcap_{i=1}^{N}\\left(X_{i}, \\epsilon_{i}\\right) \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}$.\n2. For all $(X, \\epsilon) \\in \\mathcal{C}$ and $\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|f_{\\zeta}(X)\\right\\| \\leq B_{\\Phi}$.\n3. For all $(X, \\epsilon) \\in \\mathcal{C}$ we have $\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|\\nabla_{\\hat{Y}} \\ell(g(X, \\epsilon), f_{\\zeta}(X))\\right\\| \\leq B_{\\ell}$.\n4. For all $(X, \\epsilon) \\in \\mathcal{C}$ and $\\forall \\zeta, \\zeta^{\\prime} \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|f_{\\zeta}(X)-f_{\\zeta^{\\prime}}(X)\\right\\| \\leq \\tilde{L}_{\\Phi}\\left\\|\\zeta-\\zeta^{\\prime}\\right\\|_{\\infty, 2}$.\n5. For all $(X, \\epsilon) \\in \\mathcal{C}$ and $\\forall \\zeta, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}:\\left\\|f_{\\zeta}(X)-f_{\\zeta^{\\prime}}(Z)\\right\\| \\leq \\tilde{L}_{\\phi}\\left\\|\\zeta-\\zeta^{\\prime}\\right\\|_{2}$.\n6. For any $\\hat{Y}_{1}, \\hat{Y}_{2} \\in \\mathbb{R}^{n_{Y}}$ we have $\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(Y, \\hat{Y}_{1}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(Y, \\hat{Y}_{2}\\right)\\right\\| \\leq L\\left\\|\\hat{Y}_{1}-\\hat{Y}_{2}\\right\\|$.\n7. $B_{n r m}(\\mathcal{C}):=\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu}^{2}\\right|<\\infty$.\n8. $B_{p l r}(\\mathcal{C}):=\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right), f_{\\zeta^{\\prime}} \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right|<\\infty$.\n9. $B_{e q l}(\\mathcal{C}):=\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right), f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu}\\right|<\\infty$.\n\nNext, we define the events\n\n$$\n\\begin{aligned}\n& \\mathcal{E}_{c v x}(\\epsilon):=\\left\\{\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left|\\mathcal{C}_{\\mu_{N}}\\left(f_{\\zeta}\\right)-\\mathcal{C}_{\\mu}\\left(f_{\\zeta}\\right)\\right| \\leq \\epsilon+B_{n r m}(\\mathcal{C})\\right\\} \\\\\n& \\mathcal{E}_{e q l}(\\epsilon):=\\left\\{\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu_{N}}\\right| \\leq \\epsilon+B_{e q l}(\\mathcal{C})\\right\\}\n\\end{aligned}\n$$\n\nSince $\\Omega^{\\circ}(\\cdot)$ is positively homogeneous function we can ignore the scalar $-\\frac{1}{\\lambda}$ while defining the events below:\n\n$$\n\\begin{aligned}\n\\mathcal{E}_{p l r}(\\epsilon) & :=\\left\\{\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left|\\Omega_{\\mu_{N}}^{\\circ}\\left(\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)-\\Omega_{\\mu}^{\\circ}\\left(\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)\\right| \\leq \\epsilon+B_{p l r}(\\mathcal{C})\\right\\} \\\\\n\\mathcal{E}_{n r m}(\\epsilon) & :=\\left\\{\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left|\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu_{N}}^{2}-\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu}^{2}\\right| \\leq \\epsilon+B_{n r m}(\\mathcal{C})\\right\\}\n\\end{aligned}\n$$\n\nFinally, define the following good event:\n\n$$\n\\mathcal{E}_{\\text {good }}(\\epsilon):=\\mathcal{E}_{c v x}\\left(\\frac{\\epsilon}{4}\\right) \\cap \\mathcal{E}_{e q l}\\left(\\frac{\\epsilon}{4}\\right) \\cap \\mathcal{E}_{p l r}\\left(\\frac{\\epsilon}{4 \\Omega\\left(f_{\\mu}^{*}\\right)}\\right) \\cap \\mathcal{E}_{n r m}\\left(\\frac{\\epsilon}{2}\\right)\n$$\n\nWhen the event $\\mathcal{E}_{\\text {good }}(\\epsilon)$ holds then we obtain following from the inequality (153),\n\n$$\n\\begin{aligned}\n-\\epsilon / 4-B_{c v x}(\\mathcal{C})- & \\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]+\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2} \\\\\n& \\leq \\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right) \\\\\n& \\leq \\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]-\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2} \\\\\n& +\\frac{\\alpha}{2}\\left[\\epsilon / 2+B_{n r m}(\\mathcal{C})\\right]+\\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\epsilon /\\left(4 \\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\right)+B_{p l r}(\\mathcal{C})\\right] \\\\\n& +\\left[\\epsilon / 4+B_{e q l}(\\mathcal{C})\\right]+\\left[\\epsilon / 4+B_{n r m}(\\mathcal{C})\\right]\n\\end{aligned}\n$$\n\nFor $\\alpha \\geq 0$, these inequalities imply that\n\n$$\n\\begin{aligned}\n\\left|N C_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-N C_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\leq & \\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& -\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2}+(1+\\alpha) \\epsilon+(1+\\alpha) B_{n r m}(\\mathcal{C})+B_{e q l}(\\mathcal{C})+\\lambda \\Omega\\left(f_{\\mu}^{*}\\right) B_{p l r}(\\mathcal{C})\n\\end{aligned}\n$$\n\nEquation (163) holds with probability $\\mathbb{P}\\left(\\mathcal{E}_{\\text {good }}(\\epsilon)\\right)$. We can bound the good event with union bound via\n\n$$\n\\mathbb{P}\\left(\\mathcal{E}_{\\text {good }}(\\epsilon)\\right) \\geq 1-\\underbrace{\\mathbb{P}\\left(\\mathcal{E}_{\\text {nrm }}^{c}\\left(\\frac{\\epsilon}{2}\\right)\\right)}_{\\text {Lemma } 8}-\\underbrace{\\mathbb{P}\\left(\\mathcal{E}_{c v x}^{c}\\left(\\frac{\\epsilon}{4}\\right)\\right)}_{\\text {Lemma } 9}-\\underbrace{\\mathbb{P}\\left(\\mathcal{E}_{e q l}^{c}\\left(\\frac{\\epsilon}{4}\\right)\\right)}_{\\text {Lemma } 10}-\\underbrace{\\mathbb{P}\\left(\\mathcal{E}_{p l r}^{c}\\left(\\frac{\\epsilon}{4 \\lambda \\Omega\\left(f_{\\mu}^{*}\\right)}\\right)\\right)}_{\\text {Lemma } 11}\n$$\n\nUnder Assumptions 1-6 and $7^{\\prime}$ we can apply Lemma $8,9,10$, and 11 to bound the probability of the occurrence of the events, $\\mathcal{E}_{c v x}(\\cdot), \\mathcal{E}_{e q l}(\\cdot), \\mathcal{E}_{p l r}(\\cdot)$, and $\\mathcal{E}_{n r m}(\\cdot)$.\n\nDefine the constants\n\n$$\n\\begin{aligned}\n& B_{1}:=4 n_{Y} L\\left[\\left(\\gamma^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{Y \\mid X}^{2}\\right] \\\\\n& B_{2}:=16 n_{y} \\gamma\\left\\|\\nabla_{\\tilde{Y}} \\ell\\right\\|_{\\mathrm{Lip}} \\sigma_{X} \\sqrt{\\left(\\gamma^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{E \\mid X}^{2}} \\\\\n& B_{3}:=16 n_{y} \\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}\\left\\|\\nabla_{\\tilde{Y}} \\ell\\right\\|_{\\mathrm{Lip}} \\sigma_{X} \\sqrt{\\left(\\gamma^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{E \\mid X}^{2}} \\\\\n& B_{4}:=128 n_{Y} \\gamma^{2} \\sigma_{X}^{2} \\\\\n& \\epsilon_{0}:=\\max \\left\\{B_{1}, B_{2}, B_{3}, B_{4}\\right\\} \\\\\n& \\epsilon_{1}:=\\max \\left\\{B_{1}, B_{2}, B_{3}, B_{4}\\right\\} \\\\\n& b_{1}:=8 B_{\\ell} \\tilde{L}_{\\Phi} \\\\\n& b_{2}:=8 \\tilde{L}_{\\Phi}\\left[B_{\\ell}+B_{\\Phi} L\\right] \\\\\n& b_{3}:=32 \\Omega\\left(f_{\\mu}^{*}\\right) \\max \\left\\{\\tilde{L}_{\\phi} B_{\\ell}, L \\tilde{L}_{\\Phi} B_{\\Phi}\\right\\} \\\\\n& b_{4}:=4 \\tilde{L}_{\\Phi} B_{\\Phi} \\\\\n& \\epsilon_{2}:=\\max \\left\\{b_{1}, b_{2}, b_{3}, b_{4}\\right\\}\n\\end{aligned}\n$$\n\nUnder the above conditions by Lemma 8 , for any $\\epsilon \\in\\left[0, B_{4}\\right]$ we have that\n\n$$\n\\mathbb{P}\\left(\\mathcal{E}_{\\text {nrm }}^{c}\\left(\\frac{\\epsilon}{2}\\right)\\right) \\leq \\delta_{\\mathcal{C}}+c_{4} \\exp \\left(\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\text {iv }}}\\left(\\frac{\\epsilon}{b_{4}}\\right)\\right)-N\\left(\\frac{\\epsilon}{B_{4}}\\right)^{2}\\right)\n$$\n\nBy Lemma 9, for any $\\epsilon \\in\\left[0, B_{1}\\right]$, we have\n\n$$\n\\mathbb{P}\\left(\\mathcal{E}_{c v x c c c c c c c c c c c c}\\left(\\frac{\\epsilon}{4}\\right)\\right) \\leq \\delta_{\\mathcal{C}}+2 \\exp \\left(\\log \\left(C_{\\mathcal{F}_{\\text {iv }}}\\left(\\frac{\\epsilon}{b_{1}}\\right)\\right)-c_{1} N\\left(\\frac{\\epsilon}{B_{1}}\\right)^{2}\\right)\n$$\n\nfor some positive constant, $c_{1}$.\nAdditionally by Lemma 10, for any $\\epsilon \\in\\left[0, B_{2}\\right]$ we have that\n\n$$\n\\mathbb{P}\\left(\\mathcal{E}_{\\text {vqf }}^{c}\\left(\\frac{\\epsilon}{4}\\right)\\right) \\leq \\delta_{\\mathcal{C}}+c_{1} \\exp \\left(\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\text {iv }}}\\left(\\frac{\\epsilon}{b_{2}}\\right)\\right)-N\\left(\\frac{\\epsilon}{B_{2}}\\right)^{2}\\right)\n$$\n\nfor some positive constant, $c_{2}$. Furthermore, by Lemma 11, for any $\\epsilon \\in\\left[0, B_{3}\\right]$ we have that\n\n$$\n\\mathbb{P}\\left(\\mathcal{E}_{\\text {pf } r}^{c}\\left(\\frac{\\epsilon}{4 \\Omega\\left(f_{\\mu}^{*}\\right)}\\right)\\right) \\leq \\delta_{\\mathcal{C}}+c_{3} \\exp \\left(\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\text {iv }}}\\left(\\frac{\\epsilon}{b_{3}}\\right)\\right)+\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\phi}}\\left(\\frac{\\epsilon}{b_{3}}\\right)\\right)-N\\left(\\frac{\\epsilon}{B_{3}}\\right)^{2}\\right)\n$$\n\nfor some positive constant, $c_{3}$.\nFor the inequalities (177), (178), (179), and (176) to all hold we choose $\\epsilon \\in\\left[0, \\epsilon_{0}\\right]$ and we upper bound the covering numbers $C_{\\mathcal{F}_{\\text {iv }}}(\\nu)$ as they are strictly decreasing in $\\nu$ by definition. Therefore, we have that\n\n$$\n\\max \\left\\{\\log \\left(C_{\\mathcal{F}_{\\text {iv }}}\\left(\\frac{\\epsilon}{b_{1}}\\right)\\right), \\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\text {iv }}}\\left(\\frac{\\epsilon}{b_{2}}\\right)\\right), \\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\text {iv }}}\\left(\\frac{\\epsilon}{b_{3}}\\right)\\right), \\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\text {iv }}}\\left(\\frac{\\epsilon}{b_{4}}\\right)\\right)\\right\\} \\leq \\log \\left(C_{\\mathcal{F}_{\\text {iv }}}\\left(\\frac{\\epsilon}{c_{2}}\\right)\\right)\n$$\n\nand\n\n$$\n\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\phi}}\\left(\\frac{\\epsilon}{b_{3}}\\right)\\right) \\leq \\log \\left(C_{\\mathcal{F}_{\\phi}}\\left(\\frac{\\epsilon}{c_{2}}\\right)\\right)\n$$\n\nNow we plug in inequalities (177), (178), (179), and (176) in the inequality (164). Denote\n\n$$\nc_{5}:=\\max \\left\\{2, c_{2}, c_{3}, c_{4}\\right\\}\n$$\n\nThen\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(\\mathcal{E}_{\\text {good }}(\\epsilon)\\right) & \\geq 1-c_{5} \\exp \\left(\\log \\left(\\mathcal{C}_{\\mathcal{F}_{W}}\\left(\\epsilon / \\epsilon_{3}\\right)\\right)\\right) \\times\\left[\\exp \\left(-c_{1} N\\left(\\frac{\\epsilon}{B_{1}}\\right)^{2}\\right)+\\exp \\left(-N\\left(\\frac{\\epsilon}{B_{2}}\\right)^{2}\\right)\\right. \\\\\n& \\left.+\\exp \\left(\\log \\left(C_{\\mathcal{F}_{\\theta}}\\left(\\frac{\\epsilon}{\\epsilon_{3}}\\right)\\right)-N\\left(\\frac{\\epsilon}{B_{3}}\\right)^{2}\\right)+\\exp \\left(-N\\left(\\frac{\\epsilon}{B_{4}}\\right)^{2}\\right)\\right]-4 \\delta_{\\mathcal{C}}\n\\end{aligned}\n$$\n\nNow we lower bound the right side by replacing $B_{1}, B_{2}, B_{3}, B_{4}$ with the upper bound $\\epsilon_{1}$ yielding\n\n$$\n\\mathbb{P}\\left(\\mathcal{E}_{\\text {good }}(\\epsilon)\\right) \\geq 1-c_{6} \\exp \\left(\\log \\left(\\mathcal{C}_{\\mathcal{F}_{W}}\\left(\\epsilon / \\epsilon_{2}\\right)\\right)+\\log \\left(C_{\\mathcal{F}_{\\theta}}\\left(\\frac{\\epsilon}{\\epsilon_{3}}\\right)\\right)-c_{7} N\\left(\\frac{\\epsilon}{\\epsilon_{1}}\\right)^{2}\\right)-4 \\delta_{\\mathcal{C}}\n$$\n\nfor some positive constants, $c_{6}, c_{7}$.\nFrom Lemma 2 we have that for any $\\nu>0$ it holds that\n\n$$\n\\log \\left(\\mathcal{C}_{\\mathcal{F}_{W}}(\\nu)\\right) \\leq R \\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\theta}}\\left(L_{\\phi} \\nu / \\gamma\\right)\\right)\n$$\n\nThen we obtain\n\n$$\n\\mathbb{P}\\left(\\mathcal{E}_{\\text {good }}(\\epsilon)\\right) \\geq 1-c_{8} \\exp \\left(R \\log \\left(C_{\\mathcal{F}_{\\theta}}\\left(\\frac{L_{\\phi} \\epsilon}{\\gamma \\epsilon_{2}}\\right)\\right)-c_{9} N\\left(\\frac{\\epsilon}{\\epsilon_{1}}\\right)^{2}\\right)-4 \\delta_{\\mathcal{C}}\n$$\n\nfor some positive constants $c_{8}, c_{9}$.\nFrom inequality (163), and (186) for any $\\epsilon \\in\\left[0, \\epsilon_{0}\\right]$ we have that\n\n$$\n\\begin{gathered}\n\\mathbb{P}\\left(\\left|N C_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-N C_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\geq \\lambda \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right]\\right. \\\\\n\\left.-\\frac{\\alpha}{2}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2}+(1+\\alpha) \\epsilon+B_{e q l}(\\mathcal{C})+B_{p l r}(\\mathcal{C})+(1+\\alpha) B_{n r m}(\\mathcal{C})\\right) \\\\\n\\leq c_{8} \\exp \\left(R \\log \\left(C_{\\mathcal{F}_{\\theta}}\\left(\\frac{L_{\\phi} \\epsilon}{\\gamma \\epsilon_{2}}\\right)\\right)-c_{9} N\\left(\\frac{\\epsilon}{\\epsilon_{1}}\\right)^{2}\\right)+4 \\delta_{\\mathcal{C}}\n\\end{gathered}\n$$\n\nNext, we derive the operation conditions for $\\epsilon$ in terms of $B_{2}, B_{3}$, and $B_{4}$.\n\n- $B_{2} \\geq B_{3}$ : observe that\n\n$$\nB_{2} \\geq B_{3} \\Longleftrightarrow \\gamma \\geq \\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}\n$$\n\nwhich establishes an upper bound on the regularization parameter.\n\n- To establish a lower bound on regularization, we will require that $\\min \\left\\{B_{2}, B_{3}\\right\\} \\geq B_{4}$ : We have that\n\n$$\n\\begin{aligned}\n\\min \\left\\{B_{2}, B_{3}\\right\\} & \\geq B_{4} \\Longleftrightarrow \\\\\n& \\left(4 \\min \\left\\{1, \\frac{\\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}}{\\gamma}\\right\\}\\right) 4 \\gamma^{2} \\sigma_{X}^{2} \\sqrt{\\left(1+\\|g\\|_{\\mathrm{Lip}}^{2} / \\gamma^{2}\\right)+\\frac{\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{Y \\mid X}^{2}}{\\gamma^{2} \\sigma_{X}^{2}}} \\geq 16 \\gamma^{2} \\sigma_{X}^{2}\n\\end{aligned}\n$$\n\nIt is sufficient to have the below inequality to hold:\n\n$$\n\\min \\left\\{1, \\frac{\\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}}{\\gamma}\\right\\} \\geq \\frac{\\gamma}{\\sqrt{\\gamma^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}}} \\Longrightarrow \\min \\left\\{B_{2}, B_{3}\\right\\} \\geq B_{4}\n$$\n\nTherefore, $\\gamma \\geq \\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}$ is sufficient condition for $B_{2} \\geq B_{3} \\geq B_{4}$.\nThen we have that\n\n$$\n\\epsilon_{0}=\\min \\left\\{B_{1}, B_{4}\\right\\}=16 n_{Y} \\gamma^{2} \\sigma_{X}^{2} \\min \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\mathrm{Lip}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nand\n\n$$\n\\epsilon_{1}=16 n_{Y} \\gamma^{2} \\sigma_{X}^{2} \\max \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\mathrm{Lip}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nNow rescale the quantities $\\epsilon \\leftarrow n_{Y} \\frac{\\epsilon}{(1+\\alpha)}$, $\\epsilon_{0} \\leftarrow n_{Y} \\epsilon_{0}$, and $\\epsilon_{1} \\leftarrow n_{Y} \\epsilon_{1}$. Then we have\n\n$$\n\\begin{aligned}\n& \\epsilon_{0}=16 \\gamma^{2} \\sigma_{X}^{2} \\min \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\mathrm{Lip}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\} \\\\\n& \\epsilon_{1}=16 \\gamma^{2} \\sigma_{X}^{2} \\max \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\mathrm{Lip}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\} \\\\\n& \\epsilon_{2}=\\max \\left\\{8 B_{\\ell} \\tilde{L}_{\\Phi}, 8\\left[\\tilde{L}_{\\Phi} B_{\\ell}+\\tilde{L}_{\\Phi} B_{\\Phi} L\\right], 32 \\Omega\\left(f_{\\mu}^{*}\\right) \\tilde{L}_{\\Phi} \\max \\left\\{\\tilde{L}_{\\phi} B_{\\ell} / \\tilde{L}_{\\Phi}, L B_{\\Phi}\\right\\}, 4 \\tilde{L}_{\\Phi} B_{\\Phi}\\right\\}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\frac{1}{n_{Y}}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\geq \\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& -\\frac{\\alpha}{2 n_{Y}}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2}+\\frac{1}{n_{Y}}\\left[B_{e q l}(\\mathcal{C})+\\lambda \\Omega\\left(f_{\\mu}^{*}\\right) B_{p l r}(\\mathcal{C})+(1+\\alpha) B_{n r m}(\\mathcal{C})\\right]+\\epsilon\\right) \\\\\n& \\leq c_{8} \\exp \\left(R \\log \\left(C_{\\mathcal{F}_{\\theta}}\\left(\\frac{L_{\\phi} \\epsilon}{\\gamma \\epsilon_{2}}\\right)\\right)-c_{9} N\\left(\\frac{\\epsilon}{(1+\\alpha) \\epsilon_{1}}\\right)^{2}\\right)+4 \\delta_{\\mathcal{C}}\n\\end{aligned}\n$$\n\nNow we bound the covering number under Assumption 4 and Lemma 2 via\n\n$$\n\\log \\left(C_{\\mathcal{F}_{\\theta}}\\left(\\frac{L_{\\phi} \\epsilon}{\\gamma \\epsilon_{2}}\\right)\\right) \\leq \\operatorname{dim}(\\mathcal{W}) \\log \\left(1+2 \\gamma \\epsilon_{2} r_{\\theta} /\\left(L_{\\phi} \\epsilon\\right)\\right) \\leq c_{11} \\operatorname{dim}(\\mathcal{W}) \\log \\left(\\gamma \\epsilon_{2} r_{\\theta} /\\left(L_{\\phi} \\epsilon\\right)\\right)\n$$\n\nfor some positive constant $c_{11}$.\nDefine\n\n$$\nB(\\mathcal{C}):=B_{e q l}(\\mathcal{C})+\\lambda \\Omega\\left(f_{\\mu}^{*}\\right) B_{p l r}(\\mathcal{C})+(1+\\alpha) B_{n r m}(\\mathcal{C})\n$$\n\nThen we have that\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(\\frac{1}{n_{Y}}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\right.\\right. & \\left.\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right) \\mid \\geq \\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& \\left.-\\frac{\\alpha}{2 n_{Y}}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2}+\\frac{1}{n_{Y}} B(\\mathcal{C})+\\epsilon\\right) \\\\\n\\leq & c_{12} \\exp \\left(c_{11} R \\operatorname{dim}(\\mathcal{W}) \\log \\left(\\gamma \\epsilon_{2} r_{\\theta} /\\left(L_{\\phi} \\epsilon\\right)\\right)-c_{12} N\\left(\\frac{\\epsilon}{(1+\\alpha) \\epsilon_{1}}\\right)^{2}\\right)+4 \\delta_{\\mathcal{C}}\n\\end{aligned}\n$$\n\nFor some fixed $\\delta \\in(0,1]$ choose\n\n$$\n\\epsilon=\\tilde{\\Theta}\\left((1+\\alpha) \\epsilon_{1} \\sqrt{\\frac{R \\operatorname{dim}(\\mathcal{W}) \\log \\left(\\gamma \\epsilon_{2} r_{\\theta} / L_{\\phi}\\right) \\log (N)+\\log (1 / \\delta)}{N}}\\right)\n$$\n\nThen the right side term of inequality (200) will be\n\n$$\n\\exp \\left(R \\operatorname{dim}(\\mathcal{W}) \\log \\left(\\gamma \\epsilon_{2} r_{\\theta} /\\left(L_{\\phi} \\epsilon\\right)\\right)-c_{12} N\\left(\\frac{\\epsilon}{(1+\\alpha) \\epsilon_{1}}\\right)^{2}\\right)=\\tilde{\\mathcal{O}}(\\delta)\n$$\n\nRewriting the equation (200), we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\frac{1}{n_{Y}}\\left|\\mathbb{N C}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathbb{N C}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\geq \\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{o}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& -\\frac{\\alpha}{2 n_{Y}}\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu_{N}}^{2}+\\frac{1}{n_{Y}} B(\\mathcal{C}) \\\\\n& +(1+\\alpha) \\epsilon_{1} \\sqrt{\\frac{R \\operatorname{dim}(\\mathcal{W}) \\log \\left(\\gamma \\epsilon_{2} r_{\\theta} / L_{\\sigma}\\right) \\log (N)+\\log (1 / \\delta)}{N}} \\leq \\delta+\\delta_{\\mathcal{C}}\n\\end{aligned}\n$$\n\nTheorem 4 has established generalization error for a generic parallel positively homogeneous network. Theorem 2 mentioned in the main text is a special case of Theorem 4, with the choice of convex set $\\mathcal{C}=\\operatorname{conv}\\left(\\mathcal{X} \\times \\mathbb{R}^{n_{Y}}\\right)$ by changing Assumption 7' to Assumption 7. Further, $B\\left(\\mathcal{X} \\times \\mathbb{R}^{n_{Y}}\\right)$ will evaluate to 0 , as $\\mathcal{P}_{\\operatorname{conv}\\left(\\mathcal{X} \\times \\mathbb{R}^{n_{Y}}\\right)}(\\cdot)$ is just an identity operator.", "tables": {}, "images": {}}, {"section_id": 21, "text": "# C APPLICATIONS \n\nIn this section, we apply our Theorem 4 for various applications. We apply our general theorem to low-rank matrix sensing, structured matrix sensing, two-layer linear neural network, two-layer ReLU neural network, and multi-head attention.", "tables": {}, "images": {}}, {"section_id": 22, "text": "## C. 1 Low-Rank Matrix Sensing\n\nIn this section, we state the corollary and its proof for matrix sensing, which is a direct consequence of Theorem 4. Firstly, we need to choose a convex set, $\\mathcal{C}$, such that the Assumption 7' is satisfied. For matrix sensing we choose, $\\mathcal{C}=\\{(X, \\epsilon):\\|X\\|_{F} \\leq g,\\|\\epsilon\\|_{F} \\leq g\\}$ to verify Assumption 7'. We need to compute, $B(\\mathcal{C})$. This involves computing the expectation over the projection. Lemma 4 is pivotal for estimating $B(\\mathcal{C})$ in all the applications that are going to be discussed here.\nLemma 4 (Projection of Gaussian vector on balls). Consider a n-dimensional Gaussian vector $\\mathbf{x} \\sim$ $\\mathcal{N}\\left(0,(1 / n) I_{n}\\right)$. Let $M$ be a fixed matrix in $\\mathbb{R}^{n \\times n}$ and $\\mathcal{A}$ be any set. Then\n\n$$\n\\left|\\left\\langle M, \\mathbb{E}\\left[\\left[\\mathbf{x} \\mathbf{x}^{T}-\\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x})^{T}\\right]\\right] \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right\\rangle\\right| \\leq \\begin{cases}g e^{-g^{2} / 2}\\|M\\|_{2} & \\text { if } g \\geq 1 \\\\ \\frac{1}{g} e^{-g^{2} / 2}\\|M\\|_{2} & \\text { otherwise }\\end{cases}\n$$\n\nwhere $\\mathcal{P}_{\\mathcal{B}(g)}(\\cdot)$ is Euclidean projection onto the ball $\\mathbb{B}(g):=\\left\\{\\mathbf{x}:\\|\\mathbf{x}\\|_{2} \\leq g\\right\\}$.\nProof. Define an event $\\mathcal{E}:=\\{\\mathbf{x} \\in \\mathcal{B}(g)\\}$. When $\\mathcal{E}$ holds the function evaluates to zero,\n\n$$\n\\mathbb{E}\\left[\\left[\\mathbf{x x}^{T}-\\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x})^{T}\\right] \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right]=\\mathbb{E}\\left[\\left(\\mathbf{x x}^{T}-\\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x})^{T}\\right) \\mathbf{1}_{\\mathcal{E}^{c}}(\\mathbf{x}) \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right]\n$$\n\nso it suffices to consider the complement of the event $\\mathcal{E}$. Now we take the inner product with $M$ yielding\n\n$$\n\\begin{aligned}\n\\left|\\left\\langle M, \\mathbb{E}\\left[\\left[\\mathbf{x x}^{T}-\\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x})^{T}\\right] \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right]\\right\\rangle\\right| & =\\left|\\left\\langle M, \\mathbb{E}\\left[\\left(\\mathbf{x x}^{T}-\\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x})^{T}\\right) \\mathbf{1}_{\\mathcal{E}^{c}}(\\mathbf{x}) \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right]\\right\\rangle\\right| \\\\\n& \\stackrel{(a)}{\\leq}\\|M\\|_{2}\\left\\|\\mathbb{E}\\left[\\left(\\mathbf{x x}^{T}-\\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x})^{T}\\right) \\mathbf{1}_{\\mathcal{E}^{c}}(\\mathbf{x}) \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right]\\right\\| \\\\\n& \\stackrel{(b)}{\\leq}\\|M\\|_{2} \\mathbb{E}\\left[\\left\\|\\left(\\mathbf{x x}^{T}-\\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x})^{T}\\right) \\mathbf{1}_{\\mathcal{E}^{c}}(\\mathbf{x})\\right\\| \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right] \\\\\n& \\stackrel{(c)}{=}\\|M\\|_{2} \\mathbb{E}\\left[\\|\\mathbf{x}\\|_{2}^{2}-g^{2}\\left|\\mathbf{1}_{\\mathcal{E}^{c}}(\\mathbf{x}) \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right]\\right. \\\\\n& \\stackrel{(d)}{\\leq}\\|M\\|_{2} \\mathbb{E}\\left[\\left|\\|\\mathbf{x}\\|_{2}^{2}-g^{2}\\right|\\left|\\mathbf{1}_{\\mathcal{E}^{c}}(\\mathbf{x}) \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right]\\right. \\\\\n& \\stackrel{(e)}{\\leq}\\|M\\|_{2} \\mathbb{E}\\left[\\left|\\|\\mathbf{x}\\|_{2}^{2}-g^{2}\\right|\\left|\\mathbf{1}_{\\mathcal{E}^{c}}(\\mathbf{x})\\right| \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right] \\\\\n& \\stackrel{(f)}{=}\\|M\\|_{2} \\int_{\\mathbf{x} \\in \\mathcal{E}^{c}}\\left|\\|\\mathbf{x}\\|_{2}^{2}-g^{2}\\right| \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{\\|\\mathbf{x}\\|_{2}^{2}}{2}} d \\mathbf{x} \\\\\n& \\stackrel{(g)}{\\leq}\\|M\\|_{2}\\left[g e^{-g^{2} / 2}-\\sqrt{\\frac{\\pi}{2}}\\left(g^{2}-1\\right) \\operatorname{erfc}(g / \\sqrt{2})\\right]\n\\end{aligned}\n$$\n\nThe aforementioned computations involves (a) Cauchy-Schwartz inequality, (b) Jensen's inequality, (c) the norm of $\\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x})$ when $\\mathbf{x} i n \\mathcal{E}^{c}$ is $g$, (d) conditioning on indicator functions, (e) removing the conditioning increases the expectation over non-negative terms, (f) we apply the density of Gaussian, (g) standard normal integral. As a consequence of Theorem 1 from Zhang et al. (2020) we bound the complement error function,\n\n$$\n\\frac{e^{-z^{2}}}{\\sqrt{\\pi} z} \\geq \\operatorname{erfc}(z) \\geq \\frac{2}{\\sqrt{\\pi}} \\frac{e^{-z^{2}}}{z+\\sqrt{z^{2}+2}}\n$$\n\nThen we have that\n\n$$\n\\left|\\left\\langle M, \\mathbb{E}\\left[\\left[\\mathbf{x} \\mathbf{x}^{T}-\\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{B}(g)}(\\mathbf{x})^{T}\\right] \\mathbf{1}_{\\mathcal{A}}(\\mathbf{x})\\right]\\right\\rangle\\right| \\leq \\begin{cases}g e^{-g^{2} / 2}\\|M\\|_{2} & \\text { if } g \\geq 1 \\\\ \\frac{1}{g} e^{-g^{2} / 2}\\|M\\|_{2} & \\text { otherwise. }\\end{cases}\n$$\n\nNow, we state the generalization bound for the low-rank matrix sensing followed by its proof.\nCorollary 4 (Low-Rank Matrix Sensing). Consider the true model for $(X, y)$, where $X \\in \\mathbb{R}^{m \\times n}$ is a random matrix with i.i.d. entries $X_{l k} \\sim \\mathcal{N}\\left(0, \\frac{1}{m n}\\right)$ and $y=\\left\\langle M^{*}, X\\right\\rangle+\\epsilon$, where $M^{*} \\in \\mathbb{R}^{m \\times n}$ and $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)$ is independent from $X$. For all $i \\in[N]$, let $\\left(X_{i}, y_{i}\\right)$ be i.i.d. samples from this true model. Consider the estimator $\\hat{y}=\\left\\langle U V^{T}, X\\right\\rangle$, where $U \\in \\mathbb{R}^{m \\times R}$ and $V \\in \\mathbb{R}^{n \\times R}$. Let $\\delta \\in(0,1]$ be fixed. Define the non-convex problem\n\n$$\n\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{MS}}((U, V)):=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left(y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right)^{2}+\\lambda \\sum_{j=1}^{R}\\left\\|\\mathbf{u}_{j}\\right\\|_{2}\\left\\|\\mathbf{v}_{j}\\right\\|_{2}\n$$\n\nand define $\\mathbb{N C}_{\\mu}^{\\text {MS }}((U, V))$ similarly with the sum over $i$ replaced by expectation taken over $(X, y)$.\nLet $(U, V)$ be a stationary point of $\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{MS}}((U, V))$. Suppose there exists $C_{U V}, B_{u}, B_{v}>0$ such that $\\left\\|U V^{T}\\right\\|_{2} \\leq$ $C_{U V}\\left\\|M^{*}\\right\\|_{*}$, and for all $j \\in[R],\\left\\|\\mathbf{u}_{j}\\right\\|_{2} \\leq B_{u},\\left\\|\\mathbf{v}_{j}\\right\\|_{2} \\leq B_{v}$. Then, with probability at least $1-\\delta$, it holds that\n\n$$\n\\begin{aligned}\n\\left|\\mathbb{N C}_{\\mu}^{\\mathrm{MS}}((U, V)))-\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{MS}}((U, V))\\right| & \\lesssim\\left\\|M^{*}\\right\\|_{*}\\left[\\left\\|\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right) X_{i}\\right\\|_{2}-\\lambda\\right] \\\\\n& +C_{U V}^{2}\\left\\|M^{*}\\right\\|_{*}^{2} \\times \\sqrt{\\frac{R \\log \\left(R\\left(C_{U V}+B_{u} B_{v}\\right)\\right)(m+n) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{aligned}\n$$\n\nProof. We set the following to obtain a generalization bound from Theorem 4 for the case of matrix sensing. First,\n\n$$\n\\begin{gathered}\n\\ell(Y, \\hat{Y})=\\frac{1}{2}\\|Y-\\hat{Y}\\|_{2}^{2} \\Longrightarrow(\\alpha, L)=(0,1) \\\\\n\\phi(W)=\\left\\langle\\mathbf{u v}^{T}, X\\right\\rangle \\\\\n\\theta(W)=\\|\\mathbf{u}\\|_{2}\\|\\mathbf{v}\\|_{2}\n\\end{gathered}\n$$\n\nEstimating $\\Omega\\left(f_{\\mu}^{*}\\right)$ : Since, $M^{*}$ is the true matrix the regularizer at globally optimal solution can be upper bounded by Proposition 2,\n\n$$\n\\Omega\\left(f_{\\mu}^{*}\\right) \\leq\\left\\|M^{*}\\right\\|_{*}\n$$\n\nEstimating $\\Omega_{\\mu_{N}}^{\\circ}(\\cdot)$ : Now we move on to compute the polar. We have\n\n$$\n\\begin{aligned}\n\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) & =\\Omega_{\\mu_{N}}^{\\circ}\\left(\\frac{1}{\\lambda}\\left(g-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) \\\\\n& =\\sup _{\\|\\mathbf{u}\\| \\leq 1 ;\\|\\mathbf{v}\\| \\leq 1} \\frac{1}{N \\lambda} \\sum_{i=1}^{N}\\left\\langle Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle, \\mathbf{u}^{T} X_{i} \\mathbf{v}\\right\\rangle \\\\\n& =\\sup _{\\|\\mathbf{u}\\| \\leq 1 ;\\|\\mathbf{v}\\| \\leq 1} \\frac{1}{N \\lambda}\\left\\langle\\mathbf{v}, \\sum_{i=1}^{N}\\left(Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right)^{T} \\mathbf{u}^{T} X_{i}\\right\\rangle \\\\\n& =\\sup _{\\|\\mathbf{u}\\| \\leq 1} \\frac{1}{N \\lambda}\\left\\|\\sum_{i=1}^{N}\\left(Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right) \\mathbf{u}^{T} X_{i}\\right\\| \\\\\n& =\\sup _{\\|\\mathbf{u}\\| \\leq 1} \\frac{1}{N \\lambda}\\left\\|\\sum_{i=1}^{N}\\left(Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right) X_{i}^{T} \\mathbf{u}\\right\\| \\\\\n& =\\frac{1}{\\lambda}\\left\\|\\frac{1}{N} \\sum_{i=1}^{N}\\left(Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right) X_{i}\\right\\|_{2}\n\\end{aligned}\n$$\n\nDefining $\\mathcal{F}_{\\theta}$ : Next, we estimate the relevant constants. First we estimate the constants from Assumption 4, suppose that $\\mathcal{B}:=\\left\\{(\\mathbf{u}, \\mathbf{v}):\\|\\mathbf{u}\\|_{2} \\leq 1,\\|\\mathbf{v}\\|_{2} \\leq 1\\right\\}$\n\n$$\n\\mathcal{F}_{\\theta}:=\\left\\{\\mathbf{u v}^{T}:\\|\\mathbf{u}\\|_{2}\\|\\mathbf{v}\\|_{2} \\leq 1\\right\\} \\cap \\mathcal{B}\n$$\n\nEstimating $L_{\\phi}$ : The Lipschitz constant $L_{\\phi}$ in the function $\\mathcal{F}_{\\theta}$ is $L_{\\phi}=\\sup _{(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}}\\left\\|\\left\\langle\\mathbf{u v}^{T},.\\right\\rangle\\right\\|_{\\text {Lip }}=1$.\nEstimating $r_{\\theta}$ : We have that from A.M-G.M inequality,\n\n$$\n\\|\\mathbf{u}\\|_{2}\\|\\mathbf{v}\\|_{2} \\leq \\frac{1}{2}\\left[\\|\\mathbf{u}\\|_{2}^{2}+\\|\\mathbf{v}\\|_{2}^{2}\\right]\n$$\n\nNow for any $(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}$ we have that $0.5\\left[\\|\\mathbf{u}\\|_{2}+\\|\\mathbf{v}\\|_{2}\\right] \\leq 1$. Therefore, $\\forall(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}$ we have\n\n$$\n\\|\\mathbf{u}\\|_{2}\\|\\mathbf{v}\\|_{2} \\leq \\frac{1}{2}\\left[\\|\\mathbf{u}\\|_{2}^{2}+\\|\\mathbf{v}\\|_{2}^{2}\\right] \\leq \\sqrt{\\frac{1}{2}\\left[\\|\\mathbf{u}\\|_{2}^{2}+\\|\\mathbf{v}\\|_{2}^{2}\\right]}\n$$\n\nThen we need that $\\mathcal{F}_{\\theta} \\subseteq \\mathbb{B}\\left(r_{\\theta}\\right)$, then must be $r_{\\theta}=\\frac{1}{\\sqrt{2}}$.\nDefining $\\mathcal{F}_{\\mathcal{W}}$ : From the corollary's assumptions we have that, $\\mathcal{B}_{R}:=\\left\\{(\\mathbf{u}, \\mathbf{v}):\\|\\mathbf{u}\\|_{2} \\leq B_{u},\\|\\mathbf{v}\\| \\leq B_{v}\\right\\}$; our hypothesis class is defined as\n\n$$\n\\mathcal{F}_{\\mathcal{W}}:=\\left\\{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\}:\\left\\|\\left\\langle U V^{T}, \\cdot\\right\\rangle\\right\\|_{\\text {Lip }}=\\left\\|U V^{T}\\right\\|_{2} \\leq \\gamma\\right\\} \\cap \\mathcal{B}_{R}\n$$\n\nAs $\\gamma \\geq \\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}$, we choose $\\gamma=C_{U V}\\left\\|M^{*}\\right\\|_{*}$ for $C_{U V} \\geq 1$. We have that\n\n$$\n\\mathcal{F}_{\\mathcal{W}}=\\left\\{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\}:\\left\\|\\left\\langle U V^{T}, \\cdot\\right\\rangle\\right\\|_{\\text {Lip }}=\\left\\|U V^{T}\\right\\|_{2} \\leq C_{U V}\\left\\|M^{*}\\right\\|_{*},\\left\\|\\mathbf{u}_{j}\\right\\| \\leq B_{u},\\left\\|\\mathbf{v}_{j}\\right\\| \\leq B_{v}\\right\\}\n$$\n\nEstimating $\\epsilon_{0}$ : From the data generating mechanism we have $\\|g\\|_{\\text {Lip }}=\\left\\|M^{*}\\right\\|_{2}, \\sigma_{X}=1, \\sigma_{Y \\mid X}=\\sigma$. Then we have the following constants from Theorem 4 :\n\n$$\n\\epsilon_{0}=16 \\gamma^{2} \\sigma_{X}^{2} \\min \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\text {Lip }}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nwhich evaluates to\n\n$$\n\\epsilon_{0}=16 C_{U V}^{2}\\left\\|M^{*}\\right\\|_{*}^{2} \\min \\left\\{1, \\frac{1+\\sigma^{2}}{4 C_{U V}^{2}}\\right\\}\n$$\n\nFrom the corollary assumption we have that $C_{U V} \\leq 0.5 \\sqrt{1+\\sigma^{2}}$, which implies that\n\n$$\n\\epsilon_{0}=4\\left(1+\\sigma^{2}\\right)\\left\\|M^{*}\\right\\|_{*}^{2}\n$$\n\nEstimating $\\epsilon_{1}$ : Similarly, we evaluate\n\n$$\n\\epsilon_{1}=16 \\gamma^{2} \\sigma_{X}^{2} \\max \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{L_{0}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y|X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nobtaining\n\n$$\n\\epsilon_{1}=16 C_{U V}^{2}\\left\\|M^{*}\\right\\|_{*}^{2} \\max \\left\\{1, \\frac{1+\\sigma^{2}}{4 C_{U V}^{2}}\\right\\}\n$$\n\nFrom corollary assumption we have that $C_{U V} \\leq 0.5 \\sqrt{1+\\sigma^{2}}$ which gives\n\n$$\n\\epsilon_{1}=16 C_{U V}^{2}\\left\\|M^{*}\\right\\|_{*}^{2}\n$$\n\nDefining convex set $\\mathcal{C}$ : Consider a convex set $\\mathcal{C}=\\mathbb{B}(g)=\\left\\{X:\\|\\operatorname{vec}(X)\\|_{2} \\leq g\\right\\}$.\nFirst and foremost we need to estimate $\\delta_{\\mathcal{C}}$ for the following inequality to hold:\n\n$$\nP\\left(\\cap_{i=1}^{N} X_{i} \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}\n$$\n\nThe probability of $X \\in \\mathcal{C}=\\mathbb{B}(g)$ is equivalent to saying the probability of the event when $\\|\\operatorname{vec}(X)\\|_{2} \\leq g$. Since, $X_{i j} \\sim \\mathcal{N}(0,1 /(m \\times n))$ as a consequence of Bernstein's Inequality (Vershynin, 2018, Corollary 2.8.3) we have that for any $t \\geq 0$,\n\n$$\nP\\left(\\left\\|\\operatorname{vec}(X)\\right\\|_{2}-1 \\mid \\leq t\\right) \\geq 1-2 \\exp \\left(-c n_{X} t^{2}\\right)\n$$\n\nfor some constant $c \\geq 0$. Now we have\n\n$$\nP\\left(\\|\\operatorname{vec}(X)\\|_{2} \\leq g\\right) \\begin{cases}\\geq 1-2 \\exp \\left(-c n_{X}(g-1)^{2}\\right) & \\text { if } g \\geq 1 \\\\ \\leq 2 \\exp \\left(-c n_{X}(g-1)^{2}\\right) & \\text { otherwise }\\end{cases}\n$$\n\nWe consider the case where $g \\geq 1$, then we have that\n\n$$\nP\\left(\\cap_{i=1}^{N} X_{i} \\in \\mathcal{C}\\right)=P\\left(\\cap_{i=1}^{N}\\|\\operatorname{vec}(X)\\|_{2} \\leq g\\right) \\geq 1-\\underbrace{2 N \\exp \\underbrace{(-c n_{X}(g-1)^{2}}_{=\\delta_{\\mathcal{C}}}}_{=\\delta_{\\mathcal{C}}}\n$$\n\nWe have that $\\delta_{\\mathcal{C}}=2 N \\exp \\left(-c n_{X}(g-1)^{2}\\right)$.\nNow we evaluate $B_{\\ell}, B_{\\Phi}, \\tilde{L}_{\\Phi}, \\tilde{L}_{\\phi}$.\nEstimating $B_{\\Phi}$ : Recall that $r_{\\theta}=\\frac{1}{\\sqrt{2}}$. Then we have\n\n$$\n\\begin{aligned}\nB_{\\Phi} & =\\sup _{Z \\in \\mathcal{C},\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|\\left\\langle U V^{T}, Z\\right\\rangle\\right\\| \\\\\n& =\\sup _{Z \\in \\mathcal{C},\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|\\left\\langle\\operatorname{vec}\\left(U V^{T}\\right), \\operatorname{vec}(Z)\\right\\rangle\\right\\| \\\\\n& =g \\sup _{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|\\operatorname{vec}\\left(U V^{T}\\right)\\right\\|_{2} \\\\\n& =g \\sup _{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|\\sum_{j=1}^{R} \\operatorname{vec}\\left(\\mathbf{u}_{j} \\mathbf{v}_{j}^{T}\\right)\\right\\|_{2} \\\\\n& =g R \\sup _{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|\\operatorname{vec}\\left(\\mathbf{u}_{j} \\mathbf{v}_{j}^{T}\\right)\\right\\|_{2} \\\\\n& =g R \\sup _{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|\\mathbf{u}_{j} \\mathbf{v}_{j}^{T}\\right\\|_{F} \\\\\n& =g R \\sup _{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|\\mathbf{u}_{j}\\right\\|_{2}\\left\\|\\mathbf{v}_{j}\\right\\|_{2} \\\\\n& =g B_{u} B_{v} R .\n\\end{aligned}\n$$\n\nEstimating $B_{\\ell}$ : Similarly, we have\n\n$$\n\\begin{aligned}\nB_{\\ell} & =\\sup _{Z \\in \\mathcal{C},\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{sv}}}\\left\\|\\left\\langle U V^{T}-M^{*}, Z\\right\\rangle\\right\\| \\\\\n& =g \\sup _{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{sv}}}\\left\\|\\operatorname{vec}\\left(U V^{T}-M^{*}\\right)\\right\\|_{2} \\\\\n& \\leq g\\left[\\left\\|M^{*}\\right\\|_{F}+B_{u} B_{v} R\\right]\n\\end{aligned}\n$$\n\nEstimating $\\tilde{L}_{\\Phi}$ : Now, we compute the Lipschitz constant with respect to $U, V$. We have that\n\n$$\n\\begin{aligned}\n\\tilde{L}_{\\Phi} & =\\sup _{Z \\in \\mathcal{C},(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{sv}}} \\frac{\\left\\|\\left\\langle U V^{T}-U^{\\prime} V^{\\prime T}, Z\\right\\rangle\\right\\|}{\\max _{j} \\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& =g \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{sv}}} \\frac{\\left\\|U V^{T}-U^{\\prime} V^{\\prime T}\\right\\|_{F}}{\\max _{j} \\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& =g R \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{sv}}} \\frac{\\left\\|\\mathbf{u}_{j} \\mathbf{v}_{j}^{T}-\\mathbf{u}_{j}^{\\prime} \\mathbf{v}_{j}^{\\prime T}\\right\\|_{F}}{\\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& =g R \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{sv}}} \\frac{\\left\\|\\left(\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right) \\mathbf{v}_{j}^{T}-\\mathbf{u}_{j}^{\\prime}\\left(\\mathbf{v}_{j}^{\\prime}-\\mathbf{v}_{j}\\right)^{T}\\right\\|_{F}}{\\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& \\leq g R \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{sv}}} \\frac{\\left\\|\\left(\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right)\\right\\|_{2}\\left\\|\\mathbf{v}_{j}\\right\\|_{2}+\\left\\|\\mathbf{u}_{j}^{\\prime}\\right\\|_{2}\\left\\|\\left(\\mathbf{v}_{j}^{\\prime}-\\mathbf{v}_{j}\\right)\\right\\|_{2}}{\\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& \\leq g R \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{sv}}} \\sqrt{\\left\\|\\mathbf{v}_{j}\\right\\|_{2}^{2}+\\left\\|\\mathbf{u}_{j}^{\\prime}\\right\\|_{2}^{2}} \\\\\n& =g \\sqrt{B_{u}^{2}+B_{v}^{2}} R .\n\\end{aligned}\n$$\n\nEstimating $\\tilde{L}_{\\phi}$ : Similarly we get $\\tilde{L}_{\\phi}=g \\sqrt{B_{u}^{2}+B_{v}^{2}}$.\nEstimating $\\epsilon_{2}$ : Recall that\n\n$$\n\\epsilon_{2}=\\max \\left\\{8 B_{\\ell} \\tilde{L}_{\\Phi}, 8 \\tilde{L}_{\\Phi}\\left[B_{\\ell}+B_{\\Phi} L\\right], 32 \\Omega\\left(f_{\\mu}^{*}\\right) \\tilde{L}_{\\phi} \\max \\left\\{B_{\\ell}, L B_{\\Phi}\\right\\}, 4 \\tilde{L}_{\\Phi} B_{\\Phi}\\right\\}\n$$\n\nFrom all the constants computed earlier, we have that\n\n$$\n\\epsilon_{2}=k_{1} g^{2} R^{2}\\left(\\left\\|M^{*}\\right\\|_{F}+B_{u} B_{v}\\right)^{2}\n$$\n\nfor some constant $k_{1} \\geq 0$.\nNext we move on estimating $B(\\mathcal{C})$ we need to analyze three terms:\nThe First Term: We define the first term via\n\n$$\nT_{1}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathrm{sv}}}\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right|\n$$\n\nFor fixed $(U, V)$, we have\n\n$$\n\\begin{aligned}\n& \\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right| \\\\\n& =\\left|\\mathbb{E}\\left[\\left\\langle M^{*}-U V^{T}, \\mathcal{P}_{\\mathcal{C}}(X)\\right\\rangle^{2}-\\left\\langle M^{*}-U V^{T}, X\\right\\rangle^{2}\\right]\\right| \\\\\n& =\\left|\\mathbb{E}\\left[\\left\\langle\\operatorname{vec}\\left(M^{*}-U V^{T}\\right) \\operatorname{vec}\\left(M^{*}-U V^{T}\\right)^{T}, \\operatorname{vec}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right) \\operatorname{vec}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)^{T}\\right\\rangle\\right.\\right. \\\\\n& \\left.-\\left\\langle\\operatorname{vec}\\left(M^{*}-U V^{T}\\right) \\operatorname{vec}\\left(M^{*}-U V^{T}\\right)^{T}, \\operatorname{vec}(X) \\operatorname{vec}\\left(X\\right)^{T}\\right\\rangle\\right] \\mid \\\\\n& =\\left|\\left\\langle\\operatorname{vec}\\left(M^{*}-U V^{T}\\right) \\operatorname{vec}\\left(M^{*}-U V^{T}\\right)^{T}, \\mathbb{E}\\left[\\operatorname{vec}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right) \\operatorname{vec}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)^{T}-\\operatorname{vec}(X) \\operatorname{vec}\\left(X\\right)^{T}\\right]\\right\\rangle\\right|\n\\end{aligned}\n$$\n\nFrom Lemma 4, taking $g \\geq 1$,\n\n$$\n\\begin{aligned}\n& \\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right| \\\\\n& \\leq g e^{-g^{2} / 2}\\left\\|\\operatorname{vec}\\left(M^{*}-U V^{T}\\right) \\operatorname{vec}\\left(M^{*}-U V^{T}\\right)^{T}\\right\\|_{2}\n\\end{aligned}\n$$\n\nwhereupon further simplifying, we obtain\n\n$$\n\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right| \\leq g e^{-g^{2} / 2}\\left\\|M^{*}-U V^{T}\\right\\|_{F}^{2}\n$$\n\nNow, applying triangular inequality and taking the supremum, we obtain\n\n$$\nT_{1} \\leq g e^{-g^{2} / 2}\\left(\\left\\|M^{*}\\right\\|_{F}+R B_{u} B_{v}\\right)^{2}\n$$\n\nThe Second Term: We define the second term via\n\n$$\n\\begin{aligned}\nT_{2}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}, W^{\\prime} \\in \\mathcal{F}_{\\theta}} & \\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}\\right. \\\\\n& \\left.-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu} \\mid\n\\end{aligned}\n$$\n\nWe have\n\n$$\n\\begin{aligned}\n& \\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu}\\right| \\\\\n& \\quad=\\left|\\mathbb{E}\\left[\\left\\langle U V^{T}-M^{*}, \\mathcal{P}_{\\mathcal{C}}(X)\\right\\rangle\\left\\langle\\mathbf{u v}^{T}, \\mathcal{P}_{\\mathcal{C}}(X)\\right\\rangle-\\left\\langle U V^{T}-M^{*}, X\\right\\rangle\\left\\langle\\mathbf{u v}^{T}, X\\right\\rangle\\right]\\right| \\\\\n& \\quad=\\left|\\left\\langle\\operatorname{vec}\\left(M^{*}-U V^{T}\\right) \\operatorname{vec}\\left(\\mathbf{u v}^{T}\\right)^{T}, \\mathbb{E}\\left[\\operatorname{vec}(X) \\operatorname{vec}(X)^{T}-\\operatorname{vec}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right) \\operatorname{vec}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)^{T}\\right]\\right\\rangle\\right| .\n\\end{aligned}\n$$\n\nAs a consequence of Lemma 4 we have\n\n$$\n\\begin{aligned}\n& \\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu}\\right| \\\\\n& \\quad \\leq g e^{-g^{2} / 2}\\left\\|\\operatorname{vec}\\left(M^{*}-U V^{T}\\right) \\operatorname{vec}\\left(\\mathbf{u v}^{T}\\right)^{T}\\right\\|_{2}=g e^{-g^{2} / 2}\\left\\|M^{*}-U V^{T}\\right\\|_{F}\\left\\|\\mathbf{u v}^{T}\\right\\|_{F}\n\\end{aligned}\n$$\n\nNow we apply supremum over $(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}$ and then $(U, V)$ obtaining\n\n$$\nT_{2} \\leq g e^{-g^{2} / 2}\\left[\\left\\|M^{*}\\right\\|_{F}+R B_{u} B_{v}\\right]\n$$\n\nThe Third Term: We define\n\n$$\n\\begin{aligned}\nT_{3}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}} & \\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}\\right. \\\\\n& \\left.-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu} \\mid\n\\end{aligned}\n$$\n\nSimilarly to the earlier item, we rewrite the above as\n\n$$\n\\left|\\left\\langle\\operatorname{vec}\\left(M^{*}-U V^{T}\\right) \\operatorname{vec}\\left(U V^{T}\\right)^{T}, \\mathbb{E}\\left[\\operatorname{vec}(X) \\operatorname{vec}(X)^{T}-\\operatorname{vec}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right) \\operatorname{vec}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)^{T}\\right]\\right\\rangle\\right|\n$$\n\nAs a consequence of Lemma 4 we have\n\n$$\n\\begin{aligned}\n& \\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}\\right| \\\\\n& \\quad \\leq g e^{-g^{2} / 2}\\left\\|\\operatorname{vec}\\left(M^{*}-U V^{T}\\right) \\operatorname{vec}\\left(U V^{T}\\right)^{T}\\right\\|_{2}=g e^{-g^{2} / 2}\\left\\|M^{*}-U V^{T}\\right\\|_{F}\\left\\|U V^{T}\\right\\|_{F}\n\\end{aligned}\n$$\n\nFinally, we apply supremum over $(U, V) \\in \\mathcal{F}_{\\mathcal{W}}$, obtaining\n\n$$\nT_{3} \\leq g e^{-g^{2} / 2} B_{u} B_{v} R\\left[\\left\\|M^{*}\\right\\|_{F}+R B_{u} B_{v}\\right]\n$$\n\nNow combining equations (250), (253), (257) we obtain that\n\n$$\nB(\\mathcal{C}) \\leq g e^{-g^{2} / 2}\\left[\\alpha\\left(\\left\\|M^{*}\\right\\|_{F}+R B_{u} B_{v}\\right)^{2}+\\left\\|M^{*}\\right\\|_{F}+R B_{u} B_{v}+B_{u} B_{v} R\\left[\\left\\|M^{*}\\right\\|_{F}+R B_{u} B_{v}\\right]\\right]\n$$\n\nWe further upper bound for simplicity as\n\n$$\nB(\\mathcal{C}) \\leq 4 g e^{-g^{2} / 2}\\left(\\left\\|M^{*}\\right\\|_{F}+R B_{u} B_{v}\\right)^{2}\n$$\n\nFrom Theorem 4 we have that\n\n$$\n\\begin{aligned}\n\\frac{1}{n_{Y}}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\lesssim & \\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& +\\frac{4}{n_{Y}} g e^{-g^{2} / 2}\\left\\{\\left\\|M^{*}\\right\\|_{F}+R B_{u} B_{v}\\right\\}^{2}+16 C_{U V}^{2}\\left\\|M^{*}\\right\\|_{*}^{2} \\times\\binom{261}{\\sqrt{2}} \\log (N)+\\log (1 / \\delta) \\\\\n& N\n\\end{aligned}\n$$\n\nholds true w.p at least $1-\\delta-2 N \\exp \\left(-c n_{X}(g-1)^{2}\\right)$.\nNow choose\n\n$$\ng=1+\\mathcal{O}\\left(\\sqrt{\\log \\left(\\sqrt{N R^{100}}\\right)}+\\log (1 / \\delta)\\right)\n$$\n\nThen we get that\n\n$$\n\\begin{aligned}\n& \\frac{1}{n_{Y}}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\\\\n& \\quad \\lesssim \\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& \\quad+\\frac{4}{n_{Y}}\\left\\{\\left\\|M^{*}\\right\\|_{F}+R B_{u} B_{v}\\right\\}^{2} \\frac{\\delta \\sqrt{\\log \\left(N R^{100}\\right)+\\log (1 / \\delta)}}{N R^{100}} \\\\\n& \\quad+16 C_{U V}^{2}\\left\\|M^{*}\\right\\|_{*}^{2} \\sqrt{\\frac{R(m+n) \\log \\left(C_{U V}{ }_{\\tilde{Y}}^{2} M^{*}\\left\\|_{*} k_{1}\\right\\| \\log (N R)+\\log (1 / \\delta)\\right\\| R^{2}\\left(\\left\\|M^{*}\\right\\|_{F}+B_{u} B_{v}\\right)^{2} \\frac{1}{\\sqrt{2}}}\\right) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{aligned}\n$$\n\nholds true w.p at least $1-\\delta$. Now ignoring loglog terms and keeping the right most term because of the dominance, we obtain\n\n$$\n\\begin{aligned}\n& \\frac{1}{n_{Y}}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\lesssim \\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& \\quad+C_{U V}^{2}\\left\\|M^{*}\\right\\|_{*}^{2} \\sqrt{\\frac{R \\log \\left(R\\left(C_{U V}+B_{u} B_{v}\\right)\\right)(m+n) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{aligned}\n$$\n\nholds true w.p at least $1-\\delta$.", "tables": {}, "images": {}}, {"section_id": 23, "text": "# C. 2 Structured Matrix Sensing \n\nNext, we move on to a slightly more generalized matrix sensing problem through which we impose certain structure in the factor $U$. Consider an atomic set $\\mathcal{U}$ that represents the set of structured columns, and suppose that $U$ consists of columns that are affine combinations of the atoms in $\\mathcal{U}$. We consider a gauge function $\\gamma_{\\mathcal{U}}(\\cdot)$ which is defined via\n\n$$\n\\gamma_{\\mathcal{U}}(\\mathbf{u}):=\\inf \\left\\{t, t \\geq 0 \\text { such that } \\mathbf{u} \\in \\operatorname{tconv}(\\mathcal{U})\\right\\}\n$$\n\nFor instance, $\\mathcal{U}$ can be the intersection of $L_{2}$ unit ball and $L_{1}$ unit ball, which induces $U V^{T}$ to be low-rank and $U$ to be sparse. Imposing such structures has been well studied for convex problems by Chandrasekaran et al. (2012). Bach (2013) analyzed such structures for non-convex matrix factorization problems. However, their work was focused primarily on the optimization guarantees whereas our result below provides generalization/recovery guarantees for structured matrix sensing problems. We have the following corollary.\n\nCorollary 5 (Structured matrix sensing). Consider the true model for $(X, y)$, where $X \\in \\mathbb{R}^{m \\times n}$ is a random matrix with i.i.d. entries $X_{l k} \\sim \\mathcal{N}\\left(0, \\frac{1}{m n}\\right)$ and $y=\\left\\langle U^{*} V^{* T}, X\\right\\rangle+\\epsilon$, where $U^{*} \\in \\mathbb{R}^{m \\times R^{*}}, V^{*} \\in \\mathbb{R}^{n \\times R^{*}}$ and $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)$ is independent from $X$. For all $i \\in[N]$, let $\\left(X_{i}, y_{i}\\right)$ be i.i.d. samples from this true model. Consider the estimator $\\hat{y}=\\left\\langle U V^{T}, X\\right\\rangle$, where $U \\in \\mathbb{R}^{m \\times R}$ and $V \\in \\mathbb{R}^{n \\times R}$. Let $\\delta \\in(0,1]$ be fixed. Define the non-convex problem with the atomic set, $\\mathcal{U}$\n\n$$\n\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{SMS}}((U, V)):=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left(y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right)^{2}+\\lambda \\sum_{j=1}^{R} \\gamma_{\\mathcal{U}}\\left(\\mathbf{u}_{j}\\right)\\left\\|\\mathbf{v}_{j}\\right\\|_{2}\n$$\n\nand define $\\mathbb{N C}_{\\mu}^{S M S}((U, V))$ similarly with the sum over $i$ replaced by expectation taken over $(X, y)$. Here $\\gamma_{\\mathcal{U}}(\\mathbf{u}):=$ $\\inf \\left\\{t ; t \\geq 0, \\mathbf{u} \\in \\operatorname{tconv}(\\mathcal{U})\\right\\}$ for some specified atomic set, $\\mathcal{U}$. Define\n\n$$\nK_{1}:=\\sum_{j=1}^{r^{*}} \\gamma_{\\mathcal{U}}\\left(\\mathbf{u}_{j}^{*}\\right)\\left\\|\\mathbf{v}_{j}^{*}\\right\\|_{2} ; K_{2}:=\\sup _{\\|\\mathbf{u}\\| \\leq 1} \\gamma_{\\mathcal{U}}(\\mathbf{u})\n$$\n\nLet $(U, V)$ be a stationary point of $\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{SMS}}((U, V))$. Suppose there exists $C_{U V}, B_{u}, B_{v}>0$ such that $\\left\\|U V^{T}\\right\\|_{2} \\leq$ $C_{U V} K_{1}$, and for all $j \\in[R],\\left\\|\\mathbf{u}_{j}\\right\\|_{2} \\leq B_{u},\\left\\|\\mathbf{v}_{j}\\right\\|_{2} \\leq B_{v}$. Then, with probability at least $1-\\delta$, it holds that\n\n$$\n\\begin{aligned}\n& \\left|\\mathbb{N C}_{\\mu}^{\\mathrm{SMS}}((U, V))\\right)-\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{SMS}}((U, V))\\left| \\lesssim K_{1}\\left[K_{2}\\left\\|\\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right) X_{i}\\right\\|_{2}-\\lambda\\right]\\right. \\\\\n& \\quad+C_{U V}^{2} K_{1}^{2} \\sqrt{\\frac{R \\log \\left(R\\left(C_{U V}+B_{u} B_{v}\\right)\\right)(m+n) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{aligned}\n$$\n\nRemarks: Similar to matrix sensing, the sample complexity required for consistency is only that $N \\gtrsim R(m+n)$ up to logarithmic terms, assuming a global minimum is found. The sample complexity is similar to low-rank matrix sensing (ignoring the scale and logarithmic dependency). To the best of our knowledge, this problem has not been studied from a statistical perspective, and our sample complexities match the corresponding convex slightly structured matrix sensing of Kakade et al. (2008). Unlike low-rank matrix sensing, the main technical challenge is to compute the polar/supremum term in the optimization error. In general, such a computation is NP-hard when the atomic set $\\mathcal{U}$ has non-negative atoms (Hendrickx and Olshevsky, 2010).\n\nProof. The proof is similar to that of Corollary 1, except for the computation of the polar. Therefore, we only compute the polar.\nEstimating $\\Omega\\left(f_{\\mu}^{*}\\right)$ : Since $M^{*}$ is the true matrix the globally optimal solution would be $M^{*}$; therefore, from Proposition 2 we have\n\n$$\n\\Omega\\left(f_{\\mu}^{*}\\right) \\leq\\left(\\sum_{j=1}^{r^{*}} \\gamma_{\\mathcal{U}}\\left(\\mathbf{u}_{j}^{*}\\right)\\left\\|\\mathbf{v}_{j}^{*}\\right\\|_{2}\\right)\n$$\n\nEstimating $\\Omega_{\\mu_{N}}^{\\circ}(\\cdot)$ : Now we move on to compute the polar.\n\n$$\n\\begin{aligned}\n\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) & =\\Omega_{\\mu_{N}}^{\\circ}\\left(\\frac{1}{\\lambda}\\left(g-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) \\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) \\\\\n& =\\Omega_{\\mu_{N}}^{\\circ}\\left(\\frac{1}{\\lambda}\\left(g-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) \\\\\n& =\\sup _{\\gamma_{\\mathcal{U}}(\\mathbf{u}) \\leq 1 ;\\|\\mathbf{v}\\| \\leq 1} \\frac{1}{N \\lambda} \\sum_{i=1}^{N}\\left\\langle Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle, \\mathbf{u}^{T} X_{i} \\mathbf{v}\\right\\rangle \\\\\n& =\\sup _{\\gamma_{\\mathcal{U}}(\\mathbf{u}) \\leq 1 ;\\|\\mathbf{v}\\| \\leq 1} \\frac{1}{N \\lambda}\\left\\langle\\mathbf{v}, \\sum_{i=1}^{N}\\left(Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right)^{T} \\mathbf{u}^{T} X_{i}\\right\\rangle \\\\\n& =\\sup _{\\gamma_{\\mathcal{U}}(\\mathbf{u}) \\leq 1} \\frac{1}{N \\lambda}\\left\\|\\sum_{i=1}^{N}\\left(Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right) \\mathbf{u}^{T} X_{i}\\right\\| \\\\\n& =\\sup _{\\gamma_{\\mathcal{U}}(\\mathbf{u}) \\leq 1} \\frac{1}{N \\lambda}\\left\\|\\sum_{i=1}^{N}\\left(Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right) X_{i}^{T} \\mathbf{u}\\right\\|\n\\end{aligned}\n$$\n\nThis yields\n\n$$\n\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) \\leq\\left[\\sup _{\\|\\mathbf{u}\\| \\leq 1} \\gamma_{\\mathcal{U}}(\\mathbf{u})\\right] \\frac{1}{N \\lambda}\\left\\|\\sum_{i=1}^{N}\\left(Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right) X_{i}^{T}\\right\\|_{2}\n$$\n\nThe rest of the proof is the same as that of low-rank matrix sensing (see section C.1).", "tables": {}, "images": {}}, {"section_id": 24, "text": "# C. 3 Two-Layer Linear NN \n\nNext, we consider the closely related problem of 2-Layer Linear Neural Networks, which is essentially a multidimensional matrix sensing problem; this is also referred to as non-convex linear regression. In practice, this approach has seemed to have better linear convergence (Arora et al., 2019) and generalization capabilities (AllenZhu et al., 2019) than vanilla linear regression. Corollary 6 provides generalization error upper bounds.\nCorollary 6 (2-Layer Linear Neural Network). Consider the true model for $(\\mathbf{x}, \\mathbf{y})$, where $\\mathbf{x} \\sim \\mathcal{N}\\left(0,(1 / n) I_{n}\\right) \\in$ $\\mathbb{R}^{n}, \\mathbf{y}=U^{*} V^{* T} \\mathbf{x}+\\epsilon$, where $U^{*} \\in \\mathbb{R}^{m \\times R^{*}}, V^{*} \\in \\mathbb{R}^{n \\times R^{*}}$, and $\\epsilon \\sim \\mathcal{N}\\left(0,\\left(\\sigma^{2} / m\\right) I_{m}\\right) \\in \\mathbb{R}^{m}$ independent from $\\mathbf{x}$. For all $i \\in[N]$, let $\\left(\\mathbf{x}_{i}, \\mathbf{y}_{i}\\right)$ be i.i.d. samples from this true model. Consider the estimator $\\hat{\\mathbf{y}}=U V^{T} \\mathbf{x}$, where $U \\in \\mathbb{R}^{m \\times R}, V \\in \\mathbb{R}^{n \\times R}$. Let $\\delta \\in(0,1]$ be fixed. Define the non-convex problem\n\n$$\n\\mathbf{N C}_{\\mu_{N}}^{\\mathbf{2 L N N}}((U, V)):=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-U\\left[V^{T} \\mathbf{x}_{i}\\right]_{+}\\right\\|_{2}^{2}+\\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right)\n$$\n\nand define $\\mathbf{N C}_{\\mu}^{\\mathbf{2 L N N}}((U, V))$ similarly with the sum over $i$ replaced by expectation taken over $(\\mathbf{x}, \\mathbf{y})$.\nLet $(U, V)$ be a stationary point of $\\mathrm{NC}_{\\mu_{N}}^{\\mathbf{2 L N N}}((U, V))$. Suppose there exists $C_{U V}, B_{u}, B_{v}>0$ such that $\\left\\|U V^{T}\\right\\|_{2} \\leq$ $C_{U V}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]$, and for all $j \\in[R],\\left\\|\\mathbf{u}_{j}\\right\\|_{2} \\leq B_{u},\\left\\|\\mathbf{v}_{j}\\right\\|_{2} \\leq B_{v}$. Then, with probability at least $1-\\delta$, it holds that\n\n$$\n\\begin{gathered}\n\\frac{1}{m}\\left|\\mathbf{N C}_{\\mu}^{\\mathbf{2 L N N}}((U, V))-\\mathbf{N C}_{\\mu_{N}}^{\\mathbf{2 L N N}}((U, V))\\right| \\lesssim \\frac{1}{2 m}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\\left[\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-\\hat{\\mathbf{y}}_{i}\\right\\|_{2}\\left\\|\\mathbf{x}_{i}\\right\\|_{2}-\\lambda\\right] \\\\\nC_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]^{2} \\sqrt{\\frac{R \\log \\left(R\\left(C_{U V}+B_{u}^{2}+B_{v}^{2}\\right)\\right)(m+n) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{gathered}\n$$\n\nSimilar to matrix sensing, we require that $N \\gtrsim R(m+n)$, with $\\frac{R(m+n)}{N} \\rightarrow 0$ for consistency at a global minimum. This matches classical results for (convex) linear regression.\n\nProof. To obtain a generalization bound from Theorem 4 for this setting, we set the following problem parameters:\n\n$$\n\\ell(Y, \\hat{Y})=\\frac{1}{2}\\|Y-\\hat{Y}\\| \\Longrightarrow(\\alpha, L)=(0,1)\n$$\n\n$$\n\\begin{gathered}\n\\phi(W)=\\langle\\mathbf{v}, \\mathbf{x}\\rangle \\mathbf{u} \\\\\n\\theta(W)=\\frac{1}{2}\\left[\\|\\mathbf{u}\\|_{2}^{2}+\\|\\mathbf{v}\\|_{2}^{2}\\right]\n\\end{gathered}\n$$\n\nEstimating $\\Omega\\left(f_{\\mu}^{*}\\right)$ : From Proposition 2 we have that\n\n$$\n\\Omega\\left(f_{\\mu}^{*}\\right) \\leq \\frac{\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}}{2}\n$$\n\nChoosing $\\mathcal{F}_{\\theta}$ :\n\n$$\n\\mathcal{F}_{\\theta}:=\\left\\{(\\mathbf{u}, \\mathbf{v}):\\|\\mathbf{u}\\|^{2}+\\|\\mathbf{v}\\|^{2} \\leq 2,\\|\\mathbf{u}\\|_{2} \\leq 1,\\|\\mathbf{v}\\|_{2} \\leq 1\\right\\}\n$$\n\nEstimating $L_{\\phi}$ :\n\n$$\nL_{\\phi}=\\sup _{(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}}\\left\\|\\mathbf{u v}^{T}(\\cdot)\\right\\|_{\\mathrm{Lip}}=\\sup _{(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}}\\left\\|\\mathbf{u v}^{T}\\right\\|_{2}=1\n$$\n\nEstimating $r_{\\theta}$ : For any $(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}$, we have that,\n\n$$\n\\frac{\\|\\mathbf{u}\\|^{2}+\\|\\mathbf{v}\\|^{2}}{2} \\leq \\sqrt{\\frac{\\|\\mathbf{u}\\|^{2}+\\|\\mathbf{v}\\|^{2}}{2}} \\Longrightarrow \\mathcal{F}_{\\theta} \\subseteq \\mathbb{B}(1 / \\sqrt{2})\n$$\n\nThen we have $r_{\\theta}=1 / \\sqrt{2}$.\nChoosing $\\mathcal{F}_{\\mathcal{W}}$ :\n\n$$\n\\mathcal{F}_{\\mathcal{W}}:=\\left\\{(U, V):\\left\\|U V^{T}\\right\\|_{2} \\leq \\gamma,\\left\\|\\mathbf{u}_{j}\\right\\| \\leq B_{u},\\left\\|\\mathbf{v}_{j}\\right\\| \\leq B_{v}\\right\\}\n$$\n\nAs $\\gamma \\geq \\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}=\\frac{\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}}{2}$, we may take $\\gamma=C_{U V}\\left[\\frac{\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}}{2}\\right]$ for some $C_{U V}$. We have that\n\n$$\n\\mathcal{F}_{\\mathcal{W}}=\\left\\{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\}:\\left\\|\\left\\langle U V^{T}, \\cdot\\right\\rangle\\right\\|_{\\mathrm{Lip}}=\\left\\|U V^{T}\\right\\|_{2} \\leq C_{U V}\\left[\\frac{\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}}{2}\\right],\\left\\|\\mathbf{u}_{j}\\right\\| \\leq B_{u},\\left\\|\\mathbf{v}_{j}\\right\\| \\leq B_{v}\\right\\}\n$$\n\nEstimating $\\epsilon_{0}$ : From the data generating mechanism we have $\\|g\\|_{\\text {Lip }}=\\left\\|M^{*}\\right\\|_{2}, \\sigma_{X}=1, \\sigma_{Y \\mid X}=\\sigma$, which yields the following constants from Theorem 4 :\n\n$$\n\\epsilon_{0}=16 \\gamma^{2} \\sigma_{X}^{2} \\min \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\mathrm{Lip}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nwhich evaluates to when $C_{U V} \\leq 0.5 \\sqrt{\\left(1+\\sigma^{2}\\right)}$\n\n$$\n\\epsilon_{0}=8 C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right] \\min \\left\\{1, \\frac{1+\\sigma^{2}}{4 C_{U V}^{2}}\\right\\}=2\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\n$$\n\nEstimating $\\epsilon_{1}$ : Similarly, we evaluate\n\n$$\n\\epsilon_{1}=16 \\gamma^{2} \\sigma_{X}^{2} \\max \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\mathrm{Lip}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nobtaining\n\n$$\n\\epsilon_{1}=8 C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\n$$\n\nChoosing the convex set $\\mathcal{C}$ : Consider a convex set $\\mathcal{C}=\\mathbb{B}(g)=\\left\\{X:\\|\\operatorname{vec}(X)\\|_{2} \\leq g\\right\\}$.\nFirst and foremost we need to estimate $\\delta_{\\mathcal{C}}$ for the following inequality to hold:\n\n$$\nP\\left(\\cap_{i=1}^{N} X_{i} \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}\n$$\n\nThe probability of $\\mathbf{x} \\in \\mathcal{C}=\\mathbb{B}(g)$ is equivalent to saying the probability of the event when $\\|\\mathbf{x}\\|_{2} \\leq g$. Since, $x_{i} \\sim \\mathcal{N}(0,1 / n)$ as a consequence of Bernstein's Inequality (Vershynin, 2018, Corollary 2.8.3) we have that for any $t \\geq 0$,\n\n$$\nP\\left(\\left\\|\\mid \\mathbf{x}\\right\\|_{2}-1 \\mid \\leq t\\right) \\geq 1-2 \\exp \\left(-c n_{X} t^{2}\\right)\n$$\n\nfor some constant $c \\geq 0$. Now we have\n\n$$\nP\\left(\\|\\mathbf{x}\\|_{2} \\leq g\\right) \\begin{cases}\\geq 1-2 \\exp \\left(-c n_{X}(g-1)^{2}\\right) & \\text { if } g \\geq 1 \\\\ \\leq 2 \\exp \\left(-c n_{X}(g-1)^{2}\\right) & \\text { otherwise }\\end{cases}\n$$\n\nWe consider the case where $g \\geq 1$, then we have that\n\n$$\nP\\left(\\cap_{i=1}^{N} X_{i} \\in \\mathcal{C}\\right)=P\\left(\\cap_{i=1}^{N}\\|\\mathbf{x}\\|_{2} \\leq g\\right) \\geq 1-\\underbrace{2 N \\exp \\left(-c n_{X}(g-1)^{2}\\right)}_{=\\delta_{\\mathcal{C}}}\n$$\n\nWe have that $\\delta_{\\mathcal{C}}=2 N \\exp \\left(-c n(g-1)^{2}\\right)$.\nNow we evaluate $B_{\\ell}, B_{\\Phi}, \\tilde{L}_{\\Phi}, \\tilde{L}_{\\phi}$.\nEstimating $B_{\\Phi}$ : We have\n\n$$\n\\begin{aligned}\nB_{\\Phi} & =\\sup _{\\mathbf{z} \\in \\mathcal{C},\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{iv}}}\\left\\|U V^{T} \\mathbf{z}\\right\\| \\\\\n& =g \\sup _{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{iv}}}\\left\\|U V^{T}\\right\\|_{2} \\\\\n& =g \\gamma\n\\end{aligned}\n$$\n\nEstimating $B_{\\ell}$ : Similarly, we have\n\n$$\n\\begin{aligned}\nB_{\\ell} & =\\sup _{\\mathbf{z} \\in \\mathcal{C},\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{iv}}}\\left\\|\\left(U V^{T}-U^{*} V^{* T}\\right) \\mathbf{z}\\right\\| \\\\\n& =g \\sup _{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{iv}}}\\left\\|U V^{T}-U^{*} V^{* T}\\right\\|_{2} \\\\\n& =g\\left(\\left\\|U^{*} V^{* T}\\right\\|_{2}+\\gamma\\right)\n\\end{aligned}\n$$\n\nEstimating $\\tilde{L}_{\\Phi}$ : Now, we compute the Lipschitz constant with respect to $U, V$. We have\n\n$$\n\\begin{aligned}\n\\tilde{L}_{\\Phi} & =\\sup _{\\mathbf{z} \\in \\mathcal{C},(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{iv}}} \\frac{\\left\\|\\left(U V^{T}-U^{\\prime} V^{\\prime T}\\right) \\mathbf{z}\\right\\|}{\\max _{j} \\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& =g \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{iv}}} \\frac{\\left\\|U V^{T}-U^{\\prime} V^{\\prime T}\\right\\|_{2}}{\\max _{j} \\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& \\leq g \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{iv}}} \\frac{\\left\\|U V^{T}-U^{\\prime} V^{\\prime T}\\right\\|_{F}}{\\max _{j} \\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& =g \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{iv}}} \\frac{\\left\\|\\sum_{j=1}^{R} \\mathbf{u}_{j} \\mathbf{v}_{j}^{T}-\\mathbf{u}_{j}^{\\prime} \\mathbf{v}_{j}^{\\prime T}\\right\\|_{F}}{\\max _{j} \\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& =g R \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{iv}}} \\frac{\\left\\|\\mathbf{u}_{j} \\mathbf{v}_{j}^{T}-\\mathbf{u}_{j}^{\\prime} \\mathbf{v}_{j}^{\\prime T}\\right\\|_{F}}{\\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& =g R \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{iv}}} \\frac{\\left\\|\\left(\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right) \\mathbf{v}_{j}^{T}-\\mathbf{u}_{j}^{\\prime}\\left(\\mathbf{v}_{j}^{\\prime}-\\mathbf{v}_{j}\\right)^{T}\\right\\|_{2}}{\\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& \\leq g R \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{iv}}} \\frac{\\left\\|\\left(\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right)\\right\\|_{2}\\left\\|\\mathbf{v}_{j}\\right\\|_{2}+\\left\\|\\mathbf{u}_{j}^{\\prime}\\right\\|_{2}\\left\\|\\left(\\mathbf{v}_{j}^{\\prime}-\\mathbf{v}_{j}\\right)\\right\\|_{2}}{\\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& =g R \\sup _{(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{\\mathrm{iv}}} \\sqrt{\\left\\|\\mathbf{v}_{j}\\right\\|_{2}^{2}+\\left\\|\\mathbf{u}_{j}^{\\prime}\\right\\|_{2}^{2}} \\\\\n& =g \\sqrt{B_{n}^{2}+B_{c}^{2}} R\n\\end{aligned}\n$$\n\nEstimating $\\tilde{L}_{\\phi}$ : Similarly we get $\\tilde{L}_{\\phi}=g \\sqrt{B_{a}^{2}+B_{c}^{2}}$.\nEstimating $\\epsilon_{2}$ : Recall that\n\n$$\n\\epsilon_{2}=\\max \\left\\{8 B_{\\ell} \\tilde{L}_{\\Phi}, 8 \\tilde{L}_{\\Phi}\\left[B_{\\ell}+B_{\\Phi} L\\right], 32 \\Omega\\left(f_{\\mu}^{*}\\right) \\tilde{L}_{\\phi} \\max \\left\\{B_{\\ell}, L B_{\\Phi}\\right\\}, 4 \\tilde{L}_{\\Phi} B_{\\Phi}\\right\\}\n$$\n\nFrom all the constants computed earlier, we have that\n\n$$\n\\epsilon_{2}=k_{1} g^{2} R^{2} C_{U V}^{2}\\left(\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right) \\sqrt{B_{a}^{2}+B_{a}^{2}}\n$$\n\nfor some constant $k_{1} \\geq 0$.\nNext, we move on to estimating $B(\\mathcal{C})$. We need to analyze three terms:\nThe First Term: Define\n\n$$\nT_{1}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right|\n$$\n\nWe have\n\n$$\n\\begin{gathered}\n\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2} \\mid= \\\\\n=\\left|\\mathbb{E}\\left[\\left\\|\\left(U^{*} V^{* T}-U V^{T}\\right) \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right\\|^{2}-\\left\\|\\left(U^{*} V^{* T}-U V^{T}\\right) \\mathbf{x}\\right\\|^{2}\\right]\\right| \\\\\n=\\left|\\mathbb{E}\\left[\\left\\langle\\left(U^{*} V^{* T}-U V^{T}\\right)\\left(U^{*} V^{* T}-U V^{T}\\right)^{T},\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right)\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right)^{T}\\right\\rangle\\right.\\right. \\\\\n\\left.-\\left\\langle\\left(U^{*} V^{* T}-U V^{T}\\right)\\left(U^{*} V^{* T}-U V^{T}\\right)^{T}, \\mathbf{x x}^{T}\\right\\rangle\\right] \\mid \\\\\n=\\left|\\left\\langle\\left(U^{*} V^{* T}-U V^{T}\\right)\\left(U^{*} V^{* T}-U V^{T}\\right)^{T}, \\mathbb{E}\\left[\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right)\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right)^{T}-\\mathbf{x x}^{T}\\right]\\right\\rangle\\right|\n\\end{gathered}\n$$\n\nFrom Lemma 4 we obtain that (taking $g \\geq 1$ )\n\n$$\n\\begin{aligned}\n& \\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right| \\\\\n& \\leq g e^{-g^{2} / 2}\\left\\|\\left(U^{*} V^{* T}-U V^{T}\\right)\\left(U^{*} V^{* T}-U V^{T}\\right)^{T}\\right\\|_{2}\n\\end{aligned}\n$$\n\non further simplifying, we get\n\n$$\n\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right| \\leq g e^{-g^{2} / 2}\\left\\|U^{*} V^{* T}-U V^{T}\\right\\|_{2}^{2}\n$$\n\nNow applying triangular inequality and taking the supremum, we obtain\n\n$$\nT_{1} \\leq g e^{-g^{2} / 2}\\left(\\left\\|U^{*} V^{* T}\\right\\|_{2}+\\gamma\\right)^{2}\n$$\n\nThe Second Term: Define\n\n$$\n\\begin{aligned}\nT_{2}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}, W^{\\prime} \\in \\mathcal{F}_{\\mathrm{II}}} & \\left|\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}\\right. \\\\\n& \\left.-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu} \\mid\n\\end{aligned}\n$$\n\nWe have\n\n$$\n\\begin{gathered}\n\\left|\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu}\\right|= \\\\\n\\left|\\mathbb{E}\\left[\\left\\langle\\left(U V^{T}-U^{*} V^{* T}\\right) \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x}), \\mathbf{u v}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right\\rangle-\\left\\langle\\left(U V^{T}-U^{*} V^{* T}\\right) \\mathbf{x}, \\mathbf{u v}^{T} \\mathbf{x}\\right)\\right]\\right|\n\\end{gathered}\n$$\n\nis the same as\n\n$$\n=\\left|\\left\\langle\\left(U^{*} V^{* T}-U V^{T}\\right)\\left(\\mathbf{u v}^{T}\\right)^{T}, \\mathbb{E}\\left[\\mathbf{x x}^{T}-\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right)\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right)^{T}\\right]\\right\\rangle \\mid\n$$\n\nAs a consequence of Lemma 4 we have\n\n$$\n\\left|\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu}\\right|\n$$\n\n$$\n\\leq g e^{-g^{2} / 2}\\left\\|\\left(U^{*} V^{* T}-U V^{T}\\right)\\left(\\mathbf{u v}^{T}\\right)^{T}\\right\\|_{2}=g e^{-g^{2} / 2}\\left\\|U^{*} V^{* T}-U V^{T}\\right\\|_{F}\\left\\|\\mathbf{u v}^{T}\\right\\|_{F}\n$$\n\nNow we apply supremum over $(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}$, and then over $(U, V) \\in \\mathcal{F}_{\\mathcal{W}}$, yielding\n\n$$\nT_{2} \\leq g e^{-g^{2} / 2}\\left[\\left\\|U^{*} V^{* T}\\right\\|_{2}+\\gamma\\right]\n$$\n\nThe Third Term: Define\n\n$$\n\\begin{aligned}\nT_{3}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}} & \\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}\\right. \\\\\n& \\left.-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}\\right|\n\\end{aligned}\n$$\n\nSimilarly to the earlier item, we rewrite the above as\n\n$$\n=\\left|\\left\\langle\\left(U^{*} V^{* T}-U V^{T}\\right)\\left(U V^{T}\\right)^{T}, \\mathbb{E}\\left[\\mathbf{x} \\mathbf{x}^{T}-\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right)\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right)^{T}\\right]\\right\\rangle\\right|\n$$\n\nAs a consequence of Lemma 4 we have\n\n$$\n\\begin{gathered}\n\\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}\\right| \\\\\n\\leq g e^{-g^{2} / 2}\\left\\|\\left(U^{*} V^{* T}-U V^{T}\\right)\\left(U V^{T}\\right)^{T}\\right\\|_{2} \\leq g e^{-g^{2} / 2}\\left\\|U^{*} V^{* T}-U V^{T}\\right\\|_{2}\\left\\|U V^{T}\\right\\|_{2}\n\\end{gathered}\n$$\n\nFinally, we apply supremum over $(U, V) \\in \\mathcal{F}_{\\mathcal{W}}$, obtaining\n\n$$\nT_{3} \\leq g e^{-g^{2} / 2} \\gamma\\left[\\left\\|U^{*} V^{* T}\\right\\|_{2}+\\gamma\\right]\n$$\n\nNow combining equations (316), (321), (325) we obtain that\n\n$$\nB(\\mathcal{C}) \\leq g e^{-g^{2} / 2}\\left[\\alpha\\left(\\left\\|U^{*} V^{* T}\\right\\|_{2}+\\gamma\\right)^{2}+\\left\\|U^{*} V^{* T}\\right\\|_{2}+\\gamma+\\gamma\\left[\\left\\|U^{*} V^{* T}\\right\\|_{2}+\\gamma\\right]\\right]\n$$\n\nWe further upper bound for simplicity as\n\n$$\nB(\\mathcal{C}) \\leq g e^{-g^{2} / 2}(1+\\gamma)\\left(\\left\\|U^{*} V^{* T}\\right\\|_{2}+\\gamma\\right)\n$$\n\nFrom Theorem 4 we have that\n\n$$\n\\begin{gathered}\n\\frac{1}{n_{Y}}\\left|\\mathbb{N C}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathbb{N C}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\lesssim \\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n+\\frac{4}{n_{Y}} g e^{-g^{2} / 2}(1+\\gamma)\\left(\\left\\|U^{*} V^{* T}\\right\\|_{2}+\\gamma\\right)+8 C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]^{2} \\times(\n\\end{gathered}\n$$\n\nholds true w.p at least $1-\\delta-2 N \\exp \\left(-c n_{X}(g-1)^{2}\\right)$.\nNow choose\n\n$$\ng=1+\\mathcal{O}(\\sqrt{\\log (N)+\\log (1 / \\delta)})\n$$\n\nNow ignoring loglog terms and keep the right most term because of the dominance,\n\n$$\n\\begin{aligned}\n& \\frac{1}{n_{Y}}\\left|\\mathbb{N C}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathbb{N C}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\lesssim \\frac{\\lambda}{n_{Y}} \\Omega\\left(f_{\\mu}^{*}\\right)\\left[\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right)-1\\right] \\\\\n& +C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]^{2} \\sqrt{\\frac{R \\log \\left(R\\left(C_{U V}+B_{u}^{2}+B_{v}^{2}\\right)\\right)(m+n) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{aligned}\n$$\n\nholds true w.p at least $1-\\delta$.", "tables": {}, "images": {}}, {"section_id": 25, "text": "# C. 4 Two-Layer ReLU NN \n\nNext, we present and prove the generalization bound for the two-layer ReLU neural network. This is one step ahead of all the linear models that were discussed earlier. Similarly to the Gaussian projections discussed in matrix sensing, we discuss ReLU projection results that will be used in the main proof.\nLemma 5 (ReLU projection 1). Consider $U_{1}, U_{2} \\in \\mathbb{R}^{m \\times r}, V_{1}, V_{2} \\in \\mathbb{R}^{n \\times r}$. Denote, convex set $\\mathcal{C}=\\mathbb{B}(g)$ that is $g$-radius hyper sphere, then we have that\n\n$$\n\\begin{aligned}\n\\mid \\mathbb{E}\\left[\\left\\|U_{1}\\left[V_{1}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\right.\\right. & \\left.-U_{2}\\left[V_{2}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\left\\|^{2}-\\right\\| U_{1}\\left[V_{1}^{T} \\mathbf{x}\\right]_{+}-U_{2}\\left[V_{2}^{T} \\mathbf{x}\\right]_{+}\\left\\|^{2}\\right]\\right\\| \\\\\n& \\leq 2 g e^{-g^{2} / 2}\\left[\\left\\|U_{1}\\right\\|_{F}^{2}\\left\\|V_{1}\\right\\|_{F}^{2}+\\left\\|U_{2}\\right\\|_{F}^{2}\\left\\|V_{2}\\right\\|_{F}^{2}\\right]\n\\end{aligned}\n$$\n\nProof. First, we re-write\n\n$$\n\\begin{aligned}\n\\left\\|U_{1}\\left[V_{1}^{T} \\mathbf{x}\\right]_{+}-U_{2}\\left[V_{2}^{T} \\mathbf{x}\\right]_{+}\\right\\|^{2}= & \\left\\|\\sum_{j=1}^{r} \\mathbf{u}_{j 1}\\left[\\mathbf{v}_{j 1}^{T} \\mathbf{x}\\right]_{+}-\\mathbf{u}_{j 2}\\left[\\mathbf{v}_{j 2}^{T} \\mathbf{x}\\right]_{+}\\right\\|^{2} \\\\\n= & \\left\\|\\sum_{j=1}^{r} \\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T} \\mathbf{x} \\mathbf{1}_{\\mathbf{v}_{j 1}^{T} \\mathbf{x} \\geq 0}-\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T} \\mathbf{x} \\mathbf{1}_{\\mathbf{v}_{j 1}^{T} \\mathbf{x} \\geq 0}\\right\\|^{2} \\\\\n= & \\sum_{j=1}^{r} \\sum_{l=1}^{r}\\left[\\left\\langle\\left(\\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T}\\right)^{T}\\left(\\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T}\\right), \\mathbf{x x}^{T} \\mathbf{1}_{\\mathbf{v}_{j 1}^{T} \\mathbf{x} \\geq 0}\\right\\rangle+\\left\\langle\\left(\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T}\\right)^{T}\\left(\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T}\\right), \\mathbf{x x}^{T} \\mathbf{1}_{\\mathbf{v}_{j 2}^{T} \\mathbf{x} \\geq 0}\\right\\rangle\\right. \\\\\n& \\left.\\quad-2\\left\\langle\\left(\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T}\\right)^{T}\\left(\\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T}\\right), \\mathbf{x x}^{T} \\mathbf{1}_{\\mathbf{v}_{j 1}^{T} \\mathbf{x} \\geq 0} \\mathbf{1}_{\\mathbf{v}_{j 2}^{T} \\mathbf{x} \\geq 0}\\right\\rangle\\right]\n\\end{aligned}\n$$\n\nNote that $\\mathbf{1}_{\\mathbf{v}^{T} \\mathbf{x}>0}=\\mathbf{1}_{\\mathbf{v}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})>0}$. Similarly, we have\n\n$$\n\\begin{aligned}\n\\left\\|U_{1}\\left[V_{1}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}-U_{2}\\left[V_{2}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\right\\|^{2} & =\\sum_{j=1}^{r} \\sum_{l=1}^{r}\\left[\\left\\langle\\left(\\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T}\\right)^{T}\\left(\\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T}\\right), \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})^{T} \\mathbf{1}_{\\mathbf{v}_{j 1}^{T} \\mathbf{x} \\geq 0}\\right\\rangle\\right. \\\\\n& \\left.+\\left\\langle\\left(\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T}\\right)^{T}\\left(\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T}\\right), \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})^{T} \\mathbf{1}_{\\mathbf{v}_{j 2}^{T} \\mathbf{x} \\geq 0}\\right\\rangle\\right. \\\\\n& \\left.\\left.-2\\left\\langle\\left(\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T}\\right)^{T}\\left(\\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T}\\right), \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})^{T} \\mathbf{1}_{\\mathbf{v}_{j 1}^{T} \\mathbf{x} \\geq 0} \\mathbf{1}_{\\mathbf{v}_{j 2}^{T} \\mathbf{x} \\geq 0}\\right\\rangle\\right]\n\\end{aligned}\n$$\n\nNow computing the difference between equations (334), and (335) we get\n\n$$\n\\begin{aligned}\n\\mid \\mathbb{E}\\left[\\left\\|U_{1}\\left[V_{1}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}-U_{2}\\left[V_{2}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\right\\|^{2}-\\left\\|U_{1}\\left[V_{1}^{T} \\mathbf{x}\\right]_{+}-U_{2}\\left[V_{2}^{T} \\mathbf{x}\\right]_{+}\\right\\|^{2}\\right]\\right|= \\\\\n\\left.\\mid \\sum_{j=1}^{r} \\sum_{l=1}^{r} \\mathbb{E}\\left[\\left\\langle\\left(\\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T}\\right)^{T}\\left(\\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T}\\right),\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})^{T}-\\mathbf{x x}^{T}\\right) \\mathbf{1}_{\\mathbf{v}_{j 1}^{T} \\mathbf{x} \\geq 0}\\right\\rangle\\right.\\right. \\\\\n& \\left.\\quad+\\left\\langle\\left(\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T}\\right)^{T}\\left(\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T}\\right),\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})^{T}-\\mathbf{x x}^{T}\\right) \\mathbf{1}_{\\mathbf{v}_{j 2}^{T} \\mathbf{x} \\geq 0}\\right\\rangle \\\\\n& \\left.\\left.-2\\left\\langle\\left(\\mathbf{u}_{j 2} \\mathbf{v}_{j 2}^{T}\\right)^{T}\\left(\\mathbf{u}_{j 1} \\mathbf{v}_{j 1}^{T}\\right),\\left(\\mathcal{P}_{\\mathcal{C}}(\\mathbf{x}) \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})^{T}-\\mathbf{x x}^{T}\\right) \\mathbf{1}_{\\mathbf{v}_{j 1}^{T} \\mathbf{x} \\geq 0} \\mathbf{1}_{\\mathbf{v}_{j 2}^{T} \\mathbf{x} \\geq 0}\\right\\rangle\\right]\\right\\}\n\\end{aligned}\n$$\n\nAfter applying Lemma 4 and the triangular inequality we obtain, for $g \\geq 1$,\n\n$$\n\\begin{aligned}\n\\mid \\mathbb{E}\\left[\\left\\|U_{1}\\left[V_{1}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\right.\\right. & \\left.-U_{2}\\left[V_{2}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\left\\|^{2}-\\right\\| U_{1}\\left[V_{1}^{T} \\mathbf{x}\\right]_{+}-U_{2}\\left[V_{2}^{T} \\mathbf{x}\\right]_{+}\\left\\|^{2}\\right]\\right\\| \\\\\n& \\stackrel{(a)}{\\leq} g e^{-g^{2} / 2} \\sum_{j=1}^{r}\\left[\\sum_{l=1}^{r}\\left\\|\\mathbf{u}_{j 1}\\right\\|^{2}\\left\\|\\mathbf{v}_{j 1}\\right\\|^{2}\\right. \\\\\n& \\left.+2\\left\\|\\mathbf{u}_{j 1}\\right\\|\\left\\|\\mathbf{u}_{j 2}\\right\\|\\left\\|\\mathbf{v}_{j 1}\\right\\|\\left\\|\\mathbf{v}_{j 2}\\right\\|+\\left\\|\\mathbf{u}_{j 2}\\right\\|^{2}\\left\\|\\mathbf{v}_{j 2}\\right\\|^{2}\\right] \\\\\n& \\stackrel{(b)}{\\leq} 2 g e^{-g^{2} / 2} \\sum_{j=1}^{r} \\sum_{l=1}^{r}\\left[\\left\\|\\mathbf{u}_{j 1}\\right\\|^{2}\\left\\|\\mathbf{v}_{j 1}\\right\\|^{2}+\\left\\|\\mathbf{u}_{j 2}\\right\\|^{2}\\left\\|\\mathbf{v}_{j 2}\\right\\|^{2}\\right] \\\\\n& \\stackrel{(c)}{\\leq} 2 g e^{-g^{2} / 2}\\left[\\left\\|U_{1}\\right\\|_{F}^{2}\\left\\|V_{1}\\right\\|_{F}^{2}+\\left\\|U_{2}\\right\\|_{F}^{2}\\left\\|V_{2}\\right\\|_{F}^{2}\\right]\n\\end{aligned}\n$$\n\nIn (a) we apply triangular inequality and apply Lemma 4 , (b) we use the identity that $a^{2}+2 a b+b^{2}=(a+b)^{2}$, and (c) we use the identity that $\\left(a^{2} c^{2}+b^{2} d^{2}\\right) \\leq\\left(a^{2}+b^{2}\\right)\\left(c^{2}+d^{2}\\right)$. This completes the proof.\n\nLemma 6 (ReLU projection 2). Consider $U_{1}, U_{2} \\in \\mathbb{R}^{m \\times r}, V_{1}, V_{2} \\in \\mathbb{R}^{n \\times r}$. Denote the convex set $\\mathcal{C}=\\mathbb{B}(g)$; that is, the $g$-radius hyper sphere. Then we have that\n\n$$\n\\begin{aligned}\n& \\left|\\mathbb{E}\\left[\\left\\langle U_{1}\\right| V_{1}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}-U_{2}\\left[V_{2}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}, U_{1}^{\\prime}\\left[V_{1}^{\\prime T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}-U_{2}^{\\prime}\\left[V_{2}^{\\prime T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\right\\rangle \\\\\n& -\\left\\langle U_{1}\\left[V_{1}^{T} \\mathbf{x}\\right]_{+}-U_{2}\\left[V_{2}^{T} \\mathbf{x}\\right]_{+}, U_{1}^{\\prime}\\left[V_{1}^{\\prime T} \\mathbf{x}\\right]_{+}-U_{2}^{\\prime}\\left[V_{2}^{\\prime T} \\mathbf{x}\\right]_{+}\\right\\rangle\\right] \\\\\n& \\leq 2 g e^{-g^{2} / 2}\\left\\|\\left\\|U_{1}\\right\\|_{F}\\left\\|U_{1}^{\\prime}\\right\\|_{F}\\left\\|V_{1}\\right\\|_{F}\\left\\|V_{1}^{\\prime}\\right\\|_{F}+\\left\\|U_{2}\\right\\|_{F}\\left\\|U_{2}^{\\prime}\\right\\|_{F}\\left\\|V_{2}\\right\\|_{F}\\left\\|V_{2}^{\\prime}\\right\\|_{F}\\right]\n\\end{aligned}\n$$\n\nProof. The proof is similar to the proof of Lemma 5.\nCorollary 7 (Two-Layer ReLU Neural Network). Consider the true model for $(\\mathbf{x}, \\mathbf{y})$, where $\\mathbf{x} \\sim \\mathcal{N}\\left(0,(1 / n) I_{n}\\right) \\in$ $\\mathbb{R}^{n}, \\mathbf{y}=U^{*}\\left[V^{* T} \\mathbf{x}\\right]_{+}+\\epsilon$, where $U^{*} \\in \\mathbb{R}^{m \\times R^{n}}, V^{*} \\in \\mathbb{R}^{n \\times R^{n}}$, and $\\epsilon \\sim \\mathcal{N}\\left(0,\\left(\\sigma^{2} / m\\right) I_{m}\\right) \\in \\mathbb{R}^{m}$ independent from $\\mathbf{x}$. For all $i \\in[N]$, let $\\left(\\mathbf{x}_{i}, \\mathbf{y}_{i}\\right)$ be i.i.d. samples from this true model. Consider the estimator $\\hat{\\mathbf{y}}=U\\left[V^{T} \\mathbf{x}\\right]_{+}$, where $U \\in \\mathbb{R}^{m \\times R}, V \\in \\mathbb{R}^{n \\times R}$. Let $\\delta \\in(0,1]$ be fixed. Define the non-convex problem\n\n$$\n\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{ReLU}}((U, V)):=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-U\\left[V^{T} \\mathbf{x}_{i}\\right]_{+}\\right\\|_{2}^{2}+\\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right)\n$$\n\nand define $\\mathbb{N C}_{\\mu}^{\\mathrm{ReLU}}((U, V))$ similarly with the sum over $i$ replaced by expectation taken over $(\\mathbf{x}, \\mathbf{y})$.\nLet $(U, V)$ be a stationary point of $\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{ReLU}}((U, V))$. Suppose there exists $C_{U V}, B_{u}, B_{v}>0$ such that $\\left\\|U V^{T}\\right\\|_{2} \\leq$ $C_{U V}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]$, and for all $j \\in[R],\\left\\|\\mathbf{u}_{j}\\right\\|_{2} \\leq B_{u},\\left\\|\\mathbf{v}_{j}\\right\\|_{2} \\leq B_{v}$. Then with probability at least $1-\\delta$, it holds that\n\n$$\n\\begin{aligned}\n& \\frac{1}{m}\\left|\\mathbb{N C}_{\\mu}^{\\mathrm{ReLU}}((U, V))-\\mathbb{N C}_{\\mu_{N}}^{\\mathrm{ReLU}}((U, V))\\right| \\lesssim \\frac{1}{2 m}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\\left[\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-\\hat{\\mathbf{y}}_{i}\\right\\|_{2}\\left\\|\\mathbf{x}_{i}\\right\\|_{2}-\\lambda\\right] \\\\\n& +C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\\left[\\frac{R(m+n) \\log \\left(R(m+n)\\left(C_{U V}+B_{u}^{2}+B_{v}^{2}\\right)\\right) \\log (N)+\\log (1 / \\delta)}{N}\\right]^{1 / 2}\n\\end{aligned}\n$$\n\nProof. To obtain a generalization bound from Theorem 4 for this setting, we set the following problem parameters:\n\n$$\n\\begin{gathered}\n\\ell(Y, \\hat{Y})=\\frac{1}{2}\\|Y-\\hat{Y}\\| \\Longrightarrow(\\alpha, L)=(0,1) \\\\\n\\phi(W)=\\left[\\langle\\mathbf{v}, \\mathbf{x}\\rangle\\right]_{+} \\mathbf{u} \\\\\n\\theta(W)=\\frac{1}{2}\\left[\\|\\mathbf{u}\\|_{2}^{2}+\\|\\mathbf{v}\\|_{2}^{2}\\right]\n\\end{gathered}\n$$\n\nEstimating $\\Omega\\left(f_{\\mu}^{*}\\right)$ : From Proposition 2 we have that\n\n$$\n\\Omega\\left(f_{\\mu}^{*}\\right) \\leq \\frac{\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}}{2}\n$$\n\nChoosing $\\mathcal{F}_{\\theta}$ :\n\n$$\n\\mathcal{F}_{\\theta}:=\\left\\{(\\mathbf{u}, \\mathbf{v}):\\|\\mathbf{u}\\|^{2}+\\|\\mathbf{v}\\|^{2} \\leq 2,\\|\\mathbf{u}\\|_{2} \\leq 1,\\|\\mathbf{v}\\|_{2} \\leq 1\\right\\}\n$$\n\nEstimating $L_{\\phi}$ : The Lipschtiz constant $L_{\\phi}$ in the function $\\mathcal{F}_{\\theta}$ is $L_{\\phi}:=\\sup _{\\|\\mathbf{u}\\| \\leq 1,\\|\\mathbf{v}\\| \\leq 1}\\left\\|\\mathbf{u}\\left[\\mathbf{v}^{T}.\\right]_{+}\\right\\|_{\\text {Lip }} \\leq$ $\\sup _{\\|\\mathbf{u}\\| \\leq 1,\\|\\mathbf{v}\\| \\leq 1}\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|=1$.\nEstimating $r_{\\theta}$ : For any $(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}$, we have that,\n\n$$\n\\frac{\\|\\mathbf{u}\\|^{2}+\\|\\mathbf{v}\\|^{2}}{2} \\leq \\sqrt{\\frac{\\|\\mathbf{u}\\|^{2}+\\|\\mathbf{v}\\|^{2}}{2}} \\Longrightarrow \\mathcal{F}_{\\theta} \\subseteq \\mathbb{B}(1 / \\sqrt{2})\n$$\n\nThen we have $r_{\\theta}=1 / \\sqrt{2}$.\nChoosing $\\mathcal{F}_{\\mathcal{W}}$ : From the corollary's assumptions we have that $\\mathcal{B}_{R}:=\\left\\{(\\mathbf{u}, \\mathbf{v}):\\|\\mathbf{u}\\|_{2} \\leq B_{u},\\|\\mathbf{v}\\|_{2} \\leq B_{v}\\right\\}$; our hypothesis class is defined as\n\n$$\n\\mathcal{F}_{\\mathcal{W}}:=\\left\\{(U, V):\\left\\|U\\left[V^{T}\\right.\\right]_{+}\\left\\|_{\\text {Lip }} \\leq\\right\\| U \\|_{2}\\|V \\|_{2} \\leq \\gamma,\\left\\|\\mathbf{u}_{j}\\right\\| \\leq B_{u},\\left\\|\\mathbf{v}_{j}\\right\\| \\leq B_{v}\\right\\}\n$$\n\nFrom Proposition 2, we have that, $\\Omega\\left(f_{\\mu}^{*}\\right) \\leq \\frac{1}{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right] . \\quad$ As we require $\\gamma \\geq \\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}=$ $\\frac{1}{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]$, we set $\\gamma=C_{U V} \\frac{\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]}{2}$.\n\n$$\n\\begin{gathered}\n\\mathcal{F}_{\\mathcal{W}}=\\left\\{\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\}:\\left\\|U\\left[V^{T}\\right]_{+}\\right\\|_{\\text {Lip }}=\\left\\|U V^{T}\\right\\|_{2} \\leq \\frac{C_{U V}}{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right] \\\\\n\\left\\|\\mathbf{u}_{j}\\right\\| \\leq B_{u},\\left\\|\\mathbf{v}_{j}\\right\\| \\leq B_{v}\\}\n\\end{gathered}\n$$\n\nEstimating $\\Omega_{\\mu_{N}}^{\\circ}(\\cdot)$ : We have\n\n$$\n\\begin{aligned}\n\\Omega_{\\mu_{N}}^{\\circ}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) & =\\Omega_{\\mu_{N}}^{\\circ}\\left(\\frac{1}{\\lambda}\\left(g-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) \\\\\n& =\\sup _{\\|\\mathbf{u}\\| \\leq 1:\\|\\mathbf{v}\\| \\leq 1} \\frac{1}{N \\lambda} \\sum_{i=1}^{N}\\left\\langle Y_{i}-U\\left[V^{T} \\mathbf{x}_{i}\\right]_{+}, \\mathbf{u}\\left[\\mathbf{v}^{T} \\mathbf{x}_{i}\\right]_{+}\\right\\rangle \\\\\n& =\\sup _{\\|\\mathbf{v}\\| \\leq 1} \\frac{1}{N \\lambda} \\sum_{i=1}^{N}\\left[\\mathbf{v}^{T} \\mathbf{x}_{i}\\right]_{+}\\left\\|Y_{i}-\\hat{Y}_{i}\\right\\|_{2} \\\\\n& \\leq \\frac{1}{N \\lambda} \\sum_{i=1}^{N} \\sup _{\\|\\mathbf{v}\\| \\leq 1}\\left[\\mathbf{v}^{T} \\mathbf{x}_{i}\\right]_{+}\\left\\|Y_{i}-\\hat{Y}_{i}\\right\\|_{2} \\\\\n& =\\frac{1}{N \\lambda} \\sum_{i=1}^{N}\\left\\|\\mathbf{x}_{i}\\right\\|_{2}\\left\\|Y_{i}-\\hat{Y}_{i}\\right\\|_{2}\n\\end{aligned}\n$$\n\nEstimating $\\epsilon_{0}$ : From the data generating mechanism we have $\\|g\\|_{\\text {Lip }} \\leq\\left\\|U^{*}\\right\\|_{2}\\left\\|V^{*}\\right\\|_{2} \\leq \\frac{1}{2}\\left[\\left\\|U^{*}\\right\\|_{F}+\\left\\|V^{*}\\right\\|_{F}\\right]$, $\\sigma_{X}=1, \\sigma_{Y \\mid X}=\\sigma$. Then we have the following constants from Theorem 4 :\n\n$$\n\\epsilon_{0}=16 \\gamma^{2} \\sigma_{X}^{2} \\min \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\text {Lip }}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nwhich evaluates to\n\n$$\n\\epsilon_{0}=8 C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right] \\min \\left\\{1, \\frac{\\left(1+\\sigma^{2}\\right)}{4 C_{U V}^{2}}\\right\\}\n$$\n\nLet $C_{U V} \\leq 0.5 \\sqrt{1+\\sigma^{2}}$ then we have\n\n$$\n\\epsilon_{0}=2\\left(1+\\sigma^{2}\\right)\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\n$$\n\nEstimating $\\epsilon_{1}$ : Similarly,\n\n$$\n\\epsilon_{1}=16 \\gamma^{2} \\sigma_{X}^{2} \\max \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\text {Lip }}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nobtaining\n\n$$\n\\epsilon_{1}=8 C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\n$$\n\nDefining a convex set $\\mathcal{C}$ : Consider a convex set $\\mathcal{C}=\\mathbb{B}(g)=\\left\\{\\mathbf{x}:\\|\\mathbf{x}\\|_{2} \\leq g\\right\\}$.\nFirst and foremost we need to estimate, $\\delta_{\\mathcal{C}}$ for the following inequality to hold:\n\n$$\nP\\left(\\cap_{i=1}^{N} \\mathbf{x}_{i} \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}\n$$\n\nThe probability of $\\mathbf{x} \\in \\mathcal{C}=\\mathbb{B}(g)$ is equivalent to saying the probability of the event when $\\|\\mathbf{x}\\|_{2} \\leq g$. Since, $x_{i} \\sim \\mathcal{N}(0,1 / n)$ as a consequence of Bernstein's Inequality (Vershynin, 2018, Corollary 2.8.3) we have that, for any $t \\geq 0$,\n\n$$\nP\\left(\\left|\\|\\mathbf{x}\\|_{2}-1\\right| \\leq t\\right) \\geq 1-2 \\exp \\left(-c n_{X} t^{2}\\right)\n$$\n\nfor some constant $c \\geq 0$. Now we have\n\n$$\nP\\left(\\|\\mathbf{x}\\|_{2} \\leq g\\right) \\begin{cases}\\geq 1-2 \\exp \\left(-c n_{X}(g-1)^{2}\\right) & \\text { if } g \\geq 1 \\\\ \\leq 2 \\exp \\left(-c n_{X}(g-1)^{2}\\right) & \\text { otherwise }\\end{cases}\n$$\n\nWe consider the case where $g \\geq 1$ yielding\n\n$$\nP\\left(\\cap_{i=1}^{N} \\mathbf{x}_{i} \\in \\mathcal{C}\\right)=P\\left(\\cap_{i=1}^{N}\\|\\mathbf{x}\\|_{2} \\leq g\\right) \\geq 1-\\underbrace{2 N \\exp \\left(-c n_{X}(g-1)^{2}\\right)}_{=\\delta_{\\mathcal{C}}}\n$$\n\nWe have that $\\delta_{\\mathcal{C}}=2 N \\exp \\left(-c n(g-1)^{2}\\right)$.\nNow we evaluate $B_{\\ell}, B_{\\Phi}, \\tilde{L}_{\\Phi}, \\tilde{L}_{\\phi}$.\nEstimating $B_{\\Phi}$ : We have\n\n$$\n\\begin{aligned}\nB_{\\Phi} & =\\sup _{\\mathbf{z} \\in \\mathcal{C},\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{W}}}\\left\\|U\\left[V^{T} \\mathbf{z}\\right]_{+}\\right\\|_{2} \\\\\n& \\leq \\sup _{\\mathbf{z} \\in \\mathcal{C},\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{W}}}\\left\\|U\\|_{2}\\right\\|\\left[V^{T} \\mathbf{z}\\right]_{+}\\|_{2} \\\\\n& \\leq \\sup _{\\mathbf{z} \\in \\mathcal{C},\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{W}}}\\left\\|U\\|_{2}\\right\\| V\\|_{2}\\|\\mathbf{z}\\|_{2} \\\\\n& =g \\gamma\n\\end{aligned}\n$$\n\nEstimating $B_{\\ell}$ : Similarly, we have\n\n$$\n\\begin{aligned}\nB_{\\ell} & =\\sup _{\\mathbf{z} \\in \\mathcal{C},\\left\\{\\left(\\mathbf{u}_{j}, \\mathbf{v}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{W}}}\\left\\|U\\left[V^{T} \\mathbf{z}\\right]_{+}-U^{*}\\left[V^{* T} \\mathbf{z}\\right]\\right\\|_{2} \\\\\n& =2 g \\gamma\n\\end{aligned}\n$$\n\nEstimating $\\tilde{L}_{\\Phi}$ : Now, we compute the Lipschitz constant with respect to $U, V$. We have\n\n$$\n\\begin{aligned}\n\\tilde{L}_{\\Phi} & =\\sup _{\\mathbf{z} \\in \\mathcal{C},(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{W}} \\frac{\\left\\|U\\left[V^{T} \\mathbf{z}\\right]_{+}-U^{\\prime}\\left[V^{\\prime T} \\mathbf{z}\\right]_{+}\\right\\|}{\\max _{j} \\sqrt{\\left\\|\\mathbf{u}_{j}-\\mathbf{u}_{j}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}_{j}-\\mathbf{v}_{j}^{\\prime}\\right\\|^{2}}} \\\\\n& =R \\sup _{\\mathbf{z} \\in \\mathcal{C},(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{W}} \\frac{\\left\\|\\mathbf{u}\\left[\\mathbf{v}^{T} \\mathbf{z}\\right]_{+}-\\mathbf{u}^{\\prime}\\left[\\mathbf{v}^{\\prime T} \\mathbf{z}\\right]_{+}\\right\\|}{\\sqrt{\\left\\|\\mathbf{u}-\\mathbf{u}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}-\\mathbf{v}^{\\prime}\\right\\|^{2}}} \\\\\n& =R \\sup _{\\mathbf{z} \\in \\mathcal{C},(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{W}} \\frac{\\left\\|\\left(\\mathbf{u}-\\mathbf{u}^{\\prime}\\right)\\left[\\mathbf{v}^{T} \\mathbf{z}\\right]_{+}-\\mathbf{u}^{\\prime}\\left[\\left[\\mathbf{v}^{\\prime T} \\mathbf{z}\\right]_{+}-\\left[\\mathbf{v}^{T} \\mathbf{z}\\right]_{+}\\right]\\right\\|}{\\sqrt{\\left\\|\\mathbf{u}-\\mathbf{u}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}-\\mathbf{v}^{\\prime}\\right\\|^{2}}} \\\\\n& \\leq R \\sup _{\\mathbf{z} \\in \\mathcal{C},(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{W}} \\frac{\\left\\|\\left(\\mathbf{u}-\\mathbf{u}^{\\prime}\\right)\\left[\\mathbf{v}^{T} \\mathbf{z}\\right]_{+}\\right\\|+\\left\\|\\mathbf{u}^{\\prime}\\left[\\left[\\mathbf{v}^{\\prime T} \\mathbf{z}\\right]_{+}-\\left[\\mathbf{v}^{T} \\mathbf{z}\\right]_{+}\\right]\\right\\|}{\\sqrt{\\left\\|\\mathbf{u}-\\mathbf{u}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}-\\mathbf{v}^{\\prime}\\right\\|^{2}}} \\\\\n& \\leq R \\sup _{\\mathbf{z} \\in \\mathcal{C},(U, V),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{W}} \\frac{\\left\\|\\left(\\mathbf{u}-\\mathbf{u}^{\\prime}\\right)\\left[\\mathbf{v}^{T} \\mathbf{z}\\right]_{+}\\right\\|+\\left\\|\\mathbf{u}^{\\prime}\\left[\\left[\\mathbf{v}^{\\prime T} \\mathbf{z}\\right]_{+}-\\left[\\mathbf{v}^{T} \\mathbf{z}\\right]_{+}\\right]\\right\\|}{\\sqrt{\\left\\|\\mathbf{u}-\\mathbf{u}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}-\\mathbf{v}^{\\prime}\\right\\|^{2}}} \\\\\n& \\leq g R \\sup _{\\left(U, V^{\\prime}\\right),\\left(U^{\\prime}, V^{\\prime}\\right) \\in \\mathcal{F}_{W}} \\frac{B_{v}\\left\\|\\left(\\mathbf{u}-\\mathbf{u}^{\\prime}\\right)\\right\\|+B_{u}\\left\\|\\mathbf{v}-\\mathbf{v}^{\\prime}\\right\\|}{\\sqrt{\\left\\|\\mathbf{u}-\\mathbf{u}^{\\prime}\\right\\|^{2}+\\left\\|\\mathbf{v}-\\mathbf{v}^{\\prime}\\right\\|^{2}}} \\\\\n& =g \\sqrt{B_{u}^{2}+B_{v}^{2}} R\n\\end{aligned}\n$$\n\nEstimating $\\tilde{L}_{\\phi}$ : Similarly we get $\\tilde{L}_{\\phi}=g \\sqrt{B_{\\mathrm{u}}^{2}+B_{\\mathrm{c}}^{2}}$.\nEstimating $\\epsilon_{2}$ : Recall that\n\n$$\n\\epsilon_{2}=\\max \\left\\{8 B_{\\ell} \\tilde{L}_{\\Phi}, 8 \\tilde{L}_{\\Phi}\\left[B_{\\ell}+B_{\\Phi} L\\right], 32 \\Omega\\left(f_{\\mu}^{*}\\right) \\tilde{L}_{\\phi} \\max \\left\\{B_{\\ell}, L B_{\\Phi}\\right\\}, 4 \\tilde{L}_{\\Phi} B_{\\Phi}\\right\\}\n$$\n\nFrom all the constants computed earlier, we have that\n\n$$\n\\epsilon_{2}=k_{1} g^{2} R^{2} C_{U V} \\sqrt{B_{\\mathrm{u}}^{2}+B_{\\mathrm{c}}^{2}}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\n$$\n\nfor some constant $k_{1} \\geq 0$.\nNext, we move on to estimating $B(\\mathcal{C})$. We need to analyze three terms:\nThe First Term: Define\n\n$$\nT_{1}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right|\n$$\n\nFor a fixed $(U, V)$, we have\n\n$$\n\\begin{aligned}\n& \\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right| \\\\\n& =\\left|\\mathbb{E}\\left[\\left\\|U^{*}\\left[V^{* T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}-U\\left[V^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\right\\|^{2}-\\right\\| U^{*}\\left[V^{* T} \\mathbf{x}\\right)_{+}-U\\left[V^{T} \\mathbf{x}\\right]_{+}\\right\\|^{2}\\right] \\|\n\\end{aligned}\n$$\n\nFrom Lemma 5 taking $g \\geq 1$ we have\n\n$$\n\\begin{aligned}\n\\mid\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2} & -\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2} \\mid \\\\\n& \\leq 2 g e^{-g^{2} / 2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}\\left\\|V^{*}\\right\\|_{F}^{2}+\\|U\\|_{F}^{2}\\|V\\|_{F}^{2}\\right]\n\\end{aligned}\n$$\n\nwhereupon further simplifying, we obtain\n\n$$\n\\begin{aligned}\n\\sup _{(U, V) \\in \\mathcal{F}_{\\mathrm{IV}}}\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}\\right.\\right. & \\left.-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2} \\mid \\\\\n& \\leq 2 g e^{-g^{2} / 2}\\left(\\left\\|U^{*}\\right\\|_{F}^{2}\\left\\|V^{*}\\right\\|_{F}^{2}+R^{2} \\gamma^{2}\\right)\n\\end{aligned}\n$$\n\nNow, applying triangular inequality and taking the supremum we obtain\n\n$$\nT_{2} \\leq 2 g e^{-g^{2} / 2}\\left(\\left\\|U^{*}\\right\\|_{F}^{2}\\left\\|V^{*}\\right\\|_{F}^{2}+R^{2} \\gamma^{2}\\right)\n$$\n\nThe Second Term: Define\n\n$$\n\\begin{aligned}\nT_{2}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}, W^{\\prime} \\in \\mathcal{F}_{\\mathrm{Z}}} & \\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}\\right. \\\\\n& \\left.-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu} \\mid\n\\end{aligned}\n$$\n\nFor a fixed $(U, V),(\\mathbf{u}, \\mathbf{v})$ we have\n\n$$\n\\begin{gathered}\n\\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu}\\right|= \\\\\n\\begin{gathered}\n\\left|\\mathbb{E}\\left[\\left\\langle U\\left[V^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}-U^{*}\\left[V^{* T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}, \\mathbf{u}\\left[\\mathbf{v}^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\right\\rangle\\right.\\right. \\\\\n\\left.\\left.-\\left\\langle U\\left[V^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}-U^{*}\\left[V^{* T} \\mathbf{x}\\right]_{+}, \\mathbf{u}\\left[\\mathbf{v}^{T} \\mathbf{x}\\right]_{+}\\right\\rangle\\right] \\mid\n\\end{gathered}\n\\end{gathered}\n$$\n\nAs a consequence of Lemma 6 we have\n\n$$\n\\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu}\\right|\n$$\n\n$$\n\\leq 2 g e^{-g^{2} / 2}\\left[\\|U\\|_{F}\\|V\\|_{F}\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|+\\left\\|U^{*}\\right\\|_{F}\\left\\|V^{*}\\right\\|_{F}\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|\\right]\n$$\n\nNow we apply supremum over $(\\mathbf{u}, \\mathbf{v}) \\in \\mathcal{F}_{\\theta}$, obtaining\n\n$$\nT_{2} \\leq 2 g e^{-g^{2} / 2}\\left[\\|U\\|_{F}\\|V\\|_{F}+\\left\\|U^{*}\\right\\|_{F}\\left\\|V^{*}\\right\\|_{F}\\right]\n$$\n\nFinally, we apply supremum over $(U, V) \\in \\mathcal{F}_{\\mathcal{W}}$, obtaining\n\n$$\nT_{2} \\leq 2 g e^{-g^{2} / 2}\\left[\\left\\|U^{*}\\right\\|_{F}\\left\\|V^{*}\\right\\|_{F}+R \\gamma\\right]\n$$\n\nThe Third Term: Define\n\n$$\n\\begin{aligned}\nT_{3}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathcal{W}}} & \\left\\|\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu} \\\\\n& -\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu} \\mid\n\\end{aligned}\n$$\n\nFor a fixed $(U, V)$ we can rewrite the above to\n\n$$\n\\begin{aligned}\n& \\left|\\mathbb{E}\\left[\\left\\langle U\\left[V^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}-U^{*}\\left[V^{* T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}, U\\left[V^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}\\right\\rangle\\right. \\\\\n& \\left.\\quad-\\left\\langle U\\left[V^{T} \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})\\right]_{+}-U^{*}\\left[V^{* T} \\mathbf{x}\\right]_{+}, U\\left[V^{T} \\mathbf{x}\\right]_{+}\\right\\rangle\\right] \\mid\n\\end{aligned}\n$$\n\nAs a consequence of Lemma 4 we have\n\n$$\n\\begin{gathered}\n\\left|\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\tilde{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu}\\right| \\\\\n\\leq 2 g e^{-g^{2} / 2}\\|U\\|_{F}\\|V\\|_{F}\\left[\\|U\\|_{F}\\|V\\|_{F}+\\left\\|U^{*}\\right\\|_{F}\\left\\|V^{*}\\right\\|_{F}\\right]\n\\end{gathered}\n$$\n\nFinally, we apply supremum over $(U, V) \\in \\mathcal{F}_{\\mathcal{W}}$, obtaining\n\n$$\nT_{3} \\leq 2 g e^{-g^{2} / 2} R \\gamma\\left[\\left\\|U^{*}\\right\\|_{F}\\left\\|V^{*}\\right\\|_{F}+R \\gamma\\right]\n$$\n\nNow combining $T_{1}, T_{2}$ and $T_{3}$ from equations (381), (386), (390) we have\n\n$$\nB(\\mathcal{C}) \\leq 2 g e^{-g^{2} / 2}\\left[\\alpha\\left(\\left\\|U^{*}\\right\\|_{F}^{2}\\left\\|V^{*}\\right\\|_{F}+R^{2} \\gamma^{2}\\right)+\\|U\\|_{F}\\|V\\|_{F}+R \\gamma+R \\gamma\\left[\\left\\|U^{*}\\right\\|_{F}\\left\\|V^{*}\\right\\|_{F}+R \\gamma\\right]\\right]\n$$\n\nWe further upper bound for simplicity via\n\n$$\nB(\\mathcal{C}) \\leq 4 R g e^{-g^{2} / 2} \\gamma\\left[\\|U\\|_{F}\\|V\\|_{F}+\\gamma\\right]\n$$\n\nFrom Theorem 4 we have that\n\n$$\n\\begin{gathered}\n\\frac{1}{m}\\left|\\mathbb{N C}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathbb{N C}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\lesssim \\frac{\\lambda}{2 m}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\\left[\\sup _{\\|\\mathbf{v}\\| \\leq 1} \\frac{1}{N} \\sum_{i=1}^{N}\\left[\\mathbf{v}^{T} \\mathbf{x}_{i}\\right]_{+}\\left\\|Y_{i}-U\\left[V^{T} \\mathbf{x}_{i}\\right]_{+}\\right\\|-\\lambda\\right] \\\\\n+\\frac{2}{m} R g e^{-g^{2} / 2} C_{U V}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\\left[\\|U\\|_{F}\\|V\\|_{F}+C_{U V}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\\right]+C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]^{2} \\\\\n\\sqrt{\\frac{R(m+n) \\log \\left(k_{1} g^{2} R^{2} C_{U V}^{2} \\sqrt{B_{u}^{2}+B_{v}^{2}}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]^{2}\\right) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{gathered}\n$$\n\nholds true w.p at least $1-\\delta-2 N \\exp \\left(-c n_{X}(g-1)^{2}\\right)$.\nNow choose\n\n$$\ng=1+\\mathcal{O}(\\sqrt{\\log (N R)+\\log (1 / \\delta)})\n$$\n\nAfter ignoring all the log-log terms and using only dominant terms, we have that\n\n$$\n\\begin{gathered}\n\\frac{1}{m}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\lesssim \\\\\n\\frac{\\lambda}{2 m}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]\\left[\\sup _{\\|\\mathbf{v}\\| \\leq 1} \\frac{1}{N} \\sum_{i=1}^{N}\\left[\\mathbf{v}^{T} \\mathbf{x}_{i}\\right]_{+}\\left\\|Y_{i}-U\\left[V^{T} \\mathbf{x}_{i}\\right]_{+}\\right\\|-\\lambda\\right] \\\\\n+C_{U V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]^{2} \\sqrt{\\frac{R(m+n) \\log \\left(R(m+n)\\left(C_{U V}+B_{u}^{2}+B_{v}^{2}\\right)\\right) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{gathered}\n$$\n\nholds true w.p at least $1-\\delta$.", "tables": {}, "images": {}}, {"section_id": 26, "text": "# C. 5 Multi-head Attention \n\nNext, we move to applying Theorem 4 to the single-layer multi-head attention problem. We require similar Gaussian projections arguments onto convex sets are needed to be established. For this application, we require Gaussian projections onto softmax, which is analyzed through Lemma 7. First, we define the soft max operation, $\\sigma_{t}(\\cdot): \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$.\n\n$$\n\\left[\\sigma_{t}(\\mathbf{u})\\right]_{i}:=\\exp \\left(t u_{i}\\right) /\\left(\\sum_{j=1}^{n} \\exp \\left(t u_{j}\\right)\\right)\n$$\n\nwhere $t$ is called the temperature.\nA discrete version of soft-max is known as hard-max that is defined as\n\n$$\n[\\sigma(\\mathbf{u})]_{i}:=\\mathbf{1}_{u_{i}=\\max _{i} u_{i}}\n$$\n\nNote that when $t \\rightarrow \\infty, \\sigma_{t}(\\mathbf{u}) \\rightarrow \\sigma(\\mathbf{u})$.\nLemma 7 (Gaussian Softmax Projection). Let $X \\in \\mathbb{R}^{m \\times n}$ and $X_{i j} \\sim \\mathcal{N}(0,1 /(m n))$ be independent random variables. Suppose $\\sigma_{t}(\\cdot)$ is a softmax with temperature, $t$, and $M$ is fixed matrix in $\\mathbb{R}^{m \\times n}$. Consider a convex set $\\mathcal{C}=\\left\\{X=\\left(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right): \\forall j \\in[n] ;\\left\\|\\mathbf{x}_{j}\\right\\| \\leq g\\right\\}$ for $g \\geq 1$. Then\n\n$$\n\\begin{gathered}\n\\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}}\\left|\\left\\langle M, \\mathbb{E}\\left[X \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\left(X \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}-\\mathcal{P}_{\\mathcal{C}}(X) \\sigma_{t}\\left(\\mathcal{P}_{\\mathcal{C}}(X)^{T} \\mathbf{z}\\right)\\left(\\mathcal{P}_{\\mathcal{C}}\\left(X) \\sigma_{t}\\left(\\mathcal{P}_{\\mathcal{C}}(X)^{T} \\mathbf{z}\\right)\\right)^{T}\\right]\\right\\rangle\\right| \\\\\n\\leq c_{1} m^{2}\\|M\\|_{F} g \\exp \\left(-c_{2} m g^{2}\\right)\n\\end{gathered}\n$$\n\nfor some positive constant, $c_{1}, c_{2}$.\nProof. Denote\n\n$$\nT:=\\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}}\\left|\\left\\langle M, \\mathbb{E}\\left[X \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\left(X \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}-\\mathcal{P}_{\\mathcal{C}}(X) \\sigma_{t}\\left(\\mathcal{P}_{\\mathcal{C}}(X)^{T} \\mathbf{z}\\right)\\left(\\mathcal{P}_{\\mathcal{C}}\\left(X) \\sigma_{t}\\left(\\mathcal{P}_{\\mathcal{C}}\\left(X\\right)^{T} \\mathbf{z}\\right)\\right)^{T}\\right]\\right\\rangle\\right|\n$$\n\nFirstly, we upper bound the earlier term by Cauchy-Schwartz inequality:\n\n$$\nT \\leq\\|M\\|_{F} \\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}}\\left\\|\\mathbb{E}\\left[X \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\left(X \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}-\\mathcal{P}_{\\mathcal{C}}(X) \\sigma_{t}\\left(\\mathcal{P}_{\\mathcal{C}}\\left(X\\right)^{T} \\mathbf{z}\\right)\\left(\\mathcal{P}_{\\mathcal{C}}\\left(X) \\sigma_{t}\\left(\\mathcal{P}_{\\mathcal{C}}\\left(X\\right)^{T} \\mathbf{z}\\right)\\right)^{T}\\right]\\right\\|_{F}\n$$\n\nDenote $a_{t}(\\mathbf{z}, i, j)=\\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)_{j}$ and $\\tilde{a}_{t}(\\mathbf{z}, i, j)=\\sigma_{t}\\left(\\mathcal{P}_{\\mathcal{C}}(X)^{T} \\mathbf{z}\\right)_{i} \\sigma_{t}\\left(\\mathcal{P}_{\\mathcal{C}}(X)^{T} \\mathbf{z}\\right)_{j}$,\n\n$$\nT \\leq\\|M\\|_{F} \\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}}\\left\\|\\sum_{i=1}^{T} \\sum_{j=1}^{T} \\mathbb{E}\\left[\\mathbf{x}_{i} \\mathbf{x}_{j}^{T} a_{t}(\\mathbf{z}, i, j)-\\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{i}\\right) \\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{j}\\right)^{T} \\tilde{a}_{t}(\\mathbf{z}, i, j)\\right]\\right\\|_{F}\n$$\n\nNow we apply triangular inequality,\n\n$$\nT \\leq\\|M\\|_{F} \\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}} \\sum_{i=1}^{T} \\sum_{j=1}^{T}\\left\\|\\mathbb{E}\\left[\\mathbf{x}_{i} \\mathbf{x}_{j}^{T} a_{t}(\\mathbf{z}, i, j)-\\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{i}\\right) \\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{j}\\right)^{T} \\tilde{a}_{t}(\\mathbf{z}, i, j)\\right]\\right\\|_{F}\n$$\n\nNow we apply Cauchy-Schwartz,\n\n$$\nT \\leq\\|M\\|_{F} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}}\\left\\|\\mathbb{E}\\left[\\mathbf{x}_{i} \\mathbf{x}_{j}^{T} a_{t}(\\mathbf{z}, i, j)-\\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{i}\\right) \\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{j}\\right)^{T} \\tilde{a}_{t}(\\mathbf{z}, i, j)\\right]\\right\\|_{F}\n$$\n\nWe can upper bound the earlier term via taking a supremum over indices $i, j \\in[T]$ then we have\n\n$$\nT \\leq\\|M\\|_{F} m^{2} \\sup _{i, j} \\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}}\\left\\|\\mathbb{E}\\left[\\mathbf{x}_{i} \\mathbf{x}_{j}^{T} a_{t}(\\mathbf{z}, i, j)-\\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{i}\\right) \\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{j}\\right)^{T} \\tilde{a}_{t}(\\mathbf{z}, i, j)\\right]\\right\\|_{F}\n$$\n\nObserve that argument inside the expectation is 0 on the event $X \\in \\mathcal{C}$, thereby we only have the case where $X \\notin \\mathcal{C}$ then we have\n\n$$\nT \\leq\\|M\\|_{F} m^{2} \\sup _{i, j} \\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}}\\left\\|\\mathbb{E}\\left[\\mathbf{x}_{i} \\mathbf{x}_{j}^{T} a_{t}(\\mathbf{z}, i, j)-\\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{i}\\right) \\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{j}\\right)^{T} \\tilde{a}_{t}(\\mathbf{z}, i, j) \\mathbf{1}_{\\mathcal{E}^{c}}\\right]\\right\\|_{F}\n$$\n\nOn application Cauchy-Schwartz identity again we have\n\n$$\nT \\leq\\|M\\|_{F} m^{2} \\sup _{i, j} \\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}} \\mathbb{E}\\left[\\left\\|\\mathbf{x}_{i} \\mathbf{x}_{j}^{T} a_{t}(\\mathbf{z}, i, j)-\\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{i}\\right) \\mathcal{P}_{\\mathcal{C}}\\left(\\mathbf{x}_{j}\\right)^{T} \\tilde{a}_{t}(\\mathbf{z}, i, j)\\right\\|_{F} \\mathbf{1}_{\\mathcal{E}^{c}}\\right]\n$$\n\nWe have that $\\mathbf{x} \\notin \\mathcal{C}, \\mathcal{P}_{\\mathcal{C}}(\\mathbf{x})=g \\mathbf{x} /\\|\\mathbf{x}\\|$. By using this fact we have\n\n$$\nT \\leq\\|M\\|_{F} m^{2} \\sup _{i, j} \\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}} \\mathbb{E}\\left[\\left\\|\\left(a_{t}(\\mathbf{z}, i, j)-\\frac{g^{2}}{\\left\\|\\mathbf{x}_{i}\\right\\|\\left\\|\\mathbf{x}_{j}\\right\\|} \\tilde{a}_{t}(\\mathbf{z}, i, j)\\right) \\mathbf{x}_{i} \\mathbf{x}_{j}^{T}\\right\\|_{F} \\mathbf{1}_{\\mathcal{E}^{c}}\\right]\n$$\n\nNow we recall Reverse Fatou's Lemma, for any function sequence, $f_{n} \\in L^{2}(\\mu)$, we have\n\n$$\n\\limsup _{n \\rightarrow \\infty} \\int f_{n} d \\mu \\leq \\int \\limsup _{n \\rightarrow \\infty} f_{n} d \\mu\n$$\n\non applying this identity we have\n\n$$\nT \\leq\\|M\\|_{F} m^{2} \\sup _{i, j} \\mathbb{E}\\left[\\sup _{\\mathbf{z} \\in \\mathcal{F}_{\\mathbf{z}}}\\left|a_{t}(\\mathbf{z}, i, j)-\\frac{g^{2}}{\\left\\|\\mathbf{x}_{i}\\right\\|\\left\\|\\mathbf{x}_{j}\\right\\|} \\tilde{a}_{t}(\\mathbf{z}, i, j)\\right|\\left\\|\\mathbf{x}_{i} \\mathbf{x}_{j}^{T}\\right\\|_{F} \\mathbf{1}_{\\mathcal{E}^{c}}\\right]\n$$\n\nObserve that $a_{t}(\\mathbf{z}, i, j) \\leq 1$ and $\\frac{g^{2}}{\\left\\|\\mathbf{x}_{i}\\right\\|\\left\\|\\mathbf{x}_{j}\\right\\|} \\tilde{a}_{t}(\\mathbf{z}, i, j) \\leq 1$ when $X \\notin \\mathcal{C}$ therefore we have\n\n$$\nT \\leq\\|M\\|_{F} m^{2} \\sup _{i, j} \\mathbb{E}\\left[\\left\\|\\mathbf{x}_{i} \\mathbf{x}_{j}^{T}\\right\\|_{F} \\mathbf{1}_{\\mathcal{E}^{c}}\\right]\n$$\n\nNow since $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ are iid we have\n\n$$\nT \\leq\\|M\\|_{F} m^{2} \\sup _{i, j} \\mathbb{E}\\left[\\left\\|\\mathbf{x}_{i}\\right\\|_{2} \\mathbf{1}_{\\mathcal{E}^{c}}\\right] \\mathbb{E}\\left[\\left\\|\\mathbf{x}_{j}\\right\\|_{2} \\mathbf{1}_{\\mathcal{E}^{c}}\\right]\n$$\n\nBy Gaussian integral over norm for $g \\geq 1$ we have\n\n$$\nT \\lesssim m^{2}\\|M\\|_{F} g \\exp \\left(-m g^{2}\\right)\n$$\n\nWith the above result on Gaussian softmax projection, we now state the corollary for the single-layer multi-head attention problem and its proof.\nCorollary 8 (Transformers). Consider the true model for $(X, \\mathbf{y})$, where $X \\in \\mathbb{R}^{n \\times T}$ is a random matrix with i.i.d. entries $X_{l k} \\sim \\mathcal{N}(0,1 /(n T))$ and $\\mathbf{y}=A^{*} X \\mathbf{b}^{*}+\\epsilon$, where $A^{*} \\in \\mathbb{R}^{m \\times n}, \\mathbf{b}^{*} \\in \\mathbb{S}^{T-1}$ and $\\epsilon \\sim \\mathcal{N}\\left(0,\\left(\\sigma^{2} / m\\right) I_{m}\\right)$ is independent from $X$. For all $i \\in[N]$, let $\\left(X_{i}, \\mathbf{y}_{i}\\right)$ be i.i.d. samples from this true model. Consider the estimator $\\hat{\\mathbf{y}}=\\sum_{j=1}^{R} V_{j} X \\sigma\\left(X^{T} \\mathbf{z}_{j}\\right), V_{j} \\in \\mathbb{R}^{n}, \\mathbf{z}_{j} \\in \\mathbb{R}^{n}$. Let $\\delta \\in(0,1]$ be fixed. Define the non-convex problem\n\n$$\n\\mathrm{NC}_{\\mu_{N}}^{\\mathrm{TF}}\\left(\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}\\right):=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-\\sum_{j=1}^{R} V_{j} X_{i} \\sigma_{t}\\left(X_{i}^{T} \\mathbf{z}_{j}\\right)\\right\\|_{2}^{2}+\\lambda \\sum_{j=1}^{R}\\left[\\left\\|V_{j}\\right\\|_{F}+\\delta_{\\left\\{\\mathbf{z} \\cdot\\|\\mathbf{z}\\|_{2} \\leq 1\\right\\}}\\left(\\mathbf{z}_{j}\\right)\\right]\n$$\n\nwhere, $\\sigma_{t}(\\cdot)$ is softmax function with temperature $t$, for $k \\in[T]$ defined $\\sigma_{t}(\\mathbf{u})_{k}:=\\exp \\left(t u_{k}\\right) / \\sum_{l=1}^{T} \\exp \\left(t u_{l}\\right)$ and define $\\mathrm{NC}_{\\mu}^{\\mathrm{TF}}\\left(\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}\\right)$ similarly with the sum over $i$ replaced by expectation taken over $(X, \\mathbf{y})$.\nLet $\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}$ be a stationary point of $\\mathrm{NC}_{\\mu_{N}}^{\\mathrm{TF}}\\left(\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}\\right)$. Suppose there exists $C_{V}, B_{V}>0$ such that $\\sum_{j=1}^{R}\\left\\|V_{j}\\right\\|_{F} \\leq C_{V}\\left\\|A^{*}\\right\\|_{F}$, and for all $j \\in[R],\\left\\|V_{j}\\right\\|_{F} \\leq B_{V}$.\nThen with probability at least $1-\\delta$, it holds that\n\n$$\n\\begin{gathered}\n\\frac{1}{m}\\left|\\mathrm{NC}_{\\mu}^{\\mathrm{TF}}\\left(\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}^{\\mathrm{TF}}\\left(\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}\\right) \\mid \\lesssim \\frac{1}{2 m}\\left\\|A^{*}\\right\\|_{F}\\left[\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|\\mathbf{y}_{i}-\\hat{\\mathbf{y}}_{i}\\right\\|_{2}\\left\\|X_{i}\\right\\|_{2}-\\lambda\\right] \\\\\n+C_{V}^{2}\\left\\|A^{*}\\right\\|_{F}^{2} \\sqrt{\\frac{R(m+n) \\log \\left(R(m+n)\\left(C_{V}+B_{V}\\right)\\right) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{gathered}\n$$\n\nProof. To obtain a generalization bound from Theorem 4 for the case of matrix sensing, we set the following problem parameters.\n\n$$\n\\begin{gathered}\n\\ell(Y, \\hat{Y})=\\frac{1}{2}\\|Y-\\hat{Y}\\| \\Longrightarrow(\\alpha, L)=(0,1) \\\\\n\\phi(W)=V X \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right) \\\\\n\\theta(W)=\\|V\\|_{F}+\\delta_{\\mathbf{z} \\in \\mathcal{B}(1)}\n\\end{gathered}\n$$\n\nEstimating $\\Omega_{\\mu_{N}}(\\cdot)$ : Now we move on to compute the polar:\n\n$$\n\\begin{aligned}\n\\Omega_{\\mu_{N}}^{*}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) & =\\Omega_{\\mu_{N}}^{*}\\left(\\frac{1}{\\lambda}\\left(g-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) \\\\\n& =\\sup _{\\|V\\|_{F} \\leq 1,\\|\\mathbf{z}\\| \\leq 1} \\frac{1}{N \\lambda} \\sum_{i=1}^{N}\\left\\langle Y_{i}-\\hat{Y}_{i}, V X_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right\\rangle \\\\\n& =\\sup _{\\|V\\|_{F} \\leq 1,\\|z\\| \\leq 1} \\frac{1}{N \\lambda}\\left\\|\\sum_{i=1}^{N}\\left\\langle\\left(Y_{i}-\\hat{Y}_{i}\\right)\\left(X_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}, V\\right\\rangle\\right\\| \\\\\n& =\\sup _{\\|V\\|_{F} \\leq 1,\\|z\\| \\leq 1} \\frac{1}{N \\lambda}\\left\\|\\sum_{i=1}^{N}\\left\\langle\\left(Y_{i}-\\hat{Y}_{i}\\right)\\left(X_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}, V\\right\\rangle\\right\\| \\\\\n\\Omega_{\\mu_{N}}^{*}\\left(-\\frac{1}{\\lambda} \\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right)\\right) & \\leq \\sup _{\\|z\\| \\leq 1} \\frac{1}{N \\lambda}\\left\\|\\sum_{i=1}^{N}\\left(Y_{i}-\\hat{Y}_{i}\\right)\\left(X_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}\\right\\|_{F} \\\\\n& \\leq \\frac{1}{N \\lambda} \\sum_{i=1}^{N} \\sup _{\\|z\\| \\leq 1}\\left\\|\\left(Y_{i}-\\hat{Y}_{i}\\right)\\left(X_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}\\right\\|_{F} \\\\\n& \\leq \\frac{1}{N \\lambda} \\sum_{i=1}^{N} \\sup _{\\|z\\| \\leq 1}\\left\\|Y_{i}-\\hat{Y}_{i}\\right\\|_{F}\\left\\|\\left(X_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}\\right\\|_{F} \\\\\n& \\leq \\frac{1}{N \\lambda} \\sum_{i=1}^{N} \\sup _{\\|z\\| \\leq 1}\\left\\|Y_{i}-\\hat{Y}_{i}\\right\\|_{F}\\left\\|\\left(X_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}\\right\\|_{F} \\\\\n& \\leq \\frac{1}{N \\lambda} \\sum_{i=1}^{N} \\frac{1}{\\|z\\| \\leq 1}\\left\\|Y_{i}-\\hat{Y}_{i}\\right\\|_{F}\\left\\|\\left(X_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)^{T}\\right\\|_{F} \\\\\n& \\leq \\frac{1}{N \\lambda} \\sum_{i=1}^{N}\\left\\|Y_{i}-\\hat{Y}_{i}\\right\\|_{2}\\left\\|X_{i}\\right\\|_{2}\n\\end{aligned}\n$$\n\nChoose $\\mathcal{F}_{\\theta}$ : From Assumptions 4 suppose that\n\n$$\n\\mathcal{F}_{\\theta}:=\\left\\{(V, \\mathbf{z}):\\|V\\|_{2} \\leq 1,\\|\\mathbf{z}\\|_{2} \\leq 1\\right\\}\n$$\n\nComputing $L_{\\phi}$ : The Lipschtiz constant $L_{\\phi}$ in the function $\\mathcal{F}_{\\theta}$ is $L_{\\phi}:=\\sup _{\\|V\\| \\leq 1,\\|\\mathbf{z}\\| \\leq 1}\\left\\|V(\\cdot) \\sigma\\left((\\cdot)^{T} \\mathbf{z}\\right)\\right\\|_{\\text {Lip }} \\leq$ $\\sup _{\\|V\\| \\leq 1}\\|V\\|=1$.\n\nComputing $r_{\\phi}$ : Clearly, when $r_{\\theta}=\\sqrt{2}$ we have that $\\mathcal{F}_{\\theta} \\subseteq \\mathbb{B}(\\sqrt{2})$.\nChoose $\\mathcal{F}_{\\mathcal{W}}$ : From the corollary's assumptions we have that, $\\mathcal{B}_{R}:=\\left\\{(V, \\mathbf{z}):\\|V\\|_{2} \\leq B_{V},\\|\\mathbf{z}\\|_{2} \\leq 1\\right\\}$; our hypothesis class is defined as\n\n$$\n\\mathcal{F}_{\\mathcal{W}}:=\\left\\{\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}: \\| \\sum_{j=1}^{r} V_{j} \\cdot \\sigma\\left((\\cdot)^{T} \\mathbf{z}_{j}\\right) \\|_{\\mathrm{Lip}} \\leq \\sum_{j=1}^{r}\\left\\|V_{j}\\right\\|_{F} \\leq \\gamma,\\left\\|V_{j}\\right\\|_{F} \\leq B_{V},\\|\\mathbf{z}\\| \\leq 1\\right\\}\n$$\n\nFrom Proposition 2 we have $\\Omega\\left(f_{\\mu}^{*}\\right) \\leq\\left\\|A^{*}\\right\\|_{F}$. We have $\\gamma \\geq \\Omega\\left(f_{\\mu}^{*}\\right) L_{\\phi}=\\left\\|A^{*}\\right\\|_{F}$, then we set $\\gamma=C_{V}\\left\\|A^{*}\\right\\|_{F}$. We have that\n\n$$\n\\begin{gathered}\n\\mathcal{F}_{\\mathcal{W}}:=\\left\\{\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\}: \\| \\sum_{j=1}^{r} V_{j} \\cdot \\sigma\\left((\\cdot)^{T} \\mathbf{z}_{j}\\right) \\|_{\\text {Lip }} \\leq \\sum_{j=1}^{r}\\left\\|V_{j}\\right\\|_{F} \\leq C_{V}\\left\\|A^{*}\\right\\|_{F}\\right. \\\\\n\\left.\\left\\|V_{j}\\right\\|_{F} \\leq B_{V},\\|\\mathbf{z}\\| \\leq 1\\right\\}\n\\end{gathered}\n$$\n\nEstimating $\\epsilon_{0}$ : From the data generating mechanism we have $\\|g\\|_{\\text {Lip }} \\leq\\left\\|A^{*}\\right\\|_{F}, \\sigma_{X}=1, \\sigma_{Y \\mid X}=\\sigma$, then we have the following constants from Theorem 4:\n\n$$\n\\epsilon_{0}=16 \\gamma^{2} \\sigma_{X}^{2} \\min \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\mathrm{Lip}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nthis evaluates to,\n\n$$\n\\epsilon_{0}=16\\left(\\sigma^{2}+1\\right)\\left\\|A^{*}\\right\\|_{F}^{2}\n$$\n\nwhen $C_{V} \\leq \\sqrt{1+\\sigma^{2}}$.\nEstimating $\\epsilon_{1}$ : Similarly, we evaluate\n\n$$\n\\epsilon_{1}=16 \\gamma^{2} \\sigma_{X}^{2} \\max \\left\\{1, \\frac{L}{4}\\left[1+\\frac{\\|g\\|_{\\mathrm{Lip}}^{2}}{\\gamma^{2}}\\left(1+\\frac{\\sigma_{Y \\mid X}^{2}}{\\sigma_{X}^{2}}\\right)\\right]\\right\\}\n$$\n\nobtaining,\n\n$$\n\\epsilon_{1}=16 C_{V}^{2}\\left\\|A^{*}\\right\\|_{F}^{2}\n$$\n\nwhen $C_{V} \\leq \\sqrt{1+\\sigma^{2}}$.\nChoosing the convex set $\\mathcal{C}$ : Consider a convex set $\\mathcal{C}=\\left\\{X=\\left(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{T}\\right):\\left\\|\\mathbf{x}_{j}\\right\\|_{2} \\leq g / \\sqrt{T}\\right\\}$.\nFirst and foremost we need to estimate $\\delta_{\\mathcal{C}}$ for the inequality to hold:\n\n$$\nP\\left(\\cap_{i=1}^{N} \\mathbf{x}_{i} \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}\n$$\n\nThe probability of $\\mathbf{x} \\in \\mathcal{C}=\\mathbb{B}(g)$ is equivalent to saying the probability of the event when $\\|\\mathbf{x}\\|_{2} \\leq g$. Since, $x_{i} \\sim \\mathcal{N}(0,1 / n)$ as a consequence of Bernstein's Inequality (Vershynin, 2018, Corollary 2.8.3) we have that, for any $t \\geq 0$.\n\n$$\nP\\left(\\left|\\|\\mathbf{x}\\|_{2}-1\\right| \\leq t\\right) \\geq 1-2 \\exp \\left(-c n T t^{2}\\right)\n$$\n\nfor some constant $c \\geq 0$. Now we have\n\n$$\nP\\left(\\|\\mathbf{x}\\|_{2} \\leq g / \\sqrt{T}\\right) \\begin{cases}\\geq 1-2 \\exp \\left(-c n(g-1)^{2}\\right) & \\text { if } g \\geq 1 \\\\ \\leq 2 \\exp \\left(-c n(g-1)^{2}\\right) & \\text { otherwise }\\end{cases}\n$$\n\nWe consider the case where $g \\geq 1$. Then we have that\n\n$$\nP\\left(\\cap_{i=1}^{N} \\mathbf{x}_{i} \\in \\mathcal{C}\\right)=P\\left(\\cap_{i=1}^{N}\\|\\mathbf{x}\\|_{2} \\leq g / \\sqrt{T}\\right) \\geq 1-\\underbrace{2 N \\exp \\left(-c n(g-1)^{2}\\right)}_{=\\delta_{\\mathcal{C}}}\n$$\n\nWe have that $\\delta_{\\mathcal{C}}=2 N \\exp \\left(-c n(g-1)^{2}\\right)$.\nEstimating $B_{\\Phi}$ :\n\n$$\n\\begin{aligned}\nB_{\\Phi} & =\\sup _{X \\in \\mathcal{C},\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|\\sum_{j=1}^{r} V_{j} X \\sigma_{t}\\left(X^{T} \\mathbf{z}_{j}\\right)\\right\\|_{2} \\\\\n& \\leq R \\sup _{X \\in \\mathcal{C},\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|V_{j}\\right\\|_{F}\\left\\|X \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right\\|_{2} \\\\\n& \\leq R \\sup _{X \\in \\mathcal{C},\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|V_{j}\\right\\|_{F}\\left\\|X \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right\\|_{2} \\\\\n& \\leq g R B_{V} / \\sqrt{T}\n\\end{aligned}\n$$\n\nEstimating $B_{\\ell}$ : we have\n\n$$\n\\begin{aligned}\nB_{\\ell} & =\\sup _{X \\in \\mathcal{C},\\left\\{\\left(V_{j}, \\mathbf{z}_{j}\\right)\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}}\\left\\|\\sum_{j=1}^{r} V_{j} X \\sigma_{t}\\left(X^{T} \\mathbf{z}_{j}\\right)-A^{*} X B^{*}\\right\\| \\\\\n& =g\\left[R B_{V}+\\left\\|A^{*}\\right\\|_{F}\\right] / \\sqrt{T}\n\\end{aligned}\n$$\n\nEstimating $\\tilde{L}_{\\Phi}$ : We have\n\n$$\n\\tilde{L}_{\\Phi}=g R \\sqrt{B_{V}^{2}+1} / \\sqrt{T}\n$$\n\nEstimating $\\tilde{L}_{\\phi}$ : Similarly we get $\\tilde{L}_{\\phi}=g \\sqrt{B_{V}^{2}+1} / \\sqrt{T}$ as we have only one slice of factor.\nEstimating $\\epsilon_{2}$ : Recall that,\n\n$$\n\\epsilon_{2}=\\max \\left\\{8 B_{\\ell} \\tilde{L}_{\\Phi}, 8 \\tilde{L}_{\\Phi}\\left[B_{\\ell}+B_{\\Phi} L\\right], 32 \\Omega\\left(f_{\\mu}^{*}\\right) \\tilde{L}_{\\phi} \\max \\left\\{B_{\\ell}, L B_{\\Phi}\\right\\}, 4 \\tilde{L}_{\\Phi} B_{\\Phi}\\right\\}\n$$\n\nFrom all the constants computed earlier, we have that,\n\n$$\n\\epsilon_{2}=k_{1} g^{2} R^{2} B_{V}^{2} / T\n$$\n\nfor some constant $k_{1} \\geq 0$.\nNext, we move on estimating $B(\\mathcal{C})$ we need to analyze three terms\nThe First Term is defined via\n\n$$\n\\begin{aligned}\nT_{1}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{\\mathrm{IV}}} & \\left\\|\\int_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2} \\\\\n& \\left.-\\left\\|f_{\\mu}^{*}-\\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\mu}^{2}\\right\\}\n\\end{aligned}\n$$\n\nFrom Lemma 7 we obtain that, taking $g \\geq 1$ and further simplifying, we get,\n\n$$\nT_{1} \\leq c_{1} T^{2} g e^{-c_{2} T g^{2}}\\left[R^{2} \\gamma^{2}+\\left\\|A^{*}\\right\\|_{2}\\right]\n$$\n\nThe Second Term is defined via\n\n$$\n\\begin{aligned}\nT_{2}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{W}, W^{\\prime} \\in \\mathcal{F}_{g}} & \\left\\lvert\\, \\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\phi\\left(W^{\\prime}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}\\right. \\\\\n& \\left.-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\phi\\left(W^{\\prime}\\right)\\right\\rangle_{\\mu} \\mid\n\\end{aligned}\n$$\n\nAs a consequence of Lemma 7 we have\n\n$$\nT_{2} \\leq c_{1} T^{2} g e^{-c_{2} T g^{2}}\\left[\\left\\|A^{*}\\right\\|_{2}+R \\gamma\\right]\n$$\n\nThe Third Term is defined via\n\n$$\n\\begin{aligned}\nT_{3}:=\\sup _{\\left\\{W_{j}\\right\\} \\in \\mathcal{F}_{W}} & \\left\\lvert\\, \\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right) \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}\\right. \\\\\n& \\left.-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right), \\Phi_{r}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\rangle_{\\mu} \\mid\n\\end{aligned}\n$$\n\nAs a consequence of Lemma 7 we have\n\n$$\nT_{3} \\leq c_{1} T^{2} g e^{-c_{2} T g^{2}} R \\gamma\\left[\\left\\|A^{*}\\right\\|_{2}+R \\gamma\\right]\n$$\n\nfor some positive constant, $c_{1}, c_{2}$.\nNow combining $T_{1}, T_{2}$, and $T_{3}$ from equations (437), (439), (441) we obtain that\n\n$$\nB(\\mathcal{C}) \\leq c_{1} T^{2} g e^{-c_{2} T g^{2}}\\left[\\alpha\\left(\\left\\|A^{*}\\right\\|_{2}+R^{2} \\gamma^{2}\\right)+\\left\\|A^{*}\\right\\|_{2}+R \\gamma+R \\gamma\\left[\\left\\|A^{*}\\right\\|_{2}+R \\gamma\\right]\\right]\n$$\n\nWe further upper bound for simplicity as\n\n$$\nB(\\mathcal{C}) \\leq 4 R c_{1} \\sqrt{\\frac{\\log (T)}{T^{5}}} g e^{-c_{2} g^{2}} \\gamma\\left[\\left\\|A^{*}\\right\\|_{2}+\\gamma\\right]\n$$\n\nFrom Theorem 4 we have that\n\n$$\n\\begin{gathered}\n\\frac{1}{m}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\lesssim \\frac{1}{2 m}\\left\\|A^{*}\\right\\|_{F}\\left[\\sup _{\\|x\\| \\leq 1} \\frac{1}{N}\\left\\|\\sum_{i=1}^{N}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{T}\\left(X_{i} \\sigma_{t}\\left(X^{T} \\mathbf{z}\\right)\\right)\\right\\|_{F}-\\lambda\\right] \\\\\n+\\frac{2}{m} R c_{1} T^{2} g e^{-c_{2} T g^{2}} C_{V}\\left[\\left\\|A^{*}\\right\\|_{F}\\right]\\left[\\left\\|A^{*}\\right\\|_{2}+\\gamma\\right]+C_{V}^{2}\\left[\\left\\|A^{*}\\right\\|_{F}\\right] \\\\\n\\times \\sqrt{\\frac{R(m+n) \\log \\left(C_{V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]^{2} k_{1} g^{2} R B_{V} / T\\right) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{gathered}\n$$\n\nholds true w.p at least $1-\\delta-2 N \\exp \\left(-c n_{X}(g-1)^{2}\\right)$.\nNow choose\n\n$$\ng=1+\\tilde{\\mathcal{O}}\\left(\\frac{1}{T} \\log \\left(\\frac{N}{R(m+n)+\\log (1 / \\delta)}\\right)\\right)\n$$\n\nThen we have\n\n$$\n\\frac{2}{m} R c_{1} T^{2} g e^{-c_{2} T g^{2}} C_{V}\\left[\\left\\|A^{*}\\right\\|_{F}\\right]\\left[\\left\\|A^{*}\\right\\|_{2}+\\gamma\\right] \\lesssim \\sqrt{\\frac{R(m+n) \\log \\left(C_{V}^{2}\\left[\\left\\|U^{*}\\right\\|_{F}^{2}+\\left\\|V^{*}\\right\\|_{F}^{2}\\right]^{2} k_{1} g^{2} R B_{V} / T\\right) \\log (N)+\\log (1 / \\delta)}{N}}\n$$\n\nTherefore we can upper bound the middle term to the right most leaving us behind\n\n$$\n\\begin{gathered}\n\\frac{1}{m}\\left|\\mathrm{NC}_{\\mu}\\left(\\left\\{W_{j}\\right\\}\\right)-\\mathrm{NC}_{\\mu_{N}}\\left(\\left\\{W_{j}\\right\\}\\right)\\right| \\lesssim \\frac{1}{2 m}\\left\\|A^{*}\\right\\|_{F}\\left[\\frac{1}{N} \\sum_{i=1}^{N}\\left\\|Y_{i}-\\bar{Y}_{i}\\right\\|_{2}\\left\\|X_{i}\\right\\|_{2}-\\lambda\\right] \\\\\n+C_{V}^{2}\\left[\\left\\|A^{*}\\right\\|_{F}\\right] \\sqrt{\\frac{R(m+n) \\log \\left(R(m+n)\\left(C_{V}+B_{V}\\right)\\right) \\log (N)+\\log (1 / \\delta)}{N}}\n\\end{gathered}\n$$\n\nholds true w.p at least $1-\\delta$.", "tables": {}, "images": {}}, {"section_id": 27, "text": "# D GOODS EVENTS \n\nIn this section, we provide compute the probabilities of events defined in the proof of Theorem 4. Recall the definition of our function classes:\n\n$$\n\\begin{aligned}\n\\mathcal{F}_{\\theta} & :=\\left\\{\\left\\{W_{j}\\right\\}:\\left\\|\\Phi_{R}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\text {Lip }} \\leq \\gamma, \\Theta_{R}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\gamma / L_{\\phi}\\right\\} \\\\\n\\mathcal{F}_{\\mathcal{W}} & :=\\left\\{\\left\\{W_{j}\\right\\}:\\left\\|\\Phi_{R}\\left(\\left\\{W_{j}\\right\\}\\right)\\right\\|_{\\text {Lip }} \\leq \\gamma, \\Theta_{R}\\left(\\left\\{W_{j}\\right\\}\\right) \\leq \\gamma / L_{\\phi}\\right\\} \\\\\n\\mathcal{F}_{\\Phi} & :=\\left\\{\\Phi_{R}(\\zeta): \\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}\\right\\}\n\\end{aligned}\n$$\n\nWe define the below events:\n\n$$\n\\begin{aligned}\n\\mathcal{E}_{c v x}(\\epsilon) & :=\\left\\{\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left|\\mathcal{C}_{\\mu_{N}}\\left(f_{\\zeta}\\right)-\\mathcal{C}_{\\mu}\\left(f_{\\zeta}\\right)\\right| \\leq \\epsilon+B_{n r m}(\\mathcal{C})\\right\\} \\\\\n\\mathcal{E}_{e q l}(\\epsilon) & :=\\left\\{\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu}\\right| \\leq \\epsilon+B_{e q l}(\\mathcal{C})\\right\\} \\\\\n\\mathcal{E}_{p l r}(\\epsilon) & :=\\left\\{\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left|\\Omega_{\\mu_{N}}^{v}\\left(\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)-\\Omega_{\\mu}^{v}\\left(\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)\\right| \\leq \\epsilon+B_{p l r}(\\mathcal{C})\\right\\} \\\\\n\\mathcal{E}_{n r m}(\\epsilon) & :=\\left\\{\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left|\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu_{N}}^{2}-\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu}^{2}\\right| \\leq \\epsilon+B_{n r m}(\\mathcal{C})\\right\\}\n\\end{aligned}\n$$\n\nIn each of the sections below, we discuss the technical analysis to estimate the probability of the events, $\\mathcal{E}_{\\text {cvx }}(\\epsilon)$, $\\mathcal{E}_{e q l}(\\epsilon), \\mathcal{E}_{p l r}(\\epsilon)$ and $\\mathcal{E}_{n r m}(\\epsilon)$.", "tables": {}, "images": {}}, {"section_id": 28, "text": "## D. 1 Concentration of Norms\n\nIn this section, we upper bound the probability of the event, $\\mathcal{E}_{n r m}(\\epsilon)$ through Lemma 8.\nLemma 8 (Concentration of Norms). Consider an $n_{X}$-dimensional sub-Gaussian vector $X \\sim S G\\left(0,\\left(\\sigma_{X}^{2} / n_{X}\\right) I\\right)$, and set of functions $f_{\\zeta}: \\mathbb{R}^{n_{X}} \\rightarrow \\mathbb{R}$ as parameterized by $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$. Let $\\mathcal{C}$ be some convex obeying $P\\left(\\bigcap_{i=1}^{N} X_{i} \\in\\right.$ $\\mathcal{C}) \\geq 1-\\delta_{\\mathcal{C}}$ for i.i.d samples $\\left\\{X_{i}\\right\\}_{i=1}^{N}$. Assume that for any fixed, $\\zeta, \\zeta^{\\prime} \\in \\mathcal{F}_{\\mathcal{W}}$, and fixed $Z \\in \\mathcal{C}$, we have\n\n$$\n\\left\\|f_{\\zeta}(Z)-f_{\\zeta^{\\prime}}(Z)\\right\\| \\leq \\tilde{L}_{\\Phi} d\\left(\\zeta_{1}, \\zeta_{2}\\right) \\text { and }\\left\\|f_{\\zeta}(Z)\\right\\| \\leq B_{\\Phi}\n$$\n\nDenote,\n\n$$\nB_{n r m}(\\mathcal{C}):=\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu}^{2}\\right|\n$$\n\nwhere $\\mathcal{P}_{\\mathcal{C}}(\\cdot)$ denotes the Euclidean projection onto the set $\\mathcal{C}$. Define,\n\n$$\nK:=64 n_{Y} \\gamma^{2} \\sigma_{X}^{2}\n$$\n\nThen for any $\\epsilon \\in[0, K]$,\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu_{N}}^{2}-\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu}^{2}\\right| \\geq \\epsilon+B_{n r m}(\\mathcal{C})\\right) \\\\\n& \\quad \\leq \\delta_{\\mathcal{C}}+c \\exp \\left(\\log \\left(C_{\\mathcal{F}_{\\mathcal{W}}}\\left(\\frac{\\epsilon}{4 \\tilde{L}_{\\Phi} B_{\\Phi}}\\right)\\right)-N \\frac{\\epsilon^{2}}{K^{2}}\\right)\n\\end{aligned}\n$$\n\nfor some positive constant, $c$ and $C_{\\mathcal{F}_{\\mathcal{W}}}(\\nu)$ is the $\\nu$-net covering number of the set $\\mathcal{F}_{\\mathcal{W}}$.\nProof. If $X \\in \\mathbb{R}^{n_{X}} \\sim S G\\left(\\frac{\\sigma_{X}^{2}}{n_{X}} I_{n_{X} \\times n_{X}}\\right)$, The function map, $\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|$ has Lipschitz constant of $\\left\\|f_{\\mu}^{*}\\right\\|_{\\text {Lip }}+$ $\\left\\|f_{\\zeta}\\right\\|_{\\text {Lip }} \\leq 2 \\gamma$; as $f_{\\zeta}, f_{\\mu}^{*} \\in \\mathcal{F}_{\\Phi}$. Therefore from Theorem 5.1.4 in Vershynin (2018) we have that, $f_{\\mu}^{*}(X)-f_{\\zeta}(X) \\sim$ $S G\\left(4 \\gamma^{2} \\sigma_{X}^{2} I_{n_{Y} \\times n_{Y}}\\right)$. Thus, $\\left\\|f_{\\mu}^{*}(X)-f_{\\zeta}(X)\\right\\|^{2} \\sim S E\\left(4 n_{Y} \\gamma^{2} \\sigma_{X}^{2}\\right)$\nNow, applying the concentration inequality for sub-exponential from Theorem 2.8.1 Vershynin (2018) for a fixed $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$, we have that\n\n$$\n\\mathbb{P}\\left(\\left|\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu_{N}}^{2}-\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu}^{2}\\right| \\geq \\epsilon\\right) \\leq C \\exp \\left(-N \\min \\left\\{\\frac{\\epsilon^{2}}{16 n_{Y}^{2} \\gamma^{4} \\sigma_{X}^{4}}, \\frac{\\epsilon}{4 n_{Y} \\gamma^{2} \\sigma_{X}^{2}}\\right\\}\\right)\n$$\n\nfor some positive constant, $C \\geq 0$. We use Lemma 14 for applying the concentration bounds. Now set\n\n$$\ng_{\\theta}=\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|^{2}\n$$\n\nWe need to check if the function, $g$, is Lipschitz on some metric and convex set $\\mathcal{C} \\subseteq \\mathbb{R}^{n_{X}}$, choose for any $Z \\in \\mathcal{C}$. We have $P\\left(\\bigcap_{i=1}^{N} X_{i} \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}$. Recall that\n\n1. $\\forall \\zeta_{1}, \\zeta_{2} \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|f_{\\zeta_{1}}(Z)-f_{\\zeta_{2}}(Z)\\right\\| \\leq \\tilde{L}_{\\Phi} d\\left(\\zeta_{1}, \\zeta_{2}\\right)$, for all $Z \\in \\mathcal{C}$.\n2. $\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|f_{\\zeta}(Z)\\right\\| \\leq B_{\\Phi}$, for all $Z \\in \\mathcal{C}$.\n3. For a fixed $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$,\n\n$$\n\\left|\\mathbb{E}\\left[\\left\\|f_{\\mu}^{*}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)-f_{\\zeta}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)\\right\\|^{2}-\\left\\|f_{\\mu}^{*}(X)-f_{\\zeta}(X)\\right\\|^{2}\\right]\\right| \\leq B_{n r m}(\\mathcal{C})\n$$\n\nBy exploiting the above items we have\n\n$$\n\\begin{aligned}\n\\left|g_{\\theta_{1}}-g_{\\theta_{2}}\\right| & =\\left|\\left\\|f_{\\mu}^{*}-f_{\\zeta_{1}}\\right\\|^{2}-\\left\\|f_{\\mu}^{*}-f_{\\zeta_{2}}\\right\\|^{2}\\right|,=\\left|\\left\\langle 2 f_{\\mu}^{*}-\\left(f_{\\zeta_{1}}+f_{\\zeta_{2}}\\right), f_{\\zeta_{1}}-f_{\\zeta_{2}}\\right\\rangle\\right| \\\\\n& \\leq\\left\\|2 f_{\\mu}^{*}-\\left(f_{\\zeta_{1}}+f_{\\zeta_{2}}\\right)\\right\\|^{\\circ}\\left\\|f_{\\zeta_{1}}-f_{\\zeta_{2}}\\right\\| \\\\\n& \\leq 4 \\tilde{L}_{\\Phi} B_{\\Phi} d\\left(\\zeta_{1}, \\zeta_{2}\\right)\n\\end{aligned}\n$$\n\nThen we have that for covering number $C_{\\mathcal{F}_{\\mathcal{W}}}(\\nu)=\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}}, d(.,.\\right), \\nu)$, and $K=4 \\tilde{L}_{\\Phi} B_{\\Phi}$\n\n$$\n\\begin{gathered}\n\\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu_{N}}^{2}-\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu}^{2}\\right| \\geq \\epsilon+B_{n r m}\\right) \\\\\n\\leq \\delta_{\\mathcal{C}}+C \\exp \\left(\\log \\left(C_{\\mathcal{F}_{\\mathcal{W}}}\\left(\\frac{\\epsilon}{4 \\tilde{L}_{\\Phi} B_{\\Phi}}\\right)\\right)-N \\min \\left\\{\\frac{\\epsilon^{2}}{256 n_{Y}^{2} \\gamma^{4} \\sigma_{X}^{4}}, \\frac{\\epsilon}{64 n_{Y} \\gamma^{2} \\sigma_{X}^{2}}\\right\\}\\right)\n\\end{gathered}\n$$\n\nWe conclude the result by choosing $\\epsilon \\in\\left[0,64 n_{Y} \\gamma^{2} \\sigma_{X}^{2}\\right]$.", "tables": {}, "images": {}}, {"section_id": 29, "text": "# D. 2 Concentration of Convex functions \n\nIn this section, we upper bound the probability of the event, $\\mathcal{E}_{\\text {cva }}(\\epsilon)$ through Lemma 9. In this, we consider strongly and smooth convex function (see assumption 3) through Taylor expansion of the function is always bounded quadratically. Lemma 8 plays an important role in establishing Lemma 9.\nLemma 9 (Concentration of Convex functions). Consider an $n_{X}$-dimensional sub-Gaussian vector $X \\sim$ $S G\\left(0,\\left(\\sigma_{X}^{2} / n_{X}\\right) I\\right)$, and set of functions $f_{\\zeta}: \\mathbb{R}^{n_{X}} \\rightarrow \\mathbb{R}$ as parameterized by $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$. Let $\\mathcal{C}$ be some convex obeying $P\\left(\\bigcap_{i=1}^{N} X_{i} \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}$ for i.i.d samples $\\left\\{X_{i}\\right\\}_{i=1}^{N}$. Assume that for any fixed, $\\zeta, \\zeta^{\\prime} \\in \\mathcal{F}_{\\mathcal{W}}$, and fixed $Z \\in \\mathcal{C}$, we have\n\n$$\n\\left\\|f_{\\zeta}(Z)-f_{\\zeta^{\\prime}}(Z)\\right\\| \\leq \\tilde{L}_{\\Phi} d\\left(\\zeta_{1}, \\zeta_{2}\\right) \\text { and }\\left\\|f_{\\zeta}(Z)\\right\\| \\leq B_{\\Phi}\n$$\n\nDenote\n\n$$\nB_{n r m}(\\mathcal{C}):=\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\|f_{\\mu}^{*} \\circ \\mathcal{P}_{\\mathcal{C}}-f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\|_{\\mu}^{2}-\\left\\|f_{\\mu}^{*}-f_{\\zeta}\\right\\|_{\\mu}^{2}\\right|\n$$\n\nwhere $\\mathcal{P}_{\\mathcal{C}}(\\cdot)$ denotes the Euclidean projection onto the set $\\mathcal{C}$. Define\n\n$$\nK:=n_{Y} L\\left[\\left(\\gamma^{2}+\\|g\\|_{L i p}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{L i p}^{2} \\sigma_{Y \\mid X}^{2}\\right]\n$$\n\nThen for any $\\epsilon \\in[0, K]$,\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|C_{\\mu_{N}}\\left(f_{\\zeta}\\right)-C_{\\mu}\\left(f_{\\zeta}\\right)\\right| \\geq \\epsilon+B_{n r m}(\\mathcal{C})\\right) \\\\\n\\leq & \\delta_{\\mathcal{C}}+2 \\exp \\left(\\log \\left(C_{\\mathcal{F}_{\\mathcal{W}}}\\left(\\frac{\\epsilon}{2 B_{\\ell} \\tilde{L}_{\\Phi}}\\right)\\right)-c N\\left(\\frac{\\epsilon}{K}\\right)^{2}\\right)\n\\end{aligned}\n$$\n\nfor some positive constant, $c$ and $C_{\\mathcal{F}_{\\mathcal{W}}}(\\nu)$ is the $\\nu$-net covering number of the set $\\mathcal{F}_{\\mathcal{W}}$.\n\nProof. Recall the definitions of the convex functions:\n\n$$\nC_{\\mu_{N}}(f):=\\ell(g, f)_{\\mu_{N}}+\\lambda \\Omega(f), \\text { and } C_{\\mu}(f):=\\ell(g, f)_{\\mu}+\\lambda \\Omega(f)\n$$\n\nThe difference between these two terms is\n\n$$\n\\left|C_{\\mu_{N}}(f)-C_{\\mu}(f)\\right|=\\left|\\ell(g, f)_{\\mu_{N}}-\\ell(g, f)_{\\mu}\\right|\n$$\n\nFrom assumption $3, \\ell(.,$.$) is second-order differentiable in the second argument. By 2nd-order Taylor's theorem,$ we have\n\n$$\n\\ell(Y, \\hat{Y})=\\ell\\left(Y, \\hat{Y}_{0}\\right)+\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(Y, \\hat{Y}_{0}\\right), \\hat{Y}-\\hat{Y}_{0}\\right\\rangle+\\left\\langle\\int_{0}^{1} t \\nabla_{\\hat{Y}}^{2} \\ell\\left(Y, \\hat{Y}_{0}+t\\left(\\hat{Y}-\\hat{Y}_{0}\\right)\\right) d t,\\left(\\hat{Y}-\\hat{Y}_{0}\\right)\\left(\\hat{Y}-\\hat{Y}_{0}\\right)^{T}\\right\\rangle\n$$\n\nNow choose $Y=\\hat{Y}_{0}=g(X(\\omega), E(\\omega))$, and $\\hat{Y}=f_{\\zeta}(X(\\omega))$. As $\\ell(Y, Y)=0$, and $\\nabla_{\\hat{Y}} \\ell(Y, Y)=\\mathbf{0}$. Plugging these parameters in the Taylor expansion we have that (ignoring the inputs, $(X(\\omega), E(\\omega))$ for simplicity),\n\n$$\n\\ell(g, f)=\\left\\langle\\int_{0}^{1} t \\nabla_{\\hat{Y}}^{2} \\ell\\left(g, g+t\\left(f_{\\zeta}-g\\right)\\right) d t,\\left(f_{\\zeta}-g\\right)\\left(f_{\\zeta}-g\\right)^{T}\\right\\rangle\n$$\n\nNow we apply expectation over the measure $\\mu_{N}$, and $\\mu$ respectively on the above equality. Then we have that\n\n$$\n\\begin{aligned}\n\\ell\\left(g, f_{\\zeta}\\right)_{\\mu} & =\\left\\langle\\int_{0}^{1} t \\nabla_{\\hat{Y}}^{2} \\ell\\left(g, g+t\\left(f_{\\zeta}-g\\right)\\right) d t,\\left(f_{\\zeta}-g\\right)\\left(f_{\\zeta}-g\\right)^{T}\\right\\rangle_{\\mu} \\\\\n\\ell\\left(g, f_{\\zeta}\\right)_{\\mu_{N}} & =\\left\\langle\\int_{0}^{1} t \\nabla_{\\hat{Y}}^{2} \\ell\\left(g, g+t\\left(f_{\\zeta}-g\\right)\\right) d t,\\left(f_{\\zeta}-g\\right)\\left(f_{\\zeta}-g\\right)^{T}\\right\\rangle_{\\mu_{N}}\n\\end{aligned}\n$$\n\nSince $f_{\\zeta}$ and $g$ are Lipschitz functions and the inputs are sub-Gaussian, we have that $f_{\\zeta}-g$ is a sub-Gaussian vector. As a consequence of Lemma 2.7.6 from Vershynin (2018) we obtain that $\\left(f_{\\zeta}-g\\right)\\left(f_{\\zeta}-g\\right)^{T}$ follows a sub-exponential distribution, whose concentration is well-studied.\nAs a consequence of assumption 3 the hessian is bounded, i.e, $\\alpha I \\preceq \\nabla_{\\hat{Y}}^{2} \\ell(.,.) \\preceq L I$. We can argue that the product of a bounded RV and sub-exponential RV is sub-exponential. Recall Item (iii) from Proposition 2.7.1 of Vershynin (2018). The random variable $Z$ is sub-exponential iff\n\n$$\n\\mathbb{E}_{Z}\\left[e^{\\lambda|Z|}\\right] \\leq e^{\\lambda K} ; \\forall \\lambda \\in[0,1 / K]\n$$\n\nfor some positive constant, $K \\geq 0$.\nWe now verify if $\\left\\langle H(\\mathbf{x}, \\mathbf{z}), \\mathbf{x} \\mathbf{x}^{T}\\right\\rangle$ is sub-exponential. Given that $\\mathbf{x} \\sim S G\\left(\\sigma_{X}^{2} / n_{x} I_{n_{x} \\times n_{x}}\\right), \\mathbf{z}$ is a R.V. Suppose $A \\preceq H(\\mathbf{x}, \\mathbf{z}) \\preceq B$ a.s. Then we have that\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{\\mathbf{x}, \\mathbf{z}}\\left[e^{\\lambda\\left|\\left\\langle H(\\mathbf{x}, \\mathbf{z}), \\mathbf{x x}^{T}\\right\\rangle\\right|}\\right] & \\leq \\mathbb{E}_{\\mathbf{x}, \\mathbf{z}}\\left[e^{\\lambda\\|H(\\mathbf{x}, \\mathbf{z})\\|_{2}\\left\\|\\mathbf{x x}^{T}\\right\\|_{2}}\\right] \\\\\n& \\leq \\mathbb{E}_{\\mathbf{x}, \\mathbf{z}}\\left[e^{\\lambda \\max \\{\\rho(A), \\rho(B)\\}\\|\\mathbf{x}\\|_{2}^{2}}\\right]\n\\end{aligned}\n$$\n\nwhere, $\\rho(A)$ is the spectral radius of the matrix, $A$. Since, $\\mathbf{x} \\sim S G\\left(\\sigma_{X}^{2} / n_{x} I_{n_{x} \\times n_{x}}\\right)$, we have $\\|\\mathbf{x}\\|^{2} \\sim S E\\left(\\sigma_{X}^{2}\\right)$. Then,\n\n$$\n\\mathbb{E}_{\\mathbf{x}, \\mathbf{z}}\\left[e^{\\lambda\\left|\\left\\langle H(\\mathbf{x}, \\mathbf{z}), \\mathbf{x x}^{T}\\right\\rangle\\right|}\\right] \\leq e^{\\lambda \\max \\{\\rho(A), \\rho(B)\\} \\sigma_{X}^{2}}\n$$\n\nimplies that $\\left\\langle H(\\mathbf{x}, \\mathbf{z}), \\mathbf{x x}^{T}\\right\\rangle \\sim S E\\left(\\max \\{\\rho(A), \\rho(B)\\} \\sigma_{X}^{2}\\right)$. From this analysis we have\n\n$$\n\\begin{aligned}\n& \\left\\langle\\int_{0}^{1} t \\nabla_{\\hat{Y}}^{2} \\ell\\left(g, g+t\\left(f_{\\zeta}-g\\right)\\right) d t, \\quad \\underbrace{\\left(f_{\\zeta}-g\\right)\\left(f_{\\zeta}-g\\right)^{T}}_{\\sim S E\\left(\\left[\\left|\\left\\|f_{\\zeta}\\right\\|_{\\ell_{\\mathrm{cp}}}^{2}+\\left\\|g\\right\\|_{\\ell_{\\mathrm{cp}}}^{2}\\right) \\sigma_{X}^{2}+\\left\\|g\\right\\|_{\\ell_{\\mathrm{cp}}}^{2} \\sigma_{\\hat{Y} \\mid X}^{2}\\right] I_{n_{y} \\times n_{y}}\\right)}\n\\end{aligned}\n$$\n\n$$\n\\sim S E\\left(n_{Y} \\frac{L}{2}\\left[\\left(\\left\\|f_{\\zeta}\\right\\|_{\\mathrm{Lip}}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{Y \\mid X}^{2}\\right]\\right)\n$$\n\nFor convex functions, we know that $0 \\leq \\alpha \\leq L$. As a consequence, we have $\\frac{\\alpha}{2} I \\preceq \\int_{0}^{1} t \\nabla_{\\hat{Y}}^{2} \\ell\\left(g, g+t\\left(f_{\\zeta}-g\\right)\\right) d t \\preceq \\frac{t}{2} I$. Now we apply sub-exponential concentration for a fixed $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$, yielding\n\n$$\n\\begin{gathered}\n\\mathbb{P}\\left(\\left|C_{\\mu_{N}}\\left(f_{\\zeta}\\right)-C_{\\mu}\\left(f_{\\zeta}\\right)\\right| \\geq \\epsilon\\right) \\\\\n\\leq 2 \\exp (-c N \\min \\left\\{\\left(\\frac{2 \\epsilon}{n_{Y} L\\left[\\left(\\gamma^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{Y \\mid X}^{2}\\right.}\\right)^{2}\\right. \\\\\n\\left.\\frac{2 \\epsilon}{n_{Y} L\\left[\\left(\\gamma^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{Y \\mid X}^{2}\\right]}\\right)\n\\end{gathered}\n$$\n\nfor some positive constant, $c \\geq 0$.\nNext, we move on to obtain a uniform concentration for all $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$. Now we apply covering argument from Lemma 14, and set\n\n$$\ng_{\\theta}=\\ell\\left(g, f_{\\zeta}\\right)\n$$\n\nWe need to check if the function, $g$, is Lipschitz on some metric and convex set $\\mathcal{C} \\subseteq \\mathbb{R}^{n_{X}}$, choose for any $Z \\in \\mathcal{C}$. We have $P(\\bigcap_{i=1}^{N} X_{i} \\in \\mathcal{C}) \\geq 1-\\delta_{\\mathcal{C}}$. Recall that\n\n1. $\\forall \\zeta_{1}, \\zeta_{2} \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|f_{\\zeta_{1}}(Z)-f_{\\zeta_{2}}(Z)\\right\\| \\leq \\tilde{L}_{\\Phi} d\\left(\\zeta_{1}, \\zeta_{2}\\right)$, for all $Z \\in \\mathcal{C}$.\n2. $\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(g(Z), f_{\\zeta}(Z)\\right)\\right\\| \\leq B_{\\ell}$, for all $Z \\in \\mathcal{C}$.\n3. For a fixed $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$,\n\n$$\n\\left|\\mathbb{E}\\left[\\left\\|f_{\\mu}^{*}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)-f_{\\zeta}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)\\right\\|^{2}-\\left\\|f_{\\mu}^{*}(X)-f_{\\zeta}(X)\\right\\|^{2}\\right]\\right| \\leq B_{n r m}(\\mathcal{C})\n$$\n\nFrom Taylor expansion we have that,\n\n$$\n\\begin{aligned}\n\\left|g_{\\theta_{1}}-g_{\\theta_{2}}\\right| & =\\left|\\left(\\int_{t} \\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}+t\\left(f_{\\zeta_{2}}-f_{\\zeta_{1}}\\right)\\right) d t, f_{\\zeta_{1}}-f_{\\zeta_{2}}\\right)\\right| \\\\\n& \\leq\\left\\|\\int_{t} \\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}+t\\left(f_{\\zeta_{2}}-f_{\\zeta_{1}}\\right)\\right) d t\\right\\|\\left\\|f_{\\zeta_{2}}-f_{\\zeta_{1}}\\right\\| \\\\\n& \\leq B_{\\ell}\\left\\|f_{\\zeta_{2}}-f_{\\zeta_{1}}\\right\\| \\\\\n& \\leq B_{\\ell} \\tilde{L}_{\\Phi} d\\left(\\zeta_{1}, \\zeta_{2}\\right)\n\\end{aligned}\n$$\n\nFrom Lemma 14 we have\n\n$$\n\\begin{gathered}\n\\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|C_{\\mu_{N}}\\left(f_{\\zeta}\\right)-C_{\\mu}\\left(f_{\\zeta}\\right)\\right| \\geq \\epsilon\\right) \\\\\n\\leq \\delta_{\\mathcal{C}}+2 \\exp \\left(\\log \\left(C_{\\mathcal{F}_{\\mathcal{W}}}\\left(\\frac{\\epsilon}{2 B_{\\ell} \\tilde{L}_{\\Phi}}\\right)\\right)-c N \\min \\left\\{\\left(\\frac{\\epsilon}{n_{Y} L\\left[\\left(\\gamma^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{Y \\mid X}^{2}\\right.}\\right)^{2}\\right.\\right. \\\\\n\\left.\\left.\\frac{\\epsilon}{n_{Y} L\\left[\\left(\\gamma^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{Y \\mid X}^{2}\\right.}\\right\\}\\right)\n\\end{gathered}\n$$\n\nfor some positive constant, $c$. Now restrict $\\epsilon \\in\\left[0, n_{Y} L\\left[\\left(\\gamma^{2}+\\|g\\|_{\\mathrm{Lip}}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{Y \\mid X}^{2}\\right]\\right]$. This completes our proof.", "tables": {}, "images": {}}, {"section_id": 30, "text": "# D. 3 Concentration of Equilibria \n\nIn this section, we upper bound the probability of the event, $\\mathcal{E}_{e q t}(\\epsilon)$ through Lemma 10. We first present the concentration of bi-Lipschtiz functions for sub-Gaussian inputs.\nProposition 4. Let $X \\sim S G\\left(\\frac{\\sigma_{X}^{2}}{n_{x}} I_{n_{x} \\times n_{x}}\\right)$ and $Y \\mid X \\sim S G\\left(\\frac{\\sigma_{Y \\mid X}^{2}}{n_{y}} I_{n_{y} \\times n_{y}}\\right)$, where $\\sigma_{Y \\mid X}$ is independent of $X=x$, then for any Lipschitz function, $\\phi: \\mathcal{Z} \\rightarrow \\mathbb{R}$ and a function, $f: \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathcal{Z}$, satisfies the following. Then $\\phi(f(X, Y)) \\sim S G\\left(4\\|\\phi\\|_{\\text {Lip }}^{2}\\|f\\|_{\\text {Lip }}^{2}\\left[\\sigma_{X}^{2}+\\sigma_{Y \\mid X}^{2}\\right]\\right)$.\n\n$$\n\\left\\|f\\left(X_{2}, Y_{2}\\right)-f\\left(X_{1}, Y_{1}\\right)\\right\\| \\leq\\|f\\|_{\\text {Lip }}\\left[\\left\\|X_{2}-X_{1}\\right\\|+\\left\\|Y_{2}-Y_{1}\\right\\|\\right]\n$$\n\nProof. Let us compute the moments of the random variable $\\phi(f(X, Y))$ for any Lipschitz function, $\\phi: \\mathcal{Z} \\rightarrow \\mathbb{R}$.\nBy symmetrization, we have\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{X, Y}[\\exp (\\lambda[\\phi(f(X, Y))-\\mathbb{E}_{X, Y}[\\phi(f(X, Y))]])] \\\\\n= & \\mathbb{E}_{X, Y}\\left[\\exp \\left(\\lambda\\left[\\phi(f(X, Y))-\\mathbb{E}_{X^{\\prime}, Y^{\\prime}}\\left[\\phi\\left(f\\left(X^{\\prime}, Y^{\\prime}\\right)\\right)\\right]\\right)\\right]\\right. \\\\\n= & \\mathbb{E}_{X, Y}\\left[\\exp \\left(\\lambda \\mathbb{E}_{X^{\\prime}, Y^{\\prime}}[\\phi(f(X, Y))-\\phi\\left(f\\left(X^{\\prime}, Y^{\\prime}\\right)\\right)\\right]\\right)\\right]\n\\end{aligned}\n$$\n\nBy Jensen's inequality we have\n\n$$\n\\leq \\mathbb{E}_{X, Y, X^{\\prime}, Y^{\\prime}}\\left[\\exp \\left(\\lambda\\left[\\phi\\left(f(X, Y)\\right)-\\phi\\left(f\\left(X^{\\prime}, Y^{\\prime}\\right)\\right)\\right]\\right)\\right]\n$$\n\nBy Lipschitz continuity we have\n\n$$\n\\leq \\mathbb{E}_{X, Y, X^{\\prime}, Y^{\\prime}}\\left[\\exp \\left(\\|\\phi\\|_{\\text {Lip }} \\lambda\\left[\\|f(X, Y)-f\\left(X^{\\prime}, Y^{\\prime}\\right)\\right\\|\\right]\\right.\\right.\n$$\n\nBy construction, we have.\n\n$$\n\\leq \\mathbb{E}_{X, Y, X^{\\prime}, Y^{\\prime}}\\left[\\exp \\left(\\|\\phi\\|_{\\text {Lip }}\\|f\\|_{\\text {Lip }} \\lambda\\left[\\left\\|X-X^{\\prime}\\right\\|+\\left\\|Y-Y^{\\prime}\\right\\|\\right]\\right.\\right.\n$$\n\nBy Cauchy-Schwartz's inequality we have\n\n$$\n\\leq \\mathbb{E}_{X, Y, X^{\\prime}, Y^{\\prime}}\\left[\\exp \\left(\\|\\phi\\|_{\\text {Lip }}\\|f\\|_{\\text {Lip }} \\lambda\\left[\\|X\\|+\\left\\|X^{\\prime}\\right\\|+\\left\\|Y\\|+\\left\\|Y^{\\prime}\\right\\|\\right]\\right.\\right.\n$$\n\nAs the symmetrized random variables are independent,\n\n$$\n\\leq \\mathbb{E}_{X, Y}\\left[\\exp \\left(2\\|\\phi\\|_{\\text {Lip }}\\|f\\|_{\\text {Lip }} \\lambda\\left[\\|X\\|+\\left\\|Y^{\\prime}\\right\\|\\right]\\right]\n$$\n\nNow perform conditional expectation,\n\n$$\n\\begin{gathered}\n\\leq \\mathbb{E}_{X} \\mathbb{E}_{Y \\mid X}\\left[\\exp \\left(2\\|\\phi\\|_{\\text {Lip }}\\|f\\|_{\\text {Lip }} \\lambda\\left[\\|X\\|+\\|Y\\|\\right]\\right]\\right. \\\\\n\\leq \\mathbb{E}_{X} \\exp \\left(2\\|\\phi\\|_{\\text {Lip }}\\|f\\|_{\\text {Lip }} \\lambda[\\|X\\|]\\right) \\mathbb{E}_{Y \\mid X}\\left[\\exp \\left(2\\|\\phi\\|_{\\text {Lip }}\\|f\\|_{\\text {Lip }} \\lambda[\\|Y\\|\\right]\\right] \\\\\n\\leq \\exp \\left(\\frac{\\lambda^{2}}{2} 4\\|\\phi\\|_{\\text {Lip }}^{2}\\|f\\|_{\\text {Lip }}^{2}\\left[\\sigma_{X}^{2}+\\sigma_{Y \\mid X}^{2}\\right]+\\lambda\\left[\\mathbb{E}_{X}[\\|X\\|]+\\mathbb{E}_{Y \\mid X}[\\|Y\\|\\right]\\right]\\right) \\\\\n\\leq K \\exp \\left(\\frac{\\lambda^{2}}{2} 4\\|\\phi\\|_{\\text {Lip }}^{2}\\|f\\|_{\\text {Lip }}^{2}\\left[\\sigma_{X}^{2}+\\sigma_{Y \\mid X}^{2}\\right]\\right)\n\\end{gathered}\n$$\n\nfor some constant, $K \\geq 0$.\nThis implies that, $\\phi(f(X, Y)) \\sim S G\\left(4\\|\\phi\\|_{\\text {Lip }}^{2}\\|f\\|_{\\text {Lip }}^{2}\\left[\\sigma_{X}^{2}+\\sigma_{Y \\mid X}^{2}\\right]\\right)$.\n\nWith the above result, we now state and prove the probability of the event, $\\mathcal{E}_{e q t}(\\epsilon)$.\n\nLemma 10 (Concentration of Equilibria). Consider an $n_{X}$-dimensional sub-Gaussian vector $X \\sim$ $S G\\left(0,\\left(\\sigma_{X}^{2} / n_{X}\\right) I\\right)$, and set of functions $f_{\\zeta}: \\mathbb{R}^{n_{X}} \\rightarrow \\mathbb{R}$ as parameterized by $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$. Let $\\mathcal{C}$ be some convex obeying $P\\left(\\bigcap_{i=1}^{N} X_{i} \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}$ for i.i.d samples $\\left\\{X_{i}\\right\\}_{i=1}^{N}$. Assume that for any fixed, $\\zeta_{1}, \\zeta_{2} \\in \\mathcal{F}_{\\mathcal{W}}$, and fixed $Z \\in \\mathcal{C}$, we have\n\n$$\n\\begin{gathered}\n\\left\\|\\nabla_{\\hat{Y}} \\ell(g(Z), f_{\\zeta}(Z))\\right\\| \\leq B_{\\ell},\\left\\|f_{\\zeta}(Z)\\right\\| \\leq B_{\\Phi} \\text {, and } \\\\\n\\left\\|f_{\\zeta}(Z)-f_{\\zeta^{\\prime}}(Z)\\right\\| \\leq \\hat{L}_{\\Phi} d\\left(\\zeta, \\zeta^{\\prime}\\right)\n\\end{gathered}\n$$\n\nIn addition, we have that,\n\n$$\n\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\mathbb{E}\\left[\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right), f_{\\zeta^{\\prime}} \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right]\\right|=B_{e q l}(\\mathcal{C})\n$$\n\nDefine\n\n$$\nK:=4 n_{g} \\gamma\\left\\|\\nabla_{\\hat{Y}} \\ell\\right\\|_{L i p} \\sigma_{X} \\sqrt{\\left(\\gamma^{2}+\\|g\\|_{L i p}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{L i p}^{2} \\sigma_{E \\mid X}^{2}}\n$$\n\nThen for any $\\epsilon \\in[0, K]$,\n\n$$\n\\begin{gathered}\n\\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu}\\right| \\geq \\epsilon+B_{e q l}(\\mathcal{C})\\right) \\leq \\\\\n\\delta_{\\mathcal{C}}+c \\exp \\left(\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\mathcal{W}}}\\left(\\frac{\\epsilon}{2 \\hat{L}_{\\Phi}\\left[B_{\\ell}+B_{\\Phi} L\\right]}\\right)\\right)-N \\frac{\\epsilon^{2}}{K^{2}}\\right)\n\\end{gathered}\n$$\n\nfor some positive constant, $c$ and $C_{\\mathcal{F}_{\\mathcal{W}}}(\\nu)$ is the $\\nu$-net covering number of the set $\\mathcal{F}_{\\mathcal{W}}$..\n\nProof. From Assumptions 1-5, we have that for any $g_{1}, f_{1}, g_{2}, f_{2} \\in L^{2}(\\mu)$\n\n$$\n\\begin{gathered}\n\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(g_{2}(X(\\omega), E(\\omega)), f_{2}(X(\\omega))\\right)-\\nabla_{\\hat{Y}} \\ell\\left(g_{1}(X(\\omega), E(\\omega)), f_{1}(X(\\omega))\\right)\\right\\| \\\\\n\\leq\\left\\|\\nabla_{\\hat{Y}} \\ell\\right\\|_{\\text {Lip }}\\left[\\left\\|g_{2}(X(\\omega), E(\\omega))-g_{1}(X(\\omega), E(\\omega))\\right\\|+\\left\\|f_{2}(X(\\omega))-f_{1}(X(\\omega))\\right\\|\\right]\n\\end{gathered}\n$$\n\nSince $X(\\omega)$ and $E(\\omega)$ are Lipschitz concentrated R.Vs, it holds that\n\n$$\n\\begin{gathered}\ng(X, E)|E \\sim S G\\left(\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{X}^{2} I_{n_{y} \\times n_{y}}\\right), g(X, E)|X \\sim S G\\left(\\|g\\|_{\\mathrm{Lip}}^{2} \\sigma_{E \\mid X}^{2} I_{n_{y} \\times n_{y}}\\right) \\\\\n\\text { and } f_{\\zeta}(X) \\sim S G\\left(\\left\\|f_{\\zeta}\\right\\|_{\\mathrm{Lip}}^{2} \\sigma_{X}^{2} I_{n_{y} \\times n_{y}}\\right)\n\\end{gathered}\n$$\n\nFrom Proposition 4 we have\n\n$$\n\\begin{gathered}\n\\nabla_{\\hat{Y}} \\ell(g(X(\\omega), E(\\omega)), f_{\\zeta}(X(\\omega))) \\sim \\\\\nS G\\left(4\\left\\|\\nabla_{\\hat{Y}} \\ell\\right\\|_{\\text {Lip }}^{2}\\left[\\left(\\left\\|f_{\\zeta}\\right\\|_{\\text {Lip }}^{2}+\\|g\\|_{\\text {Lip }}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\text {Lip }}^{2} \\sigma_{E \\mid X}^{2}\\right] I_{n_{y}}\\right)\n\\end{gathered}\n$$\n\nNow we have the inner product between two sub-Gaussian random variables from Proposition 7 we have that the result is sub-exponential, i.e.,\n\n$$\n\\begin{aligned}\n& \\left\\langle\\begin{array}{c}\n\\nabla_{\\hat{Y}} \\ell(g(X(\\omega), E(\\omega)), f_{\\zeta}(X(\\omega))) \\\\\n\\sim S G\\left(4\\left\\|\\nabla_{\\hat{Y}} \\ell\\right\\|_{\\text {Lip }}^{2}\\left[\\left(\\left\\|f_{\\zeta}\\right\\|_{\\text {Lip }}^{2}+\\|g\\|_{\\text {Lip }}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\text {Lip }}^{2} \\sigma_{E \\mid X}^{2}\\right] I_{n_{y} \\times n_{y}}\\right) \\sim S G\\left(\\left\\|f_{\\zeta}\\right\\|_{\\text {Lip }}^{2} \\sigma_{X}^{2} I_{n_{y}}\\right\\rangle \\\\\n& \\sim S E\\left(2 n_{y}\\left\\|\\nabla_{\\hat{Y}} \\ell\\right\\|_{\\text {Lip }}\\left\\|f_{\\zeta}\\right\\|_{\\text {Lip }} \\sigma_{X} \\sqrt{\\left(\\left\\|f_{\\zeta}\\right\\|_{\\text {Lip }}^{2}+\\|g\\|_{\\text {Lip }}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\text {Lip }}^{2} \\sigma_{E \\mid X}^{2}}\\right)\n\\end{aligned}\n$$\n\nThe class of functions, $f_{\\zeta}$ for $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$, has bounded Lipschitz constant $\\gamma$. As a consequence of the sub-exponential concentration bound from Theorem 2.8.1 in Vershynin (2018), we have that for a fixed $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$,\n\n$$\n\\mathbb{P}\\left(\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}, f_{\\zeta}\\right\\rangle_{\\mu}\\right| \\geq \\epsilon\\right) \\leq C \\exp \\left(-N \\min \\left\\{\\frac{\\epsilon^{2}}{K^{2}}, \\frac{\\epsilon}{K}\\right\\}\\right)\n$$\n\nwhere $K:=2 n_{y} \\gamma\\left\\|\\nabla_{\\hat{Y}} \\ell\\right\\|_{\\text {Lip }} \\sigma_{X} \\sqrt{\\left(\\gamma^{2}+\\|g\\|_{\\text {Lip }}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\text {Lip }}^{2} \\sigma_{E \\mid X}^{2}}$ and some positive constant, $C$.\nNow we move on to providing a uniform concentration in the inequality (506). We will apply uniform concentration result from Lemma 14, for this set:\n\n$$\ng_{\\theta}=\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle\n$$\n\nRecall the below items:\n\n1. For a fixed $Z \\in \\mathcal{C}$ we have $\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(g(Z), f_{\\zeta}(Z)\\right)\\right\\| \\leq B_{\\ell}$.\n2. For a fixed $Z \\in \\mathcal{C}$ we have $\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|f_{\\zeta}(Z)\\right\\| \\leq B_{\\Phi}$.\n3. For a fixed $Z \\in \\mathcal{C}$ we have $\\forall \\zeta, \\zeta^{\\prime} \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|f_{\\zeta}(Z)-f_{\\zeta^{\\prime}}(Z)\\right\\| \\leq \\tilde{L}_{\\Phi} d\\left(\\zeta, \\zeta^{\\prime}\\right)$.\n4. For a any $\\hat{Y}_{1}, \\hat{Y}_{2} \\in \\mathbb{R}^{n_{Y}}$ we have $\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(Y, \\hat{Y}_{1}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(Y, \\hat{Y}_{2}\\right)\\right\\| \\leq L\\left\\|\\hat{Y}_{1}-\\hat{Y}_{2}\\right\\|$.\n5. For a fixed $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$,\n\n$$\n\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\mathbb{E}\\left[\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right), f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu}\\right]\\right|=B_{e q l}(\\mathcal{C})\n$$\n\nNow we check the Lipschitz continuity of the function $g_{\\theta}$ :\n\n$$\n\\begin{aligned}\n\\left|g_{\\theta_{1}}-g_{\\theta_{2}}\\right| & =\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{1}}\\right\\rangle-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{2}}\\right), f_{\\zeta_{2}}\\right\\rangle\\right| \\\\\n& =\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{1}}-f_{\\zeta_{2}}\\right\\rangle-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{2}}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{2}}\\right\\rangle\\right| \\\\\n& \\leq\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{1}}-f_{\\zeta_{2}}\\right\\rangle\\right|+\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{2}}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{2}}\\right\\rangle\\right| \\\\\n& \\leq\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right)\\right\\|\\left\\|f_{\\zeta_{1}}-f_{\\zeta_{2}}\\right\\|+\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{2}}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right)\\right\\|\\left\\|f_{\\zeta_{2}}\\right\\| \\\\\n& \\leq B_{\\ell}\\left\\|f_{\\zeta_{1}}-f_{\\zeta_{2}}\\right\\|+B_{\\Phi} L\\left\\|f_{\\zeta_{1}}-f_{\\zeta_{2}}\\right\\| \\\\\n& \\leq \\tilde{L}_{\\Phi}\\left[B_{\\ell}+B_{\\Phi} L\\right] d\\left(\\zeta_{1}, \\zeta_{2}\\right)\n\\end{aligned}\n$$\n\nThen from Lemma 14 we have that\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}}\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta}\\right\\rangle_{\\mu}\\right| \\geq \\epsilon+B_{e q l}(\\mathcal{C})\\right) \\\\\n& \\quad \\leq \\delta_{\\mathcal{C}}+C \\exp \\left(\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\mathcal{W}}}\\left(\\frac{\\epsilon}{2 \\tilde{L}_{\\Phi}\\left[B_{\\ell}+B_{\\Phi} L\\right]}\\right)\\right)-N \\min \\left\\{\\frac{\\epsilon^{2}}{4 K^{2}}, \\frac{\\epsilon}{2 K}\\right\\}\\right)\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 31, "text": "# D. 4 Concentration of Polar \n\nIn this section, we compute the probability of the occurrence of the event, $\\mathcal{E}_{p l r}(\\epsilon)$ through Lemma 11. The analysis of $\\mathcal{E}_{p l r}(\\epsilon)$ resembles to that of $\\mathcal{E}_{e q l}(\\epsilon)$ following similar arguments.\nLemma 11 (Concentration of Polar). Consider an $n_{X}$-dimensional sub-Gaussian vector $X \\sim S G\\left(0,\\left(\\sigma_{X}^{2} / n_{X}\\right) I\\right)$, and set of functions $f_{\\zeta}: \\mathbb{R}^{\\mathrm{w}_{2}} \\rightarrow \\mathbb{R}$ as parameterized by $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}$. Let $\\mathcal{C}$ be some convex obeying $P\\left(\\bigcap_{i=1}^{N} X_{i} \\in\\right.$ $\\mathcal{C}) \\geq 1-\\delta_{\\mathcal{C}}$ for i.i.d samples $\\left\\{X_{i}\\right\\}_{i=1}^{N}$. Assume that for any fixed, $\\zeta_{1}, \\zeta_{2} \\in \\mathcal{F}_{\\mathcal{W}}, \\zeta_{1}^{\\prime}, \\zeta_{2}^{\\prime} \\in \\mathcal{F}_{\\theta}$, and fixed $Z \\in \\mathcal{C}$, we have\n\n$$\n\\begin{gathered}\n\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(g(Z), f_{\\zeta}(Z)\\right)\\right\\| \\leq B_{\\ell},\\left\\|f_{\\zeta}(Z)\\right\\| \\leq B_{\\Phi} \\\\\n\\left\\|f_{\\zeta_{1}}(Z)-f_{\\zeta_{2}}(Z)\\right\\| \\leq \\tilde{L}_{\\Phi} d\\left(\\zeta_{1}, \\zeta_{2}\\right), \\text { and }\\left\\|f_{\\zeta_{1}^{\\prime}}(Z)-f_{\\zeta_{2}^{\\prime}}(Z)\\right\\| \\leq \\tilde{L}_{\\phi} d\\left(\\zeta_{1}^{\\prime}, \\zeta_{2}^{\\prime}\\right)\n\\end{gathered}\n$$\n\nIn addition, we have that,\n\n$$\n\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left|\\mathbb{E}\\left[\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right), f_{\\zeta^{\\prime}} \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right]\\right|=B_{p l r}(\\mathcal{C})\n$$\n\nDefine\n\n$$\nK:=4 n_{Y}\\left\\|\\nabla_{\\hat{Y}} \\ell\\right\\|_{L i p} L_{\\phi} \\sigma_{X} \\sqrt{\\left(\\gamma^{2}+\\|g\\|_{L i p}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{L i p}^{2} \\sigma_{E \\mid X}^{2}}\n$$\n\nThen for any $\\epsilon \\in[0, K]$,\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}:}\\left|\\Omega_{\\mu_{N}}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)-\\Omega_{\\mu}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)\\right| \\geq \\epsilon+B_{p l r}(\\mathcal{C})\\right) \\\\\n& \\leq \\delta_{\\mathcal{C}}+c \\exp \\left(\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\mathcal{W}}}\\left(\\frac{\\epsilon}{8 \\max \\left\\{\\bar{L}_{\\phi} B_{\\ell}, L \\bar{L}_{\\Phi} B_{\\Phi}\\right\\}}\\right)\\right)\\right. \\\\\n& \\left.\\quad+\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\theta}}\\left(\\frac{\\epsilon}{8 \\max \\left\\{\\bar{L}_{\\phi} B_{\\ell}, L \\bar{L}_{\\Phi} B_{\\Phi}\\right\\}}\\right)\\right)-N \\frac{\\epsilon^{2}}{K^{2}}\\right)\n\\end{aligned}\n$$\n\nfor some positive constant, $c$ and $C_{\\mathcal{F}_{\\mathcal{W}}}(\\nu)$ (and $C_{\\mathcal{F}_{\\theta}}(\\nu)$ ) is the $\\nu$-net covering number of the set $\\mathcal{F}_{\\mathcal{W}}$ (and $\\mathcal{F}_{\\theta}$ ).\nProof. Recall the definition of the polar in Equation 6:\n\n$$\n\\begin{aligned}\n\\Omega_{\\mu_{N}}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right) & :=\\sup _{\\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu_{N}} \\\\\n\\Omega_{\\mu}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right) & :=\\sup _{\\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\n\\end{aligned}\n$$\n\nNow, by taking the difference between the above two polars, we have\n\n$$\n\\begin{aligned}\n\\left|\\Omega_{\\mu_{N}}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)\\right. & -\\Omega_{\\mu}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right) \\mid \\\\\n& =\\left|\\sup _{\\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu_{N}}-\\sup _{\\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right|\n\\end{aligned}\n$$\n\nDenote, $\\zeta_{\\mu}^{\\prime *}=\\arg \\sup _{\\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}$ and $\\zeta_{\\mu_{N}}^{\\prime *}=\\arg \\sup _{\\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu_{N}}$, then, by definition we have that\n\n$$\n\\begin{aligned}\n-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta_{\\mu}^{\\prime \\star}}\\right\\rangle_{\\mu_{N}} & +\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta_{\\mu}^{\\prime \\star}}\\right\\rangle_{\\mu} \\leq \\Omega_{\\mu_{N}}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)-\\Omega_{\\mu}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right) \\leq \\\\\n& \\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta_{\\mu}^{\\prime \\star}}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta_{\\mu}^{\\prime \\star}}\\right\\rangle_{\\mu}\n\\end{aligned}\n$$\n\nApplying modulus on both sides we obtain\n\n$$\n\\begin{aligned}\n\\left|\\Omega_{\\mu_{N}}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)\\right. & -\\Omega_{\\mu}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right) \\mid \\\\\n& \\leq \\max \\left\\{\\left|\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta_{\\mu}^{\\prime \\star}}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta_{\\mu}^{\\prime \\star}}\\right\\rangle_{\\mu}\\right|\\right. \\\\\n& \\left.\\left|\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta_{\\mu_{N}}^{\\prime \\star}}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta_{\\mu_{N}}^{\\prime \\star}}\\right\\rangle_{\\mu}\\right|\\right\\} \\\\\n& \\leq \\sup _{\\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left|\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right|\n\\end{aligned}\n$$\n\nNow, we have to compute a lower bound on\n\n$$\n\\mathbb{P}\\left(\\sup _{\\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left|\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right| \\leq \\epsilon\\right)\n$$\n\nThe computation of Equation (521) is similar to that of Lemma 10. We can re-write the concentration of polars and apply the monotonicty of probability in inequality (520) by doing this we have\n\n$$\n\\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}:}\\left|\\Omega_{\\mu_{N}}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)-\\Omega_{\\mu}^{\\circ}\\left(\\nabla_{\\bar{Y}} \\ell\\left(g, f_{\\zeta}\\right)\\right)\\right| \\geq \\epsilon\\right)\n$$\n\n$$\n\\leq \\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\Phi}, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}:\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right| \\leq \\epsilon\\right)\n$$\n\nAs the data follow the sub-Gaussian distribution from Proposition ?? and Proposition 7, we have\n\n$$\n\\begin{aligned}\n& \\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right) \\quad, \\quad f_{\\zeta^{\\prime}} \\quad\\right\\rangle_{\\mu_{N}} \\\\\n& \\sim S G\\left(4 \\|\\nabla_{\\hat{Y}} \\ell\\|_{\\text {Lip }}^{2}\\left[\\left(\\left\\|f_{\\zeta}\\right\\|_{\\text {Lip }}^{2}+\\|g\\|_{\\text {Lip }}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\text {Lip }}^{2} \\sigma_{E|X}^{2}\\right] I_{n_{Y} \\times n_{Y}}\\right) \\sim S G\\left(\\left\\|f_{\\zeta^{\\prime}}\\right\\|_{\\text {Lip }}^{2} \\sigma_{X}^{2} I_{n_{Y} \\times n_{Y}}\\right) \\\\\n& \\sim S E\\left(2 n_{Y}\\left\\|\\nabla_{\\hat{Y}} \\ell\\right\\|_{\\text {Lip }}\\left\\|f_{\\zeta^{\\prime}}\\right\\|_{\\text {Lip }} \\sigma_{X} \\sqrt{\\left(\\left\\|f_{\\zeta}\\right\\|_{\\text {Lip }}^{2}+\\|g\\|_{\\text {Lip }}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\text {Lip }}^{2} \\sigma_{E|X}^{2}}\\right)\n\\end{aligned}\n$$\n\nFrom Assumption 6 the class, $\\mathcal{F}_{\\Phi}$ has Lipschitz constant at most, $\\gamma$. From the assumption $4 \\mathcal{F}_{\\theta}$ has a Lipschitz constant at most $\\gamma_{\\theta}$. Therefore, the inner product described above is concentrated as a consequence of Theorem 2.8.1 from Vershynin (2018). Now for a fixed $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}$, we have that\n\n$$\n\\mathbb{P}\\left(\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right| \\leq \\epsilon\\right) \\leq C \\exp \\left(-N \\min \\left\\{\\frac{\\epsilon^{2}}{K^{2}}, \\frac{\\epsilon}{K}\\right\\}\\right)\n$$\n\nwhere, $K=2 n_{Y}\\left\\|\\nabla_{\\hat{Y}} \\ell\\right\\|_{\\text {Lip }} L_{\\phi} \\sigma_{X} \\sqrt{\\left(\\gamma^{2}+\\|g\\|_{\\text {Lip }}^{2}\\right) \\sigma_{X}^{2}+\\|g\\|_{\\text {Lip }}^{2} \\sigma_{E|X}^{2}}$.\nNow we utilize Lemma 14 to have this concentration uniformly for all, $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}$. Set\n\n$$\ng_{\\theta}=\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle\n$$\n\nRecall the below items:\n\n1. For a fixed $Z \\in \\mathcal{C}$ we have $\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(g(Z), f_{\\zeta}(Z)\\right)\\right\\| \\leq B_{\\ell}$.\n2. For a fixed $Z \\in \\mathcal{C}$ we have $\\forall \\zeta \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|f_{\\zeta}(Z)\\right\\| \\leq B_{\\Phi}$.\n3. For a fixed $Z \\in \\mathcal{C}$ we have $\\forall \\zeta, \\zeta^{\\prime} \\in \\mathcal{F}_{\\mathcal{W}}:\\left\\|f_{\\zeta}(Z)-f_{\\zeta^{\\prime}}(Z)\\right\\| \\leq \\tilde{L}_{\\Phi} d\\left(\\zeta, \\zeta^{\\prime}\\right)$.\n4. For a fixed $Z \\in \\mathcal{C}$ we have $\\forall \\zeta, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}:\\left\\|f_{\\zeta}(Z)-f_{\\zeta^{\\prime}}(Z)\\right\\| \\leq \\tilde{L}_{\\phi} d\\left(\\zeta, \\zeta^{\\prime}\\right)$.\n5. For a any $\\hat{Y}_{1}, \\hat{Y}_{2} \\in \\mathbb{R}^{n_{Y}}$ we have $\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(Y, \\hat{Y}_{1}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(Y, \\hat{Y}_{2}\\right)\\right\\| \\leq L\\left\\|\\hat{Y}_{1}-\\hat{Y}_{2}\\right\\|$.\n6. For a fixed $\\zeta \\in \\mathcal{F}_{\\mathcal{W}}, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}$,\n\n$$\n\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}\\left|\\mathbb{E}\\left[\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g \\circ \\mathcal{P}_{\\mathcal{C}}, f_{\\zeta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right), f_{\\zeta^{\\prime}} \\circ \\mathcal{P}_{\\mathcal{C}}\\right\\rangle_{\\mu}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right]\\right|=B_{p l r}(\\mathcal{C})\n$$\n\nNow we check the Lipschitzness of $g$ :\n\n$$\n\\begin{aligned}\n\\left|g_{\\theta_{1}}-g_{\\theta_{2}}\\right| & =\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{1}^{\\prime}}\\right\\rangle-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{2}}\\right), f_{\\zeta_{2}^{\\prime}}\\right\\rangle\\right| \\\\\n& =\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{1}^{\\prime}}-f_{\\zeta_{2}^{\\prime}}\\right\\rangle-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{2}}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{2}^{\\prime}}\\right\\rangle\\right| \\\\\n& \\leq\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{1}^{\\prime}}-f_{\\zeta_{2}^{\\prime}}\\right\\rangle\\right|+\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{2}}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right), f_{\\zeta_{2}^{\\prime}}\\right\\rangle\\right| \\\\\n& \\leq\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right)\\right\\|\\left\\|f_{\\zeta_{1}^{\\prime}}-f_{\\zeta_{2}^{\\prime}}\\right\\|,+\\left\\|\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{2}}\\right)-\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta_{1}}\\right)\\right\\|\\left\\|f_{\\zeta_{2}^{\\prime}}\\right\\| \\\\\n& \\leq B_{\\ell} \\tilde{L}_{\\phi} d\\left(\\zeta_{1}^{\\prime}, \\zeta_{2}^{\\prime}\\right)+B_{\\Phi} L \\tilde{L}_{\\Phi} d\\left(\\zeta_{1}, \\zeta_{2}\\right) \\\\\n& \\leq 2 \\max \\left\\{\\tilde{L}_{\\phi} B_{\\ell}, L \\tilde{L}_{\\Phi} B_{\\Phi}\\right\\} \\max \\left\\{d\\left(\\zeta_{1}^{\\prime}, \\zeta_{2}^{\\prime}\\right), d\\left(\\zeta_{1}, \\zeta_{2}\\right)\\right\\}\n\\end{aligned}\n$$\n\nNow, we have a product of two metric spaces whose metric is a maximum of individual metrics, therefore simply we can upper bound the covering number by product of these two metric spaces.,i.e.,\n\n$$\n\\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}} \\times \\mathcal{F}_{\\theta},\\|\\cdot\\|_{\\infty, d(\\ldots)}, \\nu\\right) \\leq \\mathcal{N}\\left(\\mathcal{F}_{\\mathcal{W}}, d(., .), \\nu\\right) \\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(., .), \\nu\\right)\n$$\n\nFrom Lemma 14 we have that\n\n$$\n\\mathbb{P}\\left(\\sup _{\\zeta \\in \\mathcal{F}_{\\mathcal{W}}, \\zeta^{\\prime} \\in \\mathcal{F}_{\\theta}}:\\left|\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu_{N}}-\\left\\langle\\nabla_{\\hat{Y}} \\ell\\left(g, f_{\\zeta}\\right), f_{\\zeta^{\\prime}}\\right\\rangle_{\\mu}\\right| \\geq \\epsilon+B_{p l r}(\\mathcal{C})\\right)\n$$\n\n$$\n\\begin{aligned}\n\\leq \\delta_{\\mathcal{C}}+C \\exp ( & \\left.\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\mathrm{vc}}}\\left(\\frac{\\epsilon}{8 \\tilde{L}_{\\Phi} \\max \\left\\{B_{\\ell}, L B_{\\Phi}\\right\\}}\\right)\\right) \\\\\n& +\\log \\left(\\mathcal{C}_{\\mathcal{F}_{\\theta}}\\left(\\frac{\\epsilon}{8 \\tilde{L}_{\\Phi} \\max \\left\\{B_{\\ell}, L B_{\\Phi}\\right\\}}\\right)\\right)-N \\min \\left\\{\\frac{\\epsilon^{2}}{4 K^{2}}, \\frac{\\epsilon}{2 K}\\right\\})\n\\end{aligned}\n$$\n\nThis completes our result.", "tables": {}, "images": {}}, {"section_id": 32, "text": "# E NUMERICAL EXPERIMENTS \n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Numerical simulations of the Lipschitz constant (or upper bound thereof) obtained for different model widths $(r)$.\n\nIn this section, we present numerical simulations for the problems of low-rank matrix sensing, two-layer ReLU neural networks, and multi-head attention. In each simulation shown in Figure E, we generated data using a teacher model with random initialization of parameters, $Y=\\Phi_{r^{*}}\\left(\\left\\{W_{j}\\right\\}\\right)(X)+\\epsilon$, where $r^{*}=64, X \\sim \\mathcal{N}\\left(0,\\left(\\sigma_{X}^{2} / n_{X}\\right) I\\right)$, and $\\epsilon \\in \\mathcal{N}\\left(0,\\left(\\sigma_{E}^{2} / n_{Y}\\right) I\\right)$. We used gradient descent to reach a stationary point for each $R$ (rank, number of neurons, or number of heads), starting from 1 and increasing up to 300 . The first factor was initialized with small-scale random values. For each subsequent factor, we initialized the new factor with the supremum obtained from the polar equation (14), following the algorithm in Haeffele and Vidal (2015, 2020).\nIn each problem shown in Figure E, we plot the upper bounds on the Lipschitz constant for these problems. For matrix sensing, the Lipschitz constant is trivially upper-bounded by $\\left\\|\\mathbf{U V}^{T}\\right\\|_{2}$; for the ReLU neural network, it is upper-bounded by $\\|\\mathbf{U}\\|_{2}\\|\\mathbf{V}\\|_{2}$; and for multi-head attention, it is upper-bounded by $\\sum_{j=1}^{r}\\left\\|\\mathbf{V}_{\\mathbf{j}}\\right\\|_{2}$. We can observe from Figure E upper bounds on the Lipschitz constants are uniformly bounded, indicating that our Assumption 6 is realistic and holds empirically.\nWe conjecture that it is possible to show that the Lipschitz constants are uniformly bounded for any stationary. However, the analysis of this is beyond the scope of this work. Similar analyses based on gradient descent can be found in Oymak and Soltanolkotabi (2019).", "tables": {}, "images": {"img-0.jpeg": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAHDBVIDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAa33TTqRvun6UtABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFQXl7b6faSXd3MkMEQ3O7nAUe9T1ynxJH/Fu9aHH+oH/AKEKAOjtL23vraO5tZUmgkG5JEOQwqHTdY0/V4nl067iuY0bazRtkA+lebQahPoHh3UvDFocXk7xppqg9FuO49lO8/hV/wAEWkWhWHiq0tZ4rWK0uzGk8oyqbUGGPr6+9AHo5OO1RLdQvcNAsqGZAGaMMNyg9CR19a84i8W3un6rp+PEX9s211OkMkZ0/wArAboyMox1xwaltbDVpviprv2bWhbn7Nbs2LRXymWKpz6evf0oA9G3VWttStbu7urWCUPNasFmUA/KSMjnGOlchFe+JNa8Wa5pdrqkVjZWDRBZhbLI5LJkqAePxOav6Vd6hfap4ksBcrHJbMiQzrCuQWTOSO/PagDqi2OtLXnun+Kdb1nU4PD0SJbapZvnVLgKGRYweNgPduPpXoVAGHe+MvDunXjWl3rFpFcLw0bSDKn39PxrXhuIriBJ4HWWKRQ6OjZDKeQQfSvNNFkvvAunXGn6v4dnvbbzpJX1K1VZfNBYnc6nnOD71ra94wtrLR9FXRLi2t49TO2C4eLMcEajk7B3HAx0oA7nNJurgtB8T3I8VW2kyazDrVveRSMlwluInhdBna23ggjNV9LvfF+uaBeapFrNva/Zpp1hj+yq/nBGIG8k8dMcCgD0bdxwM1Wv9QttMsZr28lEVvCpeRyCQorhtT8X6hLofh68juBpdvqKbrq+MHnCAheAFPHJ7mrVzqt7Z+A9X1OLX4NYeOFnhuBAihcdiBwaAO1jmSWNJEIKOoZT6g0/cK4rV9X1v+1PDun6ZcQQHULd3neSIMEwqncBx0yeM0unavquk+I9R0fWL1dQSDTxqEVwsCxNtDFWUheM5FAHZ7sUua86bUPF83hCXxTFqluu+3NzFp5tQUVOo+fO7dj14zxjvXb6Lcy3uiWVzMwMssCOxAxkkAmgC/RRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFZOp+JtG0adYNS1G3tZWXeqyvglfX9DWtVK9s7eaCV5IIncRsAzICRx70AZ1v418N3VxFb2+s2kksrBURZOWJOAB+NbuRXleg3LaN4C8PalBFAIv7Q8u8LRKT5bSsuc4yCDt5rZsvF183jm8tbloxo7CaK1O3nzYVVn59CC35UAd3u9qM156nijXLjQdDSB4V1TXblxDJJHlbeJcsWI7kKPxzWjZ6hrWi+K7PR9W1CPUYL+J2hn+ziJ43XqpC8Yx0oA7Kq1jf22pWi3VnMk0DkhZEOQcEg/qDVKx8TaJqV61lZanbT3KgkxI+WGOtc/8J+Ph1p+MY8y4xj/rs9AHabs9Bz9aN3+c1x/ijVZbbU0hi8UppuYtwtY7IXEjnJ+Y9cDpxxVGx8TazqfgW/1C0kR9R064dGYQ4E6oQT8p5BKn8DQB35bHXpRu9s1yWoeJJ7288O2eiyqH1M/aZX2htlsq5br0JOFz61jXni2e/wBc1G2i8RQaJb2UpgUNa+a8zjqTu6AHjigD0YnHajcM/XpXm8njTV5/BMV9aeR/aK6olizhSI5vnA3KDyFYEfSuz0ey1W1EzapqovnkwVVLdYli65AxknPHU9qAH6p4h0jRHjTU9QgtWkBKCV8bgOtT2Gq2OqwGawu4bmMHBaJw2D7+lcP4vvIrD4j+HZ5rSa7QWlz+7hjDsenOPaoPCF1by+JvE+v2tqbDT0iSJ7RlCOZEBJcp2yBx60AelZx1qCS+tYbyG0kmRLiYExxk8sB1xXl6eNdSutIfWovEtlDMVMsWk/ZNyleSEZ/vbiB1FbM98NT8b+Dr5UKC4s5pNh6qSgOPX/8AVQB293f2thGkl3MkKO6xqWPVj0H1ou722sLZ7m7mSGBPvO5wBXKfEcD+x9JHH/IYtP8A0OpPiaR/wr7U84+4v/oQoA64MGAI6HoapQaxYXOoz6fBdRSXduAZYQ3zIPcVZt/+PaL/AHB/KvI5baaw8YeIvFFkrNPpd+guI1/5a2zoPMHuRgMPSgD1hL+2kvJrNJka4gVWkjB5UHoT+VR6fq1jqsLTWFzHcRqxQtG2cEdR9a5HQ7iK7+IXiW4gcPFLYWbow6MCrnNcz4BkPhf+zb0krputu0ExPSO4DNsPtuGR9QKAPWbS/tb5ZGtZklWKRonKnO116qferFcb8O/+QfrfA/5Dd5+Pz12VABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABVKXV9Pg1OLTZbuJLyZd0cJb5mHqPyNXa8q8a2ksnxBN9bqTdabpcd5Fj/YmO4fiuRQB6RqGr2GlLG9/dRW6yv5aGQ43NgnH6Vb35xjkHvmvHvG15H4qeW+gYSafpMduIyDkNPM6sT77UwP+BGu8TV7sfEGDR94+xHRzdFdvPmCULnP+6elAHSSSrFG0jkBFBJPpiqltrFhd2drdw3URguseQzNt8zIzgZ79eKwV1W7utd8U6bIwNvZW0LRLt5G+Ni2T3ziuLa3vZ/Dfw9+y3/2YmdVX9yH2vtb5uevHagD1S51O1tLy1tJ5Qs90xWFME7iBk/pVyuRvLrUtL1rw1YXF6t211NMs0rW6oWAQsMDt+BrYt/E2i3epNp0GpWz3qsVMCv82R1GKALtnqFrqETy2k6TIjmNmQ5AYdRRbX9reSTx28ySPbv5cqqeUb0NebeBtb1Gw0zUYLXw9d3sf9pXB86J0C5LdME5p3hrXW0nS/G+tXFo8UkWovKbdyCQ2xcKSPegD0/PtRu6cda8sk8ZajY2cGqN4ks9QffGZtNitNo2sQCEfrkZ6ng4rovE2qzQagkUXihdNzGGW2isRcSNnu2c4HtxQB2Ofaqt9qdppqRPdzCJZZVhQkHl2PArhrbx1eDwPf6g8cdxqNtdfY4yEKJK5YBGIJ465xVbxNZ+IrKHQ/7S1aG/hk1O28wC2EJibdwFKnkZ45oA9NqlearY2E9tBd3UUMt05SBHbBkYdh+Yq5mvJvECW/irX9fuWvIIm0aFbbTw8gBNwD5jtjOeoVc/WgD1Sa6gtlVp5Y4lZggZ3CgseAOe9E11BbmMTSxxmRtkYdgN7dcDPU8V5v4uvpvEPgzw7qdleiFZr233KIg/7wtjPP8AdIbjv7VJ4x07WY7jwtHLrvmytqqKsn2NF2thsNgHnAzx3zQB6TmjPHSuF8Q6/f6C+l6NJrEC3d2XaXUZ7YbY0X0jXgsTgUvhvxJcS+JP7Hl1aLWIJIGmjuo7fymjKkAq4HHfg96AO53VFJdQRSRRyTRo8pIjVnALn2HevNdJ1bxjqvgQeJf7XtoWiSSRbcWqsJ1RmB3HPy9McAdPep/EDX+sa54Mv7O/Nl9rVmRfJD+WxjySeecggY7YzQB3s2qWlvqVtp8soW6uVdoUwTvCY3dPTIq3u5xXJT31/YeMfDulT3MdwJ7e4aeUwKrOyAYx6degNUbK98TeJoL3VNN1aDT7WKaSK1tmtVkEuzjLsTxkjt259qAO7zz0o3Y7V55c+L9XvtD8NXemrBBd6jd/Zp0lXKKQDu98Ag8frWlZXet6T4wtdI1PUo9Rt723eSOX7OsLRspHyjb1Bz3oA7KiiigAooooAKKKKACiiigApP4qWk/iH0oAWiiigBG+6fpS0jfdP0paACiiigAooooAKKKKACiiigAooooAKTIpaqSGdroxxzBFCZ+6DQBbzRmq/k3P/P0P+/Y/xo8m5/5+h/37H+NAFjNGar+Tc/8AP0P+/Y/xo8m5/wCfof8Afsf40AWM0Zqv5Nz/AM/Q/wC/Y/xo8m5/5+h/37H+NAFjNGar+Tc/8/Q/79j/ABo8m5/5+h/37H+NAFjNGar+Tc/8/Q/79j/Gjybn/n6H/fsf40AWM1ieLNIn17wtqGl27xxzXMW1WkztByDzgH0rT8m5/wCfof8Afsf40gguR/y9D/v0KAM8eHNOlvdP1G5tkfULGLyoptx+UYx0zj1rCn8F3V1pfieze6iQ6vc+fE6ZOzGMBh9V7etdb5Nz/wA/Q/79ijyLn/n6HH/TMUAcVfeHvFetjTE1CXSbWCxuEmMdsXbztvU8jjvxzWle6FrNv4tl1zR5rBlurdILiG8Dj7pJBUr7HGMV0fkXOP8Aj6H/AH7/APr0eRc/8/Q/79igDJ0fRrjT/EGt6hNJGyag8TRqmSV2pg54qCDRdTsL3xBe2ctqZ79le280thGC7cvx268Vu+Rc/wDP1/5DFHkXP/P0P+/Y/wAaAOPh8DXGlDT9Q0u7Q61DIWvJ7gnbeKx/eK+M/wDAeOK7jIqt9nuQf+Pv/wAhineTc/8AP0P+/Y/xoA5Oa38fNby2fnaBIHUoLphKr7T3Kcgn8aZJ4Ee08O6NaaZdoNR0ljJBNPHlJGYkurAdFYk/TjrXXeTc/wDP1/5DFAguR/y9D/v2KAMfSo/EjX6tqkGjwWyqeLQu8jN65YAAfgab4e0C40nw1cabNLE8sks7qyZ2jzGLDt71teRcn/l6H/fsUeRcnH+lD/v2KAOas9J8RaJ4a0rTtO/sq5NtD5dwlwXUP6FGA/mKz08FajLoniJJ5LC3vdXQKIrYMIIsDA5xkn3wK7Xybnn/AEr/AMhijyLj/n6Gf+uYoAxJNAun13Qr8SReXp1vJFIuTliygZHHt3p03h6S48ZT6tI8Zs5tL+wtHk7t3mFiemMYPrWz5Fxz/pQ5/wCmYo8i56fa/wDyGKAPKdWlvNM8H3ugw+JNJn09I3ggWNSbxh0EW3OAf4c46V6docMlroNhBKuJI7dFYZ6EKKjXQ7dbn7Qsdqs2ciQWy7s/Wrnk3P8Az9D/AL9igCzmjNV/Juf+fof9+x/jR5Nz/wA/Q/79j/GgCxmjNV/Juf8An6H/AH7H+NHk3P8Az9D/AL9j/GgCxmjNV/Juf+fof9+x/jR5Nz/z9D/v2P8AGgCxmjNV/Juf+fof9+x/jR5Nz/z9D/v2P8aALGaM1X8m5/5+h/37H+NHk3P/AD9D/v2P8aALGaTNQeTc/wDP0P8Av2P8abbtL9omjkk3hduDjHWgC1RRRQAUUUUAFIWxS1ymv6BZFL3Vb3VtRgCIXRkumRIQF/hAwPzzQB1QPtS5rC8IXN/d+FdPn1LcbqSPLMwwxHYkeuOar3fiyDRtXntNbRLK2K+Za3RfKTADLDpww9O/bNAHSZozWToOp3Wr2TXk9k1pFI+bdJD87R9mYY+XPpWB8RbS4i0KbVbfVL63e22BYYZNsZJcDJGM5wfWgDtc0gbOOOtc34z1O503wzvs5DHc3EsVukg6xl2A3Csua1l8KeJdCW3vbue01GRra5juJjJl9pZXGehyD045oA6m01yxvdSuNPgkZrm3UNIpQjAJIzz15B6VoZrk4r1D8Q7h/s975bWMdqshs5dnmLJIzDdtxjDD5s4966qQlY2I6gEigB2aXNU4VupIEc3I+YA/6scVJ5Nz/wA/Q/79j/GgCxmjNV/Juf8An6H/AH7H+NHk3P8Az9D/AL9j/GgCxmjNV/Juf+fof9+x/jR5Nz/z9D/v2P8AGgCxmjNV/Juf+fof9+x/jR5Nz/z9D/v2P8aALGajmUyQyIOrKQPxqPybn/n6H/fsf40nk3P/AD9D/v2P8aAOWtvCFynw5l8NzzQm4aNwsqklA5csp6Z4JH5Vmah8P7+58CWGkwX0KarbymV7pi21i4ZZecZ5DHtXeeRcZ/4+v/IYo8i5z/x9D/v2P8aAOe1rwrNcaZpI0m4it9Q0hla0eVSUOF2lHxztI9KZp+ia1eeJIda197JDaRNHbW1mWZQzdXYsBzj2rpBBcA/8fQ/79ijybn/n6H/fsf40ASLbwoxZIkU+oUCuC8PaD468N6LDpVnL4dlghZ2VpvPLHcxY5wAOpruvJuf+fof9+x/jSeRc/wDP1/5DFAHJNoXiWx8R3msac+lTSalBCl0lwZFETxqVBQgElTnkGrfhjRrjwzpmpnWL2Cb7RdvcvNkquGAHzZ6ciui8i4/5+R/37/8Ar1FPYPdQNDcSpLE3VHhDKfwNAHEfDbSFjvNW1VZWmtDK1rp5b+GFWJO32LH9K05NA1rSNavbzQl024tb5xLJb3pZDHJjBKsoPB9MV00VnLBEsUM6RxqMBFiAA+gp/k3P/P0P+/YoA5zUdD1jWdEs4L2TT0u4r+G6YW4YR7UbJAzkk8e1dXmq/kXP/P1+cYo8m5/5+h/37H+NAGPfaHPdeNNJ1pZYxBZwTROhJ3MXxjHGO3rVe98MzSeL49YtHiSC4tmtdSgfIMyfwMMD7wz37cV0HkXOP+PrH0jFHkXP/P0P+/YoA5DT9E8V6FaLpdgNFu7OLKwT3nmCRV7BgAd2PqKn17QtfuNX0bVdLm037XYxOki3W8I5YAHaF/HvXUeTc/8AP0P+/Y/xo8m5/wCfof8AfsUAcnfaD4k8RaVcWOuT6TAQUltJ7ASM0cqnIJ39qivdD8W+IYIdN1qbSYLAOjXDWZkMkwU524bG0H6mux8i5/5+h/37H+NHk3P/AD9D/v2P8aAJ1AVQo6DgVz+jeH5bDVdfuLlopIdTnEiouSQu3aQcitrybn/n6H/fsf403yLg4/0r/wAhigDkfB/gm88MaxrMz3cU1ndIkdqoLb40UttDZGOA2OD2qxY+DM+An8N6jIjuyv8AvYskI5OVZSQOVOD+FdP5Fz/z9D/v2P8AGgwXJ/5ev/IYoA5/wL4f1Dw5oUtrqlzFc3c1zJcPLESQS+CeoHcV1OarGC5P/L1/5DFL5Nz/AM/Q/wC/Y/xoAsZozVfybn/n6H/fsf40eTc/8/Q/79j/ABoAsZozVfybn/n6H/fsf40eTc/8/Q/79j/GgCxmjNV/Juf+fof9+x/jR5Nz/wA/Q/79j/GgCxmkzUHk3P8Az9D/AL9j/Gopluo4WcXIO0Z/1YoAu0U1CSik9cU6gAooooAKKKKACiiigAooooAKKKKACiiigBM0uaqzGVrlIo5dgKFiduehH+NO8m5/5+h/37H+NAFjNGar+Tc/8/Q/79j/ABo8m5/5+h/37H+NAFjNGar+Tc/8/Q/79j/Gjybn/n6H/fsf40AWM0Zqv5Nz/wA/Q/79j/Gjybn/AJ+h/wB+x/jQBYzXPHQpW8btrTNEbVtP+yGM53E7yc9MYwT3rZ8m5/5+h/37H+NJ5Fz/AM/X4+WM0Ac1qfgm1XwpLoehwxWkb3CTkSMzLkOpJycnooHtTta0LVh4gsdd0OSzN3DatZzw3ZYJJEWDDBUcEEeldH5Fz/z9D/v2KPIuf+fof9+x/jQBzGieHNXttQ16+1O7tZZtTjjRVgDBYyqsMc9uetQP4P1BfC2gWdtdWy6jo8qSozhmikIBBB6HnPWuuMFyf+Xof9+x/jR5Fz/z9D/v2KAMCTR9W1HUNBv797FJ7CWR5kgLlWDIVG3I/HnFdGtvCj71jjDdSwUA5qPyLnn/AEr/AMhil8m5/wCfof8Afsf40AYvhDQrjw/pt1a3Ekcjy3k1wDGSQFc5A5HWqNr4NZ7TxPaX00bQaxdPMnlZzGCoAzwORgGun8i5P/L0P+/Yo8i4/wCfof8AfoUActZ2XjK1hgs2/sCRIsIbxhJvZRx9zGN2PfFOu9E16y8UXmq6N/ZsyXqIsi3pZTEyggFSoORz04rp/Juf+fr/AMhijybn/n6H/fsUAcZbeBLuTwxrGl6lqCSXN9dG6S5jUjy3yGB2npgjpnpxSX2g+Ltc/s1dSuNKhjsbqKdxblz5+w8k5HHfjmu08i5P/L0P+/Yo8i5/5+h/37/+vQA65E5tZBblRMUIQvnAbtmuW0LwDo1lo0EGp6ZYX1/y1xcSQB97sSWOWGSOa6fyLn/n6H/fof40eRc/8/Q/79CgDi28C3qaHeaXa3VtFEmoi+00EMViwc7HGOmc9D3q9q2ha9rWmafLPNp0Gq2F6t1F5YdoHwCMNn5u/Wum8i5/5+v/ACGKPIuf+fof9+xQBzGpaBreorp2q+bp0Wu2TMAAHa3ljYYKHPP0OOK0dHi8QG8Z9Vh0mCELhUsy7uT6ksBj6VreTc/8/Q/79ijybn/n6H/fsUAc1o3he60z4df8I7LNC1z5E0XmKTszIzEds/xDtTLzwvqR0vw8bC4tV1HR1AHnhjFJlNrDjkdM9K6jyLjOftXP/XMUeRc/8/Q/79igDAbRNTu/Eeg6xeSWivZQTx3EcJYqTIABtyOnHfFZ0egeKNFF9Y6FPprafcyvNE90XEluXOW4AwwycjpXYeRcZz9qH/fsUeRc/wDP0P8Av2KAOXTwY9pZ+GrW0nUppVz50ryZDS5B3ED1JJPNal7o09z4t03V0kjENrDLE6NncS2MY7dq1PIuOf8ASh/37FHkXP8Az9D/AL9igCzmjNV/Juf+fof9+x/jR5Nz/wA/Q/79j/GgCxmjNV/Juf8An6H/AH7H+NHk3P8Az9D/AL9j/GgCxmjNV/Juf+fof9+x/jR5Nz/z9D/v2P8AGgCxmjNV/Juf+fof9+x/jR5Nz/z9D/v2P8aAJ80fxD6VWiaZbxopJd6+WGHygd8VZ/iH0oAWiiigBG+6fpS0jfdP0paACiiigAooooAKKKKACiiigAooooAKrj/kIN/1zH86sVXH/IQb/rmP50AWKKKKAEzRmsvxHeT6b4b1K9tiBPBbvImRkZAyKo+BdVudd8EaPqd6we5uLZXkYDALeuKAOjooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAqrD/x/3P0T+Rq1VWH/AI/7n6J/I0AWqKKKACiiigArgPEFv4k1DxKDLoX23RLUq0ECXkcfnSf3n3dcdhXf0mKAKmnzXNxZRSXdn9jmI+aDzA+z8RxWBqvhNvEmoytrcwk06IEWlrExGGII8xj/AHhnj0rqsUYoAytDtNSsrF7XUrpLrynKwzDO54+2/wD2qxvH0Gs6nosul6XpH2oTqrNMbhIwhDA4w3XpXXYox780AcnqthqnibwrJBPYrp2oRSpLBG86yKWQhgSy9jyMe1Qx2mueIPEOlXWqaWNNtNMLy7WuEkaaYrtBXb0UZPXrXZYo20AGPemTf6l/901JUc3+pf8A3TQA22/49If9wfyqaobb/j0h/wBwfyqagApM0juERmPQDJrhPht4vm13wVJq2tXcCPHeSwtK5Eahdw2jn6igDvM0tcn4S8ZxeKdS160jhEZ0q78gENnzFIOG/Q11lABRSZozQAtJmgnFcvqPiprLx9o/hoW6smoW8szTbuV2jgAUAdTRRSbvY0ALRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFQXf8Ax6S/7pqeobv/AI9Jf900ASJ/q1+gp1NT/Vr9BTqACiiigAoopM5oAWim7sDJHH1pc+1AC0U3d7UuaAFopM+1Juwf060AOoqlqepLpljJdNbTzqnJSBdzY7nBIqWyvI76ygu4wwjmQSLuGDgjPNAA3/IRj/65N/MVYqu3/IRj/wCuTfzFWKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiijNABRRRQAUUUUAFFFFABRRTXdY0Z2ICqMkntQAuaWvKdK+KK638Y00PTruOfRZLRlQhSN0yjeTk47AivVc0ALRRRQAUUUUAFFFFABRRRQAUUUUAVh/yEz/ANcR/M1Y/iH0quP+Qmf+uI/masfxD6UALRRRQAjfdP0paRvun6U1pUjQu7KqKMlmIAA9fpQA+isTTfFOmak96FuYY/sty1sS8y/OQAdwwehzx9DUuia/ba6L1rVT5dpdval9wIcqBllI7ZOPwNAGtRRRQAUUUUAFFFFABRRRQAVXH/IQb/rmP51YquP+Qg3/AFzH86ALFFFFAGB42bZ4J1pvSzk/9BNY/wAIZPM+FmhHriFl/JiK0/iA2z4f663pZyfyrB+CUnmfCfSDn7pmX8pWoA9CooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAqrD/AMf9z9E/katVVh/4/wC5+ifyNAFqiiigAorJ1nxBZaJFbvcuMzzpCq71BG5gueT0Geaj1PxRpumxW7+fHO088cCxwyqWJY4z17UAbVFFFABRRRQAUUUUAFFFFABUc3+pf/dNSVHN/qX/AN00ANtv+PSH/cH8qmqG2/49If8AcH8qmoAzNYvYbfS78GZBKtrJLs3DdtCnnHXtXzZpM9xqHwPbRrbm41HxH9nUY6/u0k/mBUnxG1LVW+JGsapaS8wXCaREhPyhXiII6+u4/U+1T+CrA6d4v0jwyZPM+xeI7iUkjG7y4kGcUAZfw613XfBPjSys5owia3PElyLhDuKCVkyM8g53c19VXE4traWdh8saFz+AzXkXxq8OxRT6T40+0lH02aGGSLYMFPMyDnPGCx/Ouv8AEPjXRH0jULK2vkluptHnvYdgJDRhG5z+FAFjw940i13wG3ib7P5KpDLM8JcEgJknn6D0q34J8RN4s8IafrbQLA90rFolbcFIYr1wPTP4182/DPWDZeE/Hds8j4k0htiZOATuTOPq4r1T4Z+KbHw38ErbU9QZzDbSvEEjXLOxfhQPU5oA9A8Z6rNofg3VtTt3CTW9szxsRnDdq8isPEE2t/Ev4c6hdSB7i40jdKwGMuTIpP5rWl49+IujeKvh9baZpUz/ANoa26QpbEfNGd4BD44HNeZ+DNRY/EPwPbSEiWxU2soPY+fM38mFAH1nurzm5168T49WWirdyCxfSmLW+75C+S2ceuB1rN+Mfi+78L6n4XW0vZreOSeWW4EbEblUKAD6j5jXjfhDx5Ja/EPSPEHiC6nmFpDJHNM7GR2HluB168kCgD6+oqlb6raXNha3qSqsN0qGIuwXduGQPr7VO1zEkyQtJGJZMlIy3zNj0H86AJqjknihXdLIiLnGXYAfrVObW9Pt9XtdKmuY0vbpGkhiJ5dVxkj86+dfFvj251Lw54qsrrUfMmi14GyiY8rCrHAHsNooA+mc0tch8PvG9p430Sa7toJYfssv2dxIRlsKDu47HP6V1xb2oAM+1LXFeFfHS+I/F/iLQzFGh0uQCJ1bmVOhJ+h/mK7MNkUAOoqKO5hmd0ilR2jba4VgSp9Djp+NSZoAWikJwM8Y+tGaAFopN3SloAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACobv/AI9Jf901NUN3/wAekv8AumgCRP8AVr9BTqan+rX6CnUAFFNLhQSeABkk9BWPa+KNMutRvrMXMKNaFAztMuH3DPHPboaANqq17ZQahZS2lwGaGVdrhWKkj6jmqWm6/a6pqepWVt8/2Bo1eVWDIxZd2AR6DGauag14llI1hHFJdAfIkrbVY/XtQB5tYRaZZePdJt9Btr3Sc+b9qW6LpHcoBwFDk72zzxXouqNfLps500RG7UAxLL90nPQ/UcVzUmma/r+taVcapbWmn2mn3H2kJDOZZJZApAGcDA5565rpdVgvLjTZ4bC6W1uXUqkzLu2Z74yOfSgDmrbxlca3dw6fpNkyX0bD+0BcA7bQA8qf7xbtj/61ddNCJ4JIi7IHUqWQ4Iz3B9a5WPwRBpclndaNcNbX8LAXE8g3/bEJ+cS9Nx7g9jXWSFxGxRQWxwCcZoA4fwzpkOkfEDWLK2kuGiFlDJmeZpW3FmycsTTBptv4u8a6/Bqvmy2emLDBbQrKyBWdNzP8pHOSBmpdPsfFMXi6fWJ9P09Y7qKOCRVuiSiqSd33eevSrd3pWs6V4mv9W0SC2uk1KOMXEM0pjKSICquDg8YwMe1AFPw9fXtx4GvbZ0nvrm1mnsAQQXcKxCk5I7EZro/DYnTw9ZQ3FrLbTQxLGySYzkDGeM8VF4X0R9C0OO0nkWW4d3mncdGkdiT+HP6Vt4oAgb/kIx/9cm/mKsVXb/kIx/8AXJv5irFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUU1nCKWYgKASSewFAC7qWvKfiF8S49N8Kabq3hjUopRJqiwzMqhsxqrF1ORx0XnrXppvoBpxvy4+ziLzt3+zjOfyoAxh420QeLZPDL3Ij1FEVgr8Byc8A+vFUvA/ji38Y2WpXCxLbmxungYbsjYPutntkA/SvCdQttYXVdK+K948YtLvUkZoxw0SBtoGO/wAqmn/D+/ufDnhHxT5zbWv9G+2Q7u4LvFke/egD6as7621C1S6s5454HBKSRtuVhnHBHuCPwqcnAyeK8k+BVzPaeHNV0K+k/eaZciQAnASORNw/DO4/jWn4r+K+g2mn6jZabd/a737EZIWh5TczeWFz65OfpQB6JBcRXMQlgkSWM9HjYMD+IqWvLvgfJeW/hfUdGviPtGm37xEegYKx/UtXqNABRWJ4m8U6d4T06K/1NnWCSdIAUGTubOD9ODWyHzQAufauH+J/ja38GeFpJGQTXl2Ght4d2CTjlvoBz09PWk8Z/EXT/D1tq1nayq+sWln9pSNlyhJbABPrntWFonhLWvF3iC61nx5Y2yRHTls7W3gkJTD53uOTg8/r7CgDmovCMOk/BnSfFemZttasANVM/wB4ybsblOe20Dj2969v0u7+36TZ3nBNxBHLx7qDVK68OWcvhGbw5CDFZtaG1UfeKqV2jr1rnfhLe3Nz4HS1u5zPNYXM1mWIwdqMQv6YoA7ykz7Utch4q8aL4c8Q+HdK8hJW1W58liWwY14GR68nFAHX0UmaWgAooooAKKKTNAC0VR0zWdP1m3kn066juIo5WhdkOcOvUVeoArD/AJCZ/wCuI/masfxD6VXH/ITP/XEfzNWP4h9KAFooooARvun6VDc2kF7ayW1zEssEqlZI3GQwPY5qZvun6UtAHkp0nw8up6la6F8Ol1aOC4KTzu0UaLJtGUTec4HAxwAa77wurx6OIm0FdDEblUs0dGXHXI2cDJJrl7/TNW0bXdQubPxtpul299N532S4slbaxABOTIDk4z2rqfDUt1Npjm71q01eTzSPtNpEI1AwPlwGYZH170AbdN3D2zVHWNVi0jS5byVS2wYVF5Z2PAUe5NcZ4ThvrX4gamuoTySXMunxXEqbiVR2c8KPQDA/CgD0LPtRmszXJdUg0xptIhhnuoyG8mUkeao+8oP8LEdDWHpvjB/Eeo21voVufJiIbUJbhSPI/wCmQH9/IOfT3oA68nFIGzn2qG7tUvbSW2kZ1SVdrGNirAexHIrkPBVpHp3ifxZYQPK1vbz24jWSRnK5iyeT7k0AdoZFBAJAz05qjqmsQ6X9nV4pZprhykUMK7ncgEnA9ABzXG6dpUXi7UPEt7qDysba+ksLRRIyiARqPmAXuSaNHudRvtJ8Na80E189n59tcLGAZHDfKHAJAPKDPPc0Adxp2owapZR3dtuMb5GGGCpBwQR65p4/5CDf9cx/OszwzaXNppT/AGqIxSzzyzmMkEpuYkA44zzWmP8AkIN/1zH86ALFFFUdW1SHRtJutSuVcw20TSuEGWIA7D1oA8z+LnxD07SbfUvCc8T+fd6Y0izq2QrsSFQjHfBOc+lWfgHciX4ZRQ9Gt7uaNgfXIb+TCvB/iprMGv8Aj+71S2WQQXNvbSRiQfMoaBGx9fmr3D4P/wDEq1bxN4ffClZIL+JPVZYxnH0woz70Aes0VTXVbB742KXlu12BkwCUFx68dat59qAFopM03zV37Ny78Z255xQA+ikzS0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFVYf+P+5+ifyNWqqw/wDH/c/RP5GgC1RRRQBx3jrTfDr6fHqOtaP/AGjLHLHHAkSAyyOWG1Bnggnt0rH0ew+y6rbT2/wuhsWDjF0JoN0Q6FuOeldX4s0iXW9BktYL9LCVXSVLp494iKsCGxkDt61zmn3WtrqFvHP8Q9EuhvAeFLGNWlHoCJOOPQUAeg0UUUAFFFFABRRRQAUUUUAFRzf6l/8AdNSVHN/qX/3TQA22/wCPSH/cH8qmqG2/49If9wfyqagDxfw34StfGWreKZrqd4ltfFLToUAO4xEjafbBrpdD+GMemfEfU/F1xfCczySSW1uqECJn6knPPHHSovg58+h6/cnlp9dupD+O3/69ej4oAo6xoun69pkmnanbJcWkhBaN+hwcj9RXzTrPw38WeHkvNTgglmiE82nQwKdxFvKjIGGOg+cj619S03bz1/SgD5z0H4Ga3JoUi3Fz/Z19JciG4QyZSa2yCcY75UEevtUmg6Nry+L7LwZLozpoum63Jem52sVK/eQZ6YwB+dfROPekC470AeeWnwp0m1+Is3iVbe0+yNEGitVjx5cwPLgdMHrXnzfCbXv+Fka/e6f/AKI8Mv27TLpwfKLs4bYcegLCvoXb70Bcd6APlPxjf678QdYv4NV05bTUfD+mu0sULFlZlkXc2O2d3TnpXX+E/grFd+HdRfVP9TewW9xZNG+JI5ArkhgRwPmAxXpuk+CU034ga74m86N49ShSJYCnKYxvye+SAa68KAMDoOMUAfLfha18X/El7Xw9PfPaWGjxs8EohAWOVAFUEjBzn3yOalvtT8c+OvGlkLRJLXVNHZbC5ntukTs7K7/TGc/SvpDStD0/RI549Ot1gSeZp5Av8Tt1NU9G8J6doesatqdp5nn6pKJZgxG0EDHAx+PNAHzVr114/uNcv9cnupJrnwzKbSa7hURkDc3zEDAwf5YrmPDGhXvirxXDbC3kla5cvIwH3Q2cMT2Ge9fX3iHw9Brnh7VNLXy4G1CIo8qxjOexI4zjiq3hXwbpnhaxtktoIzepaR2s10F2tMEHUjtk80AfPPh3XfFXwd1W80y801m+2JuigZAyyS4G1gw5OAcEA966i5+M/i3VbC40mz8PLZay0RuI5g3EUKqXZtrDrgHv3r1zxJ4LsvEus6DqVzM6PpFw0yIqgiTOPlPoMqp/CtB/Dekvr7a49ojag1sbVpTzmPOcYoA8D03SdS0TwBZfE2yu5E1RLhpb1FHFzC8oyD+Pt/KrMHxW8VeKtPt/D9hi11zUL3da3QUIv2cZYeuDlcH2Fe1eIPD0WpeCtS0GziigWe0kggUDCIxU7T9AcVznhH4aWukQ+Gr++JGq6RavCREwKMXJJJ4ycbjQBxtrZ3fwl1/RdU1nWLiaLWPMGrknfGJzyGAA7Zxmur1L4v8Ah6K/13SYrgxXdjC/kzsR5c0oXBVT6g8fnXaeIfD1j4m0S50rUE3QTpt3KBuQ+qnsa4O7+CGhXHhK10VJ2FxDcee9+8YaWQE/Mp5HGMD2wOtAHGab8dJU+HU0Essp8TQR4jnljUpMTIRkY7hSMggV7BoHi7TdV0bSZ5NQtlur6BXWLzAGLYBIx681gQfBvwnZQL5Fm7yRSyzRl2zguoXafVRtGB9fWvl620+/OoahJZNIJNMUzfJywAkWPj3+YflQB798NPGd5qPxC8VwXs7mwk33duZG+WNY3EbFfRcEflXoWg+PPD3iPTJ9QsL+PyLcM03mHaY1XOWYdhxnPpXkVx8NdVur7wdYWT3VpaXGlPDqN5EpwASZGVun3iQMf4Vm+O/Ch+Hd9e2mhQ3ctnr9j9kRcbtsu9Pl4HOV3HnuaAPoKXXdLh0+G/kv7dbOZlWKcyDY5Y4UA9MntWhuGM9q+YfGPw78T+H7LU2hvJh4d0ryrm3jkkJV2baDtGTyGzXTeKvizN4w0BdM8LaXfiK7ZLe7vHQr9nZ2wqAqSMn6/wBaAPdUmSVd0bK65xlTmpK+e4fFepfCzxBq3hSxsJtRhF3HeQq+5ylu0e6TBz1GAP8AvqvY7Hxx4b1HTTfQaxZmJYRNIDMA0anpuB5B/rQB0GfY0Z4zXmd78WrOaPwzc6NGslrq2qfYpftIw0aBgrMAD1+YEZrorrxf9m+I9j4VFujC6s3uDNv+ZWXPy4+goA6uikzS0AFFFFABRRRQAUUUUAFQ3f8Ax6S/7pqaobv/AI9Jf900ASJ/q1+gp1NT/Vr9BTqAGSRJLG0bgMjDBB7ivLb3RPDVp4gvrHRvACatPEEa6IMcUUJx8oG89SOeB716rXBa5pmqWHiS41Wx8X6fo8V4iK9vc2YfeVGAxJcc9unSgDY8IQvb2M0H/CLx+H4w4KwxyRMJOOW/d9Og610mK57wtPeTW9wb3xDY6y24BXs4FjEfXg4Zs/8A1q6KgBMUUtFACYpaKKAExRjjFLRQAmKWiigCu3/IRj/65N/MVYqu3/IRj/65N/MVYoAKKKKACiikzQAtFJupaACiiigAooooAKKKKACkzVHWdYs9B0i51S/kKWlsm+RgCSB04Hfk15R41+M9tZyaYvh+a1vLC8SVbiZgwaMgAAL0wfmoA9kDAjIxj1zXG/ErxB/Y3hWW0tiW1TUwbSxiX7zu2Bkewzmsj4K64upfDyziubrzLqK4mi/ePl2w+/vycBxUfhy3Pif4s+IdYvZWli0KRbHT4j9yIkHzGxj72R+p9BQB4lfeG7qHV7vwUgkYWRub9nP95bYEf+PLj8a99Gp+d8CZdQDDP9gyYPqRCR/Stk+CtIi1/VfECws2oX9v5DljlVXGDtHbOBn6V5lFqgtf2ar6CV8Swq9gB3LeaFx+Rz9KAO28OeFtP8QfCDR9E1ONmtprKJztOGUn5gQex5rzv456OuhWegjS4xFA1s2mbR3QbWA+vH617b4etGsPDWl2brtaCzhiI9CqAf0p2saDpmv2yW+p2kdzFHIsiK4+6wOQRQB4t4s1B/CHi3xPa2x2zaxo1sLZR/FJkQj9Aa5rVfgzreheIdLmtEF3p09zCjeWSXT7pYuOwyD+Vdz8Q9IW8+N3gpmTKSJg/SNy36bq9l2+9AHiMPjmz8BfFzxDpN7aSvBql1buskZAEOUPJB653L37GvbmdVUsxAUDJJOAK+efit4Xm1H4lajcpE5EeiG+VwpI3Rdv0/WvSvEXiEH4KXmtCTDT6VgP/tugX/0I0AYnxltT4ktPDeg2lwCdSvWKunzDCpnI9etcza/HCS10HwrEJUlvRMItXRo+RGDtBHvjB+tVfhBJrXibWPDzXFjs0rw9DOsd0oIDu2QASTycMeleha18KdDTwdrdlo9hGL66SSSOaQbnDkhtoPYcYoA85+GPw5g8bahfeJ9d897Frpnt4w4xcHc2dx64BA4zX0WiBFCrwo4AA6CuL+FGj3eh/DbSbG+tnt7tRK0sUgwwLSseR9MV21ABXnXw7Y6Z4o8YaBIMNFfi7j7ZjlHH/oP616LXjvj7TLzQfin4b8XwXCpZXN3BYXSBiGYksMkdCu2gD2EnAzXz58Zbi8m8cpqFlhv+Ect4LoqeRuaXj9QK9T+KGuXfhz4eapqVjN5N5GI1icdiZFB/QmuI+G4tviFdeJ9R1OET217Ba20yngMwjUuBj/bBoA7/AOHviK58V+BdN1q8SJLm4VxKsQIUFXZeAfXbn8a6mvN/gr+48F3emE4fTtTuLZlPVcNn+tejswVSzEAAZJPagABz2pa8r8H/ABd/tzxIdH1LTJbf7XO66dcxJ+7lRSwO4k9QVxkZznt39UoAKwPFviiw8I+H5tUv5SiDMcWFLbpCDtXj6da368i8U6pYePfiDpHhK2ga5tdOujPqUjKDE2xfuZ78nH40AQ/s9XEp8L6raTBlkjvBKwbqfMQHP4gCvY6830BY9D+MniDTFUQw6lZwXVumMAlFCEKPbBNekUAVh/yEz/1xH8zVj+IfSq4/5CZ/64j+Zqx/EPpQAtFFFACN90/SjNDfdP0qvetdLZTNZRxyXQQ+UkrbVLe57UAeX6t4aaO/vdb1PQpdSaPWTJIFgEzy2hjCqEXuFJ5HHQ10vw4tGttEv/L0ybTLCbUJJbG1nj2SJCQv3l7ZYNx6YrM0iX4g2bam82laVtlvHlUz3z/Ku1RhflPyjnH410/hSx1K00+7m1W4imu7y7e5KwuzxxAgBUUnnAC/zoAu61oVlr9rHb3yuyRyrKhRypVh0ORXOaX4FXTfGkurrNMbUW6LErXUjPvySd2eCv1NdtSbaAM7WbK8v9Oa2sr02TuQHmC7mVO+3sDjoecVlWvg+DSL+0utFmNpswl1G2XFwnPLZP3ufvV02KWgCC6acWsptkRp9p8tZDhS3bJ7Vx+gaV4p0/xNqeo3sGleRqUsbzCK4kZowibflBQZzjNdsBjvRgUAcZJouv6RqWrnQ1sZbTU5DORcSMjW8pUKxGAdwOAe1b/h/R10PQrXTlfe0S/PIR95ick/iSa1McdaMUAJsFQj/kIN/wBcx/OrFVx/yEG/65j+dAFiuc8d8eA9cP8A05ycfhXR1zPxBbb8PtdP/Tm/8qAPnnwR4SXx3pHiWKC3hl1KC0sltHkbbsYJtOD9Fr2bxB4M1xNS0vxB4YubW31m1svslxHcZ8u4TbwOPQ/09K4b9mhcjxK3/XsP/Rte/YoA8ktPhJOvhi2vBcCz8ZRzG7OoIxYs5JOwnuuDiszS9b8afDENc+LrW51TTL4mRpLd/Ma2nOfk+hOB6c969uxx1pNg/wA/XNAHlN78WNf0dbe51rwJqFjYSOC8+/zPLjJwSwA4I9DiuJtNO8XaX4fuPinFqzLcyvJObC4DFfIZyO59wQMDivop4UljaORQ6NwVYZB/CsfxTop1rwlqWkQBFe4t2jjB4AbHyj2HSgDUs7gXdjb3KnKyxq4/EZqfNcL8OvFUd7pyeG9Qiez17SIUgubWYjc+1QN6+qng/iK4h9X8bapreseMNNnM2n6JfPYppcSsTcRA5c7R1bDLz7UAe5UVheF/Ful+LtL+3aZKW2tsmhcYeF/7rDsa3aACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKqw/8AH/c/RP5GrVVYf+P+5+ifyNAFqiiigDm/GWnzaposNrFC88TXkBuYkPLwiQbx16d/pXnsmgR6frrabbeFJzqC60Lq0v4rQfZ1t2ZWOZOgwNwwe9dt41XxTL9jTw/ZW08S3MMsjvcNG3DgkYA5XHX2qrcJ4y1a5s7O+i0nToRcRzSPb3jtK6oclQuBkHGD7UAd1RRRQAUUUUAFFFFABRRRQAVHN/qX/wB01JUc3+pf/dNADbb/AI9If9wfyqaobb/j0h/3B/KpGbahY9AM0Aec/BT5vAs0n/PTUrh//HsV6RXnPwPUj4YWTHq9xO2f+2h/wr0agAooooAKKKKACiiigBMe9LRRQAUUUUAJijFLRQAUUUUAIRnvzRilooAKKKKACs610LSrJpHtdPtYWlzvZIhlsnJz+PNaNFADQmBjt6dqa8EcpQuisUbcuVB2n1HvUlFAFDV9ItNd0i50u/QyWtymyRQcEj61X0LwzpHhqyNppFlHawlt7BRyzYAyT17Vr0UAY83hjSbjXn1uW1DX72ptDJn/AJZnqP1rz3SPgJ4fsb62ury4ku/KlkkkiKhUlDEbVI9Fx+PtXrVFAHyd480bVNG8dX1jpFk7WWm3I1CCOJMrF5nlgdPcJx2qzrkvxGtviLBqE1rLPrEUAhhlhg+TEkbfhkBmz7ivqXykEhcKAxABbHJA6Z/Ol2+/6UAecfDj4nW/irw/cNfxtb3+m24kucnPmqF5kHTrg8dvWt3w78R/DXib7HFZXypeXcZkjtJcCUAEgggZAPB4z05rkPjB4bOn+H7jxPokhsrq2tza3EcKhUlgf5SCPUbv1rzvSPDsvw40rw34t1eIK0moiR/K+ZhbtFxx65Y8UAfT2aN3tXnfxI+Itt4c8GJf6VfQm/vUSSyUjdvQkZfHpg9feuV+CfxH1vxPrN1ousTJOIrZriOYj5yd6gg+3zfpQB7fRSbuaWgAooooAKhu/wDj0l/3TU1Q3f8Ax6S/7poAkT/Vr9BTqan+rX6CnUAJmvMPFPhmW81XV9VudHbUfIubSaBNgdpIEH7yNB68k475r0yVnETmJQ0mDtBOAT6V59Yy/EODV9VnfSdK2StGVEl8+xQBj5fl/wAP60AWvh9bRxXuu3FjotzpOk3M0bW0FzbmCR3CnzG2HkLnbj8a7uuZ8LW2q/aNT1PVp7Uy3sibIbWUyRRKi7cAnuTkngV01ABRRRQAUUUUAFFFFABRRRQBXb/kIx/9cm/mKsVXb/kIx/8AXJv5irFABRRRQAVxfxK8aTeBdAs9Sit45zNfR2zBycBSrMTx3+T9a7SvFP2kJseFtHtgeXvTJj/dQj/2agDK0X4j+P7NrfxDrdp5nhmSVFd9gX5JCQrKR1K4/l617+GyMjpXBjwvFrXwat9CY7PN02Mow/hcAOp/76ArS+HOty6/4C0q/uSDcGLZKf8AaU4/pQB1dFZ9lren6jf3tjaXKS3Ni6pcxrn92SMgHt09Kv5oAWoLu9trC3e4vJ4reBPvSSuEVfqTxU2a8z+N+oxR/C3UY4pVZpJ4oDtbO1g4Yg49hQB6QtzEzKqyIzMu9VDDJX1FZniDxNpnhiwhvdWmMMEs6W6sBn52zj8OCc+gr5h8W+M9RuNZ0zUNIu2iaPSYbJ5om+7vXlc9jkH8qseIX8U36L4B1a6a6udN868SQZO5EgZwMnkngjn1oA9l+OV49r8MLuNSd1zPFDgd8tnt/u1806LpMup6LrVwVcpp8AmzzhSXUH8f8K9i07V9a+Jmu+HrG3NvNpGlpZ3d8zEbvNA+bPryMYr12fwVoEmk6jpsOm29pb6hH5c4toxGWGMDoKAPn3WPDM3gfwl4U8WaHc3Qvr+OMGItlI5JIQcqOxOOa9w+G/ha78MeHZBqV19p1K/uGvLuUHILtj19gP1ro7XRrG00uy05YEktrKNEgWVQ20Iu1Tz3x3q9igAxXk9n8GYLjXtQutav5LjT2vDc2dlE5VELNuYuO+emK9ZpMe9ABiloooAzrvQ7C91ay1SeAPd2QcQSH+HdjP8AKtGiigChrNsLrRr6EjJkt3Q/iDXid9ctf/AbwjpWf+Qhf21jIPUb2P8ANRXu8yeZC6f3lI/OvnzSJPO/4QvQG/1ln4nuN6+nlNuHH0NAHvenaVY6Ta/ZtPto7eDcX2Rrgbj1NW8UtFACAYpaKKACvPfjPAz/AA4ubtB89jc29wh9CJAM/kxr0KsjxLo6+IPDOpaQxUfa7d4gxHAYjg/gcH8KAPN/HOup491PTfBeiQNdK9xBdahcpkx28YOSrcdcc9eoxXqljpllpkJhsbWG2jZixWJAoJ9eK5j4eeB4PBXh1LVvLl1CUlrq5VeXbsM9wK7KgDznwWo0/wCJ3jrTl4ikmgvFA6ZePLn8zWz471+PTPA+uXNrcRNcwW5XCOCVZhhQR2z2rzX4s6jrvgrxhPrulQb7TV9PFnPKYyVVgSByOA2CuPXBrjhpL+GTq/hd3ZpL19JkYN1LHDOP++mYfSgD0TVNHTw3Y/C8gY+zX6QyP7yrub8ypNex54rzT40FtP8AA1jfxgZ03UYJ19sZUf8AoVP+FHxIbxxYXUF6mzUrQh5NqgKyMx2kfQDB+tAHossgjhd24VBkn0HeuA+C8Wfh3b3hUCS9uJp3PckuR/MVr/ErWpfD/wAPdY1G3CmZIhGm7pl2Cf8As36Va8DaK3h3wTpOlM6u8EA3MBjliWP6mgDXk02zl1GHUJLeNruFCkcxHzKp6jNW6KKAKw/5CZ/64j+Zqx/EPpVcf8hM/wDXEfzNWP4h9KAFooooARvun6UYob7p+lLQB5deaNqt9qd/4fuNMnawvtX+23N6GHlSW21cISDnOVCke2e9dL4G0ptIt9Zs0tntbBNUl+wwtwFi2p90f3d+/Fc43k634j1uPWvGd/pctrdeVBZ2eorbIsO1SrHuxJJzk/pXaeGbOzs9LaKy1i61WLzWP2i5u/tDA4Hy7umBxx70AbdFFFABRRRQAUUUUAFFFFABVcf8hBv+uY/nViq4/wCQg3/XMfzoAsV578ZfECaF8PruNoTI9/8A6MuDjaWHX36dK9CrxP8AaQm2+GNIhz968LfkhoAo/s0L/oniR/V7cfkJP8a96rwr9mtcaT4gb1nhH5K3+Ne60AFFFFABSYpaKAOI8a+E7+/1HTvEXh028WvaexAMows8ZGCjEfXg9sn1qb4b+HL/AMNeFfs2piIX09zLczCI5UMzcYNdjikxQB5tpkMej/HjU7WFFhh1PSFuSiDAZ0cLnHr96vSc8dK8q+JOonwp4+8MeKvs01xCsNxaTJCpJbK5RfxZv0rc8A+NtR8V3ur2mq6SNLurJkItyxZgjgkZPfpQB3VFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVVh/wCP+5+ifyNWqqw/8f8Ac/RP5GgC1RRRQBzXjO1up9GiktLV7t7a8guGgTAaREcMQM9T3x3xXHS+H77VLqXxTc6Xcw6xPqsAslc4ktrVCoOcHgEbyR7113jy9ubDww8tvfGwUzxRzXSkboY2cB2GeM4JrG07RNCW9tpovH+s3MgdWWKTWwyyHPQqODnpigD0GiiigAooooAKKKKACiiigAqOb/Uv/umpKjm/1L/7poAbbf8AHpD/ALg/lTL9vL065f8AuxMfyBp9t/x6Q/7g/lVXXX2eH9TfpttZT+SmgDj/AILLt+FGjcfeM5/8jvXoFcR8IE8v4VaCP+mUh/ORzXb0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBFNbQ3EDQzxJLEwwySKGDfUGs7W/DWkeI7COx1ayjubaNw6xnIAI4HStaigDymx+BWhR3rS6le3eoW8TKLOGR+IYgWPl+65b2rzLwzeWngH4x6rPPiz0uOW8tUYg7BgZVf/QPzr6jxXJ+Nvh/pXjqytrbUZJofs8plSSDaGJIwQcg+35CgCH4aeKrrxd4Ni1W9Mf2gzSI+wYGAxx+mK7KvmGw1y68C+JNL8I297cRQ2fiFjcHfsEsDCMDPQEHL19OBwVDAggjIIPWgB1FFFABUN3/AMekv+6amqG7/wCPSX/dNAEif6tfoKdTU/1a/QU6gBMV5p4h07WrjVtZ0m30+6a31qS2X7bGwCRQKAJVPcHAOMDnP416ZXm2qmLVPGupWOr+Lr3SIbdIjaW1nerb71I5ZjjJO6gDZ8JaGmga74gtrCyaz0otbm3i5CFwn7wrn/gP412Fc74XsLCxguEsfEF7rCuylnur4XJjOOgPb/61dFQAUUUUAFFFFABRRRQAUUUUAV2/5CMf/XJv5irFV2/5CMf/AFyb+YqxQAUUUUANZwqlm4A5JrxPxLrPh/4m+PfCmjQBrqygu7yO6RwVDFI0bjBzjg817JqLbdNum9InP6Gvmf4FWUl78UZrrBMdrbzSknoNxCD880AfTsVvHBbpBEoSJECKo7ADAFfMtx4lu4LC6+HumvcWmpf8JCRBPG5UbGkOBxz1wfpX0/XCf8Km8O/8JeviYNd/b1vPtePMGzdjgYx0zz6570AeNeBdf8Q2Grnxze3Y/sy71BLPUh2ZmGN5HbB5rqfid8YoUm02z8Jao5dJVmnnhAKMuSNnIznvXo9h8N9FtPC2peHZfNuLG/uJJ5A5AZSxBAUjptwMVzPxA8A+GdE+G2sz6ZotpBcw2qhZlj+f5cL19SOp7980AdX4s8aQeEtP0q6uog6X11HbM27aI9wJ3H2GK+a9Nj1PxN48jsN92+k6hqjzeWGbyZCCcsB90ttHXrU2oeIdd+IHhew0eRpL3UW1aRoIsDKp5agKPYEn86+nvC2gWvhvw5Y6XaQ+VHBHkrnPzty3J56k0AeNeF/hVeap8N7ayliFneLrf2qT7RGQzRIDGR06Y5H417G3g/RH8TnxE1mDqTQGBpCSQykYOV6dOOlbm33paAOI8EeBE8H6z4guoWi+z6hceZbxxrjy067T+ddvSYpaACiiigAooooAKKKKACiiigAr598LWskv7R17aEfuLG4ursD0LRqmf1FfQVeSeFLDb+0B4tusDCWiD6byp/8AZaAPW6KKKACiiigApMe9LRQAmKWiigDP1bRNP12y+x6nbLc24dZNjEj5lOQcg15/r3w0u9b+Ktp4hNzHFpcUURkQH53kjJwMflzXqFJigDn/ABp4YTxh4VutEkn8gTlD5gXOCrBv6V59428H3/hNz4y8OambQ6XZRRvaqnFwqMAd/qNpJOc8jtXsOKx/Femyax4R1fTYgGlubOWJAT/EVIFAHCfFvUY9S+GWnxKwWTV7i1CR55bcQ/Hrjj9K9TCgAAcAcAV4T4W1OPxV4s8C6fNB+80S1uDcwuAdsqDy8/mFP1Fe70AFFFFAFYf8hM/9cR/M1Y/iH0quP+Qmf+uI/masfxD6UALRRRQAjfdP0paRvun6VU1PVLTR9NuNRvpRFa26GSWQgnAHsOT+FAHn2pRy67ruo/2H4M0S+S0nMNze6iwUyygAsqgKScZAz610/gq9tLrSJ4rfR49Intbl4LuyjA2xygKTggAEFSpzjvVKXwtqDXtxqXh3xFcaTFqDCee3e0WVS5UDeA2CjHAz2q54Jt9Nt9IuPsF5cXsj3kpvLi5UrK84O1tykDBG0ADHQCgDp6KTNGeelAC0UmeOKM0ALRSZo3UALRTdw/w96o6nrFvpSw+ck0kk8nlxQwRl3dsZ4A7AAknoKANCq4/5CDf9cx/OmafqNvqllHd2rExPn7wIIIOCCOxzTx/yEG/65j+dAFivBP2lXP2bQI88F5T+g/xr3uvAP2lT/wAi+P8Art/7LQBr/s9QeRp3iSPGNt+q/kDXtFebfCbw/qOgp4ie+tWhjvNRMtsSQfMjI4YY7c16TQAUUUUAFFFFABRRRQBHJCkoAdVYA5wyg89q8xk1az8MfHO4ivZDDHrtjAkBwSGmVioH48/jXqVeZ/GTQ4pPDA8SwQj+1NGminhlGchBIMj6DOfwoA9MoqtZX0V/p9vewcw3ESyofUMAR/OrNABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVVh/4/7n6J/I1aqrD/x/3P0T+RoAtUUUUAc/4vvbSy0CRrvTBqXmSJDDZkDE0rHCqc8AZ71xtvDcaDf2d1rngbQbWzlnSJbmwdXe3djhdwKjPOBlfWuv1+LTNfd/Dk1xPFe7FukeBTug2sNrhsbQc9j71hXeh3Vteab/AMJT4suL6z+1Ri3gFksSvMOU8xlByOM845oA9AooooAKKKKACiiigAooooAKjm/1L/7pqSo5v9S/+6aAG23/AB6Q/wC4P5VQ8Sn/AIpbV/8Arym/9ANX7b/j0h/3B/Ks/wAUHHhPWT6WM/8A6AaAMP4UDHwu8P8A/Xtn/wAeNdlXHfCsY+F/h/8A69R/6Ea7GgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA4Lxz8LdF8XwXVwEW01abYTfKhdsKemMgcjjP09Kk+E9/cXngW3hupmknsppbRmb73yMVGfwrua87+Fw8qfxZbDgRa1LgemVBP6mgD0SiiigAqG7/AOPSX/dNTVDd/wDHpL/umgCRP9Wv0FOpqf6tfoKdQAV534hD6x4kurPSPCekalcWqoLu81IgKpIyEAClj8vNd/cXEVrbyzzOscUSF3djgKoGST7Yrj30SbWJ18R+GPEFxpf9oRI0m+zEiTAD5WKOAVOO9AE3gm6gb+0LBtBt9F1G0kT7Vb25BjfcCUdWAGQeevvXX1yng60srW51gDUrjUdVFwqahcXEfltvC5VQuAAm1sjGRyea6ugAooooAKKKKACiiigAooooArt/yEY/+uTfzFWKrt/yEY/+uTfzFWKACiiigChrT7ND1Bj2tpD/AOOmuM+D+j2Np4D03UobSOO9vIFM8yj5pACcZP41F8ZvFd74U8Gxy2SxM97P9lfzBn5CjE49+P1ra+GSeX8NPDwx1soyfxFAHW0UUUAFYvivSJNf8LalpULokl1A0atJnap98VtUm2gDwvwZ8L77wr8WbJ2iaXTrbTxN54U+WZjGEYfUtlsehr3THvRtpaACiiigAooooAKKKKACiiigAooooAKKKKACqkWm2cF/PfRW8SXU6qssyqNzhemTVuigAooooAKKKKACiiigAooooAKKKKACkxS0UAcX4Z+HNj4Z8Xaz4ghunmk1EnbE0YAhDNuYA55yceldpRRQAUUUUAVh/wAhM/8AXEfzNWP4h9Krj/kJn/riP5mrH8Q+lAC0UUUAI33T9K5/xrpV3rPhK+srEKbshJIlc4V2R1cKT2B24/Gugb7p+lGKAOBi+LPh+OFYr6DVbTUgMNYPYTNLuHUDC4P1zWl4Fsr2LT9S1G/tWtJtWv5L1bV+GiRgqqrf7W1Mn3NdDb39ndz3ENvdRSy277JkRwWjPYMOoqSC7guWlWCZJDDIY5NrZ2MMHB9DyKAIdVsoL/Tpre4aVYWALGJircHPBHNcF4bubR/HawaDNeR2CWrm8gvGdS75ARkR/m47tjFd7qkuoQ2RfTbeKe4DDEcr7AV784Nc5b6VrWq+LtO1vVLW3sYtOjlWOKKXzHlLrtwxwBtHXHPNAG9rU+pW2nPPpVslzcRkMYHbaZEH3gp6BsdM1i6f4wGv6ja2+hwGSNQHv5Z1Ki3HP7vH/PTjp271t6zbaheac1vp10trPIwVpmXcUQ/eKjpux0rHsvB6aFqFtc6HO0CE7b2KVi4uRz85/wBvJ6/nmgDory2F5Zy2xkkjEilS8bYYfQ1yHgu3ax8S+KtPW5upoLaeARfaJmkKgxZPJ9zXYXLTJayNbxrJMFJRWbAJ7DNcf4fsfE1l4o1S/vNOskt9TljeQx3JYxBE28DbznGe1AFSx08+L9R8SXd7d3SizvZLCzSKdkEXlqMvgHklj3o0q8vb/TvDWvyW8141mJ7e5EKguc/J5gGRnmMdP7x4q6dK1/RNS1n+xre2ubTVJftCmWXYYJWUBiRg5BIB4re8O6P/AGJoNrpxfe8Sne443MTlj7ZJoAi8MWtxa6W5uYWhlmnkmMbHJQMxIB5PNag/5CDf9cx/OptvpxUI/wCQg3/XMfzoAsV8+/tKcz+Hh/sTn8tlfQVeceL7eG7+LHg23uIo5YjBdlkkUMD9zqDQB31ku2xtx6RKP0qzSYwMDj0paACiiigAooooAKKKKACq1/YQalp1zY3KB7e4jaKRT3Vhg/oas0UAeGDxvr3wjhTw/ruk/b9PhVhp93DJgyRh/wCLrjCnoe9e3xTJNEssZ3I4DKR3BrM8SaFD4k8O3+j3DbY7qIx78AlD2YZ9CAfwrnfhdrN1qXhT+z9RwNR0id9PuAP+mfyqfxA/Q0AdzRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVVh/4/7n6J/I1aqrD/AMf9z9E/kaALVFFFAHBazqX/AAiHjSfW761uJdKv7ZIHuLeEyG3dCcbgoJwQetU9Q8UWXj2Ww0nw3FdXUQu4p7i9a2eKKFI2DHBcDLHGBiu/vb60063+0X1zFbQhgDJK4RcnpyaWa5t7WNZJpUjjLBAzMAMk4A9KALNFFFABRRRQAUUUUAFFFFABUc3+pf8A3TUlRzf6l/8AdNADbb/j0h/3B/Ks7xV/yKGtH/pwn/8ARbVo23/HpD/uD+VZvis/8UdrZ9LCf/0W1AGP8LP+SX+H/wDr1H8zXYVyHwu4+GHh7/r0X+Zrr6ACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArzz4e/u/GHjqD01QOPxQf4V6HXnngz938TfHEPrLBL+akf0oA9DooooAKhu/wDj0l/3TU1Q3f8Ax6S/7poAkT/Vr9BTqan+rX6CnUAZPiPTH1nw1qmlxyCN7y1lgVj0BZSoz+dcfp3xM0fSNPttP8QW99peq28Yhe1azkcOygD92yAhlOOK9FxVWO/s5b2ayjuoWu4VDSwhwXUHoSOuKAOY8GJd3+o614juLKewh1J4ltre4XbJ5cakB2HYnJwOwwK7Kq6XUEs8sEc0bTQ48xAwJTPIz6ZqxQAUUUUAFFFFABRRRQAUUUUAV2/5CMf/AFyb+YqxVdv+QjH/ANcm/mKsUAFFFFAHmPxT0+313xJ4L0O8TzLa6vZHkTPVVQZ/9Cr0LTNMttI0u206zUpb20YijBOSFHSuG8XnzPi54Di/ureP/wCOJ/hXotABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAFYf8AITP/AFxH8zVj+IfSq4/5CZ/64j+Zqx/EPpQAtFFFACN90/SjNDfdP0qlqs0kGmTvDdW9rNtKxS3P+rVz0zyM0AcPb+Ers6nrWt6ZMtlrK6lI0Um4NHcRbExHKAemc47itbwCL/7JrU+pWQs7m41WWVoRIHx8kY6g/wCyTzj6VzenL4r0e51OKXxT4Xt5Z71p3WWM5JKqMgeYMcDofzrr/BunLY6XdOdUg1K4u7uS5ubi3AEfmNj5VAJwAAo6k0AdJigDFLRQAmMUYpaKAExijFLRQAm2jHvS0UAFVx/yEG/65j+dWKrj/kIN/wBcx/OgCxXnviT5vjL4QHpaXZ/9Br0KvPde5+NfhYeljdH/ANBoA9CooooAKKKKACiiigAooooAKKKKACvJ4r268LfHOfT47b/iX+I0SbeeFEqoQce/H616xXnnxB/ceL/At2ONupPGf+BRkUAeh0UVS1PV7DRrT7XqV1Fa2+4L5kjYGT0FAF2imJIskayRkMjKGVh0IPSnZ9qAFoopN3SgBaKTd7UbunFAC0UUUAFFFFABRRRQAUUUUAFFFQ3FwlrbS3EpxFEhdz6ADJoAmorD8KeKdP8AGGgxavp28QSMylJBhkKnkHH4H6GtygAooooAKKQtjtRn2oAWiiigAqrD/wAf9z9E/katVVh/4/7n6J/I0AWqKKKAOY8a6YNX0e2s3gSeJ7+3MscmMMgkBYHPt261zGr6BrWhpY6XYk32gm+geITyAy2QVx8uW++mBx/EOnNanjqDWb77Ommaxo9rDa3EM88d4PmVlcEMTuGBx07+oqmItW1u7soNW8XeHpbWO5SYw2Me2SRlOVUEucc+1AHo1FFFABRRRQAUUUUAFFFJmgBajm/1L/7pp+famTf6l/8AdNADbb/j0h/3B/Kszxaf+KM13/sH3H/otq07b/j0h/3B/Ksvxef+KK14/wDUOuP/AEW1AGX8Lx/xbHw7/wBeaV11cl8Mf+SZeHf+vJK62gAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK888PfufjP4ti/562drJ+W4V6HXnlgfL+PGrL083R4m/J8f1oA9DooooAKhu/wDj0l/3TU1Q3f8Ax6S/7poAkT/Vr9BTqan+rX6CnUAJmuAuvCEmpeKta1W2nNjqcbwmzvIypPCco6g8qT2NdxdyiC0llaaOAKhPmykBE9zntXmtuvizTdY1K4m8U+FoJLvynzLGRuAXAIHmAgY+uaAN3wYNYk13xDc6zZJa3LtboVjkDK5SMgsvcA8Hnmu0zXK+EbF4pdU1C71my1O/vZI2me0ULHGFXCqBk+/Oea6DUTeiwlOnLAbrH7vz87M++KALWeKWuNj1jxBpPiTTNM1prC6h1LesclojxtE6ruOQzHK102pajDpVhLe3AcwxDLeWpZsZ9BQBcozWVdeI9MtILOZrgSLeuqW4i+ZpC3cAdgOSe1aExlEDmFQ0oU7AxwCewoAkz68UZrlNA1nXLjxLqGk6ylgDbwRzIbQP/ESMEseenoKbe6xrWpeJL7SNCaxiXTo4zcy3cbPudxuCAKRjjBznv0oA6t5UjUs7KqjqxIAFKGDAMCCD0IPWuRXUrPxN4FuLrU7a3V41ljlimwVSVMqcZ9xxWx4Zljl8M6aySKwFugJU55280AaDf8hGP/rk38xViq7f8hGP/rk38xVigAooooA868QfvPjl4PT+5ZXb/muP6V6LXnOqHf8AH3Ql/wCeejzP+bMP6V6NQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBWH/ITP/XEfzNWP4h9Krj/kJn/riP5mrH8Q+lAC0UUUAI33T9Kr3dnbX9uYLqCOeIkHZIuVyKsN90/SqOrWt/d2DxadqH2C5JG248lZdvr8rcUAcJqEkOra5qX9k/D2y1j7NcGC5vbmaKDfKANwG5SWxkc11fhW3lg0pkn0C20N/NJFrbzLKpGB82VUDJ/pXL6X4Y18XupS6d48UyNcYu1TS4tolCjPGcbsbc11nht7k2VxHd60mrTw3LxSTLAsOxhjKFR3HXPuKANuiiigAooooAKKKKACiiigAquP+Qg3/XMfzqxVcf8AIQb/AK5j+dAFivPdY5+OHhsemm3J/lXoVee6pz8ddB/2dKuP1I/woA9CooooAKKKKACiiigAooooAKKKKACvPPin+7Pha46GPW7cZ/3mAr0OvN/jTP8AYvBdrqO0t9k1K3mwPQNn+lAHoxbHb8TXmfxqtP7U0PQ9JGSb7Vo4sDqfkc/zxXP+FPjJrmt+ItFsb3RIrWzvm8qS42uAXO4qUJPTaB1z3rofFV0/iD4qeHNCsIWlOjyjUb+XI2xAjCD64OfxFAHm9j4z8WTjQ9B0W1uhf+G0llv4WbaLlEKgD3+XI/GvVvh18QZfF/2q01LTzp+pQYlWAqQGhP3WGfyP4V1Nv4b0y08Q3muwW+3ULyNIppcn5lXpx+A/IVxHxC0zVtB1iPx9oTq8llbi3vbIxkmeDeCSCO447dB7UAem1ieLNYk8P+E9T1aKNJJLSBpEST7pPbOO1WdE12w8Q6Tbanp03mW1wpZCQQeOoIPcVwXx015tJ+H8tkkO99TcW+7djYAQxOMc9MfjQBLq3xKurLwV4a1my0tb691gov2ONiPmK5cL16EEc1o/D7x1J4qS70/VLP7Brlkx+1Wu0qApYhSufwFeYfDu5k1DWPBegzf67RJb/wA9c/ddTx+jEV3vi2J/DXxL0HxPbuFg1JhpN6hHBByyNn14/wDHRQB6XRXP2/jLSLjxld+FVkddTtollZWXCuCAxCnuQCM/X643DPGoYl1wvDc9D2B/OgCSiuM8FfEPTvGD3turQ297b3EkYtvODO6KcBwOuDU+q/Ebw9ovi228NX88sV9cCPy28vMZLkhQW7cjuKAOrzS15b8JPGd34nu/EsF7dtO0F80luGIO2FiQFHsMfrXplxdQ2ltLcXDrHDEpZ3Y4CgDOTQBNSbqpR6tZz6QNUt50mszEZllQ5VlAzkGvALr4g/E/UL6bU9J0+VdMuIDNBGsG9PJUlSwPXPOfX2oA+hZ722tniSeeOIyttjDtjceuB68A14X8QPEF/wCPvENxong/ULll0yzlllFuSFuJMgbAR1+UkfUVw+op438UaNqGr6xqEynw0I/3L5Em9iAWGBjIHJJ9K97+GXhfTdI8IaPfQ2MMd/cWMbSzqmHYN84BP40AeefDn/hJfhzr9j4f1ey2aNqzoVlfjZcNEGIBHoRtI9q95MihgrEBm6AnrXF/FHw5ceI/B8q2MqRX1k4u4HZiMFAScEd8dPevLJtS1vx0vhOPRL+U6ppejvdylH5eZW2BX7ZYxnj/AGvegD6Jz7UteOaf8erN59Otb3RrhJJo/wDSnjkDfZ380x4YYHHAJPGMgc9a9V1PV7HR9Ln1O+nWKzgTfJLgsAvrgZJoA8s+OHi64sLbTvDukNcrq9zPHcKYTjKKWwuQc5LAcY7Gsz4X/ETV9f8AH90up3EiafqULPaxSMNiSJtyq/rxW74Iit/G3jrWfGU32e4tLWT7FpoCfdAAJkye/P6mn/GLw9p1j8PZtSsbOK2udPnWeGSFdhQu4DdPXNAHqlFMilEsSSL911DD6GnZ9qAFqrD/AMf9z9E/katVVh/4/wC5+ifyNAFqiiigDmfFZ0jTNMnvbrRYtQmuZIoBCI1LXEhbCKSfc1zunabeLqVsz/C7SLRBIMzrfQs0Y/vbQnJA960fGeialqIJfxWNNsHkiWOH7Ejssu4bCrk53FsYpw07xJpMtrNqXjlXtfOSMo+mxr5hJ4XcDkE9M0AdrRRRQAUUUUAFc2fFyr4mg0aTSb+LzndI7mVVWNyg5285IrpK47xFLGPG/hjMijEk27kcfJx9KANnW/ECaO1rAlrNeXl25SC3hxlsDLEk8AAdTUOk63Hr8V7amK50++tWEVxC5XfEWGQQRkEHsRWXrBEPxH8PXEpHkSW1zEjn7oc7SBnpkgH64qPSFW9+I/iWa3ZvIW0t7eSWNsfvcE4BHcKR9KANbwnJO+m3KXFzLcmG9niWSZtzFVcgAn8K3Jv9S/8Aums7SNBt9GM32ee6kEzF2E0xcbickjPck1ozf6l/900AMtTm0h/3B/KsrxiceCNfP/UOuP8A0W1aNvHKbWIicjKD+EVk+MUlHgjXyZyR/Z1xxtH/ADzagCn8M+Phn4d/68k/lXWZrkfhtHKfht4exMQDYx4G0cZFdT5U3/Pwf++RQBLmjNRCKXvOeo/hFAilGP8ASD2/hFAEuaM1F5U2MfaD/wB8igxTHOLgj/gIoAlzRmojFLzi4I5OPlHFHlTf8/B/75FAEuaM1F5Uv/Pc9c/dFAim4/0g9v4R+P50AS5ozUXlTf8APwf++RQYpiCPtB5/2RQBLmjNRGKU5xOR1x8o4o8qbn/SDz/sigCXNGai8qb/AJ+D/wB8igRTf8/B7fwj8fzoAlzRmovKm/5+D/3yKPKm5/0g8/7IoAlzRmojFKc4uCM5x8o4o8qbn/SD1z90UAS5ozUXlTf8/B/75FAim/5+D2/hH4/nQBLmjNReVN/z8H/vkUeVNj/j4P8A3yKAJc0ZqIxTHOLgj0+UcUGKXn9+e/8ACKAJc0ZqLypv+fg/98ijypf+fg9v4RQBLmjNReVL/wA/B7fwijypsY+0H/vkUAS5ozURimOcXBGenyjigxS84uCOuPlHFAEuaM1F5Uv/AD3P/fIo8qX/AJ+D2/hFAEua88b918foj2m0F/0lX/Cu+EUw/wCXg9v4RXnmrJLF8c/Dx84hpdLnj3bRzglsfpQB6RmjNRGKYggXBH/ARQYpTnFwRnOPlHFAEuahuz/okv8Auml8qXn9+ef9kVDcxyi2kJnJwM/dHagC0n+rX6CnU1P9Wv0FOoAgubeC9tXguYo5oJF2skgDKw9x0rgdba1vPEF1YaX4Fs9anskjW4uJ5IoVjyMqgLAkkD2ru7+3ubixmis7s2lw64jn8sSbD67Twa4O08M+IDrl7LbePR9uCotyqaZFjOPlJGcZ680AdD4StZra3uRN4Ws9BJYbY7WdJRL7naoxjjr61q6zqsOiaTcahOkkkcKbtka5Zj6AVn+Hft0M19Z6jr6atdQOgYC2WEw5XIBA65BBzW8VBGCBg+1AHnvhXVNO1TWk1bU70S63cgxwW4R9lpH18tSRjcRyW79OK7bVbw2GmXFytrLdMi/LBCuWkJ4AA+pq2I1ByAB9BTiDjrQB5tovh/U/CusRazcWUdzHet5cttbAsdN3tn93k8pyA2Pr7V6O7rGhdjhVGT9KXb70uOKAPPNK8S6VL8R9QuEuG8q6tYYIXMbYZwxyOnuKsRX9v4W8b+IZdULxWupLBcW8wRmViqbGTjPzZA49K7nylzkKAfYUGNW+8oYZzyM0Acl4K0rf4alfULUf6fdS3ZgmX7qux2gg98AfnXU21nb2cXlW0McMYOQqLgVNiloArt/yEY/+uTfzFWKrt/yEY/8Ark38xVigAooooA84nPm/tDW4H/LDw9k+2Zm/xr0evN9L/f8Ax/12Tr9n0aGIe25g1ekUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAVh/yEz/ANcR/M1Y/iH0quP+Qmf+uI/masfxD6UALRRRQAjfdP0paRvun6UtAHBvD4n8Manqj6Tplpqmn31y12DJdiB4JGADBsg5XIz+NbHg/SbzTbC8utRlgkvtSu2vJxAcxoSqqFU9wFRecda5O70XWL/VL7w/caZcPp17q/224vd4MT22FIjPOc5GCPbNdJ4H0p9Ig1m0S1ktbFdUl+wwsMBYtq8gf3S+8j2IoA6yiiigAooooAKKKKACiiigAquP+Qg3/XMfzqxVcf8AIQb/AK5j+dAFivPdQ5+O+jj+7pE36t/9avQq88vfm+PGmj+7o0n6uaAPQ6KKKACiiigAooooAKKKKACiiigArhvi1ot9r/w/vLDTrZ7i6aSNkiTqcGu5pCM0AeZ+PNDl0fwDoc1lE0reHpbeTai5LIgCtj3xn9ar/DHU7bxJ448beIrR/NtriS1ihkKleFjYHggHsK9TKBgQeQRgjHasLw14R03wq2pHTlZRf3JuZFJyFJ7D2oA36YYwQQ2CCMEEU+igD5qk0PxZpXj/AFew0G4a3GiRTX1nGFJWWNyG8vHQ5BIHuK7bVNbs/HGpfDm5SON4bq5kmkhYbgrLGdykH0I/lXrfkx+YZAiiQjBfaMkema8V0TwzcaX8epLKNWXSrdJNStkHRDIMED2zu49hQB6ZY+CNC03xTceIrO08nULhCspU/K2TyceprJ+LFg138Pr64jH7+xKXkRHUNGwau4qtf2MOpafcWNyN0M8ZjceoIxQB84vftN4yv/H1vIS9nqdhCQDgNHJEVcf+g/5NR6rovi/xJrutxaG8rWF/rrRTbQcQvGoKux/hX5j/AN8j2rrtd+H0fgf4TeIoEvGuzJdQ3Su64KqkiYHvgA10HwSkN34Pvb7/AJ+tRmk479P8/jQByifBvWG8PeHJLCWPSNdhkdtQuVkJcBjkHKn5iMdM96u2Hwf1HX/EGq3/AI2vHnucIlneWsmzOAPmA7Yx0Ne07aXbQB4VrXgvX/hdaQax4MYXgitHi1BpQM7QxkDkcdMkfgKxfGvxg1DVbXVtI09YTbXUNstvhMuVkjDSD3OSBX0Re2cd/Y3FpL/q54mibjswIP6GvkDwXpM9r8VNIsrhCDHqZgIb1jPI59BigDb0vWvGHhaG+8BPbzvLqKC3sIp12BN7MC8ZPZsnnpke1fS/h3Sxo3hzTtNwB9nt0iIHYgc1Ff8AhTR9U12x1q8tFk1Cx/495skFep+h5ORnpWzj3oArPptlJFcRPaQMlznz1MYxLkY+b14qeOJIY1jjVURQFVVGAAOMD8qfRQA0qCMHkd/esTw/4P0Twu92+kWSW7XcnmSkckn0HoOelbtFAHmelfB7TLTVvEs97P8AarPWAVWHZtaEF9/3s8nIGDgdK4Px5Nf+C9P1jwfe313e6XqlvDJpk1wxdkYSrvj3fTJ/L1r6HxXnfxc8J3vivRtKXTbfzrm11FJGGQMRkENyf+A/lQBv+BvCVp4O8NxabaSSSKzGVmkxnLdvwqn8VrZrv4XeIIgu4i28zA9FYN+mP0rscYGB+FRXdrFe2c1rcKHhmRo5FPdSMEUAc1F4lt9K+GNr4ikBmii0yKfap++Si4GexyQKi+H3jhPG2jS3EtqLS9t5TFcW2c7e4IzyQQfzBryjXLjUtC0Wb4U3LGaS5uIY9OuFH37d5c4btlf6V7JpPgjRtF8RX2u2cLpeXiKj/N8qgADgds4GaAOjqrD/AMf9z9E/katVVh/4/wC5+ifyNAFqiiigDB8U6Vdavo/lWEscV7BPHc25lGUZ0bcFbHY9K525XxV4luLPTtV0qx0q0jnjuJ5FvBM8uxtwCKAMDIHJ9K3PGVrd3GjRSWVq11Jb3cFw1uhw0io4Yhc9+/viuNk8O3+qXMvii60q4h1ifVYBZKzZltbZCoOcHAyN5I9xQB6tRRRQAUUUUAFYV34N8PX9493daVBLcOctI2c5/Pit2igDPvNE03ULCOxurSOS3jxsQj7pAwCD2Ip+naTY6RbfZ7C3S3i3biqDqfUnuau0UAFRzf6l/wDdNSVHN/qX/wB00ANtv+PSH/cH8qyPGh/4obX/APsHz/8Aotq17b/j0h/3B/KsfxsceBdf/wCwfP8A+gGgCp8Nhj4a+HP+vCL/ANBFdTXL/DkY+G3hz/sHw/8AoIrqKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArzrxR+6+M3gaToJI7yMn/tkx/nXotec+Pj5XxD+H0/TF7PGT/vIBQB6NRRRQAVDd/wDHpL/umpqhu/8Aj0l/3TQBIn+rX6CnU1P9Wv0FOoAK4q/tvEeheIr/AFPRdOttTtdQCNNDJciGSJ0XGQxGCuO1drXmniLTNbudY1nTLfT7h7fWpLYG/jdQsMC4Eqk5yDjOOxzQB0PhOw1FbrVNb1VrZbzUWjHkWz744UjBCru/ibJJJ+ldVXH+E9DTQdc1+2sbJrPSC8H2aIAhC+w+YyZ7cqPqDXYUAFFFFABRRRQAUUUUAFFFFAFdv+QjH/1yb+YqxVdv+QjH/wBcm/mKsUAFFFFAHm/hH/SfjF49uhyIls4Af+2fP6rXpFeb/DMef4l8eX/USay0IPqIxx/6FXpFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAFYf8hM/9cR/M1Y/iH0quP8AkJn/AK4j+Zqx/EPpQAtFFFACN90/SlzSN90/SqepXF7b2ZksLNbufcAImlEfGcE5IPSgDzqX+x9a8S64niXxRPZz2d0Y7ezi1L7MkUO1Sr4BGWOSST9K1/AHiC0uLK6sX1+HUFivpItPlluFaeaEAYJGckg7xnHQCo9X0+8m1a5kT4daRqKM/wAt1NPCHl9yGQ/zrS8L6Xps0bXkvhfSdM1C3mMeLYRSsmAP41UYPPSgDraKY8qxoXchVHVmOAKbFcQzrvhlSRfVGBFAEtFJmjNAC0Um72pkc0cufLdG29QGBx+VAElFQyXUMRCyyxozHChmAz9P0qlqusppr2sK28tzc3TlIYYsZbC7icngAAUAadVx/wAhBv8ArmP51FpmpxarZLcwo6gkqyOMMjA4IP4ipR/yEG/65j+dAFivPLnn49WftojfrIf8K9Drz2b/AJL3b+2if+1GoA9CooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACk2ilooAKKKKAKmo6dbarp1xYXkYltriNo5EPcEYNVtA8P6f4Z0eHStLh8q1hztBOSSepJ7mtSigAooooAK4vxF4Qm1Xxv4Z1u2MMcOmyzPcL0Z9wXBGOv3a7Skx70ALRRRQAUUUUAFFFFABSYpaKACiiigDntS8HabqnivS/ENxv+16crrEB907hgE/TmuhoooAKqw/8AH/c/RP5GrVVYf+P+5+ifyNAFqiiigDlvHt/LYeFnlivjYRtPFHPdIRuhiLAOy++K428fw94em0y90HxhPLeS3MSvHPq3npPGT85cM3HGTnjkV3OtnULmC8tv+EcttTt1CeVHcTptmP8AFkMDjH45rlo9OaO4hF98NNAtbaSRY3mNxASMnHA2c9elAHodpf2moQefZXUFzDnHmQyB1z6ZHerFVbHTrLTLf7PYWkFrBkt5cEYRcnvgVaoAKKKKACiiigAooooAKjm/1L/7pqSo5v8AUv8A7poAbbf8ekP+4P5VjeOD/wAUHr5/6h83/oBrZtv+PSH/AHB/KsTx1/yIOv8A/XhN/wCgGgCH4dDHw38N/wDYOh/9AFdPXM/Dwf8AFuPDf/YOg/8AQBXTUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXnHxO/d+IPAlx6a5FHn/AHiBXo9ecfF4+XYeF7rHMPiC0bPp94/0oA9HooooAKhu/wDj0l/3TU1Q3f8Ax6S/7poAkT/Vr9BTqan+rX6CnUAGa801ltM1XxrqNj4j8RzWEFukRtLSO/NsjqRy5IIJbdx1/CvQb+a6gsJpbK2FzcKuY4TIE3n0yelcdrdnfXWomf8A4V9pWps0abrie4hD7scr8yk4B460AQeCNc0+HU9W0tPEkeoWEU8aWElzdrJKxKnegYnLgHGOvWvQq43w3pNhcTTPfeD9G0u8tXUosHkysuRnOVX5T/OuyoAKKKKACiiigAooooAKKKKAK7f8hGP/AK5N/MVYqu3/ACEY/wDrk38xVigAooqteymCwuZf7kTN+hNAHBfBweb4b1e9/wCfzWbmbPr90f0r0auD+DkXl/DLTWx80sk8hPrmZ/6YrvKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAKw/5CZ/64j+Zqx/EPpVcf8hM/wDXEfzNWP4h9KAFooooARvun6VS1PUE0zTpr2SCaVYhkxwJvcjOOB3q633T9KTt1oA4v/hZNl/0AfEP/guenfDp5Z7TXr2S1uLZLvWZ54kuIyj7GVMEg/Q11kl3bRNtkuIkYnGGcA1KpUgEHII4IOaAKmo6ZaatafZb6ISwFgxTcQCRyK47R7K0j+I8r+H4Ft9OtrVob8wjbC8xYbFA6bgOuK6bxLZ6rf6HPa6PdRWt3L8omlzgL3xjvis7w1pev6OkNlcroyafGpytosokJPfLHk5oA1td/tT+zXfRmh+2RsHCTD5ZADkpnsSO9c/pXi658U6jBDpEBt7e1bOpPcLkow/5ZKB1PHXpiui1nTZtV05rOK+ltBIQJJIlBYpn5lB7ZHGaz4vCFlY31ld6S32BrdRHIsSgieP+63vnnd1oA2bu1hvrSW1uELQyqVdQxBI+o5rkPBdnb6b4r8XWVpH5dtDPbBE3E7cwhj1Pqc12FyJxay/ZTH5+07PNHy7vfFcloWh+KdO8SX+o3c+kPDqMkb3KQrIGUIgUbM+w75oAo6RpNn4rv/FN7q0C3EkeoSWFvv58lI1ABX0JLZNGhNqN7ovhzWkhkvprE3FvNEHUPIhym4biBnKLx7nmtGfw5rthqWqy6DeWCW2qP5sqXSPmGUrtZ029c4HBroND0iPRNGtdOiYssC4LEYLnufx5oAg8N2NxYaWVulCTzTSTvGDnYXYnGa0R/wAhBv8ArmP51Pt4x29qgH/IQb/rmP50AWK89k/5L5F7aED/AORWr0KvPW/5L6v/AGAV/wDRrf4UAehUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFVYf+P8Aufon8jVqqsP/AB/3P0T+RoAtUUUUAc3rfi+20O+FpLpmq3Tsm/faWhlT6ZHeuW8QeKl8SR6dYWei60kp1G3kLT2TIqqr5JJ7V6S7oqlnZQo67jj+dMiu7e44inikPcK4b+VAE9FJmloAKKKKACiiigAooooAKjm/1L/7pqSo5v8AUv8A7poAbbf8ekP+4P5Vh+PD/wAUDr//AF4Tf+gGty2/49If9wfyrC8fH/igNfP/AE4y/wDoJoAb8Phj4c+Gx/1Dbc/+QxXS1zngDj4deGv+wZbf+i1ro6ACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArzf41/J4Ht7j/nhqVvJn0+Yj+tekV558bY9/wAK9Tb/AJ5yQP8A+RVH9aAPQ6KhtpPOtYpP76BvzGamoAKhu/8Aj0l/3TU1Q3f/AB6S/wC6aAJE/wBWv0FOpqf6tfoKdQBXvLlbSynunjd1hjaQrGuWIAzgD19q5H/hZFl/0AfEP/gveu0qKa7t7f8A108cZ7B3Az+dAHHeCrmTU/EnijVTZXlpbXUlt5Iu4TEzbItp4PvXcVGjo6hlYEHupBqSgAooooAKKKKACiiigAooooArt/yEY/8Ark38xViq7f8AIRj/AOuTfzFWKACsrxJL5HhfVpDxts5j/wCOGtWua8fTGDwDrsg4xaOPzH/16AKvwui8n4ZeHwRjdaK//fWT/Wuvrn/A8P2fwJoMWMbLCEf+OCugoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooArD/kJn/riP5mrH8Q+lVx/yEz/1xH8zVj+IfSgBaKKKAEb7p+lYnir7R/YEwt4b+ZiV3Jp7KJivfaSQK22+6fpRtzwelAHjkV98Ko7h7XWdJuLC+UAyf2xbSiTnoxfkfjmu58LaboURa+8N6o8+nuNnkw3PnQq3+zydp5HAxWczan4V1PVi2h3GradqNybkS2u1pFLKAUdCRuAxweeDirngCyuLLSr8z6X/AGZBcahLPa2ZA3RRkLjdjodwbjsCB2oA6/HvRj3paKACiiigBMUFc0tFACYHToPaloooAKrj/kIN/wBcx/OrFVx/yEG/65j+dAFivPP+a/H20FP/AEa9eh154vPx+l9tBj/9GvQB6HRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVVh/4/wC5+ifyNWqqw/8AH/c/RP5GgC1RRRQB5v4yn0m21eafxLpOvXelRouJIlL2kY7llQgj6kGqemf8Ko1eeOLSbi1sb58CMxSPazZPYbsE/hmu18VaXc6rowisvKN1DPHcRRzfckKMG2n2OMZrldXvNU8RXel27eDr221C2u45ftVwEaKFFb5isgPOR2HWgD0GztmtbKG3eeSdo0CmWU5Z8dz71YoooAKKKKACiiigAooooAKjm/1L/wC6akqOb/Uv/umgBtt/x6Q/7g/lWD4/P/Fvtf8A+vGX/wBBNb1t/wAekP8AuD+VYHxA4+Huv/8AXjIP/HaAH+Ah/wAW78Nf9gu2/wDRS10Vc94DH/FvPDY/6hdt/wCilroaACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAriPi7GZfhXry+kSN+Uin+ldvXK/EiIzfDbxGgGcWEr/kpP9KANjQJfO8O6ZL/AH7SJvzQGtKsHwXL53gbQX9dPgGfpGtb1ABUN3/x6S/7pqaobv8A49Jf900ASJ/q1+gp1NT/AFa/QU6gDM137R/Yd59mW6aby/lW1KiX/gJbjNeXLf8Aw1guxb+ItG1C0viu5n1q3ld2HrvG4Y9+lex7c9ea4y8GqeHPEuoanBpM2q2OoKhf7MQZoSi4xtP3lPXjvQA7wrp/hdpTqHhXVA9ouUltrW68yHJ6blJO0/lXZVxXge1uF1DXL7+xm0ixvJo3gtpFVZCyqQ7FV+6CccfX1rtaACiiigAooooAKKKKACiiigCu3/IRj/65N/MVYqu3/IRj/wCuTfzFWKACuN+KsnlfDDX26Ztio/Eiuyrg/jC4/wCFaammf9YY4+PdxQB1Xh+LyPDumxdNltGP/HRWlVexXZYWy4+7Eo/SrFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAFYf8AITP/AFxH8zVj+IfSq4/5CZ/64j+Zqx/EPpQAtFFFACN90/SlpG+6fpS0AeTx/wBk6j4v1+31TxlqOn6pFdkQJBqJt0WEhdqhT8rc59a9C0KzmstO8qfV5dVJcslzKE3bcDC5UAHHPPXmud8QQa3Ndz/afCGja5p+7MWZVEoX3V1IJ+hq54Dl02fQpm0vRZdGjF1IstpIACsgwGPBOOg446UAdZRRRQAUUUUAFFFFABRRRQAVXH/IQb/rmP51YquP+Qg3/XMfzoAsV55Fz8fbn20KP/0a1eh155b/APJfL0+mhR/+jWoA9DooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKqw/8f9z9E/katVVh/wCP+5+ifyNAFqiiigDkPiNci08JPK9/PY24uIRcTQSFJBEXAcKw6HB/Ss/QtK0yW4gufD3jnUZogysbaS9W5Vl9MOCwz65zXXawNROmyDSorSW6yMJdlhGwzzkgE5rgJVhtNZ09te+HlrbzyXKrFf2MiOqyHoS2FI+hzQB6hRRRQAUUUUAFFFFABRRRQAVHN/qX/wB01JUc3+pf/dNADbb/AI9If9wfyrn/AIhn/i3mv/8AXlJ/Kugtv+PSH/cH8q574in/AIt3r/8A15SfyoAn8DDHw/8ADg/6hdt/6KWugrA8ED/igfDg/wCoXbf+ilrfoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACsPxjF5/gjX4sZ36dcL+cbVuVR1iI3GjX0OM+ZbyLj6qRQBh/DWbzvhvoD/8ATog/Liuqrh/hBL5vwr0I5+7Ey/k7V3FABUN3/wAekv8Aumpqhu/+PSX/AHTQBIn+rX6CnU1P9Wv0FOoAK8u1V9Ku/iBqdlrXizUNNmWOJrKO31BrdFXHzcfdLbvWvUa4/wARwa5LduE8NaPrWmlRhJpQkvTnO5Sp56cj8KANTw7YSWNtKH1+41eFyphknMbGMem5AN2fetyuM8ByaYyanFY+G30GeGdUubdtoBbGcqFOAMfSuzoAKKKKACiiigAooooAKKKKAK7f8hGP/rk38xViq7f8hGP/AK5N/MVYoAK8V+Nt1dy+JfB+jQ3Eq213cEzwhztkw8eMj25r2qvONegiu/jf4ZiljWQQ6fcSgOM4bPB+vFAHoqrtQKOgGKdRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBWH/ITP/XEfzNWP4h9Krj/AJCZ/wCuI/masfxD6UALRRRQAjfdP0paRvun6UtAHmfiPVo9L8Q3KWvj+5sbgtuktZ7JbuGE4yBkKCgxz96ul8F6S2naRPcSasmqz6lcm8lu4oxGjlgo+VQSAMKPfqe+KxzLf+D9V1cP4fvdW0zUrlrpZdPjWWVGYDcjoSCV44PocYrR8BWV1a6ZqEtxYSadBd6hLc2ljLjfBEwXAYDIUlgzYB43YoA66iiigAooooAKKKKACiiigAquP+Qg3/XMfzqxVcf8hBv+uY/nQBYrzy15+Peo+2hxf+jDXodeeWXPx71T/sCQ/wDow0Aeh0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFVYf8Aj/ufon8jVqqsP/H/AHP0T+RoAtUUUUAYvigwJoM8l1q0+lRJhjdwFQy88Abgc/lXA6cZPFF/bae3j/7ZbRTpcfZpdLEFxJsO5cNxkZH93Ndx4t0q61jQ/LshC91BPHcxRzf6uRkYMFb2OMVyupanqPie402zTwhq1jqUF3FK93cIghgVTl9soY7gRkADrmgD0qkzS1yupeItSm12bRdAtLSae1RXuri8lKxRlvuoAoJLEZPbFAHVZoqpYPePZRG/jhS6x86wuWTPsSAT+VFvqdnd3Vxawzo9xbMFmjHVM9M0AW6TNVbPUrPUDMLS4SbyJDHJtP3WHasDxf4h1jw9bm6s9KguLKMKZp5bjaRlsYCgZNAHVUmfasrXtbTQtEl1B4mlZdoSJTgu7HCr+JNZNj4g1e21uy03XrKzh/tBXNvLaSMwDqATG+4DnBPPtQB1W6mzf6l/901wFsWSDSNYEkn9o3Or+TN85+dCzqykdMBRnpxtrv5v9S/+6aAG23/HpD/uD+Vc78Rj/wAW51//AK83rorb/j0h/wBwfyrm/iScfDjX/wDrzegC74K/5ETw8PTTbb/0UtbtYfg3/kR/D/8A2Drf/wBFrW5QAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFMkGYnB5yMU+kIyCKAPPvgvlfhzbQnrBcTREemHr0KvPfhEfL0DV7X/nhrN0mPT569CoAKhu/+PSX/AHTU1Q3f/HpL/umgCRP9Wv0FOpqf6tfoKdQAV534u1KLTNdJi8cXel3UiAm0+yC7iUY67QuVzjua9Erh759Q8L+J9R1SPRbnVdP1NU8z7GqtPC6rjGxiNykehoAs+CLBwl9rc2vQ6zNqboTPbwiKJVRSoUKCeeuc+1dfXGeCYLltQ1vVDpU+lWF9LG1vaXKhJNyqQ7lASE3cce2eK7OgAooooAKKKKACiiigAooooArt/wAhGP8A65N/MVYqu3/IRj/65N/MVYoAK87u/wB58fLBf+eWiO/5yMP6V6JXmvhWP+0vjH4z1C4d3ksEtrO3yeEjZCxH/fQz+JoA9KooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooArD/kJn/riP5mrH8Q+lVx/yEz/ANcR/M1Y/iH0oAWiiigBG+6fpVHVtYtNE0976+Mot0I3tFE0hGe+FBOPwq833T9KTb70AeQ6VfeE/Fmu6zLf+KJRdfad1mUvmt1WDauzYpIGQc5yM123gbULq/0u9Se7N/Ha3slvb33/AD8xKFIbI4JBLKSOpQ1oXWk+HvEAlFzY6fflGaKQvGkhRh1UnqCO9WNG0HTfD9m1npVqtrbtIZPKQkqGOASMnjoOBxQBpUUUUAFFFFABRRRQAUUUUAFVx/yEG/65j+dWKrj/AJCDf9cx/OgCxXnmn8/HrWPbRYf/AEZXodeeaZ/yXnXPbRoB/wCP0Aeh0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFVYf+P+5+ifyNWqqw/8f9z9E/kaALVFFFAHnfxG8X6VZWL6RcancWMzTQmYxxyKWhLDfscDBbGeAc8dKpB9F0Z9N1Dwl4ha6a5uYomsvtv2hbhGIDHaSWUqDnjGO+K9HvJLSOELePCsch2ATEAMTxt56k+lZEngjwxJfQ3w0OyiuoXDpLDEI2DA5ySuM/jQB0NeW/2bokPj7xIviW4SOKdYri0Wecxxuu0hmXnBZSAPXHavUqrXOn2d6qLd2sNwE5XzYw+0+ozQBzvw+muJvCsTTvK8fnSC3klJLPFuOw5PPSsbxVpt/wCJNdkh0APY3VnE0dzqDAoJdy8Qj+8D3bt2r0JUCKFUBVAwABjFLt96AOf8IT250NLWHTn057VjDNbMvCOOpDfxA9jWT8TdSsIPCN3aXF7bxXUuwxwySKHcB1yVXqa7baKrXGnWd2ytc2sExUYBljD4/OgDkfGF3bav4MW9024hvYLO6gnka3kEgxG4ZhleMgc1HqmqWPiPxZ4Xt9Ju4bswTPeTPA4cRRhCAGI6Ekjg120Npb20XlQQRRR9dqIAPyFR22nWlnv+zW0EG85byowu4++OtAFePQNNi1H7elqgudxYMCcAnqQM4B56/Wr03+pf/dNSVHN/qX/3TQA22/49If8AcH8q5r4lH/i2+v8A/Xo39K6W2/49If8AcH8q5n4lnHw21/8A69G/mKANHwdx4J0Af9Q63/8ARa1t1i+EePBehD00+3/9FrW1QAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHnnww/d3/jC26eXrUrY/3ua9DrzvwGfI8d+O7X/p/ikH/Ao816JQAVDd/wDHpL/umpqhu/8Aj0l/3TQBIn+rX6CnU1P9Wv0FOoAq6hqEGmafPe3Ik8mBdz7ELtj2A5P4V5XHrXhHxZ4x1Fb7xLKse2IWUa3b2qoMYYEHb8+7sRXruBWVeafomtvNb3tpYXzw/LIkqJI0eRkAg528c0AYvgu8ma61jTRqLalY2Msa216zbydyktGXH3ihxk/7QzXYVk6L4b0jw7HMmkWMVokzB3SLO0ke2cD8K1qACiiigAooooAKKKKACiiigCu3/IRj/wCuTfzFWKrt/wAhGP8A65N/MVYoAK85+Hvz+PfiFL66jCn/AHyjV6NXm/wy+bxJ49l9dckT/vnNAHpFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAFYf8hM/9cR/M1Y/iH0quP+Qmf+uI/masfxD6UALRRRQAjfdP0paRvun6UtAHn7R+JvDniPUm0620u9sdQuPtKWpufJmVyoDEZHzZ2j8ea67RdQvNQsmlvtMm06ZXKeTK6sSOOQV7c/pXl2pvpmjeOPtGthYdUfWlmgvLgfKbQpgBXPAAPBHrXa+BdSbVYdbuUuJLmxbVZfsUsjbg0W1M7f8AZ378UAddRRRQAUUUUAFFFFABRRRQAVXH/IQb/rmP51YquP8AkIN/1zH86ALFeeaV/wAl31//ALBEA/8AHq9DrzzSP+S6+IfbSrf/ANCFAHodFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABVWH/j/ufon8jVqqsP8Ax/3P0T+RoAtUUUUAc/4w0u81Xw+8GnzQQ3kUsdxBLPkKjIwYHj6Vm6V4i8TPJCl9oFvcRu4je6068SRBzgkqeRjrirHj21kuvC7qIpp7dJ4pLuGEkNJArgyAYOcY7e1cHpWu6RBZ31j4eukhll8RRm0tLbKExlIgw2cYXG/Oe9AHs1FFFABRRRQAUUUUAFFFFABUc3+pf/dNSVHN/qX/AN00ANtv+PSH/cH8q5j4m8fDTXz/ANOp/mK6e2/49If9wfyrl/igf+LZ6/8A9ep/mKANXwmP+KO0Qf8AUPg/9FrWzWR4V/5FHRf+vCD/ANFrWvQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHkni7QU8O/FTw54ptbqZBqV+trdQ7vkLMmwH9entXrWa87+MwaDwVBqag7tN1G3usjsFbH8yKi+IvjzUPCviHwxZWJg+zajcBbh3XJKblHH4NmgD0qobv/AI9Jf901MDkVDd/8ekv+6aAJE/1a/QU6mp/q1+gp1ABXCanB4i0XxZd6lpUWmXFnfrGHtprgxSvIowCCRjPbHfiu7rx7xqmn6f4smvNZj/0iS8sG026lBKpEsq+Yqn+Eg5J6cGgD0rRNT1DUIpv7R0eXTpY2CgPIrh/Ugj098Vr1yHhXVl1jxJ4jls7trrTFkgEMgctGJNhDhD2HC5x3rr6ACiiigAooooAKKKKACiiigCu3/IRj/wCuTfzFWKrt/wAhGP8A65N/MVYoAK84+FHz3PjSftJ4iucH6Ef413uo6jbaVp09/eSCK2gQvI57AVwfwahlfwbdatKhU6vqVxfAEY+VmCj/ANBJoA9GooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooArD/AJCZ/wCuI/masfxD6VXH/ITP/XEfzNWP4h9KAFooooARvun6UtI33T9KWgDzqW38T+KdW1jyNW06ytrC7NvDYz6etxvAUHe7FgRuz27V0fhLV5dT024gurSG1vdPuGtLmGA5j3qAwZPRSrA4PTOO2a4bXm8Nt4n1Kazs/Ff22OTZfaho6u0UbgdGweSB2CnFbngvQprSCG80TxVJqGizyvNLHc2ytJKx+8S/DBuxyM8YxQB39FFFABRRRQAUUUUAFFFFABVcf8hBv+uY/nViq4/5CDf9cx/OgCxXnmjf8lz8SH/qGW/9K9DrzzRP+S5eJ/8AsHW39KAPQ6KKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACqsP8Ax/3P0T+Rq1VWH/j/ALn6J/I0AWqKKKAOd8YahqGmaEJNMeGK6mnigE867khDMAXI74rnhBrnhG/tb/U7vT9Ysp5kgmnSxS2nhLnCsNpIYE4z359q3/G7aMvhe4GvwzTWLlVMMAJkkYsAqqByST7157aaXpWp39lYRal4s0G5VxJZWurx7o5GGcbd24Mfbdn2oA9lDZGRS1m6Pa6laWPlarqMd/cBj+/SARZXsCoJGR61pUAFFFFABRRRQAUUUUAFRzf6l/8AdNSVHN/qX/3TQA22/wCPSH/cH8q5b4pH/i2Wv/8AXsf5iuptv+PSH/cH8q5X4p/8kx1//r2/9mFAGz4XH/FJ6P8A9eMH/oArXrK8MjHhXSB/05Q/+gCtWgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOO+KVp9t+GPiCLGStqZf++CH/8AZa8Z+LF1JqGl+CL+NiXj0dbsnPf91/U19CeIbM6l4b1WxAybi0lhA9SyEf1r5unkGt+FtCTOTB4au1I9PLlGP0UUAfT1tKJrWKVeVdAw/EUXf/HpL/umszwnc/a/B+iXGcmWwgc/jGK07v8A49Jf900ASJ/q1+gp1NT/AFa/QU6gArgdTi8Q+I/Eup6dZatZabZWKx7YZrBbkzllyWbcRgA8cV31eX+LT4el8WzNHp3iW51eGJBPc6KHYQKeVDfMB05wAeKAOo8JahdSG+0XUrS1gv8ATWVZDaDEMqOMq6qeVzg5BzyOtdRXm/g/RnJfU/D3i66ubWecNeR39srykqACrMdrKwHHIPavSKACiiigAooooAKKKKACiiigCu3/ACEY/wDrk38xViq7f8hGP/rk38xVigDkfie2z4a6+f8Ap1P6kCtXwnEIvB+jRqAALGEYx/sCsT4sts+Fmvn/AKdwPzda6PQU2eHtNX+7axD8lFAGjRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRTWcIpZjhR3oAdRUH2yD++f++TR9sh/vH/AL5NAE9FQfbIf7x/75NH2yH+8f8Avk0AT0VB9sh/vH/vk0fbIf7x/wC+TQBPRUH2yH+8f++TR9sh/vH/AL5NAE9FQfbIf7x/75NH2yH+8f8Avk0AT0VB9sh/vH/vk0fbIf7x/wC+TQBPRUH2yH+8f++TR9sh/vH/AL5NAE9FQfbIf7x/75NH2yH+8f8Avk0AT0VB9sh/vH/vk0fbIf7x/wC+TQBPRUH2yH+8f++TR9sh/vH/AL5NAE9FQfbIf7x/75NH2yH+8f8Avk0AT0VB9sh/vH/vk0+OeOXIQ5I7d6AIh/yEz/1xH8zVj+IfSq4/5CZ/64j+Zqx/EPpQAtFFFACN90/SlpG+6fpRmgDhDZ+L9A1nVbjSbHS9Q028uDcJA07RSo5UBjkgjnFV/BV/qemXt5Z6x4Z1KyudTv5LovDGsltEWCgDerHHC5JPcmodYj+Hk3iG9SfWjpGtF/38sV69q5bHckhT+ta3w6vHudH1GI6rLqq2eoy26XruH81QFIIYdeGx35BoA7SiiigAooooAKKKKACiijNABVcf8hBv+uY/nViq4/5CDf8AXMfzoAsV55oQ/wCL3+Kf+vC2/kK9DrzzQP8Aktviz2srX/0EUAeh0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFVYf+P+5+ifyNWqqw/8f9z9E/kaALVFFFAGB4t0y+1fQzBpksEV8k0c0Dz52K6sGGcVx/iCXxve2tnb6h4WguIormO4luNOuVc/Ic/Kj7Tk/jwa7LxX/YR0Gb/hIlB03I8zIc4Pb7nI/CvPZp/DWmzaZf8AhfxhcOjXsUTafHqPmrIrNtwEckigD0zRtY/tqw+1CwvrIhyhhvYfKkBHtzx7itOm7f06U6gAooooAKKKKACiiigAqOb/AFL/AO6akqOb/Uv/ALpoAbbf8ekP+4P5VynxVP8Axa/Xz/07/wDswrq7b/j0h/3B/KuS+K5x8Ltf/wCvcf8Aoa0Ab/hsY8MaSP8Apyh/9AFalZnh4Y8NaUP+nOIf+OCtOgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAExXzn4Q8PSf8LR1zwjcnZDb2t3bxuOoilwynH0INfRteU6fZLB+0XqkuOZ9MSb69F/9loA9D8P6QNB8PafpKymZbO3SASMMFgowDirt3/x6S/7pqaobv/j0l/3TQBIn+rX6CnU1P9Wv0FOoAK4m/sfFOmeKb7UtDttMvLO7VPOtppmjkLqMBgcEdOOcdq7avPfEqeApPEciatqf9l6yyrumW6ktmI7fNkIaAIfDmpazp/iXU7jWvCuo2cmqzRBWtFSeBAgK7mZGzk5ySR2FekVwvgC7Bv8AXtOh12fWrazmiMN1LMJQFdSdoYdSMc/hXdUAFFFFABRRSZoAWikzzSBs9qAHUVnX2uWOnXdva3UjpJcMFTETFck4GWAwvPqRWjQBXb/kIx/9cm/mKsVXb/kIx/8AXJv5irFAHC/GJ9vwr1z3jQf+PrXYaamzS7RP7sKD8lFcV8Zz/wAWt1Ve7bF/8fFd1ANsEQHTaB+lAEtFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFV7nlIwef3i/zqxUFz92P/AK6L/OgCbdS1y/iXQvEWqXkMujeJ5NKhVNrxLbpIGOevzCsT/hD/AB1/0UKf/wAAov8ACgD0OivPP+EP8df9FCn/APAKL/Cj/hD/AB1/0UKf/wAAov8ACgD0LNQXt5HYWU91KGMcKGRgoycD0rP8O6fqmm6Z5Grau2qXO8t57RLGccYGF445/OneJP8AkWNT4J/0aT/0E0AXIb5bnTUvYI3ZJIvMRDgMcjIH1p0Nz5lrHNLG0DMgZo5CMqT1B+lchpnifQ7vwpDp9tqtrLefYNohSQF/ljORj2xWCp0T+1/Dg17yRZ/2GuPtGPJ37gRuzxxzjP60AepGRQu4sAvXOeKBKhXduG3rnNeW/Z7CfQYWlv7K301NRmewh1EZt548Yx83VRk7akF3p95pmkPeWFtaaAl5NHcLB/x6yMBhH9PLJoA9AvtYhsZrGNlZ/tk3koyngHBOT+VWLm6aG0aeGB7kgAhIyMt9CeK4nVbPQL6LQYNKW1fTX1PDrakeWx2HIyvHYdDVG/tYNOj8aWFnCsFnGLSWOCNcIjMPmIA6Z2igDv4NWtrnUbqwQt59qqNKCOBvBI/kat+YMcflmuAgg0Gz8f6pJqlvZxzzxWslo80Y3O21wxQnnOcZqp4X/wCEU1XxTFq1mdJtpELJYWluY0lbIw0jqOclQcDoBz1oA7zWNatNDsxc3Zch3EcUca7nlc9FUDqTUGm669/dfZ5tI1KxcrvU3Ma7WHsyswB9iQayvFubPVNA1iaN3sbG4k+0lFLGMOhUPgdgevpmjVvElpquganb+HdQju9Q+xu8QtzvIO3jkZGfbrQB1SyKxIUhiOuD0oEi7tu4bvTNedaV/YH9uaB/wi3k+dsJvfJ+95W3/lr33Z/vc1RsNJsrXwHpWrxW6rqK3qFbnneAZyCA3ULg9OntQB6mZUDbSw3dQueaq2WqW1/JcpAxLW0xhkz/AHhjp69a88v20rTfE1zc7tI1e4mvlzBKM3kJ3BcJ3IXtVvTE0TStW8Sn7DaNq6TTTQQbAJpU8sHC9yDz0oA9CEqsSAynBwcHv6UCVdwXcNxGQM84ryvw/c2a+L9CfTZdNAuoJhcxadGVVRsyFkIOC2cdfmp0GlWdv4Sg1iOBRqK6yCtzg71ButpUHrtx26UAekW2qW13e3lpExMtm6pKCOMsoYY/A/nUrf8AH7H/ALjfzFcbpNroln8Q9bE9vaRahNNHJamRAHYGMbyh9znOOtdm/wDx+xf7jfzFADR/yEz/ANcR/M1Y/iH0quP+Qmf+uI/masfxD6UALRRRQAjfdP0qnqcl/DYvJp0EVxcrysUj7A3tuwcflVxvun6UtAHC3vi2GaP7LrHg3VXc8eUbVZ0J9iCRWr4bub2Y+VH4dTRtLRT5cbbUcn2RRhR9TXSY56mjb+tAC0UUUAFFFFAEU1zDbxmSeVIkHVpGCgfiaIbiK4iWWGRJI2+66MCp+hFUNa0mw1WxdL+0hukjVmVZl3ANj0Ncz4RuZLL4P2t1F/rIbGV147jcR+ooA65dUsHuzaJeW7XIJBhEq7wR/s5zWVqPiZrS8vI7exe5isER7t1cLs3DIAB6nbz2rj7jR7Ox+FlhqtvDGupRRwXgugB5jSkhmO7qc5I698V0F7peqySaq1nDDLFrMMYaR5MeQ+zaeMHK4weO9AHXRyrLEkiHKOoZT6g9KiH/ACEG/wCuY/nTra3W1tIbdTlYkVB74GKiaTZfMdjN+7/hGe9AFuvO/D3/ACWzxf8A9elr/wCgCu++0H/nlJ+Vee+HZMfGjxg2x+ba14xz/q1oA9IoqH7R/wBMZf8Avmj7Qf8AnjJ0z0oAmoqD7Qf+eMn5e+KU3BH/ACyk/KgCaioftH/TKT8qPtB/54yfl7ZoAmoqH7R/0xl/75o+0H/njL/3zQBNRUH2g/8APGTpnp74pftH/TGX/vmgCaioftB/54y/980faD/zxk/L2zQBNRUP2j/pjL/3zR9oP/PGX/vmgCaioPtB/wCeMnTPT3xS/aD/AM8Zf++aAJqKh+0f9MpPypPtH/TKT8vbNAE9FQ/aP+mUn5UfaD/zxl/75oAmoqD7Qf8AnjJ0z098Uv2g/wDPGX/vmgCaioftH/TGX/vmj7Qf+eMn5e2aAJqKh+0f9MpPyo+0f9MZf++aAJqKg+0H/njJ0z098Uv2g/8APGX/AL5oAmoqH7R/0xl/75o+0H/nlJ+VAE1FQ/aD/wA8pPyo+0f9MZf++aAJqKh+0f8ATKT/AL5pPtB/54ydM9PfFAE9FQ/aP+mMv/fNH2j/AKZSflQBNRUP2g/88ZPy9s0faP8ApjL/AN80ATVVh/4/7n6J/I1J9oP/ADxl/wC+ahtn33ly20jheD+NAFyiiigDnNY1vVdKu2P9hy3+nleJbRw0inuGQ4/MVhJrtrfagLnTfAt3NqIPy3FxZpAVPvI3IFd/toxQBXsmuWs4mvI40uSoMixtlQ3cA96s0mKzdc12y8PaY9/fsyxKcAKMsx9APXr+VAGnRVeyvIr+xhu4cmKZA6564NSiRWzggkHB56UAPopokViQpBI4IB6Vha/4u0/w46reRXTZAYmKEsqgnHJ6daAN+iqeoalbaXp0t/eSeXbxLuZv896zNM8WWeo6jHYPbXdncyxmWBLqPZ5qDqV+meRQBu7/AGps3+pf/dNcZb6zqmNP1eS8LWl5f/ZWs/KXbGjEqhDY3bshScnHXpXZzf6l/wDdNADbb/j0h/3B/KuR+LP/ACS3X/8Ar3H/AKGtddbf8ekP+4P5VyHxbP8AxazX/wDrgv8A6GtAHSaB/wAi5pf/AF6Rf+gCtGs/Qv8AkX9NHpaxf+gCtCgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACoPsdsLs3fkR/aSmzzto37c5xnrjmp6KACobv/AI9Jf901NUN3/wAekv8AumgCRP8AVr9BTqan+rX6CnUAVr5rpbKVrKOOS6CkxpI21WPoT2rkLvxayxm21vwfqZY8MiQLcxMfYg8j6iu3xxijFAHK+G7q5kmEVl4XGi6XyzGQJEzsemEX+Zrq6Tb19/aloAKKKKACqGq6fLqVkbeK/ubElgTLbEB8egJBxV+qeo3c9pZtNa2Ul5KCAIY3Ck/ieKAOb8AXlxJ4SllvLu4umhurhPNnkLuVRyBkn2Fc/CNWvfBcvjL+2L+O/O+5itkmIt0jViBH5fQ/KOp5rZ8C2+q2Gnz6Zqmjy28ck003mmVGUh2J24Bznms2PS/ENp4Vm8HR6aXBZ4otR81fLEDOTlhnduAOMY96ANjxDqS3OlaLMlvcyNJc210fKhLbUV1ZunSuuilE0SSKCAwyA3BqO0tUs7OG2j+5EiovGOAMCpsUAVpHVNQjLMB+7bqfcVL9pgP/AC2j5/2hUbgHUIwQD+7b+YqfYo7Dj2oA89+Mc8Unw7uESRG33EI4Yf3xXeRXEHkx/vo+FH8Qrhfi/g+E7KAABptUtYwPUb679EUIowOBjpQA37TB/wA9o/8AvoUfaYP+e0f/AH0KfsX0H5UbF9BQAz7TB/z2j/76FH2mA/8ALaPn/aFSbV9B+VJtA7D8qAGC5gPSaPn/AGhR9pgIz50fP+0KfsUdh+VARQMYHHtQAz7TB/z2j/76FH2mD/ntH/30Kk2r6D8qTYvoKAGfaYP+e0f/AH0KPtMH/PaP/voVJtHoPyo2j0H5UARi5gPSaPn/AGhR9pgP/LaP/voU/Yo7D8qNijsPyoAZ9pg/57R/99CobieJvLAkjJ8wcBh2PNWtq+g/KmtEjLgqCPTFAD6Kg+xwf3D/AN9Gj7HB/cP/AH0aAJ6Kg+xwf3D/AN9Gj7HB/cP/AH0aAJsUFcjB6VD9jg/uH/vo0fY4P7h/76NAEuwYwAB+FUH0eF9bGpszFxbG28s/dKlg2T78Va+xwf3D/wB9Gj7HB/cP/fRoAkMSMm1lVl7gjilKAjBAxUX2OD+4f++jR9jg/uH/AL6NAEixqn3QB9Bil257/pUX2OD+4f8Avo0fY4P7h/76NAEu3PXFG2ovscH9w/8AfRo+xwf3D/30aAJSue9IsSJnYoUHqAOtR/Y4P7h/76NH2OD+4f8Avo0ASLGqklQFJ5OB1NLtFRfY4P7h/wC+jR9jg/uH/vo0AP8AJTdu2ruxjdjn86cUBqL7HB/cP/fRo+xwf3D/AN9GgCQRIv3VUfQUu3jGai+xwf3D/wB9Gj7HB/cP/fRoAl28571BLIiXkZZ1HyN1OPSnfY4P7h/76NKltFHnaoGfXn+dAESOsmpMVYECIA4PuatfxD6VWUAakwAAzEDx9TVn+IfSgBaKKKAEb7p+lLSN90/SloAKKKKACiiigAooooAzNattSurExaZd29tM3DPPD5gI9MZFY/hXw1qeiaMNH1C/tbywSIxRrHblGwSScncc9T6V1eKMUAcNH4M1X7Hb6LcaxDJoNvIrKgtyJ3jU5WNm3YwOOQO1duEAAA6DoKXFLQAVWH/IQb/rn/WrNVx/yEG/65j+dAE+K888Nn/i8/jQ9/ItB/5DWvRKyrTw/YWWvahrUCML2/EYnYtkHYMDA7cYoA1MUY96WigApMUtFABRRRQAmKWiigApMUtFABRRRQAmKWiigAooooAKKKKAExS0UUAFFFFACYpaKKAExRilooAKKKKAExRilooATFGKWigBMUtFFACYpaKKACkxS0UAFVYf+P65+if1q1VWH/j/ALn6J/I0AWqKKKACiiigAry7xPqyXF/rcmpWmoBLKCS309FtHaMsVO6Utjbz90egB9a9RqC5tIry1mtp13xSoUdemQRgj8aAMXwVfRX3hLTniWVQkKxnzEKkkDkjPaua8WXV/pWvvJ4U3z6pPCWvbQLvjChfllPIw47Dv3rv7W1is7SK2gG2KJAiDOcAdKSKxtoJppoYUSWYhpHVeXI9aAMbwdFp66BHLp90935zF5riU/vHlP3tw7H27VT+I4H/AAgmoAdzH2H/AD0Wult7G2tGlaCFIzM2+TaMbm7k+9ZuueFdL8RFP7SSaRFGAizMqnnPIB5oAyPiED/wi9uTnykvLZpcDjb5i5z7etJ4qIfxT4RSE5nN67jH/PMRnd+H3a3bbw7p1tpM2liN5bOXO9JnL5yMYyah0rwppWj3f2u1ikM4Ty1eWRnKL/dXJ4FAEUXhS3ivoZjd3L20E7XMNo23y0kOec43H7xIBOPyFbk3+pf/AHTUlRzf6l/900ANtv8Aj0h/3B/KuP8Ai6f+LV697wr/AOjFrsLb/j0h/wBwfyqprmi2fiHRbnSb9Wa1uV2yBTg4yD1+oFAC6LxoWnf9esX/AKCK0KjggS3gjhjGI41CqPQDpUlABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABUN3/wAekv8Aumpqhu/+PSX/AHTQBIn+rX6CnU1P9Wv0FOoAKKKKACiiigAooooAKTHOc0tFACY96MUtFABRRRQBXb/kIx/9cm/mKsVXb/kIx/8AXJv5irFAHjvxYj1IeNvCCJezHTbq/hD2pHyCSNwwb1zg/pXsVZOr+HdO1y5064vY2eTT5xcQENjD9OfWtagAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooArD/AJCZ/wCuI/masfxD6VXH/ITP/XEfzNWP4h9KAFooooARvun6UtI33T9KWgAooooAKKKKACiiigAooooAKKKKACqw/wCQg3/XMfzqzUElqkr7yXDYxlWIoAnoqv8AY0/56Tf9/DR9jT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9jT/AJ6Tf9/DQBYoqv8AY0/56Tf9/DR9jT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9jT/AJ6Tf9/DQBYoqv8AY0/56Tf9/DR9jT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9jT/AJ6Tf9/DQBYoqv8AY0/56Tf9/DR9kT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9jT/AJ6Tf9/DQBYoqv8AY0/56Tf9/DR9jT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9jT/AJ6Tf9/DQBYqrF/x/XP0T+tO+xp/z0m/7+GnQ2yQszKWJbqWOaAJqKKKACiiigAooooAKKKKACiiigAooooAKjm/1L/7pqSkZQykHoRigCG1/wCPSH/cH8qnqsLGNQAJJgBwMSGl+xp/z0m/7+GgCxRVf7Gn/PSb/v4aPsaf89Jv+/hoAsUVX+xp/wA9Jv8Av4aPsaf89Jv+/hoAsUVX+xp/z0m/7+Gj7Gn/AD0m/wC/hoAsUVX+xp/z0m/7+Gj7Gn/PSb/v4aALFFV/saf89Jv+/ho+xp/z0m/7+GgCxRVf7Gn/AD0m/wC/ho+xp/z0m/7+GgCxRVf7Gn/PSb/v4aPsaf8APSb/AL+GgCxRVf7Gn/PSb/v4aPsaf89Jv+/hoAsUVX+xp/z0m/7+Gj7Gn/PSb/v4aALFFV/saf8APSb/AL+Gj7Gn/PSb/v4aALFFV/saf89Jv+/ho+xp/wA9Jv8Av4aALFFV/saf89Jv+/ho+xp/z0m/7+GgCxRVf7Gn/PSb/v4aPsaf89Jv+/hoAsUVX+xp/wA9Jv8Av4aPsaf89Jv+/hoAsUVX+xp/z0m/7+Gj7Gn/AD0m/wC/hoAsUVX+xp/z0m/7+Gj7Gn/PSb/v4aALFFV/saf89Jv+/ho+xp/z0m/7+GgCxRVf7Gn/AD0m/wC/ho+xp/z0m/7+GgCxRVf7Gn/PSb/v4aPsaf8APSb/AL+GgCxUF3/x6S/7ppPsaf8APSb/AL+GkNjGV2mSYj3kNAEyH92v0FPpAuAB6UtABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAFZj/AMTGPj/lk38xVmoZbZJmDMWDAYBViKb9jT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9jT/AJ6Tf9/DQBYoqv8AY0/56Tf9/DR9jT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9jT/AJ6Tf9/DQBYoqv8AY0/56Tf9/DR9jT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9jT/AJ6Tf9/DQBYoqv8AY0/56Tf9/DR9jT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9jT/AJ6Tf9/DQBYoqv8AY0/56Tf9/DR9jT/npN/38NAFiiq/2NP+ek3/AH8NH2NP+ek3/fw0AWKKr/Y0/wCek3/fw0fY0/56Tf8Afw0AWKKr/Y0/56Tf9/DR9kT/AJ6Tf9/DQBYoqv8AY0/56Tf9/DR9jT/npN/38NADf+Ykf+uI/mas/wAQ+lRRWqROXBYsRjLMTxUv8Q+lAC0UUUAI33T9KWkb7p+lLQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBQ1jVrbQ9Jn1G7JEMK5O3kkkgAD6k1laf4qkm1S3sNR0qfT5bpC9uZXVg+Oo46HHOKqfEUMPC6zYJht7yCabAz+7VwWqPxLNFeeJPCkNrIskq3j3B2HOIwhBP05FAHZ0UUUAFFFFABRRRQAUUUUAFFFFAHON4sMfie30aXSbyIXDOsV0+0I5VcnHOata34gTSJLW3jtZbu8u3Kw28RAJwMkkngADvWL4jngXx14V3TRjZJc78kfLmLv6UusOkHxG8P3UzAW8lrcQo5IxvO0gfiM4oA1NL8UQXyagt1byWNxp/NzDMQSi43BgRwQR3qhbeN/NaynudJubXTr6QR291IykEn7pYA5UN2rntbibU9c8ZtYHeiaRFbuydGkyzY9CdvFW/EV7bX/wAPNJhs5FeS6ktEt0RuchlJHHTABz6UAeh0UgGBS0AFFFFABRRRQAUUUUAVr26a0spbhLeS4aNciKIZZz6CsG28WTLqlpY6po9zp5vSy28jurq7AE7TtPBIBrobm5htLaW4uJFihjUs8jnAUepNcbps6+L9etdYeRI9LsmY2ELN888hBBkI9ME4FAGpqPit7fVbjTtP0u41Ca1jWS4MTqBGDyByeTgdKfL4w09fDVvrcKSzRXJCwRIPndycBceuc/kay/D08dj4u8XRXUqRyNcRXCl24aMxgA/hjBrltNUweFfDWoSjZYjXpLgbuAsTu4Un0HPX3oA77SfEzX2rSaVfafNYXyxeckcjBhJHnGVI9+1dBXF37R3fxU0UWzBmtLC4kuSvQKxVVBP1zXaUAFFFFABRRRQAUUUUAFZ2rX91Y26PaabNeyM23ZG6rtHqSe1aNU7/ABJavALz7JLKrLHKu3cp9VzxmgDM0fxN/aF/dadeWMthe28YlaOVgQyHOGDDjHBrOTx7HJEt+NLuv7GafyRf7l2/e279uc7c96yNOtbi08Ta5phvjqt3cacWW9Y4ljPIEZwdoGeeAOtVDe2snwLjto2UTtai1EecN5wbGPrnmgDtNX8SfYNRttNs7Ga/vp4zMIomA2Rg43MT0GePrVnQ9ch1yyeeOGSGWKVoZoJfvRuvUHH4VzVl/oPxPIuyFNxosSQu/TKSHeoPryDj8as+CyJtW8UXkJBtZtSIiYdGKooZh7ZoA7GiiigAooooAKKKKACiiigAooooAKKKKACiiigArntT8Tvbas+mafpk+o3UUQlmETqoRScAZJ6nB4roa43RJks/H3iiO5dY3n+zzxb2xlBHtJ/MGgDodE1q117SYNRtC3lS5G1hhlYHBUjsQQa0a434cgvo2pXag/ZrvVrqe34xmNn4I9sg12VABRRRQAUUUUAFFFFABRRRQAVXvLk2tnLOsEk5RSwjiGWf2FWKYzKqlmYBQCSTxxQBieHvEv8AbtxqFu+nXFjPYuiSRzkZ+Ybh09sVVvvGJh1C+tdP0q51AaeAbuSFlAjJG7AyfmOOcCqfhe4hPjbxhtmjO+4tyuCPm/cAcevSoPCl1Bp1x4whvpFSaPVZrlt7c+U6KUI9sZHFAG5e+LLK20Sy1OCOS6F+yJawxD5pXYEgc9OhzTtG8RjUr+6066s5bG/tlWRoJWDZRujAjqMgiuD0uKSx0fwDd3eUt0vJtxY8L5u7yyc9ODj2zXSxlbr4tmW2IaO20YxTsvQM0oKr9cAmgDtKKKKACiiigAooooAKKKKACqOqX82n2fnQWE97JuCiKHGTnvk8Yq9VDVdVs9G02W+vpRFbxjk9c+gA9TQBmaZ4oa51gaRqOmz6fevEZollZWWVR1wR3HFVLnxuI2vZ7bSbq606xkMdxdxsoUEfe2gnJC96i8PW82r643iTUmSKZojDY2QYEwQk5LN/tt39BxzWZ4bvLew+HWqwXkirNaS3aXCu3zbi7EdeuQRj1oA6fVPFEFimnra28t9daif9GghIBcAbixJ4AA61LofiBNYa6gktZbS8tHCTW8hBK55BBHBBriNGifTNd8FNqHyq+ky2yM/RZcq2PQErx71v6KVuviL4gubdg9ultbws68gyDcSPrgigDsaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKrX19b6bYXF7dPsgt42kkb0AGTVmuU+JFvPd/D3WorcFpPJDYXqQrBj+gNAC2PjMz3Nil5pF1YwahxazSspDHGQCAcrkV1VcN4pvbXUrPw0lhKjvPqFvNCEOCI1+ZvpxXc0AFFFFABRRRQAUUUUAFFFFABXN6h4sOna/aaZLpF55d1cLbpd/L5e4jPrmukri/G00Q1TwsPNQMurRkgnkDa3J9KAN7Xddi0O2hZoJLi4uJRDBbxEbpGPpn2BNQaT4kF9e3Wn3tnJYX1tGJWilYHdGc/OCOCMgisvxRKkXijwreyOv2VbmVDJwVDNGQP14/GqerRtqPji9isSGkh0SWOQp/ecsEU9s5B/OgC7H48jeGPUDpV0ujSS+Ut+WXacttD7c5257114OQCOh6V5lNe2s3wPt7eJ1MzWcdqqA/N5wIUjHruBNej2itHZwo/3ljUH64oAnooooAKKKKACiiigAqKeYw28sojZyilticlsDOB71LUbusaM7sFVRlieAO5NAHMf8JjPa3lnHquh3enwXcywRzyOrKHb7obB4z0q5qniV7TVV0uw06bULzyvOkSN1URpnAJJPfBxWIt3F411u1mSZE0PT7gSxEtg3c6/dIH9wH8zVnTpFtfiTraXDhWuLWCSEueqjIYD6N27ZoAvxeMtOk8MPrhWVY42aJ4CP3izA7THj+9nik07xS8+rQ6ZqOmT6fc3EZltxK6sJQMZAI7gEEiuDceb4f1fVIxu09vE4uAw+60SuoLj/ZyCc+1dZ4hmjvPGPhWG1kDypNLcMUP3YgmCSe2SR9aAOzooooAKKKKACiiigAooooApanez2VoZbawmvZdwAiiYA/XJ7Vl6X4me71g6Tf6bPYXhh8+NXYMJEzgkEemRkVs3TosBV7gQF/kSTIBBIOMZ4zXDaZBLpfxES3uNSbV5bmycieUASW6hh8pC/Ltb6A8UAaVx47SI3lzDpd1caXZTNDPeoy7QV4YgZyQD1Naer+JI9OayhtraS+ur7Jt4YSAWUAEtk8AAEc1xmiXUFh8ItWtbx1W5thdwTqzc+YWcge+cjHrVrTYZNN8SeEft2Vzov2Xc3QShUyM9ATj9KAOt0PxBHrS3UZtpbW7tJfKuLaXG5DjIORwQR0Na/8AEPpXGeHCtz8QvFV5bsHttlrA0inhpFQkj04BArs/4h9KAFooooARvun6UtI33T9KWgAooooAKKKKACiiigAooooAKKKKACiiigCOWCOeJ4pUV43GGRhkEe9Z+m+HNH0eZptP0+C3kYbSyLg49PpWpRQAUUUUAFFFFABRRRQAUUUUAFFFFAGJd+EPD99dNdXWk2s07HLSPHyT3q5c6Lpt7p8dhc2cMtrGAqRsowoAwMemKv0UAUtP0mx0m2NvYW0VvESWKxjGSe5qra+F9Esr/wC3W2m28dyCSJFQZUnqR6HmteigAoopkhIjYjqAaAHbvalryWHX9btPBFl4l/t57q7ldM6dKkZWbMm0ouAGBIyep4FeiarrVxp0qR2+i3+oFlLM1sECqB7swyfQCgDWorm5fG2lxeHE1wx3Rt2mFu8Yi/exybtpVlzwQRyKk03xVHfasNNuNNvtPuZIjPALpVxMgODgqx5GRkHkZoA380tcR4d8VLB4asDevPe6jdTypFBEA0sgEjDODjAA6scAevau2ByASMZ7UAQ3dnb39pLa3cSzQSqVeNxkMPQ1l2vhDw/ZTxz2uk2sUsZyjImNp7YrbooAy9T8O6TrEqS6jYQXLqNoaRMnHp9ParU2nWlxYmylt4ntSuwxFRtx6Yq1RQBnaboWmaOHGnWcNt5n3yi8tjpk1o0UUAFFFFABRRRQAUUUUAFUNT0bTtYiWLULSO4RTlQ4zj6VfooAz9N0TTdHRk06zhtg/wB7y1ALfU1B/wAIvog1L+0RptsLvdv83yxnd/e+vvWvRQBn6nomm6wiJqFnDcCM5TeuSv0NWLOxttPtUtrSFIYUGFRBgCrFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVman4e0nWXR9RsILl0GFaRASB6fStOigCKC2itYEggjSOJBtVFGAB7VLRRQAUUUUAFFFFABRRRQAUUUUAFQ3FtFdW8lvOgkikUq6t0YGpqKAMS08I6BYXaXdppNrBcIcrIkYDA4xU2oeGtH1W5W5v9Ot7idRgO6ZOPQ+v41q1naxrMOi2Rupre7mQdRbQNIQMZycDgcUAS3WnWd7ZGzuraOW2ICmNl+XA6fSmaZounaPE8en2cVurnLeWuCx9zXB6R47vrq90W2ms9UZ3ikuLwLYPyrHEWMD7oJOW6fLXo8xKwSMOoUkUAPz7UtefeA/E+pX1rNaaxMHunha6tZSMb4txUj6qRz9RV7RPFph8E6Jfags97f36hY4bdAZJn5zgEgAADJJOKAOzornI/GFqLTUZbuxvLO40+Hz57WZV8zZjIKkMVOcEdetX7rXra0tLC5kimKXskcce1QSC4yM88UAalFc0fGUEmt3ekWmmX91c2kqpO0SrsjDAEMWLdOTx14PFV38fWahrn+zdROlpN5LaiI18kHOM43btueM4xQB1tFc7f8Ai+C01d9Kt9Ovr+9WFZglqqsChzzuLADGB1x1GM0w+NdP/sSLUVtrx5Jbg2qWaxjzvOBOY8ZxkYPOce9AHS1S1LSbHV7b7NqNrDcw7g2yVdwyO9RaVqlxqKSG40q8094zjbc7DuHqCrEGtKgDJsPDGi6Xci5sdNtreYAqHjTacHtxSXPhfRLy/wDt1zptvJckgmRk5YjoT6mteigClqGk2Oq2otr+1iuIQQwWRc4I6EehpdP0uy0q38ixto4It24qg6npmrlFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABTWQMpUgEEYIIyCKdRQBk2HhnRdLujc2Wm28E3OGRANufT0rWoooAKKKKACiiigAooooAKKKKACse+8KaDqdybm+0q1uJiQS8kYJzWxRQBnvommyaWumPZQNYqMLAUBUenFLpui6do8Tx6faRW6ucv5a4LcY5NX6KAMhfC+iJqX9orptsLvdv8AM8sZ3f3vr71q5p1cb4n8XtpuoWenW1lqZne6UO0Vk7h4lG59mB8xxgcetAHY5ozXLeDvEk2v29xJNbXsYaV3iea2aNDGWwoVjwx78UvjHWpdFfQpEuVt4J9TjiuHcgL5ZVs8np2oA6jPtS1x2q+K7WXXNBtNI1a1nFzdlJ0glWQlAjHseOR1q7feMYba8ure10vUNRFnj7VLaIpSI4BxlmGSAc4GTQB0lFYs/iJf7NtL7TtPu9Tjul3x/ZQvC4zli5UD/Go7Xxbp9xo17qUsdxbCxZkuoJk/exMADggE5JyMYJzmgDdzz0pk0EdxBJBKgeKRSjIw4IPBFcLeeLLm717w3apZajpn2m8LFLhVAuIvLY8bWbvt4ODzWzF4uN1eSR2Wiald2sUxge7iWPYGBwcAsGIB6kDtQBPB4K8N20scsOjWcbxsGQrHjaRyMVc1PQNL1jy/7QsYbgx8KXXkD0zWlRQBVTTrOOwFittELMJsEGwbNvpjpVXTPDuk6PK8un2EFvI42s6LgkemfStSigAooooAKKKKACiiigAooooAp6jpVjq1t9mv7aO4hB3BZFyAfX681DpmgaVoxc6dYw27P95kX5m/GtKigDIn8MaJc6j/AGhNpts91kMZDGMkjoT6mrWo6TYatbiDULWK5jByBIucGrtFAFSw02z0u2FtY28cEIbdsQY59fc+9Wv4h9KWk/iH0oAWiiigBG+6fpS01vumnUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUyQExsB1Kmn0UAeZab4BMPgWymhsEsvFFopmimwNxlViQGI4II4/Gr2opql9qlrd6lo2p3FlLZIFtLWcL5U+SW34cZHuTXfYwMdqNvvQB5lp/hzV4vCb2kunNFP/b/ANqEIfdti8wNncTkjHfrXV6lp91N410G9jiZrW2hullkB4UsEC5574NdFtGc0baAPNfDnhvV/DMaavBaNPPOzpe2bkF9nmMVaM9sDHy9OfXmvSVO5Q2MZGcHrS4oxQAtFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVzXjTSr/VNFaDTrm5ikk/dskLKA6uQGzn0Ga6Wk20AcP4f8K31p4hvb641G+EcTRW9sHZf30KLnnA+7udsYx0712swLQSKByVIFPC4paAPOl8NapD4H0ma1t/L17SyzxREjMikndET0+ZT+YFVI/C+pw+G/C0z2dxJPp0TJdWkE/ly7WzyjbhyDjgnkccV6ft96Mc0AcHBoaalp2spa6VqdncXVm1uk+pT7i2c8Ab2OMnNRTHW9VtdAtP+EfurY2VxCbqSZk2jauDswcke9eglc98UYoA5jRtKuota8USXELRxXtwnkOTncohVc9fXNc/9m1tPBkvhEaHO07I1st5vT7P5ZP8ArN27OcHOMda9H2//AK8UbR/WgDmNM0m5svF91O0TG1/s6C3jmJ4ZlZsjGeO1ZX9kPFo2ow6jol1fRy6rJcIls6iRAfuyL8w5/Wu8xzmjb0zz9aAOT8IQ6pDcXxnS/j007fssWoOrTKf4jwTx9TXW0m3B449qWgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK881/wANa1d69dXdvf6g0dtYu1phk/10hIZF44G0D35616HSbaAMXw1or6FpMVo95cXG1F+WUjEZxyBgdPrmqPi/SZ9Vn0FYrUXEUGpRyzqQCBGFbOQeD2rqcUm33oA5PW9AVtc8P3WnadAq294XneKNVKrsYc+2aw20GfSdY1drjTNXvoL27e6hn0+5AHz9Udd64xjGeeK9I2/5xSbB17+tAHA3+lXNrbaFDBpOoHRYYnE9hazjzUcnK7juG4cnODxVCz8LanNo3ieD+zjZfa7mKezhlmDlggBClsnHIx9T1716btpdvpxQBwl5JrGu694cnGg3Npb2V0ZLl52QEEow+XDHK+/fIqlqlhf/AG6V9A0fV9O1N7gHzTKn2Rxu+ZmG45BXOeM16Pt5zS7fegBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApP4h9KWk/ioAWiiigBPWloooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApO1FFAC0UUUAf/2Q=="}}, {"section_id": 33, "text": "## F OTHER RELATED WORKS\n\nIn this section, we provide a comprehensive study of the related works of the applications that are of the concern in this work.\n\nStatistical Learning Theory (SLT): SLT provides a theoretical framework for analyzing generalization error, often producing results of the form (530). The seminal work by Vapnik (2000) established a systematic approach to deriving bounds of this nature. Over time, various approaches in SLT have attempted to estimate $\\epsilon(\\mathcal{F}, N, \\delta)$, as summarized in Table 2. A recurring challenge in these bounds is the need to quantify the \"capacity\" of the model's hypothesis class, which is particularly difficult for DNNs.\nWhile there have been attempts to estimate the VC-dimension, such as those based on the norm of the parameters (Neyshabur et al., 2017), the resulting bounds heavily depend on the norm of the parameters. Consequently, it\n\nremains unclear how to accurately estimate the sample complexity of models when varying the depth or width of DNNs. More recent work, such as Imaizumi and Schmidt-Hieber (2023), presents bounds that are tight but still dependent on the norm of the weights, assuming that the SGD iterates converge to a specific class of parameters.\nAnother line of research by Muthukumar and Sulam (2023) explores bounds that leverage the sparsity of feedforward neural networks. However, there is still a lack of data-dependent bounds that do not rely on capacity estimates for models trained on random labels.\n\n$$\n\\mathbb{P}\\left(\\sup _{f \\in \\mathcal{F}}\\left|\\mathbb{E}_{X, Y}[\\ell(Y, f(X))]-\\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(Y_{i}, f\\left(X_{i}\\right)\\right)\\right| \\leq \\epsilon(\\mathcal{F}, \\delta, N)\\right) \\geq 1-\\delta\n$$\n\n![table_1](table_1)\n\nTable 2: SLT frameworks (in chronological order)\nMatrix recovery: This is a fundamental problem in signal processing, where we seek to recover a matrix by indirect measurements, like random measurements, and random entry access. We typically have limited measurements; the problem itself is ill-posed when reconstructing the matrix. However, if the underlying matrix has certain special structures like low-rankedness, or sparsity in entries, the problem becomes tractable so as to reconstruct the true matrix. In practice, the problem tends to have low-rankedness, therefore having immense literature in this area, our work also presents such results, considering the optimization.\nLet, $Y_{i}=\\left\\langle M^{*}, X_{i}\\right\\rangle+\\epsilon \\in \\mathbb{R}$, where, $X_{i} \\in \\mathbb{R}^{m \\times n}(m \\geq n)$ is Gaussian entried matrix, $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^{2}\\right)$ and $M^{*} \\in \\mathbb{R}^{m \\times n}$ is a $r^{*}$-rank matrix. Consider the below problem\n\n![table_2](table_2)\n\nTable 3: Optimization problems for matrix sensing\n\n$$\n\\min _{r \\in \\mathbb{N}, U \\in \\mathbb{R}^{m \\times r}, V \\in \\mathbb{R}^{n \\times r}}\\left\\|Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right\\|^{2}+\\frac{\\lambda}{2}\\left[\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right]\n$$\n\nThe optimization problem in (531) is non-convex due to its rank-minimization nature, an NP-HARD problem. However, under certain specific conditions on the measurement matrices $X_{i}$, the convex relaxation (532) can recover solutions to (531), as demonstrated in Recht et al. (2008). Solving the convex program (532) requires computing the Singular Value Decomposition (SVD), which has a computational complexity of $\\mathcal{O}\\left(m n^{2}\\right)$.\nTo mitigate this computational burden, the Burer-Monteiro (BM) factorization (Burer and Monteiro, 2003) is employed, yielding the bilinear factorization in the non-convex program (533). This approach is more efficient than (532) because it introduces an implicit rank constraint, $\\operatorname{rank}\\left(U V^{T}\\right) \\leq \\min (n, r)$, which reduces the runtime of SVD to $\\mathcal{O}\\left((m+n) r^{2}\\right)$. Additionally, the equivalence between the nuclear norm and the sum of Frobenius norms,\n\nas shown by Giampouras et al. (2020), further accelerates the optimization process, reducing the complexity to $\\mathcal{O}((m+n) r)$.\n\nWhile the BM factorization program (533) is non-convex, in contrast to the convex program (532), gradient descent (GD) algorithms typically guarantee only local minima for non-convex optimization problems (Reddy and Vidyasagar, 2023). However, Ge et al. (2017) has proven that the program (533) has no spurious local minima, and any local minimum is indeed a global minimum. Numerous studies (Jia et al., 2023) have explored the optimization landscapes and the convergence to global minima.\n\nOur work primarily focuses on the generalization capabilities of the BM factorization program (535), which represents the Lagrangian form of the program (533). Table F summarizes the results from the literature that provide matrix recovery guarantees; from this we can suggest there are no bounds in the literature for low-rank matrix recovery with nuclear norm regularization under noisy settings with generic parameterization. Our work presents results first of its kind.\n\n![table_3](table_3)\n\nTable 4: Summary of Related Works on Matrix Recovery. N/A is an acronym for \"Not Available\".\nTransformers: The remarkable success of Large Language Models (LLMs) (Team, 2024) can largely be attributed to their foundational architecture-Transformers (Vaswani et al., 2017). The optimization dynamics of Transformers have been a subject of extensive recent research (Bordelon et al., 2024), (Singh, 2023), (Yang et al., 2022), (Tian et al., 2023), (Nichani et al., 2024). Although Transformers exhibit impressive generalization capabilities in practical applications (Zhou et al., 2024), there is still a significant gap in the theoretical analysis of their generalization error.\n\nTo apply classical SLT bounds, one must determine the capacities of the function classes induced by Transformers. Previous attempts, such as in (Edelman et al., 2022), have made progress but were limited to scenarios where input data is bounded. In contrast, our work extends these results to settings where the inputs are not necessarily bounded.\n\nAnother line of research (Li et al., 2023), (Deora et al., 2024) has provided bounds that depend on step sizes and initialization choices for Gradient Descent (GD). For instance, Li et al. (2023) offered bounds within the context of in-context learning (Zhang et al., 2024), yet without evaluating the capacities of the stable algorithms used to train these Transformers.\n\nIn the broader literature, existing studies on generalization bounds often rely on strong assumptions, such as (i) bounded input data, (ii) algorithmic stability in some defined sense, and (iii) Lipschitz continuity of the loss function (which does not hold globally for mean squared error). Our results address these limitations by providing near-tight sample complexity bounds, offering a more comprehensive understanding of generalization in Transformer models.", "tables": {"table_1": "| Description | $\\epsilon(\\mathcal{F}, N, \\delta)$ |\n| :--: | :--: |\n| Vapnik-Chernoviks Dimension, (Vapnik, 2000) | $\\sqrt{\\frac{\\operatorname{VCdim}(\\mathcal{F})-\\log (\\delta)}{N}}$ |\n| Rademacher Complexity, (Bartlett and Mendelson, 2001) | $R_{N}(\\mathcal{F})+\\sqrt{\\frac{-\\log (\\delta)}{N}}$ |\n| PAC-Bayes Bounds, (McAllester, 1999) | $\\frac{K L(Q \\| P)-\\log (\\delta)}{N}$ |\n| Gaussian Complexity, (Bartlett and Mendelson, 2001) | $G_{N}(\\mathcal{F})+\\sqrt{\\frac{-\\log (\\delta)}{N}}$ |\n| Information-theoretic Bounds, (?) | $\\frac{1}{N} \\sum_{i=1}^{N} \\sqrt{I\\left(W ;\\left(X_{i}, Y_{i}\\right)\\right)}$ |\n| Algorithmic Stability, (Feldman and Vondrak, 2019) | $\\beta+\\sqrt{\\frac{-\\log (\\delta)}{N}}$ |", "table_2": "| $\\min _{M \\in \\mathbb{R}^{m \\times n}} \\operatorname{rank}(M)$ |  | $\\min _{M \\in \\mathbb{R}^{m \\times n}} \\|\\boldsymbol{M}\\|_{\\star}$ |  |\n| :--: | :--: | :--: | :--: |\n| s.t. $\\quad\\left\\|Y_{i}-\\left\\langle M, X_{i}\\right\\rangle\\right\\| \\leq \\delta$ |  | s.t. $\\quad\\left\\|Y_{i}-\\left\\langle M, X_{i}\\right\\rangle\\right\\| \\leq \\delta$ |  |\n| $\\min _{r \\in \\mathbb{N}, U \\in \\mathbb{R}^{m \\times r}, V \\in \\mathbb{R}^{n \\times r}} \\quad\\left\\|U V^{T}\\right\\|_{\\star}$ |  | $\\min _{r \\in \\mathbb{N}, U \\in \\mathbb{R}^{m \\times r}, V \\in \\mathbb{R}^{n \\times r}} \\quad \\frac{1}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right)$ |  |\n| s.t. $\\quad\\left\\|Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right\\| \\leq \\delta$ |  | s.t. $\\quad\\left\\|Y_{i}-\\left\\langle U V^{T}, X_{i}\\right\\rangle\\right\\| \\leq \\delta$ |  |", "table_3": "| Measurement Type | Scenario | Reference | Result |\n| :--: | :--: | :--: | :--: |\n| Exact | Under-Parameterized $\\left(r<r^{*}\\right)$ | N/A | N/A |\n|  | Exactly-Parameterized $\\left(r=r^{*}\\right)$ | N/A | Not directly available. |\n|  | Over-Parameterized $\\left(r>r^{*}\\right)$ | (St\u00f6ger and Soltansikotabi, 2021) | $\\|U U^{T}-M^{*}\\|_{F} \\leq r^{* 1 / 8}\\left(r-r^{*}\\right)^{3 / 8}$ when $r \\in\\left(r^{*}, 2 r^{*}\\right)$. |\n|  | Generic Parameterization $\\left(r \\geq 1\\right)$ | (Jia et al., 2023) | GD learns rank incrementally, $\\left\\|M^{*}-U U^{T}\\right\\|_{F} \\leq \\alpha^{1 / 20}$, but analysis is algorithmic. |\n|  | SDP Relaxation (Full SDP Matrix) | N/A | Not directly available. |\n| Noisy | Under-Parameterized $\\left(r<r^{*}\\right)$ | N/A | N/A |\n|  | Exactly-Parameterized $\\left(r=r^{*}\\right)$ | (Ma et al., 2020) | $\\left\\|M^{*}-U U^{T}\\right\\|_{F} \\leq \\sqrt{\\frac{\\log 10}{10}}$ under RIP assumptions $\\delta_{2 r^{*}} \\leq 0.1$. |\n|  |  | (Nagaldam and Wainwright, 2011) | $\\|M-M\\|_{F} \\leq \\sqrt{\\gamma^{1 / 10} \\sqrt{10}}$. |\n|  | Over-Parameterized $\\left(r>r^{*}\\right)$ | (Ma et al., 2020) | $\\left\\|M^{*}-U U^{T}\\right\\|_{F} \\leq \\sqrt{\\delta_{r *} \\gamma^{1 / 2}} \\sqrt{\\frac{1}{2}}$. |\n|  | Generic Parameterization $\\left(r \\geq 1\\right)$ | N/A | N/A |\n|  | SDP Relaxation (Full SDP Matrix) | (Cand\u00e8s and Plan, 2011) | $\\|M-M^{*}\\|_{F} \\leq \\sqrt{\\alpha r^{*}} \\sqrt{N}$ under RIP assumptions. |\n|  |  | (Koltrhinskii et al., 2011) | $\\|M-M\\|_{F} \\leq \\frac{\\max \\sqrt{\\alpha r} \\sqrt{N}}{\\sqrt{N}}$ under uniform noisy measurements. |"}, "images": {}}, {"section_id": 34, "text": "# G PRELIMINARIES \n\nThis section provides preliminaries of convex analysis and concentration of measure.", "tables": {}, "images": {}}, {"section_id": 35, "text": "## G. 1 Convex Functions\n\nDefinition 1 ( $L^{2}$ functions). A function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ is said to be square integrable on measure $\\mu$, i.e., $L^{2}(\\mu)$ if and only if,\n\n$$\n\\langle f, f\\rangle_{\\mu}=\\int_{x \\in \\mathcal{X}}\\langle f(x), f(x)\\rangle_{\\mathcal{Y}} d \\mu(x)<\\infty\n$$\n\nDefinition 2 (Convex Set, (Rockafellar, 1970)). A set $\\mathcal{C}$ is said to be convex if and only if $\\forall f, g \\in \\mathcal{C}, \\alpha f+(1-$ $\\alpha) g \\in \\mathcal{C} ; \\forall \\alpha \\in[0,1]$.\nDefinition 3 (Convex functions, (Rockafellar, 1970)). A function, $\\Omega$ is said to be convex if and only if $\\operatorname{dom}(\\Omega)$ is convex and $\\forall f, g \\in \\operatorname{dom}(\\Omega)$ and any $\\alpha \\in[0,1]$.\n\n$$\n\\Omega(\\alpha f+(1-\\alpha) g) \\leq \\alpha \\Omega(f)+(1-\\alpha) \\Omega(g)\n$$\n\nDefinition 4 (Gauge function, (Rockafellar, 1970)). The gauge function or the Minkowski functional is defined in a set $\\mathcal{C} \\in L^{2}(\\mu)$ for a point $f$ as follows,\n\n$$\n\\sigma_{\\mathcal{C}}(f):=\\inf \\{t \\geq 0 ; \\text { such that } f \\in t \\operatorname{conv}(\\mathcal{C})\\}\n$$\n\nDefinition 5 (Polar Set, (Rockafellar, 1970)). The polar set of any set $\\mathcal{C} \\subseteq L^{2}(\\mu)$ is given be\n\n$$\n\\mathcal{C}^{\\circ}:=\\left\\{g \\in L^{2}(\\mu): \\text { such that }\\langle g, f\\rangle_{\\mu} \\leq 1 ; \\forall f \\in \\mathcal{C}\\right\\}\n$$\n\nProposition 5 (Polar Properties).\nDefinition 6 (Polar function, (Rockafellar, 1970)). The polar function of any gauge function, $\\sigma$ defined in the set $\\mathcal{C} \\subseteq L^{2}(\\mu)$ is given be\n\n$$\n\\sigma_{\\mathcal{C}}^{\\circ}(g):=\\sigma_{\\mathcal{C}^{\\circ}}(g)\n$$\n\nDefinition 7 (Fenchal dual, (Rockafellar, 1970)). The fenchal-dual for any $\\mu$-measurable function, $\\Omega$ evaluated at $g \\in L^{2}(\\mu)$ is defined by,\n\n$$\n\\Omega^{*}(g):=\\sup _{f \\in L^{2}(\\mu)}\\langle g, f\\rangle_{\\mu}-\\Omega(f)\n$$\n\nLemma 12 (First Convexity, (Rockafellar, 1970)). Any function $\\Omega$ that is first-order differentiable, $\\Omega \\in \\mathcal{C}^{1}$ is convex if and only if for any $f, g \\in \\operatorname{dom}(\\Omega)$\n\n$$\n\\Omega(f) \\geq \\Omega(g)+\\langle\\nabla \\Omega(g), f-g\\rangle_{\\mu}\n$$\n\nLemma 13 (Strongly Convex, (Rockafellar, 1970)). Any function $\\Omega$ that is first-order differentiable, $\\Omega \\in \\mathcal{C}^{1}$ is said to be $\\lambda(\\geq 0)$-strongly convex if and only if for any $f, g \\in \\operatorname{dom}(\\Omega)$\n\n$$\n\\Omega(f) \\geq \\Omega(g)+\\langle\\nabla \\Omega(g), f-g\\rangle_{\\mu}+\\frac{\\lambda}{2}\\|f-g\\|_{\\mu}^{2}\n$$\n\nDefinition 8 (Lipschitz Continuous). A function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ is said to be Lipschitz continuous with Lipschitz constant $\\|f\\|_{\\text {Lip }}$ if for any $x_{2}, x_{2} \\in \\mathcal{X}$\n\n$$\n\\left\\|f\\left(x_{1}\\right)-f\\left(x_{2}\\right)\\right\\|_{\\mathcal{Y}} \\leq\\|f\\|_{L i p}\\left\\|x_{1}-x_{2}\\right\\|_{\\mathcal{X}}\n$$\n\nRemark: Lipschitz constant, $\\|f\\|_{\\text {Lip }}$ is not a norm but only a semi-norm. Because $\\|f\\|_{\\text {Lip }}=0$, it implies that $f$ can be any constant function.\n\nDefinition 9 (Lipschitz Smooth). A first-order differentiable function $f: \\mathcal{X} \\rightarrow \\mathcal{Y} \\in \\mathcal{C}^{1}$ is said to be lipchtiz smooth if $\\nabla f$ is Lipschitz continous.\nDefinition $10((L, \\lambda)$ convex function). A first-order differentiable function $f: \\mathcal{X} \\rightarrow \\mathcal{Y} \\in \\mathcal{C}^{1}$ is said to be $(L, \\lambda)$ convex if and only if $f$ is L-Lipschitz smooth and $\\lambda$-strongly convex, here $L \\geq \\lambda \\geq 0$.\nProposition 6 (Properties of Lipschitz). The below are few properties of Lipschitz functions,\n\n1. If function $f: \\mathcal{X} \\rightarrow \\mathcal{Y} \\in \\mathcal{C}^{1}$ then $\\sup _{x \\in \\mathcal{X}} \\frac{\\left\\|\\left\\langle\\nabla f(x), x\\right\\rangle\\right\\|_{\\mathcal{Y}}}{\\|x\\|_{\\mathcal{X}}}=\\|f\\|_{L i p}$.\n2. If convex function $f: \\mathcal{X} \\rightarrow \\mathcal{Y} \\in \\mathcal{C}^{1}$ is L-Lipschitz smooth then,\n\n$$\nf\\left(x_{0}\\right)+\\left\\langle\\nabla f\\left(x_{0}\\right), x-x_{0}\\right\\rangle_{\\mathcal{Y}} \\leq f(x) \\leq f\\left(x_{0}\\right)+\\left\\langle\\nabla f\\left(x_{0}\\right), x-x_{0}\\right\\rangle_{\\mathcal{Y}}+\\frac{L}{2}\\left\\|x-x_{0}\\right\\|_{\\mathcal{X}}^{2}\n$$\n\n3. If convex function $f: \\mathcal{X} \\rightarrow \\mathcal{Y} \\in \\mathcal{C}^{1}$ is $(L, \\lambda)$ convex then,\n\n$$\n\\begin{aligned}\nf\\left(x_{0}\\right)+\\left\\langle\\nabla f\\left(x_{0}\\right), x-x_{0}\\right\\rangle_{\\mathcal{Y}}+ & \\frac{\\lambda}{2}\\left\\|x-x_{0}\\right\\|_{\\mathcal{X}}^{2} \\leq f(x) \\\\\n& \\leq f\\left(x_{0}\\right)+\\left\\langle\\nabla f\\left(x_{0}\\right), x-x_{0}\\right\\rangle_{\\mathcal{Y}}+\\frac{L}{2}\\left\\|x-x_{0}\\right\\|_{\\mathcal{X}}^{2}\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 36, "text": "# G. 2 Concentration of Measure \n\nDefinition 11 (Greater than or approximately equal to). The inequality $f \\geq g$ means that $\\exists C>0$ such that $f \\geq C g$.\nDefinition 12 (Sub-Gaussianity). A random variable, $X$ is said to be sub-Gaussian with proxy variance, $\\sigma^{2}$ if the following is satisfied,\n\n$$\n\\mathbb{E}_{X}\\left[e^{t}[X-\\mathbb{E}[X]]\\right] \\leq \\exp \\left(-\\frac{t^{2} \\sigma^{2}}{2}\\right) ; \\forall t \\geq 0\n$$\n\nWe denote, $X \\sim S G\\left(\\sigma^{2}\\right)$.\nDefinition 13 (Sub-exponential). A random variable $X$ is said to be subexponential with the proxy parameter $\\lambda$ if the following is satisfied\n\n$$\n\\mathbb{E}_{X}\\left[e^{t[X-\\mathbb{E}[X]]}\\right] \\leq \\exp \\left(-\\frac{t \\lambda}{2}\\right) ; \\forall t \\geq 0\n$$\n\nWe denote $X \\sim S E(\\lambda)$.\nProposition 7 (Properties of Sub-Gaussianity and Sub-exponential). Let $X, Y$ be two random variables that need not be independent.\n\n1. $X \\in \\mathbb{R}^{n} \\sim S G\\left(\\frac{\\sigma_{X}^{2}}{n} I_{n \\times n}\\right)$ if and only if $\\|X\\|^{2} \\sim S E\\left(\\sigma_{X}^{2}\\right)$.\n2. If $X \\in \\mathbb{R}^{n} \\sim S G\\left(\\frac{\\sigma_{X}^{2}}{n} I_{n \\times n}\\right)$, then for any Lipschitz function $\\phi: \\mathcal{X} \\rightarrow \\mathbb{R}, \\phi(X) \\sim S G\\left(\\|\\phi\\|_{L i p}^{2} \\sigma_{X}^{2} / n\\right)$.\n3. If $X \\in \\mathbb{R}^{n} \\sim S G\\left(\\frac{\\sigma_{X}^{2}}{n} I_{n \\times n}\\right)$, and $Y \\in \\mathbb{R}^{n} \\sim S G\\left(\\frac{\\sigma_{Y}^{2}}{n} I_{n \\times n}\\right)$, then $\\langle X, Y\\rangle \\sim S E\\left(\\sigma_{X} \\sigma_{Y}\\right)$.\n\nLemma 14 (Uniform concentration of function). Consider an $n_{X}$-dimensional vector $X$, and a parameterized function, $g_{\\theta}: \\mathcal{X} \\rightarrow \\mathbb{R}$, where $\\theta \\in \\mathcal{F}_{\\theta}$. Let $\\mathcal{C}$ be some convex set obeying $P\\left(\\cap_{i=1}^{N} X_{i} \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}$. Assume that for any fixed $\\theta_{1}, \\theta_{1} \\in \\mathcal{F}_{\\theta}$ and any $Z \\in \\mathcal{C}$ we have\n\n$$\n\\left|g_{\\theta_{1}}(Z)-g_{\\theta_{2}}(Z)\\right| \\leq K d\\left(\\theta_{1}, \\theta_{2}\\right)\n$$\n\nIn addition, suppose that for any fixed $\\theta \\in \\mathcal{F}_{\\theta}$, we have\n\n$$\n\\left|\\mathbb{E}\\left[g_{\\theta}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)-g_{\\theta}(X)\\right]\\right| \\leq B\n$$\n\nwhere $\\mathcal{P}_{\\mathcal{C}}(\\cdot)$ denotes the Euclidean projection onto the set $\\mathcal{C}$. Finally, suppose that for any fixed $\\theta$ and $\\epsilon \\in[-t, t]$ it holds that\n\n$$\nP\\left(\\left|\\int_{\\omega}\\left(g_{\\theta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right) d \\mu_{N}(\\omega)-\\int_{\\omega}\\left(g_{\\theta} \\circ \\mathcal{P}_{\\mathcal{C}}\\right) d \\mu(\\omega)\\right| \\geq \\epsilon\\right) \\leq \\delta(\\varepsilon)\n$$\n\nThen for any $\\epsilon \\in[-t, t]$,\n\n$$\nP\\left(\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon+B\\right) \\leq \\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(., .), \\epsilon /(2 K)\\right) \\delta(\\epsilon / 4)+\\delta_{\\mathcal{C}}\n$$\n\nProof. The proof technique is similar to that of (Li and Wei, 2023, Lemma 6) but includes more general parameter sets $\\mathcal{F}_{\\theta}$. Let us define\n\n$$\nh_{\\theta}(X):=g_{\\theta}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)\n$$\n\nfrom the assumptions in the lemma, we have that,\n\n$$\nP\\left(\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon\\right) \\leq \\delta(\\varepsilon)\n$$\n\nNext, we must establish uniform concentration overall $\\theta \\in \\mathcal{E}_{\\theta}$. Let us construct a $\\nu$-net for $\\mathcal{F}_{\\theta}$. For any $\\theta^{\\prime} \\in \\mathcal{N}_{\\nu}\\left(\\mathcal{F}_{\\theta}, d(., .)\\right), \\theta \\in \\mathcal{F}_{\\theta}$ from the triangular inequality, and as\n\n$$\n\\left|h_{\\theta}-h_{\\theta^{\\prime}}\\right|=\\left|h_{\\theta}(X)-h_{\\theta^{\\prime}}(X)\\right|=\\left|g_{\\theta}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)-g_{\\theta^{\\prime}}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)\\right| \\leq K d\\left(\\theta, \\theta^{\\prime}\\right)\n$$\n\nThen for any, $X$ we have that that,\n\n$$\nh_{\\theta^{\\prime}}-K d\\left(\\theta, \\theta^{\\prime}\\right) \\leq h_{\\theta} \\leq h_{\\theta^{\\prime}}+K d\\left(\\theta, \\theta^{\\prime}\\right)\n$$\n\nIntegrating with respect to the measure $\\mu_{N}$, we obtain\n\n$$\n\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-K d\\left(\\theta, \\theta^{\\prime}\\right) \\leq \\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega) \\leq \\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)+L_{X} d\\left(\\theta, \\theta^{\\prime}\\right)\n$$\n\nSimilarly for the measure $\\mu$ we obtain\n\n$$\n\\Longrightarrow \\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)-K d\\left(\\theta, \\theta^{\\prime}\\right) \\leq \\int_{\\omega} h_{\\theta} d \\mu(\\omega) \\leq \\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)+K d\\left(\\theta, \\theta^{\\prime}\\right)\n$$\n\nNow, subtracting the above equations, we obtain\n\n$$\n\\begin{gathered}\n\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)-2 K d\\left(\\theta, \\theta^{\\prime}\\right) \\\\\n\\leq \\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega) \\leq \\\\\n\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)+2 K d\\left(\\theta, \\theta^{\\prime}\\right)\n\\end{gathered}\n$$\n\nNow, take the absolute value on both sides. Later on, applying triangular inequality, we obtain\n\n$$\n\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\leq\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right|+2 K d\\left(\\theta, \\theta^{\\prime}\\right)\n$$\n\nNow choose, $\\theta^{*}$ as $\\arg \\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right|$, then we have that,\n\n$$\n\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right| \\leq\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right|+2 K d\\left(\\theta^{*}, \\theta^{\\prime}\\right)\n$$\n\nNow choose any $\\theta^{\\prime}$ that lies at-most $\\nu$ from $\\theta^{*}$ on the metric, $d(.,$.$) , i.e, d\\left(\\theta^{\\prime}, \\theta^{*}\\right) \\leq \\nu$, we have\n\n$$\n\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\leq\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right|+2 K \\nu\n$$\n\nBy definition, we can bound the right hand term by the supremum,\n\n$$\n\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\leq 2 K \\nu+\\sup _{\\theta^{\\prime} \\in \\mathcal{N}_{\\nu}\\left(\\mathcal{F}_{\\theta}, d(.,.\\right)}\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right|\n$$\n\nWe apply the probability measure on both side, obtaining,\n\n$$\n\\begin{gathered}\nP\\left(\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon\\right) \\\\\n\\leq P\\left(\\sup _{\\theta^{\\prime} \\in \\mathcal{N}_{\\nu}\\left(\\mathcal{F}_{\\theta}, d(.,.\\right)}\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right| \\geq \\epsilon-2 K \\nu\\right)\n\\end{gathered}\n$$\n\nthe inequality is satisfied by the monotonicity of the probability measure. Now we apply the union-argument for the $\\nu$-net cover then we have\n\n$$\n\\begin{gathered}\nP\\left(\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon\\right) \\leq \\\\\nP\\left(\\bigcup_{\\theta^{\\prime} \\in \\mathcal{N}_{\\nu}\\left(\\mathcal{F}_{\\theta}, d(.,.\\right)}\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon-2 K \\nu\\right)\n\\end{gathered}\n$$\n\nNow we upper bound the right side union term with summation, and then we have\n\n$$\n\\begin{gathered}\nP\\left(\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon\\right) \\leq \\\\\n\\sum_{\\theta^{\\prime} \\in \\mathcal{N}_{\\nu}\\left(\\mathcal{F}_{\\theta}, d(.,.\\right))} P\\left(\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon-2 K \\nu\\right)\n\\end{gathered}\n$$\n\nNow we replace the summation with the $\\nu$-covering number, $\\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(.,.\\right), \\nu)$ obtaining\n\n$$\nP\\left(\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon\\right) \\leq \\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(.,.\\right), \\nu) \\delta(\\epsilon-2 K \\nu)\n$$\n\nNow set $\\nu=\\epsilon /(2 K)$ then we have\n\n$$\n\\Longrightarrow P\\left(\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon\\right) \\leq \\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(.,.\\right), \\epsilon /(2 K)) \\delta(\\epsilon / 2)\n$$\n\nNow, we have established the uniform concentration for $h_{\\theta}$. Next, we move onto relating $h_{\\theta}$ with the desired function $h_{\\theta}$.\nRecall that\n\n$$\n\\left|\\mathbb{E}\\left[h_{\\theta}(X)-g_{\\theta}(X)\\right]\\right| \\leq B\n$$\n\nAs $P\\left(\\cap_{i=1}^{N} X_{i} \\in \\mathcal{C}\\right) \\geq 1-\\delta_{\\mathcal{C}}$, we can safely claim $\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)=\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)$ with probability at least $1-\\delta_{\\mathcal{C}}$. We have\n\n$$\n\\left|\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right|=\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right|\n$$\n\n$$\n\\begin{aligned}\n& \\leq\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right|+\\left|\\int_{\\omega} h_{\\theta} d \\mu(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right| \\\\\n\\Longrightarrow & \\left|\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right| \\leq\\left|\\int_{\\omega} h_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta} d \\mu(\\omega)\\right|+B\n\\end{aligned}\n$$\n\nwith probability at least $1-\\delta_{\\mathcal{C}}$. Now we check the Lipschitzness of function $h_{\\theta}$ in $\\theta$ we have\n\n$$\n\\left|h_{\\theta_{1}}(X)-h_{\\theta_{2}}(X)\\right|=\\left|g_{\\theta_{1}}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)-g_{\\theta_{2}}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)\\right| \\leq K d\\left(\\theta_{1}, \\theta_{2}\\right)\n$$\n\nSimilarly, in expectation measure, we have that\n\n$$\n\\begin{gathered}\n\\left|\\mathbb{E}\\left[h_{\\theta_{1}}(X)\\right]-\\mathbb{E}\\left[h_{\\theta_{2}}(X)\\right]\\right|=\\left|\\mathbb{E}\\left[g_{\\theta_{1}}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)\\right]-\\mathbb{E}\\left[g_{\\theta_{2}}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)\\right]\\right| \\leq \\mathbb{E}\\left[\\left[g_{\\theta_{1}}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)-g_{\\theta_{2}}\\left(\\mathcal{P}_{\\mathcal{C}}(X)\\right)\\right]\\right] \\\\\n\\leq K d\\left(\\theta_{1}, \\theta_{2}\\right)\n\\end{gathered}\n$$\n\nConsequently for any $\\theta^{\\prime} \\in\\left\\{\\theta^{\\prime}: d\\left(\\theta, \\theta^{\\prime}\\right) \\leq \\epsilon /(2 K)\\right\\}$, we have that\n\n$$\n\\left|\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right| \\leq\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right|+\\nu+B\n$$\n\nNow we choose $\\theta=\\theta^{*}=\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right|$ then,\n\n$$\n\\begin{gathered}\n\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right|=\\left|\\int_{\\omega} g_{\\theta^{*}} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta^{*}} d \\mu(\\omega)\\right| \\\\\n\\leq\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right|+\\epsilon+B\n\\end{gathered}\n$$\n\nWe can take a supremum over $\\theta^{\\prime}$ in the upper bound of the right side term; we have\n\n$$\n\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right| \\leq B+\\epsilon+\\sup _{\\theta^{\\prime} \\in \\mathcal{F}_{\\theta^{\\prime}}}\\left|\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu_{N}(\\omega)-\\int_{\\omega} h_{\\theta^{\\prime}} d \\mu(\\omega)\\right|\n$$\n\nThen we use the inequality (568) and (571) we have that,\n\n$$\n\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right| \\leq 2 \\epsilon+B\n$$\n\nwith probability at least $1-\\left[\\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(., .), \\epsilon /(2 K)\\right) \\delta(\\epsilon / 2)+\\delta_{\\mathcal{C}}\\right]$. Now rescaling we obtain that\n\n$$\nP\\left(\\sup _{\\theta \\in \\mathcal{F}_{\\theta}}\\left|\\int_{\\omega} g_{\\theta} d \\mu_{N}(\\omega)-\\int_{\\omega} g_{\\theta} d \\mu(\\omega)\\right| \\geq \\epsilon+B\\right) \\leq \\mathcal{N}\\left(\\mathcal{F}_{\\theta}, d(., .), \\epsilon /(2 K)\\right) \\delta(\\epsilon / 4)+\\delta_{\\mathcal{C}}\n$$", "tables": {}, "images": {}}], "id": "2411.02767v2", "authors": ["Uday Kiran Reddy Tadipatri", "Benjamin D. Haeffele", "Joshua Agterberg", "Ren\u00e9 Vidal"], "categories": ["cs.LG", "eess.SP", "stat.ML"], "abstract": "We propose a general framework for deriving generalization bounds for\nparallel positively homogeneous neural networks--a class of neural networks\nwhose input-output map decomposes as the sum of positively homogeneous maps.\nExamples of such networks include matrix factorization and sensing,\nsingle-layer multi-head attention mechanisms, tensor factorization, deep linear\nand ReLU networks, and more. Our general framework is based on linking the\nnon-convex empirical risk minimization (ERM) problem to a closely related\nconvex optimization problem over prediction functions, which provides a global,\nachievable lower-bound to the ERM problem. We exploit this convex lower-bound\nto perform generalization analysis in the convex space while controlling the\ndiscrepancy between the convex model and its non-convex counterpart. We apply\nour general framework to a wide variety of models ranging from low-rank matrix\nsensing, to structured matrix sensing, two-layer linear networks, two-layer\nReLU networks, and single-layer multi-head attention mechanisms, achieving\ngeneralization bounds with a sample complexity that scales almost linearly with\nthe network width.", "updated": "2025-03-18T21:12:45Z", "published": "2024-11-05T03:24:34Z"}