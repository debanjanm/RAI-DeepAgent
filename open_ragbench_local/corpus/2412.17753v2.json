{"title": "Minimax Optimal Simple Regret in Two-Armed Best-Arm Identification", "sections": [{"section_id": 0, "text": "#### Abstract\n\nThis study investigates an asymptotically minimax optimal algorithm in the two-armed fixed-budget best-arm identification (BAI) problem. Given two treatment arms, the objective is to identify the arm with the highest expected outcome through an adaptive experiment. We focus on the Neyman allocation, where treatment arms are allocated following the ratio of their outcome standard deviations. Our primary contribution is to prove the minimax optimality of the Neyman allocation for the simple regret, defined as the difference between the expected outcomes of the true best arm and the estimated best arm. Specifically, we first derive a minimax lower bound for the expected simple regret, which characterizes the worst-case performance achievable under the location-shift distributions, including Gaussian distributions. We then rigorously show that the simple regret of the Neyman allocation asymptotically matches this lower bound, including the constant term, not just the rate in terms of the sample size, under the worst-case distribution. Notably, our optimality result holds without imposing locality restrictions on the distribution, such as the local asymptotic normality. Furthermore, we demonstrate that the Neyman allocation reduces to the uniform allocation, i.e., the standard randomized controlled trial, under Bernoulli distributions.", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1 Introduction\n\nWe address the problem of adaptive experimental design with two treatment arms, where the goal is to identify the treatment arm with the highest expected outcome through an adaptive experiment. This problem, often referred to as the fixed-budget best-arm identification (BAI) problem, has been widely studied in various fields, including machine learning (Audibert et al., 2010; Bubeck et al., 2011), operations research, economics (Kasy \\& Sautmann, 2021), and epidemiology.\n\nIn this study, we focus on the Neyman allocation algorithm, which allocates samples to the treatment arms following the ratio of their standard deviations. We prove that the Neyman allocation is asymptotically minimax optimal for the simple regret, which is the difference between the expected outcomes of the true best arm and the estimated best arm.\n\nWhile it is known that the Neyman allocation achieves asymptotic optimality for any distribution (Glynn \\& Juneja, 2004; Kaufmann et al., 2016), including worst-case scenarios, the optimal algorithm has remained unknown when the outcome variances are unknown.\n\nOur contributions are twofold. First, we derive a minimax lower bound for the simple regret under the worst-case distribution among all distributions with fixed variances. Second, we demonstrate that the Neyman allocation achieves this minimax lower bound asymptotically, including the constant term, thereby providing a complete solution to the problem. Notably, our results hold without requiring any locality restrictions on the distributions.\n\nThe remainder of this paper is organized as follows. In this section, we provide a formal problem setup, contributions, and related work. Section 2 defines the Neyman allocation. Section 3 presents the minimax lower bound, including its\n\nderivation of the minimax lower bound. Section 4 shows the regret upper bound of the Neyman allocation and proves the minimax optimality by demonstrating that the upper bound matches the lower bound.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 1.1 Problem setting \n\nWe formulate the problem as follows. There are two arms, and each arm $a \\in\\{1,2\\}$ has a potential outcome $Y(a) \\in \\mathbb{R}$. Each potential outcome follows a marginal distribution $P_{\\mu(a)}(a)$, and let $P_{\\boldsymbol{\\mu}}:=\\left(P_{\\mu(a)}(1), P_{\\mu(a)}(2)\\right)$ be the pair of the marginal distributions, where $\\boldsymbol{\\mu}:=\\{\\mu(1), \\mu(2)\\} \\in \\mathbb{R}^{2}$ represents the set of mean parameters of $(Y(1), Y(2))$. Specifically, the expected value of each outcome satisfies $\\mathbb{E}_{\\boldsymbol{\\mu}}[Y(a)]=\\mu(a)$, where $\\mathbb{E}_{\\boldsymbol{\\mu}}[\\cdot]$ is the expectation under $P_{\\boldsymbol{\\mu}}$.\n\nLet $\\boldsymbol{\\mu}_{0}:=\\left\\{\\mu_{0}(1), \\mu_{0}(2)\\right\\}$ represent the true mean parameters. The objective is to identify the best arm\n\n$$\na^{*}\\left(\\boldsymbol{\\mu}_{0}\\right)=\\arg \\max _{a \\in\\{1,2\\}} \\mu_{0}(a)\n$$\n\nthrough an adaptive experiment where data is generated from $P_{\\boldsymbol{\\mu}_{0}}$.\nLet $T$ denote the total sample size, also referred to as the budget. We consider an adaptive experimental procedure consisting of two phases:\n(1) Allocation phase: For each $t \\in[T]:=\\{1,2, \\ldots, T\\}$ :\n\n- A treatment arm $A_{t} \\in\\{1,2\\}$ is selected based on the past observations $\\left\\{\\left(A_{s}, Y_{s}\\right)\\right\\}_{s=1}^{t-1}$.\n- The corresponding outcome $Y_{t}$ is observed, where\n\n$$\nY_{t}:=\\sum_{a \\in\\{1,2\\}} Y_{t}(a)\n$$\n\nand $\\left(Y_{t}(1), Y_{t}(2)\\right)$ follows the distribution $P_{\\boldsymbol{\\mu}_{0}}$.\n(2) Recommendation phase: At the end of the experiment $(t=T)$, based on the observed outcomes, the best arm $\\widetilde{a}_{T} \\in\\{1,2\\}$ is recommended as the estimate of $a_{0}^{*}$.\n\nOur task is to design an algorithm $\\pi$ that determines how arms are selected during the allocation phase and how the best arm is recommended at the end of the experiment. An algorithm $\\pi$ is formally defined as a pair $\\left(\\left(A_{t}^{\\pi}\\right)_{t \\in[T]}, \\widetilde{a}_{T}^{\\pi}\\right)$, where $\\left(A_{t}^{\\pi}\\right)_{t \\in[T]}$ are indicators for the selected arms, and $\\widetilde{a}_{T}^{\\pi}$ is the estimator of the best arm $a_{0}^{*}$. For simplicity, we omit the subscript $\\pi$ when the dependence is clear from the context.\nThe performance of an algorithm $\\pi$ is measured by the expected simple regret, defined as:\n\n$$\n\\operatorname{Regret}_{P_{\\boldsymbol{\\mu}_{0}}}(\\pi):=\\mathbb{E}\\left[Y\\left(a^{*}\\left(\\boldsymbol{\\mu}_{0}\\right)\\right)-Y\\left(\\widetilde{a}_{T}^{\\pi}\\right)\\right]=\\mu_{0}\\left(a^{*}\\left(\\boldsymbol{\\mu}_{0}\\right)\\right)-\\mu_{0}\\left(\\widetilde{a}_{T}^{\\pi}\\right)\n$$\n\nIn other words, the goal is to design an algorithm $\\pi$ that minimizes the simple regret $\\operatorname{Regret}_{P_{\\boldsymbol{\\mu}_{0}}}(\\pi)$.\nNotation. Let $\\mathbb{P}_{P_{\\boldsymbol{\\mu}}}$ denote the probability law under $P_{\\boldsymbol{\\mu}}$, and let $\\mathbb{E}_{P_{\\boldsymbol{\\mu}}}$ represent the corresponding expectation operator. For notational simplicity, depending on the context, we abbreviate $\\mathbb{P}_{P_{\\boldsymbol{\\mu}}}[\\cdot], \\mathbb{E}_{P_{\\boldsymbol{\\mu}}}[\\cdot]$, and $\\operatorname{Regret}_{P_{\\boldsymbol{\\mu}}}(\\pi)$ as $\\mathbb{P}_{\\boldsymbol{\\mu}}[\\cdot]$, $\\mathbb{E}_{\\boldsymbol{\\mu}}[\\cdot]$, and $\\operatorname{Regret}_{\\boldsymbol{\\mu}}(\\pi)$, respectively.\nFor each $a \\in\\{1,2\\}$, let $P_{a, \\boldsymbol{\\mu}}$ denote the marginal distribution of $Y(a)$ under $P_{\\boldsymbol{\\mu}}$. The Kullback-Leibler (KL) divergence between two distributions $P_{a, \\boldsymbol{\\mu}}$ and $P_{a, \\boldsymbol{\\nu}}$, where $\\boldsymbol{\\mu}, \\boldsymbol{\\nu} \\in \\mathbb{R}^{2}$, is denoted as $\\operatorname{KL}\\left(P_{a, \\boldsymbol{\\mu}}, P_{a, \\boldsymbol{\\nu}}\\right)$. When the marginal distribution depends only on the parameters $\\mu(a)$ and $\\nu(a)$, we simplify the notation to $\\operatorname{KL}(\\mu(a), \\nu(a))$. Let $\\mathcal{F}_{t}=\\sigma\\left(A_{1}, Y_{1}, \\ldots, A_{t}, Y_{t}\\right)$ be the sigma-algebras.\nFor simplicity, we refer to the expected simple regret as the simple regret in this study, although the simple regret originally refers to the random variable $Y\\left(a^{*}\\left(\\boldsymbol{\\mu}_{0}\\right)\\right)-Y\\left(\\widetilde{a}_{T}^{\\pi}\\right)$ without expectation.", "tables": {}, "images": {}}, {"section_id": 3, "text": "### 1.2 Content of the Paper\n\nThis study proposes an asymptotically minimax optimal algorithm by deriving a minimax lower bound and demonstrating that the simple regret of the proposed algorithm exactly matches the lower bound, including the constant term, not only the rate with respect to $T$.\nFirst, we define the Neyman allocation in Section 2. Since the variance is unknown, we estimate it adaptively during the experiment. In the recommendation phase, we employ the augmented inverse probability weighting (AIPW) estimator.\n\nThe AIPW estimator is chosen because it simplifies the theoretical analysis due to its unbiasedness property for the average treatment effect (ATE), while also being known for achieving the smallest variance.\nNext, we develop a minimax lower bound. Let $\\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}$ be the class of distributions with fixed variances, formally defined in Definition 3.1. We prove that the simple regret of any algorithm that asymptotically identifies the best arm with probability one (Definition 3.2) cannot improve upon the following lower bound:\n\n$$\n\\inf _{\\pi \\in \\Pi} \\lim _{T \\rightarrow \\infty} \\sup _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\sqrt{T} \\operatorname{Regret}_{P}(\\pi) \\geq \\frac{1}{\\sqrt{e}}(\\sigma(1)+\\sigma(2))\n$$\n\nwhere $e=2.718 \\ldots$ is Napier's constant.\nFinally, we establish the worst-case upper bound for the simple regret of the Neyman allocation as follows:\n\n$$\n\\limsup _{T \\rightarrow \\infty} \\sup _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\sqrt{T} \\operatorname{Regret}_{P}\\left(\\pi^{\\mathrm{NA}}\\right) \\leq \\frac{1}{\\sqrt{e}}(\\sigma(1)+\\sigma(2))\n$$\n\nThis result proves that the Neyman allocation is asymptotically minimax optimal, as it achieves:\n\n$$\n\\limsup _{T \\rightarrow \\infty} \\sup _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\sqrt{T} \\operatorname{Regret}_{P}\\left(\\pi^{\\mathrm{NA}}\\right) \\leq \\frac{1}{\\sqrt{e}}(\\sigma(1)+\\sigma(2)) \\leq \\inf _{\\pi \\in \\Pi} \\lim _{T \\rightarrow \\infty} \\sup _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\sqrt{T} \\operatorname{Regret}_{P}(\\pi)\n$$", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 1.3 Related work \n\nAsymptotically optimal strategies have been extensively studied in the fixed-budget BAI problem. First, we note that the simple regret can be decomposed as:\n\n$$\n\\operatorname{Regret}_{P_{\\boldsymbol{\\mu}_{0}}}(\\pi)=\\operatorname{Regret}_{\\boldsymbol{\\mu}_{0}}(\\pi)=\\left(\\max _{b \\in\\{1,2\\}} \\mu(b)-\\min _{b \\in\\{1,2\\}} \\mu(b)\\right) \\mathbb{P}_{\\boldsymbol{\\mu}_{0}}\\left(\\widehat{u}_{T}^{\\pi} \\neq a^{*}\\left(\\boldsymbol{\\mu}_{0}\\right)\\right)\n$$\n\nHere, $\\mathbb{P}_{\\boldsymbol{\\mu}_{0}}\\left(\\widehat{u}_{T}^{\\pi} \\neq a^{*}\\left(\\boldsymbol{\\mu}_{0}\\right)\\right)$ is referred to as the probability of misidentification, which is also a parameter of interest in BAI (Kaufmann et al., 2016). Since there are only two treatment arms, the absolute value of the gap $\\max _{b \\in\\{1,2\\}} \\mu(b)-$ $\\min _{b \\in\\{1,2\\}} \\mu(b)$ is equivalent to the absolute value of the average treatment effect (ATE), i.e., $|\\mu(1)-\\mu(2)|$.\nFor simplicity, in this section, we assume without loss of generality that the best arm is arm 1, i.e., $a^{*}\\left(\\boldsymbol{\\mu}_{0}\\right)=1$, so that $\\max _{b \\in\\{1,2\\}} \\mu(b)-\\min _{b \\in\\{1,2\\}} \\mu(b)=\\mu(1)-\\mu(2)$ and $\\mathbb{P}_{\\boldsymbol{\\mu}_{0}}\\left(\\widehat{u}_{T}^{\\pi} \\neq a^{*}\\left(\\boldsymbol{\\mu}_{0}\\right)\\right)=\\mathbb{P}_{\\boldsymbol{\\mu}_{0}}\\left(\\widehat{u}_{T}^{\\pi} \\neq 1\\right)$.\nIn the evaluation of the simple regret, the balance between the ATE $\\mu(1)-\\mu(2)$ and the probability of misidentification $\\mathbb{P}_{\\boldsymbol{\\mu}_{0}}\\left(\\widehat{u}_{T}^{\\pi} \\neq 1\\right)$ plays a key role. When evaluating the simple regret $\\operatorname{Regret}_{\\boldsymbol{\\mu}_{0}}(\\pi)$ for each $P_{\\boldsymbol{\\mu}_{0}}$, under well-designed algorithms, such as consistent strategies explained in Definition 3.2, the probability of misidentification converges to zero as $T \\rightarrow \\infty$ with an order of $\\exp \\left(-T C\\left(\\boldsymbol{\\mu}_{0}\\right)\\right)$, where $C\\left(\\boldsymbol{\\mu}_{0}\\right)>0$ is a parameter depending on $\\boldsymbol{\\mu}_{0}$.\nSince the simple regret is the product of the ATE and the probability of misidentification, we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}_{\\boldsymbol{\\mu}_{0}}\\left(\\widehat{u}_{T}^{\\pi} \\neq 1\\right) \\approx \\exp \\left(-T C\\left(\\boldsymbol{\\mu}_{0}\\right)\\right), \\quad C\\left(\\boldsymbol{\\mu}_{0}\\right)>0 \\\\\n& \\operatorname{Regret}_{\\boldsymbol{\\mu}_{0}}(\\pi) \\approx(\\mu(1)-\\mu(2)) \\exp \\left(-T C\\left(\\boldsymbol{\\mu}_{0}\\right)\\right)=\\operatorname{gap} \\times \\text { misidentification probability }\n\\end{aligned}\n$$\n\nwhere $C\\left(\\boldsymbol{\\mu}_{0}\\right)$ depends on $\\boldsymbol{\\mu}_{0}$. In this asymptotic regime, if $\\boldsymbol{\\mu}_{0}$ is independent of $T$, the probability of misidentification dominates the convergence of the simple regret since while the probability of misidentification converges to zero at an exponential rate, the gap is a fixed constant. It means that the influence of the ATE $\\mu(1)-\\mu(2)$ becomes negligible as $T \\rightarrow \\infty$.\nWhen the variances of the outcomes are known, the optimality of the Neyman allocation for the BAI problem has been shown using various approaches (Glynn \\& Juneja, 2004; Kaufmann et al., 2014). Notably, Kaufmann et al. (2016) rigorously prove the optimality of the Neyman allocation for the probability of misidentification $\\mathbb{P}_{\\boldsymbol{\\mu}_{0}}\\left(\\widehat{u}_{T}^{\\pi} \\neq 1\\right)$ under any Gaussian distribution with finite variances in the case where $\\boldsymbol{\\mu}_{0}$ is independent of $T$, as stated in the following proposition.\nProposition 1.1 (Theorem 12 in Kaufmann et al. (2016) (informal)). Assume that $w^{*}$ is known. Allocate treatment arm 1 for the first $T_{1}=T w^{*}(1)$ samples and treatment arm 2 for the next $T_{2}=T w^{*}(2)$ samples, where $T_{1}+T_{2}=T$. Using these samples, compute\n\n$$\n\\widehat{\\mu}_{T}^{1}(1):=\\frac{1}{T_{1}} \\sum_{t=1}^{T_{1}} Y_{t}, \\quad \\widehat{\\mu}_{T}^{1}(2):=\\frac{1}{T_{2}} \\sum_{t=T_{1}+1}^{T} Y_{t}\n$$\n\nRecommend $\\widehat{a}_{T}^{\\dagger}:=\\arg \\max _{a \\in\\{1,2\\}} \\widehat{\\mu}_{T}^{\\dagger}(a)$ as the best arm. Then, for any Gaussian distribution $P \\in$ $\\left\\{\\mathcal{N}\\left(\\mu(1), \\sigma^{2}(1)\\right), \\mathcal{N}\\left(\\mu(2), \\sigma^{2}(2)\\right):(\\mu(1), \\mu(2)) \\in \\mathbb{R}^{2}\\right\\}$ with finite variances $\\sigma^{2}(1), \\sigma^{2}(2)>0$, independent of $T$, it holds for sufficiently large $T$ and any algorithm $\\pi \\in \\Pi$ that\n\n$$\n\\mathbb{P}_{P}\\left(\\widehat{a}_{T}^{\\dagger} \\neq a^{*}(P)\\right) \\leq \\exp \\left(-\\frac{T\\left(\\mu_{P}(1)-\\mu_{P}(2)\\right)^{2}}{2(\\sigma(1)+\\sigma(2))^{2}}\\right) \\leq \\mathbb{P}_{P}\\left(\\widehat{a}_{T}^{\\pi} \\neq a^{*}(P)\\right)\n$$\n\nwhere $\\mu_{P}(a)=\\mathbb{E}_{P}[Y(a)]$ and $\\Pi$ is the set of consistent strategies (Definition 3.2).\nThe above result is stronger than minimax optimality because it holds for any distribution $P$, independent of $T$.\nHowever, the problem remains open when the variances are unknown and the distributions are non-Gaussian. Even under Gaussian distributions, variance estimation during the experiment introduces estimation error, which prevents achieving the same guarantees as Kaufmann et al. (2016).\nTo address this challenge, the minimax framework plays a critical role. Adusumilli (2022) tackle this issue and demonstrate that under local asymptotic normality and diffusion approximations, the Neyman allocation is minimax optimal for the simple regret. Similarly, Kato (2024b,a) show that in the small-gap regime, where $\\mu_{P}(1)-\\mu_{P}(2) \\rightarrow 0$, the variance estimation error can be ignored for the probability of misidentification.\nThese studies, however, have notable limitations. Adusumilli (2022) rely on local asymptotic normality and diffusion processes, which are approximations that restrict the underlying distributions. Kato (2024b) avoid such approximations but focus on the small-gap regime, which may not align well with economic theory.\nIn this study, we establish minimax optimality for the simple regret without resorting to local asymptotic normality, diffusion processes, or the small-gap regime. We can estimate the variance, and our algorithms are asymptotically optimal for non-Gaussian distributions. Instead, we adopt the natural and widely-used minimax regret evaluation framework, which has strong connections to economic theory (Manski, 2000, 2002, 2004; Stoye, 2009). Notably, we show that strictly tight lower and upper bounds for the simple regret can be obtained without such approximations.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 2 The Neyman Allocation \n\nThis section introduces the Neyman allocation algorithm with the AIPW estimator. Our proposed algorithm uses the Neyman allocation in the allocation phase and the AIPW estimator in the recommendation phase.", "tables": {}, "images": {}}, {"section_id": 6, "text": "### 2.1 The Neyman allocation in the allocation phase\n\nThe Neyman allocation aims to allocate each treatment arm $a \\in\\{1,2\\}$ with probability $w^{*}(a)$, defined as:\n\n$$\nw^{*}(1):=\\frac{\\sigma(1)}{\\sigma(1)+\\sigma(2)}, \\quad w^{*}(2):=1-w^{*}(1)=\\frac{\\sigma(2)}{\\sigma(1)+\\sigma(2)}\n$$\n\nSince the variances $\\sigma^{2}(1)$ and $\\sigma^{2}(2)$ are unknown, they are estimated using observations collected during the experiment.\nIn the first round $(t=1)$, a treatment arm is randomly allocated with equal probability $1 / 2$. For each round $t=$ $2,3, \\ldots, T$, a treatment arm is allocated based on the estimated allocation probabilities $\\widehat{w}_{t}$, defined as\n\n$$\n\\widehat{w}(1):=\\frac{\\widehat{\\sigma}_{t}(1)}{\\widehat{\\sigma}_{t}(1)+\\widehat{\\sigma}_{t}(2)}, \\quad \\widehat{w}(2):=\\frac{\\widehat{\\sigma}_{t}(2)}{\\widehat{\\sigma}_{t}(1)+\\widehat{\\sigma}_{t}(2)}\n$$\n\nFor each $a \\in\\{1,2\\}$, the variance estimator $\\widehat{\\sigma}_{t}^{2}(a)$ is constructed as follows:\n\n$$\n\\widehat{\\sigma}_{t}^{2}(a):= \\begin{cases}\\widehat{\\sigma}_{t}^{2}(a) & \\text { if } \\widehat{\\sigma}_{t}^{2}(a)>0 \\\\ \\eta & \\text { if } \\widehat{\\sigma}_{t}^{2}(a)=0\\end{cases}\n$$\n\nwhere $\\widehat{\\sigma}_{t}^{2}(a)$ and the sample mean $\\widetilde{\\mu}_{t}(a)$ are given by\n\n$$\n\\widetilde{\\sigma}_{t}^{2}(a):=\\frac{1}{\\sum_{s=1}^{t-1} \\mathbb{1}\\left[A_{s}=a\\right]} \\sum_{s=1}^{t-1} \\mathbb{1}\\left[A_{s}=a\\right]\\left(Y_{s}-\\widetilde{\\mu}_{t}(a)\\right)^{2}\n$$\n\n$$\n\\widetilde{\\mu}_{t}(a):=\\frac{1}{\\sum_{s=1}^{t-1} \\mathbb{1}\\left[A_{s}=a\\right]} \\sum_{s=1}^{t-1} \\mathbb{1}\\left[A_{s}=a\\right] Y_{s}\n$$\n\nHere, $\\eta \\in(0,1)$ is a small positive constant introduced to prevent division by zero. While the choice of $\\eta$ does not affect the asymptotic properties, it may influence finite-sample performance, which is beyond the scope of this study.", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 2.2 AIPW estimator in the recommendation phase \n\nAfter the allocation phase, using the observations $\\left\\{\\left(A_{t}, Y_{t}\\right)\\right\\}_{t=1}^{T}$, the conditional expected outcome $\\mu_{0}(a)$ is estimated. For this estimation, the AIPW estimator is used, defined as follows for each $a \\in\\{1,2\\}$ :\n\n$$\n\\widehat{\\mu}_{T}^{\\mathrm{AIPW}}(a):=\\frac{1}{T} \\sum_{t=1}^{T}\\left(\\frac{\\mathbb{1}\\left[A_{t}=a\\right]\\left(Y_{t}-\\widetilde{\\mu}_{t}(a)\\right)}{\\widehat{w}_{t}(a)}+\\widetilde{\\mu}_{t}(a)\\right)\n$$\n\nThe AIPW estimator is known to be unbiased for $\\mu_{0}(a)$ and achieves the smallest asymptotic variance among mean estimators. Its unbiasedness is based on the property that for $Z_{t}(a):=\\frac{\\mathbb{1}\\left[A_{t}=a\\right]\\left(Y_{t}-\\widetilde{\\mu}_{t}(a)\\right)}{\\widehat{w}_{t}(a)}+\\widetilde{\\mu}_{t}(a)-\\mu_{0}(a),\\left\\{Z_{t}\\right\\}_{t=1}^{T}$ forms a martingale difference sequence; that is,\n\n$$\n\\mathbb{E}\\left[Z_{t}(a) \\mid \\mathcal{F}_{t-1}\\right]=\\mathbb{E}\\left[\\frac{\\mathbb{1}\\left[A_{t}=a\\right]\\left(Y_{t}-\\widetilde{\\mu}_{t}(a)\\right)}{\\widehat{w}_{t}(a)}+\\widetilde{\\mu}_{t}(a)-\\mu_{0}(a) \\mid \\mathcal{F}_{t-1}\\right]=0\n$$\n\nThis property significantly simplifies the theoretical analysis. Additionally, as shown later, since the variance of the mean estimator is the main factor influencing simple regret, reducing this variance directly enhances the overall performance of the algorithm. This type of estimator has been employed in existing studies, such as Hadad et al. (2021) and Kato et al. (2020).\n\nBy contrast, the sample mean $\\widetilde{\\mu}_{t}(a)$ is a biased estimator because $\\mathbb{E}_{P_{0}}\\left[\\widetilde{\\mu}_{t}(a)\\right]=\\mu_{0}(a)$ does not strictly hold. While the asymptotic properties of the AIPW estimator can also be applied to the sample mean, proving this requires more complex techniques. For instance, Hahn et al. (2011) demonstrate the asymptotic normality of the sample mean estimator by first showing its asymptotic equivalence to the AIPW estimator and then proving the asymptotic normality of the latter. However, their proof relies on stochastic equicontinuity, which is insufficient for our analysis since we also evaluate the large deviation property of the AIPW estimator. Although the sample mean performs better in finite samples, the AIPW estimator suffices when the focus is on the asymptotic properties of the algorithm.\nFurthermore, the inverse probability weighting (IPW) estimator $\\widehat{\\mu}_{T}^{\\mathrm{IPW}}(a):=\\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\mathbb{1}\\left[A_{t}=a\\right] Y_{t}}{\\widehat{w}_{t}(a)}$ is unbiased but has a larger variance compared to the AIPW estimator.", "tables": {}, "images": {}}, {"section_id": 8, "text": "## 3 Statistical models and lower bounds\n\nIn this section, we derive a minimax lower bound. We first define a class of distributions considered in this study. Then, we present the minimax lower bound.", "tables": {}, "images": {}}, {"section_id": 9, "text": "### 3.1 Location-shift models\n\nIn this study, we consider the location-shift model with fixed unknown variances.\nDefinition 3.1 (Location-shift models). Fix $\\boldsymbol{\\sigma}^{2}:=\\left\\{\\sigma^{2}(1), \\sigma^{2}(2), \\ldots, \\sigma^{2}(K)\\right\\} \\in \\mathbb{R}^{2}$, which is a vector of variances unknown to us. Then, the location-shift model is defined as follows:\n\n$$\n\\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}:=\\left\\{P_{\\boldsymbol{\\mu}}: \\boldsymbol{\\mu} \\in \\mathbb{R}^{2}, \\operatorname{Var}_{\\boldsymbol{\\mu}}(Y(a))=\\sigma(a) \\forall a \\in\\{1,2\\},(1),(2), \\text { and }(3)\\right\\}\n$$\n\nwhere $\\operatorname{Var}_{\\boldsymbol{\\mu}}(\\cdot)$ denotes a variance operator under $P_{\\boldsymbol{\\mu}}$, and (1) are (2) are defined as follows:\n(1) A distribution $P_{\\mu_{a}, a}$ has a probability mass function or probability density function, denoted by $f_{a}\\left(y \\mid \\mu_{a}\\right)$. Additionally, $f_{a}\\left(y \\mid \\mu_{a}\\right)>0$ holds for all $y \\in \\mathbb{R}$ and $\\mu(a) \\in \\mathbb{R}$.\n(2) For each $\\mu_{a} \\in \\Theta$ and each $a \\in[K]$, the Fisher information $I_{a}\\left(\\mu_{a}\\right)>0$ of $P_{\\mu(a)}(a)$ exists.\n(3) Let $\\ell_{a} \\mu_{a})=\\ell_{a} \\mu_{a} \\mid y)=\\log f(y \\mid \\mu(a))$ be the likelihood function of $P_{\\mu(a)}(a)$, and $\\hat{\\ell}_{a}, \\tilde{\\ell}_{a}$, and $\\tilde{\\ell}_{a}$ be the first, second, and third derivatives of $\\ell_{a}$. The likelihood functions $\\left\\{\\ell_{a} \\mu(a)\\right)\\}_{a \\in[K]}$ are three times differentiable and satisfy the following:\n\n(a) $\\mathbb{E}_{P_{\\mu(a)} \\mid a)}\\left[\\ell_{a}(\\mu(a))\\right]=0$;\n(b) $\\mathbb{E}_{P_{\\mu(a)} \\mid a)}\\left[\\ddot{\\ell}_{a}\\left(\\mu_{a}\\right)\\right]=-I_{a}\\left(\\mu_{a}\\right)=1 / \\sigma^{2}(a)$;\n(c) For each $\\mu(a) \\in \\Theta$, there exist a neighborhood $U(\\theta)$ and a function $u(y \\mid \\mu(a)) \\geq 0$, and the following holds:\ni. $\\left|\\ddot{\\ell}_{a}(\\tau)\\right| \\leq u(y \\mid \\theta)$ for $U(\\mu(a))$;\nii. $\\mathbb{E}_{P_{\\mu(a)} \\mid a)}[u(Y \\mid \\mu(a))]<\\infty$.\n\nIn this model, only mean parameters shift, while the variances are fixed. This model includes a normal distribution as a special case.", "tables": {}, "images": {}}, {"section_id": 10, "text": "# 3.2 Minimax lower bound \n\nNext, we restrict the class of strategies to derive a tight lower bound. In this study, we consider consistent strategies defined as follows:\nDefinition 3.2 (Consistent algorithm). We say that the class of strategies, $\\Pi$, is the class of consistent strategies if the followings hold:\n\n- For any $\\pi \\in \\Pi$ and for any $P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}$ such that $\\mu_{P}(a)$ is independent of $T$, it holds that\n\n$$\n\\lim _{T \\rightarrow \\infty} \\mathbb{P}_{P}\\left(\\widetilde{a}_{T}^{\\pi}=a^{*}(P)\\right)=1\n$$\n\n- For any $\\pi \\in \\Pi$ and for any $P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}$ whose mean satisfies $\\mu_{P}(a) \\geq c / \\sqrt{T}$ for some positive constant $c$ independent of $T$, there exists $C>0$ independent of $T$ such that\n\n$$\n\\lim _{T \\rightarrow \\infty} \\mathbb{P}_{P}\\left(\\widetilde{a}_{T}^{\\pi}=a^{*}(P)\\right) \\geq C\n$$\n\nThis definition implies that any strategies belonging to the set $\\Pi$ returns the true best arm with probability one when the sample size $T$ is sufficiently large.\nTheorem 3.3 (Lower bounds). Fix $\\sigma^{2} \\in(0, \\infty)^{2}$. Then, the following holds:\n\n$$\n\\inf _{\\pi \\in \\Pi} \\liminf _{T \\rightarrow \\infty} \\sqrt{T} \\sup _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\operatorname{Regret}_{P}(\\pi) \\geq \\frac{1}{\\sqrt{c}}(\\sigma(1)+\\sigma(2))\n$$\n\nHere, $\\sqrt{T}$ is a scaling factor.\nIn other expression, we can write the statement as follows: any consistent algorithm $\\pi \\in \\Pi$ satisfies that for any location shift model $P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}$, the simple regret is lower bounded as\n\n$$\n\\operatorname{Regret}_{P}(\\pi) \\geq \\frac{\\sigma(1)+\\sigma(2)}{\\sqrt{c T}}+o\\left(\\frac{1}{\\sqrt{T}}\\right) \\quad(T \\rightarrow \\infty)\n$$", "tables": {}, "images": {}}, {"section_id": 11, "text": "### 3.3 Proof of the minimax lower bound\n\nIn the derivation of the lower bound, we employ the change-of-measure arguments. These arguments involve comparing two distributions: the baseline hypothesis and the alternative hypothesis, to establish a tight lower bound. The change-of-measure approach is a standard method for deriving lower bounds in various problems, including nonparametric regression (Stone, 1982). Local asymptotic normality is one such technique frequently used in this context (van der Vaart, 1998).\nIn the cumulative reward maximization of the bandit problem, the lower bound is derived using similar arguments and is widely recognized as a standard theoretical criterion in this area. This methodology provides a rigorous foundation for analyzing the theoretical performance limits of algorithms.\nLet us denote the number of drawn arms by\n\n$$\nN_{T}(a)=\\sum_{t=1}^{T} \\mathbb{1}\\left[A_{t}=a\\right]\n$$\n\nThen, we introduce the transportation lemma, shown by Kaufmann et al. (2016).\n\nProposition 3.4 (Transportation lemma. From Lemma 1 in Kaufmann et al. (2016)). Let $P$ and $Q$ be two bandit models with $K$ arms such that for all $a$, the marginal distributions $P(a)$ and $Q(a)$ of $Y(a)$ are mutually absolutely continuous. Then, we have\n\n$$\n\\sum_{a \\in\\{1,2\\}} \\mathbb{E}_{P}\\left[N_{T}(a)\\right] \\mathrm{KL}(P(a), Q(a)) \\geq \\sup _{\\mathcal{E} \\in \\mathcal{F}_{T}} d\\left(\\mathbb{P}_{P}(\\mathcal{E}), \\mathbb{P}_{Q}(\\mathcal{E})\\right)\n$$\n\nwhere $d(x, y):=x \\log (x / y)+(1-x) \\log ((1-x) /(1-y))$ is the binary relative entropy, with the convention that $d(0,0)=d(1,1)=0$.\n\nHere, $P$ corresponds to the baseline distribution, and $Q$ corresponds to the corresponding alternative distribution.\nIt is well known that the KL divergence can be approximated by the Fisher information of a parameter when the parameter approaches zero. We summarize this property in the following proposition.\nProposition 3.5 (Proposition 15.3.2. in Duchi (2023) and Theorem 4.4.4 in Calin \\& Udri\u015fte (2014)). We have\n\n$$\n\\lim _{\\nu(a) \\rightarrow \\mu(a)} \\frac{1}{(\\mu(a)-\\nu(a))^{2}} \\mathrm{KL}(\\mu(a), \\nu(a))=\\frac{1}{2} I\\left(\\mu_{a}\\right)\n$$\n\nThen, using Proposition 3.4 and 3.5, we prove the lower bound in Theorem 3.3 as follows.\nProof of Theorem 3.3. We decompose the simple regret as follows:\n\n$$\n\\max _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\operatorname{Regret}_{P}(\\pi)=\\max _{\\widetilde{a} \\in\\{1,2\\}} \\max _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}, \\widetilde{a}}} \\operatorname{Regret}_{P}(\\pi)\n$$\n\nwhere $\\mathcal{P}_{\\boldsymbol{\\sigma}^{2}, \\widetilde{a}}$ is subset of $\\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}$ whose best arm is not $\\widetilde{a}$ :\n\n$$\n\\mathcal{P}_{\\boldsymbol{\\sigma}^{2}, \\widetilde{a}}:=\\left\\{P \\in P_{\\boldsymbol{\\sigma}^{2}}: \\underset{a \\in\\{1,2\\}}{\\arg \\max } \\mathbb{E}[Y(a)] \\neq \\widetilde{a}\\right\\}\n$$\n\nHere, $\\widetilde{a}$ corresponds to the best arm of a baseline hypothesis.\nFirst, we investigate the case with $\\widetilde{a}=1$. Given $P_{\\boldsymbol{\\nu}} \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}, 1}$, we can lower bound $\\operatorname{Regret}_{P}(\\pi)$ as follows:\n\n$$\n\\operatorname{Regret}_{P_{\\boldsymbol{\\nu}}}(\\pi)=\\operatorname{Regret}_{\\boldsymbol{\\nu}}(\\pi)=(\\nu(2)-\\nu(1)) \\mathbb{P}_{\\boldsymbol{\\nu}}\\left(\\widetilde{a}_{T}^{\\pi}=1\\right)\n$$\n\nWe consider the lower bound of $\\mathbb{P}_{\\boldsymbol{\\nu}}\\left(\\widetilde{a}_{T}^{\\pi}=1\\right)$. We define the baseline model $P_{\\boldsymbol{\\mu}}$ with a parameter $\\boldsymbol{\\mu} \\in \\mathbb{R}^{2}$, defined as follows:\n\n$$\n\\mu(b)= \\begin{cases}\\eta & \\text { if } b=1 \\\\ 0 & \\text { if } b=2\\end{cases}\n$$\n\nwhere $\\eta>0$ is a small positive value. We take $\\eta \\rightarrow 0$ at the last step of the proof, indepdently of $T$. Corresponding to the baseline model, the best arm under the alternative hypophysis is given as 2 . We set a parameter $\\boldsymbol{\\nu} \\in \\mathbb{R}^{2}$ of the alternative model $P_{\\boldsymbol{\\nu}}$ as\n\n$$\n\\nu(b)= \\begin{cases}-\\sqrt{\\frac{1}{T}} \\sigma(1) & \\text { if } b=1 \\\\ \\sqrt{\\frac{1}{T}} \\sigma(2) & \\text { if } b=2\\end{cases}\n$$\n\nLet $\\mathcal{E}$ be the event $\\widetilde{a}_{T}^{\\pi}=2$. Between the baseline distribution $P_{\\boldsymbol{\\mu}}$ and the alternative hypothesis $P_{\\boldsymbol{\\nu}}$, from Proposition 3.4, we have\n\n$$\n\\sum_{a \\in\\{1,2\\}} \\mathbb{E}_{\\boldsymbol{\\mu}}\\left[N_{T}(a)\\right] \\mathrm{KL}\\left(P_{\\mu(a)}(a), P_{\\nu(a)}(a)\\right) \\geq \\sup _{\\mathcal{E} \\in \\mathcal{F}_{T}} d\\left(\\mathbb{P}_{\\boldsymbol{\\mu}}(\\mathcal{E}), \\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E})\\right)\n$$\n\nUnder any consistent algorithm $\\pi \\in \\Pi^{\\text {const }}$, we have $\\mathbb{P}_{\\boldsymbol{\\mu}}(\\mathcal{E}) \\rightarrow 0$ and $\\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E}) \\geq C$ as $T \\rightarrow \\infty$, where $C>0$ is a constant independent of $T$.\nTherefore, for any $\\varepsilon>0$, there exists $T(\\epsilon)$ such that for all $T \\geq T(\\varepsilon)$, it holds that\n\n$$\n0 \\leq \\mathbb{P}_{\\boldsymbol{\\mu}}(\\mathcal{E}) \\leq \\varepsilon \\leq \\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E}) \\leq 1\n$$\n\nSince $d(x, y)$ is defined as $d(x, y):=x \\log (x / y)+(1-x) \\log ((1-x) /(1-y))$, we have\n\n$$\n\\begin{aligned}\n& \\sum_{a \\in\\{1,2\\}} \\mathbb{E}_{\\boldsymbol{\\mu}}\\left[N_{T}(a)\\right] \\mathrm{KL}\\left(P_{a, \\mu_{a}}, P_{a, \\nu_{a}}\\right) \\geq d\\left(\\varepsilon, \\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E})\\right) \\\\\n& =\\varepsilon \\log \\left(\\frac{\\varepsilon}{\\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E})}\\right)+(1-\\varepsilon) \\log \\left(\\frac{1-\\varepsilon}{1-\\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E})}\\right) \\\\\n& \\geq \\varepsilon \\log (\\varepsilon)+(1-\\varepsilon) \\log \\left(\\frac{1-\\varepsilon}{1-\\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E})}\\right) \\\\\n& \\geq \\varepsilon \\log (\\varepsilon)+(1-\\varepsilon) \\log \\left(\\frac{1-\\varepsilon}{\\mathbb{P}_{\\boldsymbol{\\nu}}\\left(\\widehat{a}_{T}^{\\pi}=a^{*}\\left(P_{\\boldsymbol{\\mu}}\\right)\\right)}\\right)\n\\end{aligned}\n$$\n\nNote that $\\varepsilon$ is closer to $\\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E})$ than $\\mathbb{P}_{\\boldsymbol{\\mu}}(\\mathcal{E})$; therefore, we used $d\\left(\\mathbb{P}_{\\boldsymbol{\\mu}}(\\mathcal{E}), \\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E})\\right) \\geq d\\left(\\varepsilon, \\mathbb{P}_{\\boldsymbol{\\nu}}(\\mathcal{E})\\right)$.\nTherefore, we have\n\n$$\n\\mathbb{P}_{\\boldsymbol{\\nu}}\\left(\\widehat{a}_{T}^{\\pi}=a^{*}\\left(P_{\\boldsymbol{\\mu}}\\right)\\right) \\geq \\exp \\left(-\\frac{1}{1-\\varepsilon} \\sum_{a \\in\\{1,2\\}} \\mathbb{E}_{P}\\left[N_{T}(a)\\right] \\mathrm{KL}\\left(P_{a, \\mu_{a}}, P_{a, \\nu_{a}}\\right)+\\frac{\\varepsilon}{1-\\varepsilon} \\log (\\varepsilon)\\right)+1-\\varepsilon\n$$\n\nHere, from Proposition 3.5, for any $\\varepsilon>0$, there exists $\\Xi_{a}(\\varepsilon)$ such that for all $-\\Xi_{a}(\\varepsilon)<\\xi_{a}:=-\\mu_{a}+\\nu_{a}<\\Xi_{a}(\\varepsilon)$, the following holds:\n\n$$\n\\mathrm{KL}\\left(\\mu_{a}, \\mu_{a}+\\xi_{a}\\right) \\leq \\frac{\\xi_{a}^{2}}{2} I\\left(\\mu_{a}\\right)+\\varepsilon \\xi_{a}^{2}=\\frac{\\xi_{a}^{2}}{2 \\sigma_{a}\\left(\\mu_{a}\\right)}+\\varepsilon \\xi_{a}^{2}\n$$\n\nwhere we used $I\\left(\\mu_{a}\\right)=\\sigma^{2}(a)$.\nThen, we have\n\n$$\n\\begin{aligned}\n& \\mathbb{P}_{\\boldsymbol{\\nu}}\\left(\\widehat{a}_{T}^{\\pi}=a^{*}(\\boldsymbol{\\mu})\\right) \\geq(1-\\varepsilon) \\exp \\left(-\\frac{1}{1-\\varepsilon} \\sum_{a \\in\\{1,2\\}} \\mathbb{E}_{\\boldsymbol{\\mu}}\\left[N_{T}(a)\\right] \\mathrm{KL}\\left(P_{a, \\mu(a)}, P_{a, \\nu_{a}}\\right)+\\frac{\\varepsilon}{1-\\varepsilon} \\log (\\varepsilon)\\right) \\\\\n& \\geq(1-\\varepsilon) \\exp \\left(-\\frac{1}{1-\\varepsilon} \\sum_{a \\in\\{1,2\\}} \\mathbb{E}_{\\boldsymbol{\\mu}}\\left[N_{T}(a)\\right]\\left(\\frac{(\\mu(a)-\\nu(a))^{2}}{2 \\sigma^{2}(a)}+\\varepsilon(\\mu(a)-\\nu(a))^{2}\\right)+\\frac{\\varepsilon}{1-\\varepsilon} \\log (\\varepsilon)\\right)\n\\end{aligned}\n$$\n\nLet $\\mathbb{E}_{\\boldsymbol{\\mu}}\\left[N_{T}(a)\\right]$ be denoted by $T w_{\\boldsymbol{\\mu}}(a)$. Then, the following inequality holds:\n\n$$\n\\begin{aligned}\n& \\mathbb{P}_{\\boldsymbol{\\nu}}\\left(\\widehat{a}_{T}^{\\pi}=a^{*}(\\boldsymbol{\\nu})\\right) \\\\\n& \\geq(1-\\varepsilon) \\exp \\left(-\\frac{1}{1-\\varepsilon} \\sum_{a \\in\\{1,2\\}}\\left(T w_{\\boldsymbol{\\mu}}(a)\\left(\\frac{(\\mu(a)-\\nu(a))^{2}}{2 \\sigma^{2}(a)}+\\varepsilon(\\mu(a)-\\nu(a))^{2}\\right)\\right)+\\frac{\\varepsilon}{1-\\varepsilon} \\log (\\varepsilon)\\right)\n\\end{aligned}\n$$\n\nWe set $w_{\\boldsymbol{\\mu}}(a)$ as\n\n$$\nw_{\\boldsymbol{\\mu}}(b)= \\begin{cases}\\frac{\\sigma(1)}{\\sigma(1)+\\sigma(2)} & \\text { if } b=1 \\\\ \\frac{\\sigma(2)}{\\sigma(1)+\\sigma(2)} & \\text { if } b=2\\end{cases}\n$$\n\nBy substituting them, we have\n\n$$\n\\begin{aligned}\n& \\max _{P \\in P_{\\boldsymbol{\\mu}} \\mathrm{I}} \\operatorname{Regret}_{P}(\\pi) \\\\\n& \\geq(\\nu(2)-\\nu(1))(1-\\varepsilon) \\\\\n& \\quad \\exp \\left(-\\frac{1}{1-\\varepsilon} \\sum_{a \\in\\{1,2\\}} T w_{\\boldsymbol{\\mu}}(a)\\left(\\frac{(\\mu(a)-\\nu(a))^{2}}{2 \\sigma^{2}(a)}+\\varepsilon(\\mu(a)-\\nu(a))^{2}\\right)+\\frac{\\varepsilon}{1-\\varepsilon} \\log (\\varepsilon)\\right) \\\\\n& =\\sqrt{\\frac{1}{T}}(\\sigma(1)+\\sigma(2))(1-\\varepsilon)\n\\end{aligned}\n$$\n\n$$\n\\begin{gathered}\n\\exp \\left(-\\frac{1}{1-\\varepsilon}\\left((1+g(\\eta)) / 2+\\varepsilon T\\left(\\sqrt{\\frac{1}{T}} \\sigma(1)+\\eta\\right)^{2}+\\varepsilon T\\left(\\sqrt{\\frac{1}{T}} \\sigma(2)\\right)^{2}\\right)+\\frac{\\varepsilon}{1-\\varepsilon} \\log (\\varepsilon)\\right) \\\\\n=\\sqrt{\\frac{1}{T}}(\\sigma(1)+\\sigma(2))(1-\\varepsilon)\\left(\\exp \\left(-\\frac{1}{2(1-\\varepsilon)}(1+\\widetilde{g}(\\eta, \\varepsilon))+\\frac{\\varepsilon}{1-\\varepsilon} \\log (\\varepsilon)\\right)+1-\\varepsilon\\right)\n\\end{gathered}\n$$\n\nwhere $g(\\eta)$ and $\\widetilde{g}(\\eta, \\varepsilon)$ are terns converging to zero as $\\eta \\rightarrow 0$ and $\\varepsilon \\rightarrow 0$.\nThen, for any consistent algorithm $\\pi$, by letting $T \\rightarrow \\infty, \\varepsilon \\rightarrow 0$, and $\\eta \\rightarrow 0$, we have\n\n$$\n\\limsup _{T \\rightarrow \\infty} \\sqrt{T} \\max _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\operatorname{Regret}_{P}(\\pi) \\geq(\\sigma(1)+\\sigma(2)) \\exp (-1 / 2)\n$$", "tables": {}, "images": {}}, {"section_id": 12, "text": "# 4 Upper bound and minimax optimality \n\nIn this subsection, we establish an upper bound on the simple regret for the Neyman allocation algorithm. The bound demonstrates that the Neyman allocation achieves asymptotic minimax optimality. Specifically, the simple regret under this algorithm matches the minimax lower bound including the constant terms, not only for the rate regarding the sample size.\nFirst, we derive the following worst-case upper bound for the simple regret of the Neyman allocation.\nTheorem 4.1. For the Neyman allocation, the simple regret is upper bounded as\n\n$$\n\\limsup _{T \\rightarrow \\infty} \\sup _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\sqrt{T} \\operatorname{Regret}_{P}\\left(\\pi^{\\mathrm{NA}}\\right) \\leq \\frac{1}{\\sqrt{e}}(\\sigma(1)+\\sigma(2))\n$$\n\nWe upper bound the simple regret of the Neyman allocation algorithm in Theorem 4.1. The results in the lower bound (Theorem 3.3) and the upper bound (Theorem 4.1) imply the asymptotic minimax optimality.\nCorollary 4.2 (Asymptotic minimax optimality). Under the same conditions in Theorems 3.3 and 4.1, it holds that\n\n$$\n\\limsup _{T \\rightarrow \\infty} \\sup _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\sqrt{T} \\operatorname{Regret}_{P}\\left(\\pi^{\\mathrm{NA}}\\right) \\leq \\frac{1}{\\sqrt{e}}(\\sigma(1)+\\sigma(2)) \\leq \\min _{\\pi \\in \\mathrm{H}} \\liminf _{T \\rightarrow \\infty} \\sqrt{T} \\sup _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\operatorname{Regret}_{P}(\\pi)\n$$\n\nThis result shows that the exact asymptotic minimax optimality of the Neyman allocation.\nProof of Theorem 4.1. We present the proof of Theorem 4.1. The proof is primarily based on the following lemma from Kato (2024b).\nLemma 4.3. Under $P_{0}$, for all $a \\in\\{1,2\\} \\backslash\\left\\{a_{0}^{*}\\right\\}$ and for all $\\epsilon>0$, there exists $t(\\epsilon)>0$ such that for all $T>t(\\epsilon)$, there exists $\\underline{\\delta}_{T}(\\epsilon)>0$ such that for all $0<\\mu_{0}\\left(a_{0}^{*}\\right)-\\mu_{0}(a)<\\underline{\\delta}_{T}(\\epsilon)$, the following holds:\n\n$$\n\\mathbb{P}_{P_{0}}\\left(\\widehat{\\mu}_{T}^{\\mathrm{AlPW}}\\left(a_{0}^{*}\\right) \\leq \\widehat{\\mu}_{T}^{\\mathrm{AlPW}}(a)\\right) \\leq \\exp \\left(-\\frac{T\\left(\\mu_{0}\\left(a_{0}^{*}\\right)-\\mu_{0}(a)\\right)^{2}}{2(\\sigma(1)+\\sigma(2))^{2}}+\\epsilon\\left(\\mu_{0}\\left(a_{0}^{*}\\right)-\\mu_{0}(a)\\right)^{2} T\\right)\n$$\n\nProof of Theorem 4.1. We decompose the simple regret as\n\n$$\n\\max _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}}} \\operatorname{Regret}_{P}\\left(\\pi^{\\mathrm{NA}}\\right)=\\max _{a^{\\dagger} \\in\\{1,2\\}} \\max _{P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}, a^{\\dagger}}} \\operatorname{Regret}_{P}\\left(\\pi^{\\mathrm{NA}}\\right)\n$$\n\nWe consider the case where the data is generated from $P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}, a^{\\dagger}}$. From Lemma 4.3, for each $P \\in \\mathcal{P}_{\\boldsymbol{\\sigma}^{2}, a^{\\dagger}}$, for all $a \\in\\{1,2\\} \\backslash\\left\\{a^{*}(P)\\right\\}$, and for all $\\epsilon>0$, there exists $t(\\epsilon)>0$ such that for all $T>t(\\epsilon)$, there exists $\\underline{\\delta}_{T}(\\epsilon)>0$ such that for all $0<\\mu_{0}\\left(a_{0}^{*}\\right)-\\mu_{0}(a)<\\underline{\\delta}_{T}(\\epsilon)$, the following holds:\n\n$$\n\\mathbb{P}_{P}\\left(\\widehat{\\mu}_{T}^{\\mathrm{AlPW}}\\left(a^{*}(P)\\right) \\leq \\widehat{\\mu}_{T}^{\\mathrm{AlPW}}(a)\\right) \\leq \\exp \\left(-\\frac{T\\left(\\mu_{0}\\left(a^{*}(P)\\right)-\\mu_{0}(a)\\right)^{2}}{2(\\sigma(1)+\\sigma(2))^{2}}+\\epsilon\\left(\\mu_{0}\\left(a^{*}(P)\\right)-\\mu_{0}(a)\\right)^{2} T\\right)\n$$\n\nTherefore, we have\n\n$$\n\\operatorname{Regret}_{P_{0}}\\left(\\pi^{\\mathrm{NA}}\\right) \\leq\\left(\\mu_{0}\\left(a^{*}\\left(P_{0}\\right)\\right)-\\mu_{0}\\left(a^{\\dagger}\\right)\\right) \\exp \\left(-\\frac{T\\left(\\mu_{0}\\left(a_{0}^{*}\\right)-\\mu_{0}\\left(a^{\\dagger}\\right)\\right)^{2}}{2(\\sigma(1)+\\sigma(2))^{2}}+\\epsilon\\left(\\mu_{0}\\left(a_{0}^{*}\\right)-\\mu_{0}\\left(a^{\\dagger}\\right)\\right)^{2} T\\right)\n$$\n\nwhere $a^{\\dagger} \\neq a^{*}\\left(P_{0}\\right)$. Taking the maximum over $P_{0}$ is equal to solve the following problem:\n\n$$\n\\max _{\\left(\\mu_{0}\\left(a_{0}^{*}\\right)-\\mu_{0}\\left(a^{\\dagger}\\right)\\right) \\in \\mathbb{R}}\\left(\\mu_{0}\\left(a^{*}\\left(P_{0}\\right)\\right)-\\mu_{0}\\left(a^{\\dagger}\\right)\\right) \\exp \\left(-\\frac{T\\left(\\mu_{0}\\left(a_{0}^{*}\\right)-\\mu_{0}\\left(a^{\\dagger}\\right)\\right)^{2}}{2(\\sigma(1)+\\sigma(2))^{2}}\\right)\n$$\n\nwhere we ignored $\\epsilon\\left(\\mu_{0}\\left(a_{0}^{*}\\right)-\\mu_{0}\\left(a^{\\dagger}\\right)\\right)^{2} T$ since it is ignorable at the limit of $T \\rightarrow \\infty$. Then, the maximizer is given as\n\n$$\n\\mu_{0}^{*}\\left(a_{0}^{*}\\right)-\\mu_{0}^{*}\\left(a^{\\dagger}\\right)=\\frac{\\sigma(1)+\\sigma(2)}{\\sqrt{T}}\n$$\n\nBy substituting this maximizer into the regert upper bound, we complete the proof.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# 5 Extension to Bernoulli Distributions \n\nIn this section, we extend our results to the case where the outcomes follow Bernoulli distributions. We find that the Neyman allocation does not outperform the uniform allocation, which assigns an equal number of samples to each treatment arm.\nWhen considering Bernoulli distributions, the variances depend on the means. Specifically, if $\\mu(1)-\\mu(2) \\rightarrow 0$ and $\\mu \\in[0,1]$ such that $\\mu \\approx \\mu(1) \\approx \\mu(2)$, the variances of the outcomes for both treatment arms are given by $\\mu(1-\\mu)$, which achieves its maximum value of 0.5 .\nUsing this property of the Bernoulli distribution, we can establish the following lower bound as a corollary of Theorems 3.3 and 4.1.\nCorollary 5.1 (Minimax Lower Bound under Bernoulli Distributions). The following holds:\n\n$$\n\\inf _{\\pi \\in \\Pi} \\liminf _{T \\rightarrow \\infty} \\sqrt{T} \\sup _{P \\in \\operatorname{primoulll}} \\operatorname{Regret}_{P}(\\pi) \\geq 2 \\sqrt{\\frac{5}{e}}\n$$\n\nWe now consider the following uniform allocation algorithm (assuming $T$ is even for simplicity): for the first $T / 2$ samples, we allocate treatment arm 1 , and for the next $T / 2$ samples, we allocate treatment arm 2 . The uniform allocation algorithm achieves the following upper bound on the simple regret using the Chernoff bound.\nTheorem 5.2 (Simple Regret of Uniform Allocation). For the uniform allocation, the simple regret is upper bounded as:\n\n$$\n\\limsup _{T \\rightarrow \\infty} \\sup _{P \\in \\mathcal{P}_{\\boldsymbol{\\pi}^{2}}} \\sqrt{T} \\operatorname{Regret}_{P}\\left(\\pi^{\\mathrm{NA}}\\right) \\leq 2 \\sqrt{\\frac{5}{e}}\n$$\n\nThus, the uniform allocation is asymptotically minimax optimal for the simple regret.\nNotably, the Neyman allocation achieves the same simple regret as the uniform allocation. This result can be intuitively understood as follows: in the limit where $\\mu(1)-\\mu(2) \\rightarrow 0$, the variances of the two treatment arms become equal. Consequently, the Neyman allocation reduces to allocating an equal number of samples to each arm, which is equivalent to the uniform allocation.\nWe conclude that the Neyman allocation is as efficient as the uniform allocation in the case of Bernoulli distributions. This result implies that no algorithm can outperform the uniform allocation under Bernoulli distributions, making the Neyman allocation unnecessary in this setting. This conclusion is consistent with previous findings by Kaufmann et al. (2014, 2016), Wang et al. (2024), and Kato (2024a). Furthermore, Horn \\& Sloman (2022) empirically report that the exploration sampling algorithm proposed by Kasy \\& Sautmann (2021) performs similarly to the uniform allocation, a result that is theoretically supported by both our findings and the existing literature.", "tables": {}, "images": {}}, {"section_id": 14, "text": "# 6 Conclusion \n\nIn this study, we addressed the fixed-budget BAI problem under the challenging setting of unknown variances. By introducing the Neyman allocation algorithm combined with the AIPW estimator, we proposed an asymptotically minimax optimal solution.\nOur contributions are twofold. First, we derived the minimax lower bound for the simple regret, establishing a theoretical benchmark for any consistent algorithm. Second, we proved that the simple regret of the Neyman allocation algorithm matches this lower bound, including the constant term, not just the rate. This result demonstrates that the Neyman allocation achieves asymptotic minimax optimality even without assumptions such as local asymptotic normality, diffusion processes, or small-gap regimes.\nThe AIPW estimator played a crucial role in achieving this result, as it reduces the variance of the mean estimation, which directly impacts the simple regret. By carefully handling the variance estimation during the adaptive experiment, we showed that the estimation error does not compromise the asymptotic guarantees.\nOur findings contribute to both the theoretical understanding and practical application of adaptive experimental design. Future research could explore the extension of these results to multi-armed settings or investigate the finite-sample behavior of the proposed algorithm to complement the asymptotic analysis.", "tables": {}, "images": {}}, {"section_id": 15, "text": "## References\n\nKarun Adusumilli. Neyman allocation is minimax optimal for best arm identification with two arms, 2022. arXiv:2204.05527. 4\nJean-Yves Audibert, S\u00e9bastien Bubeck, and Remi Munos. Best arm identification in multi-armed bandits. In Conference on Learning Theory, pp. 41-53, 2010.1\nS\u00e9bastien Bubeck, R\u00e9mi Munos, and Gilles Stoltz. Pure exploration in finitely-armed and continuous-armed bandits. Theoretical Computer Science, 2011.1\nOvidiu Calin and Constantin Udri\u015fte. Geometric Modeling in Probability and Statistics. Mathematics and Statistics. Springer International Publishing, 2014. 7\nJohn Duchi. Lecture notes on statistics and information theory, 2023. URL https://web.stanford.edu/class/stats311/lecture-notes.pdf. 7\nPeter Glynn and Sandeep Juneja. A large deviations perspective on ordinal optimization. In Proceedings of the 2004 Winter Simulation Conference, volume 1. IEEE, 2004. 1, 3\nVitor Hadad, David A. Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. Proceedings of the National Academy of Sciences (PNAS), 118(15), 2021.5\nJinyong Hahn, Keisuke Hirano, and Dean Karlan. Adaptive experimental design using the propensity score. Journal of Business \\& Economic Statistics, 29(1):96-108, 2011. ISSN 07350015. URL http://www.jstor.org/stable/25800782. 5\nSamantha Horn and Sabina J. Sloman. A comparison of methods for adaptive experimentation, 2022. arXiv: 2207.00683. 10\n\nMaximilian Kasy and Anja Sautmann. Adaptive treatment assignment in experiments for policy choice. Econometrica, 89(1):113-132, 2021. 1, 10\nMasahiro Kato. Generalized neyman allocation for locally minimax optimal best-arm identification, 2024a. arXiv: 2405.19317. 4, 10\n\nMasahiro Kato. Locally optimal fixed-budget best arm identification in two-armed gaussian bandits with unknown variances, 2024b. arXIV: 2312.12741. 4, 9\nMasahiro Kato, Takuya Ishihara, Junya Honda, and Yusuke Narita. Efficient adaptive experimental design for average treatment effect estimation, 2020. arXiv:2002.05308. 5\nEmilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of a/b testing. In Conference on Learning Theory, volume 35, pp. 461-481, 2014. 3, 10\nEmilie Kaufmann, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. On the complexity of best-arm identification in multi-armed bandit models. Journal of Machine Learning Research, 17(1):1-42, 2016. 1, 3, 4, 6, 7, 10\nCharles F. Manski. Identification problems and decisions under ambiguity: Empirical analysis of treatment response and normative analysis of treatment choice. Journal of Econometrics, 95(2):415-442, 2000.4\n\nCharles F. Manski. Treatment choice under ambiguity induced by inferential problems. Journal of Statistical Planning and Inference, 105(1):67-82, 2002.\nCharles F. Manski. Statistical treatment rules for heterogeneous populations. Econometrica, 72(4):1221-1246, 2004. 4\nCharles J. Stone. Optimal Global Rates of Convergence for Nonparametric Regression. The Annals of Statistics, 10 (4): 1040 - 1053, 1982.\n\nJ\u00f6rg Stoye. Minimax regret treatment choice with finite samples. Journal of Econometrics, 151(1):70-81, 2009.\nA.W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998.\nPo-An Wang, Kaito Ariu, and Alexandre Proutiere. On uniformly optimal algorithms for best arm identification in two-armed bandits with fixed budget. In International Conference on Machine Learning (ICML), 2024. 10", "tables": {}, "images": {}}], "id": "2412.17753v2", "authors": ["Masahiro Kato"], "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.AP", "stat.TH"], "abstract": "This study investigates an asymptotically minimax optimal algorithm in the\ntwo-armed fixed-budget best-arm identification (BAI) problem. Given two\ntreatment arms, the objective is to identify the arm with the highest expected\noutcome through an adaptive experiment. We focus on the Neyman allocation,\nwhere treatment arms are allocated following the ratio of their outcome\nstandard deviations. Our primary contribution is to prove the minimax\noptimality of the Neyman allocation for the simple regret, defined as the\ndifference between the expected outcomes of the true best arm and the estimated\nbest arm. Specifically, we first derive a minimax lower bound for the expected\nsimple regret, which characterizes the worst-case performance achievable under\nthe location-shift distributions, including Gaussian distributions. We then\nshow that the simple regret of the Neyman allocation asymptotically matches\nthis lower bound, including the constant term, not just the rate in terms of\nthe sample size, under the worst-case distribution. Notably, our optimality\nresult holds without imposing locality restrictions on the distribution, such\nas the local asymptotic normality. Furthermore, we demonstrate that the Neyman\nallocation reduces to the uniform allocation, i.e., the standard randomized\ncontrolled trial, under Bernoulli distributions.", "updated": "2025-01-21T19:44:34Z", "published": "2024-12-23T18:06:20Z"}