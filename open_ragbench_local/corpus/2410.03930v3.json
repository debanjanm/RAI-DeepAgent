{"title": "Reverb: Open-Source ASR and Diarization from Rev", "sections": [{"section_id": 0, "text": "## 1 Introduction\n\nRev, as a leader in human transcription of English, has amassed the largest high quality English speech recognition dataset in the world. The research team at Rev has used this corpus to develop extremely accurate speech recognition and speaker diarization models, currently available through the rev.ai ${ }^{1}$ API.\n\nToday, we are open-sourcing our core speech recognition and diarization models for non-commercial use. We are releasing both a full production pipeline for developers as well as pared-down research models for experimentation. Rev hopes that these releases will spur research and innovation in the fast-moving domain of voice technology. The speech recognition models released today outperform all existing open source speech recognition models across a variety of long-form speech recognition domains.\n\nThis release, which we are calling Reverb, encompasses two separate models: an automatic speech recognition (ASR) model in the WeNet ${ }^{2}$ framework and two speaker diarization models in the Pyannote ${ }^{3}$ framework. For researchers, we provide simple scripts for combining ASR (\"Reverb Research\") and diarization output into a single diarized transcript. For developers, we provide a full pipeline (\"Reverb\") that handles both ASR and diarization in a production environment. Additionally, we are releasing an int8 quantized version of the ASR model within the developer pipeline (\"Reverb Turbo\") for applications that are particularly sensitive to speed and/or memory usage.\n\nReverb ASR was trained on 200,000 hours of English speech, all expertly transcribed by humans - the largest corpus of human transcribed audio ever used to train an open-source model. The quality of this data has produced the world's most accurate English ASR system, using an efficient model architecture that can be run on either CPU or GPU.\n\nAdditionally, this model provides user control over the level of verbatimicity of the output transcript, making it ideal for both clean, readable transcription and use-cases like audio editing that require transcription of every spoken word\n\n[^0]\n[^0]:    ${ }^{1}$ http://rev.ai\n    ${ }^{2}$ https://github.com/wenet-e2e/wenet\n    ${ }^{3}$ https://github.com/pyannote/pyannote-audio\n\nincluding hesitations and re-wordings. Users can specify fully verbatim, fully non-verbatim, or anywhere in between for their transcription output.\n\nFor diarization, Rev used the high-performance pyannote.audio ${ }^{4}$ library to fine-tune existing models on 26,000 hours of expertly labeled data, significantly improving their performance. Reverb diarization v1 uses the pyannote $3.0^{5}$ architecture, while Reverb diarization v2 uses WavLM [3] instead of SincNet [8] features.", "tables": {}, "images": {}}, {"section_id": 1, "text": "# 2 Reverb ASR\n### 2.1 Data\n\nRev's ASR dataset is made up of long-form, multi-speaker audio featuring a wide range of domains, accents and recording conditions. This corpus contains audio transcribed in two different styles: verbatim and non-verbatim.\n\nVerbatim transcripts include all speech sounds in the audio (including false starts, filler words, and laughter), while non-verbatim transcripts have been lightly edited for readability; see Section 2.4 for specific examples. Training on both of these transcription styles is what enables the style control feature of the Reverb ASR model.\n\nTo prepare our data for training, we employ a joint normalization and forcedalignment process, which allows us to simultaneously filter out poorly-aligned data and get the best possible timings for segmenting the remaining audio into shorter training segments. During the segmentation process, we include multispeaker segments, so that the resulting ASR model is able to effectively recognize speech across speaker switches.\n\nThe processed ASR training corpus comprises 120,000 hours of speech with verbatim transcription labels and 80,000 hours with non-verbatim labels.", "tables": {}, "images": {}}, {"section_id": 2, "text": "### 2.2 Model Architecture\n\nReverb ASR was trained using a modified version of the WeNet toolkit and uses a joint CTC/ attention architecture ${ }^{6}$. The encoder has 18 conformer layers and the bidirectional attention decoder has 6 transformer layers, 3 in each direction. In total, the model has approximately 280 M parameters.\n\nOne important modification available in Rev's WeNet release is the use of the language-specific layer mechanism [10]. While this technique was originally developed to give control over the output language of multilingual models, Reverb ASR uses these extra weights for control over the verbatimicity of the output. These layers are added to the first and last blocks of both the encoder and decoder.\n\n[^0]\n[^0]:    ${ }^{4}$ https://github.com/pyannote/pyannote-audio\n    ${ }^{5}$ https://huggingface.co/pyannote/speaker-diarization-3.0\n    ${ }^{6}$ https://www.rev.com/blog/speech-to-text-technology/what-makes-revs-v2-best-inclass\n\nThe joint CTC/ attention architecture enables experimentation with a variety of inference modes, including: greedy CTC decoding, CTC prefix beam search (with or without attention rescoring), attention decoding, and joint CTC/ attention decoding. The joint decoding available in Rev's WeNet is a slightly modified version of the time synchronous joint decoding implementation from ESPnet ${ }^{7}$.\n\nThe production pipeline uses WFST-based beam search with a simple unigram language model on top of the encoder outputs, followed by attention rescoring. This pipeline also implements parallel processing and overlap decoding at multiple levels to achieve the best possible turn-around time without introducing errors at the chunk boundaries. While the research model outputs unformatted text, the production pipeline includes a post-processing system for generating fully formatted output.", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 2.3 Benchmarks \n\nUnlike many ASR providers, Rev primarily uses long-form speech recognition corpora for benchmarking. We use each model to produce a transcript of an entire audio file, then use fstalign ${ }^{8}$ to align and score the complete transcript. We report micro-average WER across all of the reference words in a given test suite. As part of our model release, we have included our scoring scripts so that anyone can replicate our work, benchmark other models, or experiment with new long-form test suites.\n\nHere, we've benchmarked Reverb ASR against the best performing opensource models currently available: OpenAI's Whisper large-v39 and NVIDIA's Canary-1B ${ }^{10}$. Note that both of these models have significantly more parameters than Reverb ASR. For these models and Rev's research model, we use simple chunking with no overlap - 30s chunks for Whisper and Canary, and 20s chunks for Reverb. The Reverb research results use CTC prefix beam search with attention rescoring. We used Canary through Hugging Face and used the WhisperX ${ }^{11}$ implementation of Whisper. For Whisper, we use NeMo ${ }^{12}$ to normalize the model outputs before scoring.\n\nFor long-form ASR, we've used three corpora: Rev16 [7] (podcasts), Earnings21 [4] (earnings calls from US-based companies), and Earnings22 [5] (earnings calls from global companies); results are shown in Table 1\n\nFor Rev16, we have produced both verbatim and non-verbatim human transcripts. For all Reverb models, we run in verbatim mode for evaluation with the verbatim reference and non-verbatim mode for evaluation with the non-verbatim reference; results are shown in Table 2.\n\n[^0]\n[^0]:    ${ }^{7}$ https://github.com/espnet/espnet\n    ${ }^{8}$ https://github.com/revdotcom/fstalign/\n    ${ }^{9}$ https://huggingface.co/openai/whisper-large-v3\n    ${ }^{10}$ https://huggingface.co/nvidia/canary-1b\n    ${ }^{11}$ https://github.com/m-bain/whisperX\n    ${ }^{12}$ https://github.com/NVIDIA/NeMo-text-processing\n\nTable 1: WER on the Earnings21 and Earnings22 test sets.\n\n![table_0](table_0)\n\nTable 2: WER on the Rev16 test set, comparison between verbatim references and non-verbatim references.\n\n![table_1](table_1)\n\nWe have also used GigaSpeech [2] for a non-Rev transcribed benchmark. We ran Reverb ASR in verbatim mode and used the HuggingFace Open ASR Leaderboard ${ }^{13}$ evaluation scripts; results are shown in Table 3.\n\nTable 3: WER on the GigaSpeech test set.\n\n![table_2](table_2)\n\nOverall, Reverb ASR significantly outperforms the competition on long-form ASR test suites. Rev's models are particularly strong on the Earnings22 test suite, which contains mainly speech from non-native speakers of English. We see a small WER degradation from the use of the Turbo model, but a much larger gap between the production pipeline and research model - demonstrating the importance of engineering a complete system for long-form speech recognition.\n\nOn the GigaSpeech test suite, Rev's research model is worse than other opensource models. The average segment length of this corpus is 5.7 seconds; these short segments are not a good match for the design of Rev's model. These results demonstrate that despite its strong performance on long-form tests, Rev may not be not the best candidate for short-form recognition applications like voice search.", "tables": {"table_0": "| Model | Earnings21 | Earnings22 |\n| :-- | :-- | :-- |\n| Reverb Verbatim | 7.64 | 11.38 |\n| Reverb Turbo Verbatim | 7.88 | 11.60 |\n| Reverb Research Verbatim | 9.68 | 13.68 |\n| Whisper large-v3 | 13.67 | 18.53 |\n| Canary-1B | 14.40 | 19.01 |", "table_1": "| Model | Verbatim Reference | Non-Verbatim Reference |\n| :-- | :-- | :-- |\n| Reverb | 7.99 | 7.06 |\n| Reverb Turbo | 8.25 | 7.50 |\n| Reverb Research | 10.30 | 9.08 |\n| Whisper large-v3 | 10.67 | 11.37 |\n| Canary-1B | 13.82 | 13.24 |", "table_2": "| Model | GigaSpeech |\n| :-- | :-- |\n| Reverb Research Verbatim | 11.05 |\n| Whisper large-v3 | 10.02 |\n| Canary-1B | 10.12 |"}, "images": {}}, {"section_id": 4, "text": "# 2.4 Verbatimicity \n\nRev has the only AI transcription API and model that allows user control over the verbatimicity of the output. The developer pipeline offers a verbatim mode\n\n[^0]\n[^0]:    ${ }^{13}$ https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\n\nthat transcribes all spoken content and a non-verbatim mode that removes unnecessary phrases to improve readability. The output of the research model can be controlled with a verbatimicity parameter that can be anywhere between zero and one.\n\nThe Rev team has found that halfway between verbatim and non-verbatim produces a reader-preferred style for captioning - capturing all content while reducing some hesitations and stutters to make captions fit better on screen. Table 4 illustrates a range of verbatim expressions and how they tend to be treated by different models. Table 5 illustrates the three Reverb verbatimicity levels with respect to a GigaSpeech utterance.\n\nTable 4: Verbatim styles\n\n![table_3](table_3)\n\nTable 5: Example using GigaSpeech POD1000000032_S0000058\n\n![table_4](table_4)", "tables": {"table_3": "| Verbatim expression | Other models transcribe | Reverb verbatim transcribes |\n| :-- | :--: | :--: |\n| Repeated stutter words |  | $\\checkmark$ |\n| Repeated phrases |  | $\\checkmark$ |\n| Filled pauses (um, uh) |  | $\\checkmark$ |\n| \"you know\" | $\\checkmark$ | $\\checkmark$ |\n| \"kind of\" | $\\checkmark$ | $\\checkmark$ |\n| \"sort of\" | $\\checkmark$ | $\\checkmark$ |\n| \"like\" | $\\checkmark$ | $\\checkmark$ |", "table_4": "| Style | Transcription |\n| :-- | :-- |\n| Reverb verbatim | and and if you if you try and understand which ones <br> there are you it's it's a it's a long list |\n| Reverb non-verbatim | and if you try and understand which ones there are <br> it's a long list |\n| Reverb half-verbatim | and if you if you try and understand which ones <br> there are you it's a long list |"}, "images": {}}, {"section_id": 5, "text": "# 3 Reverb Diarization\n### 3.1 Data\n\nRev's diarization training data comes from the same diverse corpus as the ASR training data. However, annotation for diarization is particularly challenging, because of the need for precise timings, specifically at speaker switches, and the difficulties of handling overlapped speech. As a result, only a subset of the ASR training data is usable for diarization. The total corpus used for diarization is 26,000 hours.", "tables": {}, "images": {}}, {"section_id": 6, "text": "### 3.2 Model Architecture\n\nThe Reverb diarization models were developed using the pyannote.audio library $[6,1]$. Reverb diarization v1 is identical to pyannote $3.0^{14}$ in terms of architecture\n\n[^0]\n[^0]:    ${ }^{14}$ https://huggingface.co/pyannote/speaker-diarization-3.0\n\nbut it is fine-tuned on Rev's transcriptions for 17 epochs. Training took 4 days on a single A100 GPU. The network has 2 LSTM layers with hidden size of 256, totaling approximately 2.2 M parameters. Our most precise diarization model - Reverb diarization v2 - uses WavLM instead of the SincNet features in the pyannote3.0 basic model.", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 3.3 Benchmarks \n\nWhile DER is a valuable metric for assessing the technical performance of a diarization model in isolation, WDER [9] (Word Diarization Error Rate) is more crucial in the context of ASR because it reflects the combined effectiveness of both the diarization and ASR components in producing accurate, speakerattributed text. In practical applications where the accuracy of both \"who spoke\" and \"what was spoken\" is essential, WDER provides a more meaningful and relevant measure for evaluating system performance and guiding improvements. For this reason we only report WDER metrics. We show results for two test suites, Earnings21 and Rev16, in Table 6.\n\nTable 6: WDER on the Earnings21 and Rev16 test sets using Reverb\n\n![table_5](table_5)", "tables": {"table_5": "| Model | Earnings21 | Rev16 |\n| :-- | :-- | :-- |\n| Pyannote3.0 | 0.051 | 0.090 |\n| Reverb Diarization v1 | 0.047 | 0.077 |\n| Reverb Diarization v2 | 0.046 | 0.078 |"}, "images": {}}, {"section_id": 8, "text": "## 4 Conclusion\n\nRev is excited to release the state-of-the-art Reverb ASR and diarization models to the public. We hope that these releases will spur research and innovation in the fast-moving domain of voice technology. To get started, visit https://github.com/revdotcom/reverb for research models or https://github.com /revdotcom/revai for the complete developer solution. Schedule a demo today ${ }^{15}$ to learn more about the Rev.ai API or commercial licensing for Reverb.", "tables": {}, "images": {}}, {"section_id": 9, "text": "## References\n\n1. Bredin, H.: pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe. In: Proc. Interspeech 2023 (2023)\n2. Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., Jin, M., Khudanpur, S., Watanabe, S., Zhao, S., Zou, W., Li, X., Yao, X., Wang, Y., Wang, Y., You, Z., Yan, Z.: Gigaspeech: An evolving, multidomain asr corpus with 10,000 hours of transcribed audio. In: Proc. Interspeech 2021 (2021)\n[^0]\n[^0]:    ${ }^{15}$ https://www.rev.com/online-transcription-services/contact-sales\n\n3. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., Yu, X., Wei, F.: Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing 16(6), 1505-1518 (Oct 2022)\n4. Del Rio, M., Delworth, N., Westerman, R., Huang, M., Bhandari, N., Palakapilly, J., McNamara, Q., Dong, J., Zelasko, P., Jett\u00e9, M.: Earnings-21: A practical benchmark for ASR in the wild. In: Proc. Interspeech 2021. p. 3465-3469 (2021)\n5. Del Rio, M., Miller, C., Profant, J., Drexler-Fox, J., McNamara, Q., Bhandari, N., Delworth, N., Pirkin, I., Jett\u00e9, M., Chandra, S., Ha, P., Westerman, R.: Accents in speech recognition through the lens of a World Englishes evaluation set. Research in Language 21(3), 225-244 (2023)\n6. Plaquet, A., Bredin, H.: Powerset multi-class cross entropy loss for neural speaker diarization. In: Proc. Interspeech 2023 (2023)\n7. Radford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C., Sutskever, I.: Robust speech recognition via large-scale weak supervision. In: International Conference on Machine Learning. pp. 28492-28518. PMLR (2023)\n8. Ravanelli, M., Bengio, Y.: Speaker recognition from raw waveform with sincnet (2019), https://arxiv.org/abs/1808.00158\n9. Shafey, L.E., Soltau, H., Shafran, I.: Joint speech recognition and speaker diarization via sequence transduction (2019), https://arxiv.org/abs/1907.05337\n10. Zhou, L., Li, J., Sun, E., Liu, S.: A configurable multilingual model is all you need to recognize all languages. In: ICASSP 2022. pp. 6422-6426 (2022)", "tables": {}, "images": {}}], "id": "2410.03930v3", "authors": ["Nishchal Bhandari", "Danny Chen", "Miguel \u00c1ngel del R\u00edo Fern\u00e1ndez", "Natalie Delworth", "Jennifer Drexler Fox", "Mig\u00fcel Jett\u00e9", "Quinten McNamara", "Corey Miller", "Ond\u0159ej Novotn\u00fd", "J\u00e1n Profant", "Nan Qin", "Martin Ratajczak", "Jean-Philippe Robichaud"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "abstract": "Today, we are open-sourcing our core speech recognition and diarization\nmodels for non-commercial use. We are releasing both a full production pipeline\nfor developers as well as pared-down research models for experimentation. Rev\nhopes that these releases will spur research and innovation in the fast-moving\ndomain of voice technology. The speech recognition models released today\noutperform all existing open source speech recognition models across a variety\nof long-form speech recognition domains.", "updated": "2025-02-24T19:37:06Z", "published": "2024-10-04T21:13:58Z"}