{"title": "Probability Proofs for Stirling (and More): the Ubiquitous Role of\n  $\\mathbf{\\sqrt{2\u03c0}}$", "sections": [{"section_id": 0, "text": "#### Abstract\n\nThe Stirling approximation formula for $n$ ! dates from 1730. Here we give new and instructive proofs of this and related approximation formulae via tools of probability and statistics. There are connections to the Central Limit Theorem and also to approximations of marginal distributions in Bayesian setups. Certain formulae emerge by working through particular instances, some independently verifiable but others perhaps not. A particular case yielding new formulae is that of summing independent uniforms, related to the Irwin-Hall distribution. Yet further proofs of the Stirling flow from examining aspects of limiting normality of the sample median of uniforms, and from these again we find a proof for the Wallis product formula for $\\pi$.\n\nKey words: binomial, Central Limit Theorem, Gamma variables, history, IrwinHall, Laplace, Poisson, Stirling, Wallis", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1 INTRODUCTION\n\nThe Stirling approximation formula is a famous one, stating that\n\n$$\nn!\\doteq n^{n} \\exp (-n) \\sqrt{2 \\pi n}, \\quad \\text { in the sense of } \\quad \\frac{n!}{n^{n+1 / 2} \\exp (-n)} \\rightarrow \\sqrt{2 \\pi}\n$$\n\nIntriguingly and surprisingly, starting just by multiplying $1 \\cdot 2,1 \\cdot 2 \\cdot 3,1 \\cdot 2 \\cdot 3 \\cdot 4, \\ldots$, the formula turns out to involve the eternal mathematical constants $e=2.718282 \\ldots$ and $\\pi=$ $3.141593 \\ldots$... Our aim here is to tie the Stirling and also related formulae to the Central Limit Theorem (CLT), with the limiting normality being the leading clue to both $e$ and the for statisticians famous quantity $\\sqrt{2 \\pi}$. There are also other Stirling connections to probability and statistics, including approximations for marginal distributions in Bayesian setups.\n\nThere are of course many different proofs in the literature, going back all the way to Stirling (1730) and de Moivre (1730), with ensuing scholarly articles to understand precisely how they arrived at their findings, what the differences were, the degree to which Stirling deserves the name without de Moivre, and so on. We choose to give a brief review of these historical themes in our penultimate separate Section 6, with further comments concerning associated issues touched on in our article, such as the connection from Wallis' 1656 product formula for $\\pi$ to Stirling 1730, the use of Laplace 1774 approximations for integrals, the Irwin-Hall distribution from 1927, among other topics from the history of $1 \\cdot 2 \\cdot 3 \\cdot 4, \\ldots$. Readers mainly interested in our various CLT and marginal distribution connections may then read on, through the main sections, without necessarily caring about the historical footnotes. We do point to one such here, however, namely Pearson (1924), since it directly pertains to the core questions discussed in our article. He noted, \"I consider that the fact that Stirling showed that de Moivre's arithmetical constant was $\\sqrt{2 \\pi}$ does not entitle him\n\nto claim the theorem\" (with various later scholars disagreeing with him in this regard; see indeed Section 6). The issue is both of a technical nature, \"where does $\\sqrt{2 \\pi}$ come from\", and a key theme regarding sorting out who found what, how, and when.\n\nWith the benefit of some extra 295 years of probability, we as statisticians of today might be slightly less surprised than was de Moivre, in 1729, having received a letter from Stirling; we're used to seeing $\\sqrt{2 \\pi}$ as part of our normal workload. There is indeed a certain literature relating Stirling to probability themes. Feller (1968, Ch. 5) reaches the Stirling formula in his classic book, via careful study of its logarithm, supplemented with limiting normality calculus for the binomial (and appears to miss that one of his exercises gives the Stirling in a simpler fashion; check with Section 6 again). Hu (1988) and Walsh (1995) have contributed arguments and proofs related to approximations from the CLT, specifically with the Poisson distribution. Some of these \"easy proofs\" in the literature have needed some further finishing polish to be fully accurate, however, as commented upon in Blyth \\& Pathak (1986). These authors also contribute a new proof, based on inversion of characteristic functions. Diaconis \\& Freedman (1986) give a neat proof of Stirling, via Laplace approximations of the gamma function, at the outset without a connection to probability, though they arrived at their proof via de Finetti theorems.\n\nIn Section 2 we explain the main idea tying the CLT to Stirling, with emphasis on the special cases of the Poisson, the gamma, the binomial distributions. Then in Section 3 we establish a further connection, with partly new formulae, via the Irwin-Hall distribution, for the sum of independent uniforms. Yet another connection, via the density of the uniform median, is worked with in Section 4. This is also seen to yield a proof for the 1656 Wallis product formula for $\\pi$. We go on to see how Stirling emerges in yet further ways, via Laplace and approximations to certain marginal distributions, in Section 5. As mentioned we then have a separate Section 6 with historical notes, before we offer a few complementary remarks in our final Section 7.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 2 The CLT connection \n\nTo set the stage for what shall give us new proofs and insights, also for other related formulae, consider in general terms i.i.d. variables $X_{i}$ with mean $\\xi$ and standard deviation $\\sigma$, and with partial sums $Y_{n}=\\sum_{i=1}^{n} X_{i}$. With ' $\\rightarrow_{d}$ ' denoting convergence in distribution, $Z_{n}=$ $\\left(Y_{n}-n \\xi\\right) /(\\sqrt{n} \\sigma) \\rightarrow_{d} Z$, a standard normal, and\n\n$$\nL_{n}(c)=\\mathrm{E} Z_{n} I\\left(0 \\leq Z_{n} \\leq c\\right) \\rightarrow L(c)=\\mathrm{E} Z I(0 \\leq Z \\leq c)\n$$\n\nwith this limit equal to\n\n$$\nL(c)=\\int_{0}^{c} z \\phi(z) \\mathrm{d} z=\\phi(0)-\\phi(c)=(2 \\pi)^{-1 / 2}\\left\\{1-\\exp \\left(-\\frac{1}{2} c^{2}\\right)\\right\\}\n$$\n\nIn particular, with $c=\\infty$, the mean of the zero-truncated $Z_{n}$ tends to $1 / \\sqrt{2 \\pi}$. The key ingredient securing moment convergence of $L_{n}(c)$ to $L(c)$, automatically valid for each CLT application, is uniform integrability, via $\\mathrm{E} Z_{n}^{2}=1$ and $\\mathrm{E} Z^{2}=1$; see Hjort \\& Stoltenberg (2025, Ch. 2).\n\nIn cases where we manage to have an interesting formula for $L_{n}(c)$, therefore, we have learned something. Here we work through special cases for the CLT, aiming for situations with clear and insightful $L_{n}(c) \\rightarrow L(c)$ formulae.\n\n2.1 Poisson. Consider first the Poisson case, where $Z_{n}=\\left(Y_{n}-n\\right) / \\sqrt{n} \\rightarrow_{d} Z$, for $Y_{n} \\sim \\operatorname{Pois}(n)$. With $p_{n}(j)=\\exp (-n) n^{j} / j!, j=0,1,2, \\ldots$ the Poisson probabilities, one finds that\n\n$$\n\\begin{aligned}\n\\sum_{j \\geq n}(j-n) p_{n}(j)=n p_{n}(n)-n p_{n}(n)+(n+1) p_{n}(n+1)-n p_{n}(n+1) \\\\\n\\quad+(n+2) p_{n}(n+2)-n p_{n}(n+2)+\\cdots\n\\end{aligned}\n$$\n\nis a telescoping series, in sum equal to $n p_{n}(n)$. This leads to\n\n$$\nL_{n}(\\infty)=\\sum_{j \\geq n} \\frac{j-n}{\\sqrt{n}} p_{n}(j)=\\sqrt{n} \\exp (-n) \\frac{n^{n}}{n!}\n$$\n\nwhich by (2.1) tends to $1 / \\sqrt{2 \\pi}$, proving the Stirling.\nAlso, for a finite $c$ in (2.1), the telescoping nature of the sum leads to\n\n$$\n\\begin{aligned}\nL_{n}(c)=\\sum_{n \\leq j \\leq n+c \\sqrt{n}} \\frac{j-n}{\\sqrt{n}} p_{n}(j) & =\\sqrt{n}\\left\\{p_{n}(j)-p_{n}(n+c \\sqrt{n})\\right\\} \\\\\n& =\\sqrt{n} \\exp (-n) \\frac{n^{n}}{n!}\\left\\{1-\\frac{n^{[c \\sqrt{n}]}}{(n+1) \\cdots([n+c \\sqrt{n}])}\\right\\}\n\\end{aligned}\n$$\n\nwith $[x]=\\max \\{z \\in \\mathbb{Z}: z \\leq x\\}$ the integer value of $x$. From (2.2), therefore, we infer that\n\n$$\n\\lim _{n \\rightarrow \\infty} \\frac{n^{[c \\sqrt{n}]}}{(n+1) \\cdots([n+c \\sqrt{n}])}=\\exp \\left(-\\frac{1}{2} c^{2}\\right)\n$$\n\nThis may be verified independently, via logarithms and inspection of $\\sum_{1 \\leq j \\leq c \\sqrt{n}} \\log (1+i / n)$; the present point is that we get the formula from probability theory and the CLT.\n2.2 Gammas. Let next $Y_{n} \\sim \\operatorname{Gamma}(n, 1)$, where again $Z_{n}=\\left(Y_{n}-n\\right) / \\sqrt{n} \\rightarrow_{d} Z$. Write $g_{n}(x)=\\Gamma(n)^{-1} x^{n-1} \\exp (-x)$ for the density, with $\\Gamma(z)=\\int_{0}^{\\infty} t^{z-1} \\exp (-t) \\mathrm{d} t$ the gamma function, and let $G_{n}(x)=1-\\exp (-x)\\left\\{1+x+\\cdots+x^{n-1} /(n-1)!\\right\\}$ be the cumulative. One sees that $x g_{n}(x)=n g_{n+1}(x)$, which leads to\n\n$$\nL_{n}(c)=\\int_{n}^{n+c \\sqrt{n}} \\frac{x-n}{\\sqrt{n}} g_{n}(x) \\mathrm{d} x=\\sqrt{n} \\int_{n}^{n+c \\sqrt{n}}\\left\\{g_{n+1}(x)-g_{n}(x)\\right\\} \\mathrm{d} x\n$$\n\nFirst, for $c=\\infty$, one finds from (2.1) and (2.2), i.e., the CLT plus the mean to mean argument, that\n\n$$\nL_{n}(\\infty)=\\sqrt{n} \\exp (-n) n^{n} / n!\\rightarrow \\sqrt{2 \\pi}\n$$\n\nwhich is precisely the Stirling formula (1.1), again. Working out the case for finite $c$, one finds the additional formula\n\n$$\n\\lim _{n \\rightarrow \\infty} \\exp (-c \\sqrt{n})(1+c / \\sqrt{n})^{n}=\\exp \\left(-\\frac{1}{2} c^{2}\\right)\n$$\n\nThis may again be verified independently, but here we get it for free from the CLT and the mean to mean argument (2.1).\n\nFor the gamma case we may also work directly with the density of $Z_{n}$, which becomes\n\n$$\n\\begin{aligned}\nh_{n}(z)=\\sqrt{n} g_{n}(n+\\sqrt{n} z) & =\\sqrt{n} \\Gamma(n)^{-1}(n+\\sqrt{n} z)^{n-1} \\exp (-n-\\sqrt{n} z) \\\\\n& =\\frac{\\sqrt{n}}{n!} n^{n} \\exp (-n)(1+z / \\sqrt{n})^{n-1} \\exp (-\\sqrt{n} z)\n\\end{aligned}\n$$\n\nAppealing to the Scheff\u00e9 lemma, for convergence of densities, we see that we have proven the Stirling in one more way, via the (2.3) formula just found.\n\nThe chi-squared distribution is merely a scale factor away from the gamma, so going through details for $Z_{n}=\\left(\\chi_{n}^{2}-n\\right) /(2 n)^{1 / 2}$ will essentially bring out the same: Stirling holds, via the CLT for these distributions.\n2.3 Binomial. The symmetric binomial distribution also lends itself nicely to working with the truncation and its mean. With $X_{1}, \\ldots, X_{n}$ i.i.d. Bernoulli, we have $Y_{n}=\\sum_{i=1}^{n} X_{i}$ a binomial $\\left(n, \\frac{1}{2}\\right)$, and of course $Z_{n}=\\left(Y_{n}-n \\frac{1}{2}\\right) /\\left(\\frac{1}{2} \\sqrt{n}\\right) \\rightarrow_{d} Z$. Write $b_{n}(j)=\\binom{n}{j}\\left(\\frac{1}{2}\\right)^{n}, j=$ $0,1, \\ldots, n$ for the binomial $\\left(n, \\frac{1}{2}\\right)$ probabilities. We need to work with $\\sum_{j \\geq n / 2}(j-n / 2) b_{n}(j)$. Note that\n\n$$\nj\\binom{n}{j}=n\\binom{n-1}{j-1} \\quad \\text { and } \\quad b_{n}(j)=\\frac{1}{2} b_{n-1}(j-1)+\\frac{1}{2} b_{n-1}(j)\n$$\n\nwhich leads to\n\n$$\n(j-n / 2) b_{n}(j)=\\frac{1}{2} n\\left\\{b_{n-1}(j-1)-\\frac{1}{2} b_{n}(j)\\right\\}=\\frac{1}{4} n\\left\\{b_{n-1}(j-1)-b_{n-1}(j)\\right\\}\n$$\n\nThe telescoping nature of these terms then gives us\n\n$$\n\\sum_{j \\geq n / 2}(j-n / 2) b_{n}(j)=\\frac{1}{4} n\\left\\{b_{n-1}\\left(\\frac{1}{2} n-1\\right)-\\left(\\frac{1}{2}\\right)^{n-1}\\right\\}\n$$\n\ntaking for simplicity $n$ even. Hence, before checking the details of the $b_{n-1}(j)$, we know from the CLT and the mean to mean argument that\n\n$$\nL_{n}(\\infty)=\\sum_{j \\geq n / 2} \\frac{j-n / 2}{\\frac{1}{2} \\sqrt{n}} b_{n}(j)=\\frac{1}{2} \\sqrt{n}\\left\\{b_{n-1}\\left(\\frac{1}{2} n-1\\right)-\\left(\\frac{1}{2}\\right)^{n-1}\\right\\} \\rightarrow 1 / \\sqrt{2 \\pi}\n$$\n\nThis translates to\n\n$$\n\\frac{1}{2} \\sqrt{n}\\binom{n-1}{n / 2-1}\\left(\\frac{1}{2}\\right)^{n-1} \\rightarrow 1 / \\sqrt{2 \\pi}, \\quad \\text { or } \\quad \\frac{1}{2}(n+1)^{1 / 2}\\binom{n}{(n-1) / 2}\\left(\\frac{1}{2}\\right)^{n} \\rightarrow 1 / \\sqrt{2 \\pi}\n$$\n\nthe latter valid for $n$ odd. But this is essentially the Stirling statement (1.1), again; in this regard, see the details of Section 4. For another relevant footnote, if one starts with $n!\\doteq a \\exp (-n) n^{n+1 / 2}$, as de Moivre did, around 1730, without yet knowing the constant $a$, one learns from the CLT implied details above that one indeed must have $a=\\sqrt{2 \\pi}$.\n\nWe learn more from the general setup above, with $L_{n}(c)$ of (2.1). With $j_{n}=\\left[\\frac{1}{2} n+\\frac{1}{2} c \\sqrt{n}\\right]$, and the telescoping nature of the terms in the sum above,\n\n$$\n\\begin{aligned}\n\\sum_{n / 2 \\leq j \\leq j_{n}} \\frac{j-n / 2}{\\frac{1}{2} \\sqrt{n}} b_{n}(j) & =\\frac{1}{2} \\sqrt{n}\\left\\{b_{n-1}\\left(\\frac{1}{2} n-1\\right)-b_{n-1}\\left(j_{n}\\right)\\right\\} \\\\\n& =\\frac{1}{2} \\sqrt{n} b_{n-1}\\left(\\frac{1}{2} n-1\\right)\\left\\{1-\\frac{b_{n-1}\\left(j_{n}\\right)}{b_{n-1}\\left(\\frac{1}{2} n-1\\right)}\\right\\}\n\\end{aligned}\n$$\n\nWe are hence also learning, in the process, that\n\n$$\n\\lim _{n \\rightarrow \\infty} \\frac{b_{n}\\left(\\left[\\frac{1}{2} n+\\frac{1}{2} c \\sqrt{n}\\right]\\right)}{b_{n}\\left(\\frac{1}{2} n\\right)}=\\lim _{n \\rightarrow \\infty}\\binom{n}{\\left[\\frac{1}{2} n+\\frac{1}{2} c \\sqrt{n}\\right]} /\\binom{n}{\\frac{1}{2} n}=\\exp \\left(-\\frac{1}{2} c^{2}\\right)\n$$", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 3 SumS OF UNIFORMS AND THE IRWIN-HALL DISTRIBUTION \n\nConsider the sum $Y_{n}=\\sum_{i=1}^{n} X_{i}$ of i.i.d. uniforms on $(0,1)$, these having mean $\\xi=\\frac{1}{2}$ and variance $\\sigma^{2}=1 / 12$. The probability density $f_{n}(y)$ of $Y_{n}$ is a somewhat awkward one, with different $(n-1)$-order polynomials over the intervals $[j-1, j]$ knotted together, for $j=1, \\ldots, n$, worked out in two independent articles in the same issue of Biometrika in 1927. What we may state, before coming to, or for that matter independently of the details of $f_{n}$ and its moments, is that with $I_{n}=\\int_{0}^{n / 2} y f_{n}(y) \\mathrm{d} y$ and $J_{n}=\\int_{n / 2}^{n} y f_{n}(y) \\mathrm{d} y$, we do have\n\n$$\n\\left(I_{n}-\\frac{1}{2} n \\xi\\right) /(\\sigma \\sqrt{n}) \\rightarrow-1 / \\sqrt{2 \\pi}, \\quad \\text { and } \\quad\\left(J_{n}-\\frac{1}{2} n \\xi\\right) /(\\sigma \\sqrt{n}) \\rightarrow 1 / \\sqrt{2 \\pi}\n$$\n\nTo see how this follows from (2.1) and (2.2), note that $Z_{n}=\\left(Y_{n}-n \\xi\\right) /(\\sigma \\sqrt{n}) \\rightarrow_{d} \\mathrm{~N}(0,1)$, and $I_{n}+J_{n}=n \\xi$, so\n\n$$\n\\begin{aligned}\n0 & =\\left(I_{n}-\\frac{1}{2} n \\xi\\right)+\\left(J_{n}-\\frac{1}{2} n \\xi\\right) \\\\\n& =\\mathrm{E}\\left(Y_{n}-n \\xi\\right) I\\left\\{Y_{n} \\leq \\frac{1}{2} n\\right\\}+\\mathrm{E}\\left(Y_{n}-n \\xi\\right) I\\left\\{\\frac{1}{2} n \\leq Y_{n} \\leq n\\right\\} \\\\\n& =\\sqrt{n} \\sigma \\mathrm{E} Z_{n} I\\left\\{-\\frac{1}{2} \\sqrt{n} / \\sigma \\leq Z_{n} \\leq 0\\right\\}+\\sqrt{n} \\sigma \\mathrm{E} Z_{n} I\\left\\{0 \\leq Z_{n} \\leq \\frac{1}{2} \\sqrt{n} / \\sigma\\right\\}\n\\end{aligned}\n$$\n\nThe only thing differing between the limits of $\\left(I_{n}-\\frac{1}{2} n \\xi\\right) /(\\sqrt{n} \\sigma)$ and $\\left(J_{n}-\\frac{1}{2} n \\xi\\right) /(\\sqrt{n} \\sigma)$, therefore, are their signs, but $\\left|Z_{n}\\right| \\leq \\sqrt{n} /(2 \\sigma)$, so by (2.2),\n\n$$\n\\mathrm{E} Z_{n} I\\left\\{0 \\leq Z_{n} \\leq \\frac{1}{2} \\sqrt{n} / \\sigma\\right\\}=L_{n}(\\infty) \\rightarrow 1 / \\sqrt{2 \\pi}\n$$\n\nThe perhaps gently misnomed Irwin-Hall distribution is triangular for $n=2$, then a mix of 2nd order polymomials for $n=3$, and becomes steadily smoother and of course more and more normal looking for $n=4,5, \\ldots$, via piecewise polynomials. Formulae for the density and cumulative of the Irwin-Hall distribution on $[0, n]$ can be found in the literature, e.g. in these forms,\n\n$$\nf_{n}(y)=\\frac{1}{(n-1)!} \\sum_{j=0}^{|y|}(-1)^{j}\\binom{n}{j}(y-j)^{n-1} \\quad \\text { and } \\quad F_{n}(y)=\\frac{1}{n!} \\sum_{j=0}^{|y|}(-1)^{j}\\binom{n}{j}(y-j)^{n}\n$$\n\nProofs given are via somewhat cumbersome induction, or inversion of generating functions, but see Marengo, Farnsworth \\& Stefanic (2017) for a different geometric type of argument. It is nice now to have an occasion and a reason for finding a new formula for a key quantity, namely the truncated moment, for a more than hundred years old distribution.\n\nLemma 1. For the Irwin-Hall density $f_{n}$, with $n$ even, we have\n\n$$\nI_{n}=\\int_{0}^{n / 2} y f_{n}(y) \\mathrm{d} y=\\frac{1}{4} n-\\frac{n^{n+1}}{(n+1)!}\\left(\\frac{1}{2}\\right)^{n+1} \\sum_{j=0}^{n / 2}(-1)^{j}\\binom{n}{j}(1-2 j / n)^{n+1}\n$$\n\nProof. We note first, via partial integration, that\n\n$$\n\\int_{0}^{a}\\left(1-F_{n}\\right) \\mathrm{d} y=a\\left\\{1-F_{n}(a)\\right\\}+\\int_{0}^{a} y f_{n}(y) \\mathrm{d} y \\quad \\text { for any } a\n$$\n\nWith $s_{n}=\\int_{0}^{n / 2} F_{n} \\mathrm{~d} y$, therefore, we have $I_{n}=\\int_{0}^{n / 2}\\left(1-F_{n}\\right) \\mathrm{d} y-\\frac{1}{4} n=\\frac{1}{4} n-s_{n}$. For the\n\nlatter we have\n\n$$\n\\begin{aligned}\ns_{n} & =\\int_{0}^{n / 2} \\frac{1}{n!} \\sum_{j=0}^{|y|}(-1)^{j}\\binom{n}{j}(y-j)^{n} \\mathrm{~d} y \\\\\n& =\\frac{1}{n!} \\sum_{k=0}^{n / 2-1} \\int_{k}^{k+1} \\sum_{j=0}^{|y|}(-1)^{j}\\binom{n}{j}(y-j)^{n} \\mathrm{~d} y \\\\\n& =\\frac{1}{n!} \\sum_{k=0}^{n / 2-1} \\sum_{j=0}^{k}(-1)^{j}\\binom{n}{j} \\int_{k}^{k+1}(y-j)^{n} \\mathrm{~d} y \\\\\n& =\\frac{1}{n!} \\sum_{k=0}^{n / 2-1} \\sum_{j=0}^{k}(-1)^{j}\\binom{n}{j} \\frac{1}{n+1}\\left\\{(k+1-j)^{n+1}-(k-j)^{n+1}\\right\\} \\\\\n& =\\frac{1}{(n+1)!} \\sum_{j=0}^{n / 2-1}(-1)^{j}\\binom{n}{j} \\sum_{k=j}^{n / 2-1}\\left\\{(k+1-j)^{n+1}-(k-j)^{n+1}\\right\\}\n\\end{aligned}\n$$\n\nThis may also be expressed as\n\n$$\n\\begin{aligned}\ns_{n}= & \\frac{1}{(n+1)!}\\left[\\left\\{1^{n+1}-0^{n+1}+\\cdots+(n / 2)^{n+1}-(n / 2-1)^{n+1}\\right\\}\\right. \\\\\n& \\left.-\\binom{n}{1}\\left\\{1^{n+1}-0^{n+1}+\\cdots+(n / 2-1)^{n+1}-(n / 2-2)^{n+1}\\right\\}\\right. \\\\\n& \\left.+\\binom{n}{2}\\left\\{1^{n+1}-0^{n+1}+\\cdots+(n / 2-2)^{n+1}-(n / 2-3)^{n+1}\\right\\}+\\cdots\\right] \\\\\n= & \\frac{1}{(n+1)!}\\left[(n / 2)^{n+1}-\\binom{n}{1}(n / 2-1)^{n+1}+\\binom{n}{2}(n / 2-2)^{n+1}-\\cdots\\right] \\\\\n= & \\frac{1}{(n+1)!} \\sum_{j=0}^{n / 2}(-1)^{j}\\binom{n}{j}(n / 2-j)^{n+1}\n\\end{aligned}\n$$\n\nproving the lemma.\nVia these unchartered pathways in the terrain of truncated moments for Irwin-Hall we have reached explicit expressions for $s_{n}$ and $I_{n}=\\frac{1}{4} n-s_{n}$. The statements (3.1) are seen to be equivalent to\n\n$$\n\\frac{s_{n}}{\\sqrt{n}}=\\frac{4}{3} \\frac{n^{n+1 / 2}}{(n+1)!} \\sum_{j=0}^{n / 2}(-1)^{j}\\binom{n}{j}\\left(\\frac{1}{2}\\right)^{n}(1-2 j / n)^{n+1} \\rightarrow \\sigma / \\sqrt{2 \\pi}\n$$\n\nFrom Stirling we have\n\n$$\n\\frac{n^{n+1 / 2}}{(n+1)!} \\doteq \\frac{n^{n+1 / 2}}{(n+1)^{n+3 / 2} \\exp (-(n+1)) \\sqrt{2 \\pi}}=\\frac{\\exp (n)}{n+1}\\left(\\frac{n}{n+1}\\right)^{n+1 / 2} \\frac{e}{\\sqrt{2 \\pi}}\n$$\n\nin the sense that the ratio between the two sides converges to 1 . This invites examining the product form $s_{n} / \\sqrt{n}=a_{n} b_{n}$, with\n\n$$\na_{n}=\\frac{n+1}{\\exp (n)} \\frac{n^{n+1 / 2}}{(n+1)!}, \\quad b_{n}=\\frac{1}{2} \\frac{\\exp (n)}{n+1} \\sum_{j=0}^{n / 2}(-1)^{j}\\binom{n}{j}\\left(\\frac{1}{2}\\right)^{n}(1-2 j / n)^{n+1}\n$$\n\nHere $a_{n} \\rightarrow 1 / \\sqrt{2 \\pi}$, and by mathematical necessity the complicated looking $b_{n}$ needs to tend to $\\sigma$. So, in a rather roundabout fashion, via the CLT and moment convergence, we\n\nhave managed to prove not merely the Stirling formula, once again, but also the impressive looking\n\n$$\n\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\exp (n) \\sum_{j=0}^{n / 2}(-1)^{j}\\binom{n}{j}\\left(\\frac{1}{2}\\right)^{n}(1-2 j / n)^{n+1}=1 / \\sqrt{3}\n$$", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 4 Stirling and Wallis from the median of a uniform sample \n\nHere we shall prove the Stirling approximation formula (1.1) once more, and as a by-product also the famous Wallis 1656 product formula for $\\pi$, starting again with a simple i.i.d. uniform sample $U_{1}, \\ldots, U_{n}$.\n4.1 Stirling, again. In Section 3 we worked with the sum of the uniforms, but now our attention is on their median, say $M_{n}$. With $n=2 m+1$ odd, for simplicity, an easy and well-known argument gives its density as\n\n$$\ng_{n}(x)=\\frac{(2 m+1)!}{m!m!} x^{m}(1-x)^{m} \\quad \\text { for } x \\in[0,1]\n$$\n\nwhich is a Beta $(m+1, m+1)$. Via a well-known representation for the Beta, in terms of a ratio of Gammas, we may write $M_{n}=U_{m+1} /\\left(U_{m+1}+V_{m+1}\\right)$, where $U_{m+1}=A_{1}+\\cdots+$ $A_{m+1}$ and $V_{m+1}=B_{1}+\\cdots+B_{m+1}$, say, involving i.i.d. unit exponentials. It then follows straightforwardly and instructively that\n\n$$\n(m+1)\\left\\{U_{m+1} /(m+1)-1\\right\\} \\rightarrow_{d} Z_{1}, \\quad(m+1)\\left\\{V_{m+1} /(m+1)-1\\right\\} \\rightarrow_{d} Z_{2}\n$$\n\nwith these limits being independent standard normals, and that $Z_{n}=2 \\sqrt{n}\\left(M_{n}-\\frac{1}{2}\\right) \\rightarrow_{d}$ $Z=\\left(Z_{1}-Z_{2}\\right) / \\sqrt{2}$, a standard normal, via an application of the delta method. The two limiting standard normals $Z_{1}$ and $Z_{2}$ featured here are incidentally just as in Section 2.2, but here they play a different role, just to establish that $Z_{n} \\rightarrow_{d} Z$.\n\nWe learn more by working with the density of $Z_{n}$, which can be written\n\n$$\n\\begin{aligned}\n\\frac{1}{2 \\sqrt{n}} g_{n}\\left(\\frac{1}{2}+\\frac{z}{2 \\sqrt{n}}\\right) & =\\frac{1}{2 \\sqrt{n}} \\frac{(2 m+1)!}{m!m!}\\left(\\left(\\frac{1}{2}+\\frac{z}{2 \\sqrt{n}}\\right)\\left(\\frac{1}{2}-\\frac{z}{2 \\sqrt{n}}\\right)\\right)^{m} \\\\\n& =\\frac{1}{2} \\sqrt{n} \\frac{(2 m)!}{m!m!}\\left(1-\\frac{z^{2}}{n}\\right)^{m}\\left(\\frac{1}{2}\\right)^{2 m}\n\\end{aligned}\n$$\n\nSince $\\left(1-z^{2} / n\\right)^{m} \\rightarrow \\exp \\left(-\\frac{1}{2} z^{2}\\right)$, and we already know that $Z_{n} \\rightarrow_{d} Z$, we must by necessity have\n\n$$\nc_{n}=\\frac{1}{2}(2 n+1)^{1 / 2}\\binom{2 n}{n}\\left(\\frac{1}{2}\\right)^{2 n} \\rightarrow c=1 / \\sqrt{2 \\pi}\n$$\n\nThis is actually related to what de Moivre (1730) was working on (without finding the $\\sqrt{2 \\pi}$ connection), good approximations to the middle term of the binomial expansion of $(1+1)^{2 n}$. With $Y_{2 n}$ a binomial $\\left(2 n, \\frac{1}{2}\\right)$, we may translate the above to the middle binomial probability\n\n$$\nP\\left(Y_{2 n}=n\\right)=b_{2 n}\\left(n, \\frac{1}{2}\\right)=\\binom{2 n}{n}\\left(\\frac{1}{2}\\right)^{2 n} \\doteq \\frac{1}{\\sqrt{\\pi n}}\n$$\n\nassociated also with the recurrent nature of symmetric random walks; see Remark C in Section 7.\n\nBy inserting for $n$ !, Stirling's formula implies the limit in (4.1), i.e., that $c_{n} \\rightarrow 1 / \\sqrt{2 \\pi}$; but the implication also goes the other way around. To see this, recall that by the trapezoidal approximation for integrals,\n\n$$\n\\log n!-\\frac{1}{2} \\log n=\\int_{0}^{n} \\log x \\mathrm{~d} x+O(1 / n)=n \\log n-n+1+O(1 / n)\n$$\n\nas $n$ tends to infinity. Since $(2 n+1) / 2 n \\rightarrow 1$, clearly $c_{n} \\doteq \\frac{1}{2}(2 n)^{1 / 2}\\left\\{(2 n)!/(n!)^{2}\\right\\}\\left(\\frac{1}{2}\\right)^{2 n}$, which combined with (4.1) entails that\n\n$$\n\\frac{\\sqrt{2 \\pi n}\\{(2 n)!/ n!\\} 2^{-2 n-1 / 2}}{n!} \\rightarrow 1\n$$\n\nIf we take the logarithm of $\\{(2 n)!/ n!\\} 2^{-2 n-1 / 2}$ and use the trapezoidal rule,\n\n$$\n\\begin{aligned}\n\\log \\left(\\{(2 n)!/ n!\\} 2^{-2 n-1 / 2}\\right) & =\\log ((2 n)!)-\\log (n!)-2 n \\log 2-\\frac{1}{2} \\log 2 \\\\\n& =\\left\\{\\log ((2 n)!)-\\frac{1}{2} \\log (2 n)\\right\\}-\\left\\{\\log (n!)-\\frac{1}{2} \\log (n)\\right\\}-2 n \\log 2 \\\\\n& =\\{2 n \\log (2 n)-2 n+1\\}-\\{n \\log n-n+1\\}-2 n \\log 2+O(1 / n) \\\\\n& =n \\log n-n+O(1 / n)\n\\end{aligned}\n$$\n\nwhich shows that $\\{(2 n)!/ n!\\} 2^{-2 n-1 / 2} \\doteq n^{n} \\exp (-n)$, once again proving the Stirling's formula. This proof is, admittedly, very close to the standard proof of Stirling, the point is, yet again, that we get $\\sqrt{2 \\pi}$ for free from the CLT.\n4.2 The Wallis product. An intriguing formula, from Wallis (1656), says that\n\n$$\n\\frac{\\pi}{2}=\\frac{2}{1} \\cdot \\frac{2}{3} \\cdot \\frac{4}{3} \\cdot \\frac{4}{5} \\cdot \\frac{6}{5} \\cdot \\frac{6}{7} \\cdot \\frac{8}{7} \\cdot \\frac{8}{9} \\cdots=\\prod_{j=1}^{\\infty} \\frac{2 j}{2 j-1} \\frac{2 j}{2 j+1}\n$$\n\nHere we derive this via our Stirling efforts. For the product up to $n$, write\n\n$$\nw_{n}=\\prod_{j=1}^{n} \\frac{2 j}{2 j-1} \\frac{2 j}{2 j+1}=\\frac{1}{2 n+1} \\prod_{j=1}^{n} \\frac{(2 j)^{4}}{\\{(2 j)(2 j-1)\\}^{2}}=\\frac{1}{2 n+1} 2^{4 n} /\\binom{2 n}{n}^{2}\n$$\n\nWe recognise its square root, from (4.2), and have\n\n$$\n1 / \\sqrt{w_{n}}=(2 n+1)^{1 / 2}\\binom{2 n}{n}\\left(\\frac{1}{2}\\right)^{2 n} \\rightarrow(2 / \\pi)^{1 / 2}\n$$\n\nproving (4.4). Enthrallingly, we have explained via understanding the behaviour of the median in uniform samples that the Wallis formula must be true.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 5 Stirling via Laplace approximations \n\nSuppose $g(x)$ is a smooth function with a clear maximum $g_{\\max }=g\\left(x_{0}\\right)$ at position $x_{0}$, and write $c=-g^{\\prime \\prime}\\left(x_{0}\\right)$. Then the Laplace approximation is\n\n$$\n\\int \\exp (g) \\mathrm{d} x \\doteq \\int \\exp \\left\\{g\\left(x_{0}\\right)-\\frac{1}{2} c\\left(x-x_{0}\\right)^{2}\\right\\} \\mathrm{d} x=\\exp \\left(g_{\\max }\\right) \\sqrt{2 \\pi / c}\n$$\n\nThe $\\doteq$ is to be read as 'approximately equal to' in the sense that the ratio between the left and right hand sides tends to 1 provided the steepness of the maximum, i.e. the size of $c$, increases. Note that the $\\sqrt{2 \\pi}$ factor comes from the familiar normal integral, not\n\nstemming from probability, per se, but from the natural Taylor expansion approximation in the exponent of the function being integrated.\n\nNow try this with the function $g(x)=n \\log x-x$, for $x$ positive. It has $g^{\\prime}(x)=n / x-1$ and $g^{\\prime \\prime}(x)=-n / x^{2}$; hence $x_{0}=n, g_{\\max }=n \\log n-n$, and $c=1 / n$. We find\n\n$$\n\\int_{0}^{\\infty} x^{n} \\exp (-x) \\mathrm{d} x=n!\\doteq n^{n} \\exp (-n) \\sqrt{2 \\pi n}\n$$\n\ni.e. Stirling, once again. This is essentially what Diaconis \\& Freedman (1986) do, but presented somewhat differently. The argument may be made rigorous by letting $n$ increase and assess the error made in the Laplace approximation (5.1).\n\nThe Laplace argument is known in model selection statistics for being an ingredient in the BIC, the Bayesian Information Criterion, see Claeskens \\& Hjort (2008, Ch. 3). One version of the essential computation there is as follows. Consider data from a one-dimensional parametric model $f(x, \\theta)$, of sample size $n$, leading to the likelihood function $L_{n}(\\theta)$, say, with ensuing log-likelihood function $\\ell_{n}(\\theta)$. Letting $\\widehat{\\theta}$ be the maximum likelihood estimator, with $L_{n, \\max }=L_{n}(\\widehat{\\theta})$ and observed Fisher information $J_{n}=-\\ell_{n}^{\\prime \\prime}(\\widehat{\\theta})$, we may integrate over the parameter region to find\n\n$$\n\\int L_{n}(\\theta) \\mathrm{d} \\theta \\doteq L_{n, \\max }\\left(2 \\pi / J_{n}\\right)^{1 / 2}\n$$\n\nThe left hand side is recognised as being the marginal distribution for the data, in the Bayesian setup with a flat prior for the parameter.\n\nIt is now instructive to see how this approximation pans out for a few models. Consider first the case of a single $X \\sim \\operatorname{Pois}(\\theta)$, with likelihood $\\exp (-\\theta) \\theta^{x} / x$ ! maximised for $\\widehat{\\theta}=x$. The integral in (5.2) is simply 1 , and the $L_{n, \\max }$ is $\\exp (-x) x^{x} / x$ !. The BIC approximation by almost magic delivers Stirling:\n\n$$\n1 \\doteq\\left\\{\\exp (-x) x^{x} / x!\\right\\}(2 \\pi x)^{1 / 2}\n$$\n\nExamination of the error involved in the basic approximation (5.1) shows how this simple Poisson argument may be made rigorous, by letting $x$ increase. We may also work through the case of $X_{1}, \\ldots, X_{n}$ i.i.d. from the Poisson. Writing $Z=\\sum_{i=1}^{n} X_{i}$, the likelihood is proportional to $\\exp (-n \\theta) \\theta^{z}$. The maximum likelihood estimator is $\\widehat{\\theta}=z / n$, the sample average. The BIC approximation (5.2) is seen to become\n\n$$\n\\frac{z!}{n^{z+1}} \\doteq \\exp (-z)\\left(\\frac{z}{n}\\right)^{z} \\sqrt{2 \\pi} \\frac{z^{1 / 2}}{n}\n$$\n\ni.e. Stirling, again, now expressed in terms of $z$.\n\nNext consider $X_{1}, \\ldots, X_{n}$ i.i.d. from the exponential model $\\theta \\exp (-\\theta x)$ for $x$ positive. The likelihood is $\\theta^{n} \\exp (-z \\theta)$, with $z=\\sum_{i=1}^{n} x_{i}$, leading to the maximum likelihood estimator $\\widehat{\\theta}=n / z$, the inverse of the sample average. The BIC approximation formula is seen to lead to Stirling again, lo $\\ell^{j}$ behold:\n\n$$\n\\frac{n!}{z^{n+1}} \\doteq\\left(\\frac{n}{z}\\right)^{n} \\exp (-n) \\sqrt{2 \\pi} \\frac{\\sqrt{n}}{z}\n$$\n\nWe allow ourselves including also the binomial model here. With $X$ a binomial $(n, p)$, and with maximum likelihood estimator $\\widehat{p}=x / n$, the BIC approximation yields\n\n$$\n\\frac{x!(n-x)!}{(n+1)!} \\doteq\\left(\\frac{x}{n}\\right)^{x}\\left(\\frac{n-x}{n}\\right)^{n-x}\\left(\\frac{x}{n} \\frac{n-x}{n}\\right)\\right)^{1 / 2} \\frac{\\sqrt{2 \\pi}}{\\sqrt{n}}\n$$\n\nInstructively, this is seen to break down into Stirling for $x$ !, Stirling for $(n-x)$ !, and Stirling for $(n+1)$ !, in partes tre!", "tables": {}, "images": {}}, {"section_id": 6, "text": "# 6 Notes on the historical background for different theMES \n\nIn our introduction section we briefly pointed to Pearson (1924), who ventured that Stirling did not quite deserve to have his and only his name attached to the famous $n$ ! formula, his point being that de Moivre (1730) was essentially onto the same formula, but without the $\\sqrt{2 \\pi}$ constant. Other scholars have disagreed with Pearson in this particular regard; see the detailed comments in Tweddie (1922), Tweddle (1984) (the latter writing from the University of Stirling), LeCam (1986, Section 3), Dutka (1991), G\u00e9linas (2017). \"Stirling greatly surprised de Moivre by the introduction of $\\pi$ into the calculation of the ratio of the coefficient of the middle term in $(1+x)^{n}$ to the sum of all coefficients\", i.e. of $\\binom{n}{n / 2}\\left(\\frac{1}{2}\\right)^{n}$, writes Cajori (1923), reviewing the Tweddie (1922) book. When going through the historical details associated with the $\\log n$ ! formula, from (1.1), complete with an error term of the right order $O(1 / n)$, G\u00e9linas (2017) argues, \"Likewise indeed, James Stirling must keep his indisputable priority for the discovery, proof, and publication of Stirling's series [the formula for $\\log n!$ ] including of course the explicit constant $\\log \\sqrt{2 \\pi}$.\"\n\nIn Section 3 we found new uses for the Irwin-Hall distribution, so named due to two papers published in the same Biometrika 1927 volume, Irwin (1927) and Hall (1927). E.S. Pearson put in an informative editorial note inside the latter article (pp. 243-244): \"Proceeding by completely different methods, Irwin and Hall have obained equations for the frequency distribution of means in samples from a population following a 'rectangular' law of distribution [...]. Their results are of considerable interest, but for the light they throw upon the distribution of moments in samples from populations in which the variable lies within a finite or limited range.\" As is clear from careful historical notes in e.g. Sheynin (1973) and Marengo, Farnsworth \\& Stefanic (2017), however, the distribution is in reality far older, going back to Lagrange and Laplace in the latter 18th and the early 19th century, associated also with early uses of generating functions.\n\nIn Section 4.1 we derived Stirling's formula by combining the CLT for the median of uniforms and the trapezoidal rule (see Eq. 4.3). Intriguingly, the trapezodial rule dates back at least to a century b.C., when Babylonian astronomers used it in tracking Jupiter, see Ossendrijver (2016).\n\nIn Section 4 we discussed the Wallis 1656 product formula, and its relation to both Stirling and the de Moivre binomial middle probability approximation. Wallis' crucial role in developing infinitesimals, including boldly inventing the symbols $\\infty$ and $\\frac{1}{\\infty}$, in an intellectual era where thinking along such lines was partly considered precarious and even heretic, is vividly described in Alexander (2014, Chs. 8, 9) - how a dangerous mathematical theory shaped the modern world, no less. There were several intellectual heavy-weights among his antagonists, from Thomas Hobbes to the politically powerful Jesuite school.\n\nIn our Section 5, with various Stirling proofs based on Laplace approximations to integrated likelihoods, we pointed to Diaconis \\& Freedman (1986), whose argument essentially matches the first of our proofs in the section mentioned. The Diaconis \\& Freedman (1986) argument is at the outset a mathematical one, not related to probability or statistics per se. It is incidentally the Laplace approximation itself which pushes the analysis towards the $\\sqrt{2 \\pi}$ constant, via the classical Gaussian integral. They explain however how they \"stumbled on\" their proof as a by-product of developing theory for finite forms of de Finetti's theorem for exponential families; see Diaconis \\& Freedman (1980). In the spirit pointing to crucial achievements in the distant past we ought also to point to Laplace (1774), the origin\n\nof the integration approximation technique carrying his name.\nThe treatment of Stirling in Feller (1968) deserves a footnote, since the book is a classic, carefully studied in previous generations. He arrives at the Stirling approximation in several steps, first examining the logarithm (page 52) and then quite a bit later using the normal approximation to the binomial (page 180) to infer that the constant involved must be $\\sqrt{2 \\pi}$; for some of the technicalities he also credits Robins (1955). Interestingly, he tosses out Exercise 22, page 66, concerning Laplace approximation of the gamma function - seemingly not noticing that this is in fact the Stirling formula!, and, incidentally, close to the main argument used in Diaconis \\& Freedman (1986).", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 7 CONCLUDING REMARKS \n\nIn our article we have demonstrated that the classic Stirling (1730) approximation formula (1.1), the middle binomial probability formula (4.2) associated with de Moivre (1730) (though he did not know the right constant), and the Wallis (1656) product formula (4.4), are essentially equivalent; starting with any one of these historically important achievements, one may deduce the others. We round off our article by offering a few concluding remarks.\n\nRemark A. Various classical and not too hard calculations for teaching statistics concern the mean squared error when estimating the binomial $p$, based on $X \\sim \\operatorname{Bin}(n, p)$. It is of course $\\mathrm{E}(X / n-p)^{2}=p(1-p) / n$. But in a Teacher's Corner contribution, Blyth (1980) asks about the \"somewhat more natural measure\" $D_{n}(p)=\\mathrm{E}_{p}|X / n-p|$, i.e. using absolute loss. As we know this is less easy mathematically, and it takes some stamina, here Blyth actually uses results from Frisch (1924), to establish exact formulae for $d_{n}$, the maximum risk over $p$. Correcting a little mistake in his formulae, we find\n\n$$\nd_{n}=\\sqrt{n} \\max _{a \\mid p} D_{n}(p)=\\sqrt{n}\\binom{n-1}{\\frac{1}{2} n-1}\\left(\\frac{1}{2}\\right)^{n}\n$$\n\nfor even $n$. Blyth goes on, via the Stirling formula, to show that $d_{n} \\rightarrow d=\\frac{1}{2}(2 / \\pi)^{1 / 2}=$ $1 / \\sqrt{2 \\pi}$, explicitly using the Stirling formula in the process. But we may also utilise the basic reasoning of Section 2 to argue that we know that $\\sqrt{n} \\mathrm{E}\\left|X_{n} / n-p\\right|$ must tend to $(2 / \\pi)^{1 / 2}\\{p(1-p)\\}^{1 / 2}$, via the CLT for binomials, and hence deduce from $d_{n} \\rightarrow d$ that, once more, Stirling holds.\n\nRemark B. Consider a symmetric random walk, with steps to the right and the left with equal probabilities, starting at zero. The position at time $n$ is $Y_{n}=R_{n}-L_{n}=2 R_{n}-n$, with $R_{n}$ a binomial $\\left(n, \\frac{1}{2}\\right)$. The chance of revisiting zero after $2 n$ steps is\n\n$$\np_{n}=P\\left(Y_{2 n}=0\\right)=\\binom{2 n}{n}\\left(\\frac{1}{2}\\right)^{2 n}\n$$\n\nwhich we have met and worked with in Section 4.1. de Moivre could prove the approximation $c / \\sqrt{n}$, but without finding the right $c$ constant. The point now is to derive the Stirling related approximtaion $1 /(\\pi \\sqrt{n})$ again, in yet another way. Let $Z_{2 n}=2(2 n)^{1 / 2}\\left\\{R_{2 n} /(2 n)-\\frac{1}{2}\\right\\}$, which tends to the standard normal. We have\n\n$$\np_{n}=P\\left(\\left|Y_{2 n}\\right|<\\frac{1}{2}\\right)=P\\left(\\left|Z_{2 n}\\right|<1 /(2 n)^{1 / 2}\\right) \\doteq 2 \\phi(0) 1 /(2 n)^{1 / 2}=1 /(\\pi \\sqrt{n})\n$$\n\nonce more with ' $\\doteq$ ' in the precise sense that the ratio between the two sides tends to 1 . Rounding off this remark, we mention that this particular approximation, in the random\n\nwalk context, can be used to approximate the number of visits $\\mathrm{E} N_{2 n}$ to zero, in the course of time steps $1, \\ldots, 2 n$, as $(1 / \\sqrt{\\pi}) \\sum_{i=1}^{n} 1 / \\sqrt{i} \\doteq(1 / \\sqrt{\\pi}) \\frac{1}{2} \\sqrt{n}$. The point here is also that this number indeed tends to infinity, related of course to the recurrency property of the symmetric random walk; each state will be revisited infinitely many times, with probability 1.\n\nRemark C. As in Section 2, consider $Y_{n}$ a Poisson with parameter $n$. Via results worked out in Crow (1958), one may deduce a perhaps surprisingly simple expression for the mean absolute deviation, $\\mathrm{E}\\left|Y_{n}-n\\right|=2 \\exp (-n) n^{n+1} / n$ !. Via the CLT and uniform integrability, we must have $\\mathrm{E}\\left|Y_{n}-n\\right| / \\sqrt{n} \\rightarrow(2 / \\pi)^{1 / 2}$, the absolute mean of a standard normal. We have rediscovered the Stirling, for the $(n+1)$ st time in this article.", "tables": {}, "images": {}}, {"section_id": 8, "text": "# REFERENCES \n\nAlexander, A. (2014). Infinitesimal: How a Dangerous Mathematical Theory Shaped the Modern World. London: Oneworld Publication.\n\nBlyth, C. R. (1980). Expected absolute error of the usual estimator of the binomial parameter. The American Statistician, 155-157.\n\nBlyth, C. R. \\& Pathak, P. K. (1986). A note on easy proofs of Stirling's theorem. The American Mathematical Monthly, 376-379.\n\nCAJORI, F. (1923). Book review: James Stirling. A sketch of his life and works, along with his scientific correspondence, by Charles Tweedie. Isis 5, 429-432.\n\nClaeskens, G. \\& Hjort, N. L. (2008). Model Selection and Model Averaging. Cambridge: Cambridge University Press.\n\nCrow, E. L. (1958). The mean deviation of the Poisson distribution. Biometrika 45, $556-559$.\n\nDE Moivre, A. (1730). Miscellanea Analytica de Seriebus et Quadraturis. London. Followed by Miscellaneis Analyticus Supplementum.\n\nDiaconis, P. \\& Freedman, D. (1980). Finite exchangeable sequences. Annals of Probability 8, 745-764.\n\nDiaconis, P. \\& Freedman, D. (1986). An elementary proof of Stirling's formula. The American Mathematical Monthly 93, 123-125.\n\nDutka, J. (1991). The early history of the factorial function. Archive for History of Exact Sciences 43, 225-249.\n\nFeller, W. (1968). An Introduction to Probability Theory and Its Application, Vol. 1, 3rd ed. New York: Wiley.\n\nFrisch, R. (1924). Solution d'un probleme du calcul des probabilites. Skandinavisk Aktuarietidskrift 7, 153-174.\n\nG\u00e9linas, J. (2017). Original proofs of Stirling's series for log (n!). arXiv preprint arXiv:1701.06689 .\n\nHall, P. (1927). The distribution of means for samples of size n drawn from a population in which the variate takes values between 0 and 1 , all such values being equally probable. Biometrika 19, 240-245.\n\nHjort, N. L. \\& Stoltenberg, E. A. (2025). Statistical Inference: 777 Exercises and 77 Stories. Cambridge: Cambridge University Press.\n\nHu, C. (1988). A statistical method of approach to Stirling's formula. American Statistician 42, 204-205.\n\nIrwin, J. O. (1927). On the frequency distribution of the means of samples from a population having any law of frequency with finite moments, with special reference to pearson's type ii. Biometrika 19, 225-239.\n\nLaplace, P. S. (1774). Memoire sur la probabilite des causes par les \u00e9v\u00e9nements. Memoires de math\u00e9matique et de Physique Presentes a I'Academie Royale des Sciences, par Divers Savants, \\& Lus dans ses Assembl\u00e9es. Paris: xx. English translation by S. Stigler, Technical Report \\#164, Department of Statistics, University of Chicago, September, 1984.\n\nLeCam, L. (1986). The central limit theorem around 1935. Statistical Science 1, 78-96.\nMarengo, J. E., Farnsworth, D. L. \\& Stefanic, L. (2017). A geometric derivation of the Irwin-Hall distribution. Hindawi International Journal of Mathematics and Mathematical Sciences xx, 1-6.\n\nOssendHjver, M. (2016). Ancient Babylonian astronomers calculated Jupiter's position from the area under a time-velocity graph. Science 351, 482-484.\n\nPearson, K. (1924). Historical note on the origin of the normal curve of errors. Biometrika 14, 402-404.\n\nRobins, H. (1955). A remark on Stirling's formula. The American Mathematical Monthly 62, 26-29.\n\nSheynin, O. B. (1973). Finite random sums (a historical essay). Archive for History of Exact Sciences 9, 275-305.\n\nStirling, J. (1730). Methodus differentialis: sive tractatus de summatione et interpolatione serierum infinitarum. London: Londini: Typis Gul. Bowyer; impensis G. Strahan.\n\nTweddie, J. (1922). James Stirling. A Sketch of His Life and Works, along with His Scientific Correspondence. Oxford: Clarendon Press.\n\nTweddle, I. (1984). Approximating n!, historical origins and error analysis. American Journal of Physics 52, 487-488.\n\nWallis, J. (1656). Arithmetica Infinitorum. London: xx.\nWALSH, D. P. (1995). Equating Poisson and normal probability functions to derive Stirling's formula. American Statistician 49, 204-205.", "tables": {}, "images": {}}], "id": "2410.19555v1", "authors": ["Nils Lid Hjort", "Emil Aas Stoltenberg"], "categories": ["math.PR", "math.ST", "stat.OT", "stat.TH"], "abstract": "The Stirling approximation formula for $n!$ dates from 1730. Here we give new\nand instructive proofs of this and related approximation formulae via tools of\nprobability and statistics. There are connections to the Central Limit Theorem\nand also to approximations of marginal distributions in Bayesian setups.\nCertain formulae emerge by working through particular instances, some\nindependently verifiable but others perhaps not. A particular case yielding new\nformulae is that of summing independent uniforms, related to the Irwin--Hall\ndistribution. Yet further proofs of the Stirling flow from examining aspects of\nlimiting normality of the sample median of uniforms, and from these again we\nfind a proof for the Wallis product formula for $\\pi$.", "updated": "2024-10-25T13:33:54Z", "published": "2024-10-25T13:33:54Z"}