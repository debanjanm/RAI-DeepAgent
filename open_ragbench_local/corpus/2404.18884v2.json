{
  "title": "Reputation and Risk in Regimes",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nHow valuable is reputation to a regime seeking to deter uprisings? I show the answer depends almost entirely on players' strategic uncertainty over other players actions. Without higher-order uncertainty, reputational effects are overshadowed by stage-game strategic complementarities: any payoff attainable in the complete-information repeated game remains attainable in the corresponding reputation game. However, when agents face globalized uncertainty over their payoffs, the regime's value for reputation rises sharply. In particular, the regime can leverage globalized strategic uncertainty to successful develop a reputation and secure their Stackelberg payoff in all pure equilibria of the reputation game, even as that same globalized strategic uncertainty forces them to their minimax payoff in the associated complete-information repeated game. Moreover, the globalized reputation game selects a unique pooling Markov equilibria among the continuum of Markov equilibria present in the complete-information game, refining predictions about equilibrium play. To establish these results, I develop new tools for analyzing non-normalized discounted repeated games with endogenous exit, where commitment actions vary with the regime's patience.\n\n\nKeywords: Repeated Global Games, Reputation, Regime Change, Endogenous Exit. JEL Codes: C73, D74, I14.\n\n[^0]\n[^0]:    *Department of Economics, Massachusetts Institute of Technology. daniel57@mit.edu.\n    A previous version of this paper was circulated under \"Reputation in Repeated Global Games of Regime Change.\" Much of this paper was written when I was an undergraduate at Northwestern University. I thank George-Marios Angeletos, Alvaro Sandroni, Nina Fl\u00fcegel, Drew Fudenberg, Eric Gao, Andrew Koh, Andrew Komo, Stephen Morris, Alessandro Pavan, Harry Pei, Udayan Vaidya, Alex Wolitzky, Muhamet Yildiz, and Alison Zhao for helpful feedback regarding this project. I also thank conference participants at Stonybrook and the Carroll Round for comments. This research was partially supported by the NSF Graduate Research Fellowship and the Northwestern University Baker program in undergraduate research. All opinions and remaining errors are my own.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "# 1 Introduction \n\n\"Am I going to die? AM I? . . Am I going out like Stan Chera?\"<br>- Donald Trump, after being diagnosed with Covid-19.\n\nFor many autocrats, the fear of death looms large. Driven both by the prospect of permanent removal from their lucrative positions and by the horrific ways in which their predecessors in spirit have met their ends - Mussolini in Italy, Ceausescu in Romania, and Gadaffi in Libya, among others - autocrats often take excessively cruel postures against dissent in order to stave off violent revolutions. Such violence is often costly both to the populace (as the recipients) and the regime (via sanctions, losses in credibility, and logistical costs), and thus prima facie should not be observed along the equilibrium path. Despite this observation, such cruelty is ubiquitous. One potential way to attempt to resolve this empirical puzzle with economic theory is to model repression as a costly signal, which may convey that (unsuccessful) dissent will be met with intense crackdowns, forestalling regime change. Such signaling may deter revolt by convincing would-be revolutionaries that failure would be dealt with harshly, driving down the expected size and success rate of revolts ${ }^{1}$.\n\nHowever, the above logic, while colloquially compelling, is necessarily incomplete. First, so long as no regime actively enjoys punishing dissidents, signaling about preferences alone is insufficient to resolve lack-of-commitment problems and convince agents they will be punished in equilibrium ${ }^{2}$. Second, whenever the regime cannot stave off the entire populace unanimously revolting, there always exists an equilibrium where everyone attacks and the regime is defeated, regardless of regime signaling incentives ${ }^{3}$. Finally, since the regime always faces some probability of exit from the game (i.e. a nonzero probability of successful revolution), their effective discount rate (normalized by the exit probability) may be bounded away from 1 , and hence even as they become arbitrarily patient they may not be incentivized to undertake costly actions today to secure future rewards tomorrow.\n\nIn this paper, I analyze a minimal model of repeated regime change with exit which completes the above signaling intuition while delivering new economic insights. Formally, I study a repeated coordination game where a continuum of short-lived agents (the citizenry) decide whether or not to take a risky action (revolution) that succeeds if and only if the\n\n[^0]\n[^0]:    ${ }^{1}$ There is some empirical evidence such cost-benefit analysis is first order in driving protest size: Cantoni et al. (2022) write \"Variation in turnout may reflect changes in the perceived benefits of the public good,\" and call for more formal models to understand the dynamics of protest (including variations in their size).\n    ${ }^{2}$ Importantly, this is because the regime only punishes if the attack is unsuccessful; hence, in the static game levying punishment must always be a dominated strategy as long as it is costly for the regime.\n    ${ }^{3}$ See Vives (1990) and Milgrom and Roberts (1990) the \"largest\" rationalizable strategy in the regime change game remains an equilibrium regardless of the cost of failure.\n\nmass of agents attacking is larger than some exogenous fundamental (the strength of the regime). While they make these decisions, a long-run agent (the regime) can choose to set the cost of failed coordination (i.e. punishments to dissidents) at some cost to themselves. If a revolution is ever successful, then the game ends, and the regime secures 0 payoff forever, while survival allows all players to live to (potentially) fight another day.\n\nThis framework allows me to flexibly analyze several distinct informational environments by varying the strategic uncertainty that agents face about each other, the strength of the regime, and the regime's preferences. First, I follow the approach of Fudenberg and Levine (1989) and Fudenberg and Levine (1992) in modeling signaling incentives by endowing the long-run player with a commitment type who always takes the same action, and allow a rational regime to imitate this commitment type as a way to signal they will take harsh actions in the future. Second, I follow the approach of Carlsson and van Damme (1993) and Angeletos et al. (2007) in endowing agents in the supermodular regime change game with private, idiosyncratic signals about the strength of the regime, instead of supposing it is common knowledge. I interpret these private signals as ideological disagreements among the citizenry as to the stability of the regime, which in turns causes higher-order uncertainty about the size of the revolution. Regime change is thus potentially thwarted by two different forces: persistent uncertainty about the cruelty of the regime, and transient, strategic uncertainty about the total proportion of fellow citizens who are engaging in the uprising.\n\nI show that, perhaps surprisingly, both channels of uncertainty are necessary to accomodate the unique prediction that the regime chooses to enact cruel punishments in order to deter revolution. Without globalized strategic uncertainty, I show not only that the feasible payoff set in the complete information game where the regime's rationality is known is exactly the same as in the incomplete information game where the regime faces reputational incentives, but that the payoff set has an \"anything goes\" feature: it converges to $[0, \\infty)$ as the regime's discount rate approaches 1 . Moreover, each equilibrium payoff can be attained in Markov strategies which condition only on the (common) posterior belief about the regime's type (Proposition 3), defanging standard selection techniques in applied theory. Similarly, if the regime's type is known but short-run players face idiosyncratic uncertainty about the exogenous fundamental, a folk theorem in pure strategies also holds, with the payoff set converging to $[0, \\infty)$, though Markov strategies in this case select for the stage game Nash equilibrium: all agents always attack, the regime never punish dissidents, and regime change occur (almost) immediately (Proposition 4). These negative results hold for all regime discount rates and agent information structures.\n\nThe main result in this paper - Theorem 1 - shows that combining reputational incentives for the regime with strategic uncertainty for the dissidents both guarantees the regime a high\n\npayoff in pure strategies and admits a unique pure (Markovian) equilibrium ${ }^{4}$ wherein the regime always punishes as-if they were the commitment type. Importantly, our result holds in the patient limit (i.e. the standard reputation limit in Fudenberg and Levine (1989)), as idiosyncratic and aggregate uncertainty about the fundamental vanishes (i.e. the selection limit in Carlsson and van Damme (1993)). Consequently, I view Theorem 1 as making two distinct contributions. First, it provides a justification for selecting the government-preferred Markov perfect equilibrium in the complete information, perfectly coordinated game by jointly perturbing the regime's type and the agents' information. Second, it highlights the minimal modeling assumptions necessary to formally capture standard \"deterrence\" intuition for harsh punishment against dissent in repeated games with exit.\n\nTheorem 1 must overcome a few important technical hurdles which render standard selection arguments from the reputation or global games literature infeasible. First, since the rate at which the game ends depends on the regime's action (in contrast to canonical reputation models), the regime's Stackelberg action varies with the discount rate, and thus characterizing whether the regime benefits from commitment in the patient limit can be difficult. To resolve this issue, Proposition 2 establishes the existence of a strongman effect: more patient regimes care about the future more, and thus are crueler, which may be of independent interest when understanding variation in autocratic behavior. Methodologically, it is proven via a a novel comparative statics technique for discounted payoff streams which cross 0 at most once from below. Second, so long as there is a positive probability of exit, payoffs normalized by the discount rate (as is standard in the canonical repeated games literature), are vacuous in our setting - all discounted payoff streams converge to 0 in the patient limit. Consequently, to build a nontrivial theory of payoffs, the results of this paper are all written for non-normalized payoff streams, even though this implies the regime's lack of commitment problem may be first order even in the patient limit ${ }^{5}$. I resolve this problem by leveraging the symmetry of the underlying global game to show the optimal commitment posture is endogenously chosen in such a way that the implied dynamic enforcement constraint must still always be satisfied in the patient limit (Lemma 3). These results may be of interest to the development of more general political economy models with exit, as they clarify abstractly the distortions exit can have on players' strategic considerations in the repeated game ${ }^{6}$. Consequently, I view these results as an independent contribution of this paper.\n\n[^0]\n[^0]:    ${ }^{4}$ Throughout the paper I focus on pure-strategy Nash equilibrium. For the permissive equilibrium results - Propositions 3 and 4, this is without loss of generality. However, this may not be true for the main positive result; understanding the payoff set with mixed strategies is a promising direction for future research, and is partially discussed in Proposition 5.\n    ${ }^{5}$ This is never a problem in normalized repeated games, since the principal gains at most $\\frac{(1-\\delta)}{\\delta}$ units of future consumption deviating, and hence any nonzero punishment enforces actions today as $\\delta \\rightarrow 1$.\n    ${ }^{6}$ Indeed, the political science literature has explicitly written that exit can affect the efficiency of equilibria\n\nThe remainder of this paper is organized as follows. I first review the related literature. Section 2 introduces the formal model, and establishes some preliminary observations. Section 3 analyzes the sets of feasible payoffs when either there is no global games structure or reputational incentives. Finally, Section 4 presents the main results and some relevant corollaries. Section 5 discusses variations and potential directions for future work. All proofs are in Appendix A. Appendix B contains supplementary material.\n\nRelated Literature. My modeling choices draw from and contribute to several distinct strands of literature. First, the idea reputation formation can benefit a long-lived player by signaling they will play a commitment action was first formalized and introduced by Kreps and Wilson (1982) and Milgrom and Roberts (1982) for the chain-store game. Fudenberg and Levine (1989) and Fudenberg and Levine (1992) generalized this idea to arbitrary stage games and arbitrary monitoring structures, using concentration results in Bayesian learning. Ely and Valimaki (2003) and Ely et al. (2008) show a stylized form of exit can lead to \"reputational failure\" results even if the long-lived player lives forever. Cripps et al. (2004) consider the sustainability of reputation in incomplete monitoring games, while Pei and Li (2021) show that in complete information games, unique payoff selection need not imply unique equilibrium selection: there may be a continuum of behaviors associated with a unique Stackelberg payoff. In contrast, reputation is a strictly asymptotic phenomenon in my model, and I obtain a unique equilibrium prediction. Finally, the observation exit probabilities and future patience can impact a model in distinct ways even if they behave the same way in the long-run player's discounted payoff was also pointed out by Pei (2024) and Pei (2025) in reputation and community enforcement models, respectively, though in his framework player's records are endogenous and exit is exogenous, while the opposite is true in my model. Consequently, the driving forces behind our selection results are distinct.\n\nSecond, I draw from the literature on global games, first pioneered by Carlsson and van Damme (1993) for investment games and Morris and Shin (1998) for regime change games. Frankel et al. (2003) generalize the selection criteria for arbitrary supermodular games and give conditions for the selection of the risk-dominant equilibrium. A review of the early literature can be found in Morris and Shin (2003). Edmond (2013) show in the static game that regime interference in short-run player coordination frictions (via \"signal-jamming\") may lead to equilibrium multiplicity, while Inostroza and Pavan (2024) show that if the regime has commitment, their ability to design additional information may select a unique rationalizable strategy. Further afield, Boleslavsky et al. (2021) study the effect of regime commitment\n\n[^0]\n[^0]:    in the patient limit: Acemoglu and Robinson (2006) writes \"These considerations make politically powerful groups fear losing power...even when such change will benefit society as a whole.\"\n\nto information on a global regime change game, Bueno de Mesquita and Shadmehr (2022) andMorris and Shadmehr (2023) studies how to flexibly change agent payoffs from success to maximize the probability of successful revolution, and Morris and Shadmehr (2024) study endogenous responses of revolutionaries in response to distinct repression structures. Finally, Angeletos et al. (2006) show multiplicity may arise if the regime has signaling incentives when they take their action, while Jann and Schottmuller (2021) show uniqueness can prevail if the regime's action is unobserved.\n\nGlobal games have also been applied to dynamic supermodular games, though often standard methods are insufficient to select for a unique equilibrium. Angeletos et al. (2007) and Angeletos and Pavan (2013) study the consequences of multiplicity in a dynamic model of regime change with a persistent fundamental. Importantly, multiplicity in these papers are driven by endogenous learning due to the persistent fundamental; while in my model the state is independently drawn in each period and multiplicity results from endogenous learning about the regime's persistent type. Chassang (2010) leverages the exit structure of a dynamic symmetric investment game to obtain some unique predictions. Giannitsarou and Toxvaerd (2003) study a dynamic supermodular game and show how recursive variations on standard methods can select for the unique Markov perfect equilibria in the globalized informational environment; in particular, their results highlight that Markov selection in dynamic global games may be a natural desiderata.\n\nThe closest paper to mine is Huang (2017), which is the only other paper to study a dynamic global game where a policymaker faces reputational incentives. In this model, agents observe signals about a persistent state, and thus the learning dynamics about the exogenous fundamental imply reputational effects overwhelm coordination effects, and no agent attacks in the unique equilibrium (and so the regime never needs to take a costly action). Conversely, in my model the state is transient, and thus the interaction between coordination affects and reputational effects are more subtle: some agents may attack, but the regime on-path will have to incur costly punishments. Moreover, in his model the regime undertakes a costly action to defend against the attack, while in mine the regime takes a costly action after the attack has been deterred. The stark behavioral differences in our models highlight the subtle economic interaction between the dynamic incentives for the regime and agent stage-game complementarities. Importantly, because the regime never attacks in the model of Huang (2017), I see my framework - where costs are levied only if the regime survives - as a more natural way to model situations where costly regime actions are observed on path (i.e. the autocrat's problem in the motivating example).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 2 Model\n### 2.1 The Stage Game\n\nThere exist a single long-run player 1 (the \"regime\") and a continuum of short-run players $2_{i}$ (the \"population\"). Before any player acts, nature draws a state $\\theta \\in \\mathbb{R}$ from a full-support distribution $P_{\\theta}$, with mean $\\mu$ and variance $\\tau<\\infty$. Each short-run player $2_{i}$ observes a private signal $x_{i}=\\theta+\\varsigma \\varepsilon_{i}$, where $\\varsigma>0$ is a parameter determining the strength of correlation of the noise, and $\\varepsilon_{i} \\sim F_{\\varepsilon}$ is some idiosyncratic noise drawn i.i.d. across players. I impose the following assumptions on the information structure.\n\nAssumption 1 (Informational Assumptions). The following hold.\n(1) $F_{\\varepsilon}$ and $P_{\\theta}$ admit strictly positive, bounded continuous densities; $F_{\\varepsilon}$ is symmetric around 0 , and $P_{\\theta}$ is symmetric around $\\mu \\in\\left(\\frac{1}{2}, 1\\right)$.\n(2) The first-order conditional distribution $F_{\\theta}(\\cdot \\mid x)$ has strictly positive density and is decreasing in its second argument.\n(3) The second-order conditional distribution $F_{x}(\\cdot \\mid x)$ has strictly positive density and is decreasing in its second argument.\n\nThat $F_{\\varepsilon}$ is symmetric can be interpreted as a requirement that players' private signals are unbiased white noise. Similarly, that $P_{\\theta}$ is symmetric around its mean $\\mu$ can be interpreted as the common belief agents' hold after observing a public, unbiased signal about some fixed state $\\mu$, after coming in with an improper prior (see for example, Morris and Shin (2003)). The requirement that $\\mu>\\frac{1}{2}$ is imposed to ensure the state is not, on average, in the \"bad\" dominance region, where strategic interaction is irrelevant. ${ }^{7}$.\n\nAssumption 1 also guarantees $F_{x}(x)$ admits a strictly positive density; along with the additive noise structure, this implies the conditional beliefs $F_{x}(x \\mid \\theta)$ are decreasing in the realized state. Moreover, Assumption 1 and the additive structure of noise are sufficient to imply the distributions in Parts (2) and (3) are differentiable in their conditioning argument. Together, these smoothness assumptions are an MLRP-like condition that ensures a firstorder approach is valid, and are standard in the literature ${ }^{8}$\n\n[^0]\n[^0]:    ${ }^{7}$ I only need the third part of Assumption (1.1) for payoff guarantee results; for negative results where there is a continuum of payoffs or the regime attains their minimax payoff, it is not necessary.\n    ${ }^{8}$ These technical assumptions are satisfied by all canonical global games information structures; in particular, normal-normal updating satisfies these conditions.\n\nAfter observing their private signals, short-run agents take an action $a_{i} \\in\\{0,1\\}$. Let $A=\\int a_{i} d i$ be the total mass of agents taken the risky action $a_{i}=1$. Simultaneously ${ }^{9}$, longrun players choose to take action $c \\in \\mathbb{R}_{+}$at a cost $\\kappa(c)$. Given actions $\\left(a_{i}, A, \\theta, c\\right)$, payoffs for player 1 and $2_{i}$ are, respectively\n\n$$\nv(A, \\theta, c)= \\begin{cases}0 & \\text { if } \\theta<A \\\\ 1-\\kappa(c) & \\text { if } \\theta \\geq A\\end{cases} \\quad \\text { and } \\quad u_{i}\\left(a_{i}, A, \\theta, c\\right)= \\begin{cases}a_{i} & \\text { if } \\theta<A \\\\ -a_{i} c & \\text { if } \\theta \\geq A\\end{cases}\n$$\n\nPayoffs here mirror the standard regime-change game payoffs of Carlsson and van Damme (1993), Frankel et al. (2003), and others, where $\\theta$ is the \"peg\" or strength of the regime, and $A$ is the total size of the attack, with two exceptions. First, the cost of a failed attack, $c$, is endogenously chosen by the regime and can vary given their strategies. Second, the cost to the regime is only incurred if the attack is unsuccessful; in particular, if $\\theta<A$ then the regime falls, and so cannot implement their action. I interpret $c$ as the value of \"punishing\" aggressive agents (for example, harsh prison sentences, work camps, social isolation through propaganda, etc.), and thus interpret $\\kappa$ as the cost to the regime of enacting punishment $c$.\n\nAssumption 2 (Payoff Assumptions). $\\kappa(c)$ is a strictly increasing, continuously differentiable function satisfying $\\kappa(0)=\\kappa^{\\prime}(0)=0$ and $\\lim _{c \\rightarrow \\infty} \\kappa(c)=\\infty$.\n\nAssumption 2 is a technical Inada-type condition which guarantees an optimal punishment exists. That $\\kappa(c)=0$ implies a lack-of-commitment problem for the regime; their best-response against any strategy of the short-run players is to set $c=0$, though they may benefit from being able to commit to some $c>0$ as a way to increasing their survival probability. That $\\kappa^{\\prime}(c)=0$ implies the optimal commitment posture is nonzero, a basic nontriviality assumption. I assume the structure of $\\kappa$ cannot vary with the size of the attack, though this assumption can be significantly relaxed at the cost of a much more complicated exposition. Unless otherwise stated, I assume throughout the paper for all results that Assumptions 1 and 2 hold. One common situation where Assumption 1 does not hold is the case where $\\varsigma=0$, i.e. all short-run players commonly observe the state. Whenever I analyze this version of the model, I will refer to the corresponding stage game as the nonglobal stage game. The game where $\\varsigma>0$ and Assumption 1 holds is the globalized stage game.\n\nThe long-run player's stage-game strategy $s_{1} \\in \\Delta(\\mathbb{R})$ is a choice of cost to levy on failed revolutionaries. Short-run agent stage-game strategies are functions $s_{2, i}: \\mathbb{R} \\rightarrow \\Delta(\\{0,1\\})$ is a choice of whether or not to attack given their private signal. A first step is to characterize\n\n[^0]\n[^0]:    ${ }^{9}$ It is not important for our analysis that the long-run player moves simultaneously with the short-run players. In particular, our model is equivalent to one where the long-run player acts after observing whether or not the revolution was successful.\n\nthe set of rationalizable profiles $s_{2, i}$ in the stage game for fixed values of $c$.\nProposition 1. Suppose $\\varsigma<\\bar{\\varsigma}$ for some $\\bar{\\varsigma}>0$ sufficiently small. Then for all $c$, there exists functions $x^{*}, \\theta^{*}: \\mathbb{R}_{+} \\rightarrow \\mathbb{R}$ such that\n(1) The strategy $s(x, c)=\\mathbf{1}\\left\\{x<x^{*}(c)\\right\\}$ is the (essentially) unique strategy surviving iterative deletion of strictly dominated strategies.\n(2) $x^{*}(c) \\in \\mathcal{C}^{1}$, and is strictly decreasing in $c$.\n(3) $\\theta^{*}(c) \\in \\mathcal{C}^{1}$, is strictly decreasing in $c$, and $\\int s(x, c) d F_{x \\mid \\theta} \\geq \\theta$ if and only if $\\theta \\leq \\theta^{*}(c)$.\n\nThe first statement follows from a standard argument, following Frankel et al. (2003), adapted to our setting. Here, I use \"essentially\" to mean off a set of measure zero (in particular, ties can be broken the other way at the point $x=x^{*}(c)$ ). The second and third statements then follow from some standard comparative static arguments. The technical details of the proof are given in Appendix A. . The value of the cutoffs $x^{*}(c)$ and $\\theta^{*}(c)$ will be useful in deriving (the properties of) the regime's optimal commitment value. I assume throughout the remainder of the paper that $\\varsigma$ is chosen sufficiently small for uniqueness, noting $\\varsigma$ can be chosen independently of $c$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 2.2 The Repeated Game \n\nSuppose time is discrete and indexed by $t=0,1,2, \\ldots$ In each period, player 1 and $\\left\\{2_{i}\\right\\}_{i \\in[0,1]}$ play the stage game described above. I use subscripts $t$ to denote actions or states at time- $t$, e.g. $A_{t}$ or $\\theta_{t}$. All players discount the future at a common rate $\\delta$, though short-run players' behavior myopically. ${ }^{10}$ At the conclusion of the game, one of two outcomes occur:\n(1) The attack at time $t, A_{t}$, is larger than the state $\\theta_{t}$, so the game ends.\n(2) The game continues, and the long-run player's choice of time- $t$ punishment is perfectly revealed to all agents ${ }^{11}$.\n\nThe first possibility (that $A_{t} \\geq \\theta_{t}$ ) departs from the standard repeated game setting in that the game ends at the first time a revolution is successful, following the modeling choice of Angeletos et al. (2007).\n\n[^0]\n[^0]:    ${ }^{10}$ For ease of notation, I will index short-run players at time $t$ by $2_{i, t}$ to denote a short-run player who is taking an action at time $t$. Note that because there are continuum of players whose individual actions do not affect the aggregate state, it is without loss of generality to model them as as myopic agents even though they may in principle be long-lived.\n    ${ }^{11}$ In principle, one may also allow past states and actions to be (noisily) monitored. However, because past signals and actions do not affect any players' histories and all short-run player strategies are measurable with respect to the cost, restricting only to observing past costs is without loss of generality.\n\nThe long-run player can be one of several types: a rational type $\\omega_{R}$, who seeks to maximize their total discounted expected payoff, or one of a discrete number of behavioral types $\\omega_{c} \\in$ $\\Omega \\backslash\\left\\{\\omega_{R}\\right\\}$, who myopically plays $c$ in each period ${ }^{12}$. Players have a full-support prior belief $\\nu_{0} \\in \\Delta(\\Omega)$ over the long-run player's type.\n\nA time- $t$ history $h_{t}$ is a sequence $\\left\\{c_{s}\\right\\}_{s=0}^{t-1}$ of past costs levied by the principal. Let $H_{t}$ be the set of all time- $t$ histories and $H^{\\infty}$ be the set of all histories. A strategy profile is a tuple $\\sigma=\\left\\{\\sigma_{1}, \\sigma_{2 i}\\right\\}$ consisting of functions $\\sigma_{1}: H \\times \\Omega \\rightarrow \\Delta(\\mathbb{R})$ and $\\sigma_{2 i}: H \\rightarrow \\Delta(\\{0,1\\})^{\\mathbb{R}}$ that map histories into stage-game strategies. Every strategy profile $\\sigma$ induces a probability measure ${ }^{13}$ over histories $\\mathbb{P}^{\\sigma}$. Given a strategy profile $\\sigma$ and full-support prior $\\nu_{0} \\in \\Delta(\\Omega)$, players have a belief $\\nu_{t}\\left(\\cdot \\mid h_{t}\\right) \\in \\Delta(\\Omega)$ of the long-run player's type. At any time- $t$ history $h_{t}$, the strategy profile $\\sigma\\left(h_{t}\\right)$ induces an expected payoff\n\n$$\nV\\left(\\sigma\\left(h_{t}\\right)\\right)=\\mathbb{E}_{\\theta}\\left[\\left(P_{\\theta}\\left(\\mathbb{E}_{x}\\left[\\sigma_{2 i}\\left(h_{t}\\right)\\left(x_{i}\\right) \\mid \\theta\\right] \\leq \\theta\\right)\\right)\\left(1-\\kappa\\left(\\sigma_{1}\\left(h_{t}, \\omega_{R}\\right)\\right)\\right)\\right]\n$$\n\nwhere if $\\sigma_{1}\\left(h_{t}\\right)$ is a mixed strategy then it refers to the expected action. The long-run player's expected discounted payoff given a strategy profile $\\sigma$ is then\n\n$$\n\\mathcal{V}(\\sigma)=\\mathbb{E}^{\\sigma}\\left[\\sum_{t=0}^{\\infty} \\delta^{t} V\\left(\\sigma\\left(h_{t}\\right)\\right)\\right]\n$$\n\nExpected payoffs are not normalized by $(1-\\delta)$, as is standard in the literature on repeated games (see Fudenberg et al. (1990) and Fudenberg and Tirole (1991)). Here, this is because the choice of exit - which affects players' payoffs in the game - will affect the total discounted payoff. In particular, if $\\underline{\\tau}=P_{\\theta}(0)$ is the amount of time the regime is guaranteed to fail, then for any strategies $\\sigma$ and discount rate $\\delta, \\mathcal{V}(\\sigma) \\leq \\frac{1}{1-\\underline{\\tau}}<\\infty$. Consequently, if $\\mathcal{V}$ was normalized by $(1-\\delta)$, then in the patient limit all payoffs would converge to 0 and the game would be trivial ${ }^{14}$. Technically, this implies that standard arguments for computing the payoffs of repeated games and reputation do not immediately apply (in particular, as $\\delta \\rightarrow 1$, all payoffs converge to $\\infty$ ). Despite this challenge, I will be able to obtain asymptotic results about the rates of convergence for payoffs (or show they hold in finite time) in equilibrium.\n\nDefinition 1. A strategy profile $\\sigma^{*}$ is a Nash equilibrium if, for all $h^{t}$ where $\\mathbb{P}^{\\sigma^{*}}\\left(h^{t}\\right)>0$,\n(1) (Long-Run Best Replies). The rational type long-run player maximizes their discounted\n\n[^0]\n[^0]:    ${ }^{12}$ I will sometimes abuse notation and refer to $c$ as the commitment type $\\omega_{c}$, e.g. by saying $c \\in \\Omega$. Moreover, as the analysis will make clear, I will also use \" 0 \" to refer to the type of the rational type.\n    ${ }^{13}$ As exit occurs with positive probability, there exists finite time- $t$ histories $h_{t}$ which terminate, i.e. have no successor histories.\n    ${ }^{14}$ Another way to see why normalizing payoffs is inappropriate in this game is to recall normalized payoffs converge to the average time payoffs in each period as $\\delta \\rightarrow 1$ in standard repeated games - since the game ends with probability 1 in finite time, normalizing payoffs would imply all average payoffs are 0 .\n\nexpected payoff:\n\n$$\n\\mathcal{V}\\left(\\sigma^{*}\\right) \\geq \\mathcal{V}\\left(\\sigma_{1}\\left(\\cdot, \\omega_{R}\\right), \\sigma_{1}^{*}\\left(\\cdot,-\\omega_{R}\\right), \\sigma_{2 i}^{*}\\right)\n$$\n\nfor every strategy $\\sigma_{1}\\left(\\cdot, \\omega_{R}\\right)$.\n(2) (Behavioral Types). $\\sigma_{1}^{*}\\left(h_{t}, \\omega_{c}\\right)=c$ at every history.\n(3) (Short-Run Best Replies). At every history $h_{t}$ and given $\\left\\{\\sigma_{1}^{*}\\left(h^{t}, \\cdot\\right)\\right\\},\\left\\{\\sigma_{2 i}^{*}\\right\\}$ is a Nash equilibrium of the stage game where short-run players face\n\n$$\nc=c^{\\sigma}\\left(h_{t}\\right)=\\mathbb{E}_{\\nu_{t}\\left(\\cdot \\mid h_{t}\\right)}\\left[\\sigma_{1}^{*}\\left(h_{t}, \\omega\\right)\\right]\n$$\n\nA strategy profile $\\sigma^{*}$ is a pure Nash equilibrium if $\\sigma_{1}\\left(\\cdot, \\omega_{R}\\right)\\left(h_{t}\\right)$ is a pure-strategy in the stage game for every history $h_{t}$ reached on-path according to $\\mathbb{P}^{\\sigma^{*}}$. An equilibrium $\\sigma^{*}$ is Markov perfect if, for any two time- $t$ histories $h_{t} \\neq h_{t}^{\\prime}$ where $\\nu_{t}\\left(\\cdot \\mid h_{t}\\right)=\\nu\\left(\\cdot \\mid h_{t}^{\\prime}\\right), \\sigma\\left(h_{t}\\right)=\\sigma\\left(h_{t}^{\\prime}\\right)$ (that is, each player's stage-game strategy depends only on the long-run player's reputation). The game where $\\nu_{0}\\left(\\omega_{R}\\right)=1$ is the complete information repeated game, and the game where $\\nu_{0}\\left(\\omega_{R}\\right)<1$ is the reputation formation or incomplete information game. Let\n\n$$\n\\underline{V}(\\delta)=\\inf _{\\sigma^{*}}\\left\\{\\mathcal{V}\\left(\\sigma^{*}\\right): \\sigma^{*} \\text { is a Nash equilibrium }\\right\\}\n$$\n\nbe the regime's worst equilibrium payoff at discount rate $\\delta$. Analogously, let $\\underline{V}_{P}^{*}(\\delta))$ and $\\underline{V}_{M}^{*}(\\delta)$ ) be the regime's worst equilibrium payoff among all pure strategy and Markov perfect equilibria ${ }^{15}$, respectively.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 3 Baseline Analysis\n### 3.1 Commitment Postures\n\nWhen does the long-run player benefit from commitment? Suppose there was common knowledge that the long-run player was a commitment type who played some $c \\in \\mathbb{R}$ in every period. In the corresponding equilibrium, short-run players would then play the stage-game strategy $\\mathbf{1}\\left\\{x_{i t}<x^{*}(c)\\right\\}$ at every history, inducing a stationary exit probability of $P_{\\theta}\\left(\\theta^{*}(c)\\right)$ in each period. Taken together, this gives the long-run player a discounted payoff of\n\n$$\nV_{H}(c, \\delta)=\\sum_{t=0}^{\\infty} \\delta^{t}\\left(\\bar{P}_{\\theta}(c)\\right)^{t+1}(1-\\kappa(c)) \\text { where } \\bar{P}_{\\theta}\\left(\\theta^{*}(c)\\right)=1-P_{\\theta}\\left(\\theta^{*}(c)\\right)\n$$\n\n[^0]\n[^0]:    ${ }^{15}$ See Fudenberg and Tirole (1991) for the formal definition of Markov perfect equilibrium, noting the state here is the belief about the long-run player's rationality.\n\nrepresents the probability of survival given commitment posture $c$. A commitment posture $c^{*}$ is optimal at discount rate $\\delta$ if it maximizes $V_{H}(c, \\delta)$ among all $c \\in \\mathbb{R}_{+}$. Let $c^{*}(\\delta)$ be the set of all optimal commitment postures at a given discount rate $\\delta$. It will be helpful to define\n\n$$\nV_{H}^{*}(\\delta)=V_{H}\\left(c^{*}(\\delta), \\delta\\right) \\text { and } V_{L}^{*}(\\delta)=V_{H}(0, \\delta)\n$$\n\nas the best and worst feasible payoffs for the regime.\nNote the commitment posture $c^{*}(\\delta)$ to vary flexibly with the discount rate $\\delta$, which is a departure from standard models of repeated games (c.f. Fudenberg et al. (1990), Fudenberg et al. (1994)). This is because of the endogenous exit in the game - as $\\delta$ increases, the regime values the future more and consequently is willing to sacrifice consumption today if it guarantees an increased probability of regime survival; such a force is often the focus of political economy understandings of regime change (see Acemoglu and Robinson (2000)) but so far has not been analyzed in the context of reputation formation. We interpret this as a strongman effect: more patient regimes would prefer to be harsher, even if it is more costly for them to do so.\n\nProposition 2 (The Strongman Effect). The correspondence $c^{*}(\\delta)$ is nonempty, compactvalued, upper hemi-continuous in $\\delta$, and increasing (in the strong set order) in $\\delta$.\n\nThe first three properties follow from Berge's theorem of the maximumm, while monotonicity follows by noting $V_{H}(c, \\delta)$ is strictly single-crossing in $(c, \\delta)$, but is not immediate: in particular, $V_{H}$ need not be supermodular, so some care (and a quick novel mathematical lemma) are required. Details are spelled out in Appendix A. While $c^{*}(\\delta)$ is in principle a correspondence, I will throughout the rest of the paper select the largest element of the correspondence whenever it is multiple-valued. As should be clear from the subsequent analysis, this is done only for simplicity of exposition and is done without loss of generality.\n\nThe regime cannot increase their payoff by committing to a dynamic posture, since the payoff is stationary and agents are short-lived. Consequently, $V_{H}^{*}(\\delta)$ represents the theoretical upper-bound the regime can achieve even if they were able to commit to any arbitrary strategy in the entire extensive-form repeated game with exit.\n\nProposition 2 leaves open the question about whether the optimal commitment posture is nonzero. However, as alluded to in the discussion following Assumption 2, this is built into the model as show by the lemma below; the proof can be found in Appendix A.\n\nLemma 1. There exists some $\\bar{c}>0$ where for all $P_{\\theta}$, all $\\delta>0$, and all $\\varsigma>0, c^{*}(\\delta) \\geq \\bar{c}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 3.2 Baseline Informational Environments \n\nShort-lived agents face two distinct sources of uncertainty when choosing which action to take in each period: idiosyncratic and transient uncertainty over the regime strength and other agents' actions $(\\theta)$, and persistent uncertainty over the type of the regime $(\\omega)$. What is the effect of each type of uncertainty in isolation? Proposition 3 shows that in the absence of idiosyncratic uncertainty (i.e. $\\varsigma=0$ ), strategic complementarities swamp all other forces: neither the reputation nor the Markov refinements restrict the set of equilibrium payoffs relative to the feasible set at all. Moreover, in the patient limit, every nonnegative real number can be attained as a payoff to the long-run player in some Nash equilibrium, rendering all predictions useless. Conversely, without the ability to build reputation, strategic uncertainty also dominates, but in the opposite direction: the unique stage-game equilibrium that arises from idiosyncratic payoff uncertainty dominates in the Markov equilibrium, though in pure Nash equilibria any regime payoff is attainable.\n\nSome notational preliminaries before the formal statements of the results are in order. Throughout, fix some prior $P_{\\theta}$ and an arbitrary discrete set of commitment postures $\\Omega$. Let $E\\left(\\delta, \\nu_{0}, \\varsigma\\right)$ and $E^{M}\\left(\\delta, \\nu_{0}, \\varsigma\\right)$ be the set of regimes payoffs attainable in some Nash equilibrium and Markov perfect equilibrium respectively at discount rate $\\delta$ and prior $\\nu_{0} \\in \\Delta(\\Omega)$.\n\nProposition 3 (Nonglobal Payoff Sets). Fix any $P_{\\theta}, \\delta>0$, and prior $\\nu_{0}$ where $\\nu_{0}\\left(\\omega_{R}\\right) \\leq 1$. Then $E\\left(\\delta, \\nu_{0}, 0\\right)=E^{M}\\left(\\delta, \\nu_{0}, 0\\right)$, and $\\lim \\sup _{\\tau \\rightarrow 0, \\delta \\rightarrow 1} E\\left(\\delta, \\nu_{0}, 0\\right)=[0, \\infty)$.\n\nThe limit supremum over $\\tau$ is taken over all possible distributions $P_{\\theta}$ where $\\operatorname{var}\\left(P_{\\theta}\\right)<\\tau$. The formal argument can be found in Appendix A. . Note Proposition 3 implies the payoff set does not change between the complete information game $\\left(\\nu_{0}\\left(\\omega_{R}\\right)=1\\right)$ and the incomplete information game $\\left(\\nu_{0}\\left(\\omega_{R}\\right)=1\\right)$. Intuitively, this is due to the ability of agents to perfectly coordinate when $\\varsigma=0$, and hence both the smallest and largest stage-game rationalizable strategies are static Nash equilibria for any value $c$ the regime chooses. Substantively, Proposition 3 implies stage-game strategic complementarities outweigh any reputation formation effects. Moreover, the entire set of payoffs can thus be implemented in Markov perfect equilibria, highlighting that Markov refinements cannot select for unique equilibrium predictions absent further assumptions on the short-lived agents' information structures. Thus, Proposition 3 cautions against the use of Markovian refinements in repeated supermodular games as way to obtain unique predictions.\n\nNext, suppose the rationality of the long-run player is known (so there are no incentives for reputation formation), but places face globalized uncertainty in the stage game. The Markov refinement now selects a unique equilibrium, but it is the static Nash equilibrium, so the game lasts only one period before exit, there are no incentives for the regime to take any nonzero\n\naction, and essentially all players attack even though they face idiosyncratic uncertainty about the state fundamental. Moreover, while there are non-Markovian equilibria where the regime chooses a nonzero cost, even in just pure strategies the set of attainable regime payoffs is once again $\\left[V_{L}^{*}(\\delta), V_{H}^{*}(\\delta)\\right]$, which converges to the nonglobal payoff set as $\\delta \\rightarrow 1$. In either case, Proposition 4 shows that the global games perturbation alone is insufficient to build a model of regime cruelty that can (uniquely) explain observational evidence.\n\nProposition 4. Fix any $P_{\\theta}, \\delta>0$, and prior $\\nu_{0}$ such that $\\nu_{0}\\left(\\omega_{R}\\right)=1$. For all $\\varsigma>0$, $E^{M}\\left(\\delta, \\nu_{0}, \\varsigma\\right)=\\left\\{V_{L}^{*}(\\delta)\\right\\}$, but $\\left|E\\left(\\delta, \\nu_{0}, \\varsigma\\right)\\right|=\\infty$ and $E\\left(\\delta, \\nu_{0}, \\varsigma\\right) \\subset\\left[V_{L}^{*}(\\delta), V_{H}^{*}(\\delta)\\right]$, with equality for a continuum of tuples $\\left(P_{\\theta}, \\delta, \\varsigma\\right)$.\n\nThe formal argument is found in Appendix A. . That Markovian equilibria are repeated stage-game equilibria follow from the fact continuation play cannot vary with past histories at all if $\\nu_{0}\\left(\\omega_{R}\\right)=1$, since the posterior belief does not change. Without the Markovian refinement, it is possible to support any cost $c$ where $1-\\kappa(c)+\\delta V_{H}(c, \\delta) \\geq 1+\\delta V_{L}^{*}(\\delta)$ by using the minimax payoff as a grim-trigger punishment. For parameters satisfying the conditions of Theorem 1, this is exactly the set of feasible payoffs $\\left[V_{L}^{*}(\\delta), V_{H}^{*}(\\delta)\\right]$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "# 4 Equilibrium Selection \n\nWhat can be expected from Markovian equilibria when the long-run player faces reputational incentives and short-run players face a globalized information structure? Consider any history at which the long-run player chooses to separate from the commitment type they imitated in past periods. Because there is common knowledge of regime rationality at that point, the regime's continuation payoff must be $V_{L}^{*}(\\delta)$ by Proposition 4. Thus, in any Markovian equilibrium where the regime benefits from reputation, they must constantly imitate some commitment type $c \\in \\Omega$. If the long-run player benefits from reputation formation at every history reached on path (so their action is always some constant $c$ ), I will call such an equilibrium $c$-pooling. Formally,\n\nDefinition 2. $\\sigma^{*}$ is $\\underline{c \\text {-pooling }}$ if $\\sigma_{1}^{*}\\left(h_{t}, \\omega_{R}\\right)=c$ for all on-path histories $h_{t}$.\nThere are two a-priori barriers to a successful reputation result. First is if the discount rate is so low that even large changes in continuation play cannot outweigh benefits from playing stage-game best responses today. As is common in the reputation literature, I ameliorate this problem by considering reputation formation only in the patient limit, where $\\delta \\rightarrow 1$. Second is if the randomness in the system is sufficiently large that the regime spends too much time in the dominance regions $(-\\infty, 0] \\cup[1, \\infty)$. If this is the case, then their reputational posture\n\nmay also no longer be useful to them, as they are likely either to exit the game or to survive regardless of what short-run agents believe about them. To ameliorate this problem - which is novel in repeated games with exit - I consider an additional \"exiting limit\" by requiring that the variance of the state, $P_{\\theta}$, vanishes (that is, $\\tau \\rightarrow 0$ ). Importantly, the order in which $\\tau \\rightarrow 0$ and $\\delta \\rightarrow 1$ does not matter for the qualitative nature of my results, which contrasts with other results where the rate of exit and patience level are separated (c.f. Pei (2024) and Pei (2025)). This limiting requirement is formally captured in the definition below.\n\nDefinition 3. A tuple of prior $P_{\\theta}$ and discount rate $\\delta$ is $\\underline{(\\bar{\\tau}, \\bar{\\delta}) \\text {-limiting }}$ if $\\mathbb{E}\\left[P_{\\theta}\\right]>\\frac{1}{1+\\bar{c}}$, $\\operatorname{var}\\left(P_{\\theta}\\right) \\leq \\bar{\\tau}, \\delta \\geq \\bar{\\delta}$, and $\\varsigma<\\bar{\\varsigma}\\left(\\operatorname{var}\\left(P_{\\theta}\\right), \\delta\\right)$.\n\nThat $\\varsigma<\\bar{\\varsigma}\\left(\\operatorname{var}\\left(P_{\\theta}\\right), \\delta\\right)$ simply ensures the risk-dominant equilibrium is selected in the limit of the global games perturbation, and is a standard consideration in the analysis of supermodular games (see Morris and Shin (2003)). I can now state the main result of this paper. The requirement $\\mu>\\frac{1}{1+\\bar{c}}$ says that on average ${ }^{16}$, the regime will survive in the riskdominant equilibrium; if this was not the case, then even with the potential for reputation effects the threat of exit would loom sufficiently large that it would be as if the effective discount rate was trending towards zero.\n\nTheorem 1 (Equilibrium Selection). Let $\\Omega=\\left\\{0, c^{*}(\\delta)\\right\\}$. Then there exists $\\bar{\\tau}>0, \\bar{\\delta}<1$ such that for all $(\\bar{\\tau}, \\bar{\\delta})$-limiting $\\left(P_{\\theta}, \\delta\\right)$, the $c^{*}(\\delta)$-pooling strategy is the unique Markov and pure Nash equilibrium.\n\nThe hypothesis that $\\Omega=\\left\\{0, c^{*}(\\delta)\\right\\}$ guarantees existence of a Markov perfect (or pure) Nash equilibrium. More generally, the result remains true so long as $c^{*}(\\delta) \\in \\Omega$ and the game admits a unique pure strategy Nash equilibrium. One potential way to justify the restriction on $\\Omega$ (which is external to the model) is to suppose that there is a $t=-1$ stage of the game where the long-run player \"announces\" the commitment type that they intend to imitate, as in, for example Abreu and Gul (2000).\n\nThe logic of Theorem 1 is as follows. First, since $\\mu>\\frac{1}{1+\\bar{c}}$, it must be that on average the regime survives so long as short-run players expect them to play an action above $\\bar{c}$. This observation, combined with the requirement $\\delta>0$, then allows the future to \"loom large\" as it does in standard repeated games without exit, and hence implies the regime strictly wishes to play the commitment action, $c^{*}(\\delta)$, over any deviation that reveals rationality in any pure Nash equilibrium. The final step of the theorem leverages concentration results and an unraveling argument to show no mixed-strategy Markov equilibrium can exist. Intuitively,\n\n[^0]\n[^0]:    ${ }^{16}$ At the cost of a less interpretable condition, Appendix B.I. relaxes this condition significantly and obtains selection results away from the doubly patient limit.\n\nif the long-run player mixes for sufficiently many periods, their continuation payoff must converge to their Stackelberg payoff by martingale convergence theorems; but this implies that after finitely many periods, they cannot be indifferent and must get their Stackelberg payoff. However, if this is the case, then in the last period for which the regime mixes, they cannot be indifferent between $c^{*}(\\delta)$ and any other action, as if they play $c^{*}(\\delta)$ they get their highest possible continuation payoff. Together, this implies the set of Markov and pure Nash equilibrium coincide, and are $c^{*}(\\delta)$ pooling. The formal proof can be found in Appendix A.\n\nWhile Theorem 1 is written in the language of equilibrium selection, it also implies asymptotic payoff selection. In particular, for any sequence of prior beliefs $P_{\\theta}^{n} \\rightarrow \\delta_{\\mu}$ and discount rates $\\delta_{n} \\rightarrow 1$, it must be that the worst pure-strategy (or Markov perfect) equilibrium payoff converges to the regime's Stackelberg payoff. Formally,\n\nCorollary 1 (Payoff Selection.). Let $\\left\\{P_{\\theta}^{n}, \\delta_{n}, \\Omega_{n}\\right\\}$ be any sequence where $\\mathbb{E}\\left[P_{\\theta}^{n}\\right] \\geq \\frac{1}{1+\\bar{e}}$, $\\operatorname{var}\\left(P_{\\theta}^{n}\\right) \\rightarrow 0, \\delta_{n} \\rightarrow 1$, and $\\Omega_{n}=\\lim _{\\varsigma \\rightarrow 0} c^{*}\\left(\\delta_{n}\\right)$. Then\n\n$$\n\\limsup _{n \\rightarrow \\infty} \\lim _{\\varsigma \\rightarrow 0}\\left|\\underline{V}_{P}^{*}\\left(\\delta_{n}\\right)-V_{H}^{*}(\\delta)\\right|=0\n$$\n\nCorollary 1 is an immediate application of Theorem 1, and holds in the limit. However, the core logic of Theorem 1 can be applied to any commitment posture $c$ for which the the cost today is not large relative to the benefit of commitment. Corollary 2 leverages this observation to establish that, even away from the $(\\tau, \\delta) \\rightarrow(0,1)$ limit, the regime benefits from reputational formation in the globalized game, which contrasts with the folk theoremtype results in Propositions 3 and 4. The formal proof is found in Appendix A., where they need not benefit from commitment.\n\nCorollary 2 (Payoff Improvements). For any $P_{\\theta}$ and $\\delta \\in(0,1)$ and $\\varsigma<\\bar{\\varsigma}\\left(\\operatorname{var}\\left(P_{\\theta}\\right), \\delta\\right)$, there exists $c>0$ such that if $\\Omega=\\{0, c\\}$, then the unique MPE $\\sigma^{*}$ is c-pooling. Moreover, for any $\\delta$, there is some $\\varepsilon(\\delta)>0$ such that $\\mathcal{V}\\left(\\sigma^{*}\\right)>V_{L}^{*}(\\delta)+\\varepsilon(\\delta)$ for all prior beliefs $\\nu_{0}$.\n\nThe above results all restrict to pure-strategy Nash equilibria. Away from pure-strategy or Markovian equilibria, short-run players may be able to mix in potentially pathological ways that make reputation bounds hard to establish due to the endogenous exit in the game. However, I conclude with a result that implies this is, in some sense, the main barrier to analyzing arbitrary mixed equilibria is exactly exit in the finite game: there is a payoff guarantee as long as reputation has been \"useful\" for sufficiently many periods.\n\nProposition 5 (Asymptotic Payoff Selection). Let $c^{*}(\\delta) \\in \\Omega$. There exists exists $\\bar{\\tau}>0$, $\\bar{\\delta}<1$, such that for all $(\\bar{\\tau}, \\bar{\\delta})$-limiting $\\left(P_{\\theta}, \\delta\\right)$ and $\\varepsilon>0$, there exists time $T(\\varepsilon)$ such that for\n\nany equilibrium $\\sigma^{*}$ and $t \\geq T(\\varepsilon)$,\n\n$$\n\\frac{\\left|\\mathcal{V}\\left(\\sigma^{*} \\mid h_{t}\\right)-V_{H}^{*}(\\delta)\\right|}{V_{H}^{*}(\\delta)}<\\varepsilon\n$$\n\nwhenever $\\nu_{t}\\left(\\omega_{c^{*}(\\delta)} \\mid h_{t}\\right)>0$. Moreover, $\\mathbb{P}_{t}^{\\sigma^{*}}\\left(\\left\\{h_{t}: \\nu_{t}\\left(\\omega_{c^{*}(\\delta)} \\mid h_{t}\\right)>0\\right\\}\\right)>0$ for all $t$.\nThe proof can be found in Appendix A. . It follows from an application of the entropy bounds of Gossner (2011) to show that, so long as the regime has played the commitment action sufficiently many times, then it must be that even in mixed-strategy Nash equilibria, short-run players play strategies close to $x^{*}\\left(c^{*}(\\delta)\\right)$, which renders the role of exit secondorder. Interpreted in the context of the motivating example, Proposition 5 implies that regimes which have been historically cruel are more likely to reap larger gains from future cruelty, regardless of the equilibrium agents play.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 5 Discussion \n\nThis paper builds and analyzes a model of repeated regime change where successful revolution ends the game, and develops tools to handle the payoff set even when the discount rate may be effectively bounded away from 1. I show multiplicity can render any meaningful predictions irrelevant, even with stringent equilibrium refinements (e.g. restricting to Markov equilibria). This indeterminancy remains even if the regime has incentives for reputation formation, because stage-game strategic complementarities outweigh any possible role for the regime. However, if reputational incentives are combined with small idiosyncratic uncertainty in an exogenous fundamental, then the regime can exactly attain their Stackelberg payoff in the pure Nash (Markov) equilibrium, highlighting the interplay between transient, idiosyncratic uncertainty (over fundamentals and short-run agent actions) and structural uncertainty (over long-run agent actions) in refining predictions about repeated equilibrium play. To accentuate this point, I show a folk theorem holds even with the Markovian refinement so long as the regime faces no reputational incentives, highlighting the necessity of both sources of uncertainty in obtaining unique predictions.\n\nTable 5 summarizes the main theoretical contribution of the paper.\n\n![table_0](table_0)\n\nThere are several promising directions for future research. First, allowing for the cost function $\\kappa$ to depend on the size of the attack seems natural; in particular, identifying conditions on the joint cost function $\\kappa(A, c)$ so that the boundary conditions are still satisfied seems like an easy way to generalize the result. Second, endogenizing formally the announcement stage mentioned in the discussion succeeding Theorem 1 may be a natural next step to handle more general sets of commitment types. Third, allowing for the regime to be replaced by a different regime - whose value to the populace may be random and who may be subject to be replaced by revolution again (i.e. in a method similar to Acemoglu and Robinson (2000)) may allow for richer dynamics.\n\nFinally, my results may have implications for various supermodular games where there is a costly action that may lead to exit in the game which can extend beyond regime change. For example, macroeconomic models of trade wars, tariffs levied by a large economy decrease surplus for all countries but may be an effective way to extract concessions from smaller economies, though there is some probably the aggressor may be cut out from trade flows and forced into autarky. In speculative attacks against a currency, as in Huang (2017), the head of the central bank may be removed if they fail to defend against a speculative attack, and hence may be incentivized to take particularly costly actions. Finally, in models of extrajudicial non-state violence (e.g. terrorism) where commitment actions increase the probability of exit, it may be that reputational incentives counterbalance violence and lead to milder predictions, in contrast to the strongman effect (Proposition 2 in this model). Formalizing the specific economic interactions behind each of the settings above and dealing with their particularly idiosyncracies is a further potential direction for future work.",
      "tables": {
        "table_0": "|  | No Reputation | Reputation |\n| :--: | :--: | :--: |\n| (Pure) Nash, No Global Game | Folk Theorem | Folk Theorem |\n| (Pure) Nash, Global Game | Folk Theorem | Best Equilibrium |\n| Markov, No Global Game | Folk Theorem | Folk Theorem |\n| Markov, Global Game | Worst Equilibrium | Best Equilibrium |"
      },
      "images": {}
    },
    {
      "section_id": 8,
      "text": "# Appendix A: Omitted Proofs\n## PROOF OF PROPOSITION 1\n\nProof. Fix a value $c$, and note the induced stage-game is a finite monotone (decreasing) supermodular game with dominance regions $[-\\infty, 0)$ and $(1, \\infty]$. This implies by the proof of Theorem 1 of Frankel et al. (2003) that for all $\\varsigma$ sufficiently small, $s(x, c)=\\mathbf{1}\\left\\{x<x^{*}(c)\\right\\}$ is the unique equilibrium strategy profile.\n\nNext, let $U_{i}\\left(c, x, x^{*}\\right)$ be the interim utility differential between taking $a=1$ and $a=0$ for an agent observing signal $x$ when players use a cutoff strategy $x^{*}$ and the cost of failed revolution is $c$. In equilibrium, it must be that $U_{i}\\left(c, x^{*}(c), x^{*}(c)\\right)=0$. To ensure that $x^{*}(c)$ is differentiable and strictly increasing, we appeal to the implicit function theorem; some algebra gives that $x^{*}(c)$ is implicitly defined as the solution to the equation\n\n$$\nU_{i}(c, x, x)=(1+c) F_{\\theta}\\left(F_{x}(x \\mid x) \\mid x\\right)-c=0\n$$\n\nWe have that\n\n$$\n\\frac{\\partial U_{i}}{\\partial c}=F_{\\theta}\\left(F_{x}(x \\mid x) \\mid x\\right)-1 \\text { and } \\frac{\\partial U_{i}}{\\partial x}=(1+c)\\left(F_{\\theta}\\left(F_{x}(x \\mid x) \\mid x\\right) \\cdot f_{x}(x \\mid x) \\cdot \\frac{\\partial}{\\partial 2} F_{\\theta}\\left(F_{x}(x \\mid x) \\mid x\\right) \\cdot \\frac{\\partial}{\\partial 2} F_{x}(x \\mid x)\\right)\n$$\n\nwhere $\\frac{\\partial}{\\partial 2}$ refers to the derivative in the conditioning argument. Note the derivative in $c$ is strictly negative as $P_{\\theta}(\\cdot)<1$ always; meanwhile, the second term is strictly negative by Assumption 1. Thus, the implicit function theorem implies\n\n$$\n\\frac{\\partial x^{*}(c)}{\\partial c}=-\\frac{\\partial U_{i}}{\\partial c}\\left(\\frac{\\partial U_{i}}{\\partial x}\\right)^{-1}<0\n$$\n\nas desired. The last statement follows similarly. Regime change only occurs if $F_{x}\\left(x^{*}(c) \\mid \\theta\\right)>\\theta$, so $\\theta^{*}(c)$ is implicitly defined by the equation $F_{x}\\left(x^{*}(c) \\mid \\theta\\right)-\\theta=0$. Differentiating yields\n\n$$\n\\frac{d}{d \\theta}\\left(F_{x}\\left(x^{*}(c) \\mid \\theta\\right)-\\theta\\right)=\\frac{\\partial}{\\partial 2} F_{x}\\left(x^{*}(c) \\mid \\theta\\right)-1<0\n$$\n\nas $\\frac{\\partial}{\\partial 2} F_{x}(x \\mid \\theta)<0$ for all $x$; this then implies $\\theta^{*}(c)$ is globally $\\mathcal{C}^{1}$. Applying the implicit function theorem again implies its derivatives is given by\n\n$$\n\\frac{\\partial \\theta^{*}(c)}{\\partial c}=-\\left(f_{x}\\left(x^{*}(c) \\mid \\theta\\right) \\frac{\\partial x^{*}(c)}{\\partial c}\\right)\\left(\\frac{\\partial}{\\partial 2} F_{x}\\left(x^{*}(c) \\mid \\theta\\right)-1\\right)^{-1}<0\n$$\n\nand so higher costs $c$ decrease the region of states where attacks are successful.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# PROOF OF PROPOSITION 2 \n\nProof. Assumption 2 implies there exists a compact interval of the form $[0, \\bar{\\kappa}], \\bar{\\kappa}<\\infty$ from which $c^{*}(\\delta)$ must reside, for all $\\delta$. This is because as $c$ grows arbitrarily large, $1-\\tilde{\\kappa}(c)$ (and hence $V_{H}(c, \\delta)$ ) becomes negative for any $\\delta$, which is outperformed by $c=0$ which guarantees a payoff of $V_{L}^{*}(\\delta)=\\frac{\\bar{P}_{\\delta}(1)}{1-\\delta \\bar{P}_{\\theta}(1)}>1$. Since $c$ must be chosen from a compact interval and $V_{H}(c, \\delta)$ is continuous in both $c$ and $\\delta, c^{*}(\\delta)$ is nonempty, compact-valued, and upper hemi-continuous by Berge's theorem of the maximum.\n\nFinally, to prove that $c^{*}(\\delta)$ is increasing in the strong-set order, it is sufficient by Milgrom and Shannon (1994) to show $V_{H}(c, \\delta)$ is single crossing. Fix $c^{\\prime}>c$ and $\\delta$ such that $V_{H}\\left(c^{\\prime}, \\delta\\right)-V_{H}(c, \\delta) \\geq 0$; it is sufficient to show this is true for any $\\delta^{\\prime}>\\delta$. To do this, we rely on the following lemma ${ }^{17}$.\n\nLemma 2. Let $\\left\\{b_{t}\\right\\}_{t \\in \\mathbb{N}} \\subset \\mathbb{R}$ be a bounded sequence of real numbers such that $b_{t}<0$ if and only if $t \\leq T$ for some $T \\in \\mathbb{N}$. Then for any $\\delta_{1}<\\delta_{2}$,\n\n$$\n\\sum_{t=0}^{\\infty} \\delta_{1}^{t} b_{t} \\geq 0 \\Longrightarrow \\sum_{t=0}^{\\infty} \\delta_{2}^{t} b_{t}>0\n$$\n\nProof. Note\n\n$$\n\\sum_{t=0}^{\\infty} \\delta_{1}^{t} b_{t}=\\delta^{T} \\sum_{t=0}^{\\infty} \\delta_{1}^{t-T} b_{t} \\geq 0 \\Longleftrightarrow \\sum_{t=0}^{\\infty} \\delta_{1}^{t-T} b_{t}\n$$\n\nAs a function of $\\delta$, this series is absolutely convergent for all $\\delta \\in(0,1)$ and the partial sums converge uniformly on this interval so we can differentiate under the summation: thus,\n\n$$\n\\frac{d}{d \\delta} \\sum_{t=0}^{\\infty} \\delta_{1}^{t-T} b_{t}=\\sum_{t=0}^{\\infty}(t-T) \\delta_{1}^{t-T-1} b_{t}<\\infty\n$$\n\nwhere we use the fact $b_{t}$ is bounded to guarantee the sum is well-defined. Note each term in this summation is nonnegative. If $t \\leq T$, then $t-T \\leq 0$ and $b_{t}<0$, so $(t-T) \\delta^{t-T-1} b_{t} \\geq 0$, while if $t>T$, then $(t-T) \\delta^{t-T-1}>0$ and $b_{t}>0$ as well. Hence the derivative is strictly positive. We thus have that\n\n$$\n\\sum_{t=0}^{\\infty} \\delta_{2}^{t-T} b_{t}>0 \\Longrightarrow \\delta_{2}^{T} \\sum_{t=0}^{\\infty} \\delta_{2}^{t-T} b_{t}=\\sum_{t=0}^{\\infty} \\delta_{2}^{t} b_{t}>0\n$$\n\nwhere we use the fact $\\delta_{2}>\\delta_{1}$, and finally multiply by $\\delta_{2}^{T}>0$.\nNow set\n\n$$\nb_{t}=\\left(1-\\kappa\\left(c^{\\prime}\\right)\\right)\\left(\\bar{P}_{\\theta}\\left(c^{\\prime}\\right)\\right)^{t+1}-(1-\\kappa(c))\\left(\\bar{P}_{\\theta}(c)\\right)^{t+1}\n$$\n\n[^0]\n[^0]:    ${ }^{17}$ To the best of our knowledge, this simple mathematical result is novel.\n\nso that\n\n$$\nb_{t}>0 \\Longleftrightarrow \\frac{1-\\kappa\\left(c^{\\prime}\\right)}{1-\\kappa(c)}>\\left(\\frac{\\bar{P}_{\\theta}(c)}{\\bar{P}_{\\theta}\\left(c^{\\prime}\\right)}\\right)^{t+1}\n$$\n\nBecause $c^{\\prime}>c, \\theta^{*}(c)>\\theta^{*}\\left(c^{\\prime}\\right)$ by Proposition 1, so the hand side is decreasing monotonically to 0 as $t \\rightarrow \\infty$. Moreover, the left hand side of the inequality is strictly positive and independent of $t$. Thus, there exists a unique $T$ (possibly 0 ) such that $b_{t}>0$ for all $t>T$, so Lemma 2 implies $V_{H}\\left(c^{\\prime}, \\delta^{\\prime}\\right)-V_{H}\\left(c, \\delta^{\\prime}\\right)>0$. Thus $V^{*}$ is strictly single-crossing in $(c, \\delta)$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# PROOF OF LEMMA 1 \n\nProof. Fix any distribution $P_{\\theta}$ with mean $\\mu$. The principal's commitment problem for any $\\delta>0$ is given by\n\n$$\n\\max _{c \\in \\mathbb{R}_{+}} \\frac{\\bar{P}_{\\theta}(c)}{1-\\delta \\bar{P}_{\\theta}(c)}(1-\\kappa(c)) ; \\text { with derivative in } c \\text { of } \\frac{1}{\\left(1-\\delta \\bar{P}_{\\theta}(c)\\right)^{2}}-\\kappa^{\\prime}(c) \\frac{\\bar{P}_{\\theta}(c)}{\\left(1-\\delta \\bar{P}_{\\theta}(c)\\right)}\n$$\n\nClearly this derivative is strictly positive when $c=0$; when $c>0$, we can rearrange it to get that it is strictly positive so long as\n\n$$\n\\frac{1}{\\kappa^{\\prime}(c)}>\\frac{1}{4 \\delta} \\geq\\left(1-\\delta \\bar{P}_{\\theta}(c)\\right) \\bar{P}_{\\theta}(c)\n$$\n\nThe first inequality is a sufficient condition for the derivative to be positive, while the second is true by noting $\\bar{P}_{\\theta}(c) \\in[0,1]$ since $\\bar{P}_{\\theta}(c)$ is a probability and $\\delta<1$. Thus, so long and maximizing the term $(1-\\delta x) x$. Standard first order logic then implies there is some $\\bar{c}>0$, where for any $\\varsigma$, any $P_{\\theta}$, and any $\\delta, c^{*}(\\delta, \\varsigma) \\geq \\bar{c}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "## PROOF OF PROPOSITION 3\n\nProof. Fix prior $P_{\\theta}$, discount rate $\\delta>0$ and belief $\\nu_{0} \\in \\Delta(\\Omega)$. Throughout, we suppose all players have access to a public coordinating device at time $t=0$ to convexify the set of expected equilibrium payoffs.\n\nStep 1: Characterizing $E\\left(\\delta, \\nu_{0}, 0\\right)$. Suppose $\\theta \\in(-\\infty, 1)$ is publically observable. In the stage game, for any $c \\in \\mathbb{R}_{+}$, there is an equilibrium where all short-run players attack, since $\\theta<A=1$, and $1>0$ (so no short-run players prefer to deviate). This observation implies that in the repeated game, there exists an equilibrium $\\sigma^{*}$ such that at all on-path histories $h_{t}$ and beliefs $\\nu_{t}\\left(\\omega_{R} \\mid h_{t}\\right) \\in[0,1]$, short-run players attack whenever $x=\\theta<1$. Because attacks are independent of the regime's action or their reputation, they must repeatedly play their stage-game dominant strategy in equilibrium, which guarantees a payoff of $\\sum_{t=0}^{\\infty} \\delta^{t} P_{\\theta}(1)^{t+1}$.\n\nThis must be the regime's worst possible equilibrium payoff since short-run players are attacking as often as possible. Note this equilibrium payoff is attained in Markov strategies, since in particular it is simply a repeated stage game equilibrium.\n\nNext, we consider the best equilibrium payoff which is attainable in the repeated game. In the stage game, for any $\\theta \\in[0, \\infty)$, for any $c \\in[0, \\infty)$, there is an equilibrium in which no short-run players attack (this equilibrium is strict so long as $c>0$ ). This implies that there is an equilibrium where the long-run player's payoff is $\\sum_{t=0}^{\\infty} \\delta^{t} P_{\\theta}(0)^{t+1}$. This must be the regime's best payoff, since agents are attacking as little as possible and they are playing their dominant action. Moreover, this equilibrium also has a (pure) Markov implementation since it is a repeated stage-game equilibrium.\n\nClearly, neither construction relied on any facts about the prior belief $\\nu_{0}$, and so this result holds regardless of the prior (in particular, including the case where $\\nu_{0}=\\delta_{\\omega_{R}}$. Finally, as $\\tau \\rightarrow 0, P_{\\theta}(1) \\rightarrow 1$ and $P_{\\theta}(0) \\rightarrow 0$ as $\\mu \\in(0,1)$. Thus, the expected payoffs in the best and worst equilibria in the limit are\n\n$$\n\\lim _{\\tau \\rightarrow 0, \\delta \\rightarrow 1}\\left[\\frac{P_{0}(\\theta)}{1-\\delta P_{0}(\\theta)}, \\frac{P_{1}(\\theta)}{1-\\delta P_{1}(\\theta)}\\right]=[0, \\infty)\n$$\n\nThis finishes the proof.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# PROOF OF PROPOSITION 4 \n\nProof. We start with the second equality. Suppose $\\nu_{0}\\left(\\omega_{R}\\right)=1$, so that $\\nu_{t}\\left(\\omega_{R} \\mid h_{t}\\right)=1$ for any history and any strategy profile. This implies future continuation play must be constant, and thus an action $c$ is part of an equilibrium under $\\sigma$ at history $h_{t}$ if and only if\n\n$$\n1-\\kappa(c)+\\delta \\mathcal{V}\\left(\\sigma \\mid h_{t} \\cup\\{c\\}\\right) \\geq 1+\\delta \\mathcal{V}\\left(\\sigma \\mid h_{t} \\cup\\{0\\}\\right) \\Longleftrightarrow 0 \\geq \\kappa(c)\n$$\n\nThis implies $c=0$ at every history reached on path in any equilibrium $\\sigma$ where the regime has no incentive to form reputation. But then it must be that short-run players use cutoff $x^{*}(0)$ at every history, so their payoff is $V_{H}(0, \\delta)=V_{L}^{*}(\\delta)$.\n\nThe second argument. Fix any Nash equilibrium of the repeated game where it is common knowledge the long-run player is rational. Clearly the Markov perfect equilibrium is one equilibrium and is the long-run player's minimax payoff. The best equilibrium payoff that can be attained in some equilibrium $\\sigma$ must satisfy two constraints:\n(1) $1-\\kappa\\left(\\sigma_{1}^{*}\\left(h_{t}, \\omega_{R}\\right)\\right)+\\delta V_{t+1} \\geq 1+\\delta V_{L}^{*}(\\delta)$\n(2) $\\sigma_{2, t}^{*}\\left(h_{t}\\right)(x)=\\mathbf{1}\\left\\{x \\leq x^{*}\\left(\\mathbb{E}_{h_{t}}[c]\\right)\\right\\}$\n\nwhere $V_{t+1}$ is some feasible continuation value. But the optimal commitment posture need not satisfy the first constraint, and is thus a relaxed problem. Hence $E\\left(\\delta, \\nu_{0}, \\varsigma\\right) \\subset\\left[V_{L}^{*}(\\delta), V_{H}^{*}(\\delta)\\right]$. Moreover, equality of this set must hold whenever the conditions of Theorem 1 are true, since in that case there is an equilibrium where the regime plays $c^{*}(\\delta)$ in every period on-path, sustained with the punishment to get $V_{L}^{*}(\\delta)$ if they deviate.\n\nFinally, to show there are a continuum of Nash equilibria, we construct pure public perfect equilibria of the following form:\n(1) In every period and at every history $h_{t}=\\{c\\}_{s=0}^{t-1}$, the regime chooses some cost $c$ and short-run players play $\\mathbf{1}\\left\\{x<x^{*}(c)\\right\\}$.\n(2) At all other histories, the regime plays 0 and short-run players play $\\mathbf{1}\\left\\{x<x^{*}(0)\\right\\}$.\n\nClearly short-run players are best replying to long-run players. At any history at which an action other than $c$ is played, the regime also has no profitable deviations. Finally, at any history at which $c$ is played, the regime has no profitable one-shot deviations so long as\n\n$$\n1-\\kappa(c)+\\delta V(c, \\delta) \\geq 1+\\delta V_{L}^{*}(\\delta)\n$$\n\nAt $c=0$, this equality clearly holds; differentiating both sides at 0 also gives the derivative of the left-hand side is larger, as\n\n$$\n-0+\\frac{\\delta}{\\left[\\left(1-\\delta\\left(1-P_{\\theta}\\left(\\theta^{*}(0)\\right)\\right)\\right]^{2}}>0\n$$\n\nThus, first-order arguments imply there exist a continuum of values of $c$ at which the strategy profile described is a (pure-strategy) Nash equilibrium without reputation.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# PROOF OF THEOREM 1 \n\nProof. Throughout the proof of the result, we will sometimes make explicit the dependence that equilibrium objects $\\left(\\theta^{*}, x^{*}, c^{*}\\right)$ have on the underlying informational environment, parameterized by $\\left(P_{\\theta}, \\varsigma\\right)$. Whenever there is no loss of clarity without omitting these arguments though, we will do so, keeping in mind the explicit dependence.\n\nWe first prove two auxiliary lemmas which will later help the analysis.\nLemma 3. There exists some $\\eta>0$, chosen uniformly over all discount rates and prior distributions $P_{\\theta}$ where $\\mathbb{E}\\left[P_{\\theta}\\right]>\\frac{1}{1+\\bar{c}}$, such that for any $\\delta>0$ and sufficiently small noise $\\varsigma<\\zeta\\left(P_{\\theta}(\\theta, \\delta)\\right), \\theta^{*}\\left(c^{*}\\left(\\delta, P_{\\theta}, \\varsigma\\right)\\right)<\\mu-\\eta$.\n\nProof. Since $\\mu>\\frac{1}{1+\\bar{c}}, \\frac{1}{1+c^{*}}<\\mu$ for all possible feasible commitment postures $c^{*}$ attained by any prior $P_{\\theta}$, discount rate $\\delta$, and noise $\\vartheta$ by Lemma 1 . Thus,\n\n$$\n\\lim _{\\varsigma \\rightarrow 0} F_{x}\\left(x^{*}\\left(c^{*}(\\delta, \\varsigma)\\right) \\mid \\mu\\right)-\\mu \\leq \\lim _{\\varsigma \\rightarrow 0} F_{x}\\left(\\left.\\frac{1}{1+\\bar{c}} \\right\\rvert\\, \\mu\\right)-\\mu \\leq \\frac{1}{2}-\\mu<0\n$$\n\nfor every distribution $P_{\\theta}$ and discount rate $\\delta>0$. The first inequality follows from the standard result that global games select the risk-dominant equilibrium, so $\\lim _{\\varsigma \\rightarrow 0} x^{*}(c)=\\frac{1}{1+c}$ (see Morris and Shin (2003)) and Lemma 1, the second from the observations that $F_{x}=\\theta+\\sigma \\varepsilon$ is symmetric arbitrarily precise noise and $\\frac{1}{1+\\bar{c}}<\\mu$, and the final one from the assumption $\\mu>\\frac{1}{2}$. Thus, $\\theta^{*}<\\mu$ in the limit, since we know $F_{x}(x \\mid \\theta)-\\theta$ is decreasing in $\\theta$ and thus for the cutoff to hold it must be greater than $\\mu$.\n\nNext, we show there does not exist $P_{\\theta}^{n}$ and $\\delta^{n}$ such that $\\lim \\sup _{n \\rightarrow \\infty} \\lim _{\\varsigma \\rightarrow 0} \\theta^{*}\\left(c^{*}\\left(\\delta^{n}, P_{\\theta}^{n}, \\varsigma\\right)\\right)=$ $\\mu$. Assume not, and (passing to a subsequence as necessary) extract some tuple $\\left(\\delta^{n}, P_{\\theta}^{n}, \\varsigma^{n}\\right)$ such that $\\theta^{*}\\left(c^{*}\\left(\\delta^{n}, P_{\\theta}^{n}, \\varsigma^{n}\\right)\\right) \\rightarrow \\mu$. Taking $N$ large enough that $\\theta^{*}\\left(c^{*}\\left(\\delta^{N}, p_{\\theta}^{N}, \\varsigma^{N}\\right)\\right) \\in\\left(\\frac{1}{1+\\bar{c}}, \\mu\\right)$,\n\n$$\n\\begin{gathered}\n0=F_{x}\\left(x^{*}\\left(c^{*}\\left(\\delta^{N}, P_{\\theta}^{N}, \\varsigma^{N}\\right)\\right) \\mid \\theta^{*}\\left(c^{*}\\left(\\delta^{N}, P_{\\theta}^{N}, \\varsigma^{N}\\right)\\right)\\right)-\\theta^{*}\\left(c^{*}\\left(\\delta^{N}, P_{\\theta}^{N}, \\varsigma^{N}\\right)\\right) \\\\\n\\leq F_{x}\\left(\\frac{1}{1+\\bar{c}} \\mid \\theta^{*}\\left(c^{*}\\left(\\delta^{N}, P_{\\theta}^{N}, \\varsigma^{N}\\right)\\right)\\right)-\\theta^{*}\\left(c^{*}\\left(\\delta^{N}, P_{\\theta}^{N}, \\varsigma^{N}\\right)\\right) \\leq \\frac{1}{2}-\\theta^{*}\\left(c^{*}\\left(\\delta^{N}, P_{\\theta}^{N}, \\varsigma^{N}\\right)\\right)<0\n\\end{gathered}\n$$\n\na contradiction, as otherwise $0<0$.\nLemma 4. Let $\\mu>\\frac{1}{1+\\bar{c}}$. There exists $\\bar{\\tau}, \\bar{\\delta}>0$ and a positive function $\\bar{\\varsigma}(\\delta, \\tau)$, vanishing as $\\delta, \\tau \\rightarrow 0$ such that if $\\operatorname{var}\\left(P_{\\theta}\\right)<\\bar{\\tau}, \\delta>\\bar{\\delta}, \\varsigma<\\bar{\\varsigma}(\\delta, \\tau)$, then\n\n$$\n\\delta\\left(V_{H}^{*}\\left(\\delta, P_{\\theta}, \\varsigma\\right)-V_{L}^{*}\\left(\\delta, P_{\\theta}, \\varsigma\\right)\\right) \\geq \\kappa\\left(c^{*}\\left(\\delta, P_{\\theta}, \\varsigma\\right)\\right)\n$$\n\nProof. Since $c^{*}(\\delta)$ is chosen optimally and $0 \\notin c^{*}(\\delta)$ for any $\\delta>0$ by Lemma 1, we know\n\n$$\n\\frac{\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)}{1-\\delta \\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)}\\left[1-\\kappa\\left(c^{*}(\\delta)\\right)\\right]-\\frac{\\bar{P}_{\\theta}(0)}{1-\\delta \\bar{P}_{\\theta}(0)}>0 \\Longrightarrow \\frac{\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)-\\bar{P}_{\\theta}(0)}{\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)}>\\kappa\\left(c^{*}(\\delta)\\right)\n$$\n\nWe want to show that $\\delta\\left(V_{H}^{*}(\\delta)-V_{L}^{*}(\\delta)\\right)>\\kappa\\left(c^{*}(\\delta)\\right)$ for sufficiently large $\\delta$, which by some algebraic rearranging is equivalent to the requirement\n\n$$\n\\frac{\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)-\\bar{P}_{\\theta}(0)}{1-\\delta \\bar{P}_{\\theta}(0)}>\\kappa\\left(c^{*}(\\delta)\\right)(1+(1-\\delta) \\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)\n$$\n\nTaking $\\delta$ large enough and noting $\\kappa\\left(c^{*}(\\delta)\\right) \\leq 1$ and $\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right) \\leq 1$, this inequality is implied by the inequality $\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)-\\bar{P}_{\\theta}(0)>\\kappa\\left(c^{*}(\\delta)\\right)$. But this is implied by the fact $\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)-\\bar{P}_{\\theta}(0)>$\n\n$\\left.\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right) \\kappa\\left(c^{*}(\\delta)\\right)\\right.$ so long as $\\left.\\kappa\\left(c^{*}(\\delta)\\right)\\left[1-\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)\\right]\\right]$ is sufficiently small. A computation gives\n\n$$\n\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)=1-P_{\\theta}\\left(\\theta^{*}\\left(c^{*}(\\delta)\\right)\\right)>1-P_{\\theta}(\\mu-\\eta) \\geq 1-\\frac{\\operatorname{var}\\left(P_{\\theta}\\right)}{\\eta^{2}}>1-\\frac{\\bar{\\tau}}{\\eta}\n$$\n\nThe first equality is by definition; the second by Lemma 3, and the final one by Chebychev's inequality (with strictly due to the symmetry assumption on $P_{\\theta}$, since $1-P_{\\theta}(\\mu+\\eta)>0$ as well. Since $\\eta$ can be chosen independently of $P_{\\theta}, \\delta$ (so long as $\\varsigma$ vanishes), this implies that $\\bar{P}_{( }\\left(\\theta\\left(c^{*}(\\delta)\\right)\\right) \\rightarrow 1$ for any $\\delta>0$ by taking $\\bar{\\tau}$ to be sufficiently small. This implies the lemma, noting the cutoffs $\\bar{\\tau}$ and $\\bar{\\delta}$ can be chosen independently of each other.\n\nThe proof of the theorem finally follows from the following two steps.\n\nStep 1. There are no mixed MPE. Suppose not, and fix an MPE $\\sigma^{*}$ which mixes at some history $h_{t}$. First, $\\sigma_{1}^{*}\\left(h_{t}, \\omega_{R}\\right)$ can never support $c \\notin\\left\\{0, c^{*}(\\delta)\\right\\}$; this is because playing this $c$ reveals rationality and guarantees continuation payoff $V_{L}^{*}(\\delta)$, so the rational long-run player can do strictly better by playing $c^{*}(\\delta)$.\n\nThis observation implies there are only two types of (potentially) on-path histories:\n(1) \"Clean\" histories at which $\\nu\\left(\\omega_{R} \\mid h_{t}\\right)<1$ and $h_{t}=\\left\\{c^{*}(\\delta)\\right\\}_{s=0}^{t-1}$, or\n(2) \"Revealed\" histories at which $\\nu\\left(\\omega_{R} \\mid h_{t}\\right)=1$ and there exists some time $s<t$ at which the regime played 0 .\n\nClearly mixing must be impossible at revealed histories since there are no reputational incentives and $c^{*}(\\delta)=0$ is a dominant action. Moreover, there cannot be an infinite sequence of clean on-path histories at which the regime mixes. Suppose not, so there is a sequence of times $\\left\\{t_{n}\\right\\}$ under which $\\sigma^{*}\\left(h_{t_{n}}, \\omega_{R}\\right)$ mixes at all time $t_{n}$ clean histories. Since mixing is a statistically distinct action from always playing $c^{*}(\\delta), \\lim _{t \\rightarrow \\infty} \\nu_{t}\\left(\\omega_{R} \\mid h_{t}\\right) \\rightarrow 0$ among all clean histories (noting it is 1 at all other histories reached on path) by standard martingale convergence theorem arguments. However, to mix at some clean history $h_{t}$, the long-run player must be indifferent between the two actions, so\n\n$$\n1-\\kappa\\left(c^{*}(\\delta)\\right)+\\delta \\mathcal{V}\\left(\\sigma^{*} \\mid h_{t+1}\\right)=1+\\delta V_{L}^{*}(\\delta)\n$$\n\nwhere $h_{t+1}$ is the time- $t$ clean history. However, the short-run player's best reply (and thus the long-run player's value from playing $\\left.c^{*}(\\delta)\\right)$ is continuous in $\\nu_{t}\\left(\\omega_{R} \\mid h_{t}\\right)$ in Markov equilibria, and thus\n\n$$\n\\lim _{h_{t} \\rightarrow \\infty ;} \\lim _{h_{t} \\text { is clean }} \\mathcal{V}\\left(\\sigma^{*} \\mid h_{t}\\right) \\rightarrow V_{H}^{*}(\\delta)\n$$\n\nThus indifference cannot be sustained asymptotically by the strict inequality in Lemma 4.\nHence, there is a final last time $T$ at which the regime mixes at a clean history. After this history $T$, the agent deterministically plays $c^{*}(\\delta)$ at all subsequent clean history, so their continuation value after playing $c^{*}(\\delta)$ must be $V_{H}^{*}(\\delta)$. But then for the agent to be indifferent at $T$, we need $1-\\kappa\\left(c^{*}(\\delta)\\right)+\\delta V_{H}^{*}(\\delta)=1+\\delta V_{L}^{*}(\\delta)$, which is impossible, finishing the proof no MPE mixes at any on-path history (and thus the proof of the theorem).\n\nStep 2. All pure equilibria are $c^{*}(\\delta)$-pooling. Fix any Nash equilibrium $\\sigma^{*}$ of the game. Consider a clean history $h_{t}=\\left\\{c^{*}(\\delta)\\right\\}_{s=0}^{t-1}$ (this can be empty, i.e. period $t=0$ is always clean). Since $c^{*}(\\delta) \\in \\operatorname{supp}\\left(\\nu_{0}\\right), \\nu_{t}\\left(\\omega_{c^{*}(\\delta)} \\mid h_{t}\\right)>0$. Suppose $\\sigma_{1}^{*}\\left(h_{t}, \\omega_{R}\\right)$ does not support $c^{*}(\\delta)$, then the long-run player reveals that they are rational. If instead they played $c^{*}(\\delta)$, then at the time $t+1$ clean history $h_{t+1}=\\left\\{c^{*}(\\delta)\\right\\}_{s=0}^{t}, \\nu_{t}\\left(\\omega_{c^{*}(\\delta)} \\mid h_{t+1}\\right)$ and thus short-run players always best respond to $c^{*}(\\delta)$ afterwards regardless of long-run player actions. For this deviation to be impossible, it must be that\n\n$$\n1-\\kappa\\left(c^{*}(\\delta)\\right)+\\delta \\frac{\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)}{1-\\delta \\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)} \\leq 1+\\delta V_{H}^{*}(\\delta)=1+\\delta \\frac{\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)}{1-\\delta \\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)}\\left(1-\\kappa\\left(c^{*}(\\delta)\\right)\\right)\n$$\n\nwhere the left hand side is the payoff from playing this deviation and then 0 and the left hand side is the highest possible continuation payoff (conditional on surviving) in equilibrium (see 3). Rearranging gives this inequality is equivalent to the requirement\n\n$$\n\\kappa\\left(c^{*}(\\delta)\\right)\\left(\\delta \\frac{\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)}{1-\\delta \\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)}-1\\right) \\leq 0 \\Longleftrightarrow 2 \\delta \\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right) \\leq 1\n$$\n\nThis is impossible for $\\delta$ sufficiently close to 1 so long as $\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)>\\frac{1}{2}$. But of course this must be true as $P_{\\theta}$ is symmetric and $\\theta^{*}(c)<\\mu-\\eta$ by Lemma 3 . Thus, in any Nash equilibrium, at any clean history, the regime's strategy must support $c^{*}(\\delta)$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# PROOF OF COROLLARY 2 \n\nProof. Fix $P_{\\theta}, \\delta$, and $\\varsigma$ sufficiently small that there is uniqueness in the stage game for fixed c. Following the outline of the argument in Theorem 1, it is sufficient to show there exist $c>0$ such that $\\delta\\left(V_{H}(c, \\delta)-V_{L}^{*}(\\delta)\\right)>\\kappa(c)$ for these parameters, as then the strategy profile where $\\sigma_{1}^{*}\\left(h_{t}, \\omega_{R}\\right)$ is consistently $c$ at every on-path history (and 0 otherwise) is the unique MPE (along with $\\sigma_{2}^{*}\\left(h_{t}\\right)(x)=\\mathbf{1}\\left\\{x<x^{*}(c)\\right\\}$ ). But this follows simply by recalling from the\n\nlast part of Proposition 4 that\n\n$$\n\\frac{\\partial}{\\partial c} \\delta\\left(V_{H}(c, \\delta)-V_{L}(\\delta)\\right)>\\kappa^{\\prime}(c)\n$$\n\nin a neighborhood around 0 for any discount rate $\\delta$ and any $\\tau$. Finally, the computation above is independent of $\\nu_{0}$ implying the existence of $\\varepsilon(\\delta)$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "# PROOF OF PROPOSITION 5 \n\nProof. First note Step (2) of Theorem (1) implies the following lemma.\nLemma 5. There exists $\\bar{\\tau}>0, \\bar{\\delta}<1$ such that for all $(\\bar{\\tau}, \\bar{\\delta})$-limiting $\\left(P_{\\theta}, \\delta\\right)$, and any equilibrium $\\sigma^{*}, c^{*}(\\delta) \\in \\operatorname{supp}\\left(\\sigma_{1}^{*}\\left(h_{t}, \\omega_{R}\\right)\\right)$ at any history where $\\nu_{t}\\left(\\omega_{c^{*}(\\delta)} \\mid h_{t}\\right)>0$.\n\nSince $c^{*}(\\delta) \\in \\Omega, \\nu_{0}\\left(\\omega_{c^{*}(\\delta)}\\right)>0$, and hence $c^{*}(\\delta)$ is played with positive probability. Inducting on this time implies, for any every time $t$, the is a positive probability of a clean history $h_{t}=\\left\\{c^{*}(\\delta)\\right\\}_{s=0}^{t-1}$. Moreover, these are the only histories at which $\\nu_{t}\\left(\\omega_{c^{*}(\\delta)} \\mid h_{t}\\right)>0$. From here, standard arguments (see Gossner (2011)) imply that for any equilibrium strategy $\\sigma^{*}$, there is a time $T(\\varepsilon)$ which depends only on the prior $\\nu_{0}$ such that, conditional on setting a time $t>T(\\varepsilon)$ clean history, short-run players must be $\\varepsilon$ best replying to $c^{*}(\\delta)$. Since the cutoffs $x^{*}(c)$ and $\\theta^{*}(c)$ are continuous in $c$, this implies that the regime's stage-payoff at every time period $t>T(\\varepsilon)$ payoff is arbitrarily close to $V_{H}^{*}(\\delta)$. Thus, the regime's continuation payoff starting from history $h_{t}$ is bounded from below by $V_{H}^{*}(\\delta)-\\frac{1}{1-\\delta \\varepsilon}$. Since $\\frac{1}{1-\\delta \\varepsilon}$ is fixed for fixed $\\delta$ and $\\varepsilon,\\left(V_{H}^{*}(\\delta)\\right)^{-1} \\frac{1}{1-\\delta \\varepsilon}$ vanishes as we take $\\delta$ large, finishing the argument.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "## Appendix B: Supplementary Material\n## B.I: Reputation with Small $\\boldsymbol{\\mu}$.\n\nThe main goal of this subsection is to attain a necessary and sufficient condition under which a version of Theorem 1 holds. In particular, instead of identifying limiting conditions on primitives under which the dynamic enforcement constraint holds (i.e. Lemmas 1, 3, and 4), I identify directly the necessary and sufficient version of the dynamic enforcement constraint that implies Theorem 1. The statement below is a corollary of Lemma 3.\n\nCorollary B.1. Let $\\Omega=\\left\\{0, c^{*}(\\delta)\\right\\}$ and fix any $P_{\\theta}$, any $\\varsigma$ small enough, and any $\\delta$. If\n\n$$\n\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)-\\bar{P}_{\\theta}(0)>\\kappa\\left(c^{*}(\\delta)\\right)\\left(1+(1-\\delta) \\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)\\right)\n$$\n\nthen the unique Markov perfect equilibrium is $c^{*}(\\delta)$-pooling.\n\nProof. Since $c^{*}(\\delta)$ is optimal, the computation in Lemma 3 implies that the dynamic enforcement constraint is exactly\n\n$$\n\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)-\\bar{P}_{\\theta}(0)>\\left(1-\\delta \\bar{P}_{\\theta}(0)\\right) \\kappa\\left(c^{*}(\\delta)\\right)\\left(1+(1-\\delta) \\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)\\right)\n$$\n\nBut of course because $\\left(1-\\delta \\bar{P}_{\\theta}(0)\\right)<1$, this is implied by the necessary condition in the statement of the result.\n\nThis result is independent of the choice of $\\mu$, so in particular it is true even if $\\mu<$ $\\min \\left\\{\\frac{1}{2}, \\frac{1}{1+c}\\right\\}$, the running assumption throughout the main body of the paper. Note that if $c^{*}(1)$ is the continuous extension of $c^{*}(\\delta)$ to 1 , then in the patient limit this simplifies to the requirement;\n\n$$\n\\bar{P}_{\\theta}\\left(c^{*}(1)\\right)-\\bar{P}_{\\theta}(0)>\\kappa\\left(c^{*}(\\delta)\\right)\n$$\n\nwhich highlights the role of non-normalized payoffs in affecting the dynamic enforcement constraints of the game. In particular, if instead the game was normalized by $(1-\\delta)$, then the necessary and sufficient condition would look more like\n\n$$\n\\lim _{\\delta \\rightarrow 1} \\delta\\left(\\bar{P}_{\\theta}\\left(c^{*}(\\delta)\\right)-\\bar{P}_{\\theta}(0)\\right)>(1-\\delta)\\left(\\kappa\\left(c^{*}(\\delta)\\right)\\right)\n$$\n\nwhich of course would be vacuously satisfied.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# B.II: The Game Without Exit \n\nThe purpose of this appendix is to show the importance of the exit structure in driving the richness of our results. We do this in two ways: first, by showing that there is still standard equilibrium indeterminancy in the non-global game, and then showing the Fudenberg and Levine (1989) reputation bounds are tight in the globalized game. This partially highlights the role that exit plays in our negative results, and distinguishes our results in the non-global game with exit from other reputation failure results (i.e. Ely and Valimaki (2003)).\n\nSuppose now that instead of the game ending, the long-run player receives some penalty $-M<0$ at any history where $A_{t}<\\theta_{t}$. The long-run player's optimal commitment action is now independent of their discount rate, and given by\n\n$$\nV_{H}^{*}(\\delta)=\\max _{t \\in \\mathbb{R}_{+}}\\left\\{(1-\\delta) \\sum_{t=0}^{\\infty} \\delta^{t}\\left(\\bar{P}_{\\theta}(c)[1-\\kappa(c)]\\right)\\right\\}\n$$\n\nAs in the discussion prior to Proposition 2, the regime does not benefit from a dynamic\n\ncommitment posture, and so we omit it. Let $c^{*}$ be the (largest) solution to the above problem.\nLet $\\mathcal{N}(c)$ be the set of all Nash equilibrium in the stage game when short-run players face an expected cost of failure given by $c$ (this is the setting analyzed by Proposition 1 with $\\varsigma$ set to 0 ). We immediately have the following result, recalling that there always exists an equilibrium in $\\mathcal{N}(c)$ where every short-run players attacks for any $\\theta<1$.\n\nProposition B. 1 (Fudenberg and Levine, 1989.). The long-run player's payoff in any Nash equilibrium of the repeated game is at least $V_{L}^{*}(\\delta)$.\n\nMoreover, it should be clear that the lower bound here is tight, following arguments similar to 3 , regardless of the reputational posture. What happens when the game is globalized?\n\nProposition B.2. $c^{*}=c^{*}(0)$; that is, the optimal commitment posture without exit is exactly the static optimum in the game with exit. Moreover, if $\\varsigma>0$, then as $\\delta \\rightarrow 1$, the long-run player's best payoff in any Nash equilibrium is their (unique) Stackelberg payoff.\n\nProof. Rewriting payoffs in the game without exit gives that they maximization problem is\n\n$$\n\\max _{c \\in \\mathbb{R}_{+}}\\left\\{(1-\\delta) \\frac{\\bar{P}_{\\theta}(c)[1-\\kappa(c)]}{1-\\delta}\\right\\}=\\bar{P}_{\\theta}(c)[1-\\kappa(c)]\n$$\n\nBut of course, this is exactly the static problem in the game with exit. Thus the solutions to the two problems must be the same.\n\nThe second part of the result follows by applying Fudenberg and Levine (1989) after noting that if $\\varsigma>0$, then the unique strategy profile in $\\mathcal{N}(c)$ is given by $\\mathbf{1}\\left\\{x<x^{*}(c)\\right\\}$.\n\nProposition B. 2 implies two things. First, the model with exit always features a higher rate of punishment than the one without, and in fact it may be that $c^{*}(0)=0$ even if $\\kappa^{\\prime}(0)=0$. In particular, this implies the rich dynamics underpinning the interaction between regime patience and violence (as in Proposition 2) do not appear here, underscoring the difference between canonical models and ours.\n\nSecond, it show that while is a similar payoff selection result in the game without exit, the behavioral qualities are quite distinct. Consequently, it accentuates that one of the core methodological contributions of Theorem 1 is developing a language to handle reputation formation in games where the regime's action affects their survival probability.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "# References \n\nAbreu, Dilip, and Faruk Gul. 2000. \"Bargaining and reputation.\" Econometrica 68 (1): 85-117. $10.2307 / 2999476$.\n\nAcemoglu, Daron, and James A. Robinson. 2000. \"Why Did the West Extend the Franchise? Democracy, Inequality, and Growth in Historical Perspective.\" The Quarterly Journal of Economics 115 (4): 1167-1199. 10.1162/003355300555042.\n\nAcemoglu, Daron, and James A. Robinson. 2006. \"Economic backwardness in political perspective.\" American Political Science Review 100 (1): 115-131.\n\nAngeletos, George-Marios, Christian Hellwig, and Alessandro Pavan. 2006. \"Signaling in a Global Game: Coordination and Policy Traps.\" Journal of Political Economy 114 (3): $452-484$.\n\nAngeletos, George-Marios, Christian Hellwig, and Alessandro Pavan. 2007. \"Dynamic Global Games of Regime Change: Multiplicity, Learning, and Timing of Attacks.\" Econometrica 75 (3): 711-756.\n\nAngeletos, George-Marios, and Alessandro Pavan. 2013. \"Selection-free predictions in global games with endogenous information and multiple equilibria.\" Theoretical Economics 8 883-938. 10.3982/TE1156.\n\nBoleslavsky, Raphael, Mehdi Shadmehr, and Konstantin Sonin. 2021. \"Media Freedom in the Shadow of a Coup.\" Journal of the European Economic Association 19 (3): 1782-1815. 10.1093/jeea/jvaa040.\n\nCantoni, Davide, Louis-Jonas Heizlsperger, David Y. Yang, Noam Yuchtman, and Y. Jane Zhang. 2022. \"The fundamental determinants of protest participation: Evidence from Hong Kong's antiauthoritarian movement.\" Journal of Public Economics 211 104667. 10.1016/j.jpubeco.2022.104667.\n\nCarlsson, Hans, and Eric van Damme. 1993. \"Global Games and Equilibrium Selection.\" Econometrica 61 (5): 989-1018.\n\nChassang, Sylvain. 2010. \"Fear of Miscoordination and the Robustness of Cooperation in Dynamic Global Games with Exit.\" Econometrica 78 (3): 973-1006.\n\nCripps, Martin, George Mailaith, and Larry Samuelson. 2004. \"Imperfect Monitoring and Impermanent Reputations.\" Econometrica 72 (2): 407-432. 10.1111/j.1468-0262.2004.00496.x.\n\nEdmond, Chris. 2013. \"Information Manipulation, Coordination, and Regime Change.\" Review of Economic Studies 80 1422-1458. 10.1093/restud/rdt020.\n\nEly, Jeffrey, Drew Fudenberg, and David Levine. 2008. \"When is reputation bad?\" Games and Economic Behavior 63 (2): 498-526. 10.1016/j.geb.2006.08.007.\n\nEly, Jeffrey, and Juuso Valimaki. 2003. \"Bad Reputation.\" Quarterly Journal of Economics 118 (3): 785-814. 10.1162/00335530360698423.\n\nFrankel, David, Stephen Morris, and Ady Pauzner. 2003. \"Equilibrium Selection in Global Games with Strategic Complementarities.\" Journal of Economic Theory 108 (1): $1-44$.\n\nFudenberg, Drew, David Kreps, and Eric Maskin. 1990. \"Repeated Games with Long run and Short run Players.\" Review of Economic Studies 57 (4): 555-573. $10.2307 / 2298086$.\n\nFudenberg, Drew, and David Levine. 1989. \"Reputation and Equilibrium Selection in Games with a Patient Player.\" Econometrica 57 (4): 759-778.\n\nFudenberg, Drew, and David Levine. 1992. \"Maintaining a Reputation when Strategies are Imperfectly Observed.\" Review of Economic Studies 59 (3): 561-579.\n\nFudenberg, Drew, David K. Levine, and Eric Maskin. 1994. \"The Folk Theorem with Imperfect Public Information.\" Econometrica 62 (5): 997-1039. 10.2307/2951505.\n\nFudenberg, Drew, and Jean Tirole. 1991. Game Theory. Cambridge, MA: MIT Press.\nGiannitsarou, C., and F. Toxvaerd. 2003. \"Recursive Global Games.\" CEPR Discussion Paper. https://ssrn.com/abstract=1138583.\n\nGossner, Olivier. 2011. \"Simple Bounds on the Value of a Reputation.\" Econometrica 79 (5): 1627-1641. 10.3982/ECTA9385.\n\nHuang, Chong. 2017. \"Defending against speculative attacks: The policymaker's reputation.\" Journal of Economic Theory 171 (1): 1-34.\n\nInostroza, Nicolas, and Alessandro Pavan. 2024. \"Adversarial Coordination and Public Information Design.\" Theoretical Economics, Forthcoming.\n\nJann, Ole, and Christoph Schottmuller. 2021. \"Regime Change Games with an Active Defender.\" Games and Economic Behavior 129 96-113. 10.1016/j.geb.2021.05.008.\n\nKreps, David, and Robert Wilson. 1982. \"Reputation and Imperfect Information.\" Journal of Economic Theory 27 (2): 253-279. 10.1016/0022-0531(82)90030-8.\n\nBueno de Mesquita, Ethan, and Mehdi Shadmehr. 2022. \"Rebel Motivations and Repression.\" American Political Science Review 1-17. 10.1017/S0003055422000600.\n\nMilgrom, Paul, and John Roberts. 1982. \"Predation, reputation, and entry deterrence.\" Journal of Economic Theory 27 (2): 280-312. 10.1016/0022-0531(82)90031-X.\n\nMilgrom, Paul, and John Roberts. 1990. \"Rationalizability, learning, and equilibrium in games with strategic complementarities.\" Econometrica 58 (6): 1255-1277. $10.2307 / 2938316$.\n\nMilgrom, Paul, and Chris Shannon. 1994. \"Monotone Comparative Statics.\" Econometrica 62 (1): 157-180. 10.2307/2951479.\n\nMorris, Stephen, and Mehdi Shadmehr. 2023. \"Inspiring Regime Change.\" Journal of the European Economic Association 21 (6): 2635-2681. 10.1093/jeea/jvad023.\n\nMorris, Stephen, and Mehdi Shadmehr. 2024. \"Repression and Repertoires.\" AER: Insights 6 (3): 413-433. 10.1257/aeri. 20230402.\n\nMorris, Stephen, and Hyun-Song Shin. 1998. \"Unique Equilibrium in a Model of Self-Fulfilling Currency Attacks.\" The American Economic Review 88 (3): 587-597, http://www.jstor.org/stable/116850.\n\nMorris, Stephen, and Hyun-Song Shin. 2003. \"Global Games: Theory and Applications.\" Advances in Economics and Econometrics (8th world congress of the econometric society) Cambridge University Press.\n\nPei, Harry. 2024. \"Reputation Effects with Endogenous Records.\" April, URLoftheworkingpaper, Working paper.\n\nPei, Harry. 2025. \"Community Enforcement with Endogenous Records.\"\nPei, Harry, and Yingkai Li. 2021. \"Equilibrium behaviors in repeated games.\" Journal of Economic Theory 193. 10.1016/j.jet.2021.105222.\n\nVives, Xavier. 1990. \"Nash equilibrium with strategic complementarities.\" Journal of Mathematical Economics 19 (3): 305-321. 10.1016/0304-4068(90)90005-T.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2404.18884v2",
  "authors": [
    "Daniel Luo"
  ],
  "categories": [
    "econ.TH"
  ],
  "abstract": "How valuable is reputation to a regime seeking to deter uprisings? I show the\nanswer depends almost entirely on players' strategic uncertainty over other\nplayers actions. Without higher-order uncertainty, reputational effects are\novershadowed by stage-game strategic complementarities: any payoff attainable\nin the complete-information repeated game remains attainable in the\ncorresponding reputation game. However, when agents face globalized uncertainty\nover their payoffs, the regime's value for reputation rises sharply. In\nparticular, the regime can leverage globalized strategic uncertainty to\nsuccessful develop a reputation and secure their Stackelberg payoff in all pure\nequilibria of the reputation game, even as that same globalized strategic\nuncertainty forces them to their minimax payoff in the associated\ncomplete-information repeated game. Moreover, the globalized reputation game\nselects a unique pooling Markov equilibria among the continuum of Markov\nequilibria present in the complete-information game, refining predictions about\nequilibrium play. To establish these results, I develop new tools for analyzing\nnon-normalized discounted repeated games with endogenous exit, where commitment\nactions vary with the regime's patience.",
  "updated": "2025-02-27T19:48:14Z",
  "published": "2024-04-29T17:19:24Z"
}