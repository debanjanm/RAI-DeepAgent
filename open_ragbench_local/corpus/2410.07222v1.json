{"title": "Computing Systemic Risk Measures with Graph Neural Networks", "sections": [{"section_id": 0, "text": "#### Abstract\n\nThis paper investigates systemic risk measures for stochastic financial networks of explicitly modelled bilateral liabilities. We extend the notion of systemic risk measures from Biagini, Fouque, Fritelli and Meyer-Brandis (2019) to graph structured data. In particular, we focus on an aggregation function that is derived from a market clearing algorithm proposed by Eisenberg and Noe (2001). In this setting, we show the existence of an optimal random allocation that distributes the overall minimal bailout capital and secures the network. We study numerical methods for the approximation of systemic risk and optimal random allocations. We propose to use permutation equivariant architectures of neural networks like graph neural networks (GNNs) and a class that we name (extended) permutation equivariant neural networks ((X)PENNs). We compare their performance to several benchmark allocations. The main feature of GNNs and (X)PENNs is that they are permutation equivariant with respect to the underlying graph data. In numerical experiments we find evidence that these permutation equivariant methods are superior to other approaches.\n\n\nKeywords: systemic risk measure, financial network, contagion, default, graph neural network, permutation equivariant, universal approximation, message passing, deep learning\nMathematics Subject Classification (2020): 68T07, 91G45, 91G60, 91G70\nJEL Classification: C61, G10, G21, G32, G33", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1 Introduction\n\nRisk measures are fundamental tools in financial mathematics, providing a way to quantify and manage the risk associated with financial positions or portfolios. The development of these measures has evolved significantly over time, driven by both theoretical advancements and practical needs in the financial industry. The early developments in risk measures can be traced back to Harry Markowitz's seminal work \"Portfolio Selection\" in 1952 [52], introducing the mean-variance framework. In the 1990s the Value at Risk (VaR), most notably developed by JP Morgan, became widely used in risk management and regulatory frameworks due to its simplicity and intuitive appeal. However, its limitations led to a line of literature developing theoretically sound concepts of risk measures, e.g. coherent, convex or monetary risk measures, see for example [6], [29], [31], [32].\nAs the financial system became more interconnected and complex, in particular in the aftermath of the financial crisis 2007-2008, the focus was widened towards understanding and measuring systemic risk, i.e. the risk that the failure of one part of the financial system can cause a collapse of the entire system, see for example [1], [2], [55],[60].\n\n[^0]\n[^0]:    *Department of Mathematics, Imperial College London, United Kingdom. 1.gonon@imperial.ac.uk\n    ${ }^{\\dagger}$ Department of Mathematics, LMU Munich, Germany. meyerbra@math.lmu, weber@math.lmu.de\n\nMany of these systemic risk measures can be described as the application of a univariate risk measure $\\eta: L^{0} \\rightarrow \\mathbb{R}$ to the systemic risk factor $\\Lambda(X)$, where $X=\\left(X_{1}, \\ldots, X_{N}\\right) \\in L^{0}\\left(\\mathbb{R}^{N}\\right)$ represents random risk factors of a system of $N$ institutions and $\\Lambda: \\mathbb{R}^{N} \\rightarrow \\mathbb{R}$ is a function that aggregates the individual risk factors $X_{1}, \\ldots, X_{N}$ to a systemic risk factor based on some aggregation rule $\\Lambda$, i.e. the systemic risk is given by\n\n$$\n\\rho(X)=\\eta(\\Lambda(X))\n$$\n\nIf $\\eta$ is cash-invariant (see, e.g. [32]), the obtained number can often be interpreted as the amount of cash that is needed to secure the position in terms of risk measure $\\eta$. If $\\mathbb{A}$ is the corresponding acceptance set then the systemic risk measure can be reformulated as\n\n$$\n\\rho(X)=\\inf \\{m \\in \\mathbb{R} \\mid \\Lambda(X)+m \\in \\mathbb{A}\\}\n$$\n\nIn [14] an axiomatic approach for such systemic risk measures and conditions on when they can be decomposed into aggregation function and univariate risk measure is provided. This setting is further investigated for example in [42], [48].\nThe formulation in (2) highlights that the amount of cash $m \\in \\mathbb{R}$ is added to the aggregated position $\\Lambda(X)$. In contrast to this setting where the system is secured after aggregation, an alternative approach investigates the setting where capital is allocated before aggregation. This is motivated, for example, by aggregation functions that capture financial contagion between the institutions. In this case it can be significantly cheaper to at least partially prevent contagion by allocating capital before aggregation as opposed to securing the system after aggregation when the contagion already took place. This idea yields systemic risk measures of the form\n\n$$\n\\rho(X)=\\inf \\left\\{\\sum_{i=1}^{N} m_{i} \\mid m=\\left(m_{1}, \\ldots, m_{N}\\right) \\in \\mathbb{R}^{N}, \\Lambda(X+m) \\in \\mathbb{A}\\right\\}\n$$\n\nSystemic risk measures based on allocating capital before aggregation have been investigated in the context of set valued risk measures and deterministic allocations in [26]. In this approach each acceptable vector $m=\\left(m_{1}, \\ldots, m_{N}\\right)$ could be interpreted as a possible choice of capital requirements for the institutions $\\left(X_{1}, \\ldots, X_{N}\\right)$ such that the resulting financial system is safe after aggregation, i.e. $\\Lambda(X+m) \\in \\mathbb{A}$. A recent other approach that we want to focus on in this article was developed in [7] and [8]. Here, the deterministic capital allocation $m=\\left(m_{1}, \\ldots, m_{N}\\right) \\in \\mathbb{R}^{N}$ is replaced by random capital allocations $Y \\in L^{0}\\left(\\mathbb{R}^{N}\\right)$. Then the systemic risk measure is defined as\n\n$$\n\\rho(X)=\\inf _{Y \\in \\mathcal{C}}\\left\\{\\sum_{i=1}^{N} Y_{i} \\mid \\Lambda(X+Y) \\in \\mathbb{A}\\right\\}\n$$\n\nwhere the set of available allocations $\\mathcal{C} \\subseteq \\mathcal{C}_{\\mathbb{R}}$ is a subset of those random allocations which sum to a constant almost surely ${ }^{1}$\n\n$$\n\\mathcal{C}_{\\mathbb{R}}=\\left\\{Y \\in L^{0}\\left(\\mathbb{R}^{N}\\right) \\mid \\sum_{i=1}^{N} Y_{i} \\in \\mathbb{R}\\right\\}\n$$\n\nSince $\\mathbb{R} \\subseteq \\mathcal{C}_{\\mathbb{R}}$, this definition includes the setting of deterministic capital requirements. In general, this notion of systemic risk measures reflects the point of view of a lender of last resort, who would like to reserve some fixed amount of bailout capital to secure the system in the future. However, instead of committing\n\n[^0]\n[^0]:    ${ }^{1}$ We want to stress that for all $Y \\in \\mathcal{C}_{\\mathbb{R}}$ the sum of the stochastic components is deterministic, i.e. for all $Y \\in \\mathcal{C}_{\\mathbb{R}}$, there exists $M \\in \\mathbb{R}$ such that for all $\\omega \\in \\Omega$ it holds $\\sum_{i=1}^{N} Y_{i}(\\omega)=M$. We express this fact by simply writing $\\sum_{i=1}^{N} Y_{i} \\in \\mathbb{R}$.\n\nto a fixed allocation today that secures the system under all possible scenarios, the lender can wait which scenario is realised and then distribute the bailout capital. Allowing scenario dependent allocations can reduce the total capital that needs to be reserved compared to deterministic allocations, which need to cover all eventualities.\nRegarding the aggregation function $\\Lambda$, in the current literature one can conceptually differentiate between two paradigms. One, where the random variable $X$ represents the risk factors after contagious interaction of the institutions. Then the aggregation function is a rather simple function of the risk factors, e.g. a sum, the sum of losses only, a sum of losses where profit is either considered up to some threshold or only a fraction of it, or other similar variations. Examples for such risk measures include [2], [12], [44], [50], [55], [59] .\nOn the other hand, there is the approach where $X$ represents the risk factors before any contagious interaction took place. Then the aggregation function is designed to incorporate complex interaction mechanisms, that might for example include contagion channels like interbank liabilities or illiquidity cascades [3], [22], [30], [33], [34], [38], [45], [49], [57], asset fire sales [11], [13], [15], [17], [18], [24], [25], or cross-holdings [23], [63]. For complex contagion channels the aggregation function would require additional inputs such as an interbank liability matrix, an asset-bank exposure matrix, or a bank-bank cross-holdings matrix. This is where we extend the literature with our first contribution. In the setting of random allocations from [7], [8], we consider an aggregation function that allows for a vector-valued and a matrix-valued input\n\n$$\n\\Lambda: \\mathbb{R}^{N} \\times \\mathbb{R}^{N \\times N} \\rightarrow \\mathbb{R}\n$$\n\nand define systemic risk measures accordingly. This allows us to consider aggregation functions that explicitly model contagious interactions in networks of financial institutions. We focus on the Eisenberg-Noe model [22], which is the prototype of network-based contagion models.\nMore in detail, instead of considering vector valued random risk factors $X \\in L^{0}\\left(\\mathbb{R}^{N}\\right)$, we consider random assets of financial institutions $A \\in L^{0}\\left(\\mathbb{R}_{+}^{N}\\right)$ together with their explicit interbank lending network given by a random interbank liability matrix $L \\in L^{0}\\left(\\mathbb{R}_{+}^{N \\times N}\\right)$. Then, for an Eisenberg-Noe type aggregation mechanism $\\Lambda: \\mathbb{R}^{N} \\times \\mathbb{R}^{N \\times N} \\rightarrow \\mathbb{R}$ we define systemic risk measures as the smallest amount of bailout capital such that random allocations thereof secure the future system,\n\n$$\n\\rho(A, L)=\\inf _{Y \\in \\mathcal{C}}\\left\\{\\sum_{i=1}^{N} Y_{i} \\mid \\Lambda(A+Y, L) \\in \\mathbb{A}\\right\\}\n$$\n\nwhere $\\mathcal{C} \\subseteq \\mathcal{C}_{\\mathbb{R}}$. For a specific choice of $\\Lambda$ in the contagion model of Eisenberg and Noe [22], we show that this systemic risk measure is well-defined and that there exists a convex set of random allocations $Y \\in \\mathcal{C}$ that secure the system, i.e. $\\Lambda(A+Y, L) \\in \\mathbb{A}$, while satisfying $\\sum_{i=1}^{N} Y_{i}=\\rho(A, L)$. We further reformulate the systemic risk measure in a form that allows to derive an iterative optimisation algorithm for its approximation.\nIn general, the random allocations make such systemic risk measures in (4) challenging to compute numerically. Already for deterministic allocations, there exist only few approaches to compute the systemic risk measure. In [26] an algorithm is proposed that approximates the efficient frontier of the set-valued risk measure. In [5] a mixed-integer programming approach is presented to calculate the set-valued risk measure for more general aggregation functions beyond the Eisenberg-Noe model. There is even fewer literature concerned with random allocations. In [28], systemic risk measures of the form in (4) are approximated by utilising feedforward neural networks (FNNs) in order to approximate the scenario dependent random allocation as functions $f(X)$ of the risk factors $X$. In [21] they tackle a similar problem (see [9]) utilising a comparable approach.\nHere, we investigate to which extent neural networks could be applied to approximate systemic risk measures in our extended setting (6). Compared to [28], the situation is more complex, since we must rely on neural networks that are able to effectively process the liability matrix as an input. In numerical experiments, it turns out that, despite their universality property, feedforward neural networks are not able to handle this input efficiently. However, the structural properties of the problem naturally lead us to consider neural networks that respect permutation equivariance of the underlying graph data. This motivates to utilise\n\ngraph neural networks (GNNs), an alternative class of neural network architectures that have recently been successfully applied to a wider range of problems within and beyond financial mathematics. For an overview we refer to the survey papers [62] and [65]. Additionally, similarly to [41] we derive a structural characterisation of so called permutation equivariant node labeling functions. Motivated by this result, we define a permutation equivariant neural network architecture that exhibits appealing theoretical properties and, in numerical experiments, performs even better than GNNs. We call this architecture (extended) permutation equivariant neural networks ((X)PENNs).\nBefore outlining the structure of this article, we summarise our contributions.\n\n- We formulate an extension of systemic risk measures with random allocations as in (4) that incorporates more general aggregation functions that allow for random assets and a random interbank liability matrix as inputs, see (6). In this more realistic setting of random interbank liabilities we can consider aggregation functions that explicitly model contagion which is based on a network structure of the institutions. Due to the explicitly modelled contagion, there are more possibilities of detecting financial contagion and hence this setting emphasises the importance of preventing contagion, instead of paying for its losses afterwards. Additionally, the possibility of random liability matrices opens the door for network reconstruction techniques, especially those that do not provide only one estimator of the true network, but many possible ones that fulfill some boundary conditions, see for example [16], [35], [36], [39], or [54]. For an overview of network reconstruction techniques see for example [4].\n- In the contagion model of Eisenberg and Noe [22] we show for a specific aggregation function that the systemic risk measure is well-defined. In particular we show existence of \"optimal\" bailout capital allocating random variables and investigate their properties from a theoretical point of view. We further derive a reformulation of the systemic risk measure that allows us to construct an iterative algorithm for its numerical approximation.\n- In the same setting, we show that there exists a measurable target function $H^{c}: \\mathbb{R}^{N} \\times \\mathbb{R}^{N \\times N} \\rightarrow \\mathbb{R}^{N}$ such that $H^{c}(A, L)$ is an optimal allocation of the fixed bailout capital $c \\in \\mathbb{R}$. Due to its measurability, this allows for approximation via neural networks.\n- Furthermore, we characterise what we call permutation equivariant node labeling functions and, motivated by this result, propose the neural network architecture of (X)PENNs. We further prove that (X)PENNs are able to approximate any permutation equivariant node labeling functions, in particular $H^{c}$, in probability.\n- Finally, in various numerical experiments we compare the performance of FNNs, GNNs and (X)PENNs and other benchmark approaches in solving different problems related to the approximation of systemic risk measures.\n\nThis article continues as follows. In Section 2 we introduce the domain of graphs that is relevant when interpreting financial networks as graphs. Furthermore, we present the contagion model of Eisenberg and Noe [22] and the aggregation function that we consider in this framework. In Section 3 we introduce our systemic risk measures of random financial networks. Section 4 provides a reformulation of systemic risk measures that yields an iterative optimisation algorithm for their computation. In Section 5 we discuss FNNs and GNNs and characterise permutation equivariant node labeling functions. Motivated by this result we introduce the novel (X)PENN neural network architecture and discuss their theoretical properties. Section 6 contains numerical experiments where we compare the performance of FNNs, GNNs, (X)PENNs and other benchmark approaches. In the first experiments we investigate a toy example where the minimal amount of bailout capital and its allocation is known. The next experiment investigates the situation where we try to allocate a fixed amount of bailout capital optimally. Finally, the third experiment considers the approximation of the systemic risk measure, i.e. finding the minimal amount of bailout capital that secures the future network when distributed optimally.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 2 Preliminaries\n### 2.1 Notation\n\nLet $L^{p}\\left(\\mathbb{R}^{N}\\right)=L^{p}\\left(\\Omega, \\mathcal{F}, \\mathbb{P} ; \\mathbb{R}^{N}\\right)$ denote the space of $\\mathbb{R}^{N}$-valued random variables with finite $p$-norm defined on the probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$.\nWe extend \" $\\leq, \\geq$ \" in the following way to vectors and (vector-valued) random variables. For two vectors $a, b \\in \\mathbb{R}^{N}$ we interpret the inequality component-wise\n\n$$\na \\leq b \\Longleftrightarrow \\forall i \\in\\{1, \\ldots, N\\}: a_{i} \\leq b_{i}\n$$\n\nFor real or vector valued random variables $X, Y \\in L^{p}\\left(\\mathbb{R}^{N}\\right)$ we say that $X \\leq Y$ if every component of X is less or equal to Y almost surely,\n\n$$\nX \\leq Y \\Longleftrightarrow \\forall i \\in\\{1, \\ldots, N\\}: X_{i} \\leq Y_{i} \\quad \\mathbb{P} \\text {-a.s. }\n$$\n\nFurthermore, in this paper the sum of the components of vector valued random variables plays an important role. For real vectors and vector valued random variables $a \\in \\mathbb{R}^{N}$ or $X \\in L^{p}\\left(\\mathbb{R}^{N}\\right)$ respectively, we define\n\n$$\n|a|_{p}:=\\left(\\sum_{i=1}^{N}\\left|a_{i}\\right|^{p}\\right)^{\\frac{1}{p}}, \\quad|X|_{p}:=\\left(\\sum_{i=1}^{N}\\left|X_{i}\\right|^{p}\\right)^{\\frac{1}{p}}\n$$\n\nSimilarly, for real matrices and matrix valued random variables $\\ell \\in \\mathbb{R}^{N \\times N^{\\prime}}$ or $L \\in L^{p}\\left(\\mathbb{R}^{N \\times N^{\\prime}}\\right)$ respectively, we define\n\n$$\n|\\ell|_{p}:=\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{N^{\\prime}}\\left|\\ell_{i j}\\right|^{p}\\right)^{\\frac{1}{p}}, \\quad|X|_{p}:=\\left(\\sum_{i=1}^{N} \\sum_{j=1}^{N^{\\prime}}\\left|L_{i j}\\right|^{p}\\right)^{\\frac{1}{p}}\n$$\n\nIn particular $|\\cdot|_{p}$ does not involve taking any expectation for random variables. Without the index $|\\cdot|$ describes merely taking the absolute value component-wise. Furthermore, to shorten notation we define $[N]:=\\{1, \\ldots, N\\}$ for any $N \\in \\mathbb{N}$.", "tables": {}, "images": {}}, {"section_id": 3, "text": "### 2.2 The domain of graphs\n\nThe financial networks that we consider below can be naturally described as a graph $g$ containing nodes $1, \\ldots, N$ with node features $a_{1}, \\ldots, a_{N} \\in \\mathbb{R}$ and directed, weighted edges $\\ell_{i j} \\in \\mathbb{R}$ between nodes $i, j \\in[N]$. Motivated by this, we define the domain of graphs that are directed, weighted and contain node and edge features as follows.\n\nDefinition 2.1. Let $\\mathcal{D}=\\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{N \\times N \\times d^{\\prime}}$ be the domain of directed, weighted and featured graphs with $N \\in \\mathbb{N}$ nodes and with node and edge feature dimension $d$ and $d^{\\prime}$, respectively. For $g=(a, \\ell) \\in \\mathcal{D}$ we call $a$ the node features and $\\ell$ the edge features.\n\nFor $d=d^{\\prime}=1$ this domain $\\mathcal{D}$ accommodates financial networks that we will investigate later, see Definition 2.6, if we choose the assets as node-feature and the liability sizes as edge-features.\n\nRemark 1. Let $g=(a, \\ell) \\in \\mathcal{D}$. For $i, j \\in[N]$ we interpret edges with $\\ell_{i j}=(0, \\ldots, 0) \\in \\mathbb{R}^{d^{\\prime}}$ as no edge. For applications where differentiation between a zero-edge and no edges is necessary, it would be possible to extend the domain of edge features with some no-edge argument $(a, \\ell) \\in \\mathbb{R}^{N \\times d} \\times(\\mathbb{R} \\cup\\{\\mathrm{NE}\\})^{N \\times N \\times d^{\\prime}}$. Similarly we interpret nodes without any edges and with $a_{i}=(0, \\ldots, 0)$ as no nodes and hence also account for graphs with less than $N$ nodes. However, if necessary also for nodes a no-node argument NA could be introduced. Obviously such changes would require all functions operating on the graph domain to be extended accordingly.\n\nWe are interested in functions on the domain of graphs that assign some value to each node of the network.\nDefinition 2.2. We define a node labeling function $\\tau: \\mathcal{D} \\rightarrow \\mathbb{R}^{N \\times l},(a, \\ell) \\mapsto\\left(\\tau_{1}, \\ldots, \\tau_{N}\\right)$ as a function where each component $\\tau_{n} \\in \\mathbb{R}^{l}$ corresponds to a label assigned to node $n \\in[N]$.\n\nFor example, a function that maps each node in a financial network to some node-specific capital requirement would be one example of such a node labeling function.\nAs we will see later, an important property of graphs is that - in general - the nodes in a graph do not have any natural order. Imagine we have a financial network with two nodes where one node's assets equal 1 and the other's amount to 2 units of capital. From the node with assets 1 , there is a liability of 12 units towards the node with assets 2 . We can represent this financial network by\n\n$$\na=\\binom{1}{2}, \\quad \\ell=\\left(\\begin{array}{ll}\n0 & 12 \\\\\n0 & 0\n\\end{array}\\right)\n$$\n\nwhere we choose that the node with assets 1 is the first node and the node with assets 2 is the second node. However, an equally valid representation of this financial network is\n\n$$\na=\\binom{2}{1}, \\quad \\ell=\\left(\\begin{array}{ll}\n0 & 0 \\\\\n12 & 0\n\\end{array}\\right)\n$$\n\nwhere we altered the node order.\nThis example showcases that there exist multiple reasonable representations for the same underlying data when dealing whit graph-structured data. Mathematically, we describe this property by introducing permutations.\nGiven any permutation $\\sigma \\in S_{N}$, i.e. a bijection from $[N]$ to itself, we denote for any network $g=(a, \\ell) \\in \\mathcal{D}$ the permutation of the network $\\sigma(g)$ and define it component-wise as the permutation of its node features $\\sigma(a)$ and edge features $\\sigma(\\ell)$. The permutations of $a \\in \\mathbb{R}^{N \\times d}$ and $\\ell \\in \\mathbb{R}^{N \\times N \\times d^{\\prime}}$ are defined as\n\n$$\n\\sigma(a)_{i, k}=a_{\\sigma^{-1}(i), k}\n$$\n\nfor $i \\in[N], k \\in[d]$ and\n\n$$\n\\sigma(\\ell)_{i, j, k}=\\ell_{\\sigma^{-1}(i), \\sigma^{-1}(j), k}\n$$\n\nfor $i, j \\in[N], k \\in\\left[d^{\\prime}\\right]$.\nAn often desirable property of node labeling function is that the value assigned to each node should not depend on the chosen graph representation, but only on the underlying graph.\n\nDefinition 2.3. For any $l \\in \\mathbb{N}$ we call a node labeling function $\\tau: \\mathcal{D} \\rightarrow \\mathbb{R}^{N \\times l}$ permutation equivariant if any permutation $\\sigma \\in S_{N}$ of the initial numbering of nodes causes a corresponding permutation of the output,\n\n$$\n\\tau(\\sigma(g))=\\sigma(\\tau(g))\n$$\n\nSimilar to permutation equivariance, another important concept is permutation invariance.\nDefinition 2.4. Let $m, m^{\\prime}, d, N \\in \\mathbb{N}$. We call a function $f:\\left(\\mathbb{R}^{m} \\times \\mathbb{R}^{d}\\right)^{N} \\rightarrow \\mathbb{R}^{m^{\\prime}}$ permutation invariant if for any permutation $\\sigma \\in S_{N}$ of the inputs ${ }^{2}$ the result is invariant, i.e.\n\n$$\nf(\\sigma(x))=f(x)\n$$\n\n[^0]\n[^0]:    ${ }^{2}$ We identify $\\left(\\mathbb{R}^{m} \\times \\mathbb{R}^{d}\\right)^{N}$ with $\\mathbb{R}^{N \\times m \\cdot d}$ and use the definition of permutations of node-features from (9).\n\nPermutation invariant functions arise, for example, when we want to map a subset of nodes and edges (a subgraph) to some value that only depends on the set, but not the order of inputs.\nA specific subset of nodes, that plays an important role for graph neural networks, is the so called neighborhood of a node. The neighborhood of a node $i \\in[N]$ is defined as the set of all other nodes $j \\in[N]$ which have an edge towards $i$ in the underlying graph $g \\in \\mathcal{D}$.\nDefinition 2.5. The neighborhood of a node $i \\in[N]$ in a graph $g=(a, \\ell) \\in \\mathcal{D}$ is defined as\n\n$$\nN_{g}(i)=\\{j \\in[N] \\backslash\\{i\\} \\mid \\ell_{j i} \\neq 0\\}\n$$\n\nIf the referenced graph $g$ is clear in the context, we may simply write $N(i)$ instead of $N_{g}(i)$.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 2.3 Eisenberg-Noe Aggregation \n\nOne approach to design systemic risk measures utilises an aggregation function that aggregates the system to some univariate value such that the riskiness of the system can then be evaluated by applying some univariate risk to this value, see (1). Now we specify an aggregation function for financial networks based on the market clearing mechanism introduced in the seminal work of Eisenberg and Noe [22].\nDefinition 2.6 (Financial network). We model a financial network of $N$ institutions and their interbank liabilities as follows:\n(i) The institutions assets $a=\\left(a_{1}, \\ldots, a_{N}\\right) \\in \\mathbb{R}_{+}^{N}$\n(ii) The interbank liability matrix $\\ell \\in \\mathbb{R}_{+}^{N \\times N}$, where $\\ell_{i j}>0$ indicates a liability of size $\\ell_{i j}$ from bank $i$ towards bank $j$. We do not allow self loops, hence $\\operatorname{diag}(\\ell)=\\left(\\ell_{11}, \\ldots, \\ell_{N N}\\right)=(0, \\ldots, 0)$.\nIn a network of financial institutions connected by bilateral liabilities it can occur that assets and incoming cash-flows of one participant are not sufficient to meet the full amount of the outgoing liabilities. This could lead to a situation where other participants can not meet their liabilities due to the lack of incoming cash-flows, triggering a chain reaction of defaults being propagated through the network. To calculate how much value will be actually available for paying off debt at every node in an equilibrium, where everybody pays off as much debt as possible from their incoming cash-flows, we can calculate a so called clearing vector introduced by Eisenberg and Noe [22]. The utilised market clearing algorithm follows three basic principles: limited liability, which means that an institution can never pay back more than it has funds available, proportionality, which requires that in case of a default all creditors are paid back proportionally, and absolute priority, which means that any institution will use all available capital in case its liabilities cannot be fully met.\nTo describe this clearing vector mathematically, we introduce the following quantities. For a financial network of $N \\in \\mathbb{N}$ institutions $(a, \\ell) \\in \\mathbb{R}_{+}^{N} \\times \\mathbb{R}_{+}^{N \\times N}$, we define the total liability vector $\\bar{\\ell} \\in \\mathbb{R}_{+}^{N}$ containing the sum of outwards liabilities of each node by\n\n$$\n\\bar{\\ell}_{i}:=\\sum_{j=1}^{N} \\ell_{i j}\n$$\n\nDenoting the set of all relative liability matrices by\n\n$$\n\\boldsymbol{\\Pi}^{N}:=\\left\\{\\pi \\in[0,1]^{N \\times N} \\mid \\forall i \\in[N]: \\pi_{i i}=0, \\sum_{j=1}^{N} \\pi_{i j} \\in\\{0,1\\}\\right\\}\n$$\n\nwe define the relative liability matrix $\\pi \\in \\boldsymbol{\\Pi}^{N}$ by\n\n$$\n\\pi_{i j}:= \\begin{cases}\\ell_{i j} / \\bar{\\ell}_{i} & \\text { if } \\bar{\\ell}_{i}>0 \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nRemark 2. Note that we can always obtain $\\ell$ from the pair $(\\pi, \\bar{\\ell})$ and vice versa. Therefore, for every function $f: \\mathbb{R}_{+}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}^{N}$ there exists always a corresponding function $\\bar{f}: \\mathbb{R}_{+}^{N} \\times \\boldsymbol{\\Pi}^{N} \\times \\mathbb{R}_{+}^{N} \\rightarrow \\mathbb{R}^{N}$ such that\n\n$$\nf(a, \\ell)=\\bar{f}(a, \\pi, \\bar{\\ell})\n$$\n\nWe define the function $\\Phi: \\mathbb{R}_{+}^{N} \\times \\mathbb{R}_{+}^{N} \\times \\boldsymbol{\\Pi}^{N} \\times \\mathbb{R}_{+}^{N} \\rightarrow \\mathbb{R}^{N}$ by\n\n$$\n\\Phi(p, a, \\pi, \\bar{\\ell}):=\\left(\\pi^{T} p+a\\right) \\wedge \\bar{\\ell}\n$$\n\nThis function calculates how much debt could be paid back by each node $i=1, \\ldots, N$ under the assumption that the other nodes use capital $p_{j}, j=1, \\ldots, N, j \\neq i$ to pay off their debt.\nIn [22] it is shown that the function $\\Phi$ is positive, monotone increasing, concave (component-wise, since we map into $\\mathbb{R}^{N}$ ) and non-expansive (Lipschitz continuous with Lipschitz constant $K=1$ ) in its arguments $a$ and $\\bar{\\ell}$.\n\nDefinition 2.7. For given $a \\in \\mathbb{R}_{+}^{N}, \\pi \\in \\Pi^{N}, \\bar{\\ell} \\in \\mathbb{R}_{+}^{N}$, a fixed point of the function $\\Phi(\\cdot, a, \\pi, \\bar{\\ell})$ is called $a$ clearing vector.\n\nWe can interpret such a clearing vector as a state in which the system is in equilibrium: If each institution uses capital according to the clearing vector to pay back its debts, the fact that a clearing vector is a fixed point of $\\Phi$ will ensure that indeed each institution has available capital according to the clearing vector.\nThe properties of $\\Phi$ mentioned above allow Eisenberg and Noe [22] to obtain results regarding the existence of fixed points of this function. In particular, there exist a least and greatest element in the set of fixed points. A clearing vector can be obtained by applying the function iteratively or by solving a finite number of linear equations, see [22]. Furthermore, they find sufficient conditions for uniqueness of the clearing vector, e.g. $a>0$. This is an easy way to ensure unique clearing vectors, hence from now on we will demand that assets are strictly positive, i.e. $a>0$. We write $a \\in \\mathbb{R}_{++}^{N}$.\nThe following proposition provides some insights how a clearing vector depends on the financial network $(a, \\pi, \\bar{\\ell})$.\n\nProposition 2.8. Let $\\left[\\mathbb{R}_{+}^{N} \\rightarrow \\mathbb{R}_{+}^{N}\\right]$ denote the space of functions from $\\mathbb{R}_{+}^{N}$ to $\\mathbb{R}_{+}^{N}$ and let $\\mathcal{P}\\left(\\mathbb{R}^{N}\\right)$ denote the power set of $\\mathbb{R}^{N}$. Let further FIX : $\\left[\\mathbb{R}_{+}^{N} \\rightarrow \\mathbb{R}_{+}^{N}\\right] \\rightarrow \\mathcal{P}\\left(\\mathbb{R}^{N}\\right)$ be the map returning the set of fixed points $\\operatorname{FIX}(h)$ of a function $h \\in\\left[\\mathbb{R}_{+}^{N} \\rightarrow \\mathbb{R}_{+}^{N}\\right]$. Then the following hold.\n(i) For any $\\pi \\in \\Pi^{N}, \\bar{\\ell} \\in \\mathbb{R}_{+}^{N}$ the function $f_{1}: \\mathbb{R}_{++}^{N} \\rightarrow \\mathbb{R}^{N}$\n\n$$\nf_{1}(a)=F I X(\\Phi(\\cdot, a, \\pi, \\bar{\\ell}))\n$$\n\nis well-defined, positive, monotone increasing, concave and non-expansive.\n(ii) For any $a \\in \\mathbb{R}_{++}^{N}, \\pi \\in \\Pi^{N}$ the function $f_{2}: \\mathbb{R}_{+}^{N} \\rightarrow \\mathbb{R}^{N}$\n\n$$\nf_{2}(\\bar{\\ell})=F I X(\\Phi(\\cdot, a, \\pi, \\bar{\\ell}))\n$$\n\nis well-defined, positive, monotone increasing, concave and non-expansive.\n(iii) For any $a \\in \\mathbb{R}_{++}^{N}, \\bar{\\ell} \\in \\mathbb{R}_{+}^{N}$ the function $f_{3}: \\Pi^{N} \\rightarrow \\mathbb{R}^{N}$\n\n$$\nf_{3}(\\pi)=F I X(\\Phi(\\cdot, a, \\pi, \\bar{\\ell}))\n$$\n\nis well-defined and continuous.\nProof. Since for $a \\in \\mathbb{R}_{++}^{N}$ the fixed point of $\\Phi$ is unique, the well-definedness is clear for all cases. The cases (i) and (ii) follow from Lemma 5 in [22]. Case (iii) follows from Proposition 2.1 in [27] with the only variation that we define $\\Pi^{N}$ slightly differently, but since the set $\\Pi^{N}$ is closed the same reasoning holds true.\n\nIn the following we investigate the function that maps a financial network to its unique clearing vector.\nDefinition 2.9. We define the clearing vector functions $\\mathrm{CV}: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}^{N}$ and $\\overline{\\mathrm{CV}}: \\mathbb{R}_{++}^{N} \\times \\boldsymbol{\\Pi}^{N} \\times$ $\\mathbb{R}_{+}^{N} \\rightarrow \\mathbb{R}^{N}$ as\n\n$$\n\\operatorname{CV}(a, \\ell)=\\overline{\\operatorname{CV}}(a, \\pi, \\bar{\\ell})=F I X(\\Phi(\\cdot, a, \\pi, \\bar{\\ell}))\n$$\n\nFrom Proposition 2.8 it is clear that $\\overline{\\mathrm{CV}}$ is continuous with respect to all its components and it is also obvious that CV is continuous with respect to the assets $a$.\nRemark 3. Recall that we chose the relative liability matrix $\\pi \\in \\Pi^{N}$ such that for any $i \\in[N]$ the $i$-th row $\\pi_{i:} \\in \\mathbb{R}_{+}^{N}$ equals 0 or sums to 1 , depending on whether $\\bar{\\ell}_{i}$ is 0 or not. However, from the definition of $\\Phi$ in (13) it is clear that in the case $\\bar{\\ell}_{i}=0$, every fixed point $p \\in \\mathbb{R}_{+}^{N}$ of $\\Phi(\\cdot, a, \\pi, \\bar{\\ell})$ must satisfy $p_{i}=0$ for any choice of $\\pi_{i:} \\geq 0$. Subsequently $\\operatorname{CV}(a, \\pi, \\bar{\\ell})$ is the same for any choice of $\\pi_{i:} \\geq 0$ if $\\bar{\\ell}_{i}=0$. This observation will help us prove the following proposition where we show that CV is continuous with respect to the liability matrix $\\ell$.\nProposition 2.10. The clearing vector function $\\mathrm{CV}: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}^{N}$\n\n$$\n(a, \\ell) \\mapsto \\operatorname{CV}(a, \\ell)\n$$\n\nis continuous with respect to $\\ell$.\nProof. Let $(a, \\ell) \\in \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N}$ be given. We show continuity in one arbitrary entry $\\ell_{k l}, k, l \\in[N]$ of the liability matrix. To this end, fix $k, l \\in[N]$.\nLet $h: \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\boldsymbol{\\Pi}^{N} \\times \\mathbb{R}_{+}^{N}$ be the auxiliary function that maps $\\ell$ to the couple $(\\pi, \\bar{\\ell})$ according to equations (11) and (12). This function is continuous in the entry $\\ell_{k l}$ if $\\bar{\\ell}_{k}>0$. Hence, in the case $\\bar{\\ell}_{k}>0$ we have that\n\n$$\n\\operatorname{CV}(a, \\ell)=\\overline{\\operatorname{CV}}(a, h(\\ell))=\\overline{\\operatorname{CV}}(a, \\pi, \\bar{\\ell})\n$$\n\nwhich is continuous in $\\ell_{k l}$ as a composition of continuous functions.\nIn the case $\\bar{\\ell}_{k}=0$ it must hold $\\ell_{k} .=(0, \\ldots, 0)$, since the entries of $\\ell$ are non-negative. At this $\\ell$ the function $h$ is not continuous in $\\ell_{k j}$, since any increase of entry $\\ell_{k l}$ would cause a jump in $\\pi$ from $\\pi_{k l}=0$ to $\\pi_{k l}=1$. However, in this case we make use of Remark 3 , where we established that we can change the $k$-th row of the relative liability matrix without changing the result of the clearing vector function if $\\bar{\\ell}_{k}=0$. Hence, let $\\pi^{\\prime} \\in \\Pi^{N}$ be the relative liability matrix where we only changed the entry at index $k, l$ from 0 to 1 , i.e. $\\pi_{k l}^{\\prime}=1$ and $\\pi_{i j}^{\\prime}=\\pi_{i j}, i, j \\in[N], i \\neq k, j \\neq l$. Define $h^{\\prime}$ as $h^{\\prime}(\\ell)=\\left(\\pi^{\\prime}, \\bar{\\ell}\\right)$. Then for $\\bar{\\ell}_{k}=0$ the function $h^{\\prime}$ is continuous in $\\bar{\\ell}_{k l}$ and we get that\n\n$$\n\\operatorname{CV}(a, \\ell)=\\overline{\\operatorname{CV}}(a, \\pi, \\bar{\\ell})=\\overline{\\operatorname{CV}}\\left(a, \\pi^{\\prime}, \\bar{\\ell}\\right)=\\overline{\\operatorname{CV}}\\left(a, h^{\\prime}(\\ell)\\right)\n$$\n\nwhich is continuous in $\\ell_{k l}$ as a composition of continuous functions.\nBeyond continuity, one can prove further properties of the clearing vector function in its first component.\nProposition 2.11. The clearing vector function $\\mathrm{CV}: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}^{N}$\n\n$$\n(a, \\ell) \\mapsto \\operatorname{CV}(a, \\ell)\n$$\n\nis positive, monotone increasing, concave and non-expansive in its first component $a$.\nProof. This is shown in Lemma 5 of [22].\nTo conclude this section, we introduce the aggregation or loss function that we will consider, in order to obtain a univariate value associated to a financial network $(a, \\ell)$. The idea is to first calculate the clearing vector $\\mathrm{CV}(a, \\ell)$ of the network and subtract it from the total liability vector $\\bar{\\ell}$. This non-negative vector represents in each component the amount of liabilities towards other nodes that can not be provided by the corresponding node. If we sum up these values, we obtain a notion of shortfall of the network that we call $\\Lambda(a, \\ell)$.\n\nDefinition 2.12. Let $\\mathrm{CV}: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}^{N}$ be the function returning the clearing vector. Then the aggregation function $\\Lambda: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}$ is defined as\n\n$$\n\\Lambda(a, \\ell)=\\sum_{i=1}^{N} \\bar{\\ell}_{i}-\\mathrm{CV}_{i}(a, \\ell)\n$$\n\nwhere $\\bar{\\ell} \\in \\mathbb{R}^{N}$ is the total liability vector associated to $\\ell \\in \\mathbb{R}_{+}^{N \\times N}$, see (11).", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 3 Systemic risk of random financial networks \n\nIn this section we introduce the notion of systemic risk that we want to investigate. Therefore, we connect the results about the Eisenberg-Noe market clearing from Section 2.3 with systemic risk measures by considering random variables whose realisations are financial networks of the form in Definition 2.6.\n\nDefinition 3.1. We model a random network of $N$ financial institutions as\n(i) the institutions' assets $A=\\left(A_{1}, \\ldots, A_{N}\\right) \\in L^{0}\\left(\\mathbb{R}_{++}^{N}\\right)$ and\n(ii) the interbank liability matrix $L \\in L^{0}\\left(\\mathbb{R}_{+}^{N \\times N}\\right)$, where $L_{i j}$ denotes the size of the liability from bank $i$ to bank $j$. We do not allow self loops, hence $\\operatorname{diag}(L)=\\left(L_{11}, \\ldots, L_{N N}\\right)=(0, \\ldots, 0)$.\n\nModifying systemic risk measures as in (4), we define systemic risk measures of random financial networks as follows.\n\nDefinition 3.2. Let $\\mathbb{A} \\subseteq L^{0}(\\mathbb{R})$ be some acceptance set of $\\mathbb{R}$-valued random variables and $\\Lambda: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow$ $\\mathbb{R}$ be some aggregation function. Then the systemic risk measure $\\rho: G \\mapsto \\rho(G) \\in \\mathbb{R} \\cup\\{ \\pm \\infty\\}$ of random financial networks $G=(A, L)$ as in Definition 3.1 is defined as\n\n$$\n\\rho(G):=\\inf _{t \\in \\mathcal{C}}\\left\\{\\sum_{i=1}^{N} Y_{i} \\mid \\Lambda(A+Y, L) \\in \\mathbb{A}\\right\\}\n$$\n\nwhere $\\mathcal{C} \\subseteq \\mathcal{C}_{\\mathbb{R}}$ is some admissible subset of measurable random variables that sum to a constant value.\nNow we will make specific choices regarding $\\mathbb{A}, \\Lambda$, and $\\mathcal{C}$ in order to derive relevant properties of the systemic risk measure. For the remainder of the paper we make the following assumption.", "tables": {}, "images": {}}, {"section_id": 6, "text": "## Assumption 3.3.\n\n(i) Let $\\eta: L^{1}\\left(\\Omega, \\mathcal{F}, \\mathbb{P} ; \\mathbb{R}^{N}\\right) \\rightarrow \\mathbb{R}$ be a univariate risk measure that is\n\n- monotone increasing: for $X>Y: \\eta(X)>\\eta(Y)$\n- convex: for $\\lambda \\in[0,1]: \\eta(\\lambda X+(1-\\lambda) Y) \\leq \\lambda \\eta(X)+(1-\\lambda) \\eta(Y)$\n- normalised: $\\eta(0)=0$\n- continuous in $L^{1}: \\forall \\varepsilon>0 \\exists \\delta>0:\\|X-Y\\|_{1}<\\delta \\Longrightarrow|\\eta(X)-\\eta(Y)|<\\varepsilon$.\n\nWe define the acceptance set as\n\n$$\n\\mathbb{A}=\\left\\{Z \\in L^{1}(\\Omega, \\mathcal{F}, \\mathbb{P} ; \\mathbb{R}) \\mid \\eta(Z) \\leq b\\right\\}\n$$\n\nfor some acceptable risk threshold $b>0$.\n\n(ii) The liability matrix is integrable, $L \\in L^{1}\\left(\\mathbb{R}_{+}^{N \\times N}\\right)$. We denote the set of all random financial networks with integrable liability matrix by\n\n$$\n\\mathcal{G}:=\\left\\{(A, L) \\mid A \\in L^{0}\\left(\\mathbb{R}_{++}^{N}\\right), L \\in L^{1}\\left(\\mathbb{R}_{+}^{N \\times N}\\right), \\operatorname{diag}(L)=0\\right\\}\n$$\n\nand the random vector of total liabilities $\\left(\\bar{L}_{1}, \\ldots, \\bar{L}_{N}\\right)^{T}$ component-wise for $i \\in[N]$ as\n\n$$\n\\bar{L}_{i}:=\\sum_{j=1}^{N} L_{i j}\n$$\n\n(iii) The aggregation function $\\Lambda$ is chosen as in Definition 2.12.\n(iv) The set of admissible allocations $\\mathcal{C}$ consists of all non-negative allocations $Y$ whose stochastic components sum to some fixed value almost surely,\n\n$$\n\\mathcal{C}=\\left\\{Y \\in L^{0}\\left(\\mathbb{R}_{+}^{N}\\right) \\mid \\sum_{n=1}^{N} Y^{n} \\in \\mathbb{R}\\right\\} \\subseteq \\mathcal{C}_{\\mathbb{R}}\n$$\n\nRemark 4. For the systemic risk to be well-defined we can only consider non-negative random variables in $\\mathcal{C}$ due to the choice of the aggregation function. Furthermore, there is a trade-off between choosing $b \\in \\mathbb{R}_{+}, L \\in L^{\\infty}$ and $b>0, L \\in L^{1}$. If the liability matrix is bounded, Lemma 3.6 shows that we can find a finite amount of bailout capital such that the network is secured by a corresponding allocation $Y \\in \\mathcal{C}$. However, if the liability matrix is not bounded, we can only secure the network for $b>0$, but not necessarily for $b=0$. In this work we decide to continue with the choice $b>0$ and $L \\in L^{1}$. Finally, it is worth noting that the continuity property in (i) of the risk measure $\\eta$ could be dropped if $\\eta$ is a monotone increasing, convex, normalised and cash-invariant risk measure defined on $L^{\\infty}$ and if the liability matrix lies in $L^{\\infty}$ instead of $L^{1}$. This is because for two random variables $X, Y \\in L^{\\infty}$ we immediately obtain\n\n$$\n|\\eta(X)-\\eta(Y)| \\leq\\left|\\eta\\left(Y+\\|X-Y\\|_{\\infty}\\right)-\\rho(Y)\\right|=\\|X-Y\\|_{\\infty}\n$$\n\nwhich would be sufficient to recover all following results in this section.\nUnder Assumption 3.3 we can reformulate our definition of systemic risk. To indicate the dependence on the acceptable risk threshold $b$ we write $\\rho_{b}(G)$. Let $b>0$ and $\\Lambda$ be the aggregation function from Definition 2.12. Then we can rewrite the systemic risk of a network $G \\in \\mathcal{G}$ as\n\n$$\n\\rho_{b}(G)=\\inf _{Y \\in \\mathcal{C}}\\left\\{\\sum_{n=1}^{N} Y_{n} \\mid \\eta(\\Lambda(A+Y, L)) \\leq b\\right\\}\n$$\n\nwhere\n\n$$\n\\mathcal{C}=\\left\\{Y \\in L^{0}\\left(\\mathbb{R}_{+}^{N}\\right) \\mid \\sum_{n=1}^{N} Y_{n} \\in \\mathbb{R}\\right\\}\n$$\n\nFor a deterministic liability matrix $L \\in \\mathbb{R}_{+}^{N \\times N}$ this aligns well with existing definitions of systemic risk measures for vector valued risk factors, see for example (4). In particular, following [7], the systemic risk measure $\\rho_{b}: A \\mapsto \\rho_{b}((A, L))$ is a convex systemic risk measure.\nProposition 3.4. For a deterministic liability matrix $L \\in \\mathbb{R}_{+}^{N \\times N}$ the systemic risk measure\n\n$$\n\\begin{gathered}\n\\rho: L^{0}\\left(\\mathbb{R}_{++}^{N}\\right) \\rightarrow \\overline{\\mathbb{R}}_{+} \\\\\n\\rho(A)=\\rho_{b}((A, L))\n\\end{gathered}\n$$\n\nis a convex systemic risk measure. This means $\\rho$ is monotone and convex on $\\{\\rho(A)<+\\infty\\}$.\n\nIt may seem restrictive, that the previous result only holds on $\\{\\rho(X)<+\\infty\\}$, but in fact Proposition 3.7 shows that $\\rho_{b}((A, L))<+\\infty$ for all $(A, L) \\in \\mathcal{G}$.\nFor the proof of Proposition 3.4 the following observation comes in handy.\nLemma 3.5. The aggregation function $\\Lambda$ from Definition 2.12 is convex, non-expansive and monotone decreasing in its first component.\n\nProof. For $(a, \\ell) \\in \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N}$ the loss $\\Lambda$ is defined as\n\n$$\n\\Lambda(a, \\ell)=\\sum_{i=1}^{N} \\bar{\\ell}_{i}-\\mathrm{CV}_{i}(a, \\ell)\n$$\n\nwhere CV is the clearing vector function from Definition 2.9. Since CV is concave, non-expansive and monotone increasing in its first component (see Proposition 2.8) the proposition follows directly.\n\nProof of Proposition 3.4. We can apply Lemma 3.3 from [7] and only need to show the following two properties.\n(P1) For all $Y \\in \\mathcal{C}$ and the set\n\n$$\n\\mathcal{A}^{Y}=\\left\\{A \\mid A \\in L^{0}\\left(\\mathbb{R}_{+}^{N}\\right), \\eta(\\Lambda(A+Y, L)) \\leq b\\right\\}\n$$\n\nit holds: $A_{2} \\geq A_{1} \\in \\mathcal{A}^{Y} \\Longrightarrow A_{2} \\in \\mathcal{A}^{Y}$.\n(P2) For all $Y_{1}, Y_{2} \\in \\mathcal{C}$, all $A_{1}, A_{2} \\in L^{0}\\left(\\mathbb{R}_{+}^{N}\\right)$ with $\\eta\\left(\\Lambda\\left(A_{i}+Y_{i}, L\\right)\\right) \\leq b, i=1,2$ and all $\\lambda \\in[0,1]$ there exists $Y \\in \\mathcal{C}$ such that $|Y|_{1} \\leq \\lambda\\left|Y_{1}\\right|_{1}+(1-\\lambda)|Y_{2}|_{1}$ and $\\eta\\left(\\Lambda\\left(\\lambda A_{1}+(1-\\lambda) A_{2}+\\lambda Y_{1}+(1-\\lambda) Y_{2}, L\\right)\\right) \\leq b$.\n\nSince $\\Lambda$ is monotone decreasing for $A_{1} \\leq A_{2}$, it follows immediately $\\eta\\left(\\Lambda\\left(A_{2}+Y, L\\right)\\right) \\leq \\eta\\left(\\Lambda\\left(A_{1}+Y, L\\right)\\right)$, which shows (P1). For (P2) we choose $Y=\\lambda Y_{1}+(1-\\lambda) Y_{2}$. Then, since $\\Lambda$ is convex and $\\eta$ is monotone increasing and convex, we get\n\n$$\n\\begin{aligned}\n& \\eta\\left(\\Lambda\\left(\\lambda A_{1}+(1-\\lambda) A_{2}+\\lambda Y_{1}+(1-\\lambda) Y_{2}, L\\right)\\right) \\\\\n& \\leq \\eta\\left(\\Lambda\\left(\\lambda\\left(A_{1}+Y_{1}\\right)+(1-\\lambda)\\left(A_{2}+Y_{2}\\right), L\\right)\\right) \\\\\n& \\leq \\lambda \\eta\\left(\\Lambda\\left(A_{1}+Y_{1}, L\\right)\\right)+(1-\\lambda) \\eta\\left(\\Lambda\\left(A_{2}+Y_{2}, L\\right)\\right) \\leq b\n\\end{aligned}\n$$\n\nwhich shows (P2).\nLet us now return to the setting where the liability matrix is random. For $G \\in \\mathcal{G}$ we can show that the systemic risk $\\rho_{b}(G)$ is well-defined in the sense that it does not take the values $\\pm \\infty$, see Proposition 3.7 below. For the proof we will make use of the fact that for every $G$ there exists some finite allocation $Y<\\infty$, which renders $\\eta(\\Lambda(A+Y, L))$ arbitrarily small. We will show this by facilitating the following result regarding the case $L \\in L^{\\infty}$.\n\nLemma 3.6. Let $(A, L)=G \\in \\mathcal{G}$. If every component of the liability matrix $L$ is bounded by some value $U>0$, i.e. $L \\leq U$, then for\n\n$$\nY=\\left(\\begin{array}{c}\n(N-1) U \\\\\n\\vdots \\\\\n(N-1) U\n\\end{array}\\right)\n$$\n\nit holds that\n\n$$\n\\Lambda(A+Y, L)=0\n$$\n\nProof. Let $A, L$ and $Y$ be defined as described above. To see that we can completely secure the network, i.e. prevent all losses by injecting the amount $(N-1) U$ into each of the $N$ nodes, remember that CV yields for every realisation $(a+y, \\ell)=(A(\\omega)+Y(\\omega), L(\\omega)) \\in \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N}$ the value of the unique fixed point $F I X(\\Phi(\\cdot, a+y, \\pi, \\bar{\\ell})$ of the map\n\n$$\n\\Phi(p, a+y, \\pi, \\bar{\\ell})=\\left(\\pi^{T} p+a+y\\right) \\wedge \\bar{\\ell}\n$$\n\nwhere $\\bar{\\ell}_{i}=\\sum_{j=1}^{N} \\ell_{i j} \\leq(N-1) U$. However, for any non-negative vector $p \\in \\mathbb{R}^{N}$ and for our choice of $y$ from above this function is constant,\n\n$$\n\\begin{aligned}\n\\Phi(p, a+y, \\ell) & =\\left(\\pi^{T} p+a+y\\right) \\wedge \\bar{\\ell} \\\\\n& =\\underbrace{\\left(\\frac{\\pi^{T} p}{p}+\\underbrace{a}_{>0}+\\underbrace{(y-\\bar{\\ell})}_{\\geq 0}\\right)}_{\\geq 0} \\wedge \\bar{\\ell} \\\\\n& =\\bar{\\ell}\n\\end{aligned}\n$$\n\nConsequently, the only possible fixed point and hence value of CV is $\\operatorname{CV}(a+y, \\ell)=\\bar{\\ell}$, and we get\n\n$$\n\\Lambda(a+y, \\ell)=\\sum_{i=1}^{N} \\bar{\\ell}_{i}-\\mathrm{CV}_{i}(a+y, \\ell)=0\n$$\n\nHence $\\Lambda(A+Y, L)=0$, which finishes the proof.\nProposition 3.7. Let $G \\in \\mathcal{G}$ and $b>0$. Then $0 \\leq \\rho_{b}(G)<+\\infty$.\nProof. By definition it is clear that $\\rho_{b}(G) \\geq 0>-\\infty$. We show that $\\rho_{b}(G)<+\\infty$.\nIt is clear that\n\n$$\nL \\mathbb{1}_{\\{L \\nless M\\}}=L-L \\mathbb{1}_{\\{L \\leq M\\}} \\xrightarrow{M \\rightarrow \\infty} 0 \\quad \\mathbb{P} \\text {-a.s. }\n$$\n\nSince $L \\in L^{1}\\left(\\mathbb{R}_{+}^{N \\times N}\\right)$, we can apply dominated convergence to $|L|_{1}-|L|_{1} \\mathbb{1}_{\\{L \\leq M\\}} \\leq 2|L|_{1} \\in L^{1}(\\mathbb{R})$ and get\n\n$$\n\\left\\|L \\mathbb{1}_{\\{L \\nless M\\}}\\right\\|_{1}=\\left\\|L-L \\mathbb{1}_{\\{L \\leq M\\}}\\right\\|_{1} \\xrightarrow{M \\rightarrow \\infty} 0\n$$\n\nBy continuity of $\\eta$ we find an $M>0$ such that $\\eta\\left(|L|_{1} \\mathbb{1}_{\\{L \\nless M\\}}\\right)<b$, and choose $Y \\in L^{0}\\left(\\mathbb{R}_{+}^{N}\\right)$ as\n\n$$\nY=\\left(\\begin{array}{c}\n(N-1) M \\\\\n\\vdots \\\\\n(N-1) M\n\\end{array}\\right)\n$$\n\nThen we get that\n\n$$\n\\begin{aligned}\n\\eta(\\Lambda(A+Y, L)) & =\\eta\\left(\\Lambda(A+Y, L) \\mathbb{1}_{\\{L \\nless M\\}}+\\Lambda(A+Y, L) \\mathbb{1}_{\\{L \\leq M\\}}\\right) \\\\\n& =\\eta\\left(\\left(\\sum_{i=1}^{N} \\bar{L}_{i}-\\mathrm{CV}_{i}(A+Y, L)\\right) \\mathbb{1}_{\\{L \\nless M\\}}+0\\right) \\\\\n& \\leq \\eta\\left(\\mathbb{1}_{\\{L \\nless M\\}} \\sum_{i=1}^{N} \\bar{L}_{i}\\right)=\\eta\\left(|L|_{1} \\mathbb{1}_{\\{L \\nless M\\}}\\right)<b\n\\end{aligned}\n$$\n\nwhere Lemma 3.6 was used to argue that network can be completely secured in the bounded, i.e. $L \\leq M$, case.\nThis means that $Y$ secures the network and in particular\n\n$$\n\\rho_{b}(G) \\leq|Y|_{1}=N(N-1) M<+\\infty\n$$\n\nThe following corollary shows that in order compute the systemic risk we can restrict the set of available allocations to those bounded by some value $U>0$ if $U$ is large enough.\n\nCorollary 3.8. Let $G=(A, L) \\in \\mathcal{G}, b>0$ and let $\\rho_{b}(G)<U<+\\infty$. Then\n\n$$\n\\begin{aligned}\n\\rho_{b}(G): & =\\inf \\left\\{\\sum_{n=1}^{N} Y_{n} \\mid Y \\in \\mathcal{C}, \\eta(\\Lambda(A+Y, L)) \\leq b\\right\\} \\\\\n& =\\inf \\left\\{\\sum_{n=1}^{N} Y_{n} \\mid Y \\in \\mathcal{C}, Y \\leq U, \\eta(\\Lambda(A+Y, L)) \\leq b\\right\\}\n\\end{aligned}\n$$\n\nProof. It is clear that\n\n$$\n\\begin{aligned}\n& \\inf \\left\\{\\sum_{n=1}^{N} Y_{n} \\mid Y \\in \\mathcal{C}, \\eta(\\Lambda(A+Y, L)) \\leq b\\right\\} \\\\\n& \\leq \\inf \\left\\{\\sum_{n=1}^{N} Y_{n} \\mid Y \\in \\mathcal{C}, Y \\leq U, \\eta(\\Lambda(A+Y, L)) \\leq b\\right\\}\n\\end{aligned}\n$$\n\nTo see the reverse inequality, let $\\left(Y^{(k)}\\right)_{k \\in \\mathbb{N}} \\subseteq \\mathcal{C}$ be such that for all $k \\in \\mathbb{N}, \\eta\\left(\\Lambda\\left(A+Y^{(k)}, L\\right)\\right) \\leq b$ and $\\left|Y^{(k)}\\right|_{1} \\xrightarrow{k \\rightarrow \\infty} \\rho_{b}(G)$.\nWithout loss of generality we can impose that $\\left|Y^{(k)}\\right|_{1}$ actually converges monotonously from above. Then there exists some $k^{*} \\in \\mathbb{N}$ such that $\\left|Y^{(k)}\\right|_{1} \\leq U$ for all $k \\geq k^{*}$. Define\n\n$$\n\\tilde{Y}^{(k)}= \\begin{cases}Y^{(k)}, & k \\geq k^{*} \\\\ Y^{\\left(k^{*}\\right)}, & k<k^{*}\\end{cases}\n$$\n\nThe sequence $\\left(\\tilde{Y}^{(k)}\\right)_{k \\in \\mathbb{N}}$ is bounded by $U$, because a vector of non-negative entries summing up to any value $U$ is trivially bounded by $U$ in every component. Furthermore, $\\left(\\left|\\tilde{Y}^{(k)}\\right|_{1}\\right)_{k \\in \\mathbb{N}}$ converges to $\\rho_{b}(G)$ as well and hence\n\n$$\n\\begin{aligned}\n& \\inf \\left\\{\\sum_{n=1}^{N} Y_{n} \\mid Y \\in \\mathcal{C}, \\eta(\\Lambda(A+Y, L)) \\leq b\\right\\} \\\\\n\\geq & \\inf \\left\\{\\sum_{n=1}^{N} Y_{n} \\mid Y \\in \\mathcal{C}, Y \\leq U, \\eta(\\Lambda(A+Y, L)) \\leq b\\right\\}\n\\end{aligned}\n$$\n\nwhich concludes the proof.\nNow we want to discuss some further properties of the systemic risk measure as defined in (19). In particular we will prove existence of an optimal random allocation for the systemic risk $\\rho_{b}(G)$, therefore the infimum is actually a minimum. To this end we define optimal allocations as follows.\n\nDefinition 3.9. We call an allocation $Y$ optimal for the systemic risk measure defined in (19) if and only if $Y \\in \\mathcal{C},\\left|Y\\right|_{1}=\\rho_{b}(G)$ and $\\eta(\\Lambda(A+Y, L)) \\leq b$.\n\nFurthermore, while it is not necessary that an optimal allocation should be unique, we can show that the set of optimal allocations is convex and that the risk of loss equals exactly $b$ for an optimal allocation. The following proposition summarises these results.\n\nProposition 3.10. Let $G \\in \\mathcal{G}, b>0$ and $\\rho_{b}(G)$ from (19). The following holds.\n(i) There exists an optimal $Y^{*}$ where the infimum $\\rho_{b}(G)$ is attained, i.e. it is a minimum.\n\n(ii) For each optimal $Y^{*} \\in \\mathcal{C}$ the risk of loss equals exactly $b$,\n\n$$\n\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right)=b\n$$\n\n(iii) The set of all optimal allocations is a convex set.\n\nProof. Let $G=(A, L) \\in \\mathcal{G}, b>0$, and $Y^{(k)} \\subset \\mathcal{C}$ be a sequence such that $\\forall k \\in \\mathbb{N}: \\eta\\left(\\Lambda\\left(A+Y^{(k)}, L\\right)\\right) \\leq b$, and $|Y|_{1}^{(k)} \\xrightarrow[k \\rightarrow \\infty]{ } \\rho_{b}(G)$.\nWithout loss of generality we choose the sequence such that $|Y|_{1}^{(k)}$ is monotonously decreasing towards $\\rho_{b}(G)$. By Corollary 3.8 we can choose these allocations uniformly bounded by some $U \\in \\mathbb{R}, U>\\rho_{b}(G)$. Then, since $0 \\leq Y^{(k)} \\leq U$ is bounded, from Koml\u00f3s' Theorem A. 1 we obtain a new sequence\n\n$$\n\\hat{Y}^{(k)} \\in \\operatorname{conv}\\left\\{Y^{(k)}, Y^{(k+1)}, \\ldots\\right\\}\n$$\n\nthat converges almost surely to some limit random variable $\\hat{Y}^{*}$.\nSince $\\mathcal{C}$ is convex we know that $\\forall k: \\hat{Y}^{(k)} \\in \\mathcal{C}$. Furthermore, since $\\operatorname{CV}(A+Y, L)$ is concave in $Y$ we get that for $n_{k} \\in \\mathbb{N}, j \\in\\left\\{1, \\ldots, n_{k}\\right\\}, k_{j} \\in\\{k, k+1, \\ldots\\}, a_{k_{j}} \\geq 0, \\sum_{j=1}^{n_{k}} a_{k_{j}}=1$ such that,\n\n$$\n\\hat{Y}^{(k)}=\\sum_{j=1}^{n_{k}} a_{k_{j}} Y^{\\left(k_{j}\\right)}\n$$\n\nit holds that\n\n$$\n\\begin{aligned}\n\\eta\\left(\\Lambda\\left(A+\\hat{Y}^{(k)}, L\\right)\\right) & =\\eta\\left(\\sum_{i=1}^{N} \\hat{L}_{i}-\\mathrm{CV}_{i}\\left(A+\\hat{Y}^{(k)}, L\\right)\\right) \\\\\n& =\\eta\\left(\\sum_{i=1}^{N} \\hat{L}_{i}-\\mathrm{CV}_{i}\\left(A+\\sum_{j=1}^{n_{k}} a_{k_{j}} Y^{\\left(k_{j}\\right)}, L\\right)\\right) \\\\\n& \\leq \\eta\\left(\\sum_{i=1}^{N} \\hat{L}_{i}-\\sum_{j=1}^{n_{k}} a_{k_{j}} \\mathrm{CV}_{i}\\left(A+Y^{\\left(k_{j}\\right)}, L\\right)\\right) \\\\\n& \\leq \\sum_{j=1}^{n_{k}} a_{k_{j}} \\eta\\left(\\Lambda\\left(A+\\hat{Y}^{\\left(k_{j}\\right)}, L\\right)\\right) \\leq b\n\\end{aligned}\n$$\n\nand since we choose $\\left(Y^{(k)}\\right)_{k}$ such that $\\sum_{i=1}^{N} Y_{i}^{(k)}$ is decreasing\n\n$$\n\\begin{aligned}\n& \\sum_{i=1}^{N} \\hat{Y}_{i}^{(k)}=\\sum_{i=1}^{N} \\sum_{j=1}^{n_{k}} a_{k_{j}} Y_{i}^{\\left(k_{j}\\right)}=\\sum_{j=1}^{n_{k}} a_{k_{j}} \\sum_{i=1}^{N} Y_{i}^{\\left(k_{j}\\right)} \\\\\n& \\leq \\sum_{i=1}^{N} Y_{i}^{(k)} \\underset{k \\rightarrow \\infty}{ } \\rho_{b}(G)\n\\end{aligned}\n$$\n\nHence the sequence of $\\hat{Y}^{(k)}$ fulfills the same requirements as $Y^{(k)}$ and for the rest of the proof we will only consider the converging sequence of $\\hat{Y}^{(k)}$ with limit $\\hat{Y}^{*}$, but to ease up the notation we write $Y^{(k)}$ and $Y^{*}$ instead of $\\hat{Y}^{(k)}$ and $\\hat{Y}^{*}$.\n(i) Firstly, it is clear that $Y^{*} \\in \\mathcal{C}$, because we know that $0 \\leq Y^{k} \\leq U$ and hence $U \\geq Y^{*} \\in L^{\\infty}\\left(\\mathbb{R}^{N}\\right)$. Further, by construction the components of $Y^{*}$ sum to the constant value of $\\rho_{b}(G)$,\n\n$$\n\\left|Y^{*}\\right|_{1}=\\sum_{i=1}^{N} Y_{i}^{\\infty}=\\sum_{i=1}^{N} \\lim _{k \\rightarrow \\infty} Y_{i}^{(k)}=\\lim _{k \\rightarrow \\infty} \\sum_{i=1}^{N} Y_{i}^{(k)}=\\rho_{b}(G)\n$$\n\nNext we show that $Y^{*}$ actually fulfills the risk constraint $\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right) \\leq b$. We assume $\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right)=$ $b+\\varepsilon$ for some $\\varepsilon>0$ and lead this into a contradiction.\n\nFor any $k \\in \\mathbb{N}$ we have\n\n$$\n\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right)-\\eta\\left(\\Lambda\\left(A+Y^{(k)}, L\\right)\\right) \\geq \\varepsilon\n$$\n\nFurthermore, since $Y^{(k)} \\xrightarrow{k \\rightarrow \\infty} Y^{*}$ and since $\\Lambda$ is continuous it also holds\n\n$$\n\\Lambda\\left(A+Y^{(k)}, L\\right) \\xrightarrow{k \\rightarrow \\infty} \\Lambda\\left(A+Y^{*}, L\\right)\n$$\n\nWe use dominated convergence on $\\Lambda(\\cdot, L) \\leq|L|_{1}$ to obtain\n\n$$\n\\left\\|\\Lambda\\left(A+Y^{(k)}, L\\right)-\\Lambda\\left(A+Y^{*}, L\\right)\\right\\|_{1} \\xrightarrow{k \\rightarrow \\infty} 0\n$$\n\nBut by the continuity of $\\eta$ this would mean that\n\n$$\n\\eta\\left(\\Lambda\\left(A+Y^{(k)}, L\\right)\\right) \\xrightarrow{k \\rightarrow \\infty} \\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right)\n$$\n\nwhich is a contradiction to\n\n$$\n\\forall k \\in \\mathbb{N}: \\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right)-\\eta\\left(\\Lambda\\left(A+Y^{(k)}, L\\right)\\right) \\geq \\varepsilon\n$$\n\nTherefore it must hold that $\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right) \\leq b$.\nThis concludes the first part of the proof where we showed that the infimum is actually a minimum.\n(ii) Now we continue and show that the risk actually equals $b$. We already showed \" $\\leq$ \". In order to show \" $\\geq$ \", assume $\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right)=b-\\varepsilon$. By continuity of $\\eta$ we know for $\\varepsilon>0$ there exists a $\\delta>0$ such that for any $\\widetilde{Y} \\in \\mathcal{C}$\n\n$$\n\\left\\|\\Lambda\\left(A+Y^{*}, L\\right)-\\Lambda(A+\\widetilde{Y}, L)\\right\\|_{1}<\\delta \\Longrightarrow\\left|\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right)-\\eta(\\Lambda(A+\\widetilde{Y}, L))\\right|<\\varepsilon\n$$\n\nIf we define\n\n$$\n\\widetilde{Y}=Y^{*}-\\left(\\begin{array}{c}\n\\delta / 2 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right)=-\\frac{\\delta}{2} e_{1}\n$$\n\nthen since $\\Lambda$ is non-expansive (Lemma 3.5) it holds\n\n$$\n\\left|\\Lambda\\left(A+Y^{*}, L\\right)-\\Lambda(A+\\widetilde{Y}, L)\\right|_{1} \\leq\\left|Y^{*}-\\widetilde{Y}\\right|_{1}=\\delta / 2\n$$\n\nTherefore, by continuity and monotonicity of $\\eta$ it holds that\n\n$$\n\\eta(\\Lambda(A+\\widetilde{Y}, L)) \\leq \\rho_{b}(G)-\\varepsilon / 2 \\leq b\n$$\n\nHowever, this means that $\\widetilde{Y}$ is a cheaper bailout strategy than $Y^{*}$ (it sums to $\\rho_{b}(G)-\\delta / 2$ instead of $\\rho_{b}(G)$ ) that makes the system acceptable. But then $\\rho_{b}(G)$ can not be the infimum, which is a contradiction. Hence it must be true that $\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right) \\geq b$.\nThis shows that $\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right)=b$.\n\n(iii) In the last part of the proof we show that the set of points where the minimum is attained forms a convex set. Let $Y^{(1)}, Y^{(2)}$ be two optimal bailout strategies with respect to the systemic risk $\\rho_{b}(G)$ for some stochastic financial network $G \\in \\mathcal{G}$ and $b>0$. Let $\\lambda \\in(0,1)$ and $\\widetilde{Y}=\\lambda Y^{(1)}+(1-\\lambda) Y^{(2)}$. Thus we get\n\n$$\n\\begin{aligned}\n& \\eta\\left(\\Lambda(A+\\widetilde{Y}, L)\\right)=\\eta\\left(\\Lambda\\left(\\lambda\\left(A+Y^{(1)}\\right)+(1-\\lambda)\\left(A+Y^{(2)}\\right), L\\right)\\right) \\\\\n& \\leq \\eta\\left(\\lambda \\Lambda\\left(A+Y^{(1)}, L\\right)+(1-\\lambda) \\Lambda\\left(A+Y^{(2)}, L\\right)\\right) \\\\\n& \\leq \\lambda \\eta\\left(\\Lambda\\left(A+Y^{(1)}, L\\right)\\right)+(1-\\lambda) \\eta\\left(\\Lambda\\left(A+Y^{(2)}, L\\right)\\right)=b\n\\end{aligned}\n$$\n\nWith the same arguments as before it also holds \" $\\geq$ \": Otherwise, we could subtract some $\\varepsilon>0$ and make the financial network acceptable with costs strictly less than $\\rho_{b}(G)$. Hence $\\widetilde{Y}$ is an optimal bailout strategy and the set of optimal solutions must be convex.\n\nRemark 5. There is indeed no reason to assume an optimal solution should be unique. Let $\\eta(\\cdot)=\\mathbb{E}[\\cdot]$ and imagine a deterministic financial network $(A, L)$ where two nodes 1,2 owe exactly 1 unit of capital to a third node 3 . There are no other obligations and all external assets are 0 . Without any bailout capital the expected loss of this network is 2 . Assume we want to find bailout strategies that reduce the loss to 1 . It can be easily recognised that all bailout strategies that serve this purpose are $e_{1}, e_{2}$ and convex combinations of them. For any $\\lambda \\in(0,1)$,\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\Lambda\\left(A+\\lambda e_{1}+(1-\\lambda) e_{2}, L\\right)\\right] \\\\\n& =\\Lambda\\left(A+\\lambda e_{1}+(1-\\lambda) e_{2}, L\\right)=1-\\lambda+1-(1-\\lambda)=1\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 4 Computation of Systemic Risk Measures\n### 4.1 Reformulation of Systemic Risk Measures\n\nIn order to formulate an algorithm for the computation of the systemic risk measures defined in Section 3, we introduce an equivalent formulation that will be useful for the numerical computation.\n\nDefinition 4.1. Let $G \\in \\mathcal{G}$ be some stochastic financial network and $c \\in \\mathbb{R}_{+}$. We define the inner risk $\\rho_{c}^{I}(G)$ as the infimum aggregated risk that can be obtained by all capital allocations $Y$ that fulfill the capital constraint $\\sum_{n=1}^{N} Y_{n}=c$, that is,\n\n$$\n\\rho_{c}^{I}(G)=\\inf _{Y \\in \\mathcal{C}^{c}}\\{\\eta(\\Lambda(A+Y, L))\\}\n$$\n\nwhere\n\n$$\n\\mathcal{C}^{c}:=\\left\\{Y \\in L^{0}\\left(\\mathbb{R}_{+}^{N}\\right) \\mid \\sum_{n=1}^{N} Y_{n}=c\\right\\}\n$$\n\nDefinition 4.2. With the notion of inner risk from Definition 4.1 we call a bailout capital allocation $Y^{c} \\in \\mathcal{C}^{c}$ optimal for capital level $c \\in \\mathbb{R}_{+}$if and only if\n\n$$\n\\eta\\left(\\Lambda\\left(A+Y^{c}, L\\right)\\right)=\\rho_{c}^{I}(G)\n$$\n\nDefinition 4.3. Let $b>0$, we define the outer risk as\n\n$$\n\\rho_{b}^{O}(G)=\\inf \\left\\{c \\in \\mathbb{R}_{+} \\mid \\rho_{c}^{I}(G) \\leq b\\right\\}\n$$\n\nLemma 4.4. The infimum in (24) is attained, hence it is indeed a minimum.\nProof. Let $\\left(Y^{(k)}\\right)_{k \\in \\mathbb{N}} \\subseteq \\mathcal{C}^{c}$ be a sequence of allocations such that\n\n$$\n\\eta\\left(\\Lambda\\left(A+Y^{(k)}, L\\right)\\right) \\xrightarrow{k \\rightarrow \\infty} \\rho_{c}^{I}(G)\n$$\n\nSince the $Y^{(k)}$ are non-negative and sum to $c$ they are also bounded by $c$. Hence, we can use Koml\u00f3s' Theorem (see Theorem A. 1 in the appendix) in the same manner as in (21) to obtain a sequence $\\left(\\tilde{Y}^{(k)}\\right)_{k \\in \\mathbb{N}}$ that lies in $\\mathcal{C}^{c}$ as well, because\n\n$$\n\\tilde{Y}^{(k)} \\in \\operatorname{conv}\\left(\\left\\{Y^{(k)}, Y^{(k+1)}, \\ldots\\right\\}\\right)\n$$\n\nand that converges almost surely to some $\\tilde{Y}^{\\infty} \\in \\mathcal{C}^{c}$. Then also $\\Lambda\\left(A+\\tilde{Y}^{(k)}, L\\right)$ converges almost surely to $\\Lambda\\left(A+\\tilde{Y}^{\\infty}, L\\right)$. By dominated convergence it also converges in $L^{1}$, and we can use continuity of $\\eta$ to obtain\n\n$$\n\\begin{aligned}\n& \\eta\\left(\\Lambda\\left(A+\\tilde{Y}^{\\infty}, L\\right)\\right)=\\eta\\left(\\Lambda\\left(A+\\lim _{k \\rightarrow \\infty} \\tilde{Y}^{(k)}, L\\right)\\right) \\\\\n& =\\eta\\left(\\lim _{k \\rightarrow \\infty} \\Lambda\\left(A+\\tilde{Y}^{(k)}, L\\right)\\right)=\\lim _{k \\rightarrow \\infty} \\eta\\left(\\Lambda\\left(A+\\tilde{Y}^{(k)}, L\\right)\\right)=\\rho_{c}^{I}(G)\n\\end{aligned}\n$$\n\nHence the infimum is attained by $\\tilde{Y}^{\\infty}$.\nProposition 4.5. For the outer risk $\\rho_{b}^{O}(G)$ and the systemic risk $\\rho_{b}(G)$ it holds\n\n$$\n\\rho_{b}(G)=\\rho_{b}^{O}(G)\n$$\n\nProof. Let $\\left(c^{(k)}\\right)_{k \\in \\mathbb{N}} \\subseteq \\mathbb{R}_{+}$be a sequence of real numbers such that $\\rho_{c}^{I}(G) \\leq b$ and $c^{(k)} \\rightarrow \\rho_{b}^{O}(G)$. With Lemma 4.4 we get for every $c^{(k)}$ an allocation $Y^{(k)} \\in \\mathcal{C}^{a}$ such that\n\n$$\n\\eta\\left(\\Lambda\\left(A+Y^{(k)}, L\\right)\\right) \\leq b\n$$\n\nTrivially, this sequence fulfills $Y^{(k)} \\in \\mathcal{C}$ and hence those allocations are admissible for bounding the systemic risk $\\rho_{b}(G)$ from above. We conclude that $\\rho_{b}(G) \\leq \\rho_{b}^{O}(G)$.\nTo show the other direction, consider any $Y^{*} \\in \\mathcal{C}$ that is optimal for $\\rho_{b}(G)$. This means\n\n$$\n\\left|Y^{*}\\right|_{1}=\\rho_{b}(G)\n$$\n\nand by Proposition 3.10\n\n$$\n\\eta\\left(\\Lambda\\left(A+Y^{*}, L\\right)\\right)=b\n$$\n\nThen it holds that\n\n$$\n\\rho_{b}(G) \\in\\left\\{c \\in \\mathbb{R}_{+} \\mid \\rho_{c}^{I}(G) \\leq b\\right\\}\n$$\n\nSubsequently it must always be true that\n\n$$\n\\rho_{b}^{O}(G) \\leq \\rho_{b}(G)\n$$\n\nwhich concludes the proof.\nCorollary 4.6. The infimum in (26) is attained.\n\nProof. Since $\\rho_{b}(G)$ is attained (Proposition 3.10) and $\\rho_{b}(G)=\\rho_{b}^{O}(G)$ (Proposition 4.5) it follows\n\n$$\n\\rho_{b}^{O}(G) \\in\\left\\{c \\in \\mathbb{R}_{+} \\mid \\rho_{c}^{I}(G) \\leq b\\right\\}\n$$\n\nProposition 4.5 and Corollary 4.6 show that in order to calculate the systemic risk $\\rho_{b}(G)$ we can first find all capital levels $c \\in \\mathbb{R}_{+}$for which the inner risk $\\rho_{c}^{I}(G)$ is still lower or equal than $b$ and then choose the minimum of those values. This procedure will be utilised by the algorithm when approximating the systemic risk of a network.\nWe next turn to the task of finding, for each capital level $c \\in \\mathbb{R}_{+}$, a \"good\" approximation of a bailout capital allocation $Y^{c} \\in \\mathcal{C}^{c}$ that is optimal. As a first step, we show that there exist measurable functions $H^{c}: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}_{+}^{N}$ such that $H^{c}(A, L)$ is optimal for capital level $c \\in \\mathbb{R}_{+}$. This result is useful, since later we will prove that we can approximate measurable functions arbitrarily close by neural networks.\nProposition 4.7. There exist measurable functions $H^{c}: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}_{+}^{N}$ such that\n\n$$\nH^{c}(A, L) \\in \\mathcal{C}^{c}\n$$\n\nis an optimal bailout capital allocation at level $c \\in \\mathbb{R}_{+}$.\nProof. Let $h^{c}: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathcal{P}\\left(\\mathbb{R}_{+}^{N}\\right)$ be the set valued argmin function\n\n$$\nh^{c}(a, \\ell)=\\operatorname{argmin}\\left\\{\\Lambda(a+y, \\ell) \\mid y \\in \\mathbb{R}_{+}^{N},|y|=c\\right\\}\n$$\n\nThe min and the argmin of the set on the right hands side of (28) are well defined since $\\Lambda(a+\\cdot, \\ell)$ is a continuous function and we minimise it over a compact set.\nThen, according to Theorem 14.37 in [56], the argmin of a continuous function is measurable and we can even find a measurable selection $s: \\mathcal{P}\\left(\\mathbb{R}_{+}^{N}\\right) \\rightarrow \\mathbb{R}_{+}^{N}$. This means that the function $H^{c}: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}_{+}^{N}$ defined as\n\n$$\nH^{c}(a, \\ell):=s\\left(h^{c}(a, \\ell)\\right)\n$$\n\nis measurable.\nIt follows that $H^{c}(A, L) \\in \\mathcal{C}^{c}$ and by the definition of $\\rho_{c}^{I}(G)$ it is clear that $\\rho_{c}^{I}(G) \\leq \\eta\\left(\\Lambda\\left(A+H^{c}(A, L), L\\right)\\right)$. However, by the definition of $H^{c}$ and the monotonicity of $\\Lambda$ and $\\eta$ it holds that\n\n$$\n\\eta\\left(\\Lambda\\left(A+H^{c}(A, L), L\\right)\\right) \\leq \\rho_{c}^{I}(G)\n$$\n\nTherefore, $H^{c}(A, L)$ is optimal for capital level $c \\in \\mathbb{R}_{+}$.", "tables": {}, "images": {}}, {"section_id": 8, "text": "# 4.2 Iterative Optimisation Algorithm \n\nComputing systemic risk is not straight forward due to the scenario dependent allocation. However, the results from Proposition 4.5 allow to formulate an iterative optimisation algorithm.\nThe general idea is the following. Starting from some initial bailout capital $c_{\\text {init }} \\in \\mathbb{R}_{+}$, an outer numerical optimisation will first evaluate the systemic risk at the current capital level $c_{\\text {curr }}$, using the optimal allocation obtained from an inner optimisation procedure. Then it will update the current capital level to a new capital level $c_{\\text {new }}$ for the next iteration.\nBy Proposition 4.7, for any capital level $c \\in \\mathbb{R}_{+}$there exists a measurable function $H^{c}: \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N} \\rightarrow \\mathbb{R}_{+}^{N}$ such that $H^{c}(A, L)$ is an optimal bailout allocation.\nSince $c=0$ indicates that there is no capital to be allocated we focus on the case $c>0$ and consider the normalised function\n\n$$\n\\varphi^{c}=\\frac{H^{c}}{c}\n$$\n\nwhich we aim to approximate by a parameterised function $\\phi^{\\theta}$. Here $\\theta \\in \\mathbb{R}^{m}$, for some $m \\in \\mathbb{N}$, is a collection of parameters.\nMore in detail, we propose to approximate $\\varphi^{c}$ by neural networks $\\phi^{\\theta}$. For the initial capital level $c_{\\text {init }} \\in \\mathbb{R}_{+}$ the parameters $\\theta$ will be initialised randomly. At the end of each optimisation step for capital level $c \\in \\mathbb{R}_{+}$, we obtain parameters $\\theta_{o p t}^{c}$ associated with the optimal bailout capital allocation for capital level $c$. Then at the beginning of the next optimisation round for level $c_{\\text {new }} \\in \\mathbb{R}_{+}$we will use the parameters $\\theta_{o p t}^{c}$ of the previous step as the starting point for the inner optimisation.\nThe objective for the training of $\\phi^{\\theta}$ for some fixed capital level $c \\in \\mathbb{R}_{+}$is to minimise\n\n$$\nJ_{I}(\\theta)=\\Lambda\\left(A+c \\phi^{\\theta}(A, L), L\\right)\n$$\n\nRecalling the notion of inner risk (4.1) and optimality for some risk level (4.2), it is clear that if $J_{I}$ is minimal for $c \\in \\mathbb{R}_{+}$, then due to the monotonicity of $\\eta$ also $\\eta\\left(\\Lambda\\left(A+c \\phi^{\\theta}(A, L), L\\right)\\right)$ is minimal and hence the allocation $c \\phi^{\\theta}$ is optimal for $c \\in \\mathbb{R}_{+}$.\nWith the inner training procedure which finds a parameter vector $\\theta^{c}$ for any $c \\in \\mathbb{R}_{+}$at hand, the training objective of the outer optimisation is to minimise\n\n$$\nJ_{O}(c)=\\mu_{1}\\left(\\eta\\left(\\Lambda\\left(A+c \\phi^{\\theta^{c}}(A, L), L\\right)\\right)-b\\right)^{+}+\\mu_{2} c\n$$\n\nwhere $\\mu_{1} \\geq \\mu_{2}>0$ are hyper-parameters. ${ }^{3}$ The first term penalises configurations where the risk constraint is not fulfilled while the second term provides an incentive for the capital level to be as small as possible. The objective function reaches a minimum at the smallest capital level $c \\in \\mathbb{R}_{+}$at which the risk is lower or equal to $b>0$. This is exactly the outer risk defined in Definition 4.3 which is equal to the systemic risk according to Proposition 4.5.\nIn order to calculate the objective functions we draw i.i.d. samples according to the distribution of $(A, L)$ under $\\mathbb{P}$\n\n$$\n(\\mathfrak{A}, \\mathfrak{L})=\\left\\{\\left.\\left(A^{\\omega_{i}}, L^{\\omega_{i}}\\right) \\right\\rvert\\, \\omega_{i} \\in \\Omega, i=1, \\ldots, N_{M C}\\right\\}\n$$\n\nand approximate $J_{I}, J_{O}$ by their empirical estimations. The optimisation of the parameters $\\theta$ and $c$ is conducted by computing gradients of the respective objective function and applying a gradient decent method. The described algorithm outputs an approximation of the systemic risk and a corresponding bailout capital allocation $c \\phi^{\\theta}$. A summary can be found in Algorithm 1. In addition, a simple variation of the algorithm that provides an approximation of the optimal bailout capital allocation for a fixed capital level $c \\in \\mathbb{R}_{+}$is summarised in Algorithm 2.\nIf the approximation of the optimal bailout allocations works sufficiently well for every capital level $c \\in \\mathbb{R}_{+}$, the approximated systemic risk obtained in the optimisation will be close to the actual systemic risk $\\rho_{b}(G)$. Furthermore, even if the optimal allocations are not solved accurately, in any case the lowest capital level $c \\in \\mathbb{R}_{+}$that we find is an upper bound for $\\rho_{b}(G)$. Additionally, $c \\phi^{\\theta}$ provides a bailout strategy where the system is secured. Hence, even rather \"naive\" allocations $c \\phi^{\\theta}$ can yield some valuable information. They provide a value $c \\in \\mathbb{R}_{+}$that suffices to secure the system and provide instructions on how to allocate the capital in each scenario.\nRemark 6. It is worth pointing out that also the traditional approach (3) of a deterministic allocation of bailout capital $\\left(m_{1}, \\ldots, m_{N}\\right) \\in \\mathbb{R}^{N}$, i.e. an allocation that does not depend on scenario $\\omega \\in \\Omega$, is covered by this case. Then $\\phi^{\\theta}$ would not parameterise a function that maps $(a, \\ell) \\in \\mathbb{R}_{++}^{N} \\times \\mathbb{R}_{+}^{N \\times N}$ to an optimal distribution, but instead directly represents the scenario agnostic distribution\n\n$$\n\\phi^{\\theta}(a, \\ell)=\\phi^{\\theta}=\\left(\\theta_{1}, \\ldots, \\theta_{N}\\right)^{T}\n$$\n\nthat maps every scenario $(a, \\ell)$ to the same constant bailout distribution that minimises the systemic risk of G.\n\nIn the next section we discuss several types of neural networks that we use as $\\phi^{\\theta}$ in our algorithms for the computation of systemic risk measures.\n\n[^0]\n[^0]:    ${ }^{3}$ In the numerical experiments in Section 6.3 we chose $\\mu_{1}=1, \\mu_{2}=0.001$.\n\n```\nAlgorithm 1 Algorithm to approximate minimal bailout capital and its optimal allocation\n    Input: Epochs \\(N_{O}, N_{I}\\), data \\((\\mathfrak{A}, \\mathfrak{L})\\), initial capital level \\(c\\), neural network \\(\\phi^{\\theta}\\), risk threshold \\(b\\).\n    for \\(j=1, \\ldots, N_{O}\\) do\n        for \\(k=1, \\ldots, N_{I}\\) do\n            compute empirical estimation \\(\\hat{J}_{I}\\) of\n                \\(J_{I}(\\theta)=\\Lambda\\left(A+c \\phi^{\\theta}(A, L), L\\right)\\)\n            compute gradient of \\(\\nabla_{\\theta} \\hat{J}_{I}\\)\n            update \\(\\theta\\)\n    end for\n    compute empirical estimation \\(\\hat{J}_{O}\\) of\n                \\(J_{O}(c)=\\mu_{1}\\left(\\eta\\left(\\Lambda\\left(A+c \\phi^{\\theta^{c}}(A, L), L\\right)\\right)-b\\right)^{+}+\\mu_{2} c\\)\n    compute gradient \\(\\nabla_{a} \\hat{J}_{O}\\)\n    update \\(c\\)\n    end for\n    Output: \\(c, \\phi^{\\theta}\\)\n```\n\n```\nAlgorithm 2 Algorithm to approximate optimal allocation of fixed bailout capital \\(c \\in \\mathbb{R}_{+}\\)\n    Input: Epochs \\(N_{I}\\), data \\((\\mathfrak{A}, \\mathfrak{L})\\), capital level \\(c\\), neural network \\(\\phi^{\\theta}\\).\n    for \\(k=1, \\ldots, N_{I}\\) do\n        compute empirical estimation \\(\\hat{J}_{I}\\) of\n                \\(J_{I}(\\theta)=\\Lambda\\left(A+c \\phi^{\\theta}(A, L), L\\right)\\)\n    compute gradient of \\(\\nabla_{\\theta} \\hat{J}_{I}\\)\n    update \\(\\theta\\)\n    end for\n    Output: \\(\\phi^{\\theta}\\)\n```", "tables": {}, "images": {}}, {"section_id": 9, "text": "# 5 Approximation via neural networks \n\nIn this section, we introduce three different neural network architectures that we use to approximate the function that maps a financial network to an optimal bailout allocation as in Proposition 4.7. More precisely, in addition to classical feedforward neural networks (FNNs) and graph neural networks (GNNs) we present the class of permutation equivariant neural networks (PENNs), which were first described in [41]. We extend this architecture by a modification that we call extended permutation euqivariant neural network (XPENN). These XPENNs unite favourable properties of GNNs and PENNs and perform well in the numerical experiments of Section 6, where we compare all these neural network architectures.", "tables": {}, "images": {}}, {"section_id": 10, "text": "### 5.1 Feedforward neural networks\n\nFeedforward neural networks (FNNs) are the original and simplest type of neural networks. They have been successfully applied to a wide range of problems and are typically defined as follows.\n\nDefinition 5.1. Let $L \\in \\mathbb{N}$ be a number of layers and $d_{0}, \\ldots, d_{L} \\in \\mathbb{N}$ corresponding layer dimensions. Let further $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be some Borel measurable function and $W_{l}: \\mathbb{R}^{d_{l-1}} \\rightarrow \\mathbb{R}^{d_{l}}$ be an affine function for all\n\n$l \\in[L]$. Then a feedforward neural network $F: \\mathbb{R}^{d_{0}} \\rightarrow \\mathbb{R}^{d_{L}}$ is defined by\n\n$$\nF(x)=W_{L} \\circ F_{L-1} \\circ \\cdots \\circ F_{1}\n$$\n\nwhere for $l=1, \\ldots, L-1: F_{l}=\\sigma \\circ W_{l}$ and the activation function $\\sigma$ is applied component-wise.\nMotivated by their success in applications, also theoretical properties have been extensively studied. An important starting point are universal approximation theorems for learning unknown functions, see [19, 43, 51]. For example, it is a well known result that neural networks are universal approximators for measurable functions, see for example the following result from [51], as formulated in [10], that fits well to our setting.\n\nProposition 5.2 (Theorem B.1, [10]). Assume the activation function $\\sigma$ to be bounded and non-constant. Let $f:\\left(\\mathbb{R}^{d}, \\mathcal{B}\\left(\\mathbb{R}^{d}\\right)\\right) \\rightarrow\\left(\\mathbb{R}^{m}, \\mathcal{B}\\left(\\mathbb{R}^{m}\\right)\\right)$ be a measurable function and $\\mu$ be a probability measure on $\\left(\\mathbb{R}^{d}, \\mathcal{B}\\left(\\mathbb{R}^{d}\\right)\\right)$. Then for any $\\varepsilon, \\bar{\\varepsilon}>0$ there exists a neural network $g$ such that\n\n$$\n\\mu\\left(\\left\\{x \\in \\mathbb{R}^{d}:|f(x)-g(x)|_{p} \\geq \\bar{\\varepsilon}\\right\\}\\right) \\leq \\varepsilon\n$$\n\nProof. See $[10]$.\nThis shows that theoretically, feedforward neural networks can approximate any measurable function in probability. However, as we will see later, they do not perform very well in our experiment on the approximation of functions $H^{c}$ based on graph data as in Proposition 4.7.", "tables": {}, "images": {}}, {"section_id": 11, "text": "# 5.2 Graph neural networks \n\nGraph neural networks (GNNs) are a family of neural networks that are specifically designed to operate on graph-structured data. We are interested in particular in spatial graph neural networks, where information is gathered locally by message passing, i.e. the process of passing information between the nodes of a graph. Their origins lie in [20], [58], while more current literature includes [37] and the references therein, as well as [40], [46], [53], [61] and [64]. The basic idea in these GNNs is that each node in the graph is associated with a representation vector (often also called feature- or hidden-state vector), and at each iteration of message passing, the representation vector of a node is updated by aggregating the representations of its neighboring nodes and then combining them with the old representation of the node to derive an updated node representation vector.\n\nDefinition 5.3. A function\n\n$$\nf: \\bigcup_{k \\in \\mathbb{N}}\\left(\\mathbb{R}^{m_{k}} \\times \\mathbb{R}^{m_{e}}\\right)^{k} \\rightarrow \\mathbb{R}^{m_{k}^{\\prime}}\n$$\n\nthat maps any number $k \\in \\mathbb{N}$ of node and edge features $\\left(h_{1}, e_{1}\\right), \\ldots,\\left(h_{k}, e_{k}\\right) \\in \\mathbb{R}^{m_{k}} \\times \\mathbb{R}^{m_{e}}$ of dimension $m_{h} \\in \\mathbb{N}$ and $m_{e} \\in \\mathbb{N}$, respectively, to some aggregated representation vector in $\\mathbb{R}^{m_{k}^{\\prime}}$ of dimension $m_{h}^{\\prime} \\in \\mathbb{N}$ and is permutation invariant in the sense of Definition 2.4, is called an aggregation function.\n\nIn GNNs such aggregation functions are used to aggregate the features of neighboring nodes and their edges between them to an aggregated feature (or representation vector) of the neighborhood.\nRecalling that $\\mathcal{D}$ denotes the domain of graphs $\\mathcal{D}=\\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{N \\times N \\times d^{\\prime}}$ from Definition 2.1, a GNN is defined as follows.\n\nDefinition 5.4. Let $L \\in \\mathbb{N}$ be the number of iterations, $m_{1}, \\ldots, m_{L} \\in \\mathbb{N}$ the dimensions of the corresponding node representations and $m_{1}^{\\prime}, \\ldots, m_{L}^{\\prime} \\in \\mathbb{N}$ the dimensions of the neighborhood representations in the respective iteration. Recall further that $d, d^{\\prime} \\in \\mathbb{N}$ are the dimensions of the initial node and edge representations and $N \\in \\mathbb{N}$ the graph size of graphs in $\\mathcal{D}$. Choose $m_{0}=d$ and let\n\n$$\nf_{a g}^{(l)}: \\bigcup_{k \\in \\mathcal{N}}\\left(\\mathbb{R}^{m_{l-1}} \\times \\mathbb{R}^{d^{\\prime}}\\right)^{k} \\rightarrow \\mathbb{R}^{m_{l}^{\\prime}}\n$$\n\nbe aggregation functions that map any set of node and edge representations to some aggregated neighborhood representation in every iteration $l=1, \\ldots, L$. Similarly, let\n\n$$\nf_{\\text {com }}^{(l)}: \\mathbb{R}^{m_{l-1}} \\times \\mathbb{R}^{m_{l}^{\\prime}} \\rightarrow \\mathbb{R}^{m_{l}}\n$$\n\nbe functions that map a node representation and a neighborhood representation to an updated node representation.\nFor every node $i=1, \\ldots, N$, let $h_{i}^{(l)}$ its node representation in iteration $l \\in[L]$. The node features $a_{i}$ serve as the initial node representations\n\n$$\nh_{i}^{(0)}=a_{i}\n$$\n\nand the edge representations, which are not updated in this type of GNN, are given by the edge features for any nodes $i, j=1, \\ldots, N$\n\n$$\ne_{i j}=\\ell_{i j}\n$$\n\nThen, at every iteration step $l=1, \\ldots, L-1$, the new node representation $h_{i}^{(l)}$ is calculated as\n\n$$\nh_{i}^{(l)}=f_{\\text {act }}\\left(f_{\\text {com }}^{(l)}\\left(h_{i}^{(l-1)}, f_{\\text {ag }}^{(l)}\\left(\\left\\{\\left(h_{j}^{(l-1)}, e_{j i}\\right) \\mid j \\in N(i)\\right\\}\\right)\\right)\\right)\n$$\n\nwhere $f_{\\text {act }}: \\mathbb{R} \\rightarrow \\mathbb{R}$ is some non linear activation function applied component wise. In the last step the activation is not applied, such that\n\n$$\nh_{i}^{(L)}=f_{\\text {com }}^{(L)}\\left(h_{i}^{(L-1)}, f_{\\text {ag }}^{(L)}\\left(\\left\\{\\left(h_{j}^{(L-1)}, e_{j i}\\right) \\mid j \\in N(i)\\right\\}\\right)\\right)\n$$\n\nA function $f: \\mathcal{D} \\rightarrow \\mathbb{R}^{N \\times m_{L}}$ defined as\n\n$$\nf(a, \\ell)=\\left(\\begin{array}{c}\nh_{1}^{(L)} \\\\\n\\vdots \\\\\nh_{N}^{(L)}\n\\end{array}\\right)\n$$\n\nis called a graph neural network with $L$ rounds of message passing.\nRemark 7. In comparison to feedforward neural networks (Definition 5.1) it is not obvious from the above definition why such a function is called a graph neural network. The reason for this name is that the functions $f_{a g}^{(l)}$ and $f_{\\text {com }}^{(l)}$ will usually be chosen as feedforward neural networks with trainable parameters. One example is the architecture we choose for our numerical experiments which is presented in Definition 5.5.\nThe specific GNN architecture that we consider is called SAGEConv (with mean aggregation) and was introduced in [40].\nDefinition 5.5. With the notation from Definition 5.4, we call a GNN layer SAGEConv layer if the aggregation function is given by\n\n$$\nf_{a g}\\left(\\left\\{\\left(h_{j}, e_{j i}\\right) \\mid j \\in N(i)\\right\\}\\right):=\\frac{1}{|N(i)|} \\sum_{j \\in N(i)} e_{j i} h_{j}\n$$\n\nand denoting\n\n$$\nh_{N(i)}=f_{a g}\\left(\\left\\{\\left(h_{j}, e_{j i}\\right) \\mid j \\in N(i)\\right\\}\\right)\n$$\n\nthe combination function is\n\n$$\nf_{\\text {comb }}\\left(h_{i}, h_{N(i)}\\right):=W \\cdot \\operatorname{concat}\\left(h_{i}, h_{N(i)}\\right)\n$$\n\nwhere $W$ is the representing matrix of some linear function of appropriate dimension. As activation function we utilise the sigmoid activation\n\n$$\nf_{\\text {act }}(x):=\\frac{1}{1+e^{-x}}\n$$\n\nThere exists a plethora of other possible choices regarding GNN layers. One important aspect motivating us to use GNNs as in Definition 5.5 is that the message passing also accounts for edge weights, besides the node features which serve as initial node representation $h^{(0)}$. Other popular GNN layers that could be relevant include the GIN layer from [64] or NNConv layer from [37]. However, comparing multiple GNN architectures would exceed the scope of this work.\nThe most important reason why GNNs promise to be a suitable design to approximate the target function $H^{c}$ from (4.7) is the following. Clearly, if $y \\in \\mathbb{R}^{N}$ is an optimal bailout capital allocation for a financial network $(a, \\ell) \\in \\mathcal{D}$, then $\\sigma(y)$ is optimal for $\\sigma(a, \\ell)$, for any permutation $\\sigma \\in S(N)$. Hence $H^{c}$ is permutation equivariant, see Definition 2.3. Since GNNs calculate the message that reaches a node $i$ based on its neighborhood $N(i)$, the ordering of the nodes does not matter as long as the network topology is not altered. This means that the output that is calculated for each node during multiple steps of message passing is by design permutation equivariant: For $\\sigma \\in S_{N}$ the GNN $f: \\mathcal{D} \\rightarrow \\mathbb{R}^{N \\times l}$ automatically computes results that respect\n\n$$\nf(\\sigma(g))=\\sigma(f(g))\n$$\n\nThe situation is very different for feedforward neural networks. Let for example $f: \\mathbb{R}^{N \\times m} \\rightarrow \\mathbb{R}^{m^{\\prime}}$ be a neural network ${ }^{4}$ and $\\sigma \\in S_{N}$ any permutation. Then in general for some $a \\in \\mathbb{R}^{N \\times m}$ it holds\n\n$$\nf(\\sigma(a)) \\neq \\sigma(f(a))\n$$\n\nSince a feedforward neural network is not permutation equivariant by design it may need to learn this property during training when approximating $H^{c}$.", "tables": {}, "images": {}}, {"section_id": 12, "text": "# 5.3 Permutation equivariant neural networks \n\nMotivated by the fact that permutation equivariance seems to be a crucial advantage of GNNs over FNNs in the approximation of our target function $H^{c}$, we present a neural network architecture that lies at the border between FNNs and GNNs. We will call this neural network architecture permutation equivariant neural network (PENN). Such PENNs do not utilise iterated message passing, however, they process the information given by a graph in a permutation equivariant manner and \"provide a summary of the entire graph,\" as stated in [41] where this architecture was first studied. Further, we are able to recover universality, which is not available for GNNs.\nBefore we introduce PENNs, we investigate permutation equivariant functions in more detail. As outlined in Remark 22, for a graph there exists multiple possible representations $g_{1}, g_{2}, \\ldots \\in \\mathcal{D}$ that are equally valid. Each of these representations implies the choice of some node-order in the sense that we chose $a_{1}$ to be the feature of the \"first node\" and $a_{N}$ to be the feature of the \" $N$-th node\". We can incorporate this (or any other) choice of numbering in the node feature by considering extended node features $\\widetilde{a}$, where\n\n$$\n\\widetilde{a}=\\left(\\begin{array}{c}\na_{1}, 1 \\\\\n\\vdots \\\\\na_{N}, N\n\\end{array}\\right)\n$$\n\nAdding a node ID as a feature seems not very restrictive since it only highlights an arbitrary choice that was already made earlier by ordering the nodes. This motivates the following definition of the domain of ID-augmented graphs.\n\nDefinition 5.6. Let $\\bar{\\sigma}=(\\sigma(1), \\ldots, \\sigma(N))^{T}$ be the vector that we get by evaluating some permutation $\\sigma \\in S(N)$ on the numbers $1, \\ldots, N$. The ID-augmented graph domain $\\widetilde{\\mathcal{D}}$ contains any element of $\\mathcal{D}$ whose node features a were augmented with the given ordering $\\bar{\\sigma}$ :\n\n$$\n\\widetilde{\\mathcal{D}}:=\\{(\\widetilde{a}, \\ell) \\mid \\widetilde{a}=(a, \\bar{\\sigma}), \\sigma \\in S(N),(a, \\ell) \\in \\mathcal{D}\\}\n$$\n\n[^0]\n[^0]:    ${ }^{4}$ We identify $\\mathbb{R}^{N \\times m}$ with $\\mathbb{R}^{N \\cdot m}$ to be in the setting of Definition 5.1.\n\nRemark 8. Since the augmented node-ID could simply be interpreted as one additional node-feature, the ID-augmented graphs $\\widetilde{\\mathcal{D}}$ with node feature size (before augmentation) $d \\in \\mathbb{N}$ are a subset of graphs with node-feature size $d+1$. Therefore, the definition of permutation equivariant functions (Definition 2.3) extends naturally to $\\widetilde{\\mathcal{D}}$.\nThe following result is a modified version of Theorem 1 in [41]. We rigorously prove that any permutation invariant function on the domain of directed, weighted and featured graphs whose node features are augmented with node IDs exhibits a certain structure. Derived from this structure, later we will choose an architecture for permutation equivariant neural networks consisting of feedforward neural networks.\nProposition 5.7. A function $\\widetilde{\\tau}: \\widetilde{\\mathcal{D}} \\rightarrow \\mathbb{R}^{N}$ is permutation equivariant as in Definition 2.3 if and only if there exist dimensions $V \\leq N^{2} d^{\\prime}, W \\leq N(d+1)+N^{2} d^{\\prime}$ and functions $\\varphi: \\mathbb{R}^{2(d+1)+d^{\\prime}} \\rightarrow \\mathbb{R}^{V}, \\alpha: \\mathbb{R}^{d+1+V} \\rightarrow \\mathbb{R}^{W}$ and $\\rho: \\mathbb{R}^{d+1+W} \\rightarrow \\mathbb{R}$ such that for all $(\\widetilde{a}, \\ell) \\in \\widetilde{\\mathcal{D}}$ and for all $k=1, \\ldots, N$ :\n\n$$\n\\widetilde{\\tau}(\\widetilde{a}, \\ell)_{k}=\\rho\\left(\\tilde{a}_{k}, \\sum_{i=1}^{N} \\alpha\\left(\\tilde{a}_{i}, \\sum_{j \\neq i} \\varphi\\left(\\tilde{a}_{i}, \\ell_{i, j}, \\tilde{a}_{j}\\right)\\right)\\right)\n$$\n\nProof. First, we show that any $\\widetilde{\\tau}$ satisfying the R.H.S of (59) is permutation equivariant. Let $\\varphi, \\alpha$ and $\\rho$ be given and $\\gamma \\in S(N)$ be any permutation of $[N]$. Then we need to show that\n\n$$\n\\widetilde{\\tau}(\\gamma(\\widetilde{a}, \\ell))_{k}=\\gamma(\\widetilde{\\tau}(\\widetilde{a}, \\ell))_{k}\n$$\n\nIt holds by definition that\n\n$$\n\\begin{aligned}\n\\widetilde{\\tau}(\\gamma(\\widetilde{a}, \\ell))_{k} & =\\widetilde{\\tau}(\\gamma(\\widetilde{a}), \\gamma(\\ell))_{k} \\\\\n& =\\rho\\left(\\gamma(\\widetilde{a})_{k}, \\sum_{i=1}^{N} \\alpha\\left(\\gamma(\\widetilde{a})_{i}, \\sum_{j \\neq i} \\varphi\\left(\\gamma(\\widetilde{a})_{i}, \\gamma(\\ell)_{i j}, \\gamma(\\widetilde{a})_{j}\\right)\\right)\\right)\n\\end{aligned}\n$$\n\nFurther, since $\\gamma(\\widetilde{a})_{k}=\\widetilde{a}_{\\gamma^{-1}(k)}$ and $\\gamma(\\ell)_{i j}=\\ell_{\\gamma^{-1}(i), \\gamma^{-1}(j)}$ it holds\n\n$$\n\\begin{aligned}\n& \\rho\\left(\\gamma(\\widetilde{a})_{k}, \\sum_{i=1}^{N} \\alpha\\left(\\gamma(\\widetilde{a})_{i}, \\sum_{j \\neq i} \\varphi\\left(\\gamma(\\widetilde{a})_{i}, \\gamma(\\ell)_{i j}, \\gamma(\\widetilde{a})_{j}\\right)\\right)\\right) \\\\\n& =\\rho\\left(\\widetilde{a}_{\\gamma^{-1}(k)}, \\sum_{i=1}^{N} \\alpha\\left(\\widetilde{a}_{\\gamma^{-1}(i)}, \\sum_{j \\neq i} \\varphi\\left(\\widetilde{a}_{\\gamma^{-1}(i)}, \\ell_{\\gamma^{-1}(i), \\gamma^{-1}(j)}, \\widetilde{a}_{\\gamma^{-1}(j)}\\right)\\right)\\right) \\\\\n& =\\rho\\left(\\widetilde{a}_{\\gamma^{-1}(k)}, \\sum_{i=1}^{N} \\alpha\\left(\\widetilde{a}_{i}, \\sum_{j \\neq i} \\varphi\\left(\\widetilde{a}_{i}, \\ell_{i j}, \\widetilde{a}_{j}\\right)\\right)\\right) \\\\\n& =\\widetilde{\\tau}(\\widetilde{a}, \\ell)_{\\gamma^{-1}(k)}\n\\end{aligned}\n$$\n\nwhere we recall that $\\{1, \\ldots, N\\}=\\{\\gamma(1), \\ldots, \\gamma(N)\\}$. This shows that a function defined by the right hand side in (59) is indeed permutation equivariant on $\\widetilde{\\mathcal{D}}$ for any $\\varphi, \\alpha$ and $\\rho$. Next, we prove that any given permutation equivariant function function $\\widetilde{\\tau}$ can be expressed by carefully chosen $\\varphi, \\alpha$ and $\\rho$ in (59). The key idea is to construct $\\varphi, \\alpha$ such that the second argument of $\\rho$ contains all the information about the graph $(\\widetilde{a}, \\ell)$. Then, the function $\\rho$ corresponds to an application of $\\widetilde{\\tau}$ to this representation. To simplify notation, we assume node and edge features to be scalar $\\left(d=d^{\\prime}=1\\right)$. The extension to vectors is simple, but involves more indexing.\nLet $(\\widetilde{a}, \\ell)=((a, \\bar{\\sigma}), \\ell) \\in \\widetilde{\\mathcal{D}}$. We define $\\varphi$ to return a $N \\times N$ zero matrix with only one entry that is allowed to be non-zero, which is $\\ell_{i j}$ at index $\\left(\\bar{\\sigma}_{i}, \\bar{\\sigma}_{j}\\right)=(\\sigma(i), \\sigma(j))$,\n\n$$\n\\varphi\\left(\\tilde{a}_{i}, \\ell_{i j}, \\tilde{a}_{j}\\right)=\\varphi\\left(\\left(a_{i}, \\sigma(i)\\right), \\ell_{i j},\\left(a_{j}, \\sigma(j)\\right)\\right)=\\mathbb{1}_{\\sigma(i), \\sigma(j)}^{N \\times N} \\cdot \\ell_{i j}\n$$\n\nThis means that the function $\\varphi$ stores the information about the edge from node $i$ to $j$ in position $(\\sigma(i), \\sigma(j))$. After summing over all $j \\neq i$, the resulting matrix\n\n$$\nM_{\\varphi}^{i}=\\sum_{j \\neq i} \\varphi\\left(\\tilde{a}_{i}, \\ell_{i j}, \\tilde{a}_{j}\\right)\n$$\n\ncontains all information about the outgoing edges of any fixed node $i$.\nNext we define the function $\\alpha$ to augment this matrix with an $N \\times 2$ matrix that contains $a_{i}, \\sigma(i)$ in row $\\sigma(i)$ and contains zeros at all other indices.\n\n$$\n\\alpha\\left(\\tilde{a}_{i}, M_{\\varphi}^{i}\\right)=\\alpha\\left(\\left(a_{i}, \\sigma(i)\\right), M_{\\varphi}^{i}\\right)=\\left(\\mathbb{1}_{\\sigma(i), .}^{N \\times 2} \\cdot\\left(a_{i}, \\sigma(i)\\right), M_{\\varphi}\\right)\n$$\n\nThe resulting couple generated by $\\alpha$ contains the node feature of node $i$, its node ID $\\sigma(i)$ and all information about its outgoing edges. Summing all terms generated by $\\alpha$ yields\n\n$$\n\\sum_{i=1}^{N} \\alpha\\left(\\tilde{a}_{i}, M_{\\varphi}^{i}\\right)=(\\sigma(\\widetilde{a}), \\sigma(\\ell))=\\sigma(\\widetilde{a}, \\ell)\n$$\n\nIn a last step we apply $\\widetilde{\\tau}$ to $\\sigma(\\widetilde{a}, \\ell)$. For node $k$ we want to return the k-th component of $\\widetilde{\\tau}(\\widetilde{a}, \\ell)$ which corresponds to the $\\sigma(k)$-th component of $\\sigma(\\widetilde{\\tau}(\\widetilde{a}, \\ell))$ by permutation invariance. Hence we choose $\\rho$ such that\n\n$$\n\\rho\\left(\\tilde{a}_{k}, \\sigma(\\widetilde{a}, \\ell))\\right)=\\rho\\left(\\left(a_{k}, \\sigma(k)\\right), \\sigma(\\widetilde{a}, \\ell)\\right)=\\widetilde{\\tau}(\\sigma(\\widetilde{a}, \\ell))_{\\sigma(k)}=\\widetilde{\\tau}(\\widetilde{a}, \\ell)_{k}\n$$\n\nThis concludes the proof.\nThese theoretical results motivate us to introduce a certain architecture of neural networks that was already described in [41] and is defined as follows.\nDefinition 5.8. Let $\\hat{\\rho}, \\hat{\\alpha}, \\hat{\\varphi}$ be feedforward neural networks with dimensionality $\\hat{\\varphi}: \\mathbb{R}^{2 d+d^{\\prime}} \\rightarrow \\mathbb{R}^{d_{1}}, \\hat{\\alpha}$ : $\\mathbb{R}^{d+d_{1}} \\rightarrow \\mathbb{R}^{d_{2}}, \\hat{\\rho}: \\mathbb{R}^{d+d_{2}} \\rightarrow \\mathbb{R}$, for dimensions $d_{1}, d_{2} \\in \\mathbb{N}$. The function $f: \\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{N \\times \\mathbb{N} \\times d^{\\prime}} \\rightarrow \\mathbb{R}^{N}$ defined component-wise for any $k \\in[N]$ as\n\n$$\nf(a, \\ell)_{k}=\\hat{\\rho}\\left(a_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(a_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(a_{i}, \\ell_{i j}, a_{j}\\right)\\right)\\right)\n$$\n\nis called permutation equivariant neural network (PENN).\nThe next proposition provides a universality result for PENNs introduced in Definition 5.8. We show that by approximating $\\rho, \\alpha, \\varphi$ with neural networks $\\hat{\\rho}, \\hat{\\alpha}, \\hat{\\varphi}$ we can approximate the composition of $\\rho, \\alpha, \\varphi$ arbitrarily close in probability by the composition of $\\hat{\\rho}, \\hat{\\alpha}, \\hat{\\varphi}$, given a probability measure on $\\widetilde{\\mathcal{D}}$. The result we show is even more general, it holds for functions and probability measures on $\\mathbb{R}^{N \\times(d+1)} \\times \\mathbb{R}^{N \\times N \\times d^{\\prime}} \\supset \\widetilde{\\mathcal{D}}$.\nProposition 5.9. Let $\\rho, \\alpha, \\varphi$ be Borel measurable functions $\\varphi: \\mathbb{R}^{2 d_{1}+d_{2}} \\rightarrow \\mathbb{R}^{d_{3}}, \\alpha: \\mathbb{R}^{d_{1}+d_{3}} \\rightarrow \\mathbb{R}^{d_{4}}$, and $\\rho: \\mathbb{R}^{d_{1}+d_{4}} \\rightarrow \\mathbb{R}$, for arbitrary dimensions $d_{1}, \\ldots, d_{4} \\in \\mathbb{N}$. Let further for some $g: \\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}} \\rightarrow \\mathbb{R}^{N}$ and all $k \\in[N]$,\n\n$$\ng(x, y)_{k}=\\rho\\left(x_{k}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i}^{N} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)\n$$\n\nThen for any probability measure $\\mu$ on $\\left(\\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}}, \\mathcal{B}\\left(\\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}}\\right)\\right)$, any $p \\in[1, \\infty]$, and any $\\varepsilon, \\bar{\\varepsilon}>0$ there exist feedforward neural networks $\\hat{\\rho}, \\hat{\\alpha}, \\hat{\\varphi}$ such that for the function $\\hat{g}: \\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}} \\rightarrow \\mathbb{R}^{N}$, which is for $k \\in[N]$ defined by\n\n$$\n\\hat{g}(x, y)_{k}=\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i}^{N} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)\n$$\n\nit holds\n\n$$\n\\mu\\left(\\left\\{(x, y) \\in \\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}}:|g(x, y)-\\hat{g}(x, y)|_{p}>\\varepsilon\\right\\}\\right)<\\varepsilon\n$$\n\nProof. Let $\\varepsilon, \\bar{\\varepsilon}>0$. Choose $\\hat{\\rho}$ such that for all $k \\in[N]$\n\n$$\n\\mu\\left(\\left\\{\\left|\\rho\\left(x_{k}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)-\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{3 N}\\right\\}\\right) \\leq \\frac{\\varepsilon}{3 N}\n$$\n\nNote that $\\hat{\\rho}$ is Lipschitz continuous with some constant $K_{\\hat{\\rho}}>0$. Then choose $\\hat{\\alpha}$ such that for all $i \\in[N]$\n\n$$\n\\mu\\left(\\left\\{\\left|\\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)-\\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{3 N^{2} K_{\\hat{\\rho}}}\\right\\}\\right) \\leq \\frac{\\varepsilon}{3 N}\n$$\n\nFinally, also $\\hat{\\alpha}$ is Lipschitz continuous with $K_{\\hat{\\alpha}}>0$ and we can choose $\\hat{\\varphi}$ such that for all $i, j \\in[N]$\n\n$$\n\\mu\\left(\\left\\{\\left|\\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)-\\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{3 N^{2}(N-1) K_{\\hat{\\rho}} K_{\\hat{\\alpha}}}\\right\\}\\right) \\leq \\frac{\\varepsilon}{3 N(N-1)}\n$$\n\nAll these choices are valid due to Proposition 5.2.\nNext we establish some inclusion that will help us to bound the probability.\n\n$$\n\\begin{aligned}\n& \\left.\\left\\{|g(x)-\\hat{g}(x)|_{p} \\geq \\varepsilon\\right\\}=\\left\\{\\left|\\begin{array}{c}\n\\rho\\left(x_{1}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right) \\\\\n-\\hat{\\rho}\\left(x_{1}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right) \\\\\n\\vdots \\\\\n\\rho\\left(x_{n}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right) \\\\\n-\\hat{\\rho}\\left(x_{n}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right. \\\\\np\n\\end{array}\\right. \\geq \\varepsilon\\right\\} \\\\\n& \\subseteq \\bigcup_{k \\in[N]}\\left\\{\\left|\\rho\\left(x_{k}, \\sum_{i=1}^{n} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)-\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{n} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{n}\\right\\} \\\\\n& \\subseteq \\bigcup_{k \\in[N]}\\left(\\left\\{\\left|\\rho\\left(x_{k}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)-\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{3 N}\\right\\} \\\\\n& \\cup\\left\\{\\left|\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)-\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{3 N}\\right\\} \\\\\n& \\cup\\left\\{\\left|\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)-\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{3 N}\\right\\}\n\\end{aligned}\n$$\n\nThe first set in this expression is controllable. For the second term we can write\n\n$$\n\\begin{aligned}\n& \\left\\{\\left|\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)-\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{3 N}\\right\\} \\\\\n& \\subseteq\\left\\{\\left|\\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)-\\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{3 N K_{\\hat{\\rho}}}\\right\\} \\\\\n& \\subseteq \\bigcup_{i \\in[N]}\\left\\{\\left|\\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)-\\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right|_{p} \\geq \\frac{\\bar{\\varepsilon}}{3 N^{2} K_{\\hat{\\rho}}}\\right\\}\n\\end{aligned}\n$$\n\nNote that this last expression does not depend on $k$ anymore, so there is no need to include it in the union over all $k \\in[N]$. Similarly, for the third term we find\n\n$$\n\\begin{aligned}\n& \\left\\{\\left|\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)-\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right)\\right|_{p} \\geq \\frac{\\varepsilon}{3 N}\\right\\} \\\\\n& \\subseteq\\left\\{\\left|\\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)-\\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right|_{p} \\geq \\frac{\\varepsilon}{3 N K_{\\hat{\\rho}}}\\right\\} \\\\\n& \\subseteq\\left(\\bigcup_{i \\in[N]}\\left\\{\\left|\\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)-\\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right|_{p} \\geq \\frac{\\varepsilon}{3 N^{2} K_{\\hat{\\rho}}} \\right\\}\\right) \\\\\n& \\subseteq\\left(\\bigcup_{i \\in[N]}\\left\\{\\left|\\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)-\\sum_{j \\neq i} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right|_{p} \\geq \\frac{\\varepsilon}{3 N^{2} K_{\\hat{\\rho}} K_{\\hat{\\alpha}}}\\right\\}\\right) \\\\\n& \\subseteq\\left(\\bigcup_{i \\in[N]} \\bigcup_{j \\neq i}\\left\\{\\left|\\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)-\\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right)\\right|_{p} \\geq \\frac{\\varepsilon}{3 N^{2}(N-1) K_{\\hat{\\rho}} K_{\\hat{\\alpha}}}\\right\\}\\right)\n\\end{aligned}\n$$\n\nFrom this we can conclude that\n\n$$\n\\mu\\left(\\left.\\left\\{|g(x)-\\hat{g}(x)|_{p} \\geq \\varepsilon\\right\\}\\right) \\leq N \\frac{\\varepsilon}{3 N}+N \\frac{\\varepsilon}{3 N}+N(N-1) \\frac{\\varepsilon}{3 N(N-1)}=\\varepsilon\\right.\n$$\n\nDue to Proposition 5.7 and Proposition 5.9, we know that PENNs can approximate any permutationequivariant node labeling function on $\\widetilde{\\mathcal{D}}$ arbitrarily close in probability.\nFurthermore, if we can approximate such functions on $\\widetilde{\\mathcal{D}}$ then we can also approximate permutationequivariant functions on $\\mathcal{D}$, as we will show next.\nClearly any function $\\tau: \\mathcal{D} \\rightarrow \\mathbb{R}^{N}$ can be extended to a function $\\widetilde{\\tau}: \\widetilde{\\mathcal{D}} \\rightarrow \\mathbb{R}^{N}$ by\n\n$$\n\\widetilde{\\tau}(\\widetilde{a}, \\ell)=\\widetilde{\\tau}((a, \\bar{\\sigma}), \\ell):=\\tau(a, \\ell)\n$$\n\nMoreover the following Lemma holds.\nLemma 5.10. If $\\tau$ is permutation equivariant then $\\widetilde{\\tau}$ is also permutation equivariant.\nProof. For any two permutations $\\gamma, \\sigma \\in S_{N}$ and any $(\\widetilde{a}, \\ell)=((a, \\bar{\\sigma}), \\ell) \\in \\widetilde{\\mathcal{D}}$ it holds\n\n$$\n\\begin{aligned}\n\\widetilde{\\tau}(\\gamma(\\widetilde{a}, \\ell)) & =\\widetilde{\\tau}(\\gamma(\\widetilde{a}), \\gamma(\\ell))=\\widetilde{\\tau}((\\gamma(a), \\gamma(\\bar{\\sigma})), \\gamma(\\ell))=\\tau(\\gamma(a), \\gamma(\\ell)) \\\\\n& =\\gamma(\\tau(a, \\ell))=\\gamma(\\widetilde{\\tau}((a, \\bar{\\sigma}), \\ell))=\\gamma(\\widetilde{\\tau}(\\widetilde{a}, \\ell))\n\\end{aligned}\n$$\n\nThen, from Proposition 5.7 it immediately follows that we can also find a similar representation for any permutation equivariant function $\\tau$ on $\\mathcal{D}$ :\n\nCorollary 5.11. Let $\\tau: \\mathcal{D} \\rightarrow \\mathbb{R}^{N}$ be a permutation equivariant node labeling function and $\\widetilde{\\tau}: \\widetilde{\\mathcal{D}} \\rightarrow \\mathbb{R}^{N}$ its extension to $\\widetilde{\\mathcal{D}}$. Further, for any permutation $\\sigma \\in S(N)$ let $f_{\\sigma}: \\mathbb{R}^{N \\times d} \\rightarrow \\mathbb{R}^{N \\times(d+1)}$ be the function that equips node features $a$ with the order $\\bar{\\sigma}$,\n\n$$\nf_{\\sigma}(a)=\\left(\\begin{array}{c}\na_{1}, \\bar{\\sigma}_{1} \\\\\n\\vdots \\\\\na_{N}, \\bar{\\sigma}_{N}\n\\end{array}\\right)\n$$\n\nThen there exist dimensions $V, W \\in \\mathbb{N}$ and functions $\\varphi: \\mathbb{R}^{2(d+1)+d^{\\prime}} \\rightarrow \\mathbb{R}^{V}, \\alpha: \\mathbb{R}^{d+1+V} \\rightarrow \\mathbb{R}^{W}$ and $\\rho$ : $\\mathbb{R}^{d+1+W} \\rightarrow \\mathbb{R}$, such that for any $\\sigma \\in S_{N}$,\n\n$$\n\\tau(a, \\ell)_{k}=\\rho\\left(f_{\\sigma}(a)_{k}, \\sum_{i=1}^{N} \\alpha\\left(f_{\\sigma}(a)_{i}, \\sum_{j \\neq i} \\varphi\\left(f_{\\sigma}(a)_{i}, \\ell_{i, j}, f_{\\sigma}(a)_{j}\\right)\\right)\\right)\n$$\n\nIn particular we can choose the functions $\\rho, \\alpha, \\varphi$ from Proposition 5.7 whose composition represents $\\widetilde{\\tau}$.\nProof. Choose $\\rho, \\alpha, \\varphi$ from Proposition 5.7 such that they represent the extension $\\widetilde{\\tau}$ of $\\tau$ to $\\mathcal{D}$.\nThe above results imply the following procedure to approximate a node-labeling function $\\tau: \\mathcal{D} \\rightarrow \\mathbb{R}^{N}$ on graph-data $\\left\\{g_{1}, g_{2}, \\ldots\\right\\} \\subseteq \\mathcal{D}$.\n\n1. Augment the data with random node-IDs.\n2. Learn $\\widetilde{\\tau}$ with the augmented data by approximating $\\rho, \\alpha$ and $\\varphi$ from Proposition 5.7.\n3. Use the approximation of $\\widetilde{\\tau}$ to obtain an approximation of $\\tau$ according to Corollary 5.11.\n\nRemark 9. In the proof of Proposition 5.7 we need unique node IDs to identify and differentiate between the different nodes based solely on their features. Without unique node-IDs there is no guarantee that a representation as described in Proposition 5.7 can be found. In some situations it can even be proved that applying PENNs to data without node-IDs can not work. As an example we refer to the numerical experiment in Section 6.1. However, in other situations, see for example the numerical experiments in sections 6.2 and 6.3, PENNs work equally well without node IDs. This can have two reasons. Firstly, if the node features already allow to differentiate between the nodes well enough, augmenting the unique node-IDs can be redundant. Secondly, it is possible that a simpler representation of the node-labeling function exists, compared to the one guaranteed by Proposition 5.7. If this simpler representation exists and does not require node-IDs, then PENNs can work well without augmenting the nodes with IDs.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# 5.4 Extended permutation equivariant neural networks \n\nAs we will see in the numerical experiments in the following section, it can happen that important information about one node $k=1, \\ldots, N$ aggregated via\n\n$$\n\\hat{\\alpha}\\left(a_{k}, \\sum_{j \\neq k} \\hat{\\varphi}\\left(a_{k}, \\ell_{k j}, a_{j}\\right)\\right)\n$$\n\ngets lost in the summation\n\n$$\n\\sum_{i=1}^{N} \\hat{\\alpha}\\left(a_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(a_{i}, \\ell_{i j}, a_{j}\\right)\\right)\n$$\n\nWithout node numbering this information might be impossible to recover and even with node numbering can be hard to recover. For this reason we propose an extended architecture of PENNs called extended PENN (XPENN) that also contains a third component, which aggregates a node specific signal for each node based on its connections from and to all other nodes. Hence XPENNs maintain all the favourable properties of PENNs and are additionally able to mimic one iteration of message passing. This architecture is even more expressive, since the aggregation is not only across the neighbors, but all other nodes and it considers both incoming and outgoing edges. In theory, this additional component does not exhibit new information about the graph, but it solves the problem that certain information can get lost in the sum and is hard to recover. In practice it improves the performance in all numerical experiments that we consider, see Section 6.\n\nDefinition 5.12. With $\\hat{\\varphi}$ and $\\hat{\\alpha}$ as in Definition 5.8 as well as feedforward neural networks $\\hat{\\psi}: \\mathbb{R}^{2 d+2 d^{\\prime}} \\rightarrow$ $\\mathbb{R}^{d_{3}}$ and $\\hat{\\rho}: \\mathbb{R}^{d+d_{2}+d_{3}} \\rightarrow \\mathbb{R}$ for some $d_{3} \\in \\mathbb{N}$, we call the function $f: \\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{N \\times \\mathbb{N} \\times d^{\\prime}} \\rightarrow \\mathbb{R}^{N}$ that is component wise defined for $k \\in[N]$ as\n\n$$\nf(a, \\ell)_{k}=\\hat{\\rho}\\left(a_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(a_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(a_{i}, \\ell_{i j}, a_{j}\\right)\\right), \\sum_{j \\neq k} \\hat{\\psi}\\left(a_{k}, \\ell_{k j}, \\ell_{j k}, a_{j}\\right)\\right)\n$$\n\nan extended permutation equivariant neural network (XPENN).\nThe following lemma underlines that indeed XPENNs are an extension of PENNs and include all PENNs.\nLemma 5.13. For every PENN $f: \\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{N \\times \\mathbb{N} \\times d^{\\prime}} \\rightarrow \\mathbb{R}^{N}$ we find a corresponding XPENN $f^{*}: \\mathbb{R}^{N \\times d} \\times$ $\\mathbb{R}^{N \\times \\mathbb{N} \\times d^{\\prime}} \\rightarrow \\mathbb{R}^{N}$ with the same output, i.e. for all $(a, \\ell) \\in \\mathcal{D}$,\n\n$$\nf^{*}(a, \\ell)=f(a, \\ell)\n$$\n\nProof. Let $f: \\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{N \\times \\mathbb{N} \\times d^{\\prime}} \\rightarrow \\mathbb{R}^{N}$ be a PENN consisting of feedforward neural networks $\\hat{\\rho}, \\hat{\\alpha}, \\hat{\\varphi}$. Define $\\hat{\\psi} \\equiv 0$ and $\\hat{\\rho}^{*}: \\mathbb{R}^{d+d_{2}+d_{3}} \\rightarrow \\mathbb{R}$ for all $x_{1} \\in \\mathbb{R}^{d}, x_{2} \\in \\mathbb{R}^{d_{2}}, x_{3} \\in \\mathbb{R}^{d_{3}}$ as\n\n$$\n\\hat{\\rho}^{*}\\left(x_{1}, x_{2}, x_{3}\\right)=\\hat{\\rho}\\left(x_{1}, x_{2}\\right)\n$$\n\nThen the for the XPENN $f^{*}: \\mathbb{R}^{N \\times d} \\times \\mathbb{R}^{N \\times \\mathbb{N} \\times d^{\\prime}} \\rightarrow \\mathbb{R}^{N}$ given by\n\n$$\nf^{*}(a, \\ell)_{k}=\\hat{\\rho}^{*}\\left(a_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(a_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(a_{i}, \\ell_{i j}, a_{j}\\right)\\right), \\sum_{j \\neq k} \\hat{\\psi}\\left(a_{k}, \\ell_{k j}, \\ell_{j k}, a_{j}\\right)\\right)\n$$\n\nit holds for all $(a, \\ell) \\in \\mathcal{D}$\n\n$$\nf^{*}(a, \\ell)=f(a, \\ell)\n$$\n\nFrom Lemma 5.13, it follows immediately that XPENNs have the same favourable theoretical properties as PENNs. For example, we can find a corresponding representation of permutation equivariant node-labeling functions.\n\nCorollary 5.14. A function $\\widetilde{\\tau}: \\widetilde{\\mathcal{D}} \\rightarrow \\mathbb{R}^{N}$ is permutation equivariant as in Definition 2.3 if and only if there exist dimensions $V \\leq N^{2} d^{\\prime}, W \\leq N(d+1)+N^{2} d^{\\prime}, W^{\\prime} \\in \\mathbb{N}$ and functions $\\varphi: \\mathbb{R}^{2(d+1)+d^{\\prime}} \\rightarrow \\mathbb{R}^{V}, \\alpha$ : $\\mathbb{R}^{d+1+V} \\rightarrow \\mathbb{R}^{W}, \\psi: \\mathbb{R}^{2(d+1)+2 d^{\\prime}} \\rightarrow \\mathbb{R}^{W^{\\prime}}$ and $\\rho: \\mathbb{R}^{d+1+W+W^{\\prime}} \\rightarrow \\mathbb{R}$ such that for all $(\\widetilde{a}, \\ell) \\in \\widetilde{\\mathcal{D}}$ and for all $k=1, \\ldots, N$ :\n\n$$\n\\widetilde{\\tau}(\\widetilde{a}, \\ell)_{k}=\\rho\\left(\\tilde{a}_{k}, \\sum_{i=1}^{N} \\alpha\\left(\\tilde{a}_{i}, \\sum_{j \\neq i} \\varphi\\left(\\tilde{a}_{i}, \\ell_{i, j}, \\tilde{a}_{j}\\right)\\right), \\sum_{j \\neq k} \\hat{\\psi}\\left(\\tilde{a}_{k}, \\ell_{k j}, \\ell_{j k}, \\tilde{a}_{j}\\right)\\right)\n$$\n\nProof. Follows from Lemma 5.13 and Proposition 5.7.\nSimilarly, we can also recover universality for XPENNs.\nCorollary 5.15. Let $\\rho, \\alpha, \\varphi, \\psi$ be Borel measurable functions $\\varphi: \\mathbb{R}^{2 d_{1}+d_{2}} \\rightarrow \\mathbb{R}^{d_{3}}, \\alpha: \\mathbb{R}^{d_{1}+d_{3}} \\rightarrow \\mathbb{R}^{d_{4}}, \\psi$ : $\\mathbb{R}^{2 d_{1}+2 d_{2}} \\rightarrow \\mathbb{R}^{d_{3}}$ and $\\rho: \\mathbb{R}^{d_{1}+d_{4}+d_{5}} \\rightarrow \\mathbb{R}$, for arbitrary dimensions $d_{1}, \\ldots, d_{5} \\in \\mathbb{N}$. Let further for some $g: \\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}} \\rightarrow \\mathbb{R}^{N}$ and all $k \\in[N]$,\n\n$$\ng(x, y)_{k}=\\rho\\left(x_{k}, \\sum_{i=1}^{N} \\alpha\\left(x_{i}, \\sum_{j \\neq i} \\varphi\\left(x_{i}, y_{i j}, x_{j}\\right)\\right), \\sum_{j \\neq k} \\psi\\left(x_{k}, y_{k j}, y_{j k}, x_{j}\\right)\\right)\n$$\n\nThen for any probability measure $\\mu$ on $\\left(\\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}}, \\mathcal{B}\\left(\\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}}\\right)\\right)$ and any $\\varepsilon, \\bar{\\varepsilon}>0$ there exist feedforward neural networks $\\hat{\\rho}, \\hat{\\alpha}, \\hat{\\varphi}, \\hat{\\psi}$ such that for the function $\\hat{g}: \\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}} \\rightarrow \\mathbb{R}^{N}$ which is for $k \\in[N]$ defined by\n\n$$\n\\hat{g}(x, y)_{k}=\\hat{\\rho}\\left(x_{k}, \\sum_{i=1}^{N} \\hat{\\alpha}\\left(x_{i}, \\sum_{j \\neq i} \\hat{\\varphi}\\left(x_{i}, y_{i j}, x_{j}\\right)\\right), \\sum_{j \\neq k} \\hat{\\psi}\\left(x_{k}, y_{k j}, y_{j k}, x_{j}\\right)\\right)\n$$\n\nit holds\n\n$$\n\\mu\\left(\\left\\{(x, y) \\in \\mathbb{R}^{N \\times d_{1}} \\times \\mathbb{R}^{N \\times N \\times d_{2}}:|g(x, y)-\\hat{g}(x, y)|_{p}>\\bar{\\varepsilon}\\right\\}\\right)<\\varepsilon\n$$", "tables": {}, "images": {}}, {"section_id": 14, "text": "# 6 Numerical Experiments \n\nIn this section we conduct three numerical experiments. In a first experiment we consider carefully constructed financial networks where the optimal amount of bailout capital and the optimal allocation is known. We show that GNNs and XPENNs are able to solve the inner problem, i.e. find the optimal allocation of the bailout capital. Additionally we point out some limitations which we observe for FNNs and PENNs. In our second experiment we investigate how GNNs, (X)PENNs, FNNs, and benchmark approaches perform in computing the inner risk, i.e. minimising the risk of loss at some fixed level of bailout capital on more complex synthetic data, for which the optimal allocation is not known. In our third experiment we approximate the systemic risk by searching for the minimal amount of bailout capital needed to secure the system and compare the performance of GNNs, (X)PENNs, FNNs, and benchmark approaches.", "tables": {}, "images": {}}, {"section_id": 15, "text": "### 6.1 Default cascade networks and star shaped networks\n\nIn this first experiment, we design stylised networks to demonstrate how useful GNNs and XPENNs can be for problems such as learning effective bailout strategies.\n\nData We carefully construct four data sets containing realisations of stochastic networks $G_{10}, G_{20}, G_{50}$ and $G_{100} \\in \\mathcal{G}$ of increasing size. Each respective data set contains realisations with $N \\in\\{10,20,50,100\\}$ nodes. Specifically, for every $i \\in 1, \\ldots, N$ each data set contains two distinct realisations of the network $G_{N}$ : In both cases all nodes have external assets equal to 0 . However, one realisation is a default cascade. This means that every node $j \\neq i-1$ has a liability of $N-1$ units towards the next node $j+1$. Node $N$ will have a liability towards node 1 , since there is no node $N+1$. The only link that is skipped is the liability from node $i-1$ towards $i$. Therefore, the network is in fact a cascade of liabilities starting in $i$ and ending in $i-1$. The second realisation that we construct looks very different. Here each node $j \\neq i$ has a liability of 1 towards $i$, hence forming a star shaped network with $i$ in the center. Figure 1 illustrates both network types.\nFor each $N \\in\\{10,20,50,100\\}$ these $2 N$ realisations - two for every $i \\in 1, \\ldots, N$ - create a data set, that shares one critical feature. We know that it can be fully secured, i.e. there are no defaults, with $N-1$ units of bailout capital. In order to secure a cascade type network, we need to inject all capital into the starting point of the cascade. On the other hand, to rescue a star shaped network, we must distribute the bailout capital equally, i.e. 1 towards each node, among the nodes with a liability towards the center node.\n\nProblem The problem that we try to solve is to find the inner risk\n\n$$\n\\rho_{I}^{a}\\left(G_{N}\\right)=\\min _{Y \\in \\mathcal{C}^{a}}\\{\\eta(\\Lambda((A+Y, L)))\\}\n$$\n\nwith allocations in\n\n$$\n\\mathcal{C}^{a}=\\left\\{Y \\in L^{\\infty}\\left(\\mathbb{R}_{+}^{N}\\right) \\mid \\sum_{n=1}^{N} Y_{n}=a\\right\\}\n$$\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Both network types for $N=100, i=1$. On the left the default cascade starting in $i=1$. On the right the star shaped network with center $i=1$.\nfor $a=N-1$ and $\\eta$ a univariate risk measure as in Assumption 3.3. For the sake of simplicity we choose $\\eta=\\mathbb{E}[\\cdot]$ to be the expectation, but note that due to the monotonicity of the risk measure $\\eta$ any choice would ultimately lead to a path-wise minimisation of\n\n$$\n\\min _{Y \\in \\mathcal{C}^{a}}\\{\\Lambda((A+Y, L))\\}\n$$\n\nWe know that for each $N$ the theoretical minimal loss is equal to zero if we distribute the bailout $a=N-1$ optimally as explained above. To learn the optimal bailout strategy we consider the following neural network architectures and train them as described in Section 4.2.", "tables": {}, "images": {"img-0.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 16, "text": "# Models \n\n- GNN: The first model is a GNN consisting of two SAGEConv layers (5.5) of width 10 and sigmoid activation in between. The initial node features are equal to zero. This means all nodes start with identical representations and learning effective message passing is the only possibility for the model to get information about the network.\n- FNN(L): The second model is a vanilla feedforward neural network with two fully connected layers of width $N$ with sigmoid activation in between. Of course the assets are not informative, so to make it a fair comparison between the GNN and a feedforward neural network we provide the complete liability matrix as input (hence $\\operatorname{FNN}(\\mathrm{L})$ ). This neural network theoretically has all information available to perfectly distribute the bailout capital.\n- XPENN: We also consider the XPENN architecture from the previous section. In fact, this toy experiment showcases well which problems can occur for the standard PENN from Definition 5.8 and why they can be solved using the XPENN from Definition 5.12. We discuss the standard PENN at the end of this section. For comparison with GNN and FNN(L) we utilise the XPENN without node IDs and choose single layer feedforward neural networks of width 10 for $\\hat{\\rho}, \\hat{\\alpha}, \\hat{\\varphi}$ and a two layer neural network of width 10 for $\\hat{\\psi}$ with sigmoid activation.\n\nTask 1 - Overfitting We perform train runs on all four data sets and investigate if we can teach the models which perfect bailout strategy corresponds to which network structure. If the models are able to learn, this indicates that their complexity is large enough. We optimise the models with suitable learning\n\n![table_0](table_0)\n\nTable 1: Best inner risk of GNN, PENN and FNN(L) model on the stylised data set.\nrates ${ }^{5}$ making use of the ADAM optimiser which is a sophisticated version of stochastic gradient descent and train for 1000 epochs. It is worth pointing out that the first layer of the FNN(L) network connects $N+N^{2}$ neurons on one side to $N$ neurons on the other side. Hence, the numbers of parameters we are training (over $N^{3}$ ) is massively higher than the number of data points in the training set $(2 N)$. One has to expect that achieving overfitting will not be an issue whereas the number of parameters and the time it takes to train them will grow dramatically. Meanwhile the GNNs number of parameters is independent from the graph size and instead is driven by the weight matrices of constant size (in our case $20 \\times 10$ ) that map current state and neighborhood state of each node to its new node state in every step of message passing. As the size of the financial networks increases, more calculations will be involved in each step of massage passing. Hence the computation time will increase as well, but not as dramatically, especially since our networks are sparse. The XPENN architecture is also independent from the size of the network. However, it calculates a signal for every node pair, even if there is no edge between them. Therefore it can not benefit from the fact that the networks are sparse and computation time will increase quadratically.\nFor $N=10$ (see Table 1) without any bailout capital the average loss of the data set equals 45 . The GNN and XPENN are both able to learn the perfect bailout capital allocation within 1000 epochs (actually already earlier) which takes about two minutes on our machine. ${ }^{6}$ In 1000 epochs ( 2 min ), the FNN(L) model learns a strategy that leads to a loss of 2.37 or roughly $5 \\%$ of the loss without bailout. Doubling the network size to $N=20$ the loss without intervention amounts to 190 . GNN and XPENN learn perfect bailout in 1000 epochs (around 3 min ) and the FNN(L) arrives at a loss of $10.61(\\sim 5 \\%)$ after 1000 epochs ( 4 min ). At $N=50$ we observe that the run times start to diverge drastically. In 6 minutes the GNN learns perfect bailout, the XPENN needs 14 minutes, and the FNN(L) takes 30 minutes to decrease the loss from 1225 without intervention to $58.56(\\sim 5 \\%)$. Finally when $N=100$ the parameter space of the FNN(L) is so high dimensional that the learning rate for training has to be very small to wiggle the parameters in the right direction. After 4 hours of training the loss is still very high $(1,532)$ compared to no intervention $(4,950)$ with no signs of convergence. The GNN handles this case better and reaches perfect bailout in under 15 minutes. The XPENN also reaches perfect bailout, however, the 1000 epochs take 47 minutes.\nFrom the results of this small experiment we conclude that relatively good bailout obtained from a feedfor-\n\n[^0]\n[^0]:    ${ }^{5}$ In the experiments of Sections 6.1, 6.2 and 6.3 we considered for all models learning rates in $\\left\\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\\right\\}$ and the best results were selected by grid search.\n    ${ }^{6}$ The authors gratefully acknowledge the Leibniz Supercomputing Centre for funding this project by providing computing time on its Linux-Cluster.\n\nward neural network that takes in the liability matrix of a network might be possible, however, the required computing time in order to train the model becomes unfeasibly large quickly. GNN and XPENN on the other hand seem to be promising architectures. The difference in computation time between GNN and XPENN is due to the sparseness of the data. While GNNs scale with the actual edges in the network $\\left(L_{i j}>0, i, j \\in[N]\\right)$, the XPENN scales with the number of all possible $N(N-1)$ edges. Later we will see that for data that is not sparse they are actually quite similar.\n\n![table_1](table_1)\n\nTable 2: Best inner risk of GNN, XPENN and FNN(L) model on the train and test set of the stylised network data. GNN and XPENN achieve better results compared to a FNN(L).\n\nTask 2 - Generalisation Next we repeat the experiment in a similar manner, because we want to know whether the models that we train generalise to unseen data. We randomly split the data set of $2 N$ networks into two partitions. A training set containing $75 \\%$ of the networks and a test set with the remaining $25 \\%$. We optimise the models over $\\{10000,5000,2000,2000\\}$ epochs, depending on the network sizes, utilising the ADAM optimiser. We obtain the following results (Table 2).\nFor all network sizes, the performance gain of the $\\operatorname{FNN}(\\mathrm{L})$ on the training set after epoch 1000 is negligible\n\nexcept for the case $N=100$ were there is still room for improvement. However, the success on the training set can not be translated into good bailout capital allocation on the test set. In general the results do not exhibit any surprises regarding performance or run time compared to what we saw in the previous experiment. However, it is noticeable that GNN and XPENN are able to translate perfect bailout on the train set (which is found every time) into perfect bailout on the test set. The reason for this is, that reordering the nodes does not change the messages that GNN and XPENN receive from the other nodes. This invariance is a major advantage compared to the $\\operatorname{FNN}(\\mathrm{L})$. Based on this experiment, GNN and XPENN seem to be promising candidates for problems of this type.\n\nDiscussion of PENN types During our experiments we observed that, unlike the extended PENN (XPENN) which yields promising results, the normal PENN does not seem to work well even though in theory it should be able to find perfect bailout. In this paragraph we want to indicate what the problems might be on this specific data set and how the XPENN can overcome them.\nSince all assets equal zero, the nodes have identical node features. Hence it is clear from the definition that the standard PENN without node IDs will not be able to assign different bailout capital to the different nodes.\nBut even with node IDs, the PENN fails to predict good bailout already in the simplest case $N=10$. We argue that this has two main reasons. On one hand, the PENN aggregates the wrong information in the inner sum $\\sum \\varphi(\\ldots)$. On the other hand, this information is lost in the outer sum over all nodes $\\sum_{i=1}^{N} \\alpha(\\ldots)$ and can not be recovered properly by $\\rho$.\nWhat we mean by aggregating the wrong information is the following. For any node $i$ the PENN calculates signals based on $L_{i j}$ - the outgoing edges of $i$ - while it would be more useful to gather information about the incoming edges of $i$. Indeed the optimal bailout allocation in this data set is such that a node never gets capital if it has incoming liabilities. Information about the outgoing liabilities alone, on the other hand, is not enough to deduce the allocated capital. This motivates why we consider both directions (incoming and outgoing) in the XPENN architecture, see Definition 5.12.\nTo show that aggregating the \"wrong\" data is not the only problem, we also tested PENNs that consider incoming instead of outgoing liabilities. However, even then, we saw that the standard PENN is not able to predict good bailout. And this despite the fact that in order to predict perfect bailout it would be enough to recover the sum of incoming liabilities for each node. Since it is easy to obtain the sum of incoming liabilities of each node from the liability matrix (it is just the column sum of the respective columns), the problem must be obtaining the liability matrix from the outer sum $\\sum \\alpha(\\ldots)$. Additional experiments suggest that these difficulties might arise due to the inhomogeneity of the financial networks, in particular the edge weights. To solve this problem we designed the XPENN architecture such that it also consists of a node specific component. As a consequence, graph-wide information given by the outer sum and node-specific information can complement each other, but it is not necessarily required anymore, to reconstruct node specific information from the outer sum.\nWe conclude that indeed the data that we generated in this experiment can not be handled well by the standard PENN, due to the uninformative node features and the inhomogeneity of the financial networks. The XPENN architecture that we propose manages to overcome these problems by on the one hand, always considering in- and outgoing edges, and on the other hand, considering a node specific component in which each node can aggregate information about all its connections (or rather all possible connections), both incoming and outgoing. The next experiments show that on other types of data the standard PENN yields very good results as well.", "tables": {"table_0": "| $N$ | Model | Epoch | Inner Risk | Bailout | Runtime |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| 10 | None | - | 45.00 | 9.00 | - |\n| 10 | GNN | 1000 | 0.00 | 9.00 | 2 min |\n| 10 | XPENN | 1000 | 0.00 | 9.00 | 2 min |\n| 10 | FNN(L) | 1000 | 2.37 | 9.00 | 2 min |\n| 20 | None | - | 190.00 | 19.00 | - |\n| 20 | GNN | 1000 | 0.00 | 19.00 | 3 min |\n| 20 | XPENN | 1000 | 0.00 | 19.00 | 4 min |\n| 20 | FNN(L) | 1000 | 10.61 | 19.00 | 4 min |\n| 50 | None | - | 1225.00 | 49.00 | - |\n| 50 | GNN | 1000 | 0.00 | 49.00 | 6 min |\n| 50 | XPENN | 1000 | 0.00 | 49.00 | 14 min |\n| 50 | FNN(L) | 1000 | 58.56 | 49.00 | 30 min |\n| 100 | None | - | 4950.00 | 99.00 | - |\n| 100 | GNN | 1000 | 0.00 | 99.00 | 15 min |\n| 100 | XPENN | 1000 | 0.00 | 99.00 | 47 min |\n| 100 | FNN(L) | 1000 | 1532.64 | 99.00 | 146 min |", "table_1": "| $N$ | Model | Epoch | Train Risk | Test Risk | Bailout | Total Runtime |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| 10 | None | - | 47.40 | 37.80 | 00.00 | - |\n| 10 | GNN | 10 | 0.25 | 0.29 | 9.00 | 7 min |\n| 10 | GNN | 680 | 0.00 | 0.00 | 9.00 | 7 min |\n| 10 | XPENN | 10 | 21.47 | 16.27 | 9.00 | 6 min |\n| 10 | XPENN | 350 | 0.00 | 0.00 | 9.00 | 6 min |\n| 10 | FNN(L) | 10 | 21.20 | 19.12 | 9.00 | 11 min |\n| 10 | FNN(L) | 1000 | 3.91 | 26.78 | 9.00 | 11 min |\n| 10 | FNN(L) | 10000 | 3.62 | 29.35 | 9.00 | 11 min |\n| 20 | None | - | 190.00 | 190.00 | 00.00 | - |\n| 20 | GNN | 10 | 0.63 | 0.63 | 19.00 | 8 min |\n| 20 | GNN | 690 | 0.00 | 0.00 | 19.00 | 8 min |\n| 20 | XPENN | 10 | 84.05 | 84.05 | 19.00 | 13 min |\n| 20 | XPENN | 280 | 0.00 | 0.00 | 19.00 | 13 min |\n| 20 | FNN(L) | 10 | 89.10 | 96.58 | 19.00 | 20 min |\n| 20 | FNN(L) | 1000 | 26.82 | 104.59 | 19.00 | 20 min |\n| 20 | FNN(L) | 5000 | 26.64 | 103.92 | 19.00 | 20 min |\n| 50 | None | - | 1240.68 | 1177.96 | 00.00 | - |\n| 50 | GNN | 10 | 0.42 | 0.44 | 49.00 | 11 min |\n| 50 | GNN | 380 | 0.00 | 0.00 | 49.00 | 11 min |\n| 50 | XPENN | 10 | 157.01 | 148.75 | 49.00 | 21 min |\n| 50 | XPENN | 170 | 0.00 | 0.00 | 49.00 | 21 min |\n| 50 | FNN(L) | 10 | 89.10 | 96.58 | 49.00 | 47 min |\n| 50 | FNN(L) | 1000 | 144.79 | 626.50 | 49.00 | 47 min |\n| 50 | FNN(L) | 2000 | 144.60 | 458.35 | 49.00 | 47 min |\n| 100 | None | - | 4950.00 | 4950.00 | 00.00 | - |\n| 100 | GNN | 10 | 0.01 | 0.01 | 99.00 | 28 min |\n| 100 | GNN | 20 | 0.00 | 0.00 | 99.00 | 28 min |\n| 100 | XPENN | 10 | 66.11 | 66.11 | 99.00 | 61 min |\n| 100 | XPENN | 120 | 0.00 | 0.00 | 99.00 | 61 min |\n| 100 | FNN(L) | 10 | 2438.97 | 2488.43 | 99.00 | 240 min |\n| 100 | FNN(L) | 1000 | 1064.72 | 2617.05 | 99.00 | 240 min |\n| 100 | FNN(L) | 2000 | 670.28 | 2595.52 | 99.00 | 240 min |"}, "images": {}}, {"section_id": 17, "text": "# 6.2 Allocation of Fixed Bailout Capital \n\nProblem In this section we again approximate the inner risk\n\n$$\n\\rho_{I}^{\\eta}\\left(G_{N}\\right)=\\min _{Y \\in C^{+}}\\{\\eta(\\Lambda((A+Y, L)))\\}\n$$\n\nwith allocations in\n\n$$\n\\mathcal{C}^{a}=\\left\\{Y \\in L^{\\infty}\\left(\\mathbb{R}_{+}^{N}\\right) \\mid \\sum_{n=1}^{N} Y_{n}=a\\right\\}\n$$\n\nfor $a \\in \\mathbb{R}_{+}$and choosing $\\eta$ to be the expectation. However, the stochastic networks we investigate are more complex than before. Therefore, it is not clear which minimal loss could be achieved with the fixed bailout capital that we choose to be at level $a=50$.\n\nData In three case studies we investigate three different stochastic financial networks that are chosen in a way such that they all contain default cascades that we are seeking to stop. Hence the networks could be interpreted as former healthy financial networks where some kind of shock already happened.\n\n- Erd\u0151s-R\u00e9nyi (ER): This homogeneous network contains $N=100$ nodes of the same type. Their assets follow independent $\\operatorname{Beta}(2,5)$-distributions scaled with a factor of 10 . The edges of fixed liability size 1 between any two nodes are drawn independently with probability $p=0.4$.\n- Core-Periphery (CP): This inhomogeneous financial network is an adapted version of networks proposed in Feinstein et al. [26]. The network consists of a total of $N=100$ banks split in two different groups. $T_{1} \\subset\\{1, \\ldots, 100\\},\\left|T_{1}\\right|=10$, contains 10 large banks and $T_{2}$ the remaining 90 small banks. A link of a bank in group $i$ towards a bank in group $j, 1 \\leq i, j \\leq 2$ is sampled with a probability of $p_{i j}$. We set $p_{1,1}=0.7$, $p_{1,2}=p_{2,1}=0.3, p_{2,2}=0.1$. If there are links, large banks owe each other 10 , small banks 1 . Connections between large and small banks are of size 2 , if present. The assets have one-dimensional margins following $\\operatorname{Beta}(2,5)$ distributions that are correlated via a Gaussian copula with a correlation of 0.5 between any two banks. Larger bank's assets are scaled with a factor of 50 , smaller bank's with a factor of 10 .\n- Core-Periphery-fixed (CPf): This network model is essentially the same as a Core-Periphery model. However, instead of sampling the liabilities new for every realisation of the network, we only sample the liability matrix once and all realisation share this structure. As a result we only vary the assets while the network structure is fixed.\n\nModels We utilise various neural network models to learn a distribution $\\varphi^{\\theta}$ of the fixed bailout capital (see Section 4.2) and compare their performance to non-parametric benchmarks, i.e. models without the necessity to train parameters. All models generate a score for each node. We use softmax activation to transform these scores into a distribution.\n\n- GNN: A graph neural network with five SAGEConv layers of message passing. The 10 dimensional initial state of each node consists of assets, incoming liabilities, outgoing liabilities and 7 zeros. We use the sigmoid activation function between layers. The hidden states are of dimension 10 and the output layer yields a single score for each node.\n- PENN: We apply a PENN with single layer neural networks $\\hat{\\varphi}, \\hat{\\alpha}$ that produce 10 dimensional representations and a three layer neural network $\\hat{\\rho}$ with hidden states of dimension 10 producing a score. For each node the initial node features consist of assets, incoming liabilities and outgoing liabilities. For the performance it did not matter whether we include node IDs or not.\n- XPENN: This model is the extended version of the PENN. We used the same neural networks for $\\hat{\\varphi}, \\hat{\\alpha}, \\hat{\\rho}$. Additionally $\\hat{\\psi}$ is a two layer neural network of width 10 that provides a neighborhood signal for each node.\n- FNN: This model is a feedforward neural network with three fully connected layers of width 100 and sigmoid activation function. As input it takes a concatenated vector of assets, incoming and outgoing liabilities of all 100 nodes and as output we obtain a score vector of the 100 nodes.\nThis approach is inspired by Feng et al. [28]. They model a financial network as a random vector $X \\in \\mathcal{L}\\left(\\mathbb{R}^{N}\\right)$ and obtain optimal bailout from a neural network $\\varphi: X \\mapsto \\varphi(X)$. However, in their setting there is no\n\nexplicitly modelled interaction between the components of $X$. Rather the random variable $X$ represents the network after interaction, which they incorporate by allowing correlation between the components of $X$. In this setting it makes sense to use the realisation of this random variable as representation of the systems state. Applied to our setting one might think that the realisation of the assets $A$ comes closest to the system state vector $X$. However, this is not entirely true, because $A$ represents some value before the interaction. This interaction depends on the realisation of the liability matrix, which is not captured by the FNN at all. Therefore, this approach can only work well if the dependence of the optimal bailout allocation on the liability matrix is negligible - which is not true in general. To increase the chances of this approach we provide assets, incoming liabilities and outgoing liabilities of each node, instead of only the assets.\n\n- FNN(L): This model is similar to the FNN model, but as input it takes not only assets, incoming and outgoing liabilities of all 100 nodes, but additionally the liability matrix as one long vector. It can be considered a naive extension of the approach in [28] to our setting of stochastic liability matrices. We consider two fully connected layers with width 100 .\n- Linear: In this approach we fit a linear model that predicts every nodes score one at a time based on the node's assets, nominal incoming and outgoing liabilities as well as a bias term.\n- Constant: Furthermore, we are interested in how much performance we gain by making the allocation random, i.e. scenario dependent. Therefore, for comparison, we include a benchmark that learns a constant bailout allocation.\n- Uniform: This is a non-parametric benchmark where every node gets the same share of bailout capital.\n- Default: This is a non-parametric benchmark where every node that defaults without additional bailout capital receives the same amount.\n- Level-1: This is a non-parametric benchmark where every node whose nominal balance is negative $\\left(A_{i}-\\right.$ $\\left.L_{\\text {out }, i}+L_{\\text {in }, i}<0\\right)$ gets the same share of bailout capital. These nodes are the \"first round defaults\" and theoretically, there can not be additional defaults if all \"first round\" defaults are prevented.\n\nResults The results of all models can be found in Tables 3, 4 and 5 for the ER, CP and CPf network respectively. The training algorithm employed is the one presented in Section 4.2, see Algorithm 2.\nIn the homogeneous ER network (see Table 3) it seems the best model is the XPENN. It performs slightly better than GNN and PENN, which basically reach the same risk level. The Constant bailout strategy and the FNN do not work well and can not provide bailout allocations that are superior to allocating all capital uniformly. The $\\mathrm{FNN}(\\mathrm{L})$ seems to learn something on the training set, but it is the only model that does not translate learned success to unseen data, a pattern that we also observed in the previous toy experiment. The Linear model performs better than the Default method and is not far off the Level-1 method. Surprisingly the Level-1 method performs strong considering it does not need any learning and is indeed the fourth best model with a similar performance as GNN and PENN.\nIn the CP network (see Table 4) the picture is similar. GNN and PENN are the second best models only outperformed by the XPENN. The Constant model again fails to provide significantly better bailout than Uniform bailout. Level-1 bailout comes fourth, however, this time with a bigger distance to GNN and PENN as before and closely followed by the Linear model, which is again better than the Default method. This time both FNN and FNN(L) learn something on the train set, but fail to generalise to new data.\nFinally, in the CPf network (see Table 5) we observe a change of order. Basically all more complex ML models (GNN, PENN, XPENN, FNN, FNN(L)) obtain similar results, with GNN being the best and FNN(L) slightly weaker than the rest. Constant bailout is not far off the ML models, finally better than Uniform bailout, and is even better than Linear and Level-1 bailout, which are very similar to another.\nThe difference between Linear model and GNN, PENN, or XPENN indicates that in all three network types there is structural information that is needed in order to provide the best bailout. The fact that the FNN(L) does not work in the ER and CP network, i.e. the cases where there is structural information in the\n\nliability matrix, hints that such structural information can not easily be learned by feeding the matrix into a feedforward neural network. Moreover, the difference between Constant bailout and the best model shows how much performance can be gained in the ER and CP case by making allocation random.\nThe good performance of the FNN and $\\operatorname{FNN}(\\mathrm{L})$ in the CPf network shows that these approaches can work if the dependence of the system on the liability matrix is small, in the sense that the influence is constant and hence learnable. This aligns well with the results in [28].\nSince in the CPf network the liability matrix is constant, this means not only that the interaction between the banks is fixed, it also means that - unlike in the CP network - all banks are always in the same position. This might be the reason why the Constant model actually works to some extent in this network, while it performs as poor as Uniform bailout in the other networks.\n\n![table_2](table_2)\n\nTable 3: Approximated inner risk on train, validation and test set obtained from the bailout allocations learned by the different models on data sampled from the ER network type.\n\n![table_3](table_3)\n\nTable 4: Approximated inner risk on train, validation and test set obtained from the bailout allocations learned by the different models on data sampled from the CP network type.", "tables": {"table_2": "| Model | Train Risk | Val. Risk | Test Risk | Capital |\n| :--: | :--: | :--: | :--: | :--: |\n| None | 261.88 | 262.61 | 261.05 | 50.00 |\n| Uniform | 221.42 | 222.31 | 220.74 | 50.00 |\n| Default | 186.61 | 187.57 | 185.94 | 50.00 |\n| Level-1 | 174.52 | 175.38 | 173.81 | 50.00 |\n| GNN | 173.87 | 174.82 | 173.22 | 50.00 |\n| PENN | 173.92 | 174.85 | 173.28 | 50.00 |\n| XPENN | 171.79 | 172.68 | 171.15 | 50.00 |\n| FNN | 221.35 | 222.37 | 220.81 | 50.00 |\n| $\\operatorname{FNN}(\\mathrm{L})$ | 205.89 | 217.82 | 216.24 | 50.00 |\n| Linear | 178.87 | 179.94 | 178.12 | 50.00 |\n| Constant | 221.37 | 222.39 | 220.87 | 50.00 |", "table_3": "| Model | Train Risk | Val. Risk | Test Risk | Capital |\n| :--: | :--: | :--: | :--: | :--: |\n| None | 235.26 | 239.48 | 239.22 | 50.00 |\n| Uniform | 201.23 | 204.41 | 204.45 | 50.00 |\n| Default | 166.35 | 169.34 | 169.73 | 50.00 |\n| Level-1 | 157.82 | 160.71 | 160.99 | 50.00 |\n| GNN | 152.80 | 155.80 | 155.97 | 50.00 |\n| PENN | 152.84 | 155.86 | 155.99 | 50.00 |\n| XPENN | 149.28 | 152.23 | 152.32 | 50.00 |\n| FNN | 179.47 | 202.26 | 201.90 | 50.00 |\n| $\\operatorname{FNN}(\\mathrm{L})$ | 165.90 | 196.52 | 196.36 | 50.00 |\n| Linear | 159.64 | 162.72 | 162.48 | 50.00 |\n| Constant | 201.17 | 204.49 | 204.56 | 50.00 |"}, "images": {}}, {"section_id": 18, "text": "# 6.3 Computation of Systemic Risk Measures \n\nProblem In this section instead of fixing an arbitrary amount of available bailout capital, we fix some level of risk that we consider as acceptable $(b=100)$ and search for the systemic risk\n\n$$\n\\rho_{b}(G)=\\min _{Y \\in \\mathcal{C}}\\left\\{\\sum_{n=1}^{N} Y_{n} \\mid \\eta(\\Lambda(A+Y, L)) \\leq b\\right\\}\n$$\n\n![table_4](table_4)\n\nTable 5: Approximated inner risk on train, validation and test set obtained from the bailout allocations learned by the different models on data sampled from the CPf network type.\nwhere\n\n$$\n\\mathcal{C}=\\left\\{Y \\in L^{\\infty}\\left(\\mathbb{R}_{+}^{N}\\right) \\mid \\sum_{n=1}^{N} Y^{n} \\in \\mathbb{R}\\right\\}\n$$\n\nWe study the same Data and Models as before. The only difference in the training algorithm is the following. We start with some initial amount of bailout capital and after each epoch of learning how to allocate the current amount of bailout capital we increase or decrease the amount of bailout capital depending on whether the current inner risk is above or below the risk threshold of acceptability, see Algorithm 1 in Section 4.2.\n\nResults The results that we find during the search of the minimal bailout capital in the ER, CP and CPf network can be found in Tables 6, 7 and 8. The results are in line with the results in the previous experiment where we allocate a fixed amount of capital.\nIn the ER network (see Table 6) the XPENN is the best model as it needs the least bailout capital to reduce the inner risk on the train, validation and test set to an acceptable level. GNN and PENN reach similar risk with slightly more capital. Level-1, Linear and Default model already need significantly more capital to reach acceptable risk. $\\mathrm{FNN}(\\mathrm{L})$ obtains a small risk on the train set by allocating a huge amount of bailout capital, but like in the previous experiments, does not generalise to validation or test set. FNN, Constant and Uniform need much more capital to render the risk acceptable compared to the other approaches.\nIn the CP network (Table 7) XPENN is the strongest model closely followed by GNN and PENN. With a slightly larger gap the next best models are Level-1, Linear and Default model. Constant and Uniform need too much capital to compete with the other models, while FNN and $\\mathrm{FNN}(\\mathrm{L})$ can not be assessed reasonably since they both fail to generalise to validation and test data.\nComparing Level-1 or Linear versus the Constant model in both, the ER and CP network, we see that a lot of bailout capital can be saved by allowing for scenario dependent allocations as opposed to committing to a constant capital allocation. The difference between XPENN, PENN or GNN on one side and the Linear or Level-1 models on the other side shows, how much bailout capital can be saved by choosing an approach that can process the liability matrix effectively. The results of FNN and FNN(L) clearly show that these approaches are not applicable to networks with stochastic liability matrix.\nFinally in the CPf network (Table 8) we see that XPENN, GNN and PENN provide the best result but also FNN, FNN(L) and even Constant work well and outperform Linear, Level-1, Default and Uniform. The results underline again that in the case of a fixed network structure it is not necessary to utilise models that process the liability matrix effectively. Since the effect is always the same it can also be learned implicitly without being given as input to the model. It is interesting that XPENN, GNN and PENN still provide the best result. Apparently it does not harm their performance to receive the liability matrix as input anyway.\n\nOne explanation why GNN, PENN and XPENN appear to be slightly stronger than FNN and FNN(L) might be that they obtain good bailout quicker than FNN(L) and FNN which makes them more suitable to the training procedure that alternates between updating the bailout capital and learning how to allocate it. This explanation is partially backed by the observation in the toy experiment, where GNN and XPENN reached good bailout much quicker than the FNN(L).\nThe small difference between Constant bailout and the best models in the CPf network suggests, that in networks with fixed liability matrix the benefit of stochastic allocations is smaller that in stochastic networks. This makes sense since it seems easier to predict whether an institution will need capital or not if there is no uncertainty about its connections from and to other institutions in the network.\nThe poor performance of Linear and Level-1 in the CPf network is surprising, after their good performance in the ER and CP networks. This suggests that the optimal bailout of the CPf network with its randomly fixed liability matrix seems not to be driven by a linear combination of assets, incoming and outgoing liabilities or the \"first round\" defaults. Apparently there are more complex interactions at work that can be learned by the other trainable models.\n\n![table_5](table_5)\n\nTable 6: The \"Capital\" column shows for each model the best approximation of the systemic risk, i.e. the smallest amount of required bailout capital, for the ER network. The other columns show the reached inner risk on train, validation and test set.\n\n![table_6](table_6)\n\nTable 7: The \"Capital\" column shows for each model the best approximation of the systemic risk, i.e. the smallest amount of required bailout capital, for the CP network. The other columns show the reached inner risk on train, validation and test set.\n\n![table_7](table_7)\n\nTable 8: The \"Capital\" column shows for each model the best approximation of the systemic risk, i.e. the smallest amount of required bailout capital, for the CPf network. The other columns show the reached inner risk on train, validation and test set.", "tables": {"table_4": "| Model | Train Risk | Val. Risk | Test Risk | Capital |\n| :--: | :--: | :--: | :--: | :--: |\n| None | 296.15 | 301.09 | 297.41 | 50.00 |\n| Uniform | 253.85 | 257.87 | 254.87 | 50.00 |\n| Default | 218.07 | 222.05 | 219.16 | 50.00 |\n| Level-1 | 208.42 | 212.27 | 209.77 | 50.00 |\n| GNN | 199.28 | 203.21 | 200.43 | 50.00 |\n| PENN | 199.88 | 203.84 | 201.08 | 50.00 |\n| XPENN | 199.83 | 203.82 | 201.07 | 50.00 |\n| FNN | 199.86 | 203.81 | 201.21 | 50.00 |\n| FNN(L) | 200.27 | 204.20 | 201.56 | 50.00 |\n| Linear | 208.81 | 212.86 | 210.01 | 50.00 |\n| Constant | 204.08 | 207.95 | 205.57 | 50.00 |", "table_5": "| Model | Train Risk | Val. Risk | Test Risk | Capital |\n| :--: | :--: | :--: | :--: | :--: |\n| None | 261.88 | 262.61 | 261.05 | 0.00 |\n| Uniform | 100.61 | 100.94 | 100.28 | 299.90 |\n| Default | 100.30 | 100.77 | 99.83 | 146.47 |\n| Level-1 | 100.22 | 100.81 | 99.55 | 107.86 |\n| GNN | 100.22 | 101.24 | 99.64 | 95.46 |\n| PENN | 100.22 | 101.24 | 99.64 | 95.47 |\n| XPENN | $\\mathbf{9 9 . 6 3}$ | $\\mathbf{1 0 0 . 6 0}$ | $\\mathbf{9 9 . 1 1}$ | $\\mathbf{9 4 . 7 3}$ |\n| FNN | 100.61 | 101.07 | 100.42 | 299.67 |\n| FNN(L) | 95.03 | 111.99 | 111.20 | 241.63 |\n| Linear | 100.23 | 101.37 | 99.49 | 113.13 |\n| Constant | 100.61 | 101.07 | 100.42 | 299.68 |", "table_6": "| Model | Train Risk | Val. Risk | Test Risk | Capital |\n| :--: | :--: | :--: | :--: | :--: |\n| None | 235.26 | 239.48 | 239.22 | 0.00 |\n| Uniform | 100.75 | 101.89 | 102.93 | 297.67 |\n| Default | 100.26 | 102.50 | 103.21 | 132.37 |\n| Level-1 | 100.21 | 102.50 | 102.88 | 106.07 |\n| GNN | $\\mathbf{1 0 0 . 1 8}$ | 102.55 | 102.97 | 86.04 |\n| PENN | 100.20 | 102.55 | 102.99 | 86.12 |\n| XPENN | 100.21 | $\\mathbf{1 0 2 . 4 3}$ | $\\mathbf{1 0 2 . 8 5}$ | $\\mathbf{8 4 . 8 2}$ |\n| FNN | 98.32 | 137.35 | 137.52 | 167.31 |\n| FNN(L) | 99.51 | 156.26 | 156.19 | 104.67 |\n| Linear | 100.23 | 103.01 | 103.46 | 110.88 |\n| Constant | 100.64 | 101.89 | 102.96 | 297.83 |", "table_7": "| Model | Train Risk | Val. Risk | Test Risk | Capital |\n| :--: | :--: | :--: | :--: | :--: |\n| None | 296.15 | 301.09 | 297.41 | 0.00 |\n| Uniform | 101.24 | 102.35 | 101.14 | 441.05 |\n| Default | 100.44 | 102.45 | 100.81 | 219.72 |\n| Level-1 | 100.33 | 102.23 | 101.22 | 165.00 |\n| GNN | $\\mathbf{9 9 . 3 9}$ | $\\mathbf{1 0 2 . 2 1}$ | $\\mathbf{1 0 0 . 2 9}$ | 114.67 |\n| PENN | 100.57 | 103.41 | 101.51 | 114.51 |\n| XPENN | 100.45 | 103.28 | 101.41 | $\\mathbf{1 1 4 . 2 3}$ |\n| FNN | 100.31 | 103.00 | 101.15 | 117.15 |\n| $\\operatorname{FNN}(\\mathrm{L})$ | 100.09 | 102.89 | 100.99 | 116.78 |\n| Linear | 100.22 | 102.66 | 101.02 | 138.92 |\n| Constant | 100.25 | 103.07 | 101.37 | 120.96 |"}, "images": {}}, {"section_id": 19, "text": "# 7 Conclusion \n\nIn this work we extend the notion of systemic risk measures with random allocation under the \"first allocate then aggregate\" paradigm from vector valued stochastic financial networks to networks that are represented by a stochastic asset vector and a stochastic liability matrix. Choosing an aggregation function in the market clearing mechanism of Eisenberg and Noe [22] and under some further reasonable assumptions, we investigate the theoretical properties of this systemic risk measure. We show existence and provide a reformulation of the systemic risk that allows to derive an iterative optimisation algorithm.\nFurthermore, we connect the domain of financial networks to the domain of weighted, directed graphs and, motivated by this connection, investigate neural network architectures that are permutation equivariant. We study permutation equivariant neural networks (PENNs) [41] and introduce extended permutation equivariant neural networks (XPENNs) that are permutation equivariant. For both architectures we prove universal approximation results, in the sense that they can approximate any permutation equivariant node labeling function arbitrarily well in probability.\nIn numerical experiments with synthetic financial networks we show that in order to compute systemic risk measures of networks with stochastic liability matrix it seems that permutation equivariance is a crucial property of the employed neural network model. Our results highlight that potentially a lot of bailout capital can be saved by utilising models that allow for scenario dependent allocations and can effectively process the liability matrix by leveraging permutation equivariance.\nFor further work it would be interesting to investigate if similar theoretical properties can be established for contagion models with additional default mechanisms like fire-sales. Besides theoretical challenges due to the non-convexity, this would require to extend the domain of stochastic financial networks to directed heterogeneous graphs with different node and edge types. Subsequently, different neural network architectures would potentially need to be utilised for the computation of such systemic risk measures.", "tables": {}, "images": {}}, {"section_id": 20, "text": "## A Supplementary Theory\n\nTheorem A. 1 (Koml\u00f3s Theorem, see Koml\u00f3s [47] or Lemma 1.70 in F\u00f6llmer and Schied [32]). Given a sequence $\\left(\\xi_{n}\\right)$ in $L^{0}\\left(\\Omega, \\mathcal{F}_{0}, \\mathbb{P} ; \\mathbb{R}^{d}\\right)$ such that $\\sup _{n}\\left|\\xi_{n}\\right|<\\infty \\mathbb{P}$-almost surely. Then there exists a sequence of convex combinations\n\n$$\n\\eta_{n} \\in \\operatorname{conv}\\left\\{\\xi_{n}, \\xi_{n+1}, \\ldots\\right\\}\n$$\n\nwhich converges $\\mathbb{P}$-almost surely to some $\\eta \\in L^{0}\\left(\\Omega, \\mathcal{F}_{0}, \\mathbb{P} ; \\mathbb{R}^{d}\\right)$.", "tables": {}, "images": {}}, {"section_id": 21, "text": "# References \n\n[1] V. V. Acharya, L. H. Pedersen, T. Philippon, and M. Richardson. Measuring systemic risk. The review of financial studies, 30(1):2-47, 2017.\n[2] T. Adrian and M. K. Brunnermeier. Covar. Technical report, National Bureau of Economic Research, 2011.\n[3] H. Amini, R. Cont, and A. Minca. Resilience to contagion in financial networks. Mathematical finance, 26(2):329-365, 2016.\n[4] K. Anand, I. Van Lelyveld, \u00c1. Banai, S. Friedrich, R. Garratt, G. Halaj, J. Fique, I. Hansen, S. M. Jaramillo, H. Lee, et al. The missing links: A global study on uncovering financial network structures from partial data. Journal of Financial Stability, 35:107-119, 2018.\n[5] \u00c7. Ararat and N. Meimanjan. Computation of systemic risk measures: a mixed-integer programming approach. Operations Research, 71(6):2130-2145, 2023.\n[6] P. Artzner, F. Delbaen, J.-M. Eber, and D. Heath. Coherent measures of risk. Mathematical Finance, 9(3):203-228, 1999. doi: https://doi.org/10.1111/1467-9965.00068. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9965.00068.\n[7] F. Biagini, J.-P. Fouque, M. Frittelli, and T. Meyer-Brandis. A unified approach to systemic risk measures via acceptance sets. Mathematical Finance, 29(1):329-367, 2019. doi: https://doi.org/10. 1111/mafi.12170. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/mafi.12170.\n[8] F. Biagini, J.-P. Fouque, M. Frittelli, and T. Meyer-Brandis. On fairness of systemic risk measures. Finance and Stochastics, 24(2):513-564, 2020.\n[9] F. Biagini, A. Doldi, J.-P. Fouque, M. Frittelli, and T. Meyer-Brandis. Systemic optimal risk transfer equilibrium. Mathematics and Financial Economics, 15:233-274, 2021.\n[10] F. Biagini, L. Gonon, and T. Reitsam. Neural network approximation for superhedging prices. Mathematical Finance, 33(1):146-184, 2023.\n[11] Y. Braouezec and L. Wagalath. Strategic fire-sales and price-mediated contagion in the banking system. European Journal of Operational Research, 274(3):1180-1197, 2019.\n[12] M. K. Brunnermeier and P. Cheridito. Measuring and allocating systemic risk. Risks, 7(2):46, 2019.\n[13] F. Caccioli, M. Shrestha, C. Moore, and J. D. Farmer. Stability analysis of financial contagion due to overlapping portfolios. Journal of Banking $\\mathcal{G}$ Finance, 46:233-245, 2014.\n[14] C. Chen, G. Iyengar, and C. C. Moallemi. An axiomatic approach to systemic risk. Management Science, 59(6):1373-1388, 2013.\n[15] R. Cifuentes, G. Ferrucci, and H. S. Shin. Liquidity risk and contagion. Journal of the European Economic association, 3(2-3):556-566, 2005.\n[16] G. Cimini, T. Squartini, D. Garlaschelli, and A. Gabrielli. Systemic risk analysis on reconstructed economic and financial networks. Scientific reports, 5(1):1-12, 2015.\n[17] R. Cont and E. Schaanning. Monitoring indirect contagion. Journal of Banking $\\mathcal{G}$ Finance, 104:85-102, 2019.\n[18] R. Cont and L. Wagalath. Fire sales forensics: measuring endogenous risk. Mathematical finance, 26 (4):835-866, 2016.\n\n[19] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303-314, 1989.\n[20] G. A. D\u2019Inverno, M. Bianchini, M. L. Sampoli, and F. Scarselli. On the approximation capability of gnns in node classification regression tasks. arXiv preprint arXiv:2106.08992, 2023.\n[21] A. Doldi, Y. Feng, J.-P. Fouque, and M. Frittelli. Multivariate systemic risk measures and computation by deep learning algorithms. Quantitative Finance, 23(10):1431-1444, 2023.\n[22] L. Eisenberg and T. H. Noe. Systemic risk in financial systems. Management Science, 47(2):236-249, 2001. ISSN 00251909, 15265501. URL http://www.jstor.org/stable/2661572.\n[23] H. Elsinger. Financial networks, cross holdings, and limited liability. Technical report, working paper, 2009 .\n[24] Z. Feinstein. Financial contagion and asset liquidation strategies. Operations Research Letters, 45(2): $109-114,2017$.\n[25] Z. Feinstein. Capital regulation under price impacts and dynamic financial contagion. European Journal of Operational Research, 281(2):449-463, 2020.\n[26] Z. Feinstein, B. Rudloff, and S. Weber. Measures of systemic risk. SIAM Journal on Financial Mathematics, 8(1):672-708, 2017.\n[27] Z. Feinstein, W. Pang, B. Rudloff, E. Schaanning, S. Sturm, and M. Wildman. Sensitivity of the eisenberg-noe clearing vector to individual interbank liabilities. SIAM Journal on Financial Mathematics, 9(4):1286-1325, 2018.\n[28] Y. Feng, M. Min, and J.-P. Fouque. Deep learning for systemic risk measures. arXiv preprint arXiv:2207.00739, 2022.\n[29] H. F\u00f6llmer and A. Schied. Convex measures of risk and trading constraints. Finance and stochastics, $6: 429-447,2002$.\n[30] R. Frey and J. Hledik. Diversification and systemic risk: A financial network perspective. Risks, 6(2): $54,2018$.\n[31] M. Frittelli and E. R. Gianin. Putting order in risk measures. Journal of Banking \\& Finance, 26(7): $1473-1486,2002$.\n[32] H. F\u00f6llmer and A. Schied. Stochastic Finance. De Gruyter, Berlin, Boston, 2016. ISBN 9783110463453. doi: doi:10.1515/9783110463453. URL https://doi.org/10.1515/9783110463453.\n[33] P. Gai and S. Kapadia. Contagion in financial networks. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 466(2120):2401-2423, 2010.\n[34] P. Gai and S. Kapadia. Liquidity hoarding, network externalities, and interbank market collapse. Proc. R. Soc. A, 466(2401-2423):439, 2010.\n[35] A. Gandy and L. A. Veraart. A bayesian methodology for systemic risk assessment in financial networks. Management Science, 63(12):4428-4446, 2017.\n[36] A. Gandy and L. A. M. Veraart. Adjustable network reconstruction with applications to cds exposures. Journal of Multivariate Analysis, 172:193-209, 2019.\n[37] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263-1272. PMLR, 2017.\n\n[38] P. Glasserman and H. P. Young. How likely is contagion in financial networks? Journal of Banking $\\mathcal{G}$ Finance, 50:383-399, 2015.\n[39] G. Halaj and C. Kok. Assessing interbank contagion using simulated networks. Computational Management Science, 10(2):157-186, 2013.\n[40] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.\n[41] R. Herzig, M. Raboh, G. Chechik, J. Berant, and A. Globerson. Mapping images to scene graphs with permutation-invariant structured prediction. Advances in Neural Information Processing Systems, 31, 2018.\n[42] H. Hoffmann, T. Meyer-Brandis, and G. Svindland. Risk-consistent conditional systemic risk measures. Stochastic Processes and their Applications, 126(7):2014-2037, 2016.\n[43] K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2): $251-257,1991$.\n[44] X. Huang, H. Zhou, and H. Zhu. A framework for assessing the systemic risk of major financial institutions. Journal of Banking $\\mathcal{E}$ Finance, 33(11):2036-2049, 2009.\n[45] T. R. Hurd, D. Cellai, S. Melnik, and Q. Shao. Illiquidity and insolvency: a double cascade model of financial crises. Available at SSRN 2424877, 2014.\n[46] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\n[47] J. Koml\u00f3s. A generalization of a problem of steinhaus. Acta Mathematica Academiae Scientiarum Hungaricae, 18(1-2):217-229, 1967.\n[48] E. Kromer, L. Overbeck, and K. Zilch. Systemic risk measures on general measurable spaces. Mathematical Methods of Operations Research, 84:323-357, 2016.\n[49] S. H. Lee. Systemic liquidity shortages and interbank network structures. Journal of Financial Stability, $9(1): 1-12,2013$.\n[50] A. Lehar. Measuring systemic risk: A risk management approach. Journal of Banking $\\mathcal{G}$ Finance, 29 (10):2577-2603, 2005.\n[51] M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural networks, 6(6):861-867, 1993.\n[52] H. Markowitz. Portfolio selection, the journal of finance. 7 (1). N, 1:71-91, 1952.\n[53] S. Maskey, R. Levie, Y. Lee, and G. Kutyniok. Generalization analysis of message passing neural networks on large random graphs. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 4805-4817. Curran Associates, Inc., 2022.\n[54] R. Mastrandrea, T. Squartini, G. Fagiolo, and D. Garlaschelli. Enhanced reconstruction of weighted networks from strengths and degrees. New Journal of Physics, 16(4):043022, 2014.\n[55] L. H. Pedersen, V. Acharya, T. Philippon, and M. Richardson. Measuring systemic risk. NYU Working Paper, 2010.\n[56] R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317. Springer Science \\& Business Media, 2009.\n\n[57] L. C. G. Rogers and L. A. M. Veraart. Failure and rescue in an interbank network. Management Science, 59(4):882-898, 2013. ISSN 00251909, 15265501. URL http://www.jstor.org/stable/23443817.\n[58] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. Computational capabilities of graph neural networks. IEEE Transactions on Neural Networks, 20(1):81-102, 2009. doi: 10.1109/ TNN.2008.2005141.\n[59] N. A. Tarashev, C. E. Borio, and K. Tsatsaronis. Attributing systemic risk to individual institutions. BIS Working paper, 2010.\n[60] A. Tobias and M. K. Brunnermeier. Covar. The American Economic Review, 106(7):1705, 2016.\n[61] P. Veli\u010dkovi\u0107, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n[62] J. Wang, S. Zhang, Y. Xiao, and R. Song. A review on graph neural network methods in financial applications. arXiv preprint arXiv:2111.15367, 2021.\n[63] S. Weber and K. Weske. The joint impact of bankruptcy costs, fire sales and cross-holdings on systemic risk in financial networks. Probability, Uncertainty and Quantitative Risk, 2:1-38, 2017.\n[64] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.\n[65] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. Graph neural networks: A review of methods and applications. AI open, 1:57-81, 2020.", "tables": {}, "images": {}}], "id": "2410.07222v1", "authors": ["Lukas Gonon", "Thilo Meyer-Brandis", "Niklas Weber"], "categories": ["q-fin.CP", "cs.LG", "q-fin.MF"], "abstract": "This paper investigates systemic risk measures for stochastic financial\nnetworks of explicitly modelled bilateral liabilities. We extend the notion of\nsystemic risk measures from Biagini, Fouque, Fritelli and Meyer-Brandis (2019)\nto graph structured data. In particular, we focus on an aggregation function\nthat is derived from a market clearing algorithm proposed by Eisenberg and Noe\n(2001). In this setting, we show the existence of an optimal random allocation\nthat distributes the overall minimal bailout capital and secures the network.\nWe study numerical methods for the approximation of systemic risk and optimal\nrandom allocations. We propose to use permutation equivariant architectures of\nneural networks like graph neural networks (GNNs) and a class that we name\n(extended) permutation equivariant neural networks ((X)PENNs). We compare their\nperformance to several benchmark allocations. The main feature of GNNs and\n(X)PENNs is that they are permutation equivariant with respect to the\nunderlying graph data. In numerical experiments we find evidence that these\npermutation equivariant methods are superior to other approaches.", "updated": "2024-09-30T10:18:13Z", "published": "2024-09-30T10:18:13Z"}