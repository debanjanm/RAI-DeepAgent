{"title": "Condensing and Extracting Against Online Adversaries", "sections": [{"section_id": 0, "text": "#### Abstract\n\nWe investigate the tasks of deterministically condensing and extracting randomness from Online Non-Oblivious Symbol Fixing (oNOSF) sources, a natural model of defective random sources for which it is known that extraction is impossible in many parameter regimes [AORSV, EUROCRYPT'20]. A $(g, \\ell)$-oNOSF source is a sequence of $\\ell$ blocks $\\mathbf{X}=\\left(\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{\\ell}\\right) \\sim$ $\\left(\\{0,1\\}^{n}\\right)^{\\ell}$, where at least $g$ of the blocks are good (are independent and have some min-entropy), and the remaining bad blocks are controlled by an online adversary where each bad block can be arbitrarily correlated with any block that appears before it.\n\nThe existence of condensers (in regimes where extraction is impossible) was recently studied in [CGR, FOCS'24]. They proved condensing impossibility results for various values of $g$ and $\\ell$, and they showed the existence of condensers matching the impossibility results in the special case when $n$ is extremely large compared to $\\ell$ (i.e., the setting of few blocks of large length).\n\nIn this work, we make significant progress on proving the existence of condensers with strong parameters in almost all parameter regimes, even when $n$ is a large enough constant and $\\ell$ is growing. This almost resolves the question of the existence of condensers for oNOSF sources, except when $n$ is a small constant.\n\nAs our next result, we construct the first explicit condensers for oNOSF sources and achieve parameters that match the existential results of [CGR, FOCS'24]. We also obtain a much improved construction for transforming low-entropy oNOSF sources (where the good blocks only have min-entropy, as opposed to being uniform) into uniform oNOSF sources.\n\nWe find interesting connections and applications of our results on condensers to collective coin flipping and collective sampling, problems that are well-studied in fault-tolerant distributed computing. We use our condensers to provide very simple protocols for these problems.\n\nNext, we turn to understanding the possibility of extraction from oNOSF sources. For proving lower bounds, we introduce and initiate a systematic study of a new, natural notion of the influence of functions, which we call online influence, and believe is of independent interest. Using tools from Fourier analysis, we establish tight bounds on the total online influence of functions, which imply extraction lower bounds. Lastly, we give explicit extractor constructions for oNOSF sources, using novel connections to leader election protocols, and further constructing the required leader election protocols. These extractor constructions achieve parameters that go beyond standard resilient functions [AL, Combinatorica'93].\n\n[^0]\n[^0]:    *Supported by a Sloan Research Fellowship and NSF CAREER Award 2045576.\n    ${ }^{\\dagger}$ Supported by NSF GRFP grant DGE - 2139899, NSF CAREER Award 2045576 and a Sloan Research Fellowship.\n    ${ }^{\\ddagger}$ Supported by NSF Award CCF-2106429 and NSF Award CCF-2211238.", "tables": {}, "images": {}}, {"section_id": 1, "text": "# Contents \n\n1 Introduction ..... 1\n1.1 Previous Work ..... 2\n1.2 New Condenser Constructions ..... 3\n1.3 Limits on Extraction from oNOSF Sources ..... 5\n1.4 Application to Collective Coin Flipping and Collective Sampling ..... 8\n2 Proof Overview ..... 10\n2.1 Existence of oNOSF Condensers for All $\\ell$ and $n$ ..... 10\n2.2 Explicit Condensers for Uniform oNOSF Sources ..... 11\n2.3 Converting Low-Entropy oNOSF Sources to Uniform oNOSF Sources ..... 12\n2.4 Online Influence and Extractor Lower Bounds ..... 13\n2.5 Extractors via Leader Election Protocols ..... 14\n2.6 Organization ..... 15\n3 Preliminaries ..... 15\n3.1 Basic Probability Notions ..... 15\n3.2 Condensers and Extractors ..... 16\n3.3 Leader Election, Collective Coin Flipping and Sampling Protocols ..... 16\n4 Existence of Condensers for All Values of $\\ell, n$ ..... 18\n4.1 Constructing Condensers for Uniform oNOSF Sources ..... 18\n4.2 Condenser for Two Uniform oNOSF Sources ..... 19\n5 Explicit Condensers for Uniform oNOSF Sources ..... 20\n5.1 Proving the Main Theorem ..... 21\n6 Transforming Low-Entropy oNOSF Sources to Uniform oNOSF Sources ..... 23\n6.1 Low-Entropy oNOSF Source to Uniform Using Two-Source-Extractors ..... 25\n7 Online Influence and Extraction Lower Bounds ..... 26\n7.1 Basic Properties ..... 27\n7.2 A Poincar\u00e9 Inequality for Online Influence ..... 28\n7.3 A Tight Example for Maximum Online Influence ..... 31\n7.4 Online Influence of Sets and Extraction Lower Bounds ..... 32\n8 Extractors for oNOSF and oNOBF Sources via Leader Election Protocols ..... 34\n9 High Probability Leader Election Protocols ..... 35\n9.1 One Bit per Round ..... 35\n9.2 Multiple Bits per Round ..... 38\n10 Open Problems ..... 40\nA Extracting from Local oNOSF Sources ..... 45", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 1 Introduction \n\nRandomness is extremely useful in computation with wide-ranging applications in algorithm design, cryptography, distributed computing protocols, machine learning, error-correcting codes, and much more [MR95, Vad12]. Most of these applications require access to high quality randomness. However in a lot of settings, especially arising in practice, algorithms only have access to low quality source of randomness. This motivates the notion of condensers: functions that transform weak random sources into strong random sources that are of better quality.\n\nIn this line of work, the standard way of measuring the amount of randomness is using minentropy. Formally, for a source (distribution) $\\mathbf{X}$ with support $\\Omega$, define its min-entropy as $H_{\\infty}(\\mathbf{X})=$ $\\min _{x \\in \\Omega} \\log _{2}(1 / \\operatorname{Pr}[\\mathbf{X}=x])$. We will also need the notion of smooth min-entropy, which measures how close a distribution is to having high entropy. Formally, for a source $\\mathbf{X}$, its smooth min-entropy with parameter $\\varepsilon$ is defined as $H_{\\infty}^{\\varepsilon}(\\mathbf{X})=\\max _{\\mathbf{Y}:|\\mathbf{X}-\\mathbf{Y}| \\leq \\varepsilon}\\left\\{H_{\\infty}(\\mathbf{Y})\\right\\}$, where $|\\cdot|$ denotes the statistical distance (Definition 3.1).\n\nWith this, we are ready to formally define deterministic condensers:\nDefinition 1.1. A function Cond : $\\{0,1\\}^{n} \\rightarrow\\{0,1\\}^{m}$ is a $\\left(k_{\\text {in }}, k_{\\text {out }}, \\varepsilon\\right)$-condenser for a family of distributions $\\mathcal{X}$ if for all $\\mathbf{X} \\in \\mathcal{X}$ with $\\mathbf{X} \\sim\\{0,1\\}^{n}$ and $H_{\\infty}(\\mathbf{X}) \\geq k_{\\text {in }}$, we have that $H_{\\infty}^{\\varepsilon}(\\mathbf{X}) \\geq k_{\\text {out }}$.\n\nWe say $\\frac{k_{\\text {in }}}{n}$ is the input entropy rate, $\\frac{k_{\\text {out }}}{m}$ is the output entropy rate, and $m-k_{\\text {out }}$ is the entropy gap of Cond.\n\nThe task of the condenser is to make the output entropy rate as high as possible compared to the input entropy rate, or, in other words, to make the output distribution more \"condensed\". Related to this, it is also desirable to have as small entropy gap as possible. Notice that if the entropy gap is 0 , the output distribution is $\\varepsilon$-close to the uniform distribution. Such condensers with entropy gap 0 are known as randomness extractors - a topic that has been extensively studied in theoretical computer science.\n\nWhen $\\mathcal{X}$ is the family of all distributions, it is folklore that no non-trivial condensing is possible. ${ }^{1}$ So, we additionally assume that $\\mathcal{X}$ is a structured family of sources. ${ }^{2}$ Since extractors are the highest quality condensers, a significant amount of work has focused on constructing extractors for interesting family of sources, such as: sources generated by small circuits, two independent sources, algebraically generated sources, sources generated by small space sources, and many more [TV00, CZ19, DGW09, KZ07].\n\nHowever, for many natural family of sources, one can provably show that no extractor can exist. In such situations, one can still hope to show that high quality condensers exist. We note that condensers (and sources with high min-entropy rate) are very useful: the condensed distribution can be used to efficiently simulate randomized algorithms with small overhead, perform one-shot simulations for randomized protocols, cryptography and interactive proofs, and much more. [DPW14] showed these condensers are equivalent to 'unpredictability extractors' that can simulate cryptographic protocols against biased distinguishers. For details on these applications and more, see [AORSV20, DMOZ23, CGR24].\n\nIn this work, we focus on one natural family of sources where it is known that extraction is impossible (for many interesting parameter regimes). The family we consider are known as online\n\n[^0]\n[^0]:    ${ }^{1}$ Assuming $m \\leq n$ (wlog this holds since $\\left|\\operatorname{Cond}\\left(\\{0,1\\}^{n}\\right)\\right| \\leq 2^{n}$ ), $m-k_{\\text {out }} \\geq\\left(n-k_{\\text {in }}\\right)-\\log (1 /(1-\\varepsilon))$ and hence the output entropy rate cannot be more than the input entropy rate without incurring extremely large error ( $>0.999$ ).\n    ${ }^{2} \\mathrm{~A}$ different route, that has been widely studied, is to assume access to a short independent seed. In this work, we will limit ourselves to the deterministic or seedless setting.\n\nnon-oblivious symbol fixing sources (oNOSF sources). ${ }^{3}$ Formally:\nDefinition 1.2. $A(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}=\\left(\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{\\ell}\\right)$ is such that each block $\\mathbf{X}_{i}$ is over $\\{0,1\\}^{n}, g$ of the blocks are are independent sources with min-entropy $k$ (\"good blocks\"), and each \"bad block\" is an arbitrary function of the blocks with an index smaller than it. When $k=n$, we will call such sources uniform $(g, \\ell, n)$-oNOSF sources.\n\nThese sources are inspired by real-time randomness generation settings such as in blockchains. There, each subsequent block is random or controlled by an adversary. Since these sources are generated in real time, a bad block can only be a function of the blocks that have appeared so far, and it is reasonable to assume that the good blocks contain entropy and are independent. Further, there are natural cryptographic settings, such as creating a Common Reference String, that are widely used in various cryptographic protocols where oNOSF source sources naturally arise (see [AORSV20] for a discussion).\n\nOur results at a glance We almost fully resolve the question of the existence of condensers for oNOSF sources by showing that good condensers, meeting the impossibility results of [CGR24], exist when the number of blocks $\\ell$ is growing and $n$ is a large constant. We also construct explicit condensers for oNOSF sources matching the results of [CGR24] and obtain an improved construction for transforming low-entropy oNOSF sources into uniform oNOSF sources. Moreover, we find new applications of our results on condensers to collective coin flipping and collective sampling, and use these connections to provide simple protocols for these problems. In the context of extractors for oNOSF sources, we introduce the new, natural notion of online influence for Boolean functions and show extraction lower bounds for oNOSF sources by establishing tight bounds on the total online influence of functions. Lastly, using our novel connections to leader election protocols, we construct explicit extractors for uniform oNOSF sources by explicitly constructing the required leader election protocols, the results of which are summarized in Tables 1 and 2.\n\nOrganization The remainder of our introduction is structured as follows. We give an overview of previous work in Section 1.1 before presenting our main existential and explicit condenser results in Section 1.2. In Section 1.3, we present our results on the limits of extraction from oNOSF sources. In Section 1.4, we show how our results on condensers have implications for collective coin flipping and sampling protocols.", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 1.1 Previous Work \n\nExtractors The study of extractors for oNOSF sources was initiated by [AORSV20]. ${ }^{4}$ Their results include the following:\n\n- It is impossible to extract from uniform oNOSF sources when the fraction of good blocks is 0.99 .\n- An explicit transformation from $(g, \\ell, n, 0.9 n)$-oNOSF source into a source over $(\\{0,1\\}^{O(n)})^{\\ell-1}$ where $g-1$ of the blocks are uniform and independent.\n\n[^0]\n[^0]:    ${ }^{3}$ These sources are in contrast to non-oblivious symbol fixing (NOSF) sources where bad blocks can be arbitrary functions of all the good blocks. NOSF sources were introduced in [CGHFRS85] with applications in leakage-resilient cryptography, and have been well-studied.\n    ${ }^{4}$ In [AORSV20], these sources were called SHELA (Somewhere Honest Entropic Look Ahead) sources.\n\n- An explicit transformation from $(g, \\ell, n, 0.1 n)$-oNOSF source into a source over $\\left(\\{0,1\\}^{O(n)}\\right)^{100 \\ell}$ where $g-1$ of the blocks are uniform and independent.\n\nEven though the output entropy rate is only slightly more than the input-entropy rate in the second result and smaller in the third result, the fact that a lot of the blocks are truly uniform is very useful, and they find interesting cryptographic applications of these somewhere-extractors.\n\nBefore our work, the best known extractors for oNOSF sources could be obtained by using resilient functions or equivalently, extractors for NOSF sources (non-online version of oNOSF sources) constructed by [AL93, CZ19, Mek17, IMV23, IV24] ; these require $g \\geq \\ell-\\frac{\\ell}{(\\log \\ell)^{d}}$.\n\nCondensers oNOSF sources were further studied by [CGR24], where they obtained the following results regarding condensers:\n\n- When $n \\geq k \\geq \\ell$, there exist functions that can transform a $(g, \\ell, n, k)$-oNOSF source into a uniform $(g-1, \\ell-1, O(k / \\ell))$-oNOSF source (this function can be made explicit with slightly worse dependence on output length).\n- When $n \\geq 2^{\\omega(\\ell)}$, there exists condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m=O(n \\cdot \\ell / g)}$ such that for any uniform $(g, \\ell, n)$-oNOSF source $\\mathbf{X}, H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq \\frac{1}{[\\ell / g]} \\cdot m-O(\\log (n / \\varepsilon))$. Their result is not explicit.\n- It is impossible to condense from uniform $(g, \\ell, n)$-oNOSF sources with output entropy rate more than $\\frac{1}{[\\ell / g]}$.\n\nWe also mention a related family of sources, namely adversarial Chor-Goldreich sources. Uniform oNOSF sources can be seen as a special case of adversarial Chor-Goldreich sources where the good blocks are uniform. Constructing condensers where the output entropy rate is $g / \\ell$ for adversarial Chor-Goldreich sources is already a challenging task, although such condensers in various parameter regimes have been recently constructed [DMOZ23, GLZ24]. The paper of [DMOZ24] recently constructed condensers for a related more general model.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 1.2 New Condenser Constructions \n\nPrevious works only showed the existence of condensers for oNOSF sources when $n \\geq 2^{\\omega(\\ell)}$. We vastly improve on this result in two ways. First, we show that for almost all values of $n, \\ell$, even when $n$ is a small constant, excellent condensers exist. Second, we provide explicit condensers for oNOSF sources when $n \\geq 2^{\\omega(\\ell)}$. We also obtain much better transformation from low-entropy oNOSF sources to uniform oNOSF sources that work even when $k \\ll \\ell$. These results show condensers always exist, except when $n$ is a very small constant (such as $n=1$ ). To further our understanding of this case, we initiate the study of online influence of Boolean functions, a natural generalization of influence that captures the one-sided nature of our online adversary. We also discover surprising connections between condensers for oNOSF sources and protocols for natural problems in distributed computing, such as collective coin flipping and collective sampling. We now discuss our result in details below.", "tables": {}, "images": {}}, {"section_id": 5, "text": "### 1.2.1 Existential Condensers\n\nWe show how to condense from uniform $(g, \\ell, n)$-oNOSF sources for almost all settings of $\\ell$ and $n$ when $g \\geq 0.51 \\ell$. In particular, we show:\n\nTheorem 1 (Informal version of Theorem 4.1). For all $\\ell, \\varepsilon$ where $\\ell \\geq O(\\log (1 / \\varepsilon))$, and $n=10^{4}$, there exists a condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any uniform $(0.51 \\ell, \\ell, n)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq 0.99 m$ where $m=\\Omega(\\ell+\\log (1 / \\varepsilon))$. Furthermore, when $n=$ $\\omega(1)$, the output entropy rate becomes $1-o(1)$.\n\nThis is tight since [CGR24] showed it is impossible to condense uniform $(0.5 \\ell, \\ell, n)$-oNOSF sources beyond output entropy rate 0.5 .\n\nUsing our new results regarding transforming oNOSF sources to uniform oNOSF sources, we also obtain condensers for $(0.51 \\ell, \\ell, n, k)$-oNOSF sources when $n \\geq \\operatorname{poly}(\\log (\\ell))$,\n\nTheorem 2. For all $\\ell, n, \\varepsilon$ where $n=\\operatorname{poly}(\\log (\\ell / \\varepsilon)), k=O(\\log (\\ell / \\varepsilon))$, there exists a condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any $(0.51 \\ell, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-O(m / \\log (m))-O(\\log (1 / \\varepsilon))$ where $m=\\Omega(k)$.\n\nWe can also extend our result to condense from uniform $(g, \\ell, n)$-oNOSF sources for all $g, \\ell$ and constant $n$ where the output entropy rate is $1 / \\lfloor\\ell / g\\rfloor-0.001$. This is tight since [CGR24] showed it is impossible to condense such sources beyond output entropy rate $1 /\\lfloor\\ell / g\\rfloor$.\n\nPreviously, [CGR24] showed how to existentially condense from uniform $(g, \\ell, n)$-oNOSF sources when $g \\geq 0.51 \\ell$, provided $n \\geq 2^{\\omega(\\ell)}$. As $n$ gets smaller, condensing becomes harder since a uniform $(g, \\ell, n)$-oNOSF source is also a uniform $(g \\cdot n / 1000, \\ell \\cdot n / 1000,1000)$-oNOSF source. Hence, we greatly improve the parameters while using different and much simpler techniques.", "tables": {}, "images": {}}, {"section_id": 6, "text": "# 1.2.2 Explicit Condensers \n\nWe construct the first explicit condensers for oNOSF sources. Our explicit condenser construction achieves the same parameters as the existential condenser construction of [CGR24]. We show how to explicitly condense from uniform $(g, \\ell, n)$-oNOSF sources when $n \\geq 2^{\\omega(\\ell)}$ and $g \\geq 0.5 \\ell+1$. We state the results for constant $\\ell$ since that is cleaner:\n\nTheorem 3 (Informal version of Theorem 5.1). For all $n, \\varepsilon$ and constant $\\ell$, there exists an explicit condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any uniform $(0.5 \\ell+1, \\ell)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-O(\\log (m / \\varepsilon))$, where $m=\\Omega(n)$.\n\nJust like earlier, since condensing when $g=0.5 \\ell$ is impossible, this result is also tight. Using our new results regarding transforming oNOSF sources to uniform oNOSF sources, we also obtain explicit condensers for $(0.51 \\ell, \\ell, n, k)$-oNOSF sources for the same parameter regime:\n\nCorollary 1.3 (Corollary 5.2, simplified). For all $\\ell, n, \\varepsilon$ with constant $\\ell$ and $n \\geq O(\\log (1 / \\varepsilon))$, there exists an explicit condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any $(0.5 \\ell+2, \\ell, n, \\operatorname{poly}(\\log n))$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-O(\\log (m / \\varepsilon))$ where $m=\\operatorname{poly}(\\log n)$.\n\nSimilar to earlier, we can also extend our result to explicitly condense from uniform $(g, \\ell, n)$ oNOSF sources in the same parameter regime so that the output entropy rate is $1 /\\lfloor\\ell / g\\rfloor-o(1)$. Just like earlier, this is tight as well.\n\nPreviously, [CGR24] showed how to existentially condense from uniform $(g, \\ell, n)$-oNOSF sources in this parameter regime. However, they relied on the existence of a very strong pseudorandom object: \"output-light\" low-error two-source extractors. Such extractors, even without the outputlightness requirement, are extremely hard to construct and it is a major open problem to obtain\n\nsuch extractors. We are able to make this condenser explicit by building up on their ideas, making interesting observations regarding oNOSF sources, and stitching them together so that the base pseudorandom object we rely on are seeded extractors that we know how to explicitly construct with near optimal parameters.", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 1.2.3 Transforming Low-Entropy oNOSF sources to uniform oNOSF sources \n\nWe show how to existentially, as well as explicitly, with a slight loss in parameters, transform $(g, \\ell, n, k)$-oNOSF sources into uniform $(0.99 g, \\ell-1, n)$-oNOSF sources. Formally, we show:\n\nTheorem 1.4 (Informal version of Theorem 6.1). For all $\\ell, n, k, \\varepsilon$ where $n=\\operatorname{poly}(\\log (\\ell)), k=$ $O(\\log (\\ell / \\varepsilon))$, there exists a function $f$ such that $f$ transforms $(0.51 \\ell, \\ell, n, k)$-oNOSF sources into uniform $(0.509 \\ell, \\ell, m)$-oNOSF sources with error $\\varepsilon$ where $m=\\Omega(k)$.\n\nOur construction can also be made explicit with slightly worse dependence on $m$ and $\\varepsilon$. See Corollary 6.4 for the full tradeoff.\n\nPreviously, [CGR24] provided such a transformation only for $n \\geq k \\geq \\Omega(\\ell)$. Hence, our transformation makes a major improvement on their parameters. Such an improvement allows us to obtain better condensers for low-entropy oNOSF sources in the regime $n=\\operatorname{poly}(\\log (\\ell / \\varepsilon))$ (see Theorem 2).", "tables": {}, "images": {}}, {"section_id": 8, "text": "### 1.3 Limits on Extraction from oNOSF Sources\n\nNext we discuss our results on the limits of extraction from oNOSF sources. Our lower bounds are based on a new notion of influence of functions, namely online influence, that we introduce and analyze. Our upper bound results (explicit extractors) are based on a novel connection to leader election and coin-flipping protocols; to instantiate this connection and give explicit extractors, we construct the necessary protocols.", "tables": {}, "images": {}}, {"section_id": 9, "text": "## Extraction Lower bounds via Online Influence\n\nFor simplicity, let's focus on the case of $n=1$, which leads to interesting new questions about Boolean functions. We refer to such uniform $(g, \\ell, 1)$-oNOSF sources as $(g, \\ell)$-oNOBF sources; oNOBF stands for online non-oblivious bit-fixing sources. We ask what is the exact tradeoff between $g, \\ell$, and $\\varepsilon$ for extracting from oNOBF sources. Towards this, we introduce the notion of online influence.\n\nDefinition 1.5 (Online influence). For a function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, the online influence of the $i$-th bit is\n\n$$\n\\mathbf{o I}_{i}[f]=\\underset{x \\sim \\tilde{\\mathbf{U}}_{i-1}}{\\mathbb{E}}\\left[\\left|\\underset{y \\sim \\tilde{\\mathbf{U}}_{\\ell-i}}{\\mathbb{E}}[f(x, 1, y)]-\\underset{y \\sim \\tilde{\\mathbf{U}}_{\\ell-i}}{\\mathbb{E}}[f(x, 0, y)]\\right|\\right]\n$$\n\nand the total online influence is $\\mathbf{o I}[f]=\\sum_{i=1}^{\\ell} \\mathbf{o I}_{i}[f]$.\nWe believe this is an interesting new measure and is worth studying in its own right. We refer the reader to Example 7.6 for a couple of interesting examples. For monotone functions (and more generally, unate functions), it is not hard to see that online influence equals the usual notion of influence (see Lemma 7.4 for a proof). Thus, to find interesting properties of online influence\n\n(compared to standard influence, Definition 7.1), one must look at non-monotone (in fact, nonunate) Boolean functions.\n\nThe following natural question arises towards our goal of proving extractor lower bounds: for a function $f$, what is the maximum online influence out of all $n$ bits? For the usual notion of influence, this question was resolved by the well-known theorem of KKL88, who showed there always exists a bit with influence at least $\\operatorname{Var}(f) \\cdot \\Omega\\left(\\frac{\\log \\ell}{\\ell}\\right)$.\n\nWe show that surprisingly, there exists a balanced function, namely the address function, where every bit has online influence at most $O\\left(\\frac{1}{\\ell}\\right)$ (see Lemma 7.12 for a proof). This provides a separation between the usual notion of influence and online influence.\n\nWe prove a Poincar\u00e9 style inequality for total online influence, which shows that the above example is tight. For any function $f:\\{0,1\\}^{n} \\rightarrow\\{0,1\\}$, define the function $e(f)(x)=(-1)^{f(x)}$.\n\nTheorem 4 (Theorem 7.5, restated). For any $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, we have $\\operatorname{Var}(e(f)) \\leq \\mathbf{o I}[f] \\leq$ $\\sqrt{\\ell \\operatorname{Var}(e(f))}$.\n\nUsing the above result, we obtain the following extraction lower bound.\nTheorem 1.6 (Informal version of Corollary 7.21). For $\\varepsilon<0.01$, there do not exist extractors for $(0.97 \\ell, \\ell)$-oNOBF sources with error at most $\\varepsilon$.\n\nA similar extraction lower bound was shown in AORSV20 using different techniques.", "tables": {}, "images": {}}, {"section_id": 10, "text": "# Explicit Extractors via Leader Election Protocols \n\nWe now move on to presenting our explicit constructions of extractors for oNOBF and oNOSF sources. The following are our main results.\n\nTheorem 5 (informal version of Theorem 8.2). There exists an explicit function Ext : $\\{0,1\\}^{\\ell} \\rightarrow$ $\\{0,1\\}$ such that for any $(g, \\ell)$-oNOBF source $\\mathbf{X}$ where $g \\geq \\ell-\\ell / C \\log (\\ell)$, we have $\\operatorname{Ext}(\\mathbf{X}) \\approx_{\\varepsilon=1 / 100}$ $\\mathbf{U}_{1}$, where $C$ is a large constant.\n\nTheorem 6 (informal version of Theorem 8.3). There exists an explicit function Ext : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow$ $\\{0,1\\}^{n}$ such that for any $(g, \\ell, n)$-oNOSF source $\\mathbf{X}$ where $g \\geq \\ell-\\ell / C \\log ^{*}(\\ell)$ and $n \\geq \\log (\\ell)$, we have $\\operatorname{Ext}(\\mathbf{X}) \\approx_{\\varepsilon=1 / 100} \\mathbf{U}_{n}$, where $C$ is a large constant.\n\nIt is instructive to contrast our results with the non-online setting (where adversarial bits may depend on any good bit). The non-online versions of oNOSF sources and oNOBF sources are called NOSF sources and NOBF sources. For both these sources, the current best extractors require $g \\geq \\ell-\\frac{\\ell}{(\\log \\ell)^{2}}$, which is much more than what our extractors for Theorem 5 and Theorem 6 require.\n\nWe contrast the results for both settings in Tables 1 and 2. In these tables, we are providing known upper and lower bounds on the value of $b(\\ell)$, defined as the maximum number of bad symbols for which extraction is still possible with a small constant error - so lower bounds correspond to best known constructions of such functions and upper bounds refers to best known limitation of such functions. We write \" $O(\\ell)$ \" to mean \" $c \\ell$ for some small universal constant $c<1$ \".\n\nTo interpret our results in terms of (online) influence of coalitions, it will be useful to extend the definition of online influence to subsets of coordinates.\n\nDefinition 1.7. For any function $f: \\Sigma^{\\ell} \\rightarrow\\{0,1\\}$, and any $B \\subset[\\ell]$, where $B=\\left\\{i_{1}<i_{2}<\\ldots<\\right.$ $\\left.i_{k}\\right\\}$, define $\\mathbf{o l}_{B}(f)$ as follows: an online adversary $\\mathcal{A}$ samples a distribution $\\mathbf{X}$ in online manner. It starts by sampling the variables $x_{1}, x_{2}, \\ldots, x_{i_{1}-1}$ independently and uniformly from $\\Sigma$, then picking the value of $x_{i_{1}}$ depending on $x_{<i_{1}}$. Next, the variables $x_{i_{1}+1}, \\ldots, x_{i_{2}-1}$ are sampled independently and uniformly from $\\Sigma$, and $\\mathcal{A}$ sets the value of $x_{i_{2}}$ based on all set variables so for, and so on. Define the advantage of $\\mathcal{A}$ to be $a d v_{f, B}(\\mathcal{A})=|\\mathbb{E}[f(\\mathbf{X})]-\\mathbb{E}\\left[f\\left(\\mathbf{U}_{\\ell}\\right)\\right]|$. Then $\\mathbf{o l}_{B}(f)$ is defined to be $\\max _{\\mathcal{A}}\\left\\{\\operatorname{adv}_{f, B}(\\mathcal{A})\\right\\}$, where the maximum is taken over all online adversaries $\\mathcal{A}$ that control the bits in $B$.\n\nWe say a function $f$ is $(b, \\varepsilon)$-online-resilient if $\\mathbf{o l}_{B}(f) \\leq \\varepsilon$ for every set $B \\subset[\\ell]$ of size at most $b$.\n\nWe note that Definition 1.7 is a special case of Definition 1.5, for $\\Sigma=\\{0,1\\}$ and $|B|=1$.\nIn Section 7.4, we note that online-resilient functions are equivalent to extractors for uniform oNOSF source sources (with one bit output). Thus, our explicit extractor results immediately imply explicit online-resilient functions.\n\n![table_0](table_0)\n\nTable 1: $b(\\ell)$ bounds in the non-online setting.\n\n![table_1](table_1)\n\nTable 2: $b(\\ell)$ bounds in the online setting.\n\nOur main technique is a generic way to transform leader election and coin flipping protocols (formally defined in Section 3.3) into extractors for oNOBF and oNOSF sources. This is proved in Lemma 8.1; the general idea of constructing an extractor is to simulate an appropriate leader election protocol with the source at hand (oNOBF or oNOSF), and output according to the chosen leader. To instantiate this transformation, we revisit previous leader election protocols in Section 9. Our leader election protocols provide a slightly stronger than usual guarantee: a good player is elected as the leader with probability close to 1 (see Lemma 9.1 and Lemma 9.5). This contrasts with the usual guarantee in leader election protocols, where a good leader is chosen with only a non-trivial (constant) probability.\n\n[^0]\n[^0]:    ${ }^{2}$ Recall that this lower bound is for $(g, \\ell, n)$-oNOSF sources with $n \\geq \\log (\\ell)$.", "tables": {"table_0": "| Source | Lower bound | Upper bound |\n| :-- | :-- | :-- |\n| NOBF | $\\Omega\\left(\\frac{\\ell}{\\log ^{2} \\ell}\\right),[\\mathrm{AL} 93]$ | $O\\left(\\frac{\\ell}{\\log \\ell}\\right),[\\mathrm{KKL} 88]$ |\n| NOSF | $\\Omega\\left(\\frac{\\ell}{\\log ^{2} \\ell}\\right),[\\mathrm{AL} 93]$ | $O(\\ell),[\\mathrm{BKKKL} 92]$ |", "table_1": "| Source | Lower bound | Upper bound |\n| :-- | :-- | :-- |\n| oNOBF | $\\Omega\\left(\\frac{\\ell}{\\log \\ell}\\right),[\\text { Theorem } 8.2]$ | $O(\\ell), \\quad \\text { Corollary } 7.21 \\quad \\text { or }$ |\n| oNOSF | $\\Omega\\left(\\frac{\\ell}{\\log ^{2} \\ell}\\right),[\\text { Theorem } 8.3]^{5}$ | $O(\\ell),[\\text { AORSV20] }$ |"}, "images": {}}, {"section_id": 11, "text": "# 1.4 Application to Collective Coin Flipping and Collective Sampling \n\nWe now discuss applications of our results on condensers for oNOSF sources to fault-tolerant distributed computing. Condensing from oNOSF sources can be viewed as a special case of coin flipping and collective sampling protocols in the full information model that arise in fault-tolerant distributed computing.", "tables": {}, "images": {}}, {"section_id": 12, "text": "### 1.4.1 Background\n\nSay there are $\\ell$ players who have a common broadcast channel and want to jointly perform a task such as collectively flipping a coin. Some $b$ players out of them are \"bad\" and want to deter the task. We assume the bad players are computationally unbounded so cryptographic primitives are of no use. We further assume that each player has private access to uniform randomness. BL89 initiated the study of this model and aptly termed this task as \"collective coin flipping.\"\n\nThe simplest way to collectively flip a coin would be for all the players to initially agree on a function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, then synchronously broadcast one random bit $r_{i}$, and to finally agree on the output being $f\\left(r_{1}, \\ldots, r_{\\ell}\\right)$. However, synchronizing broadcasts is hard, and it could be that the bad players set their output as function of the bits of the good players. KKL88 showed that no function $f$ can handle more than $O\\left(\\frac{\\ell}{\\log \\ell}\\right)$ corruptions.\n\nOne way to allow for more corruptions (almost linear) among players is to consider \"protocols\" that allow more rounds of communication. In particular, a protocol can be thought of as a tree where each vertex represents a \"round\" where in every round the following happens: all good players sends their bits, then all bad players send their bits as a function of the bits of the good players, and they jointly compute a function of these bits. Depending on the outcome of the function, everyone branches on one branch in this tree. Furthermore, every leaf is labeled with final outcomes (say 0 or 1) and, once a leaf is reached, that is the outcome that everybody agrees on. GGL98 initiated the study of protocols where the outcomes are from a larger range and where the bad players are trying to minimize the largest probability of any outcome. They called this problem \"collective sampling.\" For a formal definition, see Section 3.3.", "tables": {}, "images": {}}, {"section_id": 13, "text": "### 1.4.2 Known Results\n\nBL89 showed that for protocols with outcomes $\\{0,1\\}, b$ bad players can always ensure that some outcome occurs with probability at least $\\frac{1}{2}+\\frac{b}{2 b}$. AN93a first constructed a protocol that can handle a linear number of corruptions. Follow-up works tried to reduce the number of rounds in this protocol where, in some settings, players were allowed to send more than one bit per round RZ01, Fei99a.\n[GGL98] showed that for all collective sampling protocols and all outcomes, there exists a way for $b$ bad players to coordinate and ensure that an outcome that happens without corruption with probability $p$, now happens with probability $p^{1-(b / n)} \\geq p\\left(1+\\frac{b}{n} \\log (1 / p)\\right)$. Nearly matching collective sampling protocols were constructed by GGL98, SV08, GVZ06. For an overview of further results and bounds, see Dod06.", "tables": {}, "images": {}}, {"section_id": 14, "text": "### 1.4.3 Connection to oNOSF Sources\n\nThe problem of extracting or condensing from oNOSF sources can be seen as special cases or variants of collective coin flipping and collective sampling that provide very simple protocols. For\n\ninstance, suppose one has an extractor or condenser $f$ for uniform $(g, \\ell, n)$-oNOSF sources. Then, consider a protocol where all $\\ell$ players take turns and output $n$ random bits. The agreed final outcome is $f$ applied on these $\\ell n$ bits. This leads to protocols that are structurally much simpler since players don't have to carefully compute whose turn it is to go in various rounds and can obliviously prepare for their turn.\n\nThe above protocol can also be viewed as a relaxed version of a 1-round protocol where instead of everyone providing their output asynchronously, they take turns and provide outputs one after another in a simple sequential manner.", "tables": {}, "images": {}}, {"section_id": 15, "text": "# 1.4.4 Previous Results Interpreted in oNOSF source context \n\nPrevious impossibility results can be interpreted in the context of extracting / condensing from uniform oNOSF sources. For instance, collective coin flipping impossibility results of [BL89] imply extraction impossibility results for uniform $(g, \\ell, n)$-oNOSF sources when $n=1$. They imply:\n\nCorollary 1.8. There does not exist an $\\frac{h}{2 \\ell}$-extractor for uniform $(g, \\ell, 1)$-oNOSF sources.\nSimilarly, we observe that the notion of collective sampling is equivalent to 0 -error condensing. Hence, lower bounds of [GGL98] imply zero-error condensing lower bounds for uniform $(g, \\ell, n)$ oNOSF sources when $n=1$. Formally:\n\nCorollary 1.9. There does not exist a condenser Cond : $\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}^{m}$ for uniform $(g, \\ell, 1)$ oNOSF sources that can guarantee output smooth min-entropy (with parameter $\\varepsilon=0$ ) more than $k=\\frac{g}{\\ell} \\cdot m$.", "tables": {}, "images": {}}, {"section_id": 16, "text": "### 1.4.5 $\\varepsilon$-Collective Sampling\n\nSince collective sampling lower bounds show that for any protocol, 0 -error condensing beyond rate $g / \\ell$ is impossible, one can naturally ask whether condensing with small error $\\varepsilon$ is possible. We call this problem $\\varepsilon$-collective sampling, where the goal is to output a distribution which is $\\varepsilon$-close to a distribution where every output has small probability.\n\nInterpreted this way, this is exactly what protocols arising out of our condensers for uniform oNOSF sources provide: Using Theorem 1, when each player has access to $10^{4}$ random bits, there exists a simple protocol that can handle $0.49 \\ell$ corrupt players such that the players can collectively sample a distribution over $m=O(\\ell)$ bits which is $2^{-\\Omega(\\ell)}$-close to having entropy 0.99 m . As far as we are aware, such a protocol is not implied by any other previous protocol. Most previous protocols are obtained through leader election protocols, which do not seem useful here since the leader has access to only constant number of bits.\n\nWe similarly obtain explicit protocols using Theorem 3 for the case when each player has access to $n \\geq 2^{\\omega(\\ell)}$ bits.", "tables": {}, "images": {}}, {"section_id": 17, "text": "### 1.4.6 Collective Coin Flipping and Sampling with Weak Random Sources\n\nA natural extension to collective coin flipping and sampling in the full information model is when all players only have access to weak source of randomness (that are independent from each other) instead of true uniform randomness. This question was first studied by [GSV05]. [KLRZ08] used network extractor protocol to transform weak random sources of each player into independent private random sources. This way, after using the network extraction protocol, players can follow\n\nthe usual collective coin flipping / sampling protocol. [GSZ21] improved the network extraction protocol using two-source non-malleable extractors.\n\nUsing our $(g, \\ell, n, k)$-oNOSF source condensers, we obtain alternative, simple $\\varepsilon$-collective sampling protocols in the setting where players have access to weak sources of randomness. We obtain such an existential protocol using Theorem 2, and explicit protocol using Corollary 1.3.", "tables": {}, "images": {}}, {"section_id": 18, "text": "# 2 Proof Overview \n\nOur proof overview begins by outlining our new existential results for condensers in Section 2.1 that is able to handle even constant block length. Next, we present our explicit condenser results in Section 2.2 before discussing of our low-entropy to uniform oNOSF source conversion in Section 2.3. We present the main ideas behind our results regarding online influence and extractor lower bounds in Section 2.4. In Section 2.5, we overview our extractor constructions for oNOBF and oNOSF sources, that is based on a general transformation from leader election protocols.", "tables": {}, "images": {}}, {"section_id": 19, "text": "### 2.1 Existence of oNOSF Condensers for All $\\ell$ and $n$\n\nHere we sketch the proof of Theorem 1. This result states that when $g=0.51 \\ell$ and $n=1000$, there exists a condenser Cond for uniform $(g, \\ell, n)$-oNOSF sources so that the output entropy rate is 0.99 , the number of output bits is $m=O(\\ell+\\log (1 / \\varepsilon))$, and the error of the condenser is $\\varepsilon$ where $\\varepsilon \\leq 2^{-\\Omega(\\ell)}$ is arbitrary.\n\nOur construction uses amazing seeded condensers (see Definition 3.3) with $1 \\cdot \\log (1 / \\varepsilon)$ dependence on seed length. We slightly modify our source and then apply such seeded condenser. Here is a proof sketch:\n\nProof sketch for Theorem 1. Let $\\mathbf{X}=\\left(\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{\\ell}\\right)$ be such a source. Let $\\mathbf{Y}_{1} \\sim\\left(\\{0,1\\}^{n}\\right)^{0.5 \\ell}$ be the source obtained by concatenating the first $0.5 \\ell$ blocks of $\\mathbf{X}$. Since $0.51 \\ell$ blocks are good, there exist at least $0.01 \\ell$ uniform blocks in $\\mathbf{Y}_{1}$. We treat $\\mathbf{Y}_{1}$ as a single distribution over $n \\ell$ bits with min-entropy $\\geq 0.01 \\ell n$.\n\nLet $\\mathbf{Y}_{2} \\sim\\{0,1\\}^{0.5 \\ell}$ be the source obtained by concatenating 1 bit from each of the last $0.5 \\ell$ blocks of $\\mathbf{X}$. Once again, since $0.51 \\ell$ blocks are good, there exist at least $0.01 \\ell$ uniform bits in $\\mathbf{Y}_{1}$.\n\nWe will use the following seeded condenser:\nTheorem 2.1 (Theorem 4.8, simplified). For all $d, \\varepsilon$ such that $d \\geq \\log (\\ell n / \\varepsilon)+O(1)$, there exists a seeded condenser sCond : $\\{0,1\\}^{0.5 \\ell n} \\times\\{0,1\\}^{d} \\rightarrow\\{0,1\\}^{m}$ s.t. for all $\\mathbf{X} \\sim\\{0,1\\}^{0.5 \\ell n}$ with $H_{\\infty}(\\mathbf{X}) \\geq$ $0.01 \\ell n$, we have $\\left.H_{\\infty}^{\\varepsilon}\\left(\\operatorname{sCond}\\left(\\mathbf{X}, \\mathbf{U}_{d}\\right)\\right) \\geq 0.01 \\ell n+d\\right.$ where $m=0.01 \\ell n+d+\\log (1 / \\varepsilon)+O(1)$.\n\nOur condenser Cond will output sCond $\\left(\\mathbf{Y}_{1}, \\mathbf{Y}_{2}\\right)$. Observe that not only is $\\mathbf{Y}_{2}$ not uniform, there could be as many as $0.49 \\ell$ \"bad bits\" in $\\mathbf{Y}_{2}$ that can depend on $\\mathbf{Y}_{1}$. To remedy this, we use the well known fact that the behavior of such adversarial $\\mathbf{Y}_{2}$ cannot be far worse than the behavior if $\\mathbf{Y}_{2}$ were uniform. In particular, say if $\\mathbf{Y}_{2}$ were uniform then the output entropy and error are $k$ and $\\varepsilon$ respectively. Then for the actual $\\mathbf{Y}_{2}$, the output entropy will be $k-0.49 \\ell$ and error will be $\\varepsilon \\cdot 2^{0.49 \\ell}$. See Lemma 4.9 for the formal statement.\n\nFor us, it means the following: let $\\varepsilon_{\\text {sCond }}, k_{\\text {sCond }}$ be such that $H_{\\infty}^{\\varepsilon_{\\text {sCond }}}\\left(\\operatorname{sCond}\\left(\\mathbf{Y}_{1}, \\mathbf{U}_{0.5 \\ell}\\right)\\right) \\geq k_{\\text {sCond }}$. Then, it must be that $H_{\\infty}^{2^{0.49 \\ell} \\cdot \\varepsilon_{\\text {sCond }}}\\left(\\operatorname{sCond}\\left(\\mathbf{Y}_{1}, \\mathbf{Y}_{2}\\right)\\right) \\geq k_{\\text {sCond }}-0.49 \\ell$. So, for our final error to be some $\\varepsilon$, we need to have $\\varepsilon_{\\text {sCond }}=\\varepsilon \\cdot 2^{-0.49 \\ell}$. For seeded condensers to exist, we need $0.5 \\ell \\geq$ $\\log \\left(\\ell n / \\varepsilon_{\\text {sCond }}\\right)+O(1)$ and we check that such an inequality can indeed be satisfied if $\\varepsilon \\geq 2^{-0.01 \\ell}$.\n\nHence, we finally obtain that our seeded condenser will output $0.01 \\ell n+O(\\ell)$ bits and will have output entropy $m-\\Delta$ where $\\Delta=O(\\ell)$. Hence, if $n$ is a large enough constant, our output entropy rate, $\\frac{m-\\Delta}{m}$, will be $\\geq 0.99$ as desired.\nRemark 2.2. Here (in the inequality $0.5 \\ell \\geq 1 \\cdot \\log \\left(\\ell n / \\varepsilon_{\\text {sCond }}\\right)$ ) we crucially used the fact that there exist seeded condensers with seed length dependence $1 \\cdot \\log (1 / \\varepsilon)$. Currently, we do not have explicit constructions with this dependence. We also couldn't have used a seeded extractor since for them, the seed length dependence is $2 \\cdot \\log (1 / \\varepsilon)$. For that to work, we would need to assume $g \\geq 0.76 \\ell$.", "tables": {}, "images": {}}, {"section_id": 20, "text": "# 2.2 Explicit Condensers for Uniform oNOSF Sources \n\nHere we sketch the proof of Theorem 3: we construct explicit condensers for uniform $(0.5 \\ell+1, \\ell, n)$ oNOSF sources where $\\ell$ is a constant and $n$ is arbitrarily growing.\n\nOur construction will be similar to that of the existential construction of condensers from [CGR24]. We will use the online nature of these sources to make more observations that will allow us to obtain an explicit construction of such condensers using explicit seeded extractors (see Definition 3.4 for a definition) as our primitive:\n\nProof sketch for Theorem 3. Let $\\mathbf{X}$ be such a source. Let's review the [CGR24] construction at a high level: they take the first $\\lceil\\ell / 2\\rceil$ blocks and treat them as a single entity. From the remaining blocks, they take first few bits of each of the blocks with the number of such bits geometrically decreasing per block and concatenate them to obtain a second entity. They then pass these two sources to an \"output-light\" two-source extractor to obtain their final output.\n\nWe first split each block in $\\mathbf{X}$ into two parts of equal sizes. The resultant source is uniform $(\\ell+1,2 \\ell, n / 2)$-oNOSF source. We call this source $\\mathbf{X}$ as well since we are just re-interpreting $\\mathbf{X}$ as this source. This simple trick turns out to be very useful since it allows us to only focus on the situation where the number of blocks is an even number.\nRemark 2.3. The construction of [CGR24] had to introduce the notion of \"output-lightness\" to deal with the case of odd number of blocks. For instance, say $\\ell=5$. Then, their first entity is obtained by concatenating the first 3 blocks and second entity by taking careful number of bits from the remaining 2 blocks. To handle scenarios where the first 3 blocks were uniform and last 2 were bad, the output-lightness property was imposed on two-source extractors, something which we do not know how to explicitly construct.\n\nJust reducing to even cases is not enough since the construction of [CGR24] required low-error two-source extractors with excellent parameters, and we do not know how to explicitly construct them. We bypass this requirement by further exploiting the fact that our adversary is online.\n\nLet $\\mathbf{W} \\sim\\left(\\{0,1\\}^{n}\\right)^{\\ell}$ be the concatenation of the first $\\ell$ blocks of $\\mathbf{X}$. Let $\\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{\\ell}$ be the sources obtained by carefully choosing the first few bits from each of the blocks $\\mathbf{X}_{\\ell+1}, \\ldots, \\mathbf{X}_{2 \\ell}$. Our final construction will be the parity of the outputs of seeded extractors applied with source $\\mathbf{W}$ and seeds $\\mathbf{Y}_{i}$. More formally, we output\n\n$$\n\\bigoplus_{i=1}^{\\ell} \\operatorname{sExt}_{i}\\left(\\mathbf{W}, \\mathbf{Y}_{i}\\right)\n$$\n\nwhere sExt ${ }_{i}$ is any explicit near optimal seeded extractor (such as the extractor from Theorem 3.5).\n\nSince the number of blocks, $2 \\ell$, is even and number of good blocks is $\\ell+1$, both $\\mathbf{W}$ and $\\mathbf{Y}$ will obtain some bits from a good block. This means, $H_{\\infty}(\\mathbf{W}) \\geq n$ and that there exists $j \\in[\\ell]$ such that $\\mathbf{Y}_{j}$ is uniform. We now condition on fixing blocks $\\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{j-1}$. Since these blocks can depend on $\\mathbf{W}, \\mathbf{W}$ will lose some small amounts of entropy (the amount will be very small since these blocks are tiny compared to the amount entropy in $\\mathbf{W}$ ). Moreover, since the adversary is online, $\\mathbf{Y}_{j}$ remains uniform even after doing this conditioning. We now view our construction as\n\n$$\ng(\\mathbf{W}) \\oplus \\bigoplus_{i=j}^{\\ell} \\operatorname{sExt}_{i}\\left(\\mathbf{W}, \\mathbf{Y}_{i}\\right)\n$$\n\nwhere $g$ is the fixed function obtained by fixing $\\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{j-1}$.\nWe now compare two scenarios: (1) Where all $\\mathbf{Y}_{j}, \\ldots, \\mathbf{Y}_{\\ell}$ are uniform (2) Only $\\mathbf{Y}_{j}$ is uniform and $\\mathbf{Y}_{j+1}, \\ldots, \\mathbf{Y}_{\\ell}$ are arbitrarily controlled by an adversary and can even depend on $\\mathbf{W}$ :\n\nIn the first scenario, we further condition on fixing $\\mathbf{Y}_{j+1}, \\ldots, \\mathbf{Y}_{\\ell}$. Since in this scenario these are independent and random, $\\mathbf{W}$ retains the same entropy and $\\mathbf{Y}_{j}$ remains uniform. So our overall output is of the form $h(\\mathbf{W}) \\oplus \\operatorname{sExt}_{j}\\left(\\mathbf{W}, \\mathbf{Y}_{j}\\right)$ for some fixed function $h$. We condition on fixing output $h(\\mathbf{W})$. Since $m \\ll H_{\\infty}(\\mathbf{W})$, we infer that $\\mathbf{W}$ still has lots of entropy when we do this fixing. So, the output is just $z \\oplus \\operatorname{sExt}_{j}\\left(\\mathbf{W}, \\mathbf{Y}_{j}\\right)$ where $z$ is a fixed string, and hence the output distribution is uniform.\n\nThe second scenario is more realistic and, in the worst case, this is what can actually happen. We then use the result that if an adversary controls few bits in the input distribution, then they cannot make the output of the condenser too bad (see Lemma 4.9 for full statement). With this, and by carefully choosing geometrically decreasing lengths of $\\mathbf{Y}_{i}$ to help control the error, we indeed obtain that the output will be condensed.", "tables": {}, "images": {}}, {"section_id": 21, "text": "# 2.3 Converting Low-Entropy oNOSF Sources to Uniform oNOSF Sources \n\nThey key part of our proof for condensing from low-entropy oNOSF sources is a transformation from low-entropy oNOSF sources to uniform oNOSF sources. Here, we sketch the proof for our transformation in Theorem 1.4 and compare it to that of [CGR24]. Both these transformations rely on two-source extractors (see Definition 3.6 for definition) as a basic primitive.\n\nGiven a $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}=\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{\\ell}$, [CGR24] uses excellent existential twosource extractors (such as from Lemma 6.6) to define output blocks $\\mathbf{O}_{i}=2 \\operatorname{Ext}\\left(\\mathbf{X}_{1} \\circ \\cdots \\circ \\mathbf{X}_{i-1}, \\mathbf{X}_{i}\\right)$ for $i \\in\\{2, \\ldots, \\ell\\}$ and define their transformation as $f(\\mathbf{X})=\\mathbf{O}_{2}, \\ldots, \\mathbf{O}_{\\ell}$. They show that $\\mathbf{O}_{i}$ is a good block if: (1) $\\mathbf{X}_{i}$ is a good block and (2) at least one block amongst $\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{i-1}$ is a good block. They showed that such a good block will be uniform and independent of the blocks $\\mathbf{O}_{2}, \\ldots, \\mathbf{O}_{i-1}$ and argued there will be $g-1$ such good output blocks. This indeed shows their output is a uniform $(g-1, \\ell-1, m)$-oNOSF source. However, each of their output blocks has length $m=O\\left(\\frac{k}{\\ell}\\right) \\leq O\\left(\\frac{n}{\\ell}\\right)$, and so they were not able to handle the case of $n=o(\\ell)$. We improve on their construction by using a \"sliding window\" based technique to obtain a much better transformation that can even handle $n=\\operatorname{poly}(\\log (\\ell))$.\n\nTheorem 2.4 (Theorem 6.1 restated). Let $d, g, g_{\\text {out }}, \\ell, n, m, k, \\varepsilon$ be such that $g_{\\text {out }} \\leq g-\\frac{\\ell-g+2}{2}, n \\geq$ $k \\geq \\log (n d-k)+m d+2 \\log \\left(2 g_{\\text {out }} / \\varepsilon\\right)$. Then, there exists a function $f:\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\left(\\{0,1\\}^{m}\\right)^{\\ell-1}$ such that for any $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, there exists uniform $\\left(g_{\\text {out }}, \\ell-1, m\\right)$-oNOSF source $\\mathbf{Y}$ for which $|f(\\mathbf{X})-\\mathbf{Y}| \\leq \\varepsilon$.\n\nThe parameter $d$ in our theorem statement above is the width of our sliding window. When we set $d=\\ell$ we recover the analysis of [CGR24]. The true advantage of our transformation emerges when $d$ is very small compared to $\\ell$. For instance, when $g=0.51 \\ell, n=\\operatorname{poly}(\\log (\\ell))$ and $k=\\operatorname{poly}(\\log (\\ell))$, we set $d$ to be a large constant and conclude that the output distribution is a uniform $(0.509 \\ell, \\ell, \\operatorname{poly}(\\log (\\ell))$-NOSF source.\n\nProof sketch of Theorem 2.4. Define $\\mathbf{O}_{i}=2 \\operatorname{Ext}\\left(\\mathbf{X}_{i-d} \\circ \\cdots \\circ \\mathbf{X}_{i-1}, \\mathbf{X}_{i}\\right)$. We call $\\mathbf{O}_{i}$ to be a good output block when $\\mathbf{X}_{i}$ is good and there's at least one good block amongst $\\left\\{\\mathbf{X}_{i-d}, \\ldots, \\mathbf{X}_{i-1}\\right\\}$.\n\nWe first compute the number of good output blocks $g_{\\text {out }}$. Let $j_{1}, \\ldots, j_{g}$ be the indices of the good input blocks in $\\mathbf{X}$ and $d_{i}=j_{i+1}-j_{i}$ be the gap between the $i$-th good block and the next $(i+1)$-th good block. If the gap $d_{i}$ is at most $d$, then $\\mathbf{O}_{i+1}$ must be a good output block. So, $g_{\\text {out }}$ is the number of $i$ such that $d_{i} \\leq d$. Since $g \\geq 0.51 \\ell$, such large gaps can't appear too often and we can calculate that $g_{\\text {out }}=g-\\frac{\\ell-g+2}{d}$ as desired.\n\nNext, we show that the good output blocks are indeed uniform conditioned on all previous output blocks. With this, we will obtain that the output distribution will be uniform $\\left(g_{\\text {out }}, \\ell-1, m\\right)$ oNOSF source as desired. Let $i$ be the index of a good output block. We want to show that $\\mathbf{O}_{i}$ is uniform conditioned on $\\mathbf{O}_{1}, \\ldots, \\mathbf{O}_{i-1}$. To do this, we first observe that any input block contributes to at most $d+1$ good output blocks. This means that $\\left(\\mathbf{X}_{i-d} \\circ \\cdots \\circ \\mathbf{X}_{i-1}\\right)$, which has min-entropy at least $k$, loses at most $d \\cdot m$ min-entropy conditioned on fixing $\\mathbf{O}_{1}, \\ldots, \\mathbf{O}_{i-1}$. Moreover, $\\mathbf{X}_{i}$ still remains uniform and independent of $\\left(\\mathbf{X}_{i-d} \\circ \\cdots \\circ \\mathbf{X}_{i-1}\\right)$ when fixing these previous output blocks. Hence, the output of the two-source extractor will indeed be uniform as desired.\n\nWe can make Theorem 2.4 explicit by using the explicit two-source extractors of Theorem 6.7 at a slight cost of dependence on $m$ and $\\varepsilon$ as seen in Corollary 6.4.", "tables": {}, "images": {}}, {"section_id": 22, "text": "# 2.4 Online Influence and Extractor Lower Bounds \n\nIn this subsection, we provide a brief overview of our results regarding online influence and sketch how they imply extractor lower bounds against oNOBF sources. We also contrast online with the established notion of influence for Boolean functions. For any function $f:\\{0,1\\}^{n} \\rightarrow\\{0,1\\}$, define the function $e(f)(x)=(-1)^{f(x)}$.\n\nA Poincar\u00e9 inequality and extractor lower bounds One fundamental inequality about regular influence is the Poincar\u00e9 inequality which states that $\\operatorname{Var}(f) \\leq \\mathbf{I}[f]$. We prove a similar result for online influence.\n\nTheorem 2.5 (Theorem 7.5 restated). For any $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, we have $\\operatorname{Var}(e(f)) \\leq \\mathbf{o I}[f] \\leq$ $\\sqrt{\\ell \\operatorname{Var}(e(f))}$\n\nIt is not hard to derive extractor lower bounds for oNOBF sources from the above result. The high level idea is to collect bits with high online influence, which is guaranteed by the first inequality in the above theorem (using an averaging argument) to form a coalition of coordinates that has enough online influence to bias the claimed extractor. We refer the reader to Theorem 7.19 for more details.\n\nThe proof of Theorem 2.5 is based on techniques from the Fourier analysis of Boolean functions. ${ }^{6}$ The following key result implies Theorem 2.5 in a straightforward way.\n\n[^0]\n[^0]:    ${ }^{6}$ We give a very brief recap of necessary notions from Fourier analysis of Boolean functions in Section 7.2.\n\nLemma 2.6 (Lemma 7.7 restated). For any $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$ and $i \\in[\\ell], \\mathbf{o I}_{i}(f)^{2} \\leq$ $\\sum_{S \\subseteq[i]} \\widehat{f}(S)^{2} \\leq \\mathbf{o I}_{i}(f)$.\n\nThe above bound is established using the following Fourier analytic characterization of online influence.\n\nClaim 2.7 (Claim 7.8 restated). For any $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, we can write the online influence of its $i$-th bit as\n\n$$\n\\mathbf{o I}_{i}[f]=\\sum_{x \\sim \\mathbf{U}_{i-1}}\\left[\\left\\lceil\\sum_{\\substack{T \\subseteq[i] \\\\ T \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)\\right\\rceil\\right]\n$$\n\nThe proof of the above result is mostly a standard Fourier analytic computation and we refer the reader to Section 7 for more details.\n\nInfluence vs Online Influence It is not hard to see that $\\mathbf{o I}_{i}[f] \\leq \\mathbf{I}_{i}[f]$ for all $i \\in[\\ell]$, with equality always holding for $i=\\ell$ as an adversarial online bit in the last index can see every good bit. Moreover, we observe that for monotone functions, the notion of online influence is equivalent to regular influence, so any separation between the two notions must come from non-monotone functions.\n\nWe exactly exhibit such a separation via the non-monotone address function $\\operatorname{Addr}_{\\ell}$ : $\\{0,1\\}^{\\log \\ell+\\ell} \\rightarrow\\{0,1\\}$ which considers its first $\\log \\ell$ bits as an index in $\\{1, \\ldots, \\ell\\}$ and then outputs the value of the chosen index. It is easy to show (as we do in Lemma 7.12) that the first $\\log \\ell$ bits of $\\operatorname{Addr}_{\\ell}$ have no online influence, while the remaining bits have online influence of $O\\left(\\frac{1}{\\ell}\\right)$. This is in contrast to the well known result of KKL88 showing that, for a balanced function such as $\\operatorname{Addr}_{\\ell}$, there must exist a bit with influence at least $\\Omega\\left(\\frac{\\log \\ell}{\\ell}\\right)$.", "tables": {}, "images": {}}, {"section_id": 23, "text": "# 2.5 Extractors via Leader Election Protocols \n\nWe sketch our main idea for constructing an extractor for oNOBF sources (Theorem 8.2). Similar ideas work more generally for extracting from oNOSF sources (Theorem 8.3). As mentioned above, we use a novel connection to leader election protocols to construct extractors. We refer the reader to Section 3.3 for a quick recap of the leader election protocols.\n\nSuppose $\\pi$ is an $(r-1)$-round leader election protocol over $\\ell$ players where in each round, each player sends 1 bit and with the guarantee that if there are at most $\\delta \\ell$ bad players, then a good player is chosen as leader with probability $1-\\epsilon$. Suppose $\\mathbf{X}$ is an $(g, \\ell r)$-oNOBF source, where $g \\geq l r-\\delta \\ell$. We simply partition the bits of $\\mathbf{X}$ into chunks $\\mathbf{X}_{1}, \\mathbf{X}_{2}, \\ldots, \\mathbf{X}_{r}$, where each $X_{i}$ is on $\\ell$ bits, and simulate the protocol $\\pi$ by using the $j$ 'th bit of $\\mathbf{X}_{i}$ as the message of the $j$ 'th player in round $i$, for all $1 \\leq j \\leq \\ell$ and $1 \\leq i \\leq r-1$. At the end of this simulation suppose $j^{*} \\in[\\ell]$ is the chosen leader. Then we output the $j^{*}$ 'th bit of $\\mathbf{X}_{r}$ as the output of the extractor.\n\nBriefly, the reason that the above is a valid simulation of $\\pi$ is the fact that the value of any bad bit in this online setting just depends on bits that appear before it, which is allowed in the leader election protocol (where in round $i$, the message of a bad player can be any function of the messages in the same round or previous rounds). The correctness of the extractor now follows from the fact that since the number of bad players (i.e., bad bits in $\\mathbf{X}$ ) is at most $\\delta \\ell$, it follows from the\n\nguarantee of the protocol that the chosen leader $j^{*} \\in[\\ell]$ is a good player with probability at least $1-\\epsilon$, and in this case the $j^{*}$ 'th bit of $\\mathbf{X}_{r}$ must be uniform.\n\nWe note here that in the usual definition of leader election protocols, the requirement is to select a good leader with constant probability, which is a weaker guarantee than what we need to instantiate the above plan. It turns out that we can combine leader election protocols from prior works, in particular from [Fei99a] and [AN93b], to construct protocols with the stronger guarantee we require. We refer the reader to Section 9 for more details on the construction of our leader election protocols.", "tables": {}, "images": {}}, {"section_id": 24, "text": "# 2.6 Organization \n\nIn the remainder of our paper, we give some preliminaries in Section 3 before moving on to our core results. Section 4 details our proofs for the existence of seedless condensers for oNOSF sources for all regimes of $\\ell$ and $n$, while Section 5 provides proofs for our explicit constructions of condensers. Next, Section 6 shows how to handle converting low-entropy oNOSF sources to uniform oNOSF source for a broader range of parameters. In Section 7 we introduce the notion of online influence and use it to provide an extraction lower bound for oNOBF sources. In Section 8, we present our explicit constructions of extractors for oNOBF and oNOSF sources using a connection to leader election protocols. In Section 9, we explicitly construct the required leader election protocols. We discuss some open questions in Section 10.\n\nIn Appendix A, we consider a natural local variant of oNOSF sources and show that it is straightforward to extract from such sources using existing extractors for small-space sources.", "tables": {}, "images": {}}, {"section_id": 25, "text": "## 3 Preliminaries\n\nIn this section we give some basic background and facts used throughout our paper. We use boldfaced font to indicate a random variable such as $\\mathbf{X}$. Often we will use o or, to indicate concatenation of blocks. So if $\\mathbf{X}_{1} \\sim\\{0,1\\}^{n}$ and $\\mathbf{X}_{2} \\sim\\{0,1\\}^{n}$, then $\\mathbf{X}_{1}, \\mathbf{X}_{2}$ will be the concatenated random variable over $\\{0,1\\}^{2 n}$. We will use the notation $[n]$ as shorthand for $\\{1, \\ldots, n\\}$. All logs in this paper will have base 2 unless stated otherwise.", "tables": {}, "images": {}}, {"section_id": 26, "text": "### 3.1 Basic Probability Notions\n\nWe measure the distance between two distributions via statistical distance:\nDefinition 3.1 (Statistical Distance). For any two distributions $\\mathbf{X}, \\mathbf{Y}$ over $\\Omega$, we define the statistical distance or total-variation distance (TV) distance as:\n\n$$\n|\\mathbf{X}-\\mathbf{Y}|=\\max _{S \\subset \\Omega}|\\operatorname{Pr}[\\mathbf{X} \\in S]-\\operatorname{Pr}[\\mathbf{Y} \\in S]|=\\frac{1}{2} \\sum_{s \\in \\Omega}|\\operatorname{Pr}[\\mathbf{X}=s]-\\operatorname{Pr}[\\mathbf{Y}=s]|\n$$\n\nWe use the notation $\\mathbf{X} \\approx_{\\varepsilon} \\mathbf{Y}$ to denote the fact that $|\\mathbf{X}-\\mathbf{Y}| \\leq \\varepsilon$.\nWe will utilize the very useful min-entropy chain rule in our constructions.\nLemma 3.2 (Min-entropy chain rule, [MW97]). For any random variables $\\mathbf{X} \\sim X$ and $\\mathbf{Y} \\sim Y$ and $\\varepsilon>0$,\n\n$$\n\\operatorname{Pr}_{y \\sim \\mathbf{Y}}\\left[H_{\\infty}(\\mathbf{X} \\mid \\mathbf{Y}=y) \\geq H_{\\infty}(\\mathbf{X})-\\log |\\operatorname{Supp}(\\mathbf{Y})|-\\log (1 / \\varepsilon)\\right] \\geq 1-\\varepsilon\n$$", "tables": {}, "images": {}}, {"section_id": 27, "text": "# 3.2 Condensers and Extractors \n\nWe recall the definition of a seeded condenser.\nDefinition 3.3. $A\\left(k_{\\text {in }}, k_{\\text {out }}, \\varepsilon\\right)$-seeded condenser sCond : $\\{0,1\\}^{n} \\times\\{0,1\\}^{d} \\rightarrow\\{0,1\\}^{m}$ satisfies the following: for every source $\\mathbf{X} \\sim\\{0,1\\}^{n}$ with $H_{\\infty}(\\mathbf{X}) \\geq k_{\\text {in }}$, and $\\mathbf{Y}=\\mathbf{U}_{d}$,\n\n$$\nH_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X}, \\mathbf{Y})) \\geq k_{\\text {out }}\n$$\n\nHere, $d$ is called the seed length of sCond.\nSeeded extractor is the special case of seeded condenser where $k_{\\text {out }}=m$. We here record the full definition for completeness sake:\n\nDefinition 3.4. $A(k, \\varepsilon)$-seeded extractor sExt : $\\{0,1\\}^{n} \\times\\{0,1\\}^{d} \\rightarrow\\{0,1\\}^{m}$ satisfies the following: for every source $\\mathbf{X} \\sim\\{0,1\\}^{n}$ with $H_{\\infty}(\\mathbf{X}) \\geq k$, and $\\mathbf{Y}=\\mathbf{U}_{d}$,\n\n$$\n\\operatorname{sExt}(\\mathbf{X}, \\mathbf{Y}) \\approx_{\\varepsilon} \\mathbf{U}_{m}\n$$\n\nHere, $d$ is called the seed length of sExt. sExt is called strong if\n\n$$\n\\operatorname{sExt}(\\mathbf{X}, \\mathbf{Y}), \\mathbf{Y} \\approx_{\\varepsilon} \\mathbf{U}_{m}, \\mathbf{Y}\n$$\n\nWe will use the following near optimal explicit construction of seeded extractors:\nTheorem 3.5 (Theorem 1.5 in [GUV09]). For all constant $0<\\alpha<1$, there exists a constant $C$ such that for all $n, k, \\varepsilon$, there exists an explicit $(k, \\varepsilon)$-seeded extractor sExt : $\\{0,1\\}^{n} \\times\\{0,1\\}^{d} \\rightarrow$ $\\{0,1\\}^{m}$ with $d=C \\log (n / \\varepsilon)$ and $m \\geq(1-\\alpha) k$.\n\nNext, we recall the definition of two-source extractors.\nDefinition 3.6. A function 2Ext : $\\{0,1\\}^{n_{1}} \\times\\{0,1\\}^{n_{2}} \\rightarrow\\{0,1\\}^{m}$ is a $\\left(k_{1}, k_{2}, \\varepsilon\\right)$-two-source extractor if for every source $\\mathbf{X}_{1} \\sim\\{0,1\\}^{n_{1}}$ with $H_{\\infty}\\left(\\mathbf{X}_{1}\\right) \\geq k_{1}$ and $\\mathbf{X}_{2} \\sim\\{0,1\\}^{n_{2}}$ with $H_{\\infty}\\left(\\mathbf{X}_{2}\\right) \\geq k_{2}$ where $\\mathbf{X}_{1}$ and $\\mathbf{X}_{2}$ are independent of each other, we have\n\n$$\n2 \\operatorname{Ext}\\left(\\mathbf{X}_{1}, \\mathbf{X}_{2}\\right) \\approx_{\\varepsilon} \\mathbf{U}_{m}\n$$\n\nIt is said to be strong in the first argument if\n\n$$\n2 \\operatorname{Ext}\\left(\\mathbf{X}_{1}, \\mathbf{X}_{2}\\right), \\mathbf{X}_{1} \\approx_{\\varepsilon} \\mathbf{U}_{m}, \\mathbf{X}_{1}\n$$", "tables": {}, "images": {}}, {"section_id": 28, "text": "### 3.3 Leader Election, Collective Coin Flipping and Sampling Protocols\n\nWe formalize the definition of protocols in the full information model. Collective coin flipping protocols, leader election protocols, and collective sampling protocols are special cases of such protocols where the output domain is $[\\ell]$ and $\\{0,1\\}$ and $\\{0,1\\}^{m}$ for some $m$ respectively.\n\nDefinition 3.7 (Protocol in the full information model). $A k$-round protocol with output domain $Y$ over $\\ell$ players where each player sends $n$ random bits per round is a function\n\n$$\n\\pi:\\left(\\left(\\{0,1\\}^{n}\\right)^{\\ell}\\right)^{k} \\rightarrow Y\n$$\n\nthat takes in the input of each of the players during each round and outputs an element from set $Y$ which is the outcome of the protocol.\n\nHere is how the protocol operates in the presence of a set $B \\subset[\\ell]$ of bad players: In round $i$, each of the players from $[\\ell] \\backslash B$ independently output a uniformly random element from $\\{0,1\\}^{n}$. Let their collective outputs be $\\alpha_{i} \\in\\left(\\{0,1\\}^{n}\\right)^{[\\ell] \\backslash B}$. Then, depending on $\\alpha_{1}, \\ldots, \\alpha_{i}$, the players in $B$ together output an element of $\\left(\\{0,1\\}^{n}\\right)^{B}$. Hence, we model the strategy of the bad players as a sequence of functions $\\sigma=\\left(\\sigma_{1}, \\ldots, \\sigma_{k}\\right)$, where\n\n$$\n\\sigma_{i}:\\left(\\left(\\{0,1\\}^{n}\\right)^{[\\ell] \\backslash B}\\right)^{i} \\rightarrow\\left(\\{0,1\\}^{n}\\right)^{B}\n$$\n\nwhere $\\sigma_{i}$ takes in the inputs of the good players from the first $i$ rounds and maps it to the output of the bad players for round $i$. For a fixed strategy $\\sigma$, the outcome of the protocol can be modeled as follows: uniform random strings $\\alpha_{1}, \\ldots, \\alpha_{k} \\in\\left(\\{0,1\\}^{n}\\right)^{[\\ell] \\backslash B}$ are chosen, and the outcome of the protocol is\n\n$$\n\\pi\\left(\\alpha_{1}: \\sigma_{1}\\left(\\alpha_{1}\\right), \\alpha_{2}: \\sigma_{2}\\left(\\alpha_{1}, \\alpha_{2}\\right), \\ldots, \\alpha_{k}: \\sigma_{k}\\left(\\alpha_{1}, \\ldots, \\alpha_{k}\\right)\\right)\n$$\n\nWe now specialize this definition to define collective coin flipping protocols\nDefinition 3.8 (Collective coin flipping protocol). A collective coin flipping protocol $\\pi$ is a protocol in the full information model with output domain $Y=\\{0,1\\}$. Furthermore, we say $\\pi$ is $(b, \\gamma)$ resilient if in the presence of any set $B$ of bad players with $|B| \\leq b$, we have that $\\max _{o \\in\\{0,1\\}} \\operatorname{Pr}\\left[\\left.\\pi\\right|_{B}=\\right.$ $o] \\leq 1-\\gamma$.\n\nNote that when $k=1$, the protocol $\\pi$ just becomes a function over $\\{0,1\\}^{\\ell}$; such 1-round coin flipping protocols which cannot be biased by any small set of bad players are also known as resilient functions.\n\nWe also specialize the definition of protocols to define leader election protocols:\nDefinition 3.9 (Leader election protocol). A leader election protocol $\\pi$ is a protocol in the full information model with output domain $Y=[\\ell]$, the number of players the protocol is operating on. Furthermore, we say $\\pi$ is $(b, \\gamma)$ resilient if in the presence of any set $B$ of bad players with $|B| \\leq b$, we have that $\\operatorname{Pr}\\left[\\left.\\pi\\right|_{B} \\in B\\right] \\leq 1-\\gamma$.\n\nRemark 3.10. The definition of resilience that we use, which is standard in the leader election and collective coin flipping literature, requires only that bad players can be elected as a leader with probability at most $1-\\gamma$. Our leader election protocols satisfy (and need) the stronger measure of quality that is standard in the pseudorandomness literature: that bad players are chosen with probability at most $\\varepsilon$ for small $\\varepsilon$.\n\nWe lastly define collective sampling protocols:\nDefinition 3.11 (Collective sampling protocol). $A$ collective sampling protocol $\\pi$ is a protocol in the full information model, typically with output domain $Y=\\{0,1\\}^{m}$ for some $m$ which is a function of $\\ell$ and $n$. The goal of collective sampling protocols is to ensure that for every output set $S \\subset\\{0,1\\}^{m}$ with density $\\mu$, in the presence of $b$ bad players, the probability that the output lies in $S$ is at most $\\varepsilon$, with the goal to make $\\varepsilon$ as close to $\\mu$ as possible.", "tables": {}, "images": {}}, {"section_id": 29, "text": "# 4 Existence of Condensers for All Values of $\\ell, n$ \n\nWe will show that there exist condensers for uniform $(g, \\ell, n)$-oNOSF sources for almost all settings of $\\ell, n$, provided $g>0.5 \\ell$. Observe that a uniform $(g, \\ell, n)$-oNOSF source is also a uniform $(g \\cdot s, \\ell \\cdot s, n / s)$-oNOSF source by simply dividing up all blocks into $s$ parts. This implies that as $n$ becomes smaller (relative to $\\ell$ ), it gets harder to condense with the hardest case being $n=1$. Our condenser will also be able to handle the case of $n=O(1)$ and $\\ell$ arbitrarily growing:\n\nTheorem 4.1 (Simplified version of Corollary 4.7). For all $g, \\ell, n, \\varepsilon, \\delta$ where $g=0.51 \\ell$, and $0.01 \\ell n \\geq 2 \\log (\\ell n / 2 \\varepsilon)+O(1)$, there exists a condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any uniform $(g, \\ell, n)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-\\Delta$ where $m=$ $0.005 \\ell n+200(\\ell+\\log (\\ell n / 2 \\varepsilon))+O(1)$ and $\\Delta=200(\\ell+\\log (\\ell n / 2 \\varepsilon))+O(1)$.\n\nNote that when $n$ is a large enough constant, $m \\geq 100 \\Delta$ and hence, the output entropy rate is at least 0.99 .\n\nIn fact, we obtain a general result for all values of $n, \\ell$ and when $g=0.5 \\ell+e$ where $e \\in \\mathbb{N}$ is arbitrary. See Lemma 4.4 for the full tradeoff; to get slightly better parameters for small $n$, see Corollary 4.6.\n\nWe combine the above condenser for uniform oNOSF sources with the transformation for lowentropy oNOSF sources to uniform oNOSF sources from Corollary 6.2 to obtain the following condenser for low-entropy oNOSF sources:\n\nCorollary 4.2. Let $g, \\ell, n, m, k, \\varepsilon$ be such that $g=0.51 \\ell, n=\\operatorname{poly}(\\log (\\ell / \\varepsilon)), k=\\Omega(\\log (\\ell / \\varepsilon)), m=$ $\\Omega(\\ell \\log (\\ell / \\varepsilon))$. Then, we can construct condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-\\Delta$ where $\\Delta=O(\\ell+\\log (1 / \\varepsilon))$.\n\nRemark 4.3. Previous condensers from [CGR24] could only show that condensers exist for uniform oNOSF sources when $\\ell=o(\\log n)$. They relied on existence of low-error two source extractors equipped with an additional \"regularity\" property. Our constructions are much simpler, recover all their results with even better parameters, and work for all values of $n$ and $\\ell$, including the hardest case of $n=O(1)$.\n\nWe provide our general construction of condensers in Section 4.1. To do that, we will require another type of condenser for two uniform oNOSF sources where the bad bits of the second block are allowed to depend on the bits of the first block. We provide this construction in Section 4.2.", "tables": {}, "images": {}}, {"section_id": 30, "text": "### 4.1 Constructing Condensers for Uniform oNOSF Sources\n\nIn this subsection, we will construct the following general condenser for uniform oNOSF sources:\nLemma 4.4 (General uniform oNOSF source condensing). For all $g, \\ell, n, \\varepsilon, e$ where $g \\geq(\\ell / 2)+e$, and en $\\geq 2 \\log (\\ell n / 2 \\varepsilon)+O(1)$, there exists a condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any uniform $(g, \\ell, n)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-\\Delta$ where $m=\\frac{\\varepsilon n}{2}+(2 \\ell-$ $e)\\left\\lceil\\frac{\\log (\\ell n / 2 \\varepsilon)+O(1)}{e}\\right\\rceil+\\log (1 / \\varepsilon)+O(1)$ and $\\Delta=(2 \\ell-2 e)\\left\\lceil\\frac{\\log (\\ell n / 2 \\varepsilon)+O(1)}{e}\\right\\rceil+\\log (1 / \\varepsilon)+O(1)$.\n\nTo do this, we will use a condenser for two distinct uniform oNOSF sources where one source can depend on the other:\n\nLemma 4.5. For all $g, \\ell, n_{x}, n_{y}, \\varepsilon$ where $n_{x} \\geq n_{y}$ and $g n_{y} \\geq \\log \\left(\\ell n_{x} / \\varepsilon\\right)+O(1)$, there exists a condenser Cond $:\\left(\\{0,1\\}^{n_{x}}\\right)^{\\ell} \\times\\left(\\{0,1\\}^{n_{y}}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that: For any uniform $\\left(g, \\ell, n_{x}\\right)$-oNOSF source $\\mathbf{X}$ and uniform $\\left(g, \\ell, n_{y}\\right)$-oNOSF source $\\mathbf{Y}$ with the additional property that bad blocks in $\\mathbf{Y}$ can depend on $\\mathbf{X}$ as well, we have that $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X}, \\mathbf{Y})) \\geq m-\\Delta$ where $m=g n_{x}+(2 \\ell-g) n_{y}+$ $\\log (1 / \\varepsilon)+O(1)$ and $\\Delta=(2 \\ell-2 g) n_{y}+\\log (1 / \\varepsilon)+O(1)$.\n\nWe construct this condenser in Section 4.2. Using this, our main general condenser can be constructed as follows:\n\nProof of Lemma 4.4. We split each block in $\\mathbf{X}$ into 2 parts to obtain a uniform $(2 g, 2 \\ell, n / 2)$-oNOSF source. We call this resultant source $\\mathbf{X}$ as well since it is the same distribution, just viewed differently. Let $\\mathbf{U}=\\left(\\mathbf{U}_{1}, \\ldots, \\mathbf{U}_{\\ell}\\right)$ and where for $1 \\leq i \\leq \\ell, \\mathbf{U}_{i}=\\mathbf{X}_{i}$. Let $\\mathbf{V}=\\left(\\mathbf{V}_{1}, \\ldots, \\mathbf{V}_{\\ell}\\right)$ where for $1 \\leq i \\leq \\ell$, we define $\\mathbf{V}_{i}$ to be prefix of length $n_{v}$ of $\\mathbf{X}_{\\ell+i}$ where $n_{v}=\\left\\lceil\\frac{\\log (\\ell n / 2 \\varepsilon)+O(1)}{e}\\right\\rceil$.\n\nWe observe that $\\mathbf{U}$ is a uniform $(e, \\ell, n / 2)$-oNOSF source and $\\mathbf{V}$ is a uniform $\\left(e, \\ell, n_{v}\\right)$-oNOSF source where bad bits in $\\mathbf{V}$ can depend on $\\mathbf{U}$ and the good bits in both sources are independent. We now define our condenser Cond to be the condenser from Lemma 4.5 applied to sources $\\mathbf{U}, \\mathbf{V}$. Hence, we will have that $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{U}, \\mathbf{V})) \\geq m-\\Delta$ where $m=e n / 2+(2 \\ell-e) n_{y}+\\log (1 / \\varepsilon)+O(1)$ and $\\Delta=(2 \\ell-2 e) n_{y}+\\log (1 / \\varepsilon)+O(1)$ as desired.\n\nOur first corollary will apply to the regime that his the hardest to condense from, namely when $n$ is very small compared to $\\ell$, even when $n=O(1)$ and $\\ell$ is arbitrarily growing:\n\nCorollary 4.6 (Small $n$ ). For all $g, \\ell, n, \\varepsilon, \\delta$ where $g \\geq(0.5+\\delta) \\ell, \\varepsilon \\geq 2^{-\\delta \\ell+O(1)}$, and $n \\leq 2^{\\delta \\ell / 2}$, there exists a condenser Cond $:\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any uniform $(g, \\ell, n)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-\\Delta$ where $m=\\delta \\ell n / 2+(2-\\delta) \\ell+\\log (1 / \\varepsilon)+O(1)$ and $\\Delta=(2-\\delta) \\ell+\\log (1 / \\varepsilon)+O(1)$.\nProof. We observe that $\\left\\lceil\\frac{\\log (\\ell n / 2 \\varepsilon)+O(1)}{e}\\right\\rceil=1$ and directly apply Lemma 4.4.\nWe also obtain the following general tradeoff for larger $n$ that may be growing with $\\ell$ or even when $\\ell=O(1)$ and $n$ growing alone (this applies to all $n$ but is most interesting when $n$ is large since Corollary 4.6 provides better tradeoff for small $n$ ).\n\nCorollary 4.7 (Larger $n$ ). For all $g, \\ell, n, \\varepsilon, \\delta$ where $g \\geq(0.5+\\delta) \\ell$, and $\\delta \\ell n \\geq 2 \\log (\\ell n / 2 \\varepsilon)+O(1)$, there exists a condenser Cond $:\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any uniform $(g, \\ell, n)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-\\Delta$ where $m=\\frac{\\delta \\ell n}{2}+(2 / \\delta-1)(\\log (\\ell n / 2 \\varepsilon)+O(1))+(2-$ $\\delta) \\ell+\\log (1 / \\varepsilon)+O(1)$ and $\\Delta=(2 / \\delta-1)(\\log (\\ell n / 2 \\varepsilon)+O(1))+2(2-\\delta) \\ell+\\log (1 / \\varepsilon)+O(1)$.\n\nProof. We observe that $\\left\\lceil\\frac{\\log (\\ell n / 2 \\varepsilon)+O(1)}{e}\\right\\rceil \\leq 1+\\frac{\\log (\\ell n / 2 \\varepsilon)+O(1)}{e}$ and apply that to the condenser from Lemma 4.4.", "tables": {}, "images": {}}, {"section_id": 31, "text": "# 4.2 Condenser for Two Uniform oNOSF Sources \n\nIn this subsection, we will prove Lemma 4.5. To construct the claimed condenser, we will use the following folklore result regarding existence of excellent seeded condensers (e.g., see Corollary 3 of [GLZ24]).\n\nTheorem 4.8. For all $n, k, d, \\varepsilon$ such that $d \\geq \\log (n / \\varepsilon)+O(1)$, there exists a seeded condenser sCond : $\\{0,1\\}^{n} \\times\\{0,1\\}^{d} \\rightarrow\\{0,1\\}^{m}$ such that for all $\\mathbf{X} \\sim\\{0,1\\}^{n}$ with $H_{\\infty}(\\mathbf{X})=k$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq k+d$ where $m=k+d+\\log (1 / \\varepsilon)+O(1)$.\n\nWe will also use the following result from [CGR24] that states an adversary can't make things too bad if it controls very few bits. We note that similar lemmas have been useful in previous construction of condensers [BCDT19, BGM22, GLZ24]:\n\nLemma 4.9 (Lemma 6.18 in [CGR24]). Let $\\mathbf{X} \\sim\\{0,1\\}^{n}$ be an arbitrary flat distribution and let Cond : $\\{0,1\\}^{n} \\rightarrow\\{0,1\\}^{m}$ be such that $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq k$. Let $G \\subset[n]$ with $|G|=n-b$ be arbitrary. Let $\\mathbf{X}_{G} \\sim\\{0,1\\}^{n-b}$ be the projection of $\\mathbf{X}$ onto $G$. Let $\\mathbf{X}^{\\prime} \\sim\\{0,1\\}^{n}$ be the distribution where the output bits defined by $G$ equal $\\mathbf{X}_{G}$ and remaining $b$ bits are deterministic functions of the $n-b$ bits defined by $G$ under the restriction that $\\operatorname{Supp}\\left(\\mathbf{X}^{\\prime}\\right) \\subset \\operatorname{Supp}(\\mathbf{X})$. Then, $H_{\\infty}^{\\varepsilon^{\\prime}}\\left(\\operatorname{Cond}\\left(\\mathbf{X}^{\\prime}\\right)\\right) \\geq k-b$ where $\\varepsilon^{\\prime}=\\varepsilon \\cdot 2^{b}$.\n\nWith this, we are ready to provide the construction of condensers for two uniform oNOSF sources:\n\nProof of Lemma 4.5. Let sCond : $\\left(\\{0,1\\}^{n_{x}}\\right)^{\\ell} \\times\\left(\\{0,1\\}^{n_{y}}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ be lossless condenser guaranteed from Theorem 4.8 with $\\varepsilon_{\\text {sCond }}=\\varepsilon \\cdot 2^{-(\\ell-g) n_{y}}$. We define $\\operatorname{Cond}(x, y)=\\operatorname{sCond}(x, y)$.\n\nLet $\\mathbf{O}_{\\text {unif }}=\\operatorname{Cond}\\left(\\mathbf{X}, \\mathbf{U}_{\\ell n_{y}}\\right)$ and $\\mathbf{O}_{a d v}=\\operatorname{Cond}(\\mathbf{X}, \\mathbf{Y})$. We argue that $\\mathbf{O}_{\\text {unif }}$ will be highly condensed and since the adversary controls so few bits in $\\mathbf{Y}, \\mathbf{O}_{a d v}$ will be condensed as well.\n\nWe first see that by the property of the seeded condenser, $H_{\\infty}^{\\varepsilon_{\\text {sCond }}}\\left(\\mathbf{O}_{\\text {unif }}\\right) \\geq g n_{x}+\\ell n_{y}$. Next we observe that $\\mathbf{O}_{a d v}$ can be obtained from $\\mathbf{O}_{\\text {unif }}$ by an adversary controlling $b=(\\ell-g) n_{y}$ bits from $\\left(\\mathbf{X}, \\mathbf{U}_{\\ell n_{y}}\\right)$ to obtain $(\\mathbf{X}, \\mathbf{Y})$ and considering the output of sCond. We apply Lemma 4.9 which allows us to compare output entropy in such scenarios and obtain that\n\n$$\nH_{\\infty}^{\\varepsilon_{\\text {sCond }} \\cdot 2^{b}}\\left(\\mathbf{O}_{a d v}\\right) \\geq H_{\\infty}^{\\varepsilon}\\left(\\mathbf{O}_{u n i f}\\right)-b \\geq\\left(g n_{x}+\\ell n_{y}\\right)-\\left((\\ell-g) n_{y}\\right)=m-\\Delta\n$$\n\nAs $\\varepsilon_{\\text {sCond }} \\cdot 2^{b}=\\varepsilon$, we indeed have that $H_{\\infty}^{\\varepsilon}\\left(\\mathbf{O}_{a d v}\\right) \\geq m-\\Delta$ as desired.", "tables": {}, "images": {}}, {"section_id": 32, "text": "# 5 Explicit Condensers for Uniform oNOSF Sources \n\nIn this section, we will prove the following main result regarding condensing from uniform oNOSF sources, matching the existential condenser parameters of [CGR24]. We state this for constant $\\ell$ but our condenser can handle any $\\ell=o(\\log n)$.\n\nTheorem 5.1 (Clean version of Theorem 5.5). For constant $g, \\ell$ where $g>\\ell / 2$, and all $n, \\varepsilon$, there exists an explicit condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any uniform $(g, \\ell)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-O(\\log (m / \\varepsilon))$ where $m=\\Omega(n)$.\n\nUsing the transformation of low-entropy oNOSF sources to uniform oNOSF sources from Corollary 6.4 by setting $d=\\ell$ (for this parameter regime, such a transformation can also be obtained using results from [CGR24]), we get an explicit condenser for low-entropy oNOSF sources:\n\nCorollary 5.2. For constant $g, \\ell$ where $g>\\ell / 2+1$, and all $n, k, \\varepsilon$ with $k \\geq \\operatorname{poly}(\\log (n))+\\log (n / \\varepsilon)+$ $O(1), \\varepsilon \\geq n^{-\\Omega(1)}$, there exists an explicit condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X}))=m-O(\\log (m / \\varepsilon))$ where $m=k^{\\Omega(1)}$.\n\nWe now use a reduction from Lemma 5.12 in [CGR24]. The reduction shows how to use condensers for $g>\\ell / 2$ to construct condensers for all $g, \\ell$ that condense up to rate $1 /\\lfloor\\ell / g\\rfloor .{ }^{7}$ Using this, we construct explicit condensers for all $(g, \\ell)$. Particularly, we get the following:\n\nCorollary 5.3. For any constant $g$ and $\\ell$ and all $n, \\varepsilon$, there exists an explicit condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any uniform $(g, \\ell)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq$ $1 /\\lfloor\\ell / g\\rfloor \\cdot m-O(\\log (m / \\varepsilon))$ where $m=\\Omega(n)$.\n\nWe again use the transformation of low-entropy oNOSF sources to uniform oNOSF sources (Corollary 6.4) to construct a condenser for low-entropy oNOSF sources for all $g$ and $\\ell$ :\n\nCorollary 5.4. For all constant $g, \\ell$, and all $n, k, \\varepsilon$ with $k \\geq \\operatorname{poly}(\\log n), \\varepsilon \\geq n^{-\\Omega(1)}$, there exists an explicit condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ such that for any $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X}))=1 /\\lfloor(\\ell-1) /(g-1)\\rfloor \\cdot m-O(\\log (m / \\varepsilon))$ where $m=\\Omega\\left(k^{-\\Omega(1)}\\right)$.", "tables": {}, "images": {}}, {"section_id": 33, "text": "# 5.1 Proving the Main Theorem \n\nHere we prove the following full version of Theorem 5.1:\nTheorem 5.5. There exists a universal constant $C$ such that for all $n, g, \\ell, \\varepsilon$ where $g>\\ell / 2$, there exists an explicit condenser Cond : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ satisfying: for any uniform $(g, \\ell)$ oNOSF source $\\mathbf{X}$ with $g>\\ell / 2$, we have $H_{\\infty}^{\\varepsilon}(\\operatorname{Cond}(\\mathbf{X})) \\geq m-(3 C)^{\\ell} \\log (2 \\ell n / \\varepsilon)$ where $m=\\frac{1}{3}$. $\\left((n / 2)-(3 C)^{\\ell} \\log (2 \\ell n / \\varepsilon)\\right)$.\n\nThe proof of this theorem follows by explicitly constructing an extractor for multiple independent uniform sources, where few of them are set to constant, with the error depending on the longest source that is non-constant. Formally, we use the following:\n\nLemma 5.6. There exists universal constant $C$ such that for all $n_{k}, k_{x}, n_{y, 1}, \\ldots, n_{y, t}, m, 0<\\varepsilon_{1} \\leq$ $\\cdots \\leq \\varepsilon_{t}<1$ satisfying $n_{y, i} \\geq C \\log \\left(2 n_{x} / \\varepsilon_{i}\\right)$ and $m=\\frac{k_{x}-\\log \\left(2 / \\varepsilon_{1}\\right)}{3}$, the following holds: There exists an explicit extractor Ext : $\\{0,1\\}^{n_{x}} \\times\\{0,1\\}^{n_{y, i}} \\times \\cdots \\times\\{0,1\\}^{n_{y, t}} \\rightarrow\\{0,1\\}^{m}$ satisfying: For all $1 \\leq j \\leq t$ and all independent sources $\\mathbf{X} \\sim\\{0,1\\}^{n_{x}}, \\mathbf{Y}_{1} \\sim\\{0,1\\}^{n_{y, 1}}, \\ldots, \\mathbf{Y}_{t} \\sim\\{0,1\\}^{n_{y, t}}$ where $H_{\\infty}(\\mathbf{X})=k_{x}$, each of $\\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{j-1}$ are fixed constants and all $\\mathbf{Y}_{j}, \\ldots, \\mathbf{Y}_{t}$ are uniform, we have that $\\operatorname{Ext}\\left(\\mathbf{X}, \\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{t}\\right)$ is $\\varepsilon_{j}$-close to $\\mathbf{U}_{m}$.\n\nWe will prove this lemma at the end of this section. Along with this extractor, we use the fact that if a function $f$ condenses any flat distribution $\\mathbf{X}$, then $f$ also condenses, with a small loss in parameters, any distribution $\\mathbf{X}^{\\prime}$ which is the same as $\\mathbf{X}$ but with a few bits controlled by an adversary. Lemma 4.9 elaborates and provides the result we need. Using this and our previously stated extractor, our condenser result follows:\n\nProof of Theorem 5.5. First, we split each block in $\\mathbf{X}$ into two to obtain uniform $(2 g, 2 \\ell, n / 2)$ oNOSF source. This source is still $\\mathbf{X}$, just with this new parameters. For $1 \\leq i \\leq \\ell$, let $n_{y, i}=$ $2 C(3 C)^{\\ell-i} \\log (2 \\ell n / \\varepsilon)$ and let $n_{y}=\\sum_{i=1}^{\\ell} n_{y, i}$. Let $\\mathbf{Y}_{i}$ be the length $n_{y, i}$ prefix of the block $\\mathbf{X}_{\\ell+i}$. Let $\\mathbf{W}=\\left(\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{\\ell}\\right)$ and let $\\mathbf{Y}=\\left(\\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{\\ell}\\right)$. We use the extractor Ext from Lemma 5.6 with\n\n[^0]\n[^0]:    ${ }^{7}$ The statement of the lemma in [CGR24] does not explicitly state it as a reduction but such a reduction easily follows from the proof of the lemma.\n\n$k_{x}=n / 2-n_{y}-\\log (2 / \\varepsilon), m$ from the lemma statement, and for $1 \\leq i \\leq \\ell$, we set $\\varepsilon_{i}=\\left(\\frac{\\varepsilon}{2 \\ell n}\\right)^{(3 C)^{\\ell-i}}$. With this, we define our condenser as:\n\n$$\n\\operatorname{Cond}(\\mathbf{X})=\\operatorname{Ext}\\left(\\mathbf{W}, \\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{\\ell}\\right)\n$$\n\nWe easily compute and check that our parameter settings satisfy the requirements of Lemma 5.6. We will show that the output entropy (with error $\\varepsilon$ ) is at least $m-n_{y}$. We compute that $n_{y} \\leq$ $(3 C)^{\\ell} \\log (2 \\ell n / \\varepsilon)$, the output entropy gap. Hence if we show this, then our condenser will indeed have the claimed property.\n\nWe now show that our condenser construction is correct. First, since $2 g>\\ell$, there exists at least one good block amongst $\\mathbf{X}_{\\ell+1}, \\ldots, \\mathbf{X}_{2 \\ell}$ and hence, at least one good block amongst $\\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{\\ell}$. Let this good block appear at index $j \\in[\\ell]$. Similarly, there exists at least one good block amongst $\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{\\ell}$ and so, $H_{\\infty}(\\mathbf{W}) \\geq n / 2$. Let $\\mathbf{A}=\\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{j-1}$ and let $\\mathbf{B}=\\mathbf{Y}_{j+1}, \\ldots, \\mathbf{Y}_{\\ell}$. Then, $\\mathbf{Y}=\\left(\\mathbf{A}, \\mathbf{Y}_{j}, \\mathbf{B}\\right)$. We will show that $H_{\\infty}^{\\varepsilon}\\left(\\operatorname{Ext}\\left(\\mathbf{W},\\left(\\mathbf{A}, \\mathbf{Y}_{j}, \\mathbf{B}\\right)\\right)\\right) \\geq m-n_{y}$.\n\nWe will now consider fixings of $\\mathbf{A}$. We say a fixing of $\\mathbf{A}=a$ is good if $H_{\\infty}(\\mathbf{W} \\mid \\mathbf{A}=a) \\geq$ $n / 2-n_{y}-\\log (2 / \\varepsilon)=k_{x}$. By the min-entropy chain rule (Lemma 3.2), at least $1-\\varepsilon / 2$ fraction of fixings of $\\mathbf{A}$ are good. As $\\mathbf{X}$ is an oNOSF source, $\\mathbf{X}_{\\ell+j}$ is independent of blocks $\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{\\ell+j-1}$. Hence, $\\mathbf{Y}_{j}$ remains uniform and independent of $\\mathbf{W}$, for every fixing of $\\mathbf{A}$. We will show that for every good fixing of $\\mathbf{A}=a, H_{\\infty}^{\\varepsilon / 2} \\operatorname{Ext}(\\mathbf{W}, \\mathbf{Y}) \\geq m-\\sum_{i=j+1}^{n} n_{y, i} \\geq m-n_{y}$. This will prove our result as our total error will be $\\varepsilon$ and the min-entropy guarantee will be $m-n_{y}$, as desired.\n\nConsider the best case scenario when $(\\mathbf{B} \\mid \\mathbf{A}=a)=\\mathbf{U}_{|\\mathbf{B}|}$. This is unrealistic since it is possible that all bits in $\\mathbf{B}$ are bad and arbitrarily depend on the remaining bits. Nevertheless, it is instructive to see what happens in this scenario. In this case, $\\mathbf{W}, \\mathbf{Y}$ are independent distributions and we can infer that $\\operatorname{Ext}(\\mathbf{W}, \\mathbf{Y}) \\approx_{\\varepsilon_{j}} \\mathbf{U}_{m}$. However, as alluded before, all bits in $\\mathbf{B}$ can be adversarially set. To overcome this, we invoke Lemma 4.9 that allows us to compare how worse off our output distribution can be compared to the best case scenario. We conclude that even when $\\mathbf{B}$ is completely adversarially controlled, $H_{\\infty}^{\\varepsilon^{\\prime}}(\\operatorname{Ext}(\\mathbf{W}, \\mathbf{Y})) \\geq m-|\\mathbf{B}|=m-\\sum_{i=j+1} n_{y, i}$ where\n\n$$\n\\begin{aligned}\n\\varepsilon^{\\prime} & =\\varepsilon_{j} \\cdot 2^{|\\mathbf{B}|} \\\\\n& =\\left(\\frac{\\varepsilon}{2 \\ell n}\\right)^{(3 C)^{\\ell-j}} \\cdot 2^{\\sum_{i=j+1}^{\\ell} n_{y, i}} \\\\\n& =\\left(\\frac{\\varepsilon}{2 \\ell n}\\right)^{(3 C)^{\\ell-j}} \\cdot 2^{2 C \\log (2 \\ell n / \\varepsilon) \\sum_{i=j+1}^{\\ell}(3 C)^{\\ell-i}} \\\\\n& =\\left(\\frac{\\varepsilon}{2 \\ell n}\\right)^{(3 C)^{\\ell-j}} \\cdot\\left(\\frac{2 \\ell n}{\\varepsilon}\\right)^{2 C \\frac{(3 C)^{\\ell-j-1}}{3 C-1}} \\\\\n& \\leq\\left(\\frac{\\varepsilon}{2 \\ell n}\\right)^{(3 C)^{\\ell-j}} \\cdot\\left(\\frac{2 \\ell n}{\\varepsilon}\\right)^{(3 C)^{\\ell-j}-1} \\\\\n& \\leq \\frac{\\varepsilon}{2 \\ell n} \\\\\n& \\leq \\varepsilon / 2\n\\end{aligned}\n$$\n\nThis proves our claim, showing that for all good fixings, our output is highly condensed.\nWe need to be careful when invoking Lemma 4.9 since it requires that $\\left(\\mathbf{W}, \\mathbf{A}, \\mathbf{Y}_{j}, \\mathbf{U}_{|\\mathbf{B}|}\\right)$ should be a flat distribution. While that may not be true, we can express $\\mathbf{W}$ as a convex combination\n\nof flat sources with same min-entropy and since $\\mathbf{A}$ is fixed and $\\mathbf{Y}_{j}$ and $\\mathbf{U}_{|\\mathbf{B}|}$ are independent and uniform, we can express the joint distribution as convex combination of flat sources, for each of them invoke the lemma, and conclude that the original distribution will be condensed as well.\n\nLastly, we show how to construct our multi-source extractor with the desired properties. Using various seeded extractors, we construct our final extractor as follows:\n\nProof of Lemma 5.6. For $1 \\leq i \\leq t$, let $\\mathrm{sExt}_{i}:\\{0,1\\}^{n_{x}} \\times\\{0,1\\}^{n_{y, i}} \\rightarrow\\{0,1\\}^{m}$ be explicit $\\left(\\varepsilon_{i} / 2\\right)$ -seeded-extractor guaranteed by Theorem 3.5. Our extractor construction is:\n\n$$\n\\operatorname{Ext}\\left(x, y_{1}, \\ldots, y_{t}\\right)=\\bigoplus_{i=1}^{t} \\operatorname{sExt}_{i}\\left(x, y_{i}\\right)\n$$\n\nLet $\\mathbf{Z}_{\\text {good }}=\\operatorname{sExt}_{j}\\left(\\mathbf{X}, \\mathbf{Y}_{j}\\right)$ and let $\\mathbf{Z}_{\\text {rest }}=\\bigoplus_{1 \\leq i \\leq t, i \\neq j} \\operatorname{sExt}_{i}\\left(\\mathbf{X}, \\mathbf{Y}_{i}\\right)$. Notice that our final output distribution is $\\mathbf{Z}_{\\text {good }} \\oplus \\mathbf{Z}_{\\text {rest }}$. We will argue that on most fixings of $\\mathbf{Z}_{\\text {rest }}$, the output will be close to uniform.\n\nBy Lemma 3.2, we have that\n\n$$\n\\operatorname{Pr}\\left[H_{\\infty}\\left(\\mathbf{X} \\mid \\mathbf{Z}_{\\text {rest }}=z_{\\text {rest }}\\right) \\geq k_{x}-m-\\log \\left(2 / \\varepsilon_{j}\\right)\\right] \\geq 1-\\varepsilon_{j} / 2\n$$\n\nCall the fixings $z_{\\text {rest }}$ of $\\mathbf{Z}_{\\text {rest }}$ that satisfy the above property of leaving $\\mathbf{X}$ with a lot of entropy when conditioning on them, as the \"good fixings.\" As $\\mathbf{Z}_{\\text {rest }}$ is independent of $\\mathbf{Y}_{j}$ and $\\mathbf{X}$ is left with a lot of entropy conditioning on a good fixing $z_{\\text {rest }}$, we have that\n\n$$\n\\operatorname{sExt}_{j}\\left(\\left(\\mathbf{X} \\mid \\mathbf{Z}_{\\text {rest }}=z_{\\text {rest }}\\right),\\left(\\mathbf{Y}_{j} \\mid \\mathbf{Z}_{\\text {rest }}=z_{\\text {rest }}\\right)\\right) \\approx_{\\varepsilon_{j} / 2} \\mathbf{U}_{m}\n$$\n\nAs $1-\\varepsilon_{j} / 2$ fraction of fixings of $\\mathbf{Z}_{\\text {rest }}$ are good, we conclude that $\\operatorname{Ext}\\left(\\mathbf{X}, \\mathbf{Y}_{1}, \\ldots, \\mathbf{Y}_{t}\\right) \\approx_{\\varepsilon_{j}} \\mathbf{U}_{m}$ as desired.", "tables": {}, "images": {}}, {"section_id": 34, "text": "# 6 Transforming Low-Entropy oNOSF Sources to Uniform oNOSF Sources \n\nIn this section, we show how to transform low-entropy oNOSF sources into uniform oNOSF sources. Such a transformation was also provided in [CGR24]. Here, we obtain improved bounds using a generalized construction that allows us to obtain better tradeoffs and parameters in many more regimes of $n, \\ell$. Our main theorem is:\n\nTheorem 6.1. Let $d, g, g_{\\text {out }}, \\ell, n, m, k, \\varepsilon$ be such that $g_{\\text {out }} \\leq g-\\frac{\\ell-g+2}{d}, n \\geq k \\geq \\log (n d-k)+$ $m d+2 \\log \\left(2 g_{\\text {out }} / \\varepsilon\\right)$. Then, there exists a function $f:\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\left(\\{0,1\\}^{m}\\right)^{\\ell-1}$ such that for any $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, there exists uniform $\\left(g_{\\text {out }}, \\ell-1, m\\right)$-oNOSF source $\\mathbf{Y}$ for which $|f(\\mathbf{X})-\\mathbf{Y}| \\leq \\varepsilon$.\n\nOur construction's power comes from the flexibility of setting $d$ to any desired value. For instance by setting $d$ to be a large constant, we can get the following transformation that works even when $n$ is very small compared to $\\ell$ :\n\nCorollary 6.2 (Transformation for small $n$ ). Let $g, \\ell, n, m, k, \\varepsilon, \\delta$ be such that $\\delta \\leq 0.99, g=\\delta \\ell, n=$ $\\operatorname{poly}(\\log (\\delta \\ell / \\varepsilon)), k=\\Omega(\\log (\\delta \\ell / \\varepsilon)), m=\\Omega(k)$. Then, we can construct a function $f:\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow$ $\\left(\\{0,1\\}^{m}\\right)^{\\ell-1}$ such that: for any $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, there exists uniform $(0.99 \\delta \\ell, \\ell-1, m)$ oNOSF source $\\mathbf{Y}$ such that $|f(\\mathbf{X})-\\mathbf{Y}| \\leq \\varepsilon$.\n\nWe additionally note that when we set $d=\\ell$, we recover the same construction as in [CGR24], matching its parameters. This is most interesting in the regime when say $\\ell=O(1)$ and $n$ is arbitrarily growing.\n\nCorollary 6.3 (similar parameters as Theorem 5.2 from [CGR24]). Let $g, \\ell, n, m, k, \\varepsilon$ be such that $k \\geq 1.01(\\log (n \\ell)+2 \\log (2(g-1) / \\varepsilon)), m=k / 200 \\ell$. Then, we can construct a function $f:\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\left(\\{0,1\\}^{m}\\right)^{\\ell-1}$ such that for any $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, there exists uniform $(g-1, \\ell-1, m)$-oNOSF source $\\mathbf{Y}$ such that $|f(\\mathbf{X})-\\mathbf{Y}| \\leq \\varepsilon$.\n\nTo obtain these transformations, we will use two-source extractors. In fact, using explicit construction of two-source-extractors, we also obtain an explicit transformation:\n\nCorollary 6.4 (Explicit Transformation). There exists a universal constant $C$ such that for all $d, g, g_{\\text {out }}, \\ell, n, m, k, \\varepsilon$ satisfying $g_{\\text {out }} \\leq g-\\frac{\\ell-g+2}{d}, k \\geq \\operatorname{poly}(\\log (n))+m d+2 \\log \\left(2 g_{\\text {out }} / \\varepsilon\\right)+O(1), m \\leq$ $\\operatorname{poly}(\\log n), \\varepsilon \\geq n^{-\\Omega(1)} / 2 g_{\\text {out }}$. the following holds: There exists an explicit function $f:\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow$ $\\left(\\{0,1\\}^{m}\\right)^{\\ell-1}$ such that for any $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, there exists uniform $\\left(g_{\\text {out }}, \\ell-1, m\\right)$ oNOSF source $\\mathbf{Y}$ for which $|f(\\mathbf{X})-\\mathbf{Y}| \\leq \\varepsilon$.\n\nWe can instantiate this lemma even in the case of constant $d$ and get an explicit transformation similar to Corollary 6.2 with fewer output bits per block.\n\nWe will use the following main technical lemma that shows how to use two-source extractors to obtain these transformations:\n\nLemma 6.5 (Main Lemma). Let $d, g, g_{\\text {out }}, \\ell, n, m, k_{2 \\text { Ext }}, k, \\varepsilon_{2 \\text { Ext }}$ be such that $k \\geq k_{2 \\text { Ext }}+m \\cdot d+$ $\\log \\left(1 / \\varepsilon_{2 \\text { Ext }}\\right), g_{\\text {out }} \\leq \\frac{g(d+1)-\\ell-2}{d}$. Let $2 \\mathrm{Ext}:\\{0,1\\}^{d \\cdot n} \\times\\{0,1\\}^{n} \\rightarrow\\{0,1\\}^{m}$ be $\\left(k_{2 \\text { Ext }}, \\varepsilon_{2 \\text { Ext }}\\right)$-average-case-strong two-source extractor. Then, we can construct a function $f:\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\left(\\{0,1\\}^{m}\\right)^{\\ell-1}$ such that for any $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}$, there exists $\\left(g_{\\text {out }}, \\ell-1, m\\right)$-oNOSF source $\\mathbf{Y}$ such that $|f(\\mathbf{X})-\\mathbf{Y}| \\leq \\varepsilon$ where $\\varepsilon=2 g_{\\text {out }} \\cdot \\varepsilon_{2 \\text { Ext }}$.\n\nExistentially, two-source-extractors with following parameters exist:\nLemma 6.6 (Lemma 5.4 from [CGR24]). Let $n_{1}, n_{2}, k_{1}, k_{2}, m, \\varepsilon$ be such that $k_{1} \\leq n_{1}, k_{2} \\leq n_{2}, m=$ $k_{1}+k_{2}-2 \\log (1 / \\varepsilon)-O(1), k_{2} \\geq \\log \\left(n_{1}-k_{1}\\right)+2 \\log (1 / \\varepsilon)+O(1)$, and $k_{1} \\geq \\log \\left(n_{2}-k_{2}\\right)+$ $2 \\log (1 / \\varepsilon)+O(1)$. Then, a random function $2 \\mathrm{Ext}:\\{0,1\\}^{n_{1}} \\times\\{0,1\\}^{n_{2}} \\rightarrow\\{0,1\\}^{m}$ is a $\\left(k_{1}, k_{2}, \\varepsilon\\right)$-two source extractor with probability $1-o(1)$.\n\nUsing this, our main result follows:\nProof of Theorem 6.1. We use the two-source-extractors from Theorem 6.1 and apply it in Lemma 6.5.\n\nTo make this transformation explicit, we can use the following construction of a two-sourceextractor:\n\nTheorem 6.7 ([CZ19, Mek17, Li16]). There exists a universal constant $C \\geq 1$ such that for all $n, k, m, \\varepsilon$ with $k \\geq \\log ^{C}(n), m \\leq n^{1 / C}, \\varepsilon \\geq n^{-1 / C}$, the following holds: There exists an explicit $(n, k)$ two-source-extractor $2 \\mathrm{Ext}:\\{0,1\\}^{n} \\times\\{0,1\\}^{n} \\rightarrow\\{0,1\\}^{m}$.\n\nWith this our explicit transformation follows:\nProof of Corollary 6.4. We use the explicit two-source-extractors from Theorem 6.7 and apply it in Lemma 6.5.", "tables": {}, "images": {}}, {"section_id": 35, "text": "# 6.1 Low-Entropy oNOSF Source to Uniform Using Two-Source-Extractors \n\nIn this subsection, we will prove Lemma 6.5. To do this, we will use two-source-extractors and average-case two-source-extractors. Let's first define them:\n\nDefinition 6.8. We say that 2Ext is $\\left(k_{1}, k_{2}, \\varepsilon\\right)$ average-case strong if\n\n$$\n2 \\operatorname{Ext}\\left(\\mathbf{X}_{1}, \\mathbf{X}_{2}\\right), \\mathbf{W} \\approx_{\\varepsilon} \\mathbf{U}_{m}, \\mathbf{W}\n$$\n\nfor every $\\mathbf{X}_{1}$ and $\\mathbf{W}$ such that $\\widetilde{H}_{\\infty}\\left(\\mathbf{X}_{1} \\mid \\mathbf{W}\\right) \\geq k_{1}$ with $\\mathbf{X}_{2}$ independent of $\\mathbf{X}_{1}$ and $H_{\\infty}\\left(\\mathbf{X}_{2}\\right) \\geq k_{2}$ and $\\mathbf{W}$.\n\nThis notion of average-case two-source-extractors allows us obtain a simpler chain rule:\nLemma 6.9. [DORS08] Let $\\mathbf{A}, \\mathbf{B}$, and $\\mathbf{C}$ be distributions such that $\\operatorname{Supp}(\\mathbf{B}) \\leq 2^{\\lambda}$. Then $\\widetilde{H}_{\\infty}(\\mathbf{A} \\mid$ $\\mathbf{B}, \\mathbf{C}) \\geq \\widetilde{H}_{\\infty}(\\mathbf{A}, \\mathbf{B} \\mid \\mathbf{C})-\\lambda \\geq \\widetilde{H}_{\\infty}(\\mathbf{A} \\mid \\mathbf{C})-\\lambda$.\n\nLemma 2.3 of [DORS08] shows that all two-source extractors are average-case-two-source extractors with similar parameters.\n\nLemma 6.10. [DORS08] For any $\\eta>0$, if 2 Ext is a $\\left(k_{1}, k_{2}, \\varepsilon\\right)$-two-source extractor, then 2 Ext is a $\\left(k_{1}+\\log (1 / \\eta), k_{2}, \\varepsilon+\\eta\\right)$-average-case-two-source extractor.\n\nWith this, we will finally prove our main lemma that shows how to use two-source-extractors to obtain our transformation:\n\nProof of Lemma 6.5. For $-d \\leq i \\leq 0$, define $\\mathbf{X}_{i}$ to be the random variable that always outputs $0^{n}$. For $2 \\leq i \\leq \\ell$, we output $\\mathbf{O}_{i}=2 \\operatorname{Ext}\\left(\\mathbf{X}_{i-d} \\circ \\cdots \\circ \\mathbf{X}_{i-1}, \\mathbf{X}_{i}\\right)$.\n\nFor $2 \\leq i \\leq \\ell$, we say that $\\mathbf{O}_{i}$ is good if (1) $\\mathbf{X}_{i}$ is good and (2) there exists a block amongst $\\mathbf{X}_{i-d}, \\ldots, \\mathbf{X}_{i-1}$ that is good. We observe that if $\\mathbf{O}_{i}$ is good, then $\\left|\\mathbf{O}_{i}-\\mathbf{U}_{m}\\right| \\leq \\varepsilon_{2 \\text { Ext }}$. Let $g^{\\prime}$ be the number of such good $\\mathbf{O}_{i}$. Let $j_{1}, \\ldots, j_{g}$ be the indices of the good blocks in $\\mathbf{X}$. For $1 \\leq i \\leq g-1$, let $d_{i}=j_{i+1}-j_{i}$. We observe that $g^{\\prime}$ equals number of $i$ such that $d_{i} \\leq d$. As $\\sum_{i=1}^{g-1} d_{i} \\leq \\ell$ and $d_{i} \\geq 1$, we infer that $g^{\\prime} \\geq \\frac{(g-1)(d+1)-i}{d}$. Hence, as long as $g_{\\text {out }} \\leq\\left\\lceil g^{\\prime}\\right\\rceil$, we can guarantee the desired number of good blocks in the output. This holds as long as $g_{\\text {out }} \\leq \\frac{g(d+1)-\\ell-2}{d}$.\n\nUsing Lemma 6.10, we infer that 2 Ext is $\\left(k_{2 \\text { Ext }}+\\log \\left(1 / \\varepsilon_{2 \\text { Ext }}\\right), 2 \\varepsilon_{2 \\text { Ext }}\\right)$-average-case-two-source extractor. We will use this property below.\n\nNow, using a hybrid argument we will show that\n\n$$\n\\left(\\mathbf{O}_{2}, \\ldots, \\mathbf{O}_{\\ell}\\right) \\approx_{2 g_{\\text {out }} \\cdot \\varepsilon_{2 \\text { Ext }}}\\left(\\mathbf{Y}_{2}, \\ldots, \\mathbf{Y}_{\\ell}\\right)\n$$\n\nwhere $\\mathbf{Y}=\\left(\\mathbf{Y}_{2}, \\ldots, \\mathbf{Y}_{\\ell}\\right)$ is a uniform $\\left(g_{\\text {out }}, \\ell, m\\right)$-oNOSF source that we will define as the proof goes. Let $\\mathbf{Y}^{(1)}=\\left(\\mathbf{O}_{2}, \\ldots, \\mathbf{O}_{\\ell}\\right)$ and for $2 \\leq i \\leq \\ell$, let $\\mathbf{Y}^{(i)}=\\left(\\mathbf{O}_{2}, \\ldots, \\mathbf{O}_{i}, \\mathbf{Y}_{i+1}, \\ldots, \\mathbf{Y}_{\\ell}\\right)$. Hence, $\\mathbf{Y}^{(\\ell)}=\\mathbf{Y}$. We proceed by induction. We will show that for $2 \\leq i \\leq \\ell$,\n\n$$\n\\left|\\mathbf{Y}^{(i)}-\\mathbf{Y}^{(i-1)}\\right| \\leq 2 \\varepsilon_{2 \\text { Ext }}\n$$\n\nwhenever $\\mathbf{O}_{i}$ is good and\n\n$$\n\\mathbf{Y}^{(i)}=\\mathbf{Y}^{(i-1)}\n$$\n\nwhenever $\\mathbf{O}_{i}$ is bad. By repeated applications of the triangle inequality, we will have shown that our output is indeed close to some uniform oNOSF source with desired parameters.\n\nWe proceed by induction and let $i \\geq 2$ be arbitrary. If $\\mathbf{O}_{i}$ is bad, then we let $\\mathbf{Y}_{i}=\\mathbf{O}_{i}$. Then, we indeed have that $\\mathbf{Y}^{(i)}=\\mathbf{Y}^{(i-1)}$ as desired. Otherwise, we assume $\\mathbf{O}_{i}$ is good. Then, it must be that $\\mathbf{X}_{i}$ is good. Let $i_{\\text {prev }}$ be the index of the good block before $\\mathbf{X}_{i}$ in $\\mathbf{X}$. Then, we know that $i-i_{\\text {prev }} \\leq d$. We first claim that\n\n$$\n\\widetilde{H}_{\\infty}\\left(\\mathbf{X}_{i_{\\text {prev }}} \\mid \\mathbf{O}_{1}, \\ldots, \\mathbf{O}_{i-1}\\right) \\geq k_{2 \\text { Ext }}=k-m \\cdot d\n$$\n\nFirstly, by construction, blocks $\\mathbf{O}_{2}, \\mathbf{O}_{i_{\\text {prev }}-1}$ are functions of blocks $\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{i_{\\text {prev }}-1}$. As $\\mathbf{X}_{i_{\\text {prev }}}$ is independent of $\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{i_{\\text {prev }}-1}$, we infer that $\\mathbf{X}_{i_{\\text {prev }}}$ is independent of $\\mathbf{O}_{2}, \\mathbf{O}_{i_{\\text {prev }}-1}$. As 2Ext is average-case-strong, we apply Lemma 6.9 to get that\n\n$$\n\\widetilde{H}_{\\infty}\\left(\\mathbf{X}_{i_{\\text {prev }}} \\mid \\mathbf{O}_{2}, \\ldots, \\mathbf{O}_{i-1}\\right) \\geq k-m \\cdot\\left(i-i_{\\text {prev }}\\right) \\geq k-m \\cdot d=k_{2 \\text { Ext }}+\\log (1 / \\varepsilon)\n$$\n\nwhere for the second last inequality, we used the fact that $i-i_{\\text {prev }} \\leq d$. Moreover, as $\\mathbf{X}_{i}$ is independent of $\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{i-1}$ and $\\mathbf{O}_{2}, \\ldots, \\mathbf{O}_{i-1}$ are solely functions of $\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{i-1}$, we infer that $\\mathbf{X}_{i}$ is independent of $\\mathbf{O}_{2}, \\ldots, \\mathbf{O}_{i-1}$. Hence, conditioned on fixing $\\mathbf{O}_{2}, \\ldots, \\mathbf{O}_{i-1}, \\mathbf{O}_{i}$ will be $2 \\varepsilon_{2 \\text { Ext }}$ close to $\\mathbf{U}_{m}$. This implies $\\mathbf{Y}^{(i-1)} \\approx_{2 \\varepsilon_{2 \\text { Ext }}} \\mathbf{Y}^{(i)}$ as desired. This shows that a good block in $\\mathbf{Y}$ is uniform conditioned on all previous blocks, i.e., it is independent of all the blocks before it. This shows all bad blocks can only depend on good blocks appearing before them and that good blocks are independent of each other. This implies $\\mathbf{Y}$ is indeed a uniform oNOSF source as desired.", "tables": {}, "images": {}}, {"section_id": 36, "text": "# 7 Online Influence and Extraction Lower Bounds \n\nTowards proving lower bounds on the possiblity of extraction from oNOSF sources, we introduce a new, natural notion of influence of Boolean functions, which we call online influence. For simplicity, we first start by considering the class of oNOBF sources, which corresponds to uniform $(g, \\ell, n=1)$ oNOSF sources. We formally define the notion for Boolean functions and discuss some basic properties in Section 7.1. We establish tight bounds on the online influence for general functions, including a Poincar\u00e9 style inequality, in Section 7.2. We provide an example exhibiting a separation between maximum (standard) influence and online influence in Section 7.3. Finally, in Section 7.4, we extend the definition of online influence to subsets of coordinates (and functions from $\\Sigma^{n} \\rightarrow$ $\\{0,1\\}^{m}$, for arbitrary alphabet $\\Sigma$ ). This allows us to prove the required lower bounds on extraction (and condensing) from oNOSF sources.\n\nNotation: For convenience, we introduce some notation that we use for the rest of this section. For any bit $b \\in\\{0,1\\}$, let $e(b)=(-1)^{b}$. For any Boolean function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, let $e(f)$ denote the function $e(f)(x)=(-1)^{f(x)}$.", "tables": {}, "images": {}}, {"section_id": 37, "text": "# 7.1 Basic Properties \n\nIn this section, for a function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, we will freely use commas to indicate concatenation in its input. For example, for $x \\in\\{0,1\\}^{i-1}$ and $y \\in\\{0,1\\}^{\\ell-i}$, we write $f(x, 1, y)$ to indicate $f$ applied to the tuple $\\left(x_{1}, \\ldots, x_{i-1}, 1, y_{1}, \\ldots, y_{\\ell-i}\\right)$.\n\nWhen asking about the influence of a single bit, such as the $i$-th bit, previous work has specifically looked at whether the $i$-th bit still has the ability to change the output of some function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$ after all other $\\ell-1$ bits have been set. In other words, if the $i$-th bit is a non-oblivious adversary (that is, it can look at the values of all the other bits before setting its own value), how much power does it have? This has led to a standard notion of influence defined below.\n\nDefinition 7.1 (Influence). For a function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, the influence of the $i$-th bit is\n\n$$\n\\mathbf{I}_{i}[f]=\\underset{\\substack{x \\sim \\mathbf{U}_{i-1} \\\\ y \\sim \\mathbf{U}_{n-i}}}{\\mathbb{E}}\\left\\|f(x, 1, y)-f(x, 0, y)\\right\\|\n$$\n\nand the total influence is\n\n$$\n\\mathbf{I}[f]=\\sum_{i=1}^{\\ell} \\mathbf{I}_{i}[f]\n$$\n\nHowever, in our setting of oNOSF sources and oNOBF sources, an adversarial bit can only depend on the bits that come before it. This motivates our new definition of online influence, where we prevent the $i$-th bit from depending on bits that come after it by independently sampling subsequent bits.\n\nDefinition 7.2 (Online influence). For a function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, the online influence of the $i$-th bit is\n\n$$\n\\mathbf{o I}_{i}[f]=\\underset{x \\sim \\mathbf{U}_{i-1}}{\\mathbb{E}}\\left[\\left|\\underset{y \\sim \\mathbf{U}_{\\ell-i}}{\\mathbb{E}}[f(x, 1, y)]-\\underset{y \\sim \\mathbf{U}_{\\ell-i}}{\\mathbb{E}}[f(x, 0, y)]\\right|\\right]\n$$\n\nand the total online influence is\n\n$$\n\\mathbf{o I}[f]=\\sum_{i=1}^{\\ell} \\mathbf{o} \\mathbf{I}_{i}[f]\n$$\n\nRemark 7.3. It is easy to see that for any $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, and any $i \\in \\ell$, we have $\\mathbf{o I}_{i}(f) \\leq$ $\\mathbf{I}_{i}(f)$. Further, they are the same for the last bit: $\\mathbf{I}_{\\ell}[f]=\\mathbf{o I}_{\\ell}[f]$.\n\nMany results for the influence of a function are based on working with monotone functions. In contrast, it turns out that monotone functions are not very interesting for online influence as the definition collapses to that of regular influence.\n\nLemma 7.4. If $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$ is monotone, then $\\mathbf{o I}_{i}[f]=\\mathbf{I}_{i}[f]$ for all $i \\in[\\ell]$.\nProof. Using the monotonicity of $f$, note that for any $x \\in\\{0,1\\}^{i-1}$ and any $y \\in\\{0,1\\}^{\\ell-i}$, $f(x, 1, y) \\geq f(x, 0, y)$. Thus, $\\mathbf{o I}_{i}[f]=\\mathbb{E}_{x \\sim \\mathbf{U}_{i-1}, y \\sim \\mathbf{U}_{i-1}}[f(x, 1, y)-f(x, 1, y)]=\\mathbf{I}_{i}(f)$.\n\nThus, any difference between influence and online influence can only be demonstrated by nonmonotone functions.", "tables": {}, "images": {}}, {"section_id": 38, "text": "# 7.2 A Poincar\u00e9 Inequality for Online Influence \n\nSimilar to regular influence, we prove a Poincar\u00e9-style inequality holds for online influence, and also provide an upper bound on online influence. The following is the main result of this subsection.\nTheorem 7.5. For any $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, we have $\\operatorname{Var}(e(f)) \\leq \\mathbf{o I}[f] \\leq \\sqrt{\\ell \\operatorname{Var}(e(f))}$.\nBefore proving the above result, we observe that the MAJORITY and PARITY functions provide tight examples for the upper and lower bound respectively for Theorem 7.5.\n\nExample 7.6. The majority function on $\\ell$ bits $\\mathrm{Maj}_{\\ell}:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, is monotone, and hence by by Lemma 7.4, has total online influence $\\mathbf{o I}\\left[\\mathrm{Maj}_{\\ell}\\right]=\\mathbf{I}\\left[\\mathrm{Ma}_{\\ell}\\right]=\\sqrt{2 \\ell / \\pi}+O(1 / \\sqrt{\\ell})$, achieving the upper bound (up to constants).\n\nThe PARITY function on $\\ell$ bits $\\bigoplus_{\\ell}:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$ for $i \\in[\\ell-1]$ has online influence $\\mathbf{o I}_{i}\\left[\\bigoplus_{\\ell}\\right]=0$, while $\\mathbf{o I}_{\\ell}\\left[\\bigoplus_{\\ell}\\right]=1$. Thus, PARITY meets the lower bound of Theorem 7.5. We note that this is starkly different from regular influence where $\\mathbf{I}_{i}\\left[\\bigoplus_{\\ell}\\right]=1$ for all $i$.\n\nTo prove Theorem 7.5, we will use Boolean Fourier analysis. For any $f:\\{0,1\\}^{n} \\rightarrow\\{0,1\\}$, $e(f)$ has a unique Fourier expansion given by: $e(f(x))=\\sum_{S \\subseteq[\\ell]} \\widetilde{f}(S) \\chi_{S}(x)$, where $\\chi_{S}(x)=(-1)^{\\sum_{i \\in S} x_{i}}$ and $\\widetilde{f}(S)=\\mathbb{E}_{y \\sim \\mathbf{U}_{\\ell}}[e(f)(y) \\chi_{S}(y)] .{ }^{8}$ Also recall that $\\widetilde{f}(\\emptyset)=\\mathbb{E}_{x \\sim \\mathbf{U}_{n}}[e(f)(x)], \\operatorname{Var}(e(f))=$ $\\sum_{S \\subseteq[\\ell], S \\neq \\emptyset} \\widetilde{f}(S)^{2}$, and for any $S \\neq T, \\mathbb{E}_{x \\sim \\mathbf{U}_{\\ell}}\\left[\\chi_{S}(x) \\chi_{T}(x)\\right]=0$. For more background, we refer the reader to the excellent book by O'Donnell [ODo14].\n\nThe following is our key lemma, from which Theorem 7.5 is easy to derive.\nLemma 7.7. For any $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$ and $i \\in[\\ell], \\mathbf{o I}_{i}(f)^{2} \\leq \\sum_{\\substack{S \\subseteq[i] \\\\ S \\ni i}} \\widetilde{f}(S)^{2} \\leq \\mathbf{o I}_{i}(f)$.\nWe first derive Theorem 7.5 using Lemma 7.7.\nProof of Theorem 7.5. We start with the lower bound. We have,\n\n$$\n\\mathbf{o I}[f]=\\sum_{i=1}^{\\ell} \\mathbf{o} \\mathbf{I}_{i}[f] \\geq \\sum_{i=1}^{\\ell} \\sum_{\\substack{S \\subseteq[i] \\\\ S \\ni i}} \\widetilde{f}(S)^{2}=\\sum_{\\substack{S \\subseteq[\\ell] \\\\ S \\neq \\varnothing}} \\widetilde{f}(S)^{2}=\\operatorname{Var}(e(f))\n$$\n\nwhere the inequality uses Lemma 7.7.\nThe upper bound is easy to derive as well.\n\n$$\n\\begin{aligned}\n\\mathbf{o I}[f] & =\\sum_{i=1}^{\\ell} \\mathbf{o} \\mathbf{I}_{i}[f] \\\\\n& \\leq \\sqrt{\\ell \\sum_{i=1}^{\\ell}\\left(\\mathbf{o} \\mathbf{I}_{i}[f]\\right)^{2}} \\\\\n& \\leq \\sqrt{\\ell \\sum_{i=1}^{\\ell} \\sum_{\\substack{S \\subseteq[i] \\\\\nS \\ni i}} \\widetilde{f}(S)^{2}}\n\\end{aligned}\n$$\n\n(Lemma 7.7)\n\n[^0]\n[^0]:    ${ }^{8}$ For simplicity of notation, we use $\\widetilde{f}(S)$ for $\\widetilde{e(f)}(S)$.\n\n$$\n=\\sqrt{\\ell \\operatorname{Var}(e(f))}\n$$\n\nThis completes the proof.\nWe now focus on proving Lemma 7.7. We need the following useful characterization of $\\mathbf{o I}_{i}(f)$.\nClaim 7.8. For any $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, we can write the online influence of its $i$-th bit as\n\n$$\n\\mathbf{o I}_{i}[f]=\\underset{x \\sim \\widehat{\\mathbf{U}}_{i-1}}{\\mathbb{E}}\\left[\\left|\\sum_{\\substack{T \\subseteq[i] \\\\ T \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)\\right|\\right]\n$$\n\nAssuming the above claim, let us prove Lemma 7.7. We supply the proof of Claim 7.8 below.\nProof of Lemma 7.7. We first prove the inequality $\\mathbf{o I}_{i}(f) \\geq \\sum_{\\substack{S \\subseteq[i] \\\\ S \\ni i}} \\widehat{f}(S)^{2}$. Since for any $x \\in\\{0,1\\}^{i-1}$ we have $\\left[\\mathbb{E}_{y \\sim \\mathbf{U}_{\\ell-i}}\\left[e\\left(f \\mid x, 1\\right)(y)\\right]-\\mathbb{E}_{y \\sim \\mathbf{U}_{\\ell-i}}\\left[e\\left(f \\mid x, 0\\right)(y)\\right]\\right]=2\\left|\\sum_{\\substack{T \\subseteq[i] \\\\ T \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)\\right|$ by Claim 7.8, and the fact that $\\mathbb{E}_{y \\sim \\mathbf{U}_{\\ell-i}}\\left[e\\left(f \\mid x, b\\right)(y)\\right]$ is in $[-1,1]$ for all $x \\in\\{0,1\\}^{i-1}, b \\in\\{0,1\\}$, it follows that $\\left|\\sum_{\\substack{T \\subseteq[i] \\\\ T \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)\\right|$ is in $[0,1]$.\n\nThus,\n\n$$\n\\begin{aligned}\n\\mathbf{o I}_{i}[f] & =\\underset{x \\sim \\widehat{\\mathbf{U}}_{i-1}}{\\mathbb{E}}\\left[\\left|\\sum_{\\substack{T \\subseteq[i] \\\\\nT \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)\\right|\\right] \\\\\n& \\geq \\underset{x \\sim \\widehat{\\mathbf{U}}_{i-1}}{\\mathbb{E}}\\left[\\left(\\sum_{\\substack{T \\subseteq[i] \\\\\nT \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)\\right)^{2}\\right] \\\\\n& =\\sum_{\\substack{T \\subseteq[i] \\\\\nT \\ni i}} \\sum_{\\substack{S \\subseteq[i] \\\\\nS \\ni i}} \\widehat{f}(T) \\widehat{f}(S) \\cdot \\underset{x \\sim \\widehat{\\mathbf{U}}_{i-1}}{\\mathbb{E}}\\left[\\chi_{T \\backslash\\{i\\}}(x) \\chi_{S \\backslash\\{i\\}}(x)\\right] \\\\\n& =\\sum_{\\substack{S \\subseteq[i] \\\\\nS \\ni i}} \\widehat{f}(S)^{2}\n\\end{aligned}\n$$\n\nNext, we prove $\\mathbf{o I}_{i}(f)^{2} \\leq \\sum_{\\substack{S \\subseteq[i] \\\\ S \\ni i}} \\widehat{f}(S)^{2}$. We have,\n\n$$\n\\mathbf{o I}_{i}[f]^{2}=\\left(\\underset{x \\sim \\widehat{\\mathbf{U}}_{i-1}}{\\mathbb{E}}\\left[\\left|\\sum_{\\substack{T \\subseteq[i] \\\\ T \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)\\right|\\right]\\right)^{2} \\quad(\\text { Claim 7.8) }\n$$\n\n$$\n\\begin{aligned}\n& \\leq \\underset{x \\sim \\widehat{\\mathbf{U}}_{i-1}}{\\mathbb{E}}\\left[\\left(\\sum_{\\substack{T \\subseteq[i] \\\\\nT \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)\\right)^{2}\\right] \\\\\n& =\\sum_{\\substack{S \\subseteq[i] \\\\\nS \\ni i}} \\widehat{f}(S)^{2}\n\\end{aligned}\n$$\n\n(derived above).\nNext, we show how to rewrite $\\mathbf{o I}_{i}[f]$ in terms of the Fourier coefficients of $f$.\nProof of Claim 7.8. We begin by defining the restriction $\\left.f\\right|_{x, b}(y)=f(x, b, y)$ for $x \\in\\{0,1\\}^{i-1}$, $b \\in\\{0,1\\}$, and $y \\in\\{0,1\\}^{\\ell-i}$. Thus, we can rewrite $\\mathbf{o I}_{i}[f]$ as\n\n$$\n\\mathbf{o I}_{i}[f]=\\frac{1}{2} \\cdot \\underset{x \\sim \\widehat{\\mathbf{U}}_{i-1}}{\\mathbb{E}}\\left[\\left|\\underset{y \\sim \\widehat{\\mathbf{U}}_{\\ell-i}}{\\mathbb{E}}\\left|e\\left(f\\right|_{x, 1}\\right)(y)\\right|-\\underset{y \\sim \\widehat{\\mathbf{U}}_{\\ell-i}}{\\mathbb{E}}\\left|e\\left(f\\right|_{x, 0}\\right)(y)\\right|\\right]\\right]\n$$\n\nWe would like to put the above expression in terms of Fourier coefficients of $f$. This motivates us to find the Fourier coefficients of $\\left.f\\right|_{x, b}(y)$ in terms of those of $f$, which we do via computation. We manipulate the Fourier expansion of $f(z)$ for $z=(x, b, y) \\in\\{0,1\\}^{\\ell}$ to get\n\n$$\n\\begin{aligned}\ne(f)(z) & =\\sum_{S \\subseteq[\\ell]} \\widehat{f}(S) \\chi_{S}(z) \\\\\n& =\\sum_{S \\subseteq[\\ell]} \\widehat{f}(S) \\chi_{S}(x, b, y) \\\\\n& =\\sum_{S \\subseteq[\\ell]} \\widehat{f}(S) \\chi_{S \\cap[i]}(x, b) \\chi_{S \\backslash[i]}(y) \\\\\n& =\\sum_{S \\subseteq\\{i+1, \\ldots, \\ell\\}}\\left(\\sum_{T \\subseteq[i]} \\widehat{f}(S \\cup T) \\chi_{T}(x, b)\\right) \\chi_{S}(y)\n\\end{aligned}\n$$\n\nWe also have that\n\n$$\n\\begin{aligned}\ne(f)(z) & =e(f)(x, b, y) \\\\\n& =e\\left(\\left.f\\right|_{x, b}\\right)(y) \\\\\n& =\\sum_{S \\subseteq\\{i+1, \\ldots, \\ell\\}} \\widehat{f\\left.\\right|_{x, b}(S) \\chi_{S}(y)}\n\\end{aligned}\n$$\n\nTherefore, Equation (2) and Equation (3) allow us to conclude that\n\n$$\n\\widehat{f\\left.\\right|_{x, b}(S)}=\\sum_{T \\subseteq[i]} \\widehat{f}(S \\cup T) \\chi_{T}(x, b)\n$$\n\nThus, we have\n\n$$\n\\begin{aligned}\n\\underset{y \\sim \\widehat{\\mathbf{U}}_{\\ell-i}}{\\mathbb{E}}\\left|e\\left(f\\right|_{x, b}\\right)(y)| & =\\widehat{f\\left.\\right|_{x, b}(\\varnothing)} \\\\\n& =\\sum_{T \\subseteq[i]} \\widehat{f}(T) \\chi_{T}(x, b)\n\\end{aligned}\n$$\n\n$$\n=\\sum_{\\substack{T \\subseteq[i] \\\\ T \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x) b+\\sum_{T \\subseteq[i-1]} \\widehat{f}(T) \\chi_{T}(x)\n$$\n\nWe now plug this in to our definition of $\\mathbf{o I}_{i}[f]$ in Equation (1) to get a simplified expression. Recalling the fact that for any $x \\in\\{0,1\\}^{n}, f(x)=(1-e(f)(x)) / 2$, we have\n\n$$\n\\begin{aligned}\n\\mathbf{o I}_{i}[f] & =\\frac{1}{2} \\underset{x \\sim \\mathbf{U}_{i-1}}{\\mathbb{E}}\\left[\\left|\\underset{y \\sim \\mathbf{U}_{\\ell-i}}{\\mathbb{E}}\\left[e\\left(f \\mid x, 1\\right)(y)\\right]-\\underset{y \\sim \\mathbf{U}_{\\ell-i}}{\\mathbb{E}}\\left[e\\left(f \\mid x, 0\\right)(y)\\right]\\right|\\right] \\\\\n& =\\frac{1}{2} \\underset{x \\sim \\mathbf{U}_{i-1}}{\\mathbb{E}}\\left[\\left\\lvert\\,\\left(-\\sum_{\\substack{T \\subseteq[i] \\\\\nT \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)+\\sum_{T \\subseteq[i-1]} \\widehat{f}(T) \\chi_{T}(x)\\right)-\\left(\\sum_{\\substack{T \\subseteq[i] \\\\\nT \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{b\\}}(x)+\\sum_{T \\subseteq[i-1]} \\widehat{f}(T) \\chi_{T}(x)\\right)\\right] \\mid\\right] \\\\\n& =\\underset{x \\sim \\mathbf{U}_{i-1}}{\\mathbb{E}}\\left[\\left\\lvert\\, \\sum_{\\substack{T \\subseteq[i] \\\\\nT \\ni i}} \\widehat{f}(T) \\chi_{T \\backslash\\{i\\}}(x)\\right.\\right]\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 39, "text": "# 7.3 A Tight Example for Maximum Online Influence \n\nThe lower bound on total online influence from Theorem 7.5 allows us to conclude that for balanced functions, there must be at least one bit with online influence $\\Omega(1 / \\ell)$. We can phrase this in terms of maximum influence.\n\nDefinition 7.9 (Maximum influence). For a function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, we define its maximum influence as $\\mathbf{I}_{\\max }[f]=\\max _{i \\in[\\ell]} \\mathbf{I}_{i}[f]$ and its maximum online influence as $\\mathbf{o I}_{\\max }[f]=\\max _{i \\in[\\ell]} \\mathbf{o I}_{i}[f]$.\n\nIn terms of maximum online influence, we get the following corollary from Theorem 7.5.\nCorollary 7.10. For a function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, we have $\\mathbf{o I}_{\\max }[f] \\geq \\operatorname{Var}(e(f)) / \\ell$.\nProof. By Theorem 7.5 we have that $\\mathbf{o I}[f]=\\sum_{i=1}^{\\ell} \\mathbf{o I}_{i}[f] \\geq \\operatorname{Var}(e(f))$, and the conclusion follows via an averaging argument.\n\nWe show that the bound in Corollary 7.10 is in fact tight (up to constants), as witnessed by the address function.\n\nDefinition 7.11. We define the address function $\\operatorname{Addr}_{\\ell}:\\{0,1\\}^{\\log (\\ell)+\\ell} \\rightarrow\\{0,1\\}$ as follows: For $z \\in\\{0,1\\}^{\\log (\\ell)+\\ell}$, split $z$ up as $z=(x, y)$ with $x$ of length $\\log (\\ell)$ and $y$ of length $\\ell$. Then interpret $x$ as a binary number which gives us an index $i(x) \\in[\\ell]$. The output of $\\operatorname{Addr}_{\\ell}$ is the $i(x)$-th bit of $y$, so $\\operatorname{Addr}_{\\ell}(x, y)=y_{i(x)}$.\n\nLemma 7.12. Let $m=\\ell+\\log \\ell$ and $\\operatorname{Addr}_{\\ell}$ be the function defined above. Then,\n\n- for $1 \\leq i \\leq \\log \\ell, \\mathbf{o I}_{i}\\left[\\operatorname{Addr}_{\\ell}\\right]=0$.\n- for $\\log \\ell<i \\leq m, \\mathbf{o I}_{i}\\left[\\operatorname{Addr}_{\\ell}\\right]=1 / \\ell$.\n\nThus, $\\mathbf{o I}_{\\max }\\left(\\operatorname{Addr}_{\\ell}\\right)=\\Theta(1 / m)$.\n\nProof. For $i \\in[\\log \\ell]$, no matter what the value of the $i$-th bit of $\\operatorname{Addr}_{\\ell}$ is set to, the output bit will be a uniform bit, so we immediately get that $\\mathbf{o} \\mathbf{I}_{i}[f]=0$. For $i \\in\\{\\log \\ell+1, \\ldots, m\\}$, the $i$-th bit only has control if it's selected by the first $\\log \\ell$ address bits, meaning it has a $1 / \\ell$ chance of controlling the output (and otherwise the output is uniform). Hence, $\\mathbf{o} \\mathbf{I}_{i}[f]=\\frac{1}{\\ell}$.\n\nCompared with the result of [KKL88] that $\\mathbf{I}_{\\max }[f] \\geq \\operatorname{Var}(f) \\cdot \\Omega\\left(\\frac{\\log \\ell}{\\ell}\\right)$, this exhibits a separation between maximum (standard) influence and the online influence (of balanced functions).\n\nMoreover, this analysis of the address function also shows us that it is an extractor for uniform $(\\ell-1, \\ell)$-oNOSF sources.\n\nLemma 7.13. For all $\\ell, n$ where $\\ell \\geq 2$ and $n \\geq \\log (\\ell-1)$, there exists an explicit extractor Ext : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{n}$ such that for any uniform $(\\ell-1, \\ell, n)$-oNOSF source $\\mathbf{X}$, we have $\\operatorname{Ext}(\\mathbf{X}) \\approx_{\\varepsilon} \\mathbf{U}_{n}$ where $\\varepsilon=\\frac{1}{\\ell-1}$.\n\nProof. Let Ext be defined as follows: From the first block, use the first $\\log (\\ell-1)$ bits and interpret them as an index $j \\in[\\ell-1]$. Then, output the block with index $j+1$. For a source $\\mathbf{X}$ with first block controlled by an adversary, the output will be truly uniform and for a source $\\mathbf{X}$ with adversary controlling one of the last $\\ell-1$ blocks, that block will be outputted with probability $\\frac{1}{\\ell-1}$ while a uniform block will be outputted otherwise. This makes our total error at most $\\frac{1}{\\ell-1}$ as desired.", "tables": {}, "images": {}}, {"section_id": 40, "text": "# 7.4 Online Influence of Sets and Extraction Lower Bounds \n\nFor convenience we restate the definition of online influence of sets of coordinates.\nDefinition 7.14 (Online influence, Definition 1.7 restated). For any function $f: \\Sigma^{\\ell} \\rightarrow\\{0,1\\}$, and any $B \\subset[\\ell]$, where $B=\\left\\{i_{1}<i_{2}<\\ldots<i_{k}\\right\\}$, define $\\mathbf{o} \\mathbf{I}_{B}(f)$ as follows: an online adversary $\\mathcal{A}$ samples a distribution $\\mathbf{X}$ in online manner. It starts by sampling the variables $x_{1}, x_{2}, \\ldots, x_{i_{1}-1}$ independently and uniformly from $\\Sigma$, then picking the value of $x_{i_{1}}$ depending on $x_{<i_{1}}$. Next, the variables $x_{i_{1}+1}, \\ldots, x_{i_{2}-1}$ are sampled independently and uniformly from $\\Sigma$, and $\\mathcal{A}$ sets the value of $x_{i_{2}}$ based on all set variables so for, and so on. Define the advantage of $\\mathcal{A}$ to be $a d v_{f, B}(\\mathcal{A})=$ $\\left|\\mathbb{E}[f(\\mathbf{X})]-\\mathbb{E}\\left[f\\left(\\mathbf{U}_{\\ell}\\right)\\right]\\right|$. Then $\\mathbf{o} \\mathbf{I}_{B}(f)$ is defined to be $\\max _{\\mathcal{A}}\\left\\{a d v_{f, B}(\\mathcal{A})\\right\\}$, where the maximum is taken over all online adversaries $\\mathcal{A}$ that control the bits in $B$.\n\nWe say a function $f$ is $(b, \\varepsilon)$-online-resilient if $\\mathbf{o} \\mathbf{I}_{B}(f) \\leq \\varepsilon$ for every $B$ of size at most $b$.\nIn the special case where $\\Sigma=\\{0,1\\}$ and we are considering the online influence of a single coordinate, the definition simplifies nicely.\n\nDefinition 7.15. For a function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$, the online influence of the $i$-th bit is\n\n$$\n\\mathbf{o} \\mathbf{I}_{i}[f]=\\underset{x \\sim \\mathbf{U}_{i-1}}{\\mathbb{E}}\\left[\\left|\\underset{y \\sim \\mathbf{U}_{\\ell-i}}{\\mathbb{E}}[f(x, 1, y)]-\\underset{y \\sim \\mathbf{U}_{\\ell-i}}{\\mathbb{E}}[f(x, 0, y)]\\right|\\right]\n$$\n\nand the total online influence is\n\n$$\n\\mathbf{o} \\mathbf{I}[f]=\\sum_{i=1}^{\\ell} \\mathbf{o} \\mathbf{I}_{i}[f]\n$$\n\nOnline-resilient functions are equivalent to extractors (with 1 output bit) for oNOSF sources.\n\nLemma 7.16 (online-resilient functions yield extractors). Let $f: \\Sigma^{\\ell} \\rightarrow\\{0,1\\}$ be a $\\left(b, \\varepsilon_{1}\\right)$-onlineresilient function with the property that $\\left|f\\left(\\mathbf{U}_{\\ell}\\right)-\\mathbf{U}_{1}\\right| \\leq \\varepsilon_{2}$. Then $f$ can extract from $(g=\\ell-b, \\ell)$ oNOSF sources with error at most $\\varepsilon_{1}+\\varepsilon_{2}$.\n\nProof. Consider a $(g=\\ell-b, \\ell)$-oNOSF source $\\mathbf{X}$. Recall that $\\mathbf{X}$ is created by choosing some set of bad indices $B$ of size $b$, letting the symbols in $\\bar{B}$ be uniform, and finally setting the symbols in $B$ adversarially while only depending on uniform symbols to the left of them. Using the triangle inequality for total variation distance, we get that\n\n$$\n\\begin{aligned}\n\\left|f(\\mathbf{X})-\\mathbf{U}_{1}\\right| & \\leq\\left|f(\\mathbf{X})-f\\left(\\mathbf{U}_{\\ell}\\right)\\right|+\\left|f\\left(\\mathbf{U}_{\\ell}\\right)-\\mathbf{U}_{1}\\right| \\\\\n& \\leq \\varepsilon_{1}+\\varepsilon_{2}\n\\end{aligned}\n$$\n\nas claimed.\nRemark 7.17. We note that the other direction is immediate from definitions. If Ext : $\\Sigma^{\\ell} \\rightarrow\\{0,1\\}$ is an extractor with error $\\varepsilon$ for $(g=\\ell-b, \\ell)$-oNOSF sources, then Ext is a $(b, 2 \\varepsilon)$-online-resilient function.\n\nRemark 7.18. Our results below on oNOBF extraction impossibility can be interpreted as a limit on online-resilience of balanced Boolean functions.\n\nFor $B \\subset[\\ell]$, we use the notation $\\left.f\\right|_{\\bar{B}}$ to indicate the function obtained from $f$ by letting an online adversary control the indices in $B$.\n\nTheorem 7.19. Let $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$ be such that $\\mathbb{E}_{x \\sim \\mathbf{U}_{\\ell}}[f(x)=1]=\\alpha$. Then for any $1 \\geq \\beta>\\alpha$, there exists a coalition $B \\subseteq[\\ell]$ such that $\\mathbf{o} \\mathbf{I}_{B}(f) \\geq \\beta-\\alpha$, where $|B| \\leq \\gamma \\ell$ and $\\gamma=\\frac{\\beta-\\alpha}{4 \\alpha(1-\\beta)}$.\nProof. We greedily collect the bits with the most online influence and add them to $B$ until our goal of $\\mathbb{E}_{x \\sim \\mathbf{U}_{\\ell} \\mid \\bar{B}}[f \\mid \\bar{B}(x)=1] \\geq \\beta$ is achieved. Our first step is as follows: let $B_{0}=\\varnothing, f_{0}=f$, and $i_{1}=\\operatorname{argmax}_{i \\in[\\ell]}\\left\\{\\mathbf{o} \\mathbf{I}_{i}[f]\\right\\}$. Corollary 7.10 tells us that $\\mathbf{o} \\mathbf{I}_{i_{1}} \\geq \\operatorname{Var}\\left(e\\left(f_{0}\\right)\\right) / \\ell$. Recall that if $\\mathbb{E}_{x \\sim \\mathbf{U}_{\\ell}}[f(x)=1]=p$ then $\\operatorname{Var}(e(f))=4 p(1-p)$. Because we have not yet achieved our goal of $\\mathbb{E}_{x \\sim \\mathbf{U}_{\\ell} \\mid \\bar{B}}[f \\mid \\bar{B}(x)=1] \\geq \\beta$, we have that $\\operatorname{Var}\\left(f_{0}\\right) \\geq 4 \\alpha(1-\\beta)$. Thus, we collect $i_{1}$ as $B_{1}=\\left\\{i_{1}\\right\\}$, let $f_{1}=\\left.f_{0}\\right|_{\\bar{B}_{1}}$ and see that $\\mathbb{E}_{x}\\left[f_{1}(x)\\right] \\geq \\mathbb{E}_{x}\\left[f_{0}(x)\\right]+\\mathbf{o} \\mathbf{I}_{i_{1}}\\left[f_{0}\\right] \\geq \\alpha+\\frac{4 \\alpha(1-\\beta)}{\\ell}$.\n\nWe now repeat this process $t$ times to get $B_{t}=\\left\\{i_{1}, \\ldots, i_{t}\\right\\}$ until our goal is achieved. For general $t$, let $f_{t}=\\left.f\\right|_{B_{t}}$ where $B_{t}=B_{t-1} \\cup\\left\\{i_{t}\\right\\}$ and $i_{t}=\\operatorname{argmax}_{i \\in[n] \\backslash B_{t-1}}\\left\\{\\mathbf{o} \\mathbf{I}_{i}\\left[f_{t-1}\\right]\\right\\}$. At the $(t-1)$-th step, since we have not stopped, it means that $\\mathbb{E}_{x}\\left[f_{t-1}(x)=1\\right]<\\beta$, but we of course have $\\mathbb{E}_{x}\\left[f_{t-1}(x)=1\\right] \\geq \\alpha$ as well. Thus, by Corollary 7.10 , collecting $i_{t}$ as a bad bit gives us that\n\n$$\n\\begin{aligned}\n\\mathbb{E}[f_{t}(x)] & \\geq \\mathbb{E}\\left[f_{t-1}(x)\\right]+\\mathbf{o} \\mathbf{I}_{i_{t}}\\left[f_{t-1}\\right] \\\\\n& \\geq \\alpha+\\frac{4 \\alpha(1-\\beta)}{\\ell}(t-1)+\\frac{4 \\alpha(1-\\beta)}{\\ell} \\\\\n& =\\alpha+\\frac{4 \\alpha(1-\\beta)}{\\ell} \\cdot t\n\\end{aligned}\n$$\n\nWe repeat this process until $\\operatorname{Pr}_{x}\\left[f_{t}(x)=1\\right] \\geq \\beta$. Therefore, the number of steps is the smallest $b$ such that $\\alpha+\\frac{4 \\alpha(1-\\beta)}{\\ell} \\cdot b \\geq \\beta$, meaning that the number of steps is at most $b \\leq \\ell \\cdot \\frac{\\beta-\\alpha}{4 \\alpha(1-\\beta)}$. We let $B=B_{b}$ and get the desired coalition.\n\nWe can also ask the dual question of how large we are able to make $\\beta$ given some budget $b$ of bad bits.\n\nCorollary 7.20. Let $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$ be such that $\\operatorname{Pr}_{x \\sim \\mathbf{U}_{\\ell}}[f(x)=1] \\geq \\alpha$. If we are able to control $b$ bits in an online adversarial manner, then there exists a set $B \\subseteq[\\ell]$ of indices of size $|B|=b$ such that $\\operatorname{Pr}_{x \\sim \\mathbf{U}_{\\ell} \\mid \\bar{B}}|f|_{\\bar{B}}(x)=1] \\geq \\beta$ where $\\beta \\geq \\frac{\\alpha(\\ell+4 b)}{\\ell+4 \\alpha b}$.\n\nProof. For a fixed $\\beta$, Theorem 7.19 tells us that $b \\leq \\ell \\cdot \\frac{\\beta-\\alpha}{4 \\alpha(1-\\beta)}$. Solving for $\\beta$ gives the desired bound.\n\nWe now immediately obtain our oNOBF extraction impossibility result.\nCorollary 7.21. For any balanced function $f:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$ and $0<\\varepsilon<1 / 3$, there exists a $(g=\\ell-b, \\ell)-o N O B F$ source $\\mathbf{X}$ with $b \\leq 3 \\varepsilon \\ell$ such that $|f(\\mathbf{X})-\\mathbf{U}_{1}| \\geq \\varepsilon$.\n\nProof. It is enough to find a set $B$ of indices such that $\\mathbf{o I}_{B}(f) \\geq \\beta$. By Theorem 7.19, there exists such a set $B$ of size $b=|B| \\leq \\ell \\cdot \\frac{\\varepsilon}{1-2 \\varepsilon}$. The bound on $|B|$ follows since $\\varepsilon \\leq \\frac{1}{3}$.\n\nRemark 7.22. By essentially following our Fourier analytic proof, one can similarly obtain a Poincar\u00e9 inequality for functions $f: \\Sigma^{n} \\rightarrow\\{0,1\\}$, for arbitrary alphabet $\\Sigma$. To obtain extraction impossibility for such uniform oNOSF sources with constant $\\delta$ fraction of corrupt blocks, we do the following: Let $f$ be a candidate extractor for uniform $((1-\\delta) \\ell, \\ell, n)$-oNOSF sources. Then, $f$ also extracts from uniform $(\\lceil1 / \\delta\\rceil-1,\\lceil 1 / \\delta\\rceil, \\ln /\\lceil 1 / \\delta\\rceil)$-oNOSF source. Since there exists an influential coordinate with influence $O(\\delta)$, we let the adversary control that coordinate and infer that there exists constant $\\varepsilon=O(\\delta)$ for which it is impossible to extract with error less than $\\varepsilon$.", "tables": {}, "images": {}}, {"section_id": 41, "text": "# 8 Extractors for oNOSF and oNOBF Sources via Leader Election Protocols \n\nIn this section, we provide a generic way to transform leader election and coin flipping protocols into extractors for oNOSF sources and oNOBF sources. Conceptually, given a leader election protocol, we can use an oNOSF source to simulate the protocol and then have the elected leader output its last block. We formalize this below.\n\nLemma 8.1. For any integers $r>1, \\ell>0$ and any $\\delta>0$, let $\\pi$ be an $(r-1)$-round protocol over $\\ell$ players that send $n$ bits per round such that for any $\\delta \\ell$ bad players, the protocol elects a good leader with probability $1-\\varepsilon$.\n\nThen, there exists an explicit function Ext $:\\left(\\{0,1\\}^{n}\\right)^{\\ell r} \\rightarrow\\{0,1\\}^{n}$ such that for any $(g, \\ell r, n)-$ oNOSF source $\\mathbf{X}$ where $g \\geq \\ell r-\\delta \\ell$, we have $\\operatorname{Ext}(\\mathbf{X}) \\approx_{\\varepsilon} \\mathbf{U}_{n}$.\n\nInstantiating our lemmas with the leader election protocols from Section 9, we construct explicit extractors for oNOBF sources and uniform oNOSF sources:\n\nTheorem 8.2. There exists an explicit function Ext $:\\{0,1\\}^{\\ell} \\rightarrow\\{0,1\\}$ such that for any $\\delta$ and any $(g, \\ell)$-oNOBF source $\\mathbf{X}$ where $g \\geq \\ell-\\delta \\ell / \\log (\\ell)$, we have $\\operatorname{Ext}(\\mathbf{X}) \\approx_{\\varepsilon} \\mathbf{U}_{1}$ where $\\varepsilon=C \\delta+$ $12(C \\delta)^{3 / 2}+\\log (\\ell)^{-1 / 3}$ where $C$ is a large universal constant.\n\nProof. This directly follows by instantiating Lemma 8.1 with the protocol guaranteed from Lemma 9.1.\n\nBy using a the leader election protocol of Lemma 9.5 with multiple bits per round, we construct extractors for oNOSF sources:\n\nTheorem 8.3. There exists an explicit function Ext : $\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{n}$ such that for any constant $\\delta$ and any $(g, \\ell, n)$-oNOSF source $\\mathbf{X}$ where $g \\geq \\ell-\\delta \\ell / \\log ^{*}(\\ell)$ and $n \\geq \\log (\\ell)$, we have Ext $(\\mathbf{X}) \\approx_{\\varepsilon} \\mathbf{U}_{n}$ where $\\varepsilon=C \\delta+13(C \\delta)^{3 / 2}$.\n\nProof. This directly follows by instantiating Lemma 8.1 with the protocol guaranteed from Lemma 9.5.\n\nWe finally prove our lemma regarding obtaining extractors for oNOSF Sources from leader election protocols:\n\nProof of Lemma 8.1. Define function Ext as follows: On input $\\left(y_{1}, \\ldots, y_{r}\\right)$ where $y_{i} \\in\\left(\\{0,1\\}^{n}\\right)^{\\ell}$, let $y_{i, j} \\in\\{0,1\\}^{n}$ denote the $j$ 'th block of $y_{i}$. Simulate the protocol $\\pi$ with the message of the $j$ 'th player in round $i$ being $y_{i, j}$, where $1 \\leq i \\leq r-1$ and $1 \\leq j \\leq \\ell$. Let $j^{*} \\in[\\ell]$ be the leader that is elected by $\\pi$; then output $y_{r, j^{*}}$\n\nLet us analyze Ext on some source $\\mathbf{Y} \\sim\\left(\\{0,1\\}^{n}\\right)^{\\ell r}$. Let the bad symbols in $\\mathbf{Y}$ be given by $A \\subset[\\ell] \\times[r]$ where $|A| \\leq \\delta \\ell$. Let $\\mathbf{X} \\sim\\left(\\left(\\{0,1\\}^{n}\\right)^{\\ell}\\right)^{r}$ be the exact same source as $\\mathbf{Y}$. We write $\\mathbf{X}=\\left\\{\\mathbf{X}_{i, j}\\right\\}_{1 \\leq i \\leq r, 1 \\leq j \\leq \\ell}$ and interpret it as the distribution where $\\mathbf{X}_{i, j}$ denotes the random bits of player $j$ in round $i$. Call $\\mathbf{X}_{i, j}$ a bad block if the corresponding index $(i, j)$ is in $A$, i.e., the block is bad in $\\mathbf{Y}$. Since a bad block in $\\mathbf{Y}$ can only depend on blocks before it, the corresponding bad block in $\\mathbf{X}$ satisfies the criteria for being bad in $\\mathbf{X}$; this is because a bad block in the protocol setting is allowed to depend on all blocks in the same or previous rounds. Thus $\\mathbf{X}$ has at most $\\delta \\ell$ bad blocks as well. By declaring the player corresponding to the bad block in $\\mathbf{X}$ as bad, we obtain that the distribution $\\mathbf{X}$ can be simulated by at most $\\delta \\ell$ bad players. Formally, for $1 \\leq i \\leq r$, let $B_{i} \\subset[\\ell]$ be the set of bad blocks in $\\mathbf{X}$ among all blocks in round $i$. Let $B=\\cup_{i=1}^{r} B_{i}$. We declare all players in $B$ as bad players. Finally, observe that\n\n$$\n|B| \\leq \\sum_{i=1}^{r}\\left|B_{i}\\right|=|A|=\\delta \\ell\n$$\n\nas desired. Thus the correctness of $\\pi$ implies that after $(r-1)$ rounds, the chosen leader $j^{*}$ does not belong to $B$ with probability at least $1-\\varepsilon$. By construction, it follows that $\\left(r, j^{*}\\right) \\notin A$ whenever $j^{*} \\notin B$. Thus, the output of the extractor, $\\mathbf{Y}_{r, j *}$ is uniform on $n$ bits, with probability at least $1-\\varepsilon$.", "tables": {}, "images": {}}, {"section_id": 42, "text": "# 9 High Probability Leader Election Protocols \n\nWe use this section to provide the leader election protocols that are used in Section 8. In Section 9.1, we present leader protocols where each player is allowed to send one bit per round. We tackle the case where players can send multiple bits per round in Section 9.2.", "tables": {}, "images": {}}, {"section_id": 43, "text": "### 9.1 One Bit per Round\n\nWe will construct leader election protocols with the following guarantees:\n\nLemma 9.1. There exists a universal constant $C$ and an explicit protocol over $\\ell$ players, where each player sends $n=1$ bit per round, that lasts for $C \\log (\\ell)$ rounds such that for any $\\delta>0$, if $\\delta \\ell$ players are bad, then a good leader is chosen with probability $\\geq 1-\\varepsilon$ where $\\varepsilon=\\delta+12 \\delta^{3 / 2}+\\log (\\ell)^{-1 / 3}$.\n\nWe will use the following protocol from [AN93a]:\nLemma 9.2. There exists a protocol $\\pi$ over $\\ell$ players where each player sends at most 1 bit per round, that lasts for $O(\\ell)$ rounds such that if $\\delta \\ell$ players are bad for $\\delta \\leq 1 / 4$, then a good leader is chosen with probability $\\geq 1-\\varepsilon$ where $\\varepsilon=\\delta+12 \\delta^{3 / 2}$. Furthermore, this protocol can be explicitly constructed in time $2^{O(\\ell)}$.\n\nWe will also need the Chernoff bound:\nLemma 9.3. Let $\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{n}$ be independent random variables taking values in $\\{0,1\\}$. Let $\\mathbf{X}=$ $\\sum_{i=1}^{n} \\mathbf{X}_{i}$ and let $\\mu=\\mathbb{E}[\\mathbf{X}]$. Then, $\\operatorname{Pr}\\left[\\mathbf{X} \\leq(1-\\delta) \\mu\\right] \\leq e^{-\\delta^{2} \\mu / 2}$.\n\nProof of Lemma 9.1. Our protocol will have two stages. In the first stage, we will use the lightest bin protocol from [Fei99b] until the number of players is small enough, and then in the second stage we use the protocol from Lemma 9.2. Let $C_{0}$ be a large constant that we set later. In particular, our final protocol will be:\n\n1. Let $P_{1}=[\\ell]$.\n2. In round $i$ of stage 1 , all players in $P_{i}$ will present their value in $\\{0,1\\}$ and based on that, they will be divided into $P_{i}^{0}, P_{i}^{1}$.\n3. Set $P_{i+1}$ equal to the smaller set among $P_{i}^{0}, P_{i}^{1}$ (breaking ties arbitrarily).\n4. Repeat this until the number of players becomes at most $C_{0} \\log \\ell$. Let this happens after $r$ rounds. This marks the end of the first stage.\n5. In the second stage, apply the protocol from Lemma 9.2 to $P_{r+1}$ and output the leader from that protocol.\n\nWe now analyze this protocol. We argue that at the end of the first stage, with high probability, the fraction of good players in $P_{r+1}$ will be at least $(1-\\delta)-o(1)$. For the second stage, the correctness of the protocol follows from Lemma 9.2.\n\nFor $1 \\leq i \\leq r+1$, let $g_{i}$ be the number of good players in $P_{i}$ and let $p_{i}=\\left|P_{i}\\right|$. As we always choose the lightest bin at each stage, $p_{i+1} \\leq p_{i} / 2$. Hence, we infer that $p_{i} \\leq 2^{-i+1} \\cdot \\ell$. Let $g_{1}=g$. We next lower bound $g_{i}$ :\nClaim 9.4. With probability at least $1-\\exp \\left(-(1 / 10) \\cdot\\left(g / 2^{r}\\right)\\right)$, it holds that for all $1 \\leq i \\leq r+1$, $g_{i} \\geq \\frac{g}{2^{i}}-5\\left(\\frac{g}{2^{i}}\\right)^{2 / 3}$.\n\nWe prove this claim using concentration bounds later. Using this claim, we see that in $P_{r+1}$, the number of good players will be at least\n\n$$\n\\frac{(1-\\delta) \\ell}{2^{r}}-5\\left(\\frac{(1-\\delta) \\ell}{2^{r}}\\right)^{2 / 3}\n$$\n\nout of $p_{r+1} \\leq \\frac{\\ell}{2^{r}}$ many surviving players. In particular, $g_{r+1} \\geq(1-\\delta) p_{r+1}-5 p_{r+1}^{2 / 3}$. So, in stage 2, we have $p_{r+1}$ many players remaining where the fraction of bad players is $\\delta^{\\prime}=\\delta+5 p_{r+1}^{-1 / 3}$. Applying Lemma 9.2 with these parameters, we infer that probability of electing a good leader is at least\n\n$$\n1-\\left(\\delta+5 p_{r+1}^{-1 / 3}+12\\left(\\delta+5 p_{r+1}^{-1 / 3}\\right)^{3 / 2}\\right) \\geq 1-\\delta-12 \\delta^{3 / 2}-6 p_{r+1}^{-1 / 3}\n$$\n\nwhere the last inequality follows because $p_{r+1} \\geq \\omega(1)$. Hence, our overall probability of electing a good leader is at least\n\n$$\n1-\\delta-12 \\delta^{3 / 2}-6 p_{r+1}^{-1 / 3}-\\exp (-(1 / 10) \\cdot(1-\\delta) p_{r+1}) \\geq 1-\\delta-12 \\delta^{3 / 2}-\\log (\\ell)^{-1 / 3}=1-\\varepsilon\n$$\n\nwhere the last inequality follows because we let $p_{r+1}=C_{0} \\log (\\ell)$ for a large constant $C_{0}$. We check that the number of rounds in the first stage is no more than $\\log (\\ell)$ and in stage 2 , as guaranteed by Lemma 9.2, the number of rounds is no more than $O(\\log (\\ell))$. These together give us our universal constant $C$ that we use in the claim.\n\nProof of Claim 9.4. Fix either of the two bins. We apply Lemma 9.3 with $\\delta=\\mu^{-1 / 3}$ to infer that with probability at least $1-\\exp \\left(-\\left(g_{i} / 2\\right)^{1 / 3} / 2\\right)$, it holds that the number of good players in that bin is $\\geq g_{i} / 2-\\left(g_{i} / 2\\right)^{2 / 3}$. Applying this to both bins, we infer that with probability at least $1-2 \\exp \\left(-\\left(g_{i} / 2\\right)^{1 / 3} / 2\\right)$, it holds that $g_{i+1} \\geq g_{i} / 2-\\left(g_{i} / 2\\right)^{2 / 3}$. By unravelling this recurrence and lower bounding, we see that\n\n$$\ng_{i+1} \\geq \\frac{g}{2^{i}}-\\sum_{j=1}^{i} \\frac{\\left(g / 2^{j}\\right)^{2 / 3}}{2^{i-j}}\n$$\n\nHence,\n\n$$\n\\begin{aligned}\ng_{i+1} & \\geq \\frac{g}{2^{i}}-g^{2 / 3} \\sum_{j=1}^{i} 2^{j / 3-i} \\\\\n& =\\frac{g}{2^{i}}-\\left(\\frac{g}{2^{i}}\\right)^{2 / 3} \\sum_{j=1}^{i}\\left(2^{1 / 3}\\right)^{j-i} \\\\\n& =\\frac{g}{2^{i}}-\\left(\\frac{g}{2^{i}}\\right)^{2 / 3} \\sum_{j=0}^{i-1}\\left(2^{-1 / 3}\\right)^{j} \\\\\n& \\geq \\frac{g}{2^{i}}-\\left(\\frac{g}{2^{i}}\\right)^{2 / 3} \\frac{1}{1-2^{-1 / 3}} \\\\\n& \\geq \\frac{g}{2^{i}}-5\\left(\\frac{g}{2^{i}}\\right)^{2 / 3}\n\\end{aligned}\n$$\n\nBy union bound, the overall probability that the claim holds is at least\n\n$$\n\\begin{aligned}\n1-\\sum_{i=1}^{r+1} 2 \\exp \\left(-\\left(g_{i} / 2\\right)^{1 / 3} / 2\\right) & \\geq 1-\\exp \\left(-g_{r+1} / 6\\right) \\\\\n& \\geq 1-\\exp \\left(-(1 / 10) \\cdot\\left(g / 2^{r}\\right)\\right)\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 44, "text": "# 9.2 Multiple Bits per Round \n\nIf the players are allowed to send $O(\\log \\ell)$ bits per round, then the number of rounds can be significantly improved.\n\nLemma 9.5. There exists a universal constant $C$ and an explicit protocol over $\\ell$ players where each player sends $n=\\log \\ell$ bits per round, that lasts for $C \\cdot \\log * \\ell$ rounds such that for any constant $\\delta>0$, if $\\delta \\ell$ players are bad, then a good leader is chosen with probability $1-\\varepsilon$ where $\\varepsilon=\\delta+13 \\delta^{3 / 2}$.\n\nProof. Our protocol and proof is similar to Lemma 9.1 with the key difference being that the larger value of $n$ allows us to increase the number of bins and simplify our analysis. Here, we end up being verbose and repeating ourselves for clarity. Just like earlier, our protocol will have two stages, one using the lightest bin protocol from [Fei99b] until the number of players is small enough and then resorting to the protocol from Lemma 9.2. Let $C_{0}, C_{1}$ be large constants that we set later. Our final protocol will be:\n\n1. Let $P_{1}=[\\ell]$.\n2. In round $i$ of stage 1 , all players in $P_{i}$ will present a number between 1 and $b_{i}=$ $\\left|P_{i}\\right| / \\log \\left(\\left|P_{i}\\right|\\right)^{C_{0}}$. Based on this value, they will be divided into sets $P_{j}^{j}$ where $j \\in\\left[b_{i}\\right]$.\n3. Set $P_{i+1}$ equal to the smallest set amongst $P_{i}^{1}, \\ldots, P_{i}^{b_{i}}$ (breaking ties arbitrarily).\n4. Repeat this until the number of players becomes at most $\\exp \\left((\\log (1 / \\delta))^{C_{1}}\\right)$ (stop right before it goes below this value). Let this happens after $r$ rounds. This marks the end of the first stage.\n5. In the second stage, apply the protocol from Lemma 9.2 to $P_{r+1}$ and output the leader from that protocol.\n\nWe now analyze this protocol. We argue that at the end of the first stage, with high probability, the fraction of good players in $P_{r+1}$ will be at least $(1-\\delta)-o(1)$. For the second stage, the correctness of the protocol follows from Lemma 9.2.\n\nFor $1 \\leq i \\leq r+1$, let $g_{i}$ be the number of good players in $P_{i}$ and let $p_{i}=\\left|P_{i}\\right|$. As we always choose the lightest bin at each stage, $p_{i+1} \\leq p_{i} / b_{i}$. Hence, we infer that $p_{r+1} \\leq \\ell / \\prod_{i=1}^{r} b_{i}$. Let $g=g_{1}$. We first bound $g_{i}$ :\nClaim 9.6. For any constant $C_{1}$, with probability at least $1-\\exp \\left(-\\log \\left(p_{r+1}\\right)^{1 / 5}\\right)$, it holds that for all $1 \\leq i \\leq r+1, g_{i} \\geq \\frac{g}{\\prod_{j=1}^{r} b_{j}}-2\\left(\\frac{g}{\\prod_{j=1}^{r} b_{j}}\\right)^{2 / 3}$.\n\nWe prove this claim using concentration bounds later, and we remark that $C_{0}$ will be a growing function of $C_{1}$. Using this claim, we see that in $P_{r+1}$, the number of good players will be at least\n\n$$\n\\frac{(1-\\delta) \\ell}{\\prod_{i=1}^{r} b_{i}}-2\\left(\\frac{(1-\\delta) \\ell}{\\prod_{j=1}^{r} b_{i}}\\right)^{2 / 3}\n$$\n\nout of $p_{r+1} \\leq \\frac{\\ell}{\\prod_{i=1}^{r} b_{i}}$ many surviving players. In particular, $g_{r+1} \\geq(1-\\delta) p_{r+1}-2 p_{r+1}^{2 / 3}$.\n\nSo, in stage 2, we have $p_{r+1}$ many players remaining where the fraction of bad players is $\\delta^{\\prime}=\\delta+2 p_{r+1}^{-1 / 3}$. Applying Lemma 9.2 with these parameters, we infer that probability of electing a good leader is at least\n\n$$\n1-\\left(\\delta+2 p_{r+1}^{-1 / 3}+12\\left(\\delta+2 p_{r+1}^{-1 / 3}\\right)^{3 / 2}\\right) \\geq 1-\\delta-12 \\delta^{3 / 2}-3 p_{r+1}^{-1 / 3}\n$$\n\nwhere the last inequality follows because $p_{r+1} \\geq \\omega(1)$. Hence, our overall probability of electing a good leader is at least\n$1-\\delta-12 \\delta^{3 / 2}-3 p_{r+1}^{-1 / 3}-\\exp \\left(-\\log \\left(p_{r+1}\\right)^{1 / 5}\\right) \\geq 1-\\delta-12 \\delta^{3 / 2}-\\exp \\left(-\\log \\left(p_{r+1}\\right)^{1 / 6}\\right) \\geq 1-\\delta-13 \\delta^{3 / 2}=1-\\varepsilon$\nwhere the first inequality follows because $C_{1}$ is a large enough universal constant, and $\\delta<1 / 4$. We check that the number of rounds in the first stage is no more than $O\\left(\\log ^{*}(\\ell)\\right)$ and in stage 2, as guaranteed by Lemma 9.2, the number of rounds is no more than $c \\log ^{*}(\\ell)$, where $c$ is a constant that just depends on $\\delta$ and $C_{1}$ (and is independent of $\\ell$ ). These together give us our universal constant $C$ of Lemma 9.5.\n\nProof of Claim 9.6. Fix any of the $b_{i}$ bins in round $i$. We apply Lemma 9.3 with $\\delta=\\mu^{-1 / 3}$ to infer that with probability at least $1-\\exp \\left(-\\left(g_{i} / b_{i}\\right)^{1 / 3} / 2\\right)$, it holds that the number of good players in that bin is $\\geq g_{i} / b_{i}-\\left(g_{i} / b_{i}\\right)^{2 / 3}$. Applying this to all $b_{i}$ bins, we infer that with probability at least $1-b_{i} \\exp \\left(-\\left(g_{i} / b_{i}\\right)^{1 / 3} / 2\\right)$, it holds that $g_{i+1} \\geq g_{i} / b_{i}-\\left(g_{i} / b_{i}\\right)^{2 / 3}$. By unraveling this recurrence and lower bounding, we see that\n\n$$\ng_{i+1} \\geq \\frac{g}{\\prod_{j=1}^{i} b_{j}}-\\sum_{j=1}^{i} \\frac{\\left(g / \\prod_{k=1}^{j} b_{k}\\right)^{2 / 3}}{\\prod_{k=j+1}^{i} b_{k}}\n$$\n\nFor ease of notation, let $\\alpha(u, v)=\\prod_{j=u}^{v} b_{j}$. Hence,\n\n$$\n\\begin{aligned}\ng_{i+1} & \\geq \\frac{g}{\\alpha(1, i)}-g^{2 / 3} \\sum_{j=1}^{i} \\frac{(1 / \\alpha(1, j))^{2 / 3}}{\\alpha(j+1, i)} \\\\\n& =\\frac{g}{\\alpha(1, i)}-\\left(\\frac{g}{\\alpha(1, i)}\\right)^{2 / 3} \\sum_{j=1}^{i} \\frac{(\\alpha(1, i) / \\alpha(1, j))^{2 / 3}}{\\alpha(j+1, i)} \\\\\n& =\\frac{g}{\\alpha(1, i)}-\\left(\\frac{g}{\\alpha(1, i)}\\right)^{2 / 3} \\sum_{j=1}^{i} \\alpha(j+1, i)^{-1 / 3}\n\\end{aligned}\n$$\n\nWe observe that each term in the summand is exponentially decreasing. Hence, we can upper bound the the sum by $2\\left(\\frac{g}{\\alpha(1, i)}\\right)^{2 / 3}$.\n\nThis means\n\n$$\ng_{i+1} \\geq \\frac{g}{\\alpha(1, i)}-2\\left(\\frac{g}{\\alpha(1, i)}\\right)^{2 / 3}\n$$\n\nBy union bound, the overall probability that the claim holds is at least\n\n$$\n1-\\sum_{i=1}^{r+1} b_{i} \\exp \\left(-\\left(g_{i} / b_{i}\\right)^{1 / 3} / 2\\right)=1-\\sum_{i=1}^{r+1} \\exp \\left(-\\left(g_{i} / b_{i}\\right)^{1 / 3} / 2+\\log \\left(b_{i}\\right)\\right)\n$$\n\nBy our choice of parameters, in particular by letting $C_{0}$ to be a large enough constant, we can ensure that $g_{i} / b_{i} \\geq \\operatorname{poly}\\left(b_{i}\\right)$. Thus, we can ensure that the probability that the claim holds is at least\n\n$$\n1-\\sum_{i=1}^{r+1} \\exp \\left(-\\left(g_{i} / b_{i}\\right)^{1 / 3} / 2+\\log \\left(b_{i}\\right)\\right) \\geq 1-\\sum_{i=1}^{r+1} \\exp \\left(-\\log \\left(p_{i}\\right)^{1 / 4}\\right)\n$$\n\nwhere we get the constant $1 / 4$ by appropriately increasing $C_{0}$ and we used the fact that $\\delta<1 / 4$. As $p_{i}$ is exponentially decreasing, we infer that the overall probability that the desired conclusion holds is at least\n\n$$\n1-\\exp \\left(-\\log \\left(p_{r+1}\\right)^{1 / 5}\\right)\n$$", "tables": {}, "images": {}}, {"section_id": 45, "text": "# 10 Open Problems \n\nWe list here some interesting open problems left by our work:\n\n- Some of our condenser results are only existential and not explicit. It would be very interesting to find explicit constructions with similar parameters. As we show, one way of achieving this would be to explicitly construct a seeded condenser with dependence on seed length being $1 \\cdot \\log (1 / \\varepsilon)$.\n- All our condensers have entropy gap much larger than a constant. It will be interesting to show there exist condensers with constant entropy gap (for any values of $n, \\ell$ ) for uniform oNOSF sources. A slightly weaker but equally interesting question is to construct seeded extractors for uniform oNOSF sources with constant seed length.\n- Show that there exist non-trivial condensers for oNOBF sources or show no such condenser exists. We conjecture that no condenser exists with output entropy rate larger than the input entropy rate for such sources.\n- Construct $\\varepsilon$-collective sampling protocols with fewer rounds than the ones obtained using uniform oNOSF source condensers. It will also be interesting to explicitly construct such protocols when the number of players are very large compared to the number of bits each player has access to. Further, proving lower bounds for $\\varepsilon$-collective sampling protocols is a natural direction to explore.\n- Determine the exact threshold for extracting from oNOBF sources and oNOSF sources. Our lower bounds show extraction is impossible when $g \\leq 0.99 \\ell$ while our constructions using leader election protocols require $g \\geq \\ell-\\Omega\\left(\\frac{\\ell}{\\log \\ell}\\right)$ for oNOBF sources and $g \\geq \\ell-\\Omega\\left(\\frac{\\ell}{\\log ^{s}(\\ell)}\\right)$ for $(g, \\ell, n)$-oNOSF sources where $n \\geq \\log (\\ell)$. Using the connection between extractors and leader election protocols, lower bounds for extraction imply lower bounds for leader election protocols. In particular, matching lower bounds for extraction would imply all current leader election protocols are tight, a long standing open problem.", "tables": {}, "images": {}}, {"section_id": 46, "text": "# Acknowledgements \n\nWe thank Madhur Tulsiani for asking a question that motivated us to consider the model of local oNOSF sources in Appendix A. We thank the organizers of the Dagstuhl Seminar on Algebraic and Analytic Methods in Computational Complexity and Schloss Dagstuhl for providing a stimulating research environment, where discussions between R.S. and E.C. contributed to this collaboration.", "tables": {}, "images": {}}, {"section_id": 47, "text": "## References\n\n[AORSV20] Divesh Aggarwal, Maciej Obremski, Jo\u00e3o Ribeiro, Luisa Siniscalchi, and Ivan Visconti. \"How to Extract Useful Randomness from Unreliable Sources\". en. In: Advances in Cryptology - EUROCRYPT 2020. Ed. by Anne Canteaut and Yuval Ishai. Lecture Notes in Computer Science. Cham: Springer International Publishing, 2020, pp. 343-372. ISBN: 978-3-030-45721-1. DOI: 10.1007/978-3-030-45721-1_13 (cit. on pp. 1, 2, 6, 7).\n[AL93] Mikl\u00f3s Ajtai and Nathan Linial. \"The influence of large coalitions\". en. In: Combinatorica 13.2 (June 1993), pp. 129-145. ISSN: 1439-6912. DOI: 10.1007/BF01303199 (cit. on pp. 3, 7).\n[AN93a] Noga Alon and Moni Naor. \"Coin-Flipping Games Immune Against Linear-Sized Coalitions\". In: SIAM J. Comput. 22.2 (1993), pp. 403-417. DOI: 10.1137/0222030 (cit. on pp. 8, 36).\n[AN93b] Noga Alon and Moni Naor. \"Coin-Flipping Games Immune Against Linear-Sized Coalitions\". In: SIAM J. Comput. 22.2 (1993), pp. 403-417. DOI: 10.1137/0222030 (cit. on p. 15).\n[BGM22] Marshall Ball, Oded Goldreich, and Tal Malkin. \"Randomness Extraction from Somewhat Dependent Sources\". In: 13th Innovations in Theoretical Computer Science Conference (ITCS 2022). Ed. by Mark Braverman. Vol. 215. Leibniz International Proceedings in Informatics (LIPIcs). ISSN: 1868-8969. Dagstuhl, Germany: Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2022, 12:1-12:14. ISBN: 978-3-95977-217-4. DOI: 10.4230/LIPIcs.ITCS.2022.12 (cit. on p. 20).\n[BCDT19] Avraham Ben-Aroya, Gil Cohen, Dean Doron, and Amnon Ta-Shma. \"Two-Source Condensers with Low Error and Small Entropy Gap via Entropy-Resilient Functions\". en. In: DROPS-IDN/v2/document/10.4230/LIPIcs.APPROXRANDOM.2019.43. Schloss Dagstuhl - Leibniz-Zentrum f\u00fcr Informatik, 2019. DOI: 10.4230/LIPIcs.APPROX-RANDOM.2019.43 (cit. on p. 20).\n[BL89] Michael Ben-Or and Nathan Linial. \"Collective Coin Flipping\". In: Advances In Computing Research 5 (1989), pp. 91-115 (cit. on pp. 8, 9).\n[BKKKL92] Jean Bourgain, Jeff Kahn, Gil Kalai, Yitzhak Katznelson, and Nathan Linial. \"The influence of variables in product spaces\". en. In: Israel Journal of Mathematics 77.1 (Feb. 1992), pp. 55-64. ISSN: 1565-8511. DOI: 10.1007/BF02808010 (cit. on p. 7).\n\n[CGR24] Eshan Chattopadhyay, Mohit Gurumukhani, and Noam Ringach. \"On the Existence of Seedless Condensers: Exploring the Terrain\". In: Proceedings of the 65th Annual IEEE Symposium on Foundations of Computer Science (FOCS). To appear. 2024 (cit. on pp. 1-5, 11-13, 18, 20, 21, 23, 24).\n[CL22] Eshan Chattopadhyay and Jyun-Jie Liao. \"Extractors for sum of two sources\". In: Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing. STOC 2022. New York, NY, USA: Association for Computing Machinery, June 2022, pp. 1584-1597. ISBN: 978-1-4503-9264-8. DOI: 10.1145/3519935.3519963 (cit. on p. 45).\n[CZ19] Eshan Chattopadhyay and David Zuckerman. \"Explicit two-source extractors and resilient functions\". In: Annals of Mathematics 189.3 (May 2019). Publisher: Department of Mathematics of Princeton University, pp. 653-705. ISSN: 0003-486X, 1939-8980. DOI: 10.4007/annals.2019.189.3.1 (cit. on pp. 1, 3, 25).\n[CGHFRS85] Benny Chor, Oded Goldreich, Johan Hasted, Joel Freidmann, Steven Rudich, and Roman Smolensky. \"The bit extraction problem or t-resilient functions\". In: Proceedings of the 26th Annual Symposium on Foundations of Computer Science. SFCS '85. USA: IEEE Computer Society, Oct. 1985, pp. 396-407. DOI: 10.1109/SFCS.1985.55 (cit. on p. 2).\n[Dod06] Yevgeniy Dodis. Fault-tolerant leader election and collective coin-flipping in the full information model. 2006 (cit. on p. 8).\n[DORS08] Yevgeniy Dodis, Rafail Ostrovsky, Leonid Reyzin, and Adam Smith. \"Fuzzy Extractors: How to Generate Strong Keys from Biometrics and Other Noisy Data\". In: SIAM Journal on Computing 38.1 (Jan. 2008). Publisher: Society for Industrial and Applied Mathematics, pp. 97-139. ISSN: 0097-5397. DOI: 10.1137/060651380 (cit. on p. 25).\n[DPW14] Yevgeniy Dodis, Krzysztof Pietrzak, and Daniel Wichs. \"Key Derivation without Entropy Waste\". In: Advances in Cryptology - EUROCRYPT 2014 - 33rd Annual International Conference on the Theory and Applications of Cryptographic Techniques, Copenhagen, Denmark, May 11-15, 2014. Proceedings. Ed. by Phong Q. Nguyen and Elisabeth Oswald. Vol. 8441. Lecture Notes in Computer Science. Springer, 2014, pp. 93-110. DOI: 10.1007/978-3-642-55220-5 $\\backslash .6$ (cit. on p. 1).\n[DMOZ23] Dean Doron, Dana Moshkovitz, Justin Oh, and David Zuckerman. \"Almost ChorGoldreich Sources and Adversarial Random Walks\". In: Proceedings of the 55th Annual ACM Symposium on Theory of Computing. STOC 2023. New York, NY, USA: Association for Computing Machinery, June 2023, pp. 1-9. ISBN: 978-1-4503-9913-5. DOI: 10.1145/3564246.3585134 (cit. on pp. 1, 3).\n[DMOZ24] Dean Doron, Dana Moshkovitz, Justin Oh, and David Zuckerman. \"Online Condensing of Unpredictable Sources via Random Walks\". In: Electron. Colloquium Comput. Complex. TR24-165 (2024). ECCC: TR24-165 (cit. on p. 3).\n[DGW09] Zeev Dvir, Ariel Gabizon, and Avi Wigderson. \"Extractors And Rank Extractors For Polynomial Sources\". In: Comput. Complex. 18.1 (2009), pp. 1-58. DOI: 10.1007/S00037-009-0258-4 (cit. on p. 1).\n\n[Fei99a] U. Feige. \"Noncryptographic selection protocols\". In: 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039). ISSN: 0272-5428. Oct. 1999, pp. 142-152. DOI: 10.1109/SFFCS.1999.814586 (cit. on pp. 8, 15).\n[Fei99b] Uriel Feige. \"Noncryptographic Selection Protocols\". In: 40th Annual Symposium on Foundations of Computer Science, FOCS '99, 17-18 October, 1999, New York, NY, USA. IEEE Computer Society, 1999, pp. 142-153. DOI: 10.1109/SFFCS.1999.814586 (cit. on pp. 36, 38).\n[GGL98] Oded Goldreich, Shafi Goldwasser, and Nathan Linial. \"Fault-Tolerant Computation in the Full Information Model\". In: SIAM J. Comput. 27.2 (1998), pp. 506-544. DOI: $10.1137 / 50097539793246689$ (cit. on pp. 8, 9).\n[GSV05] Shafi Goldwasser, Madhu Sudan, and Vinod Vaikuntanathan. \"Distributed Computing with Imperfect Randomness\". In: Distributed Computing, 19th International Conference, DISC 2005, Cracow, Poland, September 26-29, 2005, Proceedings. Ed. by Pierre Fraigniaud. Vol. 3724. Lecture Notes in Computer Science. Springer, 2005, pp. 288-302. DOI: $10.1007 / 11561927 \\backslash \\_22$ (cit. on p. 9).\n[GLZ24] Jesse Goodman, Xin Li, and David Zuckerman. \"Improved Condensers for ChorGoldreich Sources\". In: Proceedings of the 65th Annual IEEE Symposium on Foundations of Computer Science (FOCS). To appear. 2024 (cit. on pp. 3, 19, 20).\n[GSZ21] Vipul Goyal, Akshayaram Srinivasan, and Chenzhi Zhu. \"Multi-source Nonmalleable Extractors and Applications\". In: Advances in Cryptology - EUROCRYPT 2021 - 40th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, October 17-21, 2021, Proceedings, Part II. Ed. by Anne Canteaut and Fran\u00e7ois-Xavier Standaert. Vol. 12697. Lecture Notes in Computer Science. Springer, 2021, pp. 468-497. DOI: $10.1007 / 978-3-030-77886-6 \\backslash \\_16$ (cit. on p. 10).\n[GVZ06] Ronen Gradwohl, Salil P. Vadhan, and David Zuckerman. \"Random Selection with an Adversarial Majority\". In: Advances in Cryptology - CRYPTO 2006, 26th Annual International Cryptology Conference, Santa Barbara, California, USA, August 2024, 2006, Proceedings. Ed. by Cynthia Dwork. Vol. 4117. Lecture Notes in Computer Science. Springer, 2006, pp. 409-426. DOI: 10.1007/11818175\\_25 (cit. on p. 8).\n[GUV09] Venkatesan Guruswami, Christopher Umans, and Salil Vadhan. \"Unbalanced expanders and randomness extractors from Parvaresh-Vardy codes\". In: Journal of the ACM 56.4 (July 2009), 20:1-20:34. ISSN: 0004-5411. DOI: $10.1145 / 1538902.1538904$ (cit. on p. 16).\n[IMV23] Peter Ivanov, Raghu Meka, and Emanuele Viola. \"Efficient resilient functions\". In: Proceedings of the 2023 ACM-SIAM Symposium on Discrete Algorithms, SODA 2023, Florence, Italy, January 22-25, 2023. Ed. by Nikhil Bansal and Viswanath Nagarajan. SIAM, 2023, pp. 2867-2874. DOI: 10.1137/1.9781611977554.CH108 (cit. on p. 3).\n[IV24] Peter Ivanov and Emanuele Viola. \"Resilient functions: Optimized, simplified, and generalized\". In: CoRR abs/2406.19467 (2024). DOI: 10.48550/ARXIV.2406.19467. arXiv: 2406.19467 (cit. on p. 3).\n\n[KKL88] J. Kahn, G. Kalai, and N. Linial. \"The influence of variables on Boolean functions\". In: [Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science. Oct. 1988, pp. 68-80. DOI: 10.1109/SFCS.1988.21923 (cit. on pp. 6-8, 14, 32).\n[KLRZ08] Yael Tauman Kalai, Xin Li, Anup Rao, and David Zuckerman. \"Network Extractor Protocols\". In: 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October 25-28, 2008, Philadelphia, PA, USA. IEEE Computer Society, 2008, pp. 654-663. DOI: 10.1109/FOCS.2008.73 (cit. on p. 9).\n[KRVZ11] Jesse Kamp, Anup Rao, Salil Vadhan, and David Zuckerman. \"Deterministic extractors for small-space sources\". In: Journal of Computer and System Sciences. Celebrating Karp's Kyoto Prize 77.1 (Jan. 2011), pp. 191-220. ISSN: 0022-0000. DOI: 10.1016/j.jcss.2010.06.014 (cit. on p. 45).\n[KZ07] Jesse Kamp and David Zuckerman. \"Deterministic Extractors for Bit-Fixing Sources and Exposure-Resilient Cryptography\". en. In: SIAM Journal on Computing 36.5 (Jan. 2007), pp. 1231-1247. ISSN: 0097-5397, 1095-7111. DOI: 10.1137/S0097539705446846 (cit. on p. 1).\n[Li16] Xin Li. \"Improved Two-Source Extractors, and Affine Extractors for Polylogarithmic Entropy\". en. In: 2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS). New Brunswick, NJ, USA: IEEE, Oct. 2016, pp. 168-177. ISBN: 978-1-5090-3933-3. DOI: 10.1109/FOCS.2016.26 (cit. on p. 25).\n[Li23] Xin Li. \"Two Source Extractors for Asymptotically Optimal Entropy, and (Many) More\". In: 64th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2023, Santa Cruz, CA, USA, November 6-9, 2023. IEEE, 2023, pp. 12711281. DOI: 10.1109/FOCS57990.2023.00075 (cit. on p. 45).\n[MW97] Ueli Maurer and Stefan Wolf. \"Privacy amplification secure against active adversaries\". In: Advances in Cryptology-CRYPTO'97: 17th Annual International Cryptology Conference Santa Barbara, California, USA August 17-21, 1997 Proceedings 17. Springer. 1997, pp. 307-321 (cit. on p. 15).\n[Mek17] Raghu Meka. \"Explicit Resilient Functions Matching Ajtai-Linial\". In: Proceedings of the 2017 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). Proceedings. Society for Industrial and Applied Mathematics, Jan. 2017, pp. 1132-1148. DOI: 10.1137/1.9781611974782.73 (cit. on pp. 3, 25).\n[MR95] Rajeev Motwani and Prabhakar Raghavan. Randomized algorithms. Cambridge University Press, 1995 (cit. on p. 1).\n[ODo14] Ryan O'Donnell. Analysis of boolean functions. Cambridge University Press, 2014 (cit. on p. 28).\n[RZ01] Alexander Russell and David Zuckerman. \"Perfect Information Leader Election in log* n+O (1) Rounds\". In: J. Comput. Syst. Sci. 63.4 (2001), pp. 612-626. DOI: 10.1006/JCSS.2001.1776 (cit. on p. 8).\n[SV08] Saurabh Sanghvi and Salil P. Vadhan. \"The Round Complexity of Two-Party Random Selection\". In: SIAM J. Comput. 38.2 (2008), pp. 523-550. DOI: 10.1137/050641715 (cit. on p. 8).\n\n[TV00] Luca Trevisan and Salil P. Vadhan. \"Extracting Randomness from Samplable Distributions\". In: 41st Annual Symposium on Foundations of Computer Science, FOCS 2000, 12-14 November 2000, Redondo Beach, California, USA. IEEE Computer Society, 2000, pp. 32-42 (cit. on p. 1).\n[Vad12] Salil P. Vadhan. \"Pseudorandomness\". English. In: Foundations and Trends ${ }^{\\circledR}$ in Theoretical Computer Science 7.1-3 (Dec. 2012). Publisher: Now Publishers, Inc., pp. 1-336. ISSN: 1551-305X, 1551-3068. DOI: 10.1561/0400000010 (cit. on p. 1).", "tables": {}, "images": {}}, {"section_id": 48, "text": "# A Extracting from Local oNOSF Sources \n\nA natural variation on our definition of oNOSF sources is to consider the case where the adversary cannot remember the value of every good block in the past; rather, it can only remember the value of the most recent $s$ blocks. Arguably, this is a realistic assumption in the setting of many short blocks, where it could be difficult to introduce long range correlation.\n\nDefinition A. 1 (Local oNOSF sources). We call a $(g, \\ell, n, k)$-oNOSF source $\\mathbf{X}=\\left(\\mathbf{X}_{1}, \\ldots, \\mathbf{X}_{\\ell}\\right)$ an $s$-local $(g, \\ell, n, k)$-oNOSF source if each bad block $\\mathbf{X}_{i}$ can only depend on at most $s$ blocks $\\mathbf{X}_{i-s}, \\ldots, \\mathbf{X}_{i-1}$ that come before it.\n\nInterestingly, weakening the adversary in this way converts our oNOSF source into a smallspace source. These sources were first studied by [KRVZ11] and we refer the reader to them for a definition and background. Since the adversarial blocks of an $s$-local $(g, \\ell, n, k)$-oNOSF source can only depend on the binary string of length at most $s n$ to its left, we easily see that an $s$-local $(g, \\ell, n, k)$-oNOSF source is samplable by a space-sn source.\n\nUsing recent explicit extractors for low-space sources provided by [CL22, Li23] and the fact that a $(g, \\ell, n, k)$-oNOSF source has entropy at least $g k$, we get the following extraction result for these local online sources.\n\nTheorem A. 2 (Using the explicit extractor of [CL22]). There exists a universal constant $C$ such that for every $s$ and $k \\geq \\frac{2 s n+\\log ^{C}(n \\ell)}{g}$ there is an explicit extractor Ext $:\\left(\\{0,1\\}^{n}\\right)^{\\ell} \\rightarrow\\{0,1\\}^{m}$ with error $\\varepsilon=(n \\ell)^{-\\Omega(1)}$ and output length $m=(g k-2 s n)^{\\Omega(1)}$ for every s-local $(g, \\ell, n, k)$-oNOSF source.\n\nA similar result with slightly better entropy requirement, but constant error, can be obtained using the small-space extractor from [Li23].", "tables": {}, "images": {}}], "id": "2411.04115v2", "authors": ["Eshan Chattopadhyay", "Mohit Gurumukhani", "Noam Ringach", "Rocco Servedio"], "categories": ["cs.CC"], "abstract": "We investigate the tasks of deterministically condensing and extracting\nrandomness from Online Non-Oblivious Symbol Fixing (oNOSF) sources, a natural\nmodel of defective random sources for which extraction is impossible in many\nparameter regimes [AORSV, EUROCRYPT'20]. A $(g,\\ell)$-oNOSF source is a\nsequence of $\\ell$ blocks where $g$ of the blocks are good (are independent and\nhave some min-entropy), and the remaining bad blocks are controlled by an\nonline adversary - can be arbitrarily correlated with any block that appears\nbefore it.\n  The existence of condensers for oNOSF sources was recently studied in [CGR,\nFOCS'24]. They proved various condensing impossibility results, and showed the\nexistence of condensers when $n\\gg\\ell$.\n  We make significant progress on proving the existence of condensers in almost\nall parameter regimes, even when $n$ is a large constant and $\\ell$ is growing.\nWe next construct the first explicit condensers for oNOSF sources, matching the\nexistential results of [CGR, FOCS'24]. We also obtain a much improved\nconstruction for transforming low-entropy oNOSF sources into uniform oNOSF\nsources.\n  We find interesting applications of our results to collective coin flipping\nand collective sampling, problems that are well-studied in fault-tolerant\ndistributed computing. We use our condensers to provide very simple protocols\nfor these problems.\n  Next, we turn to understanding the possibility of extraction from oNOSF\nsources. We initiate the study of a new, natural notion of the influence of\nfunctions, which we call online influence. We establish tight bounds on the\nonline influence of functions, which imply extraction lower bounds. Lastly, we\ngive explicit extractor constructions for oNOSF sources, using novel\nconnections to leader election protocols. These extractors achieve parameters\nthat go beyond standard resilient functions [AL, Combinatorica'93].", "updated": "2025-04-02T18:00:07Z", "published": "2024-11-06T18:46:57Z"}