{"title": "Alternative proof for the bias of the hot hand statistic of streak\n  length one", "sections": [{"section_id": 0, "text": "#### Abstract\n\nFor a sequence of $n$ random variables taking values 0 or 1 , the hot hand statistic of streak length $k$ counts what fraction of the streaks of length $k$, that is, $k$ consecutive variables taking the value 1 , among the $n$ variables are followed by another 1 . Since this statistic does not use the expected value of how many streaks of length $k$ are observed, but instead uses the realization of the number of streaks present in the data, it may be a biased estimator of the conditional probability of a fixed random variable taking value 1 if it is preceded by a streak of length $k$, as was first studied and observed explicitly in [Miller and Sanjurjo, 2018]. In this short note, we suggest an alternative proof for an explicit formula of the expectation of the hot hand statistic for the case of streak length one. This formula was obtained through a different argument in [Miller and Sanjurjo, 2018] and [Rinott and Bar-Hillel, 2015].\n\n\nFor $n \\in \\mathbb{N}$, let $\\left(X_{j}\\right)_{j \\in\\{1, \\ldots, n\\}}$ be a sequence of independent, identically distributed random variables such that $\\mathbb{P}\\left(X_{1}=1\\right)=1-\\mathbb{P}\\left(X_{1}=0\\right)=p \\in[0,1]$. We are interested in testing for hot streaks, that is, we want to design a test statistic which infers, from a sequence of zeros and ones, whether the sequence of successes is independent or whether there is dependence among the successes.\n\nWe consider here the test statistic studied in [2] (see also [3]), which will be referred to as the hot hand statistic. The hot hand statistic is obtained by counting how often a success follows a streak of $k \\in \\mathbb{N}$ successes.\n\n1 Definition (Hot hand statistic). For $n \\in \\mathbb{N}_{\\geq 2}$ and any family of random variables $X=\\left(X_{j}\\right)_{j \\in\\{1, \\ldots, n\\}}$ taking values in $\\{0,1\\}^{n}$, the hot hand statistic for streak length $k \\in \\mathbb{N}_{\\leq n-1}$ is defined as\n\n$$\n\\hat{P}_{k}(X) \\stackrel{\\text { Def. }}{=} \\frac{X_{1} X_{2} \\ldots X_{k+1}+\\cdots+X_{n-k} X_{n-k+1} \\ldots X_{n}}{X_{1} X_{2} \\ldots X_{k}+X_{2} X_{3} \\ldots X_{k+1}+\\cdots+X_{n-k} X_{n-k+1} \\ldots X_{n-1}}\n$$\n\nNote that $\\hat{P}_{k}(X)$ is only well-defined for those realizations of $X$ where the denominator of (1), henceforth denoted by $D_{k}(X)$, is non-zero.\n\nWe will be specifically interested in the case $k=1$, which we write out here for clarity of exposition:\n\n$$\n\\hat{P}_{1}(X)=\\frac{X_{1} X_{2}+X_{2} X_{3}+\\cdots+X_{n-1} X_{n}}{X_{1}+X_{2}+\\cdots+X_{n-1}}\n$$\n\n[^0]\n[^0]:    *Institut f\u00fcr Mathematik, Universit\u00e4t Z\u00fcrich. E-Mail: maximilian.janisch@math.uzh.ch or mail@maximilianjanisch.com.\n\nIn [2, Theorem 1] it was established that, if $n \\in \\mathbb{N}_{\\geq 3}, k \\in \\mathbb{N}_{\\leq n-2}$, and $X_{1}, \\ldots, X_{n}$ is a family of independent, identically distributed random variables with\n\n$$\n\\mathbb{P}\\left(X_{1}=1\\right)=1-\\mathbb{P}\\left(X_{1}=0\\right)=p \\in(0,1)\n$$\n\nthen\n\n$$\n\\mathbb{E}\\left(\\hat{P}_{k}(X) \\mid D_{k}(X) \\neq 0\\right)<p\n$$\n\n2 Remark (Context for [2, Theorem 1]). The preceding result may be seen in rough analogy to how the naive sample variance estimator is biased as it uses an empirical estimate of the expected value of the underlying distribution instead of the actual expected value. It is furthermore in line with a broader literature on estimation of Markov chain transition probabilities, see e.g. $[1]$.\n\nFurthermore, the following explicit formula ${ }^{1}$ for the case $k=1$ was established, using an auxiliary random variable $\\tau$ which, conditionally on $X=\\left(X_{1}, \\ldots, X_{n}\\right)$, is uniformly distributed on those indices $j \\in\\{2, \\ldots, n\\}$ for which $X_{j-1}=1$. This argument is formulated fully in [2, Theorem A.3.2], but a shortened and intuitively accessible, albeit mathematically not fully written out, version of the proof can be found on [3, Page 3].\n\n3 Theorem (Explicit formula for $k=1$, see [2, Theorem A.3.2] and [3, Page 3]). Let $n \\in \\mathbb{N}_{\\geq 3}$ and let $X_{1}, \\ldots, X_{n}$ as well as $p$ be as in (2). Then\n\n$$\n\\mathbb{E}\\left(\\hat{P}_{1}(X) \\mid D_{1}(X) \\neq 0\\right)=\\frac{p}{1-(1-p)^{n-1}}+\\frac{p-1}{n-1}\n$$\n\nIn this short note we aim to provide an alternative proof of Theorem 3, which we deem more direct than the one found in $[2,3]$. We start with the following Lemma.\n\n4 Lemma (Expectations of reciprocal of sum of constant and binomially distributed random variable). Let $p \\in(0,1]$ and let $n \\in \\mathbb{N}$. Let $Z_{n, p}$ be a random variable with distribution Binomial $(n, p)$. Then\n\n$$\n\\mathbb{E}\\left(\\frac{1}{1+Z_{n, p}}\\right)=\\frac{1-(1-p)^{n+1}}{(n+1) p}\n$$\n\nand\n\n$$\n\\mathbb{E}\\left(\\frac{1}{2+Z_{n, p}}\\right)=\\frac{(1-p)^{n+2}+(n+2) p-1}{(n+1)(n+2) p^{2}}\n$$\n\nProof of Lemma 4. First, for $z \\in(-1, \\infty), \\frac{1}{1+z}=\\int_{0}^{1} t^{z} \\mathrm{~d} t$. Therefore, using Fubini-Tonelli and recognizing $\\mathbb{E}\\left(t^{Z_{n, p}}\\right)$ as the probability generating function of a binomial distribution, we get\n\n$$\n\\mathbb{E}\\left(\\frac{1}{1+Z_{n, p}}\\right)=\\mathbb{E}\\left(\\int_{0}^{1} t^{Z_{n, p}} \\mathrm{~d} t\\right)=\\int_{0}^{1} \\mathbb{E}\\left(t^{Z_{n, p}}\\right) \\mathrm{d} t=\\int_{0}^{1}((1-p)+p t)^{n} \\mathrm{~d} t\n$$\n\nBy making the change of variables $s=(1-p)+p t$, one obtains that the latter integral equals the right-hand side of (5), thus proving (5).\n\nSecond, for $n \\in \\mathbb{N}$, noting that $Z_{n+1, p}$ equals in distribution a Bernoulli $(p)$-distributed random variable, independent of $Z_{n, p}$, plus $Z_{n, p}$, we get\n\n$$\n\\mathbb{E}\\left(\\frac{1}{1+Z_{n+1, p}}\\right)=p \\mathbb{E}\\left(\\frac{1}{2+Z_{n, p}}\\right)+(1-p) \\mathbb{E}\\left(\\frac{1}{1+Z_{n, p}}\\right)\n$$\n\nRearranging and using (5) yields (6).\n\n[^0]\n[^0]:    ${ }^{1}$ Or rather, a variant which is equivalent to the simplified version presented here.\n\nWe are now in a position to prove Theorem 3.\nProof of Theorem 3. Firstly, by conditioning on $X_{n-1}=X_{n}=1$, we obtain\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left(\\left.\\frac{X_{n-1} X_{n}}{X_{1}+\\cdots+X_{n-1}} \\right\\rvert\\, D_{1}(X) \\neq 0\\right) \\\\\n& =\\mathbb{E}\\left(\\left.\\frac{1}{1+X_{1}+X_{2}+\\cdots+X_{n-2}}\\right) \\mathbb{P}\\left(X_{n-1}=X_{n}=1 \\mid X_{1}+\\cdots+X_{n-1}>0\\right)\\right. \\\\\n& =\\mathbb{E}\\left(\\left.\\frac{1}{1+Z_{n-2, p}}\\right) \\frac{p^{2}}{1-(1-p)^{n-1}}\\right.\n\\end{aligned}\n$$\n\nFrom (5) we thus get\n\n$$\n\\mathbb{E}\\left(\\left.\\frac{X_{n-1} X_{n}}{X_{1}+\\cdots+X_{n-1}} \\right\\rvert\\, D_{1}(X) \\neq 0\\right)=\\frac{p}{n-1}\n$$\n\nSecondly, for $j \\in\\{2, \\ldots, n-1\\}$, we have, by exchangeability of the $X_{j}$,\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left(\\left.\\frac{X_{j-1} X_{j}}{X_{1}+\\cdots+X_{n-1}} \\right\\rvert\\, D_{1}(X) \\neq 0\\right) & =\\mathbb{E}\\left(\\left.\\frac{X_{1} X_{2}}{X_{1}+\\cdots+X_{n-1}} \\right\\rvert\\, D_{1}(X) \\neq 0\\right) \\\\\n& =\\mathbb{E}\\left(\\left.\\frac{1}{2+Z_{n-3, p}}\\right) \\frac{p^{2}}{1-(1-p)^{n-1}}\\right. \\\\\n& =\\frac{p}{\\frac{1-(1-p)^{n-1}}{n-2}-\\frac{1}{n-1}}\n\\end{aligned}\n$$\n\nwhere we have used (6) for the last identity. We thus obtain\n\n$$\n\\mathbb{E}\\left(\\hat{P}_{1}(X) \\mid D_{1}(X) \\neq 0\\right)=\\sum_{j=2}^{n} \\mathbb{E}\\left(\\left.\\frac{X_{j-1} X_{j}}{X_{1}+\\cdots+X_{n-1}} \\right\\rvert\\, D_{1}(X) \\neq 0\\right)=\\frac{p}{1-(1-p)^{n-1}}+\\frac{p-1}{n-1}\n$$\n\nthus establishing (4).\n5 Remark (Theorem 3 implies (3) for the case $k=1$ ). We note by full expansion and using $p \\in(0,1)$ that\n\n$$\n\\begin{aligned}\n& \\frac{p}{1-(1-p)^{n-1}}+\\frac{p-1}{n-1}<p \\\\\n& \\Longleftrightarrow p(n-1)+(p-1)\\left(1-(1-p)^{n-1}\\right)<p(n-1)\\left(1-(1-p)^{n-1}\\right) \\\\\n& \\Longleftrightarrow(n-1)(1-p)^{n-2}<1+(n-2)(1-p)^{n-1}\n\\end{aligned}\n$$\n\nThe latter inequality, however, follows from the weighted AM-GM inequality, as\n\n$$\n1+(n-2)(1-p)^{n-1} \\geq(n-1) \\sqrt[n-1]{((1-p)^{n-1})^{n-2}}=(n-1)(1-p)^{n-2}\n$$\n\nSince $(1-p)^{n-1} \\neq 1$, this inequality is strict. Therefore, Theorem 3 implies as Corollary (3) when $k=1$.\n6 Remark (The case $k>1$ ). The presented approach does not seem to generalize well to the case $k>1$, as it would necessitate computing quantities such as\n\n$$\n\\mathbb{E}\\left(\\frac{1}{1+X_{1} \\ldots X_{k}+X_{2} \\ldots X_{k+1}+\\cdots+X_{n-k} \\ldots X_{n-1}}\\right)\n$$", "tables": {}, "images": {}}, {"section_id": 1, "text": "# References \n\n[1] Do Sun Bai. \"Efficient Estimation of Transition Probabilities in a Markov Chain\". In: The Annals of Statistics 3.6 (1975), pp. 1305-1317. ISSN: 00905364. URL: http://www.jstor .org/stable/2958250.\n[2] Joshua B. Miller and Adam Sanjurjo. \"Surprised by the Hot Hand Fallacy? A Truth in the Law of Small Numbers\". In: Econometrica 86.6 (2018), pp. 2019-2047. DOI: https://doi.o $\\mathrm{rg} / 10.3982 /$ ECTA14943. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982 /ECTA14943. URL: https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA14943.\n[3] Yosef Rinott and Maya Bar-Hillel. \"Comments on a 'Hot Hand' Paper by Miller and Sanjurjo (2015)\". In: Hebrew Business Administration \\& Economics Research eJournal (2015). See SSRN 2642450.", "tables": {}, "images": {}}], "id": "2407.10577v1", "authors": ["Maximilian Janisch"], "categories": ["stat.OT", "math.PR"], "abstract": "For a sequence of $n$ random variables taking values $0$ or $1$, the hot hand\nstatistic of streak length $k$ counts what fraction of the streaks of length\n$k$, that is, $k$ consecutive variables taking the value $1$, among the $n$\nvariables are followed by another $1$. Since this statistic does not use the\nexpected value of how many streaks of length $k$ are observed, but instead uses\nthe realization of the number of streaks present in the data, it may be a\nbiased estimator of the conditional probability of a fixed random variable\ntaking value $1$ if it is preceded by a streak of length $k$, as was first\nstudied and observed explicitly in [Miller and Sanjurjo, 2018]. In this short\nnote, we suggest an alternative proof for an explicit formula of the\nexpectation of the hot hand statistic for the case of streak length one. This\nformula was obtained through a different argument in [Miller and Sanjurjo,\n2018] and [Rinott and Bar-Hillel, 2015].", "updated": "2024-07-15T09:55:31Z", "published": "2024-07-15T09:55:31Z"}