{
  "title": "A Primer on the Analysis of Randomized Experiments and a Survey of some\n  Recent Advances",
  "sections": [
    {
      "section_id": 0,
      "text": "## 1 Introduction\n\nThe past two decades have witnessed a surge of new research in the analysis of randomized experiments. The emergence of this literature may seem surprising given the widespread use and long history of experiments as the \"gold standard\" in program evaluation, but this body of work has revealed many subtle aspects of randomized experiments that may have been previously unappreciated. This article provides an overview of some of these topics, primarily focused on stratification, regression adjustment, and cluster randomization, although we also provide short discussions on a broad range of other topics at the end of the article. We also provide a companion R package sreg, which is designed to facilitate inference in stratified (as well as potentially clustered) randomized experiments: see Trifonov et al. (2025).\n\nThe majority of our discussion is presented within the context of a framework in which units are sampled from a suitable \"super-population\"; i.e., we assume that the units are i.i.d. draws from some probability distribution. Importantly, these results differ from results that are derived within a complementary framework\n\n[^0]\n[^0]:    *We thank the co-editor and an anonymous referee for comments and suggestions that have improved the manuscript. We would also like to thank Gustavo Bobonis for helpful discussions. The third author acknowledges support from NSF grant SES-2149408.\n\nin which units are sampled from a fixed, finite population (without replacement). The special case in which all the units from the finite population are sampled is sometimes referred to as design-based inference. After defining some common notation in Section 2, we begin our review in Section 3 by providing a comparison of these two sampling frameworks. Specifically, we introduce both frameworks in the context of an analysis of the difference-in-means estimator for the average treatment effect in a completely randomized experiment. There, we argue that the super-population framework approximates a finite-population framework in which a negligible fraction of the finite population is sampled; we argue further, however, that results derived in a super-population framework may provide useful methods for inference in a finite population even outside of this limiting case.\n\nWe then turn our attention in Section 4 to the impact of stratification on subsequent inferences drawn from randomized experiments. More specifically, instead of assigning treatment status according to complete randomization, it is common to stratify first according to some baseline covariates and then assign treatment status within each stratum so as to ensure that the treatment group and control group in the experiment are \"balanced\" according to these covariates. Examples of such schemes include stratified block randomization and matched pair designs, both of which are commonly used within economics and the sciences more generally: see Rosenberger and Lachin (2015) for a textbook treatment focused on clinical trials and Bruhn and McKenzie (2009) for a review focused on development economics. We first illustrate in Section 4.1 how stratification can improve the precision of the usual difference-in-means estimator of the average treatment effect by reducing what we refer to as the ex-post (as opposed to ex-ante) bias of the estimator. Here, the ex-post bias of the estimator is used to describe its behavior conditional on treatment status rather than unconditionally. Section 4.2 develops the implications of this increase in precision for inferences about the average treatment effect more formally for the special case of stratified block randomization with finitely many \"large\" strata; Section 4.2.1 describes how this analysis changes when strata are \"small,\" for instance in the case of matched pair designs.\n\nIn Section 5 we consider the use of baseline covariates (including ones that are possibly not used in the assignment of treatment status) to further improve the precision of estimators of the average treatment effect through regression adjustment. We explain how na\u00efve regression adjustment may increase rather than decrease the precision of estimators of the average treatment effect, and how a more careful use of the covariates can ensure an improvement in (asymptotic) precision. Our discussion includes linear adjustments (Section 5.1) as well as more general forms of adjustment (Section 5.2).\n\nFinally, Section 6 extends our discussion to cluster randomized experiments, i.e., randomized experiments in which the unit of randomization is a cluster. Such designs are increasingly common in economics. Indeed, Muralidharan and Niehaus (2017) find in a survey of leading economics journals between 2001 and 2016 that more than 75 percent of randomized experiments were cluster randomized. In Section 6.1 we draw a distinction between different ways in which one may define the average effect of the treatment in such settings. In particular, we discuss two possible parameters of interest, which we call the equally-weighted\n\naverage treatment effect and the size-weighted average treatment effect, that differ in how the average effect within a cluster is aggregated across individuals. In Section 6.2 we discuss inference in cluster randomized experiments.\n\nOwing to space constraints, our discussion is, of course, necessarily incomplete. We therefore provide, at the end of each section, a guide to some of the related literature on similar topics. For completeness, in Section 7 we also briefly discuss a number of topics that were regrettably omitted from the main discussion: this includes the analysis of treatment effect heterogeneity, re-randomization, multiple testing, imperfect compliance/attrition, experiments with interference, randomization inference, policy learning, and responseadaptive designs. Athey and Imbens (2017) also provide a review of some of these topics (primarily from the design based perspective).\n\nBefore proceeding, we emphasize that our survey is limited to the analysis of randomized experiments, and as a result we will only briefly comment on some aspects of experimental design as well as practical issues surrounding implementation, in passing; classical textbook treatments on experimental design are provided in Cox and Reid (2000), Pukelsheim (2006), Atkinson et al. (2007), and Wu and Hamada (2011). Other important contributions to the theory of experimental design (including theoretical justifications for why an experimenter may want to randomize) are provided in Savage (1951), Blackwell and Girshick (1954), Kiefer (1959), Li (1983), Kallus (2018, 2021), Section 5.10 in Lehmann and Romano (2022), and Bai (2023). Important references discussing other practical issues (particularly for field experiments conducted in economics) include Duflo et al. (2007), Glennerster and Takavarasha (2013), Karlan and Appel (2017), and List (2023).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "# 2 Setup and Notation: The Potential Outcomes Framework \n\nIn this section, we present some notation which will be common to the majority of the article. Each individual $i$ in the experiment is assigned a binary treatment $D_{i} \\in\\{0,1\\}$ (we focus on settings with binary treatments, but provide references on related extensions to multiple treatments throughout the article). Let $Y_{i}(1)$ denote the potential (or counterfactual) outcome for individual $i$ if they are treated, and $Y_{i}(0)$ denote the potential outcome if they are untreated. Note that we never observe $Y_{i}(1)$ and $Y_{i}(0)$ simultaneously for the same individual, but rather we observe the outcome $Y_{i}$ given by\n\n$$\nY_{i}= \\begin{cases}Y_{i}(1) & \\text { if } D_{i}=1 \\\\ Y_{i}(0) & \\text { if } D_{i}=0\\end{cases}\n$$\n\nWe summarize the previous relationship succinctly by\n\n$$\nY_{i}=Y_{i}(1) D_{i}+Y_{i}(0)\\left(1-D_{i}\\right)\n$$\n\nFor each individual, we may also observe a vector of baseline covariates denoted by $X_{i}$. The experimental sample is thus given by $\\left\\{\\left(Y_{i}, D_{i}, X_{i}\\right): 1 \\leq i \\leq n\\right\\}$. For any variable indexed by $i$, for example $D_{i}$, we denote by $D^{(n)}$ the vector $\\left(D_{1}, D_{2}, \\ldots, D_{n}\\right)$. Note that we will always model the experimental assignments $D^{(n)}$ as random; on the other hand, as we will explain below, depending on the sampling framework employed in the analysis, $\\left\\{\\left(Y_{i}(1), Y_{i}(0), X_{i}\\right): 1 \\leq i \\leq n\\right\\}$ may either be modelled as random vectors or fixed quantities.\n\nMuch of our discussion will center on the properties of the standard difference-in-means estimator: let $n_{1}=\\sum_{1 \\leq i \\leq n} D_{i}$ denote the number of treated units in the sample and $n_{0}=n-n_{1}$ denote the number of control units. The difference-in-means estimator is then given by\n\n$$\n\\hat{\\Delta}_{n}=\\frac{1}{n_{1}} \\sum_{1 \\leq i \\leq n} Y_{i} D_{i}-\\frac{1}{n_{0}} \\sum_{1 \\leq i \\leq n} Y_{i}\\left(1-D_{i}\\right)\n$$\n\nNote that this estimator can equivalently be described as the estimator of the coefficient on $D_{i}$ when estimating the following linear regression by least squares:\n\n$$\n\\text { regress } Y_{i} \\text { on constant }+D_{i}\n$$\n\nWe will illustrate many of the basic concepts via the analysis of a completely randomized experiment. In a completely randomized experiment, the treatment assignment $D^{(n)}$ is implemented such that, for some fixed fraction of units, say $\\pi$, exactly $n_{1}=\\lfloor\\pi n\\rfloor$ units are assigned to treatment and $n_{0}=n-n_{1}$ units are assigned to control, with all such possible assignments being equally likely. Formally, let $\\left(d_{1}, \\ldots, d_{n}\\right) \\in\\{0,1\\}^{n}$, then $D^{(n)}$ is independent of $\\left\\{\\left(Y_{i}(1), Y_{i}(0), X_{i}\\right): 1 \\leq i \\leq n\\right\\}$ with distribution given by\n\n$$\nP\\left\\{D^{(n)}=\\left(d_{1}, \\ldots, d_{n}\\right)\\right\\}= \\begin{cases}\\binom{n}{n_{1}}^{-1} & \\text { if } \\sum_{1 \\leq i \\leq n} d_{i}=n_{1} \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nWe emphasize that an important feature of our discussion here and throughout the rest of the article is that we do not assume that the components of $D^{(n)}$ are independently distributed; this will play a crucial role in our subsequent analyses when studying, for example, stratified randomization in Section 4.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 3 What is Random? Finite versus Super-population Analyses of a Completely Randomized Experiment \n\nIn this section, we introduce the two main paradigms for the analysis of randomized experiments: the finitepopulation and super-population approaches to inference. In a finite-population analysis, we begin with a collection $\\left\\{\\left(y_{j}(1), y_{j}(0), x_{j}\\right): 1 \\leq j \\leq N\\right\\}$ of fixed quantities that constitute the entire population of interest. A sample of size $n \\leq N$, given by $\\left\\{\\left(Y_{i}(1), Y_{i}(0), X_{i}\\right): 1 \\leq i \\leq n\\right\\}$, is then drawn without replacement from\n\nthe population, and the experiment is performed on these $n$ individuals. The most common case considered in the literature is when $n=N$, so that the experiment is performed on the entire population. We will refer to this case as design-based inference, since the only source of uncertainty arises from the randomness in $D^{(n)}$. This perspective is often considered attractive in settings where it is difficult to conceptualize an appropriate sampling frame; Reichardt and Gollob (1999) provide further discussion.\n\nIn contrast, in a super-population analysis, the sample $\\left\\{\\left(Y_{i}(1), Y_{i}(0), X_{i}\\right): 1 \\leq i \\leq n\\right\\}$ is modeled as being i.i.d. according to some probability distribution. Informally, we may view the distribution from which the potential outcomes are drawn as summarizing an essentially infinite \"super-population.\"\n\nTo compare and contrast the super- and finite population paradigms, we present an analysis of the difference-in-means estimator $\\hat{\\Delta}_{n}$ for the average treatment effect under both approaches, following the original work of Neyman (1923), in a completely randomized experiment (as defined in Section 2). Our primary takeaway will be that, in a sense to be made formal below, we can view the super-population paradigm as an approximation to the finite-population paradigm in a regime where the sample size $n$ is a vanishing fraction of the total population size $N$. Consequently, we will show that in the super-population framework it is possible to construct consistent variance estimators of $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$, whereas this will often be impossible in the finite-population framework. Instead, in the finite population framework we will explain how to construct conservative variance estimators of $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$. Moreover, we will argue that consistent variance estimators derived via a super-population analysis are often reasonable conservative variance estimators when viewed through the lens of a finite-population analysis.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 3.1 Finite population analysis of $\\hat{\\Delta}_{n}$ \n\nFirst we consider a finite-population analysis. Recall that in this case we begin with $\\left\\{\\left(y_{j}(1), y_{j}(0), x_{j}\\right): 1 \\leq\\right.$ $j \\leq N\\}$ which are fixed, non-random quantities that describe the outcomes (and covariates) of the entire population of $N$ units. Accordingly, in this case, the average treatment effect is defined as\n\n$$\n\\Delta_{N}^{\\mathrm{fn}}=\\frac{1}{N} \\sum_{1 \\leq j \\leq N}\\left(y_{j}(1)-y_{j}(0)\\right)\n$$\n\nWe emphasize that in this framework the parameter of interest $\\Delta_{N}^{\\mathrm{fn}}$ is determined entirely by the values of the potential outcomes from the $N$ units in the population.\n\nTo analyze $\\hat{\\Delta}_{n}$ from a finite population perspective, it is often useful to re-frame the problem as a problem of survey sampling from a finite population; re-framing the problem in this way allows us to employ classical results from survey sampling (see, for instance, Cochran, 1977; Lehmann and Romano, 2022). In Appendix B. 1 we illustrate how this re-framing can be useful in deriving some \"finite- $N$ \" properties of $\\hat{\\Delta}_{n}$ (i.e., properties that hold for every finite population size $N$ ) in a completely randomized experiment. In\n\nparticular, there we show\n\n$$\nE\\left[\\hat{\\Delta}_{n}\\right]=\\Delta_{N}^{l p}\n$$\n\ni.e., that $\\hat{\\Delta}_{n}$ is an unbiased estimator for $\\Delta_{N}^{l p}$, and\n\n$$\n\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]=\\frac{S_{1}^{2}}{n_{1}}+\\frac{S_{0}^{2}}{n_{0}}-\\frac{S_{\\Delta}^{2}}{N}\n$$\n\nwhere\n\n$$\n\\begin{gathered}\nS_{d}^{2}=\\frac{1}{N-1} \\sum_{1 \\leq j \\leq N}\\left(y_{j}(d)-\\bar{y}_{N}(d)\\right)^{2} \\\\\nS_{\\Delta}^{2}=\\frac{1}{N-1} \\sum_{1 \\leq j \\leq N}\\left(y_{j}(1)-y_{j}(0)-\\Delta_{N}^{l p}\\right)^{2}\n\\end{gathered}\n$$\n\nand $\\bar{y}_{N}(d)=\\frac{1}{N} \\sum_{1 \\leq j \\leq N} y_{N}(d)$.\nThis expression for the variance of $\\hat{\\Delta}_{n}$ is fundamental to understanding why inference on $\\Delta_{N}^{l p}$ is generally conservative in the finite population paradigm: note that $S_{d}^{2}, d \\in\\{0,1\\}$ are simply the variances for the potential outcomes in the finite population, and we explain below how to construct estimators of these quantities. To rule out degenerate situations, we henceforth assume that at least one of $S_{d}^{2}, d \\in\\{0,1\\}$ are nonzero. The quantity $S_{\\Delta}^{2}$, however, is the variance of the unit-level treatment effects $\\left(y_{j}(1)-y_{j}(0): 1 \\leq j \\leq\\right.$ $N$ ) in the finite population, and these are by definition never observed for a given unit. In settings where the experiment size $n$ is a non-trivial fraction of the population size $N$ (for instance, in design-based inference when $n=N$ ), this feature of the variance introduces an unavoidable roadblock for estimating $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ consistently, ${ }^{1}$ and thus estimators of $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ will necessarily be conservative unless treatment effects are constant (i.e., that $y_{j}(1)-y_{j}(0)=\\Delta_{N}^{l p}$ for every $1 \\leq j \\leq N$ ). In cases where $n$ is a vanishing fraction of $N$, however, we see that $S_{\\Delta}^{2} / N$ becomes a negligible component of $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ as $N$ gets large, in the sense that in order to estimate $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ consistently it is sufficient to estimate only the sum of its first two terms consistently. As we show in Section 3.2, this feature of $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ when $n$ is a vanishing fraction of $N$ will exactly mirror our findings in the super-population analysis.\n\nBefore discussing estimation of $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ and related methods of inference for $\\Delta_{N}^{l p}$, we document some necessary \"large- $N$ \" properties of $\\hat{\\Delta}_{n}$ in the finite population paradigm. In the finite population framework we conceptualize our asymptotic approximations by imagining a sequence of increasingly larger populations on which we perform our experiment. As a consequence, our large- $N$ results will require some discipline on how this sequence of ever larger populations evolve as a function of $N$. Under appropriate assumptions on the sequence of populations, it can be shown that (see, e.g., Lehmann and Romano, 2022, Theorem 12.2.5)\n\n$$\n\\frac{\\hat{\\Delta}_{n}-\\Delta_{N}^{l p}}{\\sqrt{\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]}} \\stackrel{d}{\\rightarrow} N(0,1)\n$$\n\n[^0]\n[^0]:    ${ }^{1}$ Here and throughout the rest of Section 3.1, consistency should be understood as saying that the ratio of the estimator to the variance converges to one in probability.\n\nas $n \\rightarrow \\infty$ and $N \\rightarrow \\infty$. Using this result, asymptotic inference on $\\Delta_{N}^{\\mathrm{fp}}$ is straightforward once we have an estimator of $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$. Motivated by our decomposition in (2), a conservative variance estimator can be constructed by simply estimating the following upper bound on $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ :\n\n$$\nV_{n}^{\\text {obs }}=\\frac{S_{1}^{2}}{n_{1}}+\\frac{S_{0}^{2}}{n_{0}}\n$$\n\nwhich implicitly sets $S_{\\Delta}^{2}$ to its lowest possible value of zero. From this expression we see that a consistent estimator of $V_{n}^{\\text {obs }}$ can be obtained by replacing $S_{1}^{2}$ and $S_{0}^{2}$ by their natural estimators. Equivalently, viewing $\\hat{\\Delta}_{n}$ as the estimator of the coefficient on $D_{i}$ obtained by the regression in (1), it can be shown that a consistent estimator of $V_{n}^{\\text {obs }}$ can simply be obtained from the resulting heteroskedasticity-robust variance estimator (see, e.g., Angrist and Pischke, 2009, Chapter 8). An asymptotically valid $95 \\%$-confidence interval for $\\Delta_{N}^{\\mathrm{fp}}$ can therefore be constructed as\n\n$$\nC_{n}=\\left[\\hat{\\Delta}_{n}-1.96 \\cdot \\operatorname{SE}\\left(\\hat{\\Delta}_{n}\\right), \\hat{\\Delta}_{n}+1.96 \\cdot \\operatorname{SE}\\left(\\hat{\\Delta}_{n}\\right)\\right]\n$$\n\nwhere $\\operatorname{SE}\\left(\\hat{\\Delta}_{n}\\right)$ is the robust standard error of the coefficient on $D_{i}$ obtained from the regression in (1). Although $C_{n}$ is a valid confidence interval, we emphasize that it is conservative in the sense that\n\n$$\nP\\left\\{\\Delta_{N}^{\\mathrm{fp}} \\in C_{n}\\right\\} \\rightarrow p \\geq 0.95\n$$\n\nas $n, N \\rightarrow \\infty$, with equality only if $n / N \\rightarrow 0$ or $S_{\\Delta}^{2}=0$.\nWe conclude this section by considering the following natural follow-up question: could we develop more precise methods of inference by constructing less conservative variance estimators of $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ ? For the example we just presented, this would amount to considering tighter lower bounds for $S_{\\Delta}^{2}$ which are themselves consistently estimable. For instance, it follows from the Cauchy-Schwarz inequality that\n\n$$\n\\sum_{1 \\leq j \\leq N}\\left(y_{j}(1)-\\bar{y}_{N}(1)\\right)\\left(y_{j}(0)-\\bar{y}_{N}(0)\\right) \\leq\\left(\\sum_{1 \\leq j \\leq N}\\left(y_{j}(1)-\\bar{y}_{N}(1)\\right)^{2}\\right)^{1 / 2}\\left(\\sum_{1 \\leq j \\leq N}\\left(y_{j}(0)-\\bar{y}_{N}(0)\\right)^{2}\\right)^{1 / 2}\n$$\n\nwhich we can use to immediately verify the lower bound $S_{\\Delta}^{2} \\geq\\left(S_{1}-S_{0}\\right)^{2} \\geq 0$. We thus obtain the following improved upper bound on $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ :\n\n$$\n\\frac{S_{1}^{2}}{n_{1}}+\\frac{S_{0}^{2}}{n_{0}}-\\frac{\\left(S_{1}-S_{0}\\right)^{2}}{N}\n$$\n\nwhich can be used to construct a less conservative estimator of $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$. In fact, it is possible to achieve even tighter upper bounds by further exploiting the structure of $S_{\\Delta}^{2}$ : see Aronow et al. (2014) for details. We emphasize, however, that although we will argue in Section 3.2 that inferences based on the heteroskedasticity-robust standard error $\\operatorname{SE}\\left(\\hat{\\Delta}_{n}\\right)$ as in (3) will be valid under either the super-population or finite population paradigms, design-based standard errors (that is, standard errors which are valid in the finite population setting when $n=N$ ) constructed using improved upper bounds like (4) will generally be\n\ntoo small, and thus invalid when viewed from a super-population perspective.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 3.2 Super-population analysis of $\\hat{\\Delta}_{n}$ \n\nNext, we repeat the above analysis within the super-population paradigm. Recall that in this case the sample $\\left\\{\\left(Y_{i}(1), Y_{i}(0), X_{i}\\right): 1 \\leq i \\leq n\\right\\}$ is modeled as being i.i.d. according to some probability distribution. Accordingly, in this case the average treatment effect is defined as\n\n$$\n\\Delta=E\\left[Y_{i}(1)-Y_{i}(0)\\right]\n$$\n\nwhere the expectation is with respect to the distribution of the data.\nWe begin by documenting some finite-sample properties of the estimator $\\hat{\\Delta}_{n}$ in a completely randomized experiment under the super-population framework, analogous to the properties derived in Section 3.1. Using familiar properties of conditional expectations, we show in the appendix that\n\n$$\nE\\left[\\hat{\\Delta}_{n}\\right]=E\\left[Y_{i}(1)-Y_{i}(0)\\right]=\\Delta\n$$\n\nWe thus obtain that the estimator $\\hat{\\Delta}_{n}$ is also an unbiased estimator of average treatment effect in the superpopulation paradigm. Following a similar line of reasoning using the properties of conditional variances, we show in the appendix that\n\n$$\n\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]=\\frac{\\operatorname{Var}\\left[Y_{i}(1)\\right]}{n_{1}}+\\frac{\\operatorname{Var}\\left[Y_{i}(0)\\right]}{n_{0}}\n$$\n\nIt is instructive to compare the limits of the variance expressions in (2) and (6). To ensure nondegenerate limits, it is useful to scale the variances by $n$. By doing so, we see that the limit of the super-population variance mirrors the limit of the finite-population variance in a regime where we sample a vanishing fraction of the total population, i.e., $n / N \\rightarrow 0$. Since the variance of the unit-level treatment effects does not appear in our expression for $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ in (6), consistent variance estimation will be feasible in the super-population paradigm.\n\nTo discuss inference on $\\Delta$, we document some necessary large-sample properties of $\\hat{\\Delta}_{n}$ in the superpopulation framework. Under the assumption that $E\\left[Y_{i}^{2}(d)\\right]<\\infty$, it can be shown that\n\n$$\n\\sqrt{n}\\left(\\hat{\\Delta}_{n}-\\Delta\\right) \\xrightarrow{d} N\\left(0, V^{\\mathrm{cr}}\\right)\n$$\n\nas $n \\rightarrow \\infty$, where\n\n$$\nV^{\\mathrm{cr}}=\\frac{\\operatorname{Var}\\left[Y_{i}(1)\\right]}{\\pi}+\\frac{\\operatorname{Var}\\left[Y_{i}(0)\\right]}{1-\\pi}\n$$\n\n(see, for instance, Bugni et al., 2018). Using the above result, asymptotic inference on $\\Delta$ is straightforward once we have an estimator of $V^{\\text {cr }}$. As in our discussion in Section 3.1, viewing $\\hat{\\Delta}_{n}$ as the estimator of the coefficient on $D_{i}$ obtained by the regression in (1), it can be shown that a consistent estimator of $V^{\\text {cr }}$ can\n\nbe obtained from the resulting heteroskedasticity-robust variance estimator. An asymptotically valid $95 \\%$ confidence interval for $\\Delta$ is therefore once again given by $C_{n}$ in (3). Moreover, since this variance estimator is consistent, $C_{n}$ has exact asymptotic coverage, that is\n\n$$\nP\\left\\{\\Delta \\in C_{n}\\right\\} \\rightarrow 0.95\n$$\n\nas $n \\rightarrow \\infty$.\nThe above discussion illustrates an important feature of super-population and finite population analyses of randomized experiments: methods of inference developed in a super-population framework typically immediately deliver conservative methods of inference from a finite population perspective. While the preceding discussion was limited to completely randomized experiments, in Appendix A.1, we show that this feature holds much more generally under appropriate assumptions. With this in mind, the remainder of this article will focus on illustrating some of the major themes in the analysis of randomized experiments from the super-population perspective, but we will comment on any other important differences between these two perspectives whenever they arise.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 3.3 Further Reading \n\nMost of the material in this section is by-now standard, and several textbook treatments exist at various levels of formality: see in particular Imbens and Rubin (2015), Athey and Imbens (2017), and Lehmann and Romano (2022). Li and Ding (2017) provide formal statements and proofs of finite population central limit theorems. Harshaw et al. (2021) study a general method for constructing less conservative variance estimators in the design-based paradigm.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "## 4 Stratified Randomized Experiments\n\nIn this section, we outline some benefits of stratification and its consequences on subsequent experimental analyses. In a stratified randomized experiment, individuals are first divided into groups (i.e., strata) sharing similar values of their baseline covariates and then assigned to treatment so as to achieve \"balance\" across the treatment and control groups: often, this amounts to simply performing complete randomization within each stratum. Stratification is extremely common in the design of randomized experiments in all parts of the sciences (some examples in economics include Duflo et al., 2015; Berry et al., 2018; Dizon-Ross, 2019; Callen et al., 2020). A primary motivation for stratification, going back to the work of Fisher (1935), is to ensure that the treatment and control groups are similar in the sample, in contrast to complete randomization which can only ensure that this will be true in expectation. As we will show, this property of stratification can lead to an increase in precision of the difference-in-means estimator $\\hat{\\Delta}_{n}$ relative to complete randomization, and as a result subsequent inferences will be unnecessarily conservative unless this is taken into consideration.\n\nIn Section 4.1 we illustrate the benefits of stratification via a simple example. In Section 4.2 we discuss inference in stratified randomized experiments, including inference with \"small\" strata.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 4.1 Some Benefits of Stratification \n\nWe begin by illustrating the benefits of stratification via a simple example. Suppose we have an experimental sample $\\left\\{\\left(Y_{i}(1), Y_{i}(0), X_{i}\\right): 1 \\leq i \\leq n\\right\\}$, where for now we assume $X_{i} \\in\\{0,1\\}$ are binary variables, and for convenience we assume $n$ is even. Consider the following two treatment assignment mechanisms for $D^{(n)}$ :\n\n1. Complete Randomization (CR): Treatment is completely randomized with $\\pi=\\frac{1}{2}$ (see Section 2 for a formal definition).\n2. Stratified Block Randomization (SBR): Independently for each stratum (the sub-samples with $X_{i}=0$ and $X_{i}=1$ ), treatment is completely randomized with $\\pi=\\frac{1}{2}$.\n\nNote that (SBR) as defined above uses the assignment proportion $\\pi=\\frac{1}{2}$ in both strata. Although maintaining a constant assignment proportion across all strata is the most common approach in practice, in principle we could also consider using different assignment proportions in each stratum, and we comment on the implications of doing so on subsequent analyses at the end of Section 4.2. Although both assignment mechanisms (CR) and (SBR) assign exactly $n / 2$ units to treatment, (CR) does not enforce that the composition of the treated group across $X_{i} \\in\\{0,1\\}$ matches the composition in the total sample. For instance, suppose $n=100$, where 40 units have $X_{i}=0$ and 60 units have $X_{i}=1$. Figure 1 depicts one possible assignment that could result from employing (CR) versus (SBR). Although (CR) guarantees that exactly 50 units are assigned to treatment, in this realization of the assignment, units with $X=0$ are over-represented in the treatment group. In contrast, (SBR) reproduces the composition in the overall sample in both the treatment and control groups.\n\nTo formalize this intuition, we analyze two different notions of bias for the difference-in-means estimator $\\hat{\\Delta}_{n}$ under both designs. To simplify the exposition, in this section, we perform our analyses conditional on the observable characteristics $X^{(n)}$ (although we emphasize that this does not materially change the conclusions). To that end, we will temporarily switch our parameter of interest to\n\n$$\n\\Delta_{n}\\left(X^{(n)}\\right)=\\frac{1}{n} \\sum_{1 \\leq i \\leq n} E\\left[Y_{i}(1)-Y_{i}(0) \\mid X_{i}\\right]\n$$\n\nwhich is the average effect of the treatment conditional on the covariates in the sample.\nFor the estimator $\\hat{\\Delta}_{n}$, we define the ex-ante bias as\n\n$$\n\\text { Bias }^{\\text {ante }}\\left(X^{(n)}\\right)=E\\left[\\hat{\\Delta}_{n}\\left|X^{(n)}\\right|-\\Delta_{n}\\left(X^{(n)}\\right)\\right.\n$$\n\nMethod of Randomization\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Assignment across treatment and control for one realization of (CR) vs (SBR)\nwhich averages over all possible realizations of the treatment assignments. This ex-ante bias measures the average bias obtained by repeatedly running the experiment on the same realization of the covariates. In contrast, we define the ex-post bias as the bias conditional on the realized treatment assignments:\n\n$$\n\\text { Bias }^{\\text {post }}\\left(X^{(n)}, D^{(n)}\\right)=E\\left[\\hat{\\Delta}_{n}\\left[X^{(n)}, D^{(n)}\\right]-\\Delta_{n}\\left(X^{(n)}\\right)\\right.\n$$\n\nFirst we compare the ex-ante biases generated by (CR) and (SBR). Note that the marginal treatment probability of each unit satisfies $E\\left[D_{i} \\mid X^{(n)}\\right]=\\frac{1}{2}$ under both designs, i.e., the conditional probability that any given unit is assigned to treatment is one half. Combining this fact with the (conditional) exogeneity of treatment assignment, we obtain that\n\n$$\nE\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}\\right]=\\Delta_{n}\\left(X^{(n)}\\right)\n$$\n\nso that $\\operatorname{Bias}^{\\text {ante }}\\left(X^{(n)}\\right)=0$ for both designs. To compare the ex-post biases generated by (CR) and (SBR), consider the following decomposition:\n\n$$\n\\begin{aligned}\n\\operatorname{Bias}^{\\text {post }}\\left(X^{(n)}, D^{(n)}\\right) & =\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(2 D_{i}-1\\right) E\\left[Y_{i}(1)+Y_{i}(0) \\mid X_{i}\\right] \\\\\n& =\\frac{1}{n} E\\left[Y_{i}(1)+Y_{i}(0) \\mid X_{i}=1\\right] \\cdot \\operatorname{Imb}(1)+\\frac{1}{n} E\\left[Y_{i}(1)+Y_{i}(0) \\mid X_{i}=0\\right] \\cdot \\operatorname{Imb}(0)\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\operatorname{Imb}(x)=\\#\\{\\text { treated units with } X_{i}=x\\}-\\#\\{\\text { untreated units with } X_{i}=x\\}\n$$\n\nis a measure of the imbalance of treatment status for each possible value of the covariate $X_{i}$. By construction,\n\n(SBR) enforces that for any realization of $D^{(n)}$ it is the case that $\\operatorname{Imb}(x)=0$. In contrast, as depicted in Figure 1, this is not the case for (CR). As a consequence, it follows immediately that under (SBR), $\\operatorname{Bias}^{\\text {post }}\\left(X^{(n)}, D^{(n)}\\right) \\equiv 0$, whereas the ex-post bias under (CR) is not guaranteed to be identically zero unless $X_{i}$ is an irrelevant stratification variable in the sense that $E\\left[Y_{i}(1)+Y_{i}(0) \\mid X_{i}=1\\right]=E\\left[Y_{i}(1)+Y_{i}(0) \\mid X_{i}=0\\right]$.\n\nNext, we show that these properties of the ex-post bias have direct implications for the (ex-ante) variance of $\\hat{\\Delta}_{n}$ under (CR) and (SBR). By the law of total variance:\n\n$$\n\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}\\right]=E\\left[\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}, D^{(n)}\\right]\\left|X^{(n)}\\right|+\\operatorname{Var}\\left[E\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}, D^{(n)}\\right] \\mid X^{(n)}\\right]\\right.\n$$\n\nWe show in the appendix that\n\n$$\nE\\left[\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}, D^{(n)}\\right]\\left|X^{(n)}\\right|=\\frac{2}{n^{2}} \\sum_{1 \\leq i \\leq n}\\left(\\operatorname{Var}\\left[Y_{i}(1) \\mid X_{i}\\right]+\\operatorname{Var}\\left[Y_{i}(0) \\mid X_{i}\\right]\\right)\n$$\n\nAs a result, $E\\left[\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}, D^{(n)}\\right]\\left|X^{(n)}\\right|\\right.$ doesn't depend on the experimental design, and thus to compare $\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}\\right]$ under (CR) and (SBR) it suffices to study\n\n$$\n\\operatorname{Var}\\left[E\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}, D^{(n)}\\right]\\left|X^{(n)}\\right|=E\\left[\\operatorname{Bias}^{\\text {post }}\\left(X^{(n)}, D^{(n)}\\right)^{2} \\mid X^{(n)}\\right]\\right.\n$$\n\nIn words, comparing $\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}\\right]$ under (CR) versus (SBR) amounts to a comparison of the second moment of the ex-post bias under both designs; because the ex-post bias is always zero under (SBR), the variance of $\\hat{\\Delta}_{n}$ is always smaller under (SBR) than (CR). In the next section, we discuss the implications of this increase in precision on subsequent inferences.",
      "tables": {},
      "images": {
        "img-0.jpeg": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAIgBLADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAoopM0ALRQKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigBG6VyGk6xrHi2BtT0q6tbDSDI8dq0luZpLkIxUyH5lCKSDgYJIGcjOK62QAphuh4rxPTPEfiH4RpLoGr+HbzU9Bt5nNnqNoM7YmYthhjGeScEgjnqMGgDvrfxHrEXjex8N6ha2/7y0luGvIQQkwUoF2qSSh5bKktj5cE5rp7m/gtJrSKVtr3U3kRDrl9jPj8kauT8K+MfDHj67i1DTJpPt1hGw8iYbHRJNoYkdCPlXkHg/UZwfF154kX4reDrZDYC0kluntYDI+GZImBeRtvXa/AHTnrmgD1PnNLWSJNeGjzN5Gm/2kD+7TzpPJI4+823dnGeAPSuM0Px/wCIvEHhK91LT9Dt5tQspXiltlkOHdXx5a85zt+Yk9MjAOeAD0miuZ1fxWumXOiadIsVtqOsMUhE+XSNlVSwOz7xBYADKg88jvF/wk95pni6w8PazFAx1KORrK8tgyqzRjLxuhJ2nGCDuOc4oA6LUL6DTbCW8uW2wxDLtjOB9O9WAa8y+M15rlt4etls2tItPlvYI5SzN5sh3ZC9MBcqM9T+td1p7a1iX+0YbBCFAiFvK7ZP+1uUY/DNAGpRXGeF/Feran4u17QNWs7KCTS0gYPayM6uJF3ZywHbHam23izWE+JQ8LajZ2KQyWL3sU0Ejs20PtAbcAAeD6/WgDtaK4TxD4z1jw7420bSrjT7OXT9WeWOB4pGM25VGFIICrlmUZyQASSRVjxD4vv/AAjLpd1rNvavpt9crau1rvMltIykr1H7xflPQKfagDs6K4zxN4q1jwvoR8QXlhbGwieP7RbBz50SOwXO77rMCwyuMdcMcZrsI3EihgQVIBFAD6KKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKQnFBOB0rz7xn4j1C98V6b4H0G5a1vLxTPfXiDLW1uM/c9GbBGe3HrQB3j3MSSLG0qJI3CqzAEn2Hen7wCASBu6A8E1z9l4G8M2NsYRo1pOWH72W6iWaWU+ru4LMT7muZudKv8ASfix4bjivJn0N4LowWzMSIJAgyM9SuCNoJO35sYHFAHpFLTFY9T3pdx7CgB1FN3/AJ+ho3+nSgB1FM346jGKXceaAHUhOBTd5PaoL6xtdUs5LO9gWa3fG+NxwcHP9KALAbJx0PpTq83+DMMdp4V1W3hXbFFrVzGi5ztUFQBXopfj8M0APJxSA5prOFUs5CgDJJPFYnhTXV8RaH/acbpJE91cpE6Dho0mdFP4qqn8fwoA3qKYXxShue2PrQA6iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoopDQAp4pM1zmsa9djxBa+HdIWD+0Z4GupZrgFktoQwXdtBBdix2hcjoTnjFZ3iPVvEvhPw5f6sxttYit4XfbHAYZIjjhj8xDID94DBAycnFAHa0VUgu86ZHdzFQPJEj9h0yaktLqO9tYLmE5imjWRCRg4IyKAJ6KKKACiiigAoqrZahBqELy27bkSaSEnHRkYo3/jymsrV/EF9puv6Rp1vod3e2987rNdxfctQMYL8d/fHtmgDfpM/SsTxZ4lt/Cfh241a4hecoVjigThpZGOFUHtyevpmsnWtS8UaL4am1ho4b69hCt/Ztjas4bJAKg7tzYycsB0BO3jFAHZA5oqKCQywo7RtGzKGKN1XPY+9S0AFFBrnfGepazo/hm+1TR/sJksoJLiRLuN2DKiliBtYc8UAdFRWT4Y1SbW/C2k6pcLGs95ZxXEixghVZ0DEDJJxz61rUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBFcu0dtI6/eVSRxnnFY/hTW4vEnhTTdWhkjk+0QIzhOiyAfOvthsituSNZY2jb7rDBxXOweB9JsLaGHSmutMMUax77KYxlwowC6/dZsD7xBPvQBzWsaPa2nxp8L6hpkKw3lzBd/2j5YA8yFYxtdwO+9gMnqQP7tSeMPk+Lvw8dvlUHUAWPQfuRxmuv0nw9Y6RNNcxmae8nAWW6uZDJIyjouf4VH91QB3xT9Y8P6br0MEeoQM5t5RNBIjskkUg6MrKQQfxoAktr0XF9fWwjwLVkQvuzuJUN+BAI/OuJ+DYVfCF6VA+bVro8Hqdwrsv7Csv7OksgbpUlbfJLHdSJM7ccmVWD54A69AB0GKo6H4J0Lw2+7SLe5tlJZjF9uneNiRgkozlSeByRmgDF8S69ezeONJ8J6YYra6mt3vZr+SISPbxAlQIweN5IIycgDsaxPFNmtj8Tfh/G17dXU7XF0Xa4l3HGwDoMKueegGce1egar4b0zWLq1vLqFxeWhJt7qGRo5Ys8EBlIOD3ByD6VXu/B2i38aC7t5pZUlWZbg3EgmV1zjEgYMAATwDjk8UAcf8WbuO+8BWlzEGEQ1aEMSMfdlKkn2yP5V373xTWLew8r/W28s+/d93Y0YxjHfzP079m3WhabfaI+j3VokunvGI2hfJBA6c9c8A565GetGm6LaaXkwGeRyix77idpW2r0XLEnAzQBxXhkMvxp8cFgV3W9kQfUeVjP06/lUc08Uv7Q9siMC8WgFXx2JlLYPvgg/8CFdpd+G9Nu9UGplJYb8R+Ubi3maN2TOdrYOGGemc47YrNuPh/oFzqttqjxXiX1vH5aTxX00bkZJJZlYFiSeSSc9O1AGB44APxQ+HYI/5b3mP+/aUfF/B0fw2CAVPiG0yDjBHz/pXR6l4E0DV9Ui1K+hvZbyEloZf7RuF8knGfLCyAJ0H3cUuseBtB8QfZ/7VhvLkW+wxK2oXAVWUYDYDgbuvzfePrQBifGTP/Cp9d6klIun/AF1T9K7LTM/2baDt5KfyFZeqeCtD1rTYtO1KO9ubSMYEUmoXBD87vn+f5yCOC2SO1aWlaTa6LZLZ2ZuPIX7qz3MkxUYwAC7EgYHQcUAXqKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooARjha8f0INH+014lFxkNJpamAn0xB0/I/ka9gPSuF8ZeFL+XxBpvi/wAPoj61poKPbu2wXkBzuTd2bk4J455oA7gdPWoZxbfaLcziIzbyIC4G4NtJO332g/hmsC28daNJGouvtWnXQA3Wd5bSRzKfQLj5u3K5B7GrVl52sarFqsttLb2lsjrapcLskkZ8bpCp5QAABQcN8zZA4oA5vwteJ491fXtQvwZ9LsL42FnZPzH8gBeRl6OWLDG7O0DjFLqOoL4L8eaBp9vldH1wSwNaj7kE6bdrRj+EHcF2jjvjOcr4a0+TwJrWu2k8E7aPqN41/a3MMTSCJ3A3xyBQSuMDDHgjuD1de6XN4x8d6Jqn2eaHSNC8yWOSeNo2uZ224CqwDbF253YwTwM9aAMWbTDafHWLTrK5uobO90Zri7QXMjEnzjkqS3yk7EB24wM4weateK47jQviF4Tt9Cmeyj1g3FteRox2sqhWDhTx5gyxDYz0zkcVFc6g/wDwuu31j+ytaOmRaO1k1yNKuNgl81mx9zJGMfMBj3p/je7lk+InhC6g0vV7iDSp7g3k0GmTukYdFCkEJ846/dz0oAn8fwp4Wbw3qmkF7a4bWYLSdlkY+fE4YMsmT8/Qctk0343WMQ+HeoaqklxHe2nlCGSOd02hpVU5AODkMetRfFO6l1Kw0G30/TNWu5ItUtb5xDp07BIV3EknbjPI+XO4dxU/xgu47/4NavcRLOsb+QQJ4Xhf/j4Qco4DDp3FAHST+E7G90oiaa6+3Om4agszCeN/7ysMbRn+EAL2xis74Y+Jb7xF4fuItWIbVdLupLC6cAASMh+/gdM5wfcHpWi/in7NpAZtMv31IRhVsY7d2d5MYwGxt25/jztxzkVl+BNAl8D+E7q41bfLqF7cte3otoXmKyOQNqKgLMBx0zzk9KAMn4bahFpPgbxRqU4JitNVv53CnkqnzHH4Ck8GXeleI/DcWt+JnW81HUC8hWSNmW2TcQqRDGEAAByOc8k9MN+G1t9q8O69oOqabqlp/aF7dy4ubGWJXglAHDsu3OD0zn24Nangqa48H6DD4a1yG5WSxLJb3cVu8kd1FuJUgqDtbBwUODxxmgCLwnbya/oOtaHryXl5aWV9JDbz3DSRtc25GU3MCC/BIJPXjPNZvwW0HT5/hxompyRzG7DykN9ok25WZwPl3be3TFeh6dc3V7HPLPbm3t2YLbRuu2TZtGWcZ+Uk5wvUDGeSQPP/AAJBqGi+BIPCM1reQava3jREiFwjRmcuZVkxt27Ce/UY6kCgCxqevxav8UZvDl7PJFo+lWazXESBsXM74Kq+3koFbO3oT1yOKle6TSPHmh/8I6kp0u/ElvqFrBCwhiKrujlVcYQ5+UkcEY9AanvtJuPDnxHk8X29vNdafqNqLbUEgQu8DqV2ShR8zrgbSACR1we3RprEmoXdvHp0MjQ7t1xPNbuiqmDgLuALMTjoCAAc4OAQDaFLTUz3HanUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFIelLSN0oA8y8d2niLQfGNn428O6e2q7LM2V/YoTueLcXVlA5zn0B6DjrUFh8YfC3iF5NB1u0vdHuLtDbvHeRgIdwxt3duvVgBXaweIY28bXugSyIJY7KG6gQjDOGZ1f64wn0zWd8StJ0vU/AGsnVIYsQWkk0UzY3RyKpKlT9cD3zjvigCj8UZdX0z4dakukNbxwR2pjmkmY+ZsOFIQAYyQepP4dxseEH1s6Npv22GwW0+xRbGhldpCdi4yCoA/Amub8Tx3o/Z+ljvQ5vRo8AmDA7twVM59/X8a6rSNQFvoXhmJU8wXsMUYYN9wC3Z92Mc/cA7fe/CgDDHjPXbf4gv4VvNNsDLNZ/abSSCZyCN+3LkgYACueAckAZ5rY07xDqUWhavqPiHTDYjTpZ+YyD58MYyJACflyM8E9s5xXPkK37QgY8Y8MZHt/pPX9TXS+MfEMfhXwjqOty2/2gW0YxCcfOzMFUE9hlhn2oAyZPFmq/8ACIf8JbaR2F7pohNy1rGJEk8kAlisjdWABOCi5wRnoa2ptSvdW8Lw6j4dNqZLuBZoHvCyKoZcgkKCSeRxx9a5TUtPvm+GGpahrWqyvLJpMkgtLTENtFuiO1FUDcw5A+YnPpWl8O7yI+AvDlkQxlfSlk9go2rz36t/OgDF+EN54hvPAul3LrYy20088ks0kr+cS07lztC7c5z3rZ8S+J9W0DxPoVotjZSabql6lqsvmv5qEjn5cYHfvWN8Jr4ad8F9Km8vzWSSSPy92Dua5ZQD15+Ycdfzq38SSD4h8Cf9hyP/ANBNAFT4zNqK+H7Lyo7U2Y1G1O5nYP5gc4GMYC9Oc59q1vFOveLPDfhy91k2OiyR2iB3QTyliMgcfIPX1qn8ZMDwfZs2AP7VtevX79afxTOfhf4h4I/0U9f94UAa0viG207wgPEOossEK2aXM3PTKg7RnqcnAHckDvWd/bPiZ/D511LCzVPK+0JpjBvPaPGcGTdtD4527SM8Z71zHxE0681P4FrDZKzyQ2ltOyL1ZEClsDvgfN+H4V2enWFtqelWl3balfPbXMKSR7bj5SrDPp6GgCpd+PtNi8M6XrFrHJdvqxjjsLRCBJNK/RTngYOdx6DHfjNLxrPrkPw+1+S7hspkk02dZIrUsGiDRsMhmOJAM88IcZwCQAeU1TStO0LxF8OZ9KDDw7Z3dzaIzlmCyykgHLcnc27B6cAjjGe88fypF8OvETSOEDadOoLHAyyEAfUkgfU0AS+AT/xb7w2P+oXbf+i1ro65zwAc/Dzw2f8AqGW//ota6OgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKTApaKAGhQO5pdvPU0tFADdg4zzj1pdopaKAE2LnoKTYMYp1FADSO4NcV8TdF1nxL4SudB0izgf7V5ZNxLcBBHtkVumCT92u3pMUAUNJa8lsEOoWa2lwODEkokHHfIAq/tFKAB0ooAQqDRtGaWigBu0Yx29KXaPr9aWigBNoo2jGOfzpaKAEAANLRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAYt74U0fUdRm1C6ti15JHHGJ1cpJEELFSjLhkPztyCM5qv/AMIbYzSxNqF7qOowwsHjt7y43xBh0JUAbyP9rPr1roqKAIri2guraW3uIllglQpJG43K6kYIIPUEVmaT4Z03RPLFmtxthjMUCS3MkiwocZVAxOBwPwAHStiigDl5Ph74dk1ZtVMWoDUWUp9qXVboSBSc7QwkyFz26Vv3en2l/YS2N3Ak9rKhSSKUbg6n1z1qzRQBiWfhTS7G3htkW5ktIMeTbT3LyxoB0AVicgcYznGOMVLonhnSvDsRi0y3eGM8KrTPIEGSdqhidq5J4GBya1qKAMK18IaNZXUk1tBLEsk/2lrdZ3EPndd/l52g5APTGQD1Gai1jwPoWvX8V9qUN5NcROHiZb+dBEwAGUVXCoeOqgHv1roqKAOd1jwPoWvw28Oqw3l1Fbqixo+oXAUFejEB/mfn7xyx7mnah4L0XVdJi0u/W+uLKMMPLk1K4O8E5w58zL4I43E47YroKKAM7SdDsdEsBZWIuBbDAWOa5km2gAABfMZiAABwOKpReENJtxJHbLdW9tKxZ7WC6kSEk9cIGwoPcDAOeRW9RQBTu9KsL7TW026tIZbJkCGBkGzaOgx2xxj0wMdKzLrwfpN9YSWV79ru7d42i2XF3JJtBBBxk/ewT8xywycGt+igDJ8P+HNP8M2AstM+0rbjAVJrqSYIB2UOx2j2GK1qKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKTNAC0U3dzjFZt74k0PTWK3+s6dasO010iH8iRQBqUVzf/Ce+Gz/AKnUhcjsbSGS4z/37U0f8JrYt/qdM16Yeo0e5Qfm6CgDpKK5z/hLifueHNfb/t0C/wDoTCk/4S2Yn/kV9fx6+RH/APHKAOkornD4uK/f8O6+v/bnu/kxpP8AhNdOX/X2Guwe76NdEfmsZFAHSUVzg8eeGB/rdWitv+vtHgx/32BWlY6/pGp4+warY3een2e4ST/0EmgDRopM+1GaAFooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoooNABRSZ5xikLgAknGKAHUVkXnirw/p523muabbsOqzXUan8iapf8J54dOPKvpLnPQ2lrNPn/AL9q1AHSUVzf/CZ2j/6nStflHqNInT/0NRR/wlrH7nhvX2/7dFX/ANCYUAdJRXN/8JZPn/kVtfx6+RH/APHKP+EvC/6zw94gT/txLf8AoJNAHSUVzf8Awm2mJ/r7PW4B6yaNdY/MRkfrSjx54XyBLrVtbE8f6UTB/wChgUAdHRVGz1nTNRANjqNndD/phOr5/I1dBPpQAtFJmloAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKDQAUGm7vb8PWubm8VPfzyWfhyy/tWaNiklz5my0iYdQ0uDuI7qgJ9cUAdJuOcYrDvPGGjWly1oly17er1tbCJriRT/tBAdv1bAqsvhe61T5vEmrTXobk2dtm3th7FQdzj/fYg+gres9OstOtVtbG0gtrdfuxQxhFH4AYoAwjqnifUf+PHQbewiPSTVLkb/wDv1Fu/VxSjQNdu/wDkI+KbhFP/ACy021jt1+mX8xvyYV0mKXFAHN/8IJoEnN7Bc6i3c6heS3IP/AXYr+AFalloOj6aALDSrG0A6CC3RMfkK0KKAExS4oooAMUUUUAFGKKKAEIB61mX3hrQtTOb/RtPuj6z2yOfzIrUooA5o+BtFhGbD7dpzDp9hvpolH/AA2z8xSHRfEdmc2Hihp1HIj1S0SUfTdH5Z/PNdNSbR/8AqoA5oaz4ksB/xMvDq3UY/wCW2lXIkP1McgQ/gC341b07xboup3P2SG9EV73tLqNoJv8Av24DH6gYrawPzqnqOk6fq1t9n1Gyt7uH+5PGHA9xnoaALmfcYpRXLN4e1PSfn8PavKiDpY6iWuIT7K5O9P8Avogf3Txiaz8Vxpdx6frVnJpN/I22JZmDQzt6RSj5WPop2t/s0AdHRSA5JHpS0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRTHkWNWd2VUUZZmOAB6mgBxOKQuBnPAHUnjFcy3ie71dzF4XsVvEztOo3BKWiH/AGSPmlP+4Nv+2KWPwiuoYk8R6hcaux5+zt+5tV9vJU4b/gZf60ATXHjTR0na2spJdUulOGh06IzlT6My/Kn/AAJhURvvFuoD/RNJsdLjPR9QnMsg/wC2cfy/+RK6G3tYLSBILaGOGFBhY41Cqo9gKk2j0oA5seHNXu+dS8V3zDvHYQx2yfnhnH/fdOHgTw65Bu7F9QbrnUbiS6z/AN/GYV0QGKWgCjZaNpemqFsdNs7VR0EECoB+Qq7ilooATApcUUUAFFFFABgUhUEYIBFLRQBkXvhXw9qRJvdD064Y87pLVGOfXOM1RPgjSoedPn1PTm7fY7+VV/79lin/AI7XS0UAcz/ZXiiyP+heI47xR/BqdkpJ9t8Xl4+pU0f2/rlh/wAhbw3K8Y6z6XOtyo9yjbH/AAVWrpcCgj60AZOmeJ9H1eVoLO/ja5X71tIDFMn+9G4Dj8RWsDVDVNE0vWYlj1GxguVX7hkQFkPqp6qfcEGsc6NrmjfNoeqG6gHSx1Vi4x6JPy6/8C8z6egB1FFYOm+KLe7vV06+t5dL1QgkWt3gGTHUxsDtkH0OfUCt0HJx7UALRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAHpVW+vrbT7Ka8vJkgtoEMkkjnAVR3qy3SuVkiHibxXJBLltK0Z0Z0z8s94QHXPqsalTj+8w/u0ARx2N74xJm1RZ7HQ25i077kt0D/FORyFPP7vuPvd1HVQW0NrbxwW8SQwxqFSONQqqo6AAcAVIFAOaWgBAADxS0UUAFFFFABRRRQAUUUhOBQAtFc1438WR+D/C91qrQ+dOiYhh5+d/f0UdSf6kVa1fxHBougJqlxGZDL5aQwxn5pZZCAiDPqT17DmgDbormr7xDf6G9hJrFpbJaXk6W7TW8rN9mkfhQ2VG5SeN3GCRxzkdIDmgBaKKKACiiigAooooAQgEYIzVa/sbTUbOW0vraO4tpRtkikUMrDryPwq1SEZoA5B3vvBJ3PJNf+HB94uS89gPXPWSIe/zKO7AYHWQzJcRJLE6SROoZHQ5VgeQQe4pWUEYxkHg5rltHX/hG/EjaCvGmXqNc6cCeImUjzYR7fMHUehcD7tAHWUUgz3paACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKDRSHpQBR1TVrTRrCS9vpRHBHjOBlmJ4AUDksTwAOSawYdGvPE7C68RxvDY9YNHDfIB2NwR99u+z7q99xANO06IeI/Es+qzgtYaXM9tYRno0y/LLN74OY19NrnuMdUBigBEjWNVVAFVRgADAA9AKdiiigAooooAKKKKACiiigAopCcCuW8Y+Mo/C66dEkInur68itlQ8BFZwGcn8cD39cGgDqqKx/EWvx6BZQyeS1xc3Nwlra26HBmmfouf4RjJJ7AE1TufEV3o2o6bb6zBbLBqMot4rm3disc5BKowI6Ng4b1GCBQB0lFIDmloAKKKKACiiigApMClooAo6rpNhq9g1nf2sc8BwQrjow6FT1DDsQQR2xXPpfX3hCeOLV7h73RJGCRajIf3tqScBJ8D5lJ4EnUHhuu49aRnvUc0EU8DwTRrJFIpR43GVZSMEEdCMdqAHq2T26dqdXL+Gmk0jU7vwxO7vFbRi406RzktbE4KE9zG3y/7pT1rp880ALRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFACHpXN+BsP4fluD/AK2fUL2SU/7X2mQY/AAL+FdIc44rl9El/sfxJqmhzDalzI+o2J/vq5HnKPdZCW+kg9CaAOpopAc9qWgAooooAKKKKACiiigApkzpFC0kjKqKMszHAA9SfSn1BeWsF7Zy2tzEJYJkKSIejKRgigDzj4jRyXnwy8Q6xOjo8tuqW0b5Bjh8xCCR/eYgMe4+UHlTSfEF2Enw9iyfLbXbRmGepA4/nT/iN4N+3eE7zS9A8PXFze3KrsmW5RY4yHUnd5kgPKg9Ae1T6l4Vn1TwjYpZaVLp2paXfwahBbXUyN50seMjcjsAGG4DkcjoBQAvxsUL8KNXccMrwEH0PnJyPzNd/bszwRs4wxUEj0OBXFeKrK68cWFnoiafc29nLdRS6hJdR7BHGhDGMDPzMxAAK5Xqc13C47dOlADqKKKACiiigAooooAKKKKAEPTriua8VjZdeHZ04lj1eILg84ZHRh/3yxrpWOFzxgcnNcvdN/bXja0tIstbaKDdXL9vtDoVjT6hGdj6ZT1oA6gUtIOvPXFLQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABTJWKxOyjJAJA9afSHpQBz3gJVHgHQXBy0thDM7f3ndQzH8WJP410Vcv4Tk/sxr7w1KNsmnSlrYf89LV2LRkf7uTGfdPcV04OaAFooooAKKKKACiiigAooooAr313FYWUt1O2I4xk46n0A9STwB3JArzb4hWs0eh6Pe3igX11r9k0q5BEahjsiB9FB69CxY8Zr0fUNPtdTtGtbyETQlgxUkjkHIPHoQD+FeaeP8AwdNeJpltoXhm6uDBqENzNMLuNUeJdxZAHlBycjt+NAF7x9Ix+IPw9gzmF72dmHYsEXH8zR8ZpGh8H2M8ZIkj1a1dD6EMcVc17w5PqGl6DqGmabJbX2jXwuorG4lTe6ZO+PeGZQWByDuIzgGpPEdjceMrrRLJLK5t7C0vo767luY9mTGCViVSctknkjK4B5oA7YClpq/hTqACiiigAooooAKKKKACkYZHP1paRvumgDmtaHl+NPDEin945uoGP+wYt5H03RpXSj19a5mzf+2/G9xeJzZ6PE1nG3Z7hyrSY/3FVFz6s47V0+KACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooJwM03d/+ugB1FNyfSlB5xQAtFFFABRRRQAUUUhOKAFopAc9aWgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAAjIrI17RF1m0i8ud7a+tpBNaXaAFoZAMZx3UjIK9CCa16QjNAHP6L4ha4nOl6pAtlrMSkvBuJSZR/y0hY/eU5/3geCOhO+rZJHpVDV9EsNatFhvod2xt8UqsUkhYdGRxyre4NYq3HiHw4Nt1DJrunAfLcQKBdxD/bThZPquD/snrQB1dFZula9pmtxM+n3kc+w4kQZEkZ9HQgMp9iAa0QcnHGRQAtFFFABRRRQAUUUUAJtFG0e9LRQA0qD60oGKWigAooooAKKKKACikJwKN3tQAtITge9ZereI9L0RkjvbpRcSj9zaxAyTTf7ka5Y/gMVlOviDxKNsgk0HTG6qrBr2UehIysQPtub3U0ATaxrtxNeNomghJtUI/fTld0Vip/jk9WxyqdT3wOa0dD0e30TTltIGkdixkmmkOZJpTyzue7E/4DAAFS6ZpVjo9klnYWyQQLlgq9Sx6sSeSx7kkk9zV0KASR3oAUDFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFBGaKKAMPxBob6h9nv7CZbfVrPJt5nGUdT96KQd0YDB7jgjkUuieII9UaW1nhaz1S3A+02Uh+ZP8AaU9HQ9mHHbrkDbIzWTrWgWWtJE04kiuYWLW93A+yaAnure/cHIPcGgDVB5pa5VdS13w9lNXtH1SyXpqFjF+9UdzLAOfxj3Z/urW5pmsafrNoLrTryG5h6Fo3ztPofQ+xwR3oAvUUmeaWgAooooAKKKKAA0mBjHalooATaPejb7mlooATHNLRRQAUUUUAFFFFABSE4FJu+nvzWPqvijTNKmFrLK898w3JZWqGWdvfYvIHucD3oA2CxGOK5fU9Zutau5dE8PTbHRvLvNSXlLT1VOzy47dF6t0AKNp+ueJP+Qq7aRpjH/jxtpc3Eo9JJV+6P9lDn/axkHorKxtdOs4rOyt47e2iXbHFGoVVHoAKAGaVptro+nQ6fZRiO3gXai5JJ5JJJPJJJJJPJJJq5SAAHNLQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUARXUIubaSFnkRZFKlo3KsM9wRyDXkkb6noXxrt9E1TXtXm0i+gM+nI90dvmDko5/iHytge616+3SvKvjTbG50izvdNDtrejSf2jE8OCYYV+8zE9ASoI9dpxnBwAdR4uhvNcMeiaNqF1ZX6uk011byYW3jzzvH8RYcKv49Bzt6Lo/wDY9u0R1G/vmbBMt7OZG/DoB+AFZ/gi5sdR8JafqViWdb2ITyyuwZ3lP3yxHVgwI9sYGAK6IDFAC0UE4Ga5y51LxalzKlt4c0yWAORHI+rsjOueCV8g4+mTQB0dFcx/anjT/oV9J/8AB03/AMj1YsdQ8UzXsaX2gabbWxP7yWLVWlZR7L5K5/MUAbxOBXLeJvEN1a65o3h3TGjTUNVeRjO67hBDGu52x3Y/dXPGTk5xiuoBzXmV4zSftHachJ2x6AxUZ7mR+aANvX9XvvBcunahdahLf6VPdJaXYuY4w8G/hZFZFUYDD5gQevBGK7MHJrzn44qP+FUamSMlZICPb96td9p8jTWFtK+SzwoxJ75GaALNFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFJilooAydU8NaRq8qz3dmv2pBhLqJjFOn+7IpDD6ZxWcNM8TaWc6drMOpQjpBqseHx6CaMD8yjGunprYAzQBzY8VXdpxq/h3UrYDrNaJ9siPuPKy+Pqgq5YeLfD+py+TaaxZyTgZMBlCyr9UOGH4irep6pZaRZSXmoXCwQJgF2ySSeiqByWJ4AHJPaucOhzeM54bvxFZLBpcR8y20yVVMjns057evljj+8W6AA7ANk9MUua5w+BtEi/48UvNO9BYX00Cj/gCsF/SkHhvVID/ofi7VVUdEuY4Jh+ZjDf8Aj1AHSZpa5z7D4whP7rXdJnHpPpbg/mswH6UhfxrH/wAstAn9/Mmi/wDZWoA6Siuc+2eM1HOi6E/01aZf/bc0n9oeMv8AoXdG/wDBzJ/8jUAdJRXNm78at00bQU+uqzN/7bijPjWT+HQIfxmlx+i0AdHmjP8AkVzgsPF03+u8QaZAPS20t8j8XmP8qQeGdQnJ+2eLNYkH9yEQQL/47Hu/8eoA6PeBknge9Yt54x8PWM5t5tWtmuRx9ngJml/74TLfpWJrfw5sL2zRrWSaa9hcSJ/ak8l5DMRn5JEkZhtOf4cYODzjB0vC2oadLHLp0OmQ6Tf2wH2nT1jVCnoy4GHQ9mHHbg5AAE/4SXU747dI8NX0gPSfUCLOIfUMDJ/45SDRfEOp86trv2WE9bXSY/L49GmfLn6qErpuvalxQBmaV4f0rRA/9n2UcLyHMsvLSSn1dzlmPuSa0to/+tS0UAIBiloooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACgjNFFACYFY2peFdJ1K5+2PA1vf4x9ttJDDP+LpgkexyPatqigDl1s/Felkiz1C01eAdItQTyJR7ebEu0/wDfv8af/wAJc9nxrOhapYY6ypD9pi+u6HcQPdgtdGwGKzdZ1qx0S0E965yzBIoUUvJM56Iijlief/rDmgBmneJ9E1h9mnatZXMg6xxzKXH1XOR+IrWya42PwsfE99HqniuytyqAm10wqrrEDxmVv+Wj47fcXtng1f8A+EH0iEf6DLqOnn0s9QmjX/vjdt/SgDo8njilzzXNDw9rEBP2XxfqRHZbq3glA/ERqx/OlFn4xh/1es6NcL6S6ZIjfms39KAOkormzJ41j/5d9Am9/Pmi/wDZGpftnjNeuh6E/wBNWlX/ANt6AOjormxqHjI/8y7o3/g5k/8Akag3XjRumkaBH9dUmb/23FAHSUma5zPjVzyfD8H/AH+lx/6Dmgab4smz5/iOwiz2tdLKkfi8rfyoA6PNNMgVSzkKo5JPAFc6PC15Of8ATvFWtz+qxvDAPw8tFb9aydd+Hlm4gv7CAX93bEsbbV5nuorpT1UmUsUb+6w6dxgmgDbufG3h62m8gapDc3A4MFkGuZc/7kYY1CfEGtX/ABpXhqdE7XGqTC2T6hQGk/AqtWvDmp6bqFk62FstlJbt5dxYsgjkt3A+6yjjp0I4I5BIraHP+NAHNjQNY1I51vX5hGetrpam1jPsXyZD+DLn0rX0zRNM0WAw6bZQWqE7m8tAC59WPVj7nmrwAHQUtACBQOlLRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFITgUtGM0AcP4l+J3h/RpV0+DU7OXVJZhbqhkzHbuTgtMw+6FPUdeMY9OhstJtdP065W5kFwbgF7y5mwDNxgluwXHAHQDitbaPrSbR+PrQB4l8JfFujeHdT1zwjPrVq1hDdmXTLqWdQkqMcFQxIGfukAdSWxXtqtn/9VLgUY5oAD2rnLjQtfluppIfF11bxO5ZIlsoGCDsMlcnHTnmukpMUAcx/wj/iT/odbv8A8ALf/wCIqxY6NrltexTXXiq5vIFPzQNZwIH49VXI/CugoxQA3p9a898V2h0j4neHPFsuV0/yZNOvJv4YA2TGzHspYkEngcV6GRnrSNGrqVblSMEHvQB558Uox4j0nT/CdlIJLvU7uJnVDnyrdDueVvRRgD3JwK9CRQgCqMKowB6VBa6bY2CstnaQWwcgsIYwmSOmcdatYoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKRiQOBmgAJwM1ja3r8OlGG2jhku9RuM/ZrKEjfIfUk8Ko7seB7niquo6/cXd7Lo/h+OO4v4ztubl8mCy/3yPvPg8RjnpuKjk3NE0C30cSyl5LnULghrq9mwZZz2zjoo7KMAfmSAVNM8PzSX66tr0yXepoP3EaA+RZg9RGD1b1kPJ6DaOK6LHNAULjHYYFLQAUUUUAJgelLiiigAxRRRQAYzSbR6UtFABiiiigBCAetY2u+HodXWGeKZ7PUrbJtb2H78RPUHsyHup4PsQCNqjGaAOc0XxDNJenR9agSz1iNchFJ8q6Qf8tISeo9V+8vfsa6IHJ6cVm63olnrlkILoSK6MHhnibZLA46OjdiPyPQgjisrTNbvNN1CLRPERQXcny2d8i7Yr0Dt/sSjuvQ9VzyAAdRRTVbPp+dOoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoooPSgApCcCgnAJrlrvXb3W7qXTPDbKoiYx3Wquu+K3YdUjHSSQen3V78/LQBc1nxF9kuV0zTbf7frEq7ktUbAiX/npK38Cfqeigmm6N4e+zXh1PVLj+0NXdCrTlcJCp/ghTPyL+rY5NXdG0Oy0S2aG1Vy0h3zTyuXlnfuzseWP8ugwBWkABQABQKWiigApMDOcc0tFACYHpS0UUAGKKKKADFJilooAKQjI9KWigDA17w6b6ePU9NnWy1qBdsVzt3LIn/PKVf40P5rnII7u0LxAupSS2N3B9h1e2ANxZyNnrwHRv44z2YfQgHIrcIzWRrug2+rwxSCWS0v7Yl7W9h/1kDH6/eU9Ch4bv6gA1gSTTq5vRddnF6dF1uKO11cAsjLnybtR1eIn04yhOV9xgnolYk9sUAOooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACkJwKDwKoarq9no1i15qEwiiBCjglnY9FVRksT2UZJoAuvKsSNJIyoigszMcAAdSTXKm+v/ABgfK0qWWx0Q8SaioxLdDuIM/dX/AKaHr/CP4qdHpV74plFzr8JttLDbodIJB8zHIe4xwx7+X90d9x6dSEAUKBgDoKAKunabZ6XZRWdjbpb28a4VE7fj3PUk9cknvVsKF6UAYpaACiiigAooooAKKKKACiiigAooooAKKKKACiiigBCM1S1TSrLV9PlsdQgS4tpeGjf65BBHII6gjkGr1IVDDBoA5K21S98K3Men69O1xpsjBLTVpOqkniK47Buwk6N3wevWgkmorq2gu7aW3uYklglUpJHIu5WUjkEHqCO1ckst14FOyZpbvwyOFlOXl04dg3d4R2bqg65AyADs6KjhmSeJJYnV43UMjqchge4Pce9SUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFITj3oADxVe7vYLC2kubuVILaJS0kshAVAO5PYVU1jXbLRbVZLtnaSU7ILaJS8s79kRByTx9ByTgAmsmz0K91m8i1LxOE/dkSWulo26K3P8AekP/AC1k6c42r/D60ARf8TDxn18/TvDx47x3F8P5xxH8GYf3R16azsrextora1gjgghUJHFEoVUHoAOgqxilxzQAgGKWiigAooooAKKKKACiiigAooooAKKKKACiiigApCAetLRQBnaxotlrdj9lvEYgMHikjbbJC46OjdVYdj/SsfTtZvNHv4tH8RuhllOyy1ILtju/RWHRJfbo38PoOoIyMGqmo6bZ6pYS2d9bpcW0o2yRvyCP5gjqCOc0AWlYknIIp1chDqN74QlS01u4a60d2CW+qyctCScCO4/QCTof4gDyetVs0AOooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApGOBQTiub1DX7q+vZtI8OiOS6jbZc30gzBZn0P/PST0QHj+IjgEAt614gj0xo7OG3e91S4UmCxiIDP/tMTwiDux/Ing19I8Pyi+XV9bnW91UA+XtBENoD1WJT0OOC5+Zh7cVb0XQbXRxKyM895OQ1zeTHdLMw6bjxwOygADoAK1gAKAAKAc80tFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAB1prIGXDcg9jTqKAOPeyvPBkr3OkQPdaEzF59NjG57UnkvAO69zGPX5cHg9NYaha6pYw31jcR3FrMu6OWM5DCrJHFcrfaRfaJfTaz4djEgmO+90zdtS5P/PSPssv6N35wwAOrorO0jWbPW7BbyykLoWKOjDa8Tj7yOp5VgeoPNaANAC0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFBpCSB2/OgAJwKwNY8RG3uxpWlW4v8AWHUMIVbCQKekkzfwr+rcgA1UutbvfEE8mn+GnEcCMUudWK7kjI6pCDxI/UZ+6p65PFbGj6JY6LZm1tI3+di8ssjFpJn7u7HlmPr/AIUAVdE8PCzuG1LUbg3+ryrte6dcBFJzsiXoiZA4GScZJNbgGKXFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBFNBFNC8M0ayRSKUdHG4MvcHPUVyQ+1+BjhRLd+GvQAvLpw/m8I/NB6jp2JGaRkBHPI9D0oAZBPHcwRzwyJLDIodJI23KynkEHuCOc1LXIzafeeE7iS90SB7nSXYvc6VGMtESeZLcevcx9/4cHg9Hp2pWmrWMV7Yzxz20q7kkQ9e3Poc5BB5BBFAFuikFLQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFITgUALRSA5paACiiigAooooAKKKKACiiigAooooAKKKKACqLavYpd3Nq95As9tCLidGcAxxndhm9B8p5/xo1fU4tI0ya9mWRxGPliiQvJI3ZUUclicACvAdP8BfETxL41fxXqFlZ2YnnWV7XUJcq8QYFYii5JUBRwwHT1oA9fN1qHjEhNPkm0/QTy16oKT3Y9Is8oh/56dT/DgYaui0/T7TTLGKysreO3tol2pEgwB6/XJ5JPU1YUZX5gMkcindKAAAA570tIDzziloAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApMA+tLRQBzer+H7iO/bXNBlS31XAE8cnEN6g/hlA6ED7rjke44ot/GukDSp7zUZxpsloyxXlvdEB7eQ9FIHUHPDDgjkGujbpXjfxb+GXiTxrqceo6Zd2Dw28WyO1dTFIe/3+QxznrtAB9SSQD2NSSeQPwp1cb8ONV1W78Oxad4gs5rXWdPUQzrMD+9UDCyKejAgckE8g+tdlQAUUUUAFFFFABRRRQAUUUUAFFFITigBaKQHNLQAUUUUAFFFFABRRRQAUUUUAFFBpAc0ALRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABSMcDPpS1keJNaOg6LNeR2k15c42W9rAhZ55D91QAPzPYAntQBLJr+lwjUDLf26DTiouyzgeTkBhu9MgjFYPl6h4zOZxcad4eYfLFzHcXq+r944z/d4Zu+BlT5X4K+HHjz/hMo/E+swWCLLdG5ube9k3GTJJyFUMAwySuSMED6V9BADIPegCO3tYLWCO3t4Y4YYlCpHGoVUUdAAOAPapQoBzSZxSg0ALRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFACEVzGo6NeaTezaz4dVfOlbfeaczbIrs92B6LKQMbujdG7FeoprDjH9aAMO38X6LNY21094tv590tmIbgFJUnJx5TKeQ/PT8eRzW6Dk4xXhfxP8AhX4u8ReJZfEOl3NjMVCiGCIeRKoX7vzdGb/aJHoMAAV6p4N1m71nQIZNTtJbPVYR5V7bypsZZBwWA7q3UHoQfagDoaKKKACiiigAooooAKKKKACiiigAopCaAaAFooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKMZoooA4zVNF03W/iJFBqdnFdRJpJdUlXIDebjP5Vf/AOFf+Ev+gBZf9+6P+alr/wBgc/8Ao4V0lAHN/wDCv/CX/QAsv+/dH/Cv/CX/AEALL/v3XSUUAc3/AMK/8Jf9ACy/790f8K/8Jf8AQAsv+/ddJRQBzf8Awr/wl/0ALL/v3SHwB4T4xoFl/wB8V0tc/wCL7/WLDTLU6LYTXc013HDL5BUPDEc7nG4EcYA5BHOaAIh4B8Jf9AGy/wC+OlH/AAgHhP8A6AFln/crC17WNU8L+NvC1jbXk97a6xM8E9rcbWZQAv71GABGMkkcjHYVqXWr3OqePj4atbpre2srIXd7LCBvdmbCRgnO0YBYkc9MEc0AWR4A8JE4/sCy/wC/dL/wr/wl/wBACy/791UsNaudO8fS+F7yd7iC4she2U0gG9cMVeJiAN2OCD1x1zXX0Ac3/wAK/wDCX/QAsv8Av3R/wr/wl/0ALL/v3XSUUAc0fh/4SIwdAssf9c6X/hX/AIT/AOgDZf8AfuukooA5v/hX/hP/AKANl/37rJ1Xwvoeia34ZuNM0y3tZn1Ty2eJcEqbeY4PtkA/hXdVzfin/kJeFv8AsMD/ANJ56AOiAxTqQUtABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAARmuD8O+EfD+sWl/e6jpNtc3L6rqAaWRMsQLuVRz9AB+Fd5XO+C/+QNd/wDYW1H/ANLJqAG/8K/8Jf8AQBsv++KP+Ff+Ev8AoAWX/fuukooA5v8A4V/4S/6AFl/37pD8P/Cf/QAsv+/ddLQaAOZPgDwmOmgWX/fuj/hAPCf/AEALH/v3WN4+1fxDoc2iSW+oW0drea3a2hjjtj5hjZiWDOWI5244UdetdD4x8QJ4U8JahrJjEjW0f7uPszsQqD6biPwoArjwD4TP/MAsv+/dKPAHhM/8wCy/74rP10614d8ITa6NTlutQsYhc3MbhRDOowZECgfKMZ2kcjAyTzXV6dew6lp1rfW7FoLmFZoie6sAR+hoAxf+Ff8AhL/oAWX/AH7o/wCFf+Ev+gBZf9+66SigDm/+Ff8AhL/oAWX/AH7rL8SeBfC9t4X1aeHQ7NJYrKZ0dU5VghIIruKyPFX/ACKGtf8AXhP/AOi2oAtaPzotgxJJNtHnP+6Ku1S0b/kB6f8A9e0f/oIq7QAUUUUAFFFFABRRRQAUUUUAIen0rif+Ee0nXfHuu/2pYxXfk21oI/NGdmfNzj64H5V256Gub0r/AJH3xH/17Wf/ALVoAP8AhX/hL/oAWX/fuj/hX/hL/oAWX/fuukooA5v/AIV/4S/6AFl/37o/4V/4S/6AFl/37rpKKAOb/wCFf+Ev+gBZf9+6P+Ff+Ev+gBZf9+66SkJwM0Ac23gDwkB/yALL/v3SDwD4TJx/wj9l/wB8VFr3iAx+LdI8MrcXFo+oRSyieCIMx2DIUEgqvG4kkH7uO+Rkza1qnhf4jaRoN1fSajpWtRyeQZwvm20sYyRuUDchyOvPPXAoA2z4B8J/9ACy9v3dKPh/4T76BZf9+6p2Op3HiTxdrdlFeS2+m6Q0duRbkBp5mXc5LYyAowoAxznOelS+GNdupfEmu+GtQmM9zpjRyQ3BUAzQyLld2AAWUgqSAM8cUAWP+Ff+Ev8AoAWX/fuj/hX/AIS/6AFl/wB+66SigDm/+Ff+Ev8AoAWX/fuk/wCFfeEj/wAwCy/7910tFAHN/wDCv/CZJJ0GyJPfy6Q/D7wlz/xIbLn/AKZ10tFAHFDw9pGh+ONDbS7CG0M0N0JPKXG8AIRmu0ArntU/5Hbw7/1yu/5JXRUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFHWiigDz3wZ4L8N6j4M0i7vNHtprma2V5JXXLMx6k+9bo+H3hIH/AJAFl/37pfh//wAiBof/AF6J/KukoA5v/hX/AIS/6AFl/wB+6P8AhX/hL/oAWX/fuukooA5o+APCQGf7Asv+/dNPgHwmP+YBYgepSugvLhbSzmuXVmWJGchRkkAE8e9cNpl3qXi/wSviPStfvbW9nieSGLyoxDG6kjyyrKSVyMFs5PUYzigDa/4QDwl/0ALL1/1dH/CA+E/+gBY/9++tc3a/Ema6+Elv4oW3iXU7hhaRxc+WbhpPLzjOdv8AHj04zWn4outS8H+Gv7fTUbm+NmyNewzBdtxGWCvtAA2EA5GMDjkGgDSHgDwmf+YBZf8Aful/4V/4S/6AFl/37rfgljniSWJg0boGUjuD0NS0Ac3/AMK/8Jf9ACy/790f8K/8Jf8AQAsv+/ddJRQB5/4v8G+HdN8LXl7ZaRa29zDsaOWNcMp3r0rvwBnNc549/wCRJ1L6J/6MWujFAC0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHN/81LX/ALA5/wDRwrpK5v8A5qWv/YHP/o4V0lABRRRQAUUUUAITgVRv9VsNPe3hvr6C1ku38qBZJApkf+6uep9qv1geJPC9rr8um3biEXmmXH2i1eePzEDd9y5BI4B6jlR9CAcb4nuG8DeMNF168ll1WG/m/s+SW7AMtru5DRbQqgHncNueOtWfD6tH8c/GAfjzLK0aP3UKAf1rV1Lwhd+Jtb0q71+4txZ6ZL9oisrVSRLNxh3c9hjIUDuck9K0dQ8PyHxLb+I9Mkjj1Bbc2k0cufLuYt24AkfdKtyGweCRg8YAOa1cNJ8evDvlnPlaVcPJjspJUH869GFc/pfh6SLxLeeIdRljl1CeBbWJYwdtvApLbATyxLEknjtgCuhoAKKKKACiiigArm/FP/IS8Lf9hgf+k89dJXN+Kf8AkJeFv+wwP/SeegDoxS0gpaACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACud8F/wDIGu/+wtqP/pZNXRVzvgv/AJA13/2FtR/9LJqAOiooooAKDRSGgDz74sAfZPCf/YzWXP8A33UfxxVj8L75wMpHcQM/PbzFH8yK1PFvhPVvFMtiP7bs7W3sb6K+gQaezvvjzgM3nAEcnoB9a2L3RX1vw7d6Rr0tvdJdI0cjW8BiGD0IVmbkHB69R0oAqePnC/DvxGS2d2mTqPfMZA/nTvAMckXw+8OpJkMNNt+CMEfu1xVK98N6vrHh5PD2pX9u1kyrHc3MSsJriNcfLtPCE45OT3wBnjrYo0ijSONQqIoVVHYDoKAH0UUUAFZHir/kUNa/68J//RbVr1keKv8AkUNa/wCvCf8A9FtQBa0b/kB6f/17R/8AoIq7VLRv+QHp/wD17R/+girtABRRRQAUUUUAFFFFABRRRQAHoa5vSv8AkffEf/XtZ/8AtWukPQ1zelf8j74j/wCvaz/9q0AdJRRRQAUUUUAFIelLSHkYoAr3dxb2VrLdXUqQwQIXkkcgBVHJJPpXG6Zos2teMk8Ya0n2dIYzbaRZS8MitnMjg9HcHhewIzz0v+MPDGt+IpLIaX4kGkR2z+cy/YVuPMkH3WO5gPl6gY4OD1AxS0nwd4ot9bsr7W/Gz6va2rtItodNjgBcoyA7lY9NxPTrigCj8Mg0fiDx2kh+f+25G5GPlP3f0o0RXf48+JZFBKR6Xbo5HQMcED8s10Z8P3OneJb3W9HeHdqEaLeWs+5VkdBhJFYA7Tg4IwcgDpjJm8PeH/7Ju9T1K5mFxqepyrJcyqm1QFXaiKP7qjPXkkn6AA3qKKKACiiigAooooA53VP+R28O/wDXK7/kldFXO6p/yO3h3/rld/ySuioAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOb+H/8AyIGh/wDXon8q6Sub+H//ACIGh/8AXon8q6SgAooooARulcp4sur69t5PDuhsq6jdx7Zbg8rZwHgyN/tEZVV6k89Aa6iYSGFxE4SQghWK7gD2OOM/nXm8HgHxxatOYfiS0ZnlMshGixEsx6kkvk9h9AB0GKAMzx9odj4c+H3hrStMJa00/W7ZZGyGJOXLFvcs2T0611PxYZV+Fuv5YBfs6jJPq6irI8FJd+BZfDms38l9JMXkmvhGI3MrSGQOByAQcYHoAOlGp+HdU8R6dBpOt3Nq1gsqPdmBGDXgQhgpU8RqSMnls44xQBs+HI5YvDWlRzKVkSzhDqeobYM/rmtSmr6U6gAooooA5vx7/wAiTqX0T/0YtdGK5zx7/wAiTqX0T/0YtdGKAFooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOb/5qWv/AGBz/wCjhXSVzf8AzUtf+wOf/Rwro2JA4oAWiszR9Zi1m3uJ4ABHDdS22Q2dxjYqT+YP+NaJYjPH/wBegB1FNDEnHAP1pN/GTjnp70APoIzTdx78D1oyfpQA7FJiszQ9ai13S11CAKIHllRCGzuCSMgYH32g/j1NaO5vSgBwAHSlpu488UbjnpQA6imF8f5/pS7uvQigB1FN3fhWfqWsRabcabA6hnv7oW0Yzg52M5PvgLQBpVzfin/kJeFv+wwP/SeeuiDZOPbNc74p/wCQl4W/7DA/9J56AOjFLSCloAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK53wX/AMga7/7C2o/+lk1dFXO+C/8AkDXf/YW1H/0smoA6KikJwM0m7/IoAdQRkU3cfSjefQ0ALj60bRTd5OSOcelU9W1OPSNIvtRnA8u0geZstjIUE4z+FAF7aKXFQW9wbi2imC7fMQNjrjIzUm/IPTOM9aAH0hOKTf3PA9aD8ykcjNACg5NZPir/AJFDWv8Arwn/APRbVz/hC91U+N/F2j3+qT6hb6cbP7M08casokjZ2BKKoPOO3YV0Hir/AJFDWv8Arwn/APRbUAWtG/5Aen/9e0f/AKCKu1S0b/kB6f8A9e0f/oIq7QAUUUUAFFFFABRRRQAUUUUAB6Gub0r/AJH3xH/17Wf/ALVrpD0Nc3pX/I++I/8Ar2s//atAHSUU13CIWJAUcknoBWfomrpreh2eqRJtjuoxIq5zhT09O1AGlRTdx44o3Htz9KAHUEZpu/2o3etAC4pcVBc3UVpaTXM7hIYUaR3PQKBkmq+k6l/ami2Oo+WIhdW8c+zdu27lBxn8aAL20GgDHSkDHvijd/nFADqKbuPpRu9ce9ADqKYXODgcil3fSgB1FZl/rMVjqmlaeyq02oyyIilsFVSNnLYxz0UY4+9WiCT1xigDn9U/5Hbw7/1yu/5JXRVzuqf8jt4d/wCuV3/JK6KgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5v4f8A/IgaH/16J/Kukrm/h/8A8iBof/Xon8q6PNAC0UmcntSB/b8e1ADutJtFJuJHA7UBiTjH44oAUrn1owD15rO1rWI9F00Xkqhg00UKrnGWkkVBzz3atDdx+uKAHYopu44o3H256UAKTgUAk1FOjXFtLEk7wO6FVli2lkyOGGQRkdRkEcdK5L4d6lqWoW3iCPVL+S+lsdbubKKaSNEby02hQQiqM9TnHegC/wCPf+RJ1L6J/wCjFroxXOePf+RJ1L6J/wCjFroxQAtFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBzf/ADUtf+wOf/RwrcvrOG/s5LacyiJx8xhmeJvwZCGH4H2rD/5qWv8A2Bz/AOjhXRk4oA8x+D+i2kXg63u0e8V4725RYxezCLAlZRmPfsPHqP1qT7DP/wALjutIi1HUl06bRheTRG9lf955zLhdzHYDweMfdx0OK0vD3hXVNEsW0SQ20mmpqRvY7lZGEpTzfNVCm3Gd4AJ3YxSjRvEi/EaXxILXSjavp408RG+k37RKX3/6nGecbf1oAydcu7zwyfCPg0avqNwdQml+1ahhmuWhjG/YNoLAnIXcOQB+NajvdWnizR30KHU20+dpINRhmhnWNV2kpKDIBtYMMEjkg89BjU8UeGpNam0vUbKWO31bSpzPaySAsh3DDxsB/Cw4yORgYq9Eur3VxAbiOCyhjbfKsMxlaYgcLkouFzznqcYwM0AcrGqz6nr1lq2qNd3zXLmz/sozNLZREDYH2/LGw98A85Jzip/Bt5J44+FNhd608ry3ULidoZXgMhR2TOUIIztGQOPwp3hTw1rfhXTL/S4vsdx9ou5Z01BpWDneR80ibeWHs2CAOVqTwN4Y1HwxpUGk3U8T2dis8UDRysxuFklLhnUqArKPlGN3VuncAy/g7pVsvw60LUBLeee0cnyG9mMX+scf6ovs/wDHffrTWs50+NcmkwalqCafPopvZ4WvZX+fzyp2ZbKfw/dxwMdDWl4S8LaroOkaXoVw1s9npVzLJFdRyNunQ79gZNvyn95k8kfKMZzwv9h+Iv8AhZp8TfZ9LNl/Z/8AZvlfbZPM2ed5nmf6rG7ttz/wKgClZJdr8VNT8Ox310uj/YIr4wmdyyyFiu1HJ3Kp6kAjkDoCQbXh25ntfiJ4n0P7TPLp1vb21zBHNIZDCzA7gpYk4OAcZwMcYp9tofiOH4k3PiKS30v7FPaLZGNbyQyKivu348rBJz93Ix6mjSdC8RWvxB1PxBdW+l/ZNQhigKR3chkjWMHDYMQDE56ZGPU0AY/g7WIvF+gT61rEmqtJfTSi3jto7jZaRKxVAhjG3eMZL9eevGKzvFVzrifBW41XUp9StNa06QRLKs0luZlM6IHdFIDbkx1HBJIxXYaBoGpeEI7rTtLitLrSGmea1SaZonttxyYz8jBkyTg9Rnoe1Xxv4a8QeI/BFzoNtPYy3F626a4uJnjWHEiuFRVRtw425JBHB5zigCv48F34c8M22tW1/d/2lDe2xlk89/Ll3SBXUx527Tk8ADHHeqvj3RbO4+Ivgud2uhLc3U0blLuVMKsWRsAYbOnJXBPfNanjbRPEnirwvFpkFtpVvO80csrPeyMiFHDAL+5BbOOcgY96teIdD1XV5/D+rJHZxajpV00zWpnZo5EZSrKH2A5wcg7cZ496AOlsLOGwtxbwNMyIT/rp3mbJ55ZySfxNYvin/kJeFv8AsMD/ANJ560tItZ4BdT3WwXF3OZnSNtyphVQKCQM/Ki54HOazfFP/ACEvC3/YYH/pPPQB0YpaQUtABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXO+DP8AkC3n/YW1H/0smroq53wX/wAga8/7C2o/+lk1AGX461q9h1Xw54csbh7WXWrp1luY+HjhjUM4U9mIIAPbmr7eH7uw8Qabc6Xdi30uJJVv4JJZJGnyvyHLMRkHknqemTmneLvDDa6unXtncJb6rpdyLmzlddyE/wASMBztYcHHP8quRDWbue3+1RW1lFE2+RYZ2mMpwQFGUXC5Oc9TgDAzQBzPgaX/AITnQG8S6o07C9nlFtbrO6JbxI5VQApHzfKSW688EDineGLqTWrnxR4U1O6u5TpN0kaXSXDxytC43x5dCCWGCCf4u+eavaDoGo+D7e503SobW70pp3ntklnaJ7fectGcIwZQckHg84wetO0Hw9feH7XVb1BaX2u6ncfablnlaGHPRUBCuwRV6ZBJ56dgDmvh5b3Wv/Ca21DU9S1CW5eGcRSreSq6bZHG7IbJbIJyc8YA44rM8QSN4n/Z1j1jVJZZb1LJXLLK6K7hwNzqpCueM/MCAemK6jwf4e8SeGPACeHpYNKnuYFkSKVbyRVk3s7EsPKyuNwAxnOO1Q6b4J1Q/CiXwZqr2UTrbGGK5tZnkDNuLAspRcYOOATmgDqNF0a1tNNRYpr5hcW6BvOv5pSBj+He52nnquD+lcJ4b0a/1uXxnpJ13Vre3tdSMNlMt5I8sJ2oT8zMSwAAABOOT3wR2elWmrobe51GG0W5tLI20aQzsySO20sxJQFQTGuOCRk1meE9F8RaNrGtT39vpf2fVb5rwmC8kZ4SUA2hWiAf7o5yKAILK8m8Q/EDVdCmubj+ztBtrdXRZShuppVLb5CuCQFH3ehJJIPGHjUZfDvxL07QI5ZZNM1i1leKGRy5t5o/mO0nkIV/h6A9MdKvzeHrnT/F9x4k0cQyPfQJDfWs0hQSbPuSKwDfMBxgjBB6jHK2/h27vPGC+JtV8gS21u1vY2sLl1iDHLOzkDLHpwMAetAGd4V/5Kr4/wD+4d/6INdJ4q/5FDWv+vCf/wBFtWB4e0XxFp3jbXdZvrbSxbawbfcsN5I7w+VGU4BiAbJPquPet7xQ27wfrX/XhP8A+i2oAt6N/wAgPT/+vaP/ANBFXapaN/yA9P8A+vaP/wBBFXaACiiigAooooAKKKKACiiigAPQ1zelf8j74j/69rP/ANq10h6Gub0r/kffEf8A17Wf/tWgCbxpp9vqXg/VYbrzTGtrLJtjmePcQjYB2kZHseD3FYnw00m1h8BaDeCa882XT4wyyX0zRjKjO1GcqvTjAGO2K7DUbVb/AE25s3YqlxE0TMOoDAjP61y3hfQNX0zStG0u/FoY9GVhFLDMx88hWjTKlfl+VzkZPOMUAc7o2jXeo+LfHOgDWtVhsLX7Ots32yWSSJ5Ic5Dsxb5eSATg7snOBVzU9WuI/G+i+Dpr3UHtLXTBe3s8KSNNdkMI1DeWNwBILNjg5weMg6Ph7RPEmm+Mdd1a8ttKNvq8kDMsV7IzwCKPZwDCA+eD/Dj3q/rXhu4n8S2HiXSpIV1O0ia2kjnJEdxAxyULAEqQfmBAPuOeACjpkl9b+N/IsYtRbQbqyLOLiKULb3CsMbPMAIDKTwOMj3Och5SdL8Qw32ryXetwy3EkFxpJuHFqvWNHKgohGOVOBxyD1PbRRapdXXm3fl2lusbKsNvKXZ2bHzMxUYwM4Az94k9AK5nwz4U1fQPBz+FXFo6MZUbUUlYNIsjElyhX74DYA3EcDntQBnavN/wlvwIk1bUzI9ydIe4PlStErSiM8lVIBGRnacj2rpPBGk21n4Y0i4jmvGmm0+HcJr2aVRlFPyo7FR+AGPxrN0TwbqMHgi58NapcwiN9OOnRPA7MCP3n70qQMMQy/Lz93rzWn4b0nV7K10qDUktVOl2P2RTBMzidsIA/KjaMJ05+8fTkA53wvYzS/Efxlo76jqL6VZfY3jhlvZXbdJEW++zFtud3GecjOQAKm8JpdX3irxVo11fXUul6Xdp9ljM77x5ibirPncyrjgE/xHOcDF3QND8Sad418Q63d22leRrHkHy4r2Rmh8mIoOsI3Bj9Me9J4c0XxJpPiXxBqd3baU0WrSpMqQ3shaJkj2qvMQBBIHPGOeDQBR8PeIbjSdI8ctczTXcGg3lx9mWaQu4jWMOIy55IzkAkk89elV9HkfVvAUV/cXmsnX762+1LdwW9yBFIwDKqBV2bBhVx91sc5zWn4b8MaxZ3viRNbtdMksNduHnlWC6kdowybSmDEoYYHXI69K0PD+la94a0aPRIhZ3ttagx2l1JM0biP+ESIFOSvTIPOOi0Acr40udYPgDQNcle8sNdFxaJLEtxJHGXZhuV41baQT6jOOOnFa3jPz/DZ8N39jf3v2iXWra1uGkuXZZ45NwYMhO33GANvbHSpvGXhvX9c8P2GlWMtjM8M0NxLdXk7xs7xtuwFVG6/UY9DS+MtE8SeI7bRo7a20mFrK/hv5PNvZMFoy37tcQ8g5HzHH+7QBkeKdCspfjF4TYteK93DemVo72ZD8ka7QpDjYBk8LgHJzXpFnaxWdskELStGmQDLM0rdecsxLE/UmuY1zQtUvtc8OeIreO1+3aUJlmtGmIR1lQK22TZnKkAglQDz0re0i0ms7RkuChmklkmfYcgF3LbQTyQM4zgZxnAzgAGdqn/ACO3h3/rld/ySuirndU/5Hbw7/1yu/5JXRUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHNeAP8Akn+h/wDXon8qyvEGp3OpfEXSvB8VxLb2b2b6hetC5R5UDFVjDDBUFhk4OcDGRmtXwB/yIGhf9eifyqLX/DVzdeItM8SaTJEmqWKtC0U5IjuYW+8jMASuPvAgHkcg54AK9/De+FLvVdciuVOg2ulPIbDe7OZ0+bcCxIA2jHGM55zUfhbSv7e8IWGr6rdXMmo6jbpcmaKdo/I3jKrEAcIFBA464+bOa2nsbzVxNBqscUNhLBJA9rFKX83eNpLHC4AXIAGeueMVmaNpniHw5oEeiWaWF5Hap5VndXE7oVQfdEiBDkqMD5T82P4aAObh1m68S/CTxBcXt1cxapov2yFrq0ne3Mk0CMVk/dkZBypKnjPbpT7iO7j+Ci6++o3w1ZNIjvI50uXG1hGGUbc7TxgNkHdznNa58G3mlfDu+8OaO1td3eoJOtzdXcrQh5JlbfLhUfJyRhfQfeqKfw54jn+F48LCDSlu/sK2BmN5Js2CNU8z/VZzx93p/tUAY3xGtIdY8KeFdXujP9qmv7AHy7iREG85YhA20NyfmxuHrXoUeiWcdhPYCa/MUpBdm1CdpB06SFy69B0Pr71zWp+FtX1bwLp+lTmyt9U02W3mt2SZ5IpGhxjcSilQ3OcA49TW/bQaokl7qL21r9tljiiitjct5YVNx+Z9nBzI/RTwB+ABwngPR9S8ReDoJ7nxBqkU9rqcnkzLcszNHHM2VfcTu3cgk54AHrna0m/bxb4y8Q2tzJINM0WSO1gto5GRZJCCXeQDBbkBQpyowTjPNWfAGha74a0dtL1SLTmi82adZrW5dzudy23a0agDk87vwqxH4evNF8ValrejrBNFqojN3bTytHiRAQHRgrdQSCpHPByMYIBR0/U5tE+Jr+FTPLNYXmnfb7YTSF2gcOVZAxySpAyMnjGBwab8M/8AmcAev/CTXv8A7JV6x8PXy+Jr3xTf/ZpNUa0+x2drHKwihjBLYLlclmbGTt4HY1H4G0PXNBn1tdTi07ytR1GfUVa1undo2kK/IQ0a5AAPzZ/CgC749/5EnUvon/oxa6MVznj3/kSdS+if+jFroxQAtFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABSHPaloIzQBx+rXz6R46iv5NO1K5tn00wb7Ozkn2v5oODtBxxVk+NrQ/wDMG8Rf+Cef/wCJrp8UYoA5f/hNLP8A6AviP6f2PP8A/E0f8Jraf9AbxH/4KJ//AImuoxRigDmP+E2tP+gN4j/8FE//AMTSf8JraAYGi+Ih9NHn/wDia6jFGKAOX/4TWz5/4kviLnr/AMSef/4ml/4TW0zn+xvEf/gnn/8Aia6fFGKAOX/4TSz/AOgL4j/8E8//AMTR/wAJraZz/Y3iP/wTz/8AxNdRijFAHLjxraD/AJg3iL/wTz//ABNH/Ca2YGP7F8Rf+Cef/wCJrqMUYoA5g+NbQ/8AMF8R/wDgnn/+Jo/4Ta05/wCJN4j5/wCoPP8A/E10+KMUAcv/AMJrZ5yNF8Rj6aPP/wDE0f8ACa2fH/El8Rcf9Qef/wCJrqMUYoA5geNrQf8AMG8R/wDgnn/+JrOv9b/t3WvDsVrpOsx/Z9S8+WS506WJFTyJVyWZcdWX867jFIQDQAClpMUtABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFACGuG0PxCNDt72xu9G10yjU76QNDpk0iMr3MjqQwXBBVga7kjNAUAYA4FAHM/wDCb2n/AEBfEX/gnn/+JpP+E1s8Y/sXxFj/ALA8/wD8TXUYoxQBy/8AwmtnnP8AYviLP/YHn/8AiaP+E1s+2i+Iv/BPP/8AE11GKMUAcx/wmtof+YL4j/8ABRP/APE0n/Ca2n/QG8R/+Cef/wCJrqMUYoA5f/hNbT/oDeI//BPP/wDE0o8a2gGBoviLH/YHn/8Aia6fFGKAOX/4TW0zn+xvEf8A4KJ//iaX/hNrT/oC+Iv/AATz/wDxNdPijFAHLjxraAD/AIk3iP8A8E8//wATWfr3iuG+8PanaW+ieITNPayxIG0mcAsykDkrx1ruMUmKAKmlI8WkWUUilXSBFZSMEEKODVygDFFABRRRQAUUUUAFFFFABRRRQAh6VxkmqHQ/G2sTz6Zqs8F1bWoils7GSdSU8zcCVBx94V2hGaTHNAHM/wDCbWh/5gviL/wTz/8AxNJ/wmtn/wBAXxF/4J5//ia6jFGKAOXHjSzA/wCQL4i/8E8//wATS/8ACbWn/QG8R/8Agon/APia6fFGKAOXPjWzOP8AiS+IuP8AqDz/APxNL/wmtp/0BfEX/gnn/wDia6fFGKAOX/4TSz/6A3iP/wAFE/8A8TS/8Jtaf9AXxF/4J5//AImunxRigDlx40sxj/iS+Iv/AATz/wDxNH/Ca2g/5gviL/wTz/8AxNdRijFAHL/8JrZ/9AXxF7f8Sefj/wAdpf8AhNbT/oC+Iv8AwTz/APxNdPijFAHL/wDCaWeMf2L4ix6f2PP/APE0v/Ca2n/QF8Rf+Cef/wCJrp8UYoA5ceNbQf8AMG8R/wDgnn/+Jpf+E2tP+gL4i/8ABPP/APE10+KTFAHGxaqdb8ZaNJb6ZqsEVtFc+bJd2EkKgsEwMsAM8GuyFGKWgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoJxRR1oA8/8J+JV0fwppmnXeia+Li2gWOQJpUzDI64IXBFbP8Awm1p/wBAbxH/AOCef/4mumwKXFAHLnxraHro3iP/AMFE/wD8TS/8Jraf9AXxFx/1B5//AImunxRigDmP+E2tP+gL4i/8E8//AMTSf8Jraf8AQF8Rf+Cef/4muoxRigDmP+E1s/8AoC+Iv/BPP/8AE0n/AAmtpn/kDeI//BPP/wDE11GKMUAcv/wmtnnP9i+Iv/BPP/8AE0v/AAmtp/0BfEX/AIJ5/wD4munxRigDl/8AhNLPg/2L4jyP+oPP/wDE0v8Awm1oD/yBvEf/AIJ5/wD4munxRigDgfFHiNdZ8O3WnWmi6/585RU8zSpkX76nklcDgV3oNLikxzmgBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiikzj6+lAC0VHLPFBC0ssiRxqMs7sAAPrWBJ468PbzHa3zajIONmmwvdHPp+7DAfjigDo6K5r/hJNWuf+PDwnqLL2kvJYbdT+G9nH4rR5vjS4Ixa6FYg/3p5rkj8ljH60AdJnmjdziub/ALL8Vyk+b4msoge1rpW3H/fcr0o8Oau/E3jPWPpDBaIP1hJ/WgDo80ZrnP8AhE5z9/xT4gY/9d4l/wDQYxS/8Ik//Qya/n1+1L/8RQB0ROKM1zn/AAilyo+TxX4gT386Fv8A0KI0n/CO61H/AKjxlqpx2nt7V/5RKf1oA6Wiua/s3xbCcxeI9PmHpc6USfzSVf5UfaPGdsMvYaHej1ju5bcn6BkcfrQB0tFc1/wk2pW3/IQ8KatGO8lqYrlPyV95/wC+akh8ceHJZRDLqSWcx4EV+jWr59MShSfwoA6GimJIrqHRgykZDA8Ee1OByaAFooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKQnAzQAtFJux14qG5u7ezhaa6nigiX70krhFH1JoAnormz460KQkWE8+qEHH/ABLbaS5XP++gKj8TSf8ACQ65c4+xeErxQejX11DAPyVnYfiKAOlpCfauc3+NLjkJoNiPQtNdEfpHSf2V4pl/13im2jz2tdLVMf8Afcj0AdJmjPPSubHhvVZP9f4y1kn/AKZQ2iD/ANEk/rTv+ETnP3/FHiBj/wBd41/lGKAOizRn3rnf+ESbH/Iya+D6/al/+JpP+EUul+54s19T6mSBv/QojQB0eaWua/4R/XIz+58Y6kw9Li1tX/8AQYloFh4vg5i1/S7j0W40tgT/AMCSYY/KgDpaK5oXfjK2/wBbpWjXijqYL2SFvwVoyP8Ax6j/AISq8tzjUPC2s247vCkdyv8A5DYsf++aAOlorAtfGvh26uFtxqtvBcscC3u828p/4BIA36VuhsgEdDQA6ikzziloAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoopGJAyOaAFJxTDIACzEAAZJJ6Vjax4ii06eOxt4JL/VJhuhsoCNxH95yeETP8R7jAyeKpR+GbrWWE3im7F0h5XTbclbVPZv4pj67vl9FFAEk/jK1lnkttEtLnWrhCVb7GB5KH0aZiEH0BJ9qZ9h8U6pg3uqW+kQn/lhpyebKB7yyDH5Rj2Pr0kFvDbQJBBEkUUY2okahVUegA4FPxQBz0PgjQRKs15aPqVwvIm1KZrpgfYOSF/4CAK30iSJFSNFRF4CqMAU7FLQAmBS4oooAMUUUUAFFFFABRiiigAxRiiigBMCo57eG5iMU8SSxnqjqGB/A1LRQBzcngjRYmaXTUuNImJzv0ydoBn1Ma/I3/AlNMMPi3Sube6tNbgH/LO6X7PPj/rogKMf+AL9RXTkZppQGgDn7bxjp/2lLPVI59HvXO1YdQUIJD6JICUc+ysTXQBiT07VFd2lve2slvdwRzwSDa8cqBlYehB61zh0HU9A/eeG7nzLVeulXshaPHpFIctGfQHcvsOoAOqorI0bxDa6wJYlSS2voMC5srgbZYT7juvowyp7GtYHJPtQAtFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABSE4xxQxIGQKwdV8SfZ7z+y9Ltv7R1YqGMCvtSAHo0z8hAewwWPYHsAbcsyQxtJIypGoyzMcAAdST6Vzh8Yx6huj8OWE+ssOPtEREdqp95m4Yf7gc+1JD4TOoyrc+J7r+1ZlIZLXbss4j6LFn5yP7z7j6Y6V0wRVAVRgAYAHGBQBzQ0nxLqfOo63Hp8Lf8u+kxjdj0M0gJP1VUqe18FeH7ecXMmnreXQ5Fxfu1zID7NIWI/DFdBijFACBQAABgDgAdqMAUtFABijAoooAKKKKACiiigAxSYpaKAExRtHpS0UAQXVla31u1vd28VxC3DRyoHU/UHisFvBGl23z6RLe6O/b+z52SMf8AbI5j/wDHa6WjrQBzGfF2k8/6HrsA6jH2W5x+sbn/AL4q1p/i3Tb27WwnM2n6i3Szvo/Kkb/c/hk+qFhW5gVV1HTLHVbNrTULWG5t26xyoGH157+9AFoNk4pa5Q6drXhsh9Gmk1TTx1028lzKg9IZm6/7rkj/AGlrY0jXbLW7d5bOQh4n8ueCVSksL/3XU8qf59s5oA06KRTkUtABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFACMcDisLxBrFxbNb6XpaRyavelhCHGUhRcb5n/ANlcjjuxA75reNcx4XQajqGsa/J8z3F09nbn+5BA7R4HpmQSN77h6UAaOh6FbaLBJsZ57udt91dzcyzv6sfboAOAOABWqBg9TQF296WgAooooAKKKKACiiigApCcUtQXt1FY2M93OcQwRtI5HZQCT/KgBZbmKBA00iRKTgF2wCfrUm/2xXlvhDVNO1nw9P4l8QOp1HUfNk814DKlhBuKogYqVjXChucbs5Oal8QCfw34M8MeGYNVnvG1PUrfT5L1SEZoHcs5XB4G35RjoKAPSY7mOUuEljdkOGCtnB9D6VKCTXn3xHW28K+FrfxHpdtFbT6RcwMnkIE3ws6o8Rx1UqRx7A9q9ATGMjv3oAdRRRQAUUUUAFFFFABSbQaWigDE13QI9U8q7tpms9Vtuba9QZKeqsP40PdT19iAQ7w9rLarbzRXUAttSs38m8tgciN8ZBU90YEFT6cHkEDYbpXMaug0vxfo+qx/Kt6Tpt1j+IbWkiY/RlZR/wBdT+AB1FFIOtLQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABSGlpCcCgDnde1S7mvY9A0ZwmoTR+bPcldws4ckb8d3JyFHqCTkKQdPR9Hs9EshaWaMFLF5HdizyufvO7HlmPcn+QFZHgmMXekya9JzcaxIbvceoiPEKfQRhPxLHvXTAYOaAAKF6UtFFABRRRQAUUUUAFFFFACE4FQvdQxuiSSxo7nCKzAFj6D/61Z3irXE8NeF9R1h1DfZIGkVT0ZsfKPxOB+NcNpdvpV74H2apqQj1nV7ZGu9Xlg3iOWUAhN5GwbchQmR7DJoA9P3VHDcw3G7yZY5Npw2xgdp9DXA+MJJLMeDvCTXM0w1G7jhu5WOGnhiUM4JH947c+xI70/wAfyQ+Fk0LX9Ohjtnh1GG2mEShRJbPkMhA644I9CKAPQATmlpBxS0AFFFFABRRRQAUUUUAJtH51ga7oEk9wmr6TIttrUC4SQ8JOg58qQd1PY9VPI7g9BSGgDM0HWI9b04XKxPBKrGOe3k+/DKvDI3uD+YwRkEGtSuYYf2V8QI2jGIdatmEo7faIcFW+pjZgf+ua+ldMKAFooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigBG4HFc34JPk6Rd2Df62y1G6hcHrgys6H8UdD+NdIelcprDN4Y11vEKKTplzGsOqYGfJ2/cuMdwAdr/AOzg9FNAHWUUyORZUV0ZWRgGDKcgg9CD3p9ABRRRQAUUUUAFFFFABWb4gsX1Pw5qVjEMyXNrLCn1ZCo/nWlQRmgDz74QQ2w+E2loUAVlmE4cdSJGDBvTGMfQV51pC3r/AA38AajdFja2HiSIJI5+7bmQgMT2APH5dq9mm8HaZI14I3u7eC+cyXdvBOyRzMfvEgcjPfaRu5zmtGbRNMn0c6RJZQnTjGIvswXCBB0AA6Y9ulAHGfGVJLr4cXenxAG5vrm2t4Ezgu5lQgfof5130MYiiSMdEUKPwArKi8NWYurW4uZ7u9azObUXUu8RNjG4f3mxxubJHODya2Me9AC0UUUAFFFFABRRRQAUUUGgBD0rmvFH7/UvDdgmPNk1JZsf3Uijd2P0ztX6sK3ru7gsrSW6upo4YIVLySSNhUUckk9hWB4fhn1bVZvE15C8KyR/Z9PgkXDRQZyXYHkNIQDjsFUHnNAHTL09qWgDAooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApkqh4mQ9GGDT6RunXFAHO+A3J8D6PbsAJbS2WzlUdpIf3Tj/vpDXR1yUsg8JeIJruQ7dD1WVTK5+7aXWAu5vRHAGT2cc/erqw2SRigB1FFFABRRRQAUUUUAFFFFAHIfFKwl1L4Za9bQKWkFv5oA6ny2DkfktZk0dqfgNIu0GA+Hi4J558jduz655+tegkBhg1zn/CD6MbU2JW5OmGTzP7O89vs+d27G3+7nnbnb7UAedEXz2fwk1bUN++OYQSO5Od0qAR5z3IUfifeul+Ltu2oeHtI0yL5pr3WbaFAP8AgRJ+gAJrt9R0ix1bTnsL23WW1fH7vJXGDkEEYIIIBBHI7VUh8OWiahb31zNdXt1bKVt3uZN3kg8EqAAMkcbiCfegDXFLSAYFLQAUUUUAFFFFABRRRQAUh6UE46VS1TVbTR9Pmvr6URwQgFmwSSScAADksTgADkkgDrQBj6ti48c+HrZOWto7m8k/2U2CIZ+pl4/3T6V0o61z3hqwu2mvNb1SLyr/AFAqBATn7PAufLjz68lmxxuY9gK6EDFAC0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFRXMxt7aSZYpJSilhHGMs2Ow965LTviRpuqeJZPD1vpurLqUJHnxyWwUQrx8zktwOQffPFAHZUVheJfFNp4V0o6nfW91JarIsbtbxhyhY4GRkdSQPxHrVrRdYOs2zT/2bqFkvG0XsPlM2f9nJI/HFAGnRRRQAUUUUAFFIxwP6UySZIkZ5GVUXJLE4FAElFZuka9pmvJcvpd7FeR20xglkhJKCQAEgHoeGHIJH61pUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUhUMpUgEEYIPcUtFAHJfYNR8JO8mjW7X2ikln01T+9t88kwZ4K9/LJ7/ACkY2nc0jXNP1y2M+n3KTIp2yLgq8bd1dTyrexGa0CAetY2q+GLDUroXymWy1JV2rfWbeXKB6E9HX/ZYEe1AGyCe9LXLi78UaIMXdlHrlqP+W1lthuAPeJjtY+6sPZTV3TvFmi6ncfZYr1YrwdbW5RoJh/wBwCfqARQBt0UmetAOaAFooooAKKKKACiiigAooooAKKKKACiig0AFFJurBvfGOjWdy1nHO99fr1s7CNp5Qf8AaC/c+rYHvQBvE4rL1nxDYaHGn2uRjPKdsFtEpkmnb0RByf5DuQKzDJ4p1wYSOLw/aH+JytxdsP8AdGY4/wATJ9BWjpPhzTtIkkngjeW8lGJby4cyTyfV25x/sjAHYCgDNg0e/wDEN1Fe+IoxBaROHttJVtyhh0edhw7DqFHyqf7xAI6gADpQFApaACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACg80UUAQ3NtDdW8kE8SSwyqUdHGQykYII9K5eNNT8IDZbwz6poC8LGgL3Vkv90DrLGOw++vT5h060jIo2gjHUe/NAFTTdVsdYskvNOuorq2fpJE2RnuD6EdweRVsEntWFqPhWzub1tRspp9M1N/vXdowUyenmIQVkH+8CfQiqv9qeI9Hwup6WuqW463mmDEmPVoGOf++Gb2FAHUUVj6Z4o0bWJWgs7+JrlPv20gMcyf70bgMPxFa+7nnigBaKM0UAFFFFABRRRQAUUUUAFFFFABRRRQAUUUwvt5bAA5J9KAH0hOOlc/ceNNISd7WweXVrxDhrfTU89lPozD5E/4Gwqv5PijW8G4li0G0b/lnbET3TD0LkbIz9A3swoA0NY8R2OlSJaNvutQlGYrC2XfNJ77f4V9WYhR3NUtP0O91HUYtY8RmNriElrSwibdDaHpuzgeZJj+IjA/hA6nS0nQNN0WN1srYJJKd00zsXllPq7tlmPXqeO1aYAHSgAChelLRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUHigBGGRXkfxE3eDPiFoHjqIFbOY/2dqe0cFCPlY464GT9Y1r1maaOCF5ZpFjjRSzu5AAA7k9hXEeItBl+Inh2+hmL29jJC39nxHKNLJj5JpB1C5+6h7Ek8kBQDasbaTXruLVr+JktYjusLRxgr6SyD++R0X+EH1PHQBQDmuA+D/iKTXPBEVrebl1LSnNjco/3ht4UkfTj6qa9AoAQnFAOaDXneop8Mzqd2L/RLaS785jO50iVyz5OSWEZzznmgD0Wg15h5Xwn/wCgBa/+CSb/AON1o6FH8OxrVt/Y2jW8Oobj5LppMsZU4P8AEYwBx3zQB3uc1TvtI03UxF/aGn2t35Tbo/tEKybD6jIODVsdc0pOKAPNvg4qiy8WBVAA8R3YAHphK9Krzb4O/wDHn4t6j/io7o/olek0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFBGaKKAE2iqeo6Rp2r2/wBn1Kxt7yH+5cRBwPzq7SE4H+NAHNN4PSzG7RdY1XTcfdiSfz4vpslDAD2XbWXea34n0XVbbTI20vXrubB8iON7WWOPODJIwLqqj1IXPQAmtK81y71e8l0zw7sJicx3Opuu+G2YdUUf8tJB6dF7n+E6mjaFZ6JA8VsHeSZvMuLiZt0s7/3nbufboBwABxQBnf8ACS6rbcX/AIT1NOv7y0eK4T9GD/8AjtH/AAnehRj/AEp72yI6i80+eAD8XQD9a6QKBS4oAwrfxp4Xu/8Aj38RaVIf7ovEyPwzmtSHULO4wYLqCUHoUkVv5GluLCzu/wDj5tYJv+ukYb+dZc3gzwtcHM3hvR5D6vYxE/8AoNAG3mjJz0rnf+EA8I9vDemL/uW6r/IUf8ID4Uxj+wbPHpsoA6ItzVaXUrG3GZ7y2ix13yqv9axv+EA8I9/DemN/v26t/MVZi8H+GLfBh8OaRGR/csYh/wCy0AR3HjfwrakibxHpKt/dF5GW/IHNQf8ACc6NJxaLqN6SMj7Lp08gP/Agm38zW/BaW1qu23t4oh0xGgX+VS4oA47UvHN5YpC//CM38MU0giS4v5Y4Igx6byGdkB6ZKgZIq55XjG/H7y80jSkPa3he6cfR2KL/AOOGuguLeC5t3t7iJJYZFKPG6BldT1BB4IIrlA9z4IfbKZbvw1/DKcvLpw9G7vCPXqnfI5UAtjwZaXZ/4nWo6lq5PVLq42xH/tlHtQj6g1u2Wn2em2y21jawWsC/digjCKPwHFSwzRzxpLE6SRuoZXRshgehHsexqSgBNopaKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACkwKWkJx0oAoanoelazEsWpafbXSr93zYwxT3U9QfpWSfCc1kM6N4g1WxHaGWUXUWPTE25gPZWFdIXAGScDrk8Vyk2r33iiR7Tw/MbfTlbbPq4XO4jqtuDw3oZD8owcbuwBSPiHxNZa8NGig0rXbhV3zm2L2ht1IJUyE71BPZc5Oc4xzWt/wlF/b4F/4V1iHjl4BFcL+Gxy3/AI6K1NI0iy0axFnYw+VHuLsdxZpHP3nZjyzE9SeTV8AA0Ac4fHnh+MZurm4svX7bZTW+PxdAKtW3jDw1ec23iHSpvaO8jb+RraxVW50ywvDm6srac+ssSv8AzFAD4r22uADBcQy56bHDfyqbNYcvgnwpOSZfDWjuT3axjJ/9BqH/AIQDwj/D4d05P9yAL/KgDo80hbH/ANeud/4QHwof+YDZY/650DwB4Q7+GdJb/ftEb+YoA15tVsLbme+tYgOpeZVx+ZrLn8c+Fbdtj+I9LL/3EukZvyBJqeHwj4atz+48PaTF/uWUa/yWtSG2gt1CwQxxKOgRQo/SgDAPjfS5MCzt9UvWPT7Pps5U/wDAyoX9aoXnjbUIbyztY/DNzbvev5dvJqdzHbxO393chkKsecKQCccCuyx9fzqrqOn2mp2E1nfQJPazLtkjfoR/j6d80AYf2XxffEC41TTdNTHK2Vs07j6PIQv5pTl8E6bcHdq899rLZyRqFwXjP/bFcR/+O1Vhv7zwlMlprM73OkOwjt9UkOWgJOBHOe46ASd/4sHk9apzQBHBa29pbpb20KQQoMLHEoVVHoAOn4VLtHPv3paKAADFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUhpaCM0AcT4h8K+JdevISfE9pBYRTiYWI0sskoVsqsp87LjpkDAPpXYuHaJhGyhyCFYjIB7HHcfjUm0YxRtFAHnWhfD7XfD/inVNetvEtm0mqOHu7c6WRETuzlcTZBGTySfvHg16KDnr1owBRjFAAQCMGjAzmlooAKTANLRQAmAOlUtSi1OaFV0y7tLWUNlmubVpwR6ACRMHpzk/Sr1GKAOE8J+Ctd8IvdiLxFZXcF9fNe3KS6YwYs+N+1hN8uQO4Pau6Bz6fhS4pMDOaAFooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApCcCgnArN1fXLPRbVZrxmLSMEhgiUvLM/ZUQcsf/wBZwMmgC5cXcFpbS3NzNHDBEu95JGCqg9ST0rmC+oeMuEM+n+H2/jGY575fbvHGfX7zD+6Oslrod5rl1HqHiRFWONvMttKVg0cJ7NKekkn5qvbJw1dTtGc96AK9nZ21jaxWtrBHBBCmyOONQqqPQAdqsAYpcUUAFFFFABRRRQAUUUUAFFFFABRRRQAEZppUEc8/XmnUEZoA5GSyu/B0r3WkwPcaGzF59NjG57Yk5Z4B/d7mIeuVAPB6Sw1C11OxhvbKeOe2mUNHLGchhVkjiuXvtIvdGv5tY8PRq/nMZL3Td21Lk/34z0SXjr0bvzhgAdTRWdpGs2et2Au7OQsu4o8bDa8Tj7yOp5VgeoNaANAC0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRSE4FACk4qlqWp2elWMl7f3CQW0Q3NI5x+A9SfQck8AGqmt+ILbRkijZJLm+uCVtbK3wZZmHXAPAA7sSAO5qjp2gXV5fx6v4jeOe+jO62tIzm3s/93Iyz+rkZ64CjqAVlstQ8YN5mqRzWGh8FNPJ2zXQ/vTY5VD2j6/3scrXWRRRwxJFEixxooVVQYCgdAB2pwGDS0AIFAOaWiigAooooAKKKKACiiigAooooAKOtFFAEU0EU0DwyxrJG6lGRxuDA9Qc9Qa5MfavBB+US3XhrqVALy6cPUd3hH5oPUdOwIzSMoPJGcUAMguI7mKOaCRJIZFDpIjAqwPIII6gjnIqWuSm0+88KTyXuiQPcaU7F7nSoxzGScmS3H6mPoeoweD0Wn6ja6rYxXtjOk9tKuVkU9f8D2IPIIoAt0UgpaACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigApCcDNBbHvXLXWu3uuXMmneG3RY43MdzqrLvjhI6pGOkknr/AAr3yfloAuaz4hNrdLpemwfbtYkXclsrYWJf+ekrfwJ+GT0UE0aP4eFrdHU9SuDf6vIpDXLDCxKf4IlzhF/MnuTVzR9EstEtmhtUYvI2+aeVi8sz92dzyx//AFDArRAA6UAAAHSloooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAEIzRgUtFAHO6voNwt6da0KRINVCgSpJ/qbxB/BIB0Po45HuOKt6Jr0GsxShY3t723by7qzm4lgbGcEdCD2YcEdDWsRkYrE1zw+uoSxahZTmy1e3G2G6UZDLnPlyL/Gh9D0zkEGgDbBzS1haHr5vppdO1C3FlrEC7prYtlXXp5kbcb0Pr1HQgGttW3H2xQA6iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACig0xnCjLEKBySeOKAHGud1TxDO9++j6FCl3qagGZnJ8mzB6GUjnOOQg+Y+w5qpJql/4tZrfQZmtNIB2zasPvTditt6/9dOn93P3h0Gl6TY6NYJZWFusMC5O0clmPVmJ5ZieSTyTQBT0Pw/BpLzXMsz3mpzgC4vpvvyAdFHZUHZRwPc5J2QoHSgDFLQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFACECuZ1DR7zS76XWPDyr50rb7vT2fZHdnuwPRZePvdD0bsV6ekxQBn6RrVprdn9ptGYbWKSxSjZJC46o6nlWHp9CMg5rR71z+s6FM94NY0WZLXWFUKS4PlXSDpHKByR6MOV7ZGQZ9E16LV1miaJrS/tjsurOUjfC3Y8feU8kMOCPQ5FAGzRSBskjiloAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACig+1NJORxkZoAUnAqveXtvYW0l1eTRwW0SlpJpWCqg9STVTWNcs9Gtke53vLMdkFtCu+Wd/wC6ijkn9AOSQAayrHQrzV7uLVPEojYoQ9tpqENDbnsznpLJ7/dX+Ed6AImW/wDGP+s+0adoB4CZMdzfDvkcGOM+nDt32jg9PbWsFpbx29tCkMMShY441CqgHQADoPapsDOe9AGKAADBzS0UUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFIRmlooAydb0G21mCMs8lveW7GS1vIeJbd/VT3B6FTww4INUtJ1y4hvxouupHBqYB8iZOIbxB1aP0bu0fJHXkc10RGRzVHVtHs9asDZ3sZaPIZGVirxOOjow5Vh2I5FAF4NkkUtctYaveaHeQ6V4glWRJW2Wep4wk/okmOEl9P4Xxxg/KOnDHHP50AOooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoooNABSE4FJuNZOt+IINIMNusUl3qFxkW1lBjzJT3PJAVR3YkAUAW9T1Sy0ixkvNQuEgt0xl2PU9gB1JPYDJPQCudj06/8AF7+drUMllov3o9LbiS49GuCOg/6ZA/72eVq1pnh6ee/TV/EMkd1qK8wQR5NvZD0jB+83q7cnsFGRXSYwaAGxxpEioihUUYCgYAH0p4GKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArF1zQY9SMN5bTtZarbDFtexrkqD1Rh0ZDxlT9RggEbVIQD1oAwtF1+S7uW0zVLcWWsxLueDdlJlBx5kTH7yH35U8EdzuqSazNa0O01m2RJy8U8Tb7e5hO2WB+zK39DwQSCCDWfpet3VrqCaJr6pHqDf8e1yg2w3qjuv91wPvIfqMjoAdJRTdxGc49qUHPagBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiikJwKAForntW8Q39nrceladozahM1sblz9pWIIu7b/EOeai/tvxR/wBCh/5Uov8ACgDpqK5n+2/FH/Qof+VKL/Cj+2/FH/Qof+VKL/CgDpqK5n+2/FH/AEKH/lSi/wAKP7b8Uf8AQof+VKL/AAoA6aiuZ/tvxR/0KH/lSi/wo/tvxR/0KH/lSi/woA6aiuZ/tvxR/wBCh/5Uov8ACj+2/FH/AEKH/lSi/wAKAOmormf7b8Uf9Ch/5Uov8KP7b8Uf9Ch/5Uov8KAOmormf7b8Uf8AQof+VKL/AAo/tvxR/wBCh/5Uov8ACgDpScCvIb34zJH8Qb3w5Z29tPASlpa3MsuyP7RnDlzz+7BJGQM/J/tAjq9Yv/GOoaVPZ2vhxrKSZSn2hNQiZo1PUr/tY6Hsa5HQ/h3Y6KgP/CuEvph1mv8AU4pi3/ASNn5CgD0XRdASyuZNQvp2vtXlGJLtxjav9yNeQiew5Pck1uAYrmRrXidRgeD8Adv7Si/wpq+J9Xg1LTrbUvDjWcV9cfZ0mF6km1tjPyAM4whoA6mikBzS0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUEZGDRRQBWv7C11Kxms7yBLi3mXbJHIMhh/n0rx/wAcfEa++GjHQbS5h1Sdgr20k7Ey2keeY5cD5yR9053YOWB4Le0MMivN9Vt7fxj9q8/wBbalbx3M1qt291HHK3lSNGxUjDgZQ9/SgDudE1a213R7PVLJ99tdQrKhzyMjocdx0PuK0K818I6b4h8HWs9hp/hqd9NdzJDbz6nExt2P3grYyVPBwe+eea6T+2/FH/Qof+VKL/CgDpqK5n+2/FH/AEKH/lSi/wAKP7b8Uf8AQof+VKL/AAoA6aiuZ/tvxR/0KH/lSi/wo/tvxR/0KH/lSi/woA6aiuZ/tvxR/wBCh/5Uov8ACj+2/FH/AEKH/lSi/wAKAOmormf7b8Uf9Ch/5Uov8KP7b8Uf9Ch/5Uov8KAOmormf7b8Uf8AQof+VKL/AAqC98TeI7CxuLyfwiRDbxtLIRqUZO1Rk9vQUAdbRUFnci8s4LgLtEsayAE5xkZqegAooooAKKKKACiiigAooooAKKD7VzV74k1NNdutL0zQWvzaxRySS/a0iHz7sABhz900AdLRXM/234o/6FD/AMqUX+FH9t+KP+hQ/wDKlF/hQB01Fcz/AG34o/6FD/ypRf4Uf234o/6FD/ypRf4UAdNRXM/234o/6FD/AMqUX+FH9t+KP+hQ/wDKlF/hQB01Fcz/AG34o/6FD/ypRf4Uf234o/6FD/ypRf4UAdNRXM/234o/6FD/AMqUX+FH9t+KP+hQ/wDKlF/hQB01Fcz/AG34o/6FD/ypRf4Uf234o/6FD/ypRf4UAdNTXYKpYkADqScCua/tzxR/0KH/AJUov8KyfEb+LfEGjzaWnh+WxiuPknlg1GIyGP8AiVSRhSemcHjPfkAHMaZ8ZJdf8XX/AId0yC1Dz3PlaZeXDkRBVGGdxnLElSyqMZyBkda9M0Xw/b6Q0s7SSXeo3HNzez4Mkvtxwqjso4H1yT59oXga18PmJ7X4awy3EZDCe61KKV9w6NyMA/7oFdj/AG14oXp4P6/9ROL/AAoA6cAA570tc1ZeI9UfXLXTNS0E2Buo5Hik+1pKDsxkEL9a6QGgBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAQgEYPSqOqaTZatp72d7D5sLkHgkMrDoysOVYHkMMEHmr9IRkUAeI+K/i3e+CNZg8ONPDqjW1zG1zfA/vPs+ctG6gAebjgsOORwDkD2m1uIru2iuIJEkhlQSRuhyGUjII9sEV5heafa+O9Li1C4+HNvcx3SCSO5F9FFKQe+4YYfTNafhWDxP4V0ldKg8OTXNlCx+zCfUoS8SE52FgPmAzxxwOKAPQqK5n+2/FH/Qof8AlSi/wo/tvxR/0KH/AJUov8KAOmormf7b8Uf9Ch/5Uov8KP7b8Uf9Ch/5Uov8KAOmormf7b8Uf9Ch/wCVKL/Cj+2/FH/Qof8AlSi/woA6aiuZ/tvxR/0KH/lSi/wo/tvxR/0KH/lSi/woA6aiuZ/tvxR/0KH/AJUov8KP7b8Uf9Ch/wCVKL/CgDpqK4+/8Wa9pdm95e+FGitoyvmONQjYqCQM4A56114zmgBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACgjNFFAHNYH/AAssf9gc/wDo6ulrm/8Ampa/9gc/+jhXSUAFFFFABRRRQAhqtfX9tptsbi6k2R5CjClmYnoFUAlifQDNWTXP+I/D13rd7o1za6tLp7adeC4YJHu85cEMh54yCRnngng5oAktfFulXGrR6VI89rqEql4YLuB4TMo6lCww2O4BJHcVcv8AWrTTp7e3mZ3ubksILeJC0kmByQOwHGScAZHPIzwXxKU6r4r8GaRpw3apFqK37OmMwW6ffZj2B469SuKt6Ncve/G/xIkhz/Z+mW1vF7B/3jfmSPyoA7LTdZs9TkuIYHZbi1YLcQSIUkiJGRlT2I5B5B5weK0K8+1C4ay+O+kRxsQt/o0sUozw2xy6nHqORn0Negj9KAFooooAQjNGKWigArm/FP8AyEvC3/YYH/pPPXSVzfin/kJeFv8AsMD/ANJ56AOjFLSCloAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK5zwYM6Nd8n/kLaj3/wCnyaujrnfBf/IGu/8AsLaj/wClk1AHQgAdKWiigAoopCcCgCK6uoLK0lurmRYoIUMkkjHAVQMkn8KqabrVnquiW2sW77bK4hE6ySfLhCM5PpWB4wJ1jRdatAM2VlZyvcekk3llljz6LkMffaP7wriNfvpbL9mqw8lyr3Fla25OOdrlQR/3zkfjQB6VH4t0x4re4LTx2dy6pDeSQOsMhY4XDEcBiQATgHIwTkZ3Ac9a5fx/axN8NPEEOxdkWnTMq44BRCRj6ECrvg2+l1PwVod9O5eeewhkkY92KDJ/PNAG5RRRQAVkeKxnwfrf/XhP/wCi2rXrI8Vf8ihrX/XhP/6LagCzo3/IEsD620f/AKCKvVS0b/kB6f8A9e0f/oIq7QAUUUUAFFFFABRRRQAUUUUAIelc5pX/ACPviLj/AJdrP/2rXSHoa5vSv+R98R/9e1n/AO1aAOkooooAKKKKACkPSlpDQBTvtTg09UM29nkOI4okLu5xk4AyeO57VUsfEunX+qzaUryQ6lFGJWtbiMxuUPRwD95c8ZGcHrUWt+HtN1LUdL1m9W4NxpDPLb+SWPLAZBVQS3QcDn69K5HT418TfGH+3NxtItHsTbxW1wpiuJmfdmQxthhHhjjcBk80Advd67aWt+tgBJPetH5v2eBC7qmcbmxwozwCSM4OM4qbTNVtNXgeazl3rHI0UgKlWjcdVZTgqw9CO4PeuM+H10+o+LPHd5LnzF1UWo9kiXav9T+NN0C4a3+NniuwQkQ3Fja3RXtvUBM/kefoKAPQ6KKKACjFFFABiiiigDnNV58beHf+ud3/AOgpXRVz2qf8jt4d/wCuV3/JK6KgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5rwAM+ANC7f6ImPyrpcVzfw/8A+RA0P/r0T+VdJQAUUUUAIeKwrvxdpllbTXcgumsYM+bdxW0jxKB1O4DkDuVyBg5xit1sY56V5zrVrp3gTwDqGgaTbXri5jlVJJ1doYTKCu6SUjYiDPr+ZNAHdDVLNtN/tJbqA2Plef8AafMHl+XjO7d0xjvVGLxTp8jWm/z4I7xwltLPA8aSk9ACRwT2BxntmvOfFWnf8Ix8IPDnh1LpbmK5v7W1mlU5WVXcyNtP90kYHtXV/FmPd8L9ccErJFGkqMDgqyyKwIPY5oA7RTkZp1UdFu2v9D0+9f79xbRytgY5ZQT/ADq9QAUUUUAc34+APgnUvon/AKGtdGK5zx7/AMiTqX0T/wBGLXRigBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDm/+alr/ANgc/wDo4V0lc3/zUtf+wOf/AEcK6SgAooooAKKKKAEJwMnpXN+MvGOmeDdHF5fzIryuIoEbd8znucAkKOSSAeBxkkA9LSYFAHkuh/EXwcl6I9L1N9S8SarcQxSTy2sieYzMqgDIwsaAkhc9B/ESSdqC2bRfjLe31wNlrrthGkEpOFM8eAY8/wB4ryB3wcdK9A2iop7WC6iaK4iSaJhhkkUMrfUGgDhkt/7c+My6pb4kstG0027zqcr9okcny89yE5OOmQDXfUyC3htoligiSONRhVRQoH4CpKACiiigAooooAK5vxT/AMhLwt/2GB/6Tz10lc34p/5CXhb/ALDA/wDSeegDoxS0gpaACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACud8F/8AIGu/+wtqP/pZNXRVzvgv/kDXf/YW1H/0smoA6KiiigAprgshAOD2OM4PrTqCM0Aec+KYJ/DXgfUYbjxTBGps5xGlxDGjXLlWJAOclmJPTnJ6VzFvpsviv4Btpmm6rBqN5b2VtJHaQhN8DRnd5bAHO4hWAzjkfhXtm0UYoA4bxjr9pqnwxu3sJBPNq9qbW1gQ5eSWUbdgH94ZOR2wc10/h3TTo3hvS9LLBjZ2kUBYfxFVCk/pVtLC0junuo7aFLhxhpVQB2HuepqcACgBaKKKACsjxV/yKGtf9eE//otq16yPFX/Ioa1/14T/APotqALWjf8AID0//r2j/wDQRV2qWjf8gPT/APr2j/8AQRV2gAooooAKKKKACiiigAooooAD0Nc3pX/I++I/+vaz/wDatdIehrm9K/5H3xH/ANe1n/7VoA6SiiigAooooAKQ9KWgjNAHI6xresaf430ezCWMfh+5hk+03Fw21vMGdqKc4ySV4wcjdXM6+JNY+Nvhp9Ffe+mwSHU54iCscbZ2o59Tzgf7QOMZr1B4Y5Y2jkRXRhgqwyCPemW9nbWkIhtreKCIdEiQIB+AoA4bwjbHw9478W6fdHyxqNwmpWZbgTKwIk2+pVhgjrgg96d4Ttf7R+JHinxNH81jIkNjaSj7s2xf3jL6qGwoI4OD6V21zY2t7GI7q3injBzslQOPyNTJGsahUUKo4CgYAoAdRRRQAUUUUAFFFFAHO6p/yO3h3/rld/ySuirndU/5Hbw7/wBcrv8AkldFQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAc38P/APkQND/69E/lXSVzfw//AORA0P8A69E/lXSUAFFFFAFbUHuY9NupLJFe6WJ2hRz8rPg7QfbOK4yx8UC4+Glxf+K5tPhuxbTLeQROMA/MBGUYkhyuAV65z9K7sjIqu2n2b3S3L2sLXC42ysgLj/gXWgDxW08L69/wofTY7m3na+029TUYbRlJk8pXJCY652sSB17V2nxG1CDXPh5JYaTNHd3WteVBZIjZ83c4y3HRQoZiTwMda77aP6VXhsLS3nknhtoY5pPvyJGAzc55I5NACafarY2FtZocpbxLEpI6hQBn9KtUgAHSloAKKKKAOb8e/wDIk6l9E/8ARi10YrnPHv8AyJOpfRP/AEYtdGKAFooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOb/5qWv8A2Bz/AOjhXSGub/5qWv8A2Bz/AOjhW3f3EtpZSTQWkt3Ko+WCJlDufYsQPzI4zQBYBzS1598NvEeo6xoz3Fzot4v2m/una4EkJii/eN8v+s3kD7vCnn25rVPjqJfE1x4fk0bUY9Qjh8+KM+SfPTJG5SJMY+Un5iDxjqQCAdZRXMR+ONObw1Ya1JbXsQv5BDbWckOLiWQkgIFJ6nB5JAxznHNW08SRprNtpN/aS2V1dRtJbeYysku376hgfvKMEg9umcHABuUGub/4S+GdtTOm2U9/HpkrQ3RgdNyuoywVSwJI564zg4zxm0PEMV74ZTWtGtpNViljEkEcLLGZPxcqBzwc8+1AGyrbuR0pa4H4ZeIL7V/CVhcz6Tdj7W9xO95uiMO5pnYgASF+px93t2FaZ8cwJ4qm8OS6TqEeoLD58CnyiLhNxXKYkPox+bHCnODxQB1dFc1D4ytpNXutGeyu49Xtwr/YyELSIwJDqwbbt4IJJGDgHkjNrSfEtvqup6jpht5rW/08p50M205VxlWUqxBBH4g9RQBt0Vz6eKVvJr5dL0661CKxdoppYiqhpF+9HHuI3sDgHoMnGc5Ao6r8QdP07wtb+JYbK8vtKmAzcW4jAiy4QBw7qwO444BwQc4oA66kJxj3rnb3xhbadFa3d1ZXaabczpBHe4TYC5wrMN24KTgZI7+nNYPjXxHqNl4t8MaZBo11Nby3rSGZJYh5+2JvkQM4/vclto+XjOaAPQBXOeKf+Ql4W/7DA/8ASeetuxuZbq2WWazns3Of3M7IWH/fDMv61ieKf+Ql4W/7DA/9J56AOjFLSCloAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK53wX/yBrv/ALC2o/8ApZNXRVzvgz/kC3f/AGFtR/8ASyagDoqKy9c1+x8P2SXV67fvZVhhijXdJPIxwqIO7H8vXA5qnH4pji1mw0nUrSSxvdQV2tY3kR9+wZZTtJwcZPpweemQDoKKwE8TfbLm7i0uwmvks3MU8yskaCQH5kBYjcw78bQeM5ziS18Swaho0+o6daXV40EjQy2kYRJ45FOGRg7KAy9Tz06ZyMgG3SE4Gew61yWk+PYNc8ODXNN0nUJ7URtJIAIw6bc5XBcBmwAcAnr1B4rO8b+MXHwvudb0Sylvbe8siUnDoiwK427nDMGyN3RQeRzjrQB3wOaWsTRtTu59NUy6NfW7RQKyiZ4CZuOi7JGwf97b1HNYcfxLtJLPVphomredpUxiu7YRxtJHwPmwrkEHPGCc4bsCaAO3pGbaOaxJ/EsC31vp1rbzXWozwi4FugCmKLpvkLEBRnjHJz0BwcP07xBDe6nNpU8MlpqUUazG3lIy8ZON6MCQy5yOORjkDIoANF8UaXr95fWljJP9osCguYp7WSBoy4JUEOoPQE/lTvFX/Ioa1/14T/8Aotq5vwoP+Lq+P/b+zwPp5B/+vXSeKv8AkUNa/wCvCf8A9FtQBa0b/kB6f/17R/8AoIq7VLRv+QHp/wD17R/+girtABRRRQAUUUUAFFFFABRRRQAHoa5vSv8AkffEf/XtZ/8AtWukPQ1zelf8j74j/wCvaz/9q0AdIelICGGQcj1FZXiXVbrRdButQs9Okv5oI2cQo6IBhScsWI4GO2T6CsD4d6vfXvg/R/P0m9UNZI5u5HhMcp46bZC/OcjKj3xQB2tFcUnxHt3fWIDoWrLe6Su+4tCsTSbdm/I2uVPBXoSTuGB1xrXHiyyhj0pY4LmW81VPMtLNUCyldoZiwYgKFByST7DJwCAb9FYtj4ihutbuNFngktdRhhE/kyEESRk4DoQSCMgg5wQe3eqP/CbWsthf6jZWk15p+nyvDczQuhZWQ/vMJnJCjnsSOQCDmgDqCcCmq24AjBBGQR0Ncv4n8UyWPga417RLR9SRrRriKSN0VUTbuEjbiDgDnABPGMCjwNqF1c+F9Iil0q9t1TToMXE7QlZTsXkbZC3PX5gKAOqork9N8cx6lq+o6QujajDqViU822l8rdhl3bsq5XABX+LPzDjORU+n+M7bVJ7qztbK6bUrSUw3Nm2wNCQAdzNu27TkYIJJ5wDg4AOlorG0LxLaa9bXkkSS28tlcPbXUFwArQyL1BwSCMEEEEgg/WqC+NreXR59bttPvJ9HhLbrtAvzIpw0ioTuZBg5PU4JAbigDqKDXLa/45svD9lp9/LZ3Nxp160SrewFPKj8w/KWywbGOchTU9z4shsbnT0vtPvbaDULhba3uJAm0yMCUDAMWXdjjI68HBoA6Atjjv0pc84rz3xL4k1KD4j+G9JTRbyS23XE6lJYQ1yyxMvyhpBgKJM/NgnsOOe6s55Lm3SWW1mtXYZMMxUuhz0OxmX8iaAMbVP+R28O/wDXK7/kldFXO6p/yO3h3/rld/ySuioAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOb+H/8AyIGh/wDXon8q6Sua8AHHw/0P/r0T+VXdZ8RWmjS2ds6S3F9eyGO1tIADJKRyxGSAABySSBQBsUVhWXiaC4119DuYja6mlsLswl1cGPdtzkHsSBzjrxmobfxWdQtHv9M0m8vrBS22eIoDMB1aJWYF1yDzxnHGeKAOjornL7xhZ2/hQeJbG2udU07ymmLWezcEUEsxDsvA2kEDJz24NV38dWqeHF8QjTr59J8pZZLhQnyKcZO0tuIXPOAehxmgDqmO0ZJA+tAJrgPiR4lvNP0XTVsNOmube+vbVTdpJGqgGQNtAZgxZguOQF+bluMV1cep3bafNcHQ76KWPhbV3g8yQcfdIlK/mw6UAatFcRZfEywvtJtdRh0rVGgmuhaSBYVZoHLlAHAb2ycZwCPUA7tz4hjTWTo9nbyXl+kayzJGQqwIxwpkYnAzg4AyTjOMc0AatzOttbSzuHKxqWIjjaRiAOyqCWPsBk1naD4j03xLbXFxpkzyR287W8okheJkkXGVKuARjI7fypNL1+DU7u6sHhltdQtQrTW02NwVs7XBBIZTg8gnoQcHiub+GXA8YY4/4qe9/wDZKANfx7/yJOpfRP8A0YtdGK5zx7/yJOpfRP8A0YtdGKAFooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKQ8CgDnP+alr/ANgc/wDo4V0bdK5v/mpY6f8AIHPf/psK6Q4NAHm/wyvRp/gNF2B5hq81q0W7BVmuCD9SFO7GOg7datpsb48TMSC48NIAfTNyf/rGuvj0XS4dRfUI7C2W8c5acRrvJxjOeuccZqqfCXhtrw3beH9KNyZPMMxs495fOd27Gc570Ach8Q7aWLxd4K1aSeS3021uZ4Z51CnyWlQLGx3AjBIIyR39TXQahotk+r6Nc6lqN9c3cFwXs4z5Y+co27OxASNuepxXRzW8FzA8E8SSwuCrxyKGVh6EHqPaq1lpGm6cxaysreBiu3dHGqnGc4yO3t0oA4jwlqUniGz1fXRNDoumreSgw2kEayPsOGlnZlPzHGeMYHc1H8IZxH8M9F01kkE7Q3bHj7m24ZcHuDl//HT0rtv+Ef0f7Y93/ZlqLh3EjyCIAu46MfVvc81Na6Vp1jNcTWljb28tw++Z4olVpG9WIHJ5PJ9T60AcL8K74Wvwy8LwCMO80stuRuxsIaVz2PI2Hjipfk/4aAUZGR4X4Hp/pP8AhXZW+i6XaXk15b2FtDczEtJLHEqsxPU5Hc8ZPfAzVf8A4RXw79tN7/YOl/azJ5vn/ZI/M35zu3YznPOaAOWsyp+OupNwzDQYguOwMp4/lRorbvjL4rCvg/2fadPo3+fxrqU8K+Hor4X0ehaYl4JPMFwtpGJN+c7t2M5z3og8KeHbW8W8t9B0yG6Vt6zx2kauG9QwGc0AcP8ACfT7q38ISaRNqt3a6hp11NFd2yLD8rF2YN8yE4YEEHJB5x0qn4+sNL0v4H69b6XJLJbfaFYySkfO5uk3lcAcbt2MDHHHFemXei6Xf3AuLvT7aacLt8ySJWbb6ZI6e3Sm3+gaNqiQpqGk2N4kIIiW4t0kEYOMhdwOOg6eg9KAOQ+LpSP4fv8AdUC9tduTgf61f6elP8bgp428BTtlYxqE0Zc9AzQnaM++D9a6WXwn4cntobabQNLkghBEUbWcZWMHrtGOM98VYfQ9JfTV01tNtPsKEFLcQqI0IOQQuMAg85FAElhe/a571BEFW3n8kOGzvwisT04wWK4/2ayPFP8AyEvC3/YYH/pPPW9bWtvZQJBbQxwwoMLHGoVR36D3z+dYPin/AJCXhb/sLj/0mnoA6MUtIKWgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArnPBn/IFu/+wtqP/pZNXRE4Fc74MI/sa7Hf+1tR7/8AT5NQBz3xFV7XxR4K1ickaVZ6g6XLfwxtIqrG7egDA8++O9dlevYx3dm8sSNds7RWreXudSwO7B6qMDk+3fgVdnghuYXhniSWKQbXR1DKw9CDwRVWy0bTNNYtY2FvbsV25ijCkL/dHoPbpQBxvwgiez8Ef2RdDZqenXc8N5GfvBzIzAn2KspB7im/D5XufE/jXV4udJvL9FtZAcrI0alZHU9CMkc9yD6GuzvND0rUJxPeadazyhdm+SJWO3+6T3X26VJcaXp11YGwuLG2msyqr9nkiVo8DoNpGMDsKAOC+E3lr8FNPbKjMd0Wbjr5snJrBiQy/suERqzt/Z5Jxz0lJJ/DB/CvT4/CnhyK3mt49B0tIJiDLGtpGFcjONwxzjJ61PY6FpGlxTRafpdlaRzcSpb26Rh+McgAZ6nr60AVtN1BHOnWkKiRJbHzzKrcADYFwOh3bmPX+E9e3M+ANv8Ab/jdlClzrbA4Poi/1z+tdfaaFpNhbTW1np1rBBMu2SOOJVV15+Ujpjk8dOTUNl4X8P6bdLdWOh6ba3C52ywWscbjII4YDPQkfjQByehpJYfGXxSL04GpWtrNYu3R4412OFPqGIyPxpdTWS/+NehGyJZdM0+dr9weFWQYjRvct82PTmu3vNOstRjWO9tYbhFO5RKgbafUZ6H3otNOsdPhaGztIbeN23OsSBQx7k46k+poA43wq4PxV+IBBBwdP79P3BzXS+KTnwjrf/XhP/6Lals/C3h3T7tLuy0HTLa5TOyaG0jR1yMHDAZpPFIA8Ia0Bx/oE4+n7tqALejf8gPT/wDr2j/9BFXao6Kc6Hp+On2aPH/fIq9QAUUUUAFFFFABRRRQAUUUUAB6Gub0r/kffEf/AF7Wf/tWujPSub0k/wDFeeIv+vay/wDatAGl4jRpfDGqxohd3s5lVV6klDxXNfD/AFBYvAPhCCNRMbm3EbMrfcCRsWb3wyhe3LV25ANULTQ9KsJZZbPTrW3eXIdoolXdk5Ocep6+tAHIeEtg+KPxAbILmSwGR6CDp+GTVTXLWe3+Nek3095NaWl5pT2NvOgQgTiTf5fzKwBK9OATjjpXZWnhTw5YXS3VnoGl29whJWWGzjR1J6kEDNX7qxtL62a2vLeK4gYANFMgdTjpweKAMA6TY23iy0v7i+vbzVY7SVIlbYAsOVLZCIv8WMZ79OlctoV5ceIfh/qfiASx6ZY3aXLw6fp8Ea5+8p80lSWkZh/Djr3r0az02x08OLO1ig343mNQpbHTPr+NQRaBo8F01zFptrHMz+YzpEoJf+8cdW9+tAHnGk3iXPwBnsIw7zR+HHlOBkYKyKB9co3Fdj4T1ADw94Ys1QOLjSUlDhvuhEjB4x3Litux0rT9MEosLK3tRK5kk8iNU3MeSTgcnmmWGiaVpQYafp9tahuCIYlXI9OO3J/M0Acd4U2H4ufEEjaW/wCJcOvbyD/n8KPBWxviD4+YY3fbLbJHXAi//X9K6i28KeHLK4Fxa6DpcEwBHmR2catggg8gZ6Ej8aLbwp4cs5TLa6DpcEhUoXitI0YqQQRkDpgkfjQBwWjWlzqVp8UbCxYfari9uIocHGHaEADP14rT8CWq6j8N9NX+2byKKOzFtcwBYR5LKNsinMeRyD1OehzXXWHhrQdKuftOnaJptnPt2iW3tY42A9MqAcVJNoOj3F211NpdnJcOQWkaFSzEdCTjkjtnpQB5345tLCy+Fui2dh5p02O9skhM5wzR+YMZzjqPbp6Vq/FJgLDwzltpHiKywf8AgTdK67UfD+javKkup6TY3skYwj3NukhUegLA4qCfwn4bukhS48P6VKkKCOJZLONhGgJIVQRwMk8D1oA5nxONnxa8DOfljaO/QHoNxjXj6nFdlp199vhlkEewJPLEPmznY5TP5qaZPomlXNjDZTadavawFTDCYl2xFem0fw47Yq1DBDawJDBGkUUa7URQAqjsAB0FAGHqn/I7eHf+uV3/ACSuirnNU/5Hbw7/ANcrv/0FK6OgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoooNAHNeAf+SfaH/wBeifyrn9bD2Hxq8P6jetjT7nTpbGB2+4lxuLYJ7Fl4A74xXQeAP+RA0LPH+hp/Kt27srTULV7W8torm3f70UyB0buMg8UAYPiu3ju9M1OwsYk/ty90y4jhkVRvC7cAFuoG5hj3+lVvh7e2o+GujuzLAlpZrFc78KInjG2Td6YKknOPWujs9LsdPVxZ2sUHmff8tQC/1PeoLjw9ot3cyXFxpVnLNIQZHeBSXIxgtx82MDGenagDzPw7bXEXwV8YXUyPHb366ld2iOu0iB0bbx6Ehj/wL3rVvti/s7jBUA+HI8Y7nyBXe6hpOm6tbrb6lp9rewKwZY7mFZFUjgEBgQDzVP8A4RPw4bMWZ0HTDah/M8g2kezd03bcYzx1oA4fxf8A8k08JTf8s4rzTZHfsq/LyT6c/rXocN752qXVmsfywRRN5gbqzF/l6cYAU9f4vzZFoOjwabLpsOlWUVjLnzLZIEWN89cqBg5wKVdE0pdPk08ada/Y5P8AWQGJSj9PvAjB6Dr6CgDkPhCqDwMWTbltQumYqOp81h29gP0qPwfFJp/xG8bWt9gXN3LBdwMeDLBtKjHc7T8p9Cfeuw07w7oejzNNpmjafZSsu1ntrZImK8cEqBxwPyqe90vT9S2fbbOG4MZ3RmRAShxjKnqDj0oA4iNJL/46Pd2ZLW2n6KLe8kXoJHkLLGf9rHzVP8MWDf8ACYYIJ/4Sa97/AO5XXxaVp8Nk9lFZW6Wjhg0CxKEbd97K4xz3qHTvD2iaRcNPpmj6fZTOuxntrZI2K5zglQDjgce1AGd49/5EnUvon/oxa6MVznj0/wDFFaj9E/8ARi10YoAWiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAwNW8MvqOrx6nbazqOm3KQG3JtPKIZC27kSRtzn0qD/AIRfVP8Aodde/wC/dp/8YrpqKAOZ/wCEX1T/AKHXXv8Av3af/GKP+EX1T/odde/792n/AMYrpqKAOZ/4RfVP+h117/v3af8Axij/AIRfVP8Aodde/wC/dp/8YrpqKAOZ/wCEX1T/AKHXXv8Av3af/GKP+EX1T/odde/792n/AMYrpqKAOZ/4RfVP+h117/v3af8Axij/AIRfVP8Aodde/wC/dp/8YrpqKAOZ/wCEX1T/AKHXXv8Av3af/GKP+EX1T/odde/792n/AMYrpqKAOZ/4RfVP+h117/v3af8Axij/AIRfVP8Aodde/wC/dp/8YrpqKAOZ/wCEX1T/AKHXXv8Av3af/GKP+EX1T/odde/792n/AMYrpqKAOZ/4RfVP+h117/v3af8AxiiLwjMdRsbu+8S6vfiym8+KGcW6pv2MmTsiUnh27966aigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooADXKp4OubeS4+w+KtatIZriW48iIWxRGkdnbbuhJxuY9Sa6qigDmf+EX1T/odde/792n/wAYo/4RfVP+h117/v3af/GK6aigDmf+EX1T/odde/792n/xij/hF9U/6HXXv+/dp/8AGK6aigDmf+EX1T/odde/792n/wAYo/4RfVP+h117/v3af/GK6aigDmf+EX1T/odde/792n/xij/hF9U/6HXXv+/dp/8AGK6aigDmf+EX1T/odde/792n/wAYo/4RfVP+h117/v3af/GK6aigDmf+EX1T/odde/792n/xiobnwdfXlrLa3HjLXpIJkMcibbUblIwRkQV1lFAEVtbra2sNuhYrEgQFupAGOaloooAKKKKACiiigAooooAKKKKACudv/CslzrNxqdnr2p6bLcRxxyraiAq+zO0/vI2OfmPeuiooA5n/AIRfVP8Aodde/wC/dp/8Yo/4RfVP+h117/v3af8AxiumooA5n/hF9U/6HXXv+/dp/wDGKP8AhF9U/wCh117/AL92n/xiumooA5n/AIRfVP8Aodde/wC/dp/8Yo/4RfVP+h117/v3af8AxiumooA5n/hF9U/6HXXv+/dp/wDGKP8AhF9U/wCh117/AL92n/xiumooA5n/AIRfVP8Aodde/wC/dp/8Yo/4RfVP+h117/v3af8AxiumooA5n/hF9U/6HXXv+/dp/wDGKP8AhF9U/wCh117/AL92n/xiumooA5n/AIRfVP8Aodde/wC/dp/8Yo/4RfVP+h117/v3af8AxiumooA5n/hF9U/6HXXv+/dp/wDGKP8AhF9U/wCh117/AL4tP/jFdNRQBzth4WkttYt9SvNf1TUZbZHSJLoQBV34yfkjU9h3roqKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACkIBGDS0UAcjZeCbvTbKGysvF+uw20K7I4wtqQo9OYM1Y/4RfVP+h117/v3af/ABiumooA5n/hF9U/6HXXv+/dp/8AGKP+EX1T/odde/792n/xiumooA5n/hF9U/6HXXv+/dp/8Yo/4RfVP+h117/v3af/ABiumooA5n/hF9U/6HXXv+/dp/8AGKP+EX1T/odde/792n/xiumooA5n/hF9U/6HXXv+/dp/8Yo/4RfVP+h117/v3af/ABiumooA5n/hF9U/6HXXv+/dp/8AGKP+EX1T/odde/792n/xiumooA5K78FXWoW7W174u1y4tnILxMtqA4BBwcQg9vWusHWlooAKKKKAP//Z"
      }
    },
    {
      "section_id": 8,
      "text": "# 4.2 Inference in Stratified Experiments \n\nIn this section, we discuss the implications of stratification for inference on $\\Delta$. For now, suppose that $X_{i}$ takes a finite number of values in $\\mathcal{X}=\\{1,2, \\ldots,|\\mathcal{X}|\\}$; this could be either because $X_{i}$ is naturally discrete (for instance, if $X_{i}$ denotes whether or not an individual graduated college), or because the researcher has discretized some continuous variables (for example, binning students by high or low test scores). In Section 4.2.1 we consider settings where units are stratified as finely as possible based on potentially continuous covariates. To fix ideas, suppose the experiment was performed using stratified block randomization as defined in Section 4.1: independently in each sub-sample $X_{i}=x$, treatment is completely randomized with $\\pi \\in(0,1)$. Under appropriate assumptions, it can be shown that\n\n$$\n\\sqrt{n}\\left(\\hat{\\Delta}_{n}-\\Delta\\right) \\xrightarrow{d} N\\left(0, V^{\\mathrm{sbr}}\\right)\n$$\n\nwhere\n\n$$\nV^{\\mathrm{sbr}}=\\frac{\\operatorname{Var}\\left[Y_{i}(1)\\right]}{\\pi}+\\frac{\\operatorname{Var}\\left[Y_{i}(0)\\right]}{1-\\pi}-\\pi(1-\\pi) \\operatorname{Var}\\left[E\\left[\\frac{Y_{i}(1)}{\\pi}+\\frac{Y_{i}(0)}{1-\\pi} \\mid X_{i}\\right]\\right]\n$$\n\nsee, for instance, Bugni et al. (2018). Comparing the variance $V^{\\text {cr }}$ obtained from complete randomization in (7) to $V^{\\text {sbr }}$, we see that $V^{\\text {sbr }} \\leq V^{\\text {cr }}$ with equality only if $X$ is an irrelevant stratification variable in the sense that $E\\left[\\frac{Y_{i}(1)}{\\pi}+\\frac{Y_{i}(0)}{1-\\pi} \\mid X_{i}\\right]$ is constant; note that this is exactly the condition that guaranteed that the ex-post bias of $\\hat{\\Delta}_{n}$ under complete randomization with $\\pi=1 / 2$ was zero in Section 4.1.\n\nRecall that we argued in Section 3.2 that the heteroskedasticity-robust variance estimator from the regression described in (1) is consistent for $V^{\\text {cr }}$ under complete randomization. It can be shown that the same holds under stratified block randomization, from which it follows that the robust variance estimator is generally conservative for $V^{\\text {sbr }}$. In words, the robust variance estimator does not properly account for the gain in precision obtained by having performed stratification. How then could we conduct non-conservative inference on $\\Delta$ in the presence of stratification? A straightforward option is to modify the variance estimator of $V^{\\text {sbr }}$ to account for the gain in precision. When the stratification variable $X$ takes only a finite number of values and there are at least two units in each of treatment and control, a simple solution is as follows: first, using the law of total variance, we re-write $V^{\\text {sbr }}$ as\n\n$$\nV^{\\mathrm{sbr}}=E\\left[\\frac{\\operatorname{Var}\\left[Y_{i}(1) \\mid X_{i}\\right]}{\\pi}\\right]+E\\left[\\frac{\\operatorname{Var}\\left[Y_{i}(0) \\mid X_{i}\\right]}{1-\\pi}\\right]+\\operatorname{Var}\\left[E\\left[Y_{i}(1)-Y_{i}(0) \\mid X_{i}\\right]\\right]\n$$\n\nExploiting the discreteness in $X$, we can expand this as\n\n$$\nV^{\\mathrm{sbr}}=\\sum_{x \\in \\mathcal{X}} p(x)\\left(\\frac{\\operatorname{Var}\\left[Y_{i}(1) \\mid X_{i}=x\\right]}{\\pi}+\\frac{\\operatorname{Var}\\left[Y_{i}(0) \\mid X_{i}=x\\right]}{1-\\pi}\\right)+\\sum_{x \\in \\mathcal{X}} p(x)\\left(E\\left[Y_{i}(1)-Y_{i}(0) \\mid X_{i}=x\\right]-\\Delta\\right)^{2}\n$$\n\nwhere $p(x)=P\\left\\{X_{i}=x\\right\\}$. From this expression it is clear how to construct a consistent estimator of $V^{\\text {sbr }}$ by simply replacing all of the unknown means and variances by their sample counterparts.\n\nLet us briefly comment on the implications of the above discussion for design-based inference. Although we argued at the end of Section 3.2 that, in general, consistent estimators for the super-population variance immediately provide conservative estimators in the finite-population framework, it is worth mentioning that in this case the estimator is potentially \"excessively\" conservative: indeed, from the design-based perspective, it can be shown that a less conservative estimator is given by any consistent estimator of\n\n$$\n\\sum_{x \\in \\mathcal{X}} \\frac{n(x)}{n}\\left(\\frac{S_{1}^{2}(x)}{\\pi}+\\frac{S_{0}^{2}(x)}{1-\\pi}\\right)\n$$\n\nwhere $n(x)$ denotes the number of observations in stratum $x$ and $S_{d}^{2}(x)$ for $d \\in\\{0,1\\}$ are the population variances of the potential outcomes in stratum $x$ (see, for instance, Imbens and Rubin, 2015, for details). Note that this expression mimics only the first component of $V^{\\text {sbr }}$; this is because the second component arises from the random fluctuations in $n(x) / n$ versus $p(x)$, but these exactly coincide in a design-based framework. However, because of this, consistent estimators of (10) are not guaranteed to be valid if we view the sample as being drawn from a larger (finite or super-) population.\n\nFinally, we conclude with a short discussion on settings in which the assignment proportions differ across\n\nstrata (see, for instance, Karlan and Zinman, 2010, for an example). We continue to assume that treatment is assigned using stratified block randomization but we let $\\pi(x)$, the fraction of units that are treated in stratum $x \\in\\{1,2, \\ldots, \\mathcal{X}\\}$, differ across strata. Our first observation is that the difference-in-means estimator is generally no longer consistent for $\\Delta$; intuitively, since the probability of assignment differs across strata, the experimental design induces selection bias with respect to the stratification variable since the composition of the covariates differs between treatment and control. Instead, we could consider the following estimator which computes a weighted average of the stratum-specific difference-in-means estimators, with weights determined by the strata sizes:\n\n$$\n\\hat{\\Delta}_{n}^{\\text {est }}=\\sum_{x \\in \\mathcal{X}} \\frac{n(x)}{n} \\hat{\\Delta}_{n}(x)\n$$\n\nwhere $n(x)$ denotes the number of observations in stratum $x$ and $\\hat{\\Delta}_{n}(x)$ denotes the difference-in-means estimator computed in stratum $x$. This estimator is consistent, and moreover it can be shown that its asymptotic variance is given by\n\n$$\n\\sum_{x \\in \\mathcal{X}} p(x)\\left(\\frac{\\operatorname{Var}\\left[Y_{i}(1) \\mid X_{i}=x\\right]}{\\pi(x)}+\\frac{\\operatorname{Var}\\left[Y_{i}(0) \\mid X_{i}=x\\right]}{1-\\pi(x)}\\right)+\\sum_{x \\in \\mathcal{X}} p(x)\\left(E\\left[Y_{i}(1)-Y_{i}(0) \\mid X_{i}=x\\right]-\\Delta\\right)^{2}\n$$\n\nfor which a consistent estimator can once again be constructed by taking sample analogs; see Bugni et al. (2019) for details.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 4.2.1 Variance Estimation with Small Strata \n\nWhen $X$ was discrete, a natural estimator of $V^{\\text {shr }}$ could be constructed by computing sample analogs of the means and variances of the potential outcomes at the stratum level. If, however, $X_{i}$ contains continuous components and the experiment is \"finely stratified\" in such a way that there is only one treated or control observation per stratum, then this logic breaks down. A leading example of when this occurs is with pairwise matching: in a matched pairs experiment, units are paired together based on their observed covariate values and then treatment is assigned such that, in each pair, one unit is selected at random to receive treatment and the other control. With only one observation per stratum with a given treatment, we cannot estimate the stratum-level variances by simply taking sample analogs as proposed in the previous section. As discussed in Klar and Donner (1997), this added challenge to variance estimation has often been perceived as a fundamental analytical limitation of matched pair designs. In this section, we briefly illustrate how the variance of $\\hat{\\Delta}_{n}$ can still be consistently estimated by using a \"collapsed-strata\" estimator in the spirit of Hansen et al. (1953).\n\nTo streamline the exposition, in what follows we focus only on the setting where $\\pi=\\frac{1}{2}$ and units are matched into pairs. First, note that it can be shown under appropriate assumptions that, even with small strata, the limiting variance of $\\hat{\\Delta}_{n}$ is still given by (9) (see, for instance, Bai et al., 2022). The unconditional variances $\\operatorname{Var}\\left[Y_{i}(1)\\right]$ and $\\operatorname{Var}\\left[Y_{i}(0)\\right]$ in (9) are consistently estimable using their sample coun-\n\nterparts. We thus focus on estimating the last term on the right-hand side of (9), which in this case is $(1 / 2) \\operatorname{Var}\\left[E\\left[Y_{i}(1)+Y_{i}(0) \\mid X_{i}\\right]\\right]$. To that end, note that by elementary properties of the variance and the law of iterated expectations,\n\n$$\n\\operatorname{Var}\\left[E\\left[Y_{i}(1)+Y_{i}(0) \\mid X_{i}\\right]\\right]=E\\left[E\\left[Y_{i}(1)+Y_{i}(0) \\mid X_{i}\\right]^{2}\\right]-E\\left[Y_{i}(1)+Y_{i}(0)\\right]^{2}\n$$\n\nThe second term on the right-hand side is again unconditional and thus easy to estimate using a sample analog. Therefore, it suffices to consistently estimate\n\n$$\nE\\left[E\\left[Y_{i}(1)+Y_{i}(0) \\mid X_{i}\\right]^{2}\\right]\n$$\n\nIntuitively, to estimate this last quantity we would want independent variation in $Y_{i}(1)$ and $Y_{i}(0)$ for each given value of $X_{i}$. Let us suppose for a moment that we did in fact have two pairs of units $\\left\\{Y_{1}, Y_{2}\\right\\}$ and $\\left\\{Y_{3}, Y_{4}\\right\\}$ sharing the same value of $X$. In each pair, there is exactly one treated and control observation per stratum. It follows that that $Y_{1}+Y_{2}$ and $Y_{3}+Y_{4}$ both share the same conditional mean $E[Y(1)+Y(0) \\mid X]$. Moreover, across pairs, the outcomes are (conditionally) independent. As a consequence, we might conjecture that $\\left(Y_{1}+Y_{2}\\right)\\left(Y_{3}+Y_{4}\\right)$ is approximately equal to $E[Y(1)+Y(0) \\mid X]^{2}+\\left(\\epsilon_{1}+\\epsilon_{2}\\right)\\left(\\epsilon_{3}+\\epsilon_{4}\\right)$, where $E\\left[\\epsilon_{i} \\epsilon_{j} \\mid X^{(n)}\\right]=0$ for $i$ and $j$ in different pairs. Therefore, we can construct a consistent estimator for (12) by averaging over these products across pairs of pairs. Of course in practice, this scenario is not realistic, as units which are paired together will typically not share the same value of $X$. If, however, matching is performed so that we expect pairs and \"adjacent\" pairs to have similar characteristics, then this intuition can be formalized to construct a consistent variance estimator for (12), and thus it is possible to construct a consistent estimator of $V^{\\text {sbr }}$ even with \"small\" strata; for a formal exposition of this idea, see Bai et al. (2022, 2024e).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# 4.3 Further Reading \n\nThe exposition in Section 4.1 is most closely inspired by Bai (2022), who establishes the finite-sample optimality of certain matched-pair designs. Much of the discussion in Section 4.2 comes from Bugni et al. (2018), who study inference for covariate-adaptive randomization when the treated fraction is constant across strata; Bugni et al. (2019) extends the analysis to settings with multiple treatments and where the treatment proportions are allowed to differ across strata. Bai et al. (2022) studies inference for matched pair designs and develops the estimation strategy discussed in Section 4.2.1. Cytrynbaum (2023b) generalizes these estimation and inference procedures beyond pair-matching and jointly analyzes the stratification problem combined with the problem of selecting a representative sample based on covariates. Bai et al. (2024e) generalizes Bai et al. (2022) to settings with multiple treatments. Pashley and Miratrix (2021) and Bai et al. (2025a) provide overviews of design-based analyses of stratified experiments.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "# 5 Regression Adjustment in Randomized Experiments \n\nIn this section, we consider the role of regression adjustment using baseline covariates in the analysis of randomized experiments. A primary motivation for regression adjustment is that it hopefully improves estimation precision in settings where the covariates are correlated with the experimental outcome. This practice has, however, often come under scrutiny. An influential paper of Freedman (2008), for instance, points out that standard linear regression adjustment only guarantees a gain in precision under strong assumptions, and concludes that in general \"[...] randomization does not justify the assumptions behind the OLS model.\" In Section 5.1 we review part of Freedman's critique, present a resolution popularized by Lin (2013), and briefly discuss some implications for inference. In Section 5.2 we explain how Lin's resolution is a special case of an estimation procedure based on the doubly-robust moment condition of the average treatment effect, and show how this perspective leads to more general regression adjustment strategies beyond linear adjustment.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "### 5.1 Linear Regression Adjustment in a Completely Randomized Experiment\n\nWe revisit the setting of a completely randomized experiment with $\\pi \\in(0,1)$. In particular, we assume that treatment assignment is independent of the outcomes and the baseline covariates; we briefly comment on settings where the treatment assignment may itself depend on the baseline covariates (for instance, in a stratified randomized experiment) in Section 5.1.1 and provide further references in Section 5.3. To begin, recall from (1) that the unadjusted difference-in-means estimator can be described as the OLS estimator of the coefficient on $D_{i}$ in a linear regression of $Y_{i}$ on a constant and $D_{i}$. A natural starting point for regression adjustment is then to instead consider the OLS estimator of the coefficient on $D_{i}$ in a linear regression of $Y_{i}$ on a constant, $D_{i}$, and the covariates $X_{i}$ :\n\n$$\n\\text { regress } Y_{i} \\text { on constant }+D_{i}+X_{i}\n$$\n\nLet $\\tilde{\\beta}_{n}$ be the resulting estimator of the coefficient on $D_{i}$ from the above regression, and let $\\tilde{\\gamma}_{n}$ be the resulting estimator of the coefficient on $X_{i}$. We emphasize here that we do not view the regression in (13) as describing the true data generating process for the outcomes $Y_{i}$, but treat this simply as the description of an estimation procedure. Since the experimental assignment guarantees that $D$ and $X$ are independent, it is not surprising given standard regression logic that $\\tilde{\\beta}_{n}$ remains consistent for the average treatment effect $\\Delta$. One might expect further that $\\tilde{\\beta}_{n}$ is more efficient than the simple difference-in-means if $X_{i}$ is correlated with the experimental outcomes. It can be shown, however, that $\\tilde{\\beta}_{n}$ is asymptotically normal with variance given by (see, for instance, Negi and Wooldridge, 2021; Ma et al., 2022)\n\n$$\nV^{\\text {pool }}=V^{\\text {cr }}-\\frac{1}{\\pi(1-\\pi)} \\gamma^{\\prime} \\Sigma_{X} \\gamma+\\frac{2(2 \\pi-1)}{\\pi(1-\\pi)} \\gamma^{\\prime} \\Sigma_{X}(\\gamma(1)-\\gamma(0))\n$$\n\nwhere $\\gamma$ denotes the probability limit of $\\tilde{\\gamma}_{n}, \\Sigma_{X}$ denotes the covariance matrix of $X_{i}$, and $\\gamma(d)$ is the probability limit of the coefficient on $X_{i}$ in a linear regression of the potential outcome $Y_{i}(d)$ on a constant and $X_{i}$. Note that the second term in this expression is always weakly negative, and is strictly negative whenever the covariates are correlated with the outcomes in the sense that $\\gamma^{\\prime} \\Sigma_{X} \\gamma>0$. However, the sign of the third term in the variance is ambiguous, and as a result, there is no guarantee that $V^{\\text {pool }}$ is smaller than $V^{\\text {cr }}$ in general. Note that the third term of $V^{\\text {pool }}$ is zero in the special cases where either $\\pi=1 / 2$ or $\\gamma(1)=\\gamma(0)$, so that in these cases we can in fact conclude that $V^{\\text {pool }} \\leq V^{\\text {cr }}$. In words, regression adjustment based on the regression in (13) is not guaranteed to improve precision in general, but (weakly) improves precision in the special cases where assignment is equal between treatment and control, or treatment effects are sufficiently homogeneous, e.g., if treatment effects are constant such that $Y_{i}(1)-Y_{i}(0)=\\Delta$.\n\nThe difficulty with estimation based on (13) is that the regression pools together the observations under both treatment and control when estimating the relationship between the outcome and the covariates. To explain how this could be resolved, we first review the classical problem of estimation of a population mean using regression (see, for example, Cochran, 1977). Let us suppose for a moment that $Y_{i}(d)$ and $E\\left[X_{i}\\right]$ were observed, and suppose we wished to estimate $E\\left[Y_{i}(d)\\right]$ using one of the following two regressions:\n\n$$\n\\begin{aligned}\n& \\text { regress } Y_{i}(d) \\text { on constant } \\\\\n& \\text { regress } Y_{i}(d) \\text { on constant }+\\left(X_{i}-E\\left[X_{i}\\right]\\right)\n\\end{aligned}\n$$\n\nLet $\\hat{\\alpha}_{n}^{(1)}$ be the resulting estimator of the coefficient on the constant from the regression in (14), and let $\\hat{\\alpha}_{n}^{(2)}$ and $\\hat{\\gamma}_{n}^{(2)}$ be the resulting estimators of the coefficient on the constant and the coefficient on $\\left(X_{i}-E\\left[X_{i}\\right]\\right)$ from the regression in (15). We then obtain from elementary properties of regression that\n\n$$\n\\hat{\\alpha}_{n}^{(1)}=\\frac{1}{n} \\sum_{1 \\leq i \\leq n} Y_{i}(d)\n$$\n\nand\n\n$$\n\\hat{\\alpha}_{n}^{(2)}=\\frac{1}{n} \\sum_{1 \\leq i \\leq n} Y_{i}(d)-\\hat{\\gamma}_{n}^{(2)}\\left(\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(X_{i}-E\\left[X_{i}\\right]\\right)\\right)\n$$\n\nIt follows that both $\\hat{\\alpha}_{1}$ and $\\hat{\\alpha}_{2}$ are consistent estimators of $E\\left[Y_{i}(d)\\right]$ (note here that it was important that we de-meaned the observable characteristics $X_{i}$ ), and that both are asymptotically normal with variances given by $\\operatorname{Var}\\left[Y_{i}(d)\\right]$ and\n\n$$\n\\operatorname{Var}\\left[Y_{i}(d)\\right]-\\frac{\\operatorname{Cov}\\left[Y_{i}(d), X\\right]^{2}}{\\operatorname{Var}[X]}\n$$\n\nrespectively. We thus see immediately that $\\hat{\\alpha}_{n}^{(2)}$ is a more precise estimator of $E\\left[Y_{i}(d)\\right]$ whenever the covariates $X_{i}$ are correlated with the outcome $Y_{i}(d)$.\n\nLin (2013) leverages this insight for the purpose of estimating the average treatment effect in a completely randomized experiment by effectively implementing the regression (15) in each of the subsamples $D_{i}=d$ for\n\n$d \\in\\{0,1\\}$, and then taking the difference of the results. This procedure can be operationalized by estimating a linear regression model with an additional interaction term between $D$ and $X$ :\n\n$$\n\\text { regress } Y_{i} \\text { on constant }+D_{i}+X_{i}+D_{i}\\left(X_{i}-\\bar{X}_{n}\\right)\n$$\n\nLet $\\hat{\\beta}_{n}$ be the resulting estimator of the coefficient on $D_{i}$ from the above regression. It can then be shown that $\\hat{\\beta}_{n}$ is a consistent and asymptotically normal estimator of $\\Delta$ and that its asymptotic variance $V^{\\text {sat }}$ satisfies $V^{\\text {sat }}-V^{c r} \\leq 0$, with equality if and only if\n\n$$\n\\operatorname{Cov}\\left[\\frac{Y_{i}(1)}{\\pi}+\\frac{Y_{i}(0)}{1-\\pi}, X_{i}\\right]=0\n$$\n\n(see, for instance, Negi and Wooldridge, 2021; Ma et al., 2022). From this we can see that regression based on (16) always weakly improves precision regardless of the value of $\\pi$, and we should expect that it strictly improves precision whenever the potential outcomes are correlated with the baseline covariates (outside of pathological cases). Further note that (17) holds if $E\\left[\\frac{Y_{i}(1)}{\\pi}+\\frac{Y_{i}(0)}{1-\\pi} \\mid X_{i}\\right]$ is constant; this is exactly the condition that guaranteed that $X_{i}$ is an irrelevant stratification variable in Section 4. From this we can conclude that if $X_{i}$ is an irrelevant variable for stratification, then we cannot increase estimation efficiency through adjusting for $X_{i}$ either. We further explore the relationship between stratification and regression adjustment at the end of Section 5.2.\n\nWe conclude this section by noting that the above analysis suggests that, when $\\pi=1 / 2$, regression adjustment based on (13) may be preferred to adjustment based on (16); in this case both estimators have the same asymptotic variance and (13) involves the estimation of fewer parameters. We caution, however, that this conclusion is very specific to the case of a binary treatment, and that strategies based on interacted models tend to generalize more broadly beyond the special case we considered here.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# 5.1.1 Inference on $\\Delta$ when using Linear Regression Adjustment: Some Caveats \n\nWe briefly discuss some complications surrounding inference on $\\Delta$ based on the regressions (13) and (16) under complete randomization, and mention some related complications when generalizing beyond complete randomization. Recall that in Section 3 we explained that the robust variance estimator obtained from the regression in (1) is consistent for the asymptotic variance of the difference-in-means estimator $\\hat{\\Delta}_{n}$. A natural follow-up question is then whether or not the robust variance estimators obtained from the regressions (13) and (16) are consistent for (or at least an upper bound on) the asymptotic variances of $\\hat{\\beta}_{n}$ and $\\hat{\\beta}_{n}$, respectively. In the case of the regression in (13), the answer is yes: the robust variance estimator is consistent because $\\widehat{\\beta}_{n}$ is simply the OLS estimator of the best linear predictor of $Y$ given $1, D$, and $X$. In the case of the regression in (16), however, the answer is no whenever we view the sample as being drawn from a larger (finite or super-) population. The issue here is similar to our discussion surrounding consistent estimators of the variance when studying stratified randomized experiments in Section 4.2. In particular, note that the regression in (16)\n\ninvolves a de-meaning of the baseline covariates, and when we view the sample as being drawn from a larger population, the random fluctuations in $\\bar{X}_{n}$ versus $E\\left[X_{i}\\right]$ need to be taken into consideration. As a result, the robust variance estimator obtained from regression (16) is not guaranteed to be valid outside of the design-based framework (in which case $\\bar{X}_{n}$ is non-random). When moving beyond complete randomization to settings with stratified assignment, the complications become more subtle. When stratification is based on a finite number of discrete categories, an easy solution is to perform regression adjustment in each stratum separately and then aggregate the stratum-level estimates; we provide relevant references for this and more complicated settings involving stratification in Section 5.3. With these considerations in mind, we suggest that practitioners use our R command sreg to perform regression adjustment in (stratified) randomized experiments: see Trifonov et al. (2025).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# 5.2 Double Robustness and General Regression Adjustments \n\nIn this section, we explain how Lin's interacted regression (16) is a special case of a more general class of estimators based on a doubly-robust moment function. To simplify the exposition we once again assume that treatment is completely randomized with treated fraction $\\pi \\in(0,1)$. Note that because the treatments are independent of the potential outcomes and covariates, the probability of assignment of any individual $i$ satisfies $P\\left\\{D_{i}=1 \\mid X_{i}=x\\right\\} \\equiv \\pi$. Moreover, it follows that the treatments are independent of the potential outcomes conditional on the covariates.\n\nConsider the parameter $\\Delta_{0}$ defined by the following moment equation:\n\n$$\nE\\left[\\frac{D_{i}\\left(Y_{i}-\\tilde{\\mu}_{1}\\left(X_{i}\\right)\\right)}{\\pi}-\\frac{\\left(1-D_{i}\\right)\\left(Y_{i}-\\tilde{\\mu}_{0}\\left(X_{i}\\right)\\right)}{1-\\pi}+\\tilde{\\mu}_{1}\\left(X_{i}\\right)-\\tilde{\\mu}_{0}\\left(X_{i}\\right)-\\Delta_{0}\\right]=0\n$$\n\nwhere $\\tilde{\\mu}_{1}(x), \\tilde{\\mu}_{0}(x)$ are arbitrary researcher-defined functions of $x$ which we call the \"working models\" for the conditional expectations $E\\left[Y_{i}(1) \\mid X_{i}=x\\right]$ and $E\\left[Y_{i}(0) \\mid X_{i}=x\\right]$. Note that\n\n$$\n\\begin{aligned}\nE\\left[\\frac{D_{i}\\left(Y_{i}-\\tilde{\\mu}_{1}\\left(X_{i}\\right)\\right)}{\\pi}+\\tilde{\\mu}_{1}\\left(X_{i}\\right)\\right] & =E\\left[\\frac{D_{i} Y_{i}(1)}{\\pi}-\\frac{D_{i} \\tilde{\\mu}_{1}\\left(X_{i}\\right)}{\\pi}+\\tilde{\\mu}_{1}\\left(X_{i}\\right)\\right] \\\\\n& =E\\left[E\\left[\\frac{D_{i} Y_{i}(1)}{\\pi} \\mid X_{i}\\right]-\\frac{E\\left[D_{i} \\mid X_{i}\\right] \\tilde{\\mu}_{1}\\left(X_{i}\\right)}{\\pi}+\\tilde{\\mu}_{1}\\left(X_{i}\\right)\\right] \\\\\n& =E\\left[\\frac{\\pi E\\left[Y_{i}(1) \\mid X_{i}\\right]}{\\pi}-\\frac{\\pi \\tilde{\\mu}_{1}\\left(X_{i}\\right)}{\\pi}+\\tilde{\\mu}_{1}\\left(X_{i}\\right)\\right] \\\\\n& =E\\left[Y_{i}(1)\\right]\n\\end{aligned}\n$$\n\nCombining this with a similar argument for the other symmetric term in (18), it follows that the solution to (18) is $\\Delta_{0}=\\Delta$ for any choice of working models $\\tilde{\\mu}_{1}(\\cdot)$ and $\\tilde{\\mu}_{0}(\\cdot)$. Equation (18) is the famous \"doubly-robust\" moment equation for the ATE due to Robins et al. (1995). ${ }^{2}$ Since $\\pi$ is known in a randomized experiment\n\n[^0]\n[^0]:    ${ }^{2}$ The term \"doubly-robust\" is due to the fact that, if conversely $\\tilde{\\mu}_{1}(x)=E\\left[Y_{i}(1) \\mid X_{i}=x\\right]$ and $\\tilde{\\mu}_{0}(x)=E\\left[Y_{i}(0) \\mid X_{i}=x\\right]$, equation (18) still identifies $\\Delta$ even if $\\pi \\neq P\\left\\{D_{i}=1 \\mid X_{i}=x\\right\\}$.\n\nby construction, estimators based on taking a sample analog of (18) will be consistent for $\\Delta$ regardless of the choice of $\\tilde{\\mu}_{1}(\\cdot)$ and $\\tilde{\\mu}_{0}(\\cdot)$. In this way, we obtain the well-known augmented inverse-propensity weighted (AIPW) estimator of $\\Delta$ :\n\n$$\n\\hat{\\Delta}_{n}^{\\mathrm{AIPW}}=\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(\\frac{D_{i}\\left(Y_{i}-\\hat{\\mu}_{1, n}\\left(X_{i}\\right)\\right)}{\\pi}-\\frac{\\left(1-D_{i}\\right)\\left(Y_{i}-\\hat{\\mu}_{0, n}\\left(X_{i}\\right)\\right)}{1-\\pi}+\\hat{\\mu}_{1, n}\\left(X_{i}\\right)-\\hat{\\mu}_{0, n}\\left(X_{i}\\right)\\right)\n$$\n\nwhere $\\hat{\\mu}_{1, n}(\\cdot)$ and $\\hat{\\mu}_{0, n}(\\cdot)$ are consistent estimators of the working models. $\\hat{\\Delta}_{n}^{\\text {AIPW }}$ can recover a wide range of covariate-adjusted estimators of $\\Delta$ by specifying different choices of $\\tilde{\\mu}_{1}(\\cdot)$ and $\\tilde{\\mu}_{0}(\\cdot)$ and their corresponding estimators. For instance, if we set $\\hat{\\mu}_{d, n}=\\frac{1}{n_{d}} \\sum_{1 \\leq i \\leq n} Y_{i} I\\left\\{D_{i}=d\\right\\}$, then we recover the difference-in-means estimator $\\hat{\\Delta}_{n}$ (if we instead set $\\hat{\\mu}_{1, n}(\\cdot)=\\hat{\\mu}_{0, n}(\\cdot)=0$, then we recover the Horvitz-Thompson estimator of the average treatment effect). Alternatively, if we let $\\hat{\\gamma}_{n}(d)$ be the OLS estimator of the coefficient on $X_{i}$ in a regression of $Y_{i}$ on a constant and $X_{i}$ for observations with $D_{i}=d$, and set\n\n$$\n\\hat{\\mu}_{d, n}\\left(X_{i}\\right)=\\left(X_{i}-\\bar{X}_{n}\\right)^{\\prime} \\hat{\\gamma}_{n}(d)\n$$\n\nfor $d \\in\\{0,1\\}$, then $\\hat{\\Delta}_{n}^{\\text {AIPW }}$ recovers Lin's interacted linear regression estimator (16).\nIf $\\hat{\\mu}_{1, n}(\\cdot)$ and $\\hat{\\mu}_{0, n}(\\cdot)$ are appropriately chosen non-parametric estimators of the true conditional mean functions $E\\left[Y_{i}(1) \\mid X_{i}=x\\right]$ and $E\\left[Y_{i}(0) \\mid X_{i}=x\\right]$, then $\\hat{\\Delta}_{n}^{\\text {AIPW }}$ becomes a non-parametric regression-adjusted estimator of the average treatment effect. Under appropriate assumptions, it can be shown that $\\hat{\\Delta}_{n}^{\\text {AIPW }}$ is asymptotically normal with variance given by\n\n$$\nV^{*}=E\\left[\\frac{\\operatorname{Var}\\left[Y_{i}(1) \\mid X_{i}\\right]}{\\pi}\\right]+E\\left[\\frac{\\operatorname{Var}\\left[Y_{i}(0) \\mid X_{i}\\right]}{1-\\pi}\\right]+\\operatorname{Var}\\left[E\\left[Y_{i}(1)-Y_{i}(0) \\mid X_{i}\\right]\\right]\n$$\n\nsee Tu et al. (2023), Rafi (2023) for details. Importantly, this variance is the efficient variance for estimating $\\Delta$ using a randomized experiment when the probability of assignment is exogenously constrained to be $\\pi$ (see Armstrong, 2022; Rafi, 2023; Bai et al., 2025b).\n\nNote that $V^{*}$ coincides exactly with the asymptotic variance $V^{\\text {sbr }}$ obtained when using the unadjusted estimator $\\hat{\\Delta}_{n}$ in a finely stratified randomized experiment with \"small\" strata (see Section 4.2.1). We have thus demonstrated two alternative methods for achieving the efficient variance $V^{*}$ when estimating $\\Delta$ via a randomized experiment: ${ }^{3}$\n\n1. Assign treatment using complete randomization and estimate $\\Delta$ using $\\hat{\\Delta}_{n}^{\\text {AIPW }}$ with suitable nonparametric estimators $\\hat{\\mu}_{d, n}(\\cdot)$ of the conditional means $E\\left[Y_{i}(d) \\mid X_{i}\\right]$.\n2. Assign treatment using \"fine stratification\" (for instance, if $\\pi=1 / 2$ this could be a matched pairs design as described in Section 4.2.1) and estimate $\\Delta$ using $\\hat{\\Delta}_{n}$.\n[^0]\n[^0]:    ${ }^{3}$ Of course, this list of methods is not exhaustive. For instance, efficient estimation could also be achieved under i.i.d. randomization using a standard inverse-propensity weighted (IPW) estimator, where the propensity score is non-parametrically estimated as a function of $X$ : see in particular Hirano et al. (2003) for details.\n\nThis demonstrates that experiments which assign treatment using fine stratification effectively perform nonparametric regression adjustment \"by design\"; this feature of finely stratified experiments is further explored in Cytrynbaum (2023b) and Bai et al. (2025b). In principle, both strategies could be combined, by first stratifying on the covariates which might be expected to be most predictive for the outcome of interest, and then adjusting for a (potentially high-dimensional) set of additional covariates via regression ex-post: see Bai et al. (2024c) for details.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "# 5.3 Further Reading \n\nThe exposition in Section 5.1 closely follows Negi and Wooldridge (2021) and Ma et al. (2022), although some of their expressions have been modified so that they could be more easily related to Sections 3 and 4 of this paper. The use of doubly-robust estimators for regression adjustment in randomized experiments goes back at least to the work of Tsiatis et al. (2008); Tu et al. (2023) and Rafi (2023) extend these results to general stratified experiments with finitely many strata. Bai et al. (2024c) and Cytrynbaum (2023a) consider regression adjustment in settings with \"small\" strata, in the sense of Section 4.2.1. Wang et al. (2023a) study regression adjustment for general parameters defined by estimating equations. Recent work on design-based analyses of regression adjusted estimators can be found in Aronow and Middleton (2013), Wu and Gagnon-Bartsch (2018), Liu et al. (2021), Chang (2023), Chiang et al. (2023).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "## 6 The Analysis of Cluster Randomized Experiments\n\nUntil now, our focus has been on experiments where treatment is assigned at the individual level and each individual's outcome depends only on their own treatment. In this section, we consider randomized experiments where treatment is assigned at an aggregated level which we call a cluster: for example, when evaluating an educational intervention we may observe outcomes at the student-level, but assign treatment at the school-level, so that every student in the school receives the same treatment. This type of cluster-level assignment is extremely common in practice: see, for instance, Angrist and Lavy (2009); Banerjee et al. (2015); Cr\u00e9pon et al. (2015); Bruhn et al. (2016); Romero et al. (2020). There are two common explanations for why a researcher would consider cluster-level assignment as opposed to individual-level assignment: first, there could be logistical constraints on the experiment that require that every individual in a cluster receives the same treatment. Second, we may be concerned that there is treatment interference, i.e., the treatment statuses of individuals in a cluster may affect the outcomes of others. In Section 6.1 we introduce the framework and define some relevant analogs to the average treatment effect in this setting. In Section 6.2, we discuss inference in cluster randomized experiments under complete randomization and stratified block randomization.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# 6.1 Defining Average Treatment Effects in Cluster Randomized Experiments \n\nTo accommodate cluster-level assignment, we first modify our notation relative to what we have considered thus far. Let $Y_{i, g}$ denote the observed outcome of the $i$ th unit in the $g$ th cluster, $D_{g}$ denote an indicator for whether or not the $g$ th cluster is treated, and $N_{g}$ the size of the $g$ th cluster. Further denote by $Y_{i, g}(1)$ the potential outcome of the $i$ th unit in the $g$ th cluster if the cluster is treated and by $Y_{i, g}(0)$ the potential outcome of the $i$ th unit in the $g$ th cluster if not treated. As usual, the observed outcome and potential outcomes are related to treatment assignment by the relationship\n\n$$\nY_{i, g}=Y_{i, g}(1) D_{g}+Y_{i, g}(0)\\left(1-D_{g}\\right)\n$$\n\nIn practice it is sometimes the case that the researcher does not sample all of the units in a given cluster. To allow for this possibility, define $\\mathcal{M}_{g}$ to be the subset of $\\left\\{1, \\ldots, N_{g}\\right\\}$ corresponding to the observations within the $g$ th cluster that are sampled by the researcher. For example, in the event that all observations in a cluster are sampled, $\\mathcal{M}_{g}=\\left\\{1, \\ldots, N_{g}\\right\\}$ and $\\left|\\mathcal{M}_{g}\\right|=N_{g}$.\n\nWith this updated notation, our sampling framework models\n\n$$\n\\left\\{\\left(\\left(Y_{i, g}(1), Y_{i, g}(0): 1 \\leq i \\leq N_{g}\\right), \\mathcal{M}_{g}, N_{g}\\right): 1 \\leq g \\leq G\\right\\}\n$$\n\nas a collection of $G$ independent and identically distributed draws from a distribution of clusters. Importantly, we note here that the cluster sizes $N_{g}$ are modeled as random variables which are potentially related to the potential outcomes. In this framework, we introduce two natural parameters that arise as generalizations of the average treatment effect $\\Delta$ which we focused on in earlier sections. These parameters differ in the way they aggregate, or average, the individual level treatment effects.\n\nBoth parameters of interest we consider can be written as weighted averages of the cluster-level average treatment effects:\n\n$$\nE\\left[\\omega_{g}\\left(\\frac{1}{N_{g}} \\sum_{1 \\leq i \\leq N_{g}}\\left(Y_{i, g}(1)-Y_{i, g}(0)\\right)\\right)\\right]\n$$\n\nfor different choices of (possibly random) weights $\\omega_{g}$ satisfying $E\\left[\\omega_{g}\\right]=1$. The first parameter of interest corresponds to the choice of $\\omega_{g}=1$, thus weighting the average effect of the treatment across clusters equally:\n\n$$\n\\Delta^{\\mathrm{eq}}:=E\\left[\\frac{1}{N_{g}} \\sum_{1 \\leq i \\leq N_{g}}\\left(Y_{i, g}(1)-Y_{i, g}(0)\\right)\\right]\n$$\n\nWe refer to this quantity as the equally-weighted cluster-level average treatment effect. Since $\\Delta^{\\text {eq }}$ assigns an equal weight to each cluster regardless of size, it can be thought of as the average treatment effect where the clusters themselves are the units of interest. The second parameter of interest corresponds to the choice of $\\omega_{g}=N_{g} / E\\left[N_{g}\\right]$, thus weighting the average effect of the treatment across clusters in proportion to their\n\nsize:\n\n$$\n\\Delta^{\\text {size }}:=E\\left[\\frac{1}{E\\left[N_{g}\\right]} \\sum_{1 \\leq i \\leq N_{g}}\\left(Y_{i, g}(1)-Y_{i, g}(0)\\right)\\right]\n$$\n\nWe refer to this quantity as the size-weighted cluster-level average treatment effect. Since $\\Delta^{\\text {size }}$ assigns a weight proportional to each cluster's size, it can be thought of as the average treatment effect where individuals are the units of interest.\n\nNote that in empirical settings with treatment effect heterogeneity (so that $Y_{i, g}(1)-Y_{i, g}(0)$ is random) and cluster-size heterogeneity, we should expect that $\\Delta^{\\text {eq }}$ and $\\Delta^{\\text {size }}$ are indeed distinct parameters with differing policy interpretations. For instance, suppose the experiment studies the effect of an educational intervention on students' reading level. If the policy-maker is interested in raising the average reading level across all students, then the magnitude of $\\Delta^{\\text {size }}$ is the relevant parameter. If, on the other hand, the policymaker has concerns about raising the average reading level across all schools, then the magnitude of $\\Delta^{\\text {eq }}$ would also be important to consider.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "# 6.2 Inference in Cluster Randomized Experiments \n\nIn this section, we study estimation and inference on $\\Delta^{\\text {eq }}$ and $\\Delta^{\\text {size }}$. We begin with the setting of a completely randomized experiment; note that in the context of a cluster randomized experiment this means that a fraction $\\pi \\in(0,1)$ of clusters is assigned to treatment and the rest to control.\n\nWe begin by first studying the probability limit of the difference-in-means estimator obtained from a regression of the individual-level outcomes $Y_{i, g}$ on a constant and the cluster-level treatment $D_{g}$. With our new notation, this is given by\n\n$$\n\\hat{\\Delta}_{G}:=\\frac{\\sum_{1 \\leq g \\leq G} \\sum_{i \\in \\mathcal{M}_{g}} Y_{i, g} D_{g}}{\\sum_{1 \\leq g \\leq G}\\left|\\mathcal{M}_{g}\\right| D_{g}}-\\frac{\\sum_{1 \\leq g \\leq G} \\sum_{i \\in \\mathcal{M}_{g}} Y_{i, g}\\left(1-D_{g}\\right)}{\\sum_{1 \\leq g \\leq G}\\left|\\mathcal{M}_{g}\\right|\\left(1-D_{g}\\right)}\n$$\n\nIt can be shown under appropriate assumptions that\n\n$$\n\\hat{\\Delta}_{G} \\xrightarrow{P} E\\left[\\frac{1}{E\\left[\\left|\\mathcal{M}_{g}\\right|\\right]} \\sum_{i \\in \\mathcal{M}_{g}}\\left(Y_{i, g}(1)-Y_{i, g}(0)\\right)\\right]=: \\vartheta\n$$\n\nas $G \\rightarrow \\infty$; see Bugni et al. (2022) for details. This parameter corresponds to a sample-weighted cluster-level average treatment effect and in general does not coincide with either $\\Delta^{\\text {eq }}$ or $\\Delta^{\\text {size }}$. Of course, in some special cases, $\\vartheta$ does coincide with $\\Delta^{\\text {eq }}$ or $\\Delta^{\\text {size }}$. For instance, if we sample the same number of observations in every cluster, then $\\vartheta$ coincides with $\\Delta^{\\text {eq }}$. If instead we sample observations proportionally to the size of each cluster, then $\\vartheta$ coincides with $\\Delta^{\\text {size }}$.\n\nIn order to do inference on $\\Delta^{\\text {eq }}$ and $\\Delta^{\\text {size }}$ regardless of the specific choice of sampling design, we now present estimators which are consistent for these parameters more generally. In the case of $\\Delta^{\\text {eq }}$, we consider\n\nthe following difference-in-means estimator computed on the cluster-average outcomes:\n\n$$\n\\hat{\\Delta}_{G}^{\\mathrm{eq}}:=\\frac{\\sum_{1 \\leq g \\leq G} \\bar{Y}_{g} D_{g}}{\\sum_{1 \\leq g \\leq G} D_{g}}-\\frac{\\sum_{1 \\leq g \\leq G} \\bar{Y}_{g}\\left(1-D_{g}\\right)}{\\sum_{1 \\leq g \\leq G}\\left(1-D_{g}\\right)}\n$$\n\nwhere $\\bar{Y}_{g}=\\frac{1}{\\left|\\mathcal{M}_{g}\\right|} \\sum_{i \\in \\mathcal{M}_{g}} Y_{i, g}$. Note that $\\hat{\\Delta}_{G}^{\\text {eq }}$ may be obtained as the estimator of the coefficient on $D_{g}$ from the following regression:\n\n$$\n\\text { regress } \\bar{Y}_{g} \\text { on constant }+D_{g}\n$$\n\nas such, it is exactly the difference-in-means estimator obtained from viewing the clusters as the experimental units of interest, with outcomes defined by their cluster averages. Under appropriate assumptions it can be shown that\n\n$$\n\\sqrt{G}\\left(\\hat{\\Delta}_{G}^{\\mathrm{eq}}-\\Delta^{\\mathrm{eq}}\\right) \\xrightarrow{d} N\\left(0, V^{\\mathrm{eq}}\\right)\n$$\n\nas $G \\rightarrow \\infty$, where\n\n$$\nV^{\\mathrm{eq}}=\\frac{\\operatorname{Var}\\left[\\bar{Y}_{g}(1)\\right]}{\\pi}+\\frac{\\operatorname{Var}\\left[\\bar{Y}_{g}(0)\\right]}{1-\\pi}\n$$\n\nwith $\\bar{Y}_{g}(d)=\\frac{1}{\\left|\\mathcal{M}_{g}\\right|} \\sum_{i \\in \\mathcal{M}_{g}} Y_{i, g}(d)$ (see Bugni et al., 2022, for details). We thus obtain that $\\hat{\\Delta}_{G}^{\\text {eq }}$ is a consistent and asymptotically normal estimator of $\\Delta^{\\text {eq }}$ with asymptotic variance which exactly mirrors the asymptotic variance $V^{\\text {cr }}$ obtained in Section 3.2, but with the individual level outcomes $Y_{i}(d)$ replaced with the clusterlevel average outcomes $\\bar{Y}_{g}(d)$. It is therefore not surprising that a consistent estimator of $V^{\\text {eq }}$ can be obtained from the resulting heteroskedasticity-robust variance estimator of the regression in (20). More generally, when studying $\\Delta^{\\text {eq }}$ in a cluster randomized experiment, all of the tools introduced in Sections 3-5 can be applied by simply studying the experiment as if the clusters were individuals with outcomes given by the cluster averages $\\bar{Y}_{g}$.\n\nIn the case of $\\Delta^{\\text {size }}$, we consider the following cluster-size weighted difference-in-means estimator:\n\n$$\n\\hat{\\Delta}_{G}^{\\text {size }}=\\frac{\\sum_{1 \\leq g \\leq G} \\bar{Y}_{g} N_{g} D_{g}}{\\sum_{1 \\leq g \\leq G} N_{g} D_{g}}-\\frac{\\sum_{1 \\leq g \\leq G} \\bar{Y}_{g} N_{g}\\left(1-D_{g}\\right)}{\\sum_{1 \\leq g \\leq G} N_{g}\\left(1-D_{g}\\right)}\n$$\n\nNote that $\\hat{\\Delta}_{G}^{\\text {size }}$ may be obtained as the estimator of the coefficient on $D_{g}$ in the following weighted least squares regression:\n\n$$\n\\text { regress } Y_{i, g} \\text { on constant }+D_{g} \\text { using weights } N_{g} /\\left|\\mathcal{M}_{g}\\right|\n$$\n\nUnder appropriate assumptions it can be shown that\n\n$$\n\\sqrt{G}\\left(\\hat{\\Delta}_{G}^{\\text {size }}-\\Delta^{\\text {size }}\\right) \\xrightarrow{d} N\\left(0, V^{\\text {size }}\\right)\n$$\n\nas $G \\rightarrow \\infty$, where\n\n$$\nV^{\\text {size }}:=\\frac{\\operatorname{Var}\\left[\\bar{Y}_{g}(1)\\right]}{\\pi}+\\frac{\\operatorname{Var}\\left[\\bar{Y}_{g}(0)\\right]}{1-\\pi}\n$$\n\nwith\n\n$$\n\\widetilde{Y}_{g}(d):=\\frac{N_{g}}{E\\left[N_{g}\\right]}\\left(\\hat{Y}_{g}(d)-\\frac{E\\left[\\tilde{Y}_{g}(d) N_{g}\\right]}{E\\left[N_{g}\\right]}\\right)\n$$\n\nsee Bugni et al. (2022) for details. We thus obtain that $\\hat{\\Delta}_{G}^{\\text {size }}$ is a consistent and asymptotically normal estimator of $\\Delta^{\\text {size }}$ with asymptotic variance which again mirrors $V^{\\text {cr }}$, but now with the individual-level outcomes $Y_{i}(d)$ replaced with the re-scaled cluster-level outcomes $\\widetilde{Y}_{g}(d)$. It turns out that in this case, a consistent estimator of $V^{\\text {size }}$ can be obtained from the regression (21) by computing the resulting clusterrobust variance estimator. Equivalently, it is straightforward to construct a sample analog estimator of $V^{\\text {size }}$ where the infeasible re-scaled outcomes $\\widetilde{Y}_{g}(d)$ are replaced by feasible analogs:\n\n$$\n\\hat{Y}_{g}:=\\frac{N_{g}}{\\frac{1}{G} \\sum_{1 \\leq j \\leq G} N_{j}}\\left(\\hat{Y}_{g}-\\frac{\\frac{1}{G} \\sum_{1 \\leq j \\leq G} \\tilde{Y}_{j} I\\left\\{D_{j}=D_{g}\\right\\} N_{j}}{\\frac{1}{G} \\sum_{1 \\leq j \\leq G} I\\left\\{D_{j}=D_{g}\\right\\} N_{j}}\\right)\n$$\n\nWe find this latter approach particularly useful when moving beyond completely randomized experiments to settings with stratification: for example, under stratified block randomization, the limiting variance of $\\hat{\\Delta}_{G}^{\\text {size }}$ equals\n\n$$\n\\frac{\\operatorname{Var}\\left[\\widetilde{Y}_{g}(1)\\right]}{\\pi}+\\frac{\\operatorname{Var}\\left[\\widetilde{Y}_{g}(0)\\right]}{1-\\pi}-\\pi(1-\\pi) \\operatorname{Var}\\left[E\\left[\\frac{\\widetilde{Y}_{g}(1)}{\\pi}+\\frac{\\widetilde{Y}_{g}(0)}{1-\\pi} \\mid X_{g}\\right]\\right]\n$$\n\nwhere $X_{g}$ denotes the variables used in stratification (see Bugni et al., 2022; Bai et al., 2024d, for details). This variance mirrors the variance $V^{\\text {shr }}$ for individual-level stratified experiments defined in (9), and therefore similar inference procedures apply to cluster randomized experiments by modifying the individual-level outcomes to suitable cluster-level counterparts such as $\\hat{Y}_{g}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 19,
      "text": "# 6.3 Further Reading \n\nThe material in this section is most closely related to Bugni et al. (2022) and Bai et al. (2024d). Donner and Klar (2000) contains a textbook treatment of early work in cluster randomized experiments. Su and Ding (2021) and Wang et al. (2024) study regression adjustment for cluster randomized experiments without stratification (Bugni et al. (2022) and Bai et al. (2024d) discuss extensions of these results to settings with stratification). Other recent work on cluster randomized experiments (mostly from a design-based perspective) includes Imai et al. (2009), Schochet (2013), Middleton and Aronow (2015), Schochet et al. (2021), de Chaisemartin and Ramirez-Cuellar (2024).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "# 7 Other Topics\n### 7.1 Treatment Effect Heterogeneity and Quantile Treatment Effects\n\nThis article focused exclusively on estimation and inference of the unconditional average treatment effect. Recent work on the analysis of randomized experiments has studied inference for quantile treatment effects: see in particular Zhang and Zheng (2020), Jiang et al. (2021), and Jiang et al. (2023). We may also be interested in heterogeneity of the average treatment effect as a function of the observable characteristics (i.e., the conditional average treatment effect, or CATE). Although estimation and inference on the CATE has been an extremely active area of research in causal inference more broadly (see, for instance, Kennedy, 2023, for an in-depth discussion and further references), to our knowledge almost all of this work maintains the assumption that treatment assignment is independent and identically distributed across individuals; this precludes, for instance, stratified block randomization or pair matching. Some exceptions are Zhang and Ma (2023), who present tests for treatment-covariate interactions and study their properties under general covariate-adaptive stratified randomization schemes, and Ding et al. (2019); Cytrynbaum (2025), who study parametric approximations to the CATE under complete randomization and finely stratified re-randomization designs, respectively.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "### 7.2 Re-randomization\n\nRe-randomization is a method of experimental assignment which, in a similar spirit to stratification, is intended to enforce \"balance\" in the covariate distributions between treatment and control groups. In a standard re-randomization procedure, researchers specify a balance criterion for the covariate values and then repeatedly generate assignments using a completely randomized design until an assignment is found which achieves an acceptable covariate distribution according to the balance criterion. An excellent historical summary is given in Morgan and Rubin (2012). Similarly to stratification, re-randomization leads to an increase in precision relative to complete randomization, and as a result inferences will be unnecessarily conservative unless this is taken into consideration. However, unlike with stratification, where correcting this issue usually amounts to simply modifying the standard errors, corrected inferences for re-randomization are sometimes further complicated by the fact that the limiting distribution of the difference-in-means estimator is not asymptotically normal (although normality can be restored using ex-post linear adjustment: see Li and Ding, 2020; Cytrynbaum, 2025, for details). For an in-depth theoretical discussion of re-randomization, see Li et al. (2018), Li et al. (2020), Zhao and Ding (2021b), Lu et al. (2023), Branson et al. (2024). Wang et al. (2023b) and Cytrynbaum (2025) study how to combine re-randomization with stratification.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 22,
      "text": "# 7.3 Multiple Testing \n\nOur discussion has so far mostly focused on inference about a single parameter of interest, namely the average effect of a binary treatment on an outcome of interest. In many experiments, however, there may be many parameters of interest: there may be multiple treatments, and so it may be of interest to compare the average effect of each of these treatments with the control or with each other; there may be multiple outcomes of interest, and so it may be of interest to examine these effects for each of these outcomes of interest; finally, there may be multiple subgroups of interest (defined by observed, baseline characteristics, as in Section 7.1), and so it may be of interest to examine these effects separately for these different subgroups. In many cases, the methods described previously can be modified in a straightforward fashion for inference about any one of these parameters, but it is often of interest to examine at least some subset of them simultaneously in order to determine, e.g., which of the parameters are equal to zero or not. This naturally leads to a problem of testing multiple null hypotheses simultaneously. If one were to test each of these null hypotheses in the usual way (i.e., ensuring that the probability of Type I error is controlled adequately for each null hypothesis separately), then the probability of some false rejection across all of the null hypotheses may be quite high. A conventional solution to this problem is to require control of the familywise erorr rate - the probability of any false rejection across all of the null hypotheses under consideration. Methods for this problem for experiments in which treatment status is assigned in an i.i.d. fashion across units are developed in List et al. (2019, 2023); see also Lee and Shaikh (2014). A very complicated treatment assignment scheme that arises in the context of a well known experiment in the early childhood education literature is treated in Heckman et al. (2024). These results build upon general results in the multiple testing literature described in Romano and Wolf $(2005,2010)$. In some cases, especially when the number of null hypotheses under consideration is very large, it may be desirable to consider error rates that penalize false rejections less severely, such as the $k$ familywise error rate (defined as the probability of $k$ or more false rejections), the (tail probability of) the false discovery proportion (defined as the fraction of total rejections that are false), or the false discovery rate (defined as the expected value of the false discovery proportion). Some relevant results for the control of such error rates are described in Romano and Wolf (2010); see also Romano and Wolf (2007) and Romano et al. (2008).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 23,
      "text": "### 7.4 Imperfect Compliance and Attrition\n\nEven the most well designed experiments encounter challenges that can complicate subsequent analyses. Two common issues that arise in practice are imperfect compliance and attrition.\n\nImperfect compliance arises when units assigned to the treatment group end up not taking up treatment, and/or units assigned to the control group manage to obtain the treatment. Of course, if researchers are simply interested in the effect of the assignment to treatment as opposed to the true receipt of the treatment (i.e., the intention-to-treat estimand), then everything we have discussed thus far applies directly. However,\n\nif the decision to comply with the treatment is not exogenous but is instead determined by the unobserved characteristics of the units, then the experiment no longer point identifies the causal effect of the true receipt of treatment on the outcome (i.e., the average treatment effect). Most recent work on the analysis of randomized experiments with imperfect compliance has adopted the framework of Imbens and Angrist (1994), where treatment assignment is used an an instrument for the receipt of the treatment and the primary focus is on the so-called \"local\" average treatment effect (i.e., the average treatment effect for those units who comply with the treatment assignment): see, in particular, Ansel et al. (2018), Bugni and Gao (2023), Ren (2023), and Bai et al. (2024a). Alternatively, a similar instrumental variables strategy could be used to partially identify the average treatment effect: see Manski (1990), Balke and Pearl (1997), Bhattacharya et al. (2008), Bhattacharya et al. (2012), Machado et al. (2019), Bugni et al. (2024).\n\nAttrition arises when outcomes are not observed for some subset of the experimental units. This situation could arise, for instance, if researchers lose track of subjects in the experiment. As with imperfect compliance, if the decision for the unit to drop out of the experiment is not exogenous, but is instead determined by their unobserved characteristics, then the experiment no longer point identifies the average treatment effect. Standard resolutions to this issue include modelling the selection process (as in Heckman, 1979) or partial identification (as in Horowitz and Manski, 2000). A textbook treatment on the analysis of randomized experiments with attrition is given in Gerber and Green (2012), and DiNardo et al. (2006) review different methods for dealing with attrition within the context of the Moving to Opportunity (MTO) experiment. Ghanem et al. (2023) develop a test for attrition bias in randomized experiments. Fukumoto (2022) and Bai et al. (2024b) study the problem of attrition in the setting of matched pair experiments, and revisit common recommendations about whether or not to drop pairs with an attrited unit.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 24,
      "text": "# 7.5 Network Experiments and Experiments with Interference \n\nOther than in Section 6, we have so far maintained that each individual's outcome depends on only their own treatment. A rapidly growing literature considers the analysis of experiments in the presence of interference, i.e., that the outcome of a given individual in the experiment may be affected by the treatment statuses of others (see Halloran and Struchiner, 1995, for an early discussion). The simplest example of such a setting is what is called partial interference, where units are grouped into a collection of disjoint clusters, and interference is possible between individuals within the same cluster; Section 6 discussed a special case of such a setting where every unit in the cluster receives the same treatment. Hudgens and Halloran (2008), Basse and Feller (2018), Imai et al. (2021), Vazquez-Bare (2023), Leung (2023), and Liu (2023) study twostage experiments in settings with partial interference, where first clusters are randomly assigned to different treatment proportions, and then individuals within the clusters are assigned to treatment with (marginal) probability according to their cluster's assigned proportion.\n\nMore generally, we could consider settings with complex patterns of interference, for instance, if individuals interact on a large network. Manski (2013) and Aronow and Samii (2017) develop the concept of\n\neffective treatments or exposure mappings, which summarize how a given unit's outcome is affected by the treatments of other units. For example, in a network context, the exposure mapping may dictate that only the treatments of a unit's direct links affect their outcome. The exposure mapping formulation of interference has had a major influence on the subsequent literature: examples include Leung (2020), Viviano (2020), Forastiere et al. (2021), Auerbach and Tabord-Meehan (2021), Munro et al. (2021), Li and Wager (2022), Leung (2022), Gao and Ding (2023), Park and Kang (2023), Viviano et al. (2023), S\u00e4vje (2024). Some recent papers that study general forms of interference without employing the formalism of exposure mappings are Wager and Xu (2021), S\u00e4vje et al. (2021), Hu et al. (2022), and Faridani and Niehaus (2024).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 25,
      "text": "# 7.6 Randomization Inference \n\nAn extremely important topic in the analysis of randomized experiments which we did not cover in this article is the idea of randomization inference. Mechanically, randomization inference generates the null distribution of the test statistic by repeatedly re-assigning treatments to the experimental sample and re-computing the resulting test statistic. If the true value of the test-statistic is too \"large\" relative to this null distribution then the null hypothesis is rejected. The primary strength of these types of tests is that, for appropriate null hypotheses, they can be shown to be finite sample valid.\n\nOne such type of test, going back to the work of Hoeffding (1952), is where the data satisfies some form of group invariance under the null hypothesis, with respect to a group of transformations of the data. For example, consider a completely randomized experiment where we wish to test the null hypothesis\n\n$$\nH_{0}: Y_{i}(1) \\stackrel{d}{=} Y_{i}(0)\n$$\n\nIn this case, a valid group of transformations is given by the transformations which permute the treatment assignments of the individuals. Lehmann and Romano (2022) provide a comprehensive textbook introduction to these types of tests in very general settings (even outside the context of randomized experiments). Romano et al. (2024) in this issue provide a survey of recent advances.\n\nA similar but distinct concept, going back to the work of Fisher (1925) and typically employed in the design-based paradigm, is where the null hypothesis is \"sharp\" in the sense that the test statistic under any counterfactual treatment assignment can be imputed from the data. For example, consider a completely randomized experiment conducted on a finite population of $n$ individuals, then the canonical \"sharp\" null is given by\n\n$$\nH_{0}: Y_{i}(1)=Y_{i}(0) \\text { for all } 1 \\leq i \\leq n\n$$\n\nIn this case, as we permute the treatment assignments of the individuals, we can perfectly impute the value of the test statistic under the null hypothesis. Imbens and Rubin (2015) provide a comprehensive textbook introduction to these \"Fisher-style\" tests. Recent work (particularly in settings with interference) includes Athey et al. (2018), Basse et al. (2019b), Basse et al. (2019a).\n\nIt is important to emphasize that, in both cases, finite-sample validity is only guaranteed for very specific choices of the null hypothesis. In particular, in either case considered above, if we instead consider the null hypothesis that the average treatment effect is equal to zero, then we would have neither the required group invariance property nor a \"sharp\" null which would guarantee finite-sample validity. However, a recent series of papers establishes that the randomization test can be asymptotically valid for these \"weak\" nulls, while retaining its finite-sample validity for the \"sharp\" null, as long as the test-statistic is constructed appropriately. See Chung and Romano (2013), Bugni et al. (2018), Bai et al. (2022), and Bai et al. (2024d), for examples in a super-population context, and Zhao and Ding (2021a), Wu and Ding (2021) for examples in a design-based context.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 26,
      "text": "# 7.7 Policy Learning \n\nOur discussion has focused on the problem of estimation and inference of treatment effect parameters. A recent literature, popularized in econometrics by Manski (2004), seeks to instead use the data in order to directly inform the allocation of treatments over the population as a function of the observable characteristics in order to maximize welfare. Formally, the problem is framed as a statistical decision problem in the framework of Wald (1949) (see Hirano and Porter, 2020, for a comprehensive introduction). Extensive work on this topic, which is now often referred to as \"policy learning\", exists at the intersection of econometrics, statistics, and computer science; important contributions to this literature which also provide comprehensive overviews are Kitagawa and Tetenov (2018) and Athey and Wager (2021). Relevant work in econometrics on this topic (primarily focused on settings where data are obtained from a randomized experiment) includes Dehejia (2005), Stoye (2009), Hirano and Porter (2009), Bhattacharya and Dupas (2012), Viviano (2019), Ananth (2020), Azevedo et al. (2020), Kitagawa and Tetenov (2021), Mbakop and Tabord-Meehan (2021), Sun (2021), Viviano (2022), Kitagawa and Wang (2023), Higbee (2023), Kock et al. (2023).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 27,
      "text": "### 7.8 Response-Adaptive Designs and Bandit Experiments\n\nUntil now, we have assumed that the experimental design being employed by the researcher does not use information from earlier waves of experimentation. In this section, we briefly comment on a rapidly growing literature which considers response-adaptive experimental designs, which are designs that can adapt throughout the experiment as a result of data that have already accrued. Response-adaptive designs have a long history in the analysis of clinical trials; Hu and Rosenberger (2006) and Rosenberger and Lachin (2015) provide textbook introductions.\n\nSome recent work on response-adaptive designs in econometrics and statistics studies procedures to construct feasible analogs of the Neyman allocation, in order to efficiently estimate treatment effect parameters; examples include Hahn et al. (2011), Tabord-Meehan (2023), Blackwell et al. (2022), Li and Owen (2023), Wei et al. (2024). Cai and Rafi (2022) demonstrate that some of these procedures may have poor finite-\n\nsample properties when the data used to estimate the optimal treatment assignment proportions is not sufficiently large.\n\nMuch of the recent work on adaptive designs is related to bandit problems and/or best arm identification. These are essentially policy learning problems (in the sense defined in Section 7.6) where the researcher wishes to choose a policy to maximize welfare, either for the participants in the experiment itself or for the broader population of interest. Bubeck et al. (2012) and Lattimore and Szepesv\u00e1ri (2020) provide comprehensive textbook introductions. Some recent work on this topic includes Russo (2016), Russo and Van Roy (2016), Agrawal and Goyal (2017), Kasy and Sautmann (2021), Adusumilli (2021), Lieber (2022), Kuang and Wager (2023), Kato et al. (2024), Higbee (2024). The analysis of treatment effect parameters in these contexts can be particularly challenging since the experiment was not necessarily designed to facilitate inference. Recent work on inference in adaptive experiments includes Bibaut et al. (2021), Hadad et al. (2021), Zhang et al. (2021), Adusumilli (2023), Chen and Andrews (2023), Hirano and Porter (2023).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 28,
      "text": "# A Additional Details\n## A. 1 A General Comparison of Super-population and Finite-population Variances\n\nIn this section we formalize a claim made in Section 3.1 about the general relationship between the finite population and super-population variance of $\\hat{\\Delta}_{n}$.\n\nWhen presenting the regularity conditions maintained in a finite-population analysis, researchers will often use the motivation that their conditions hold with probability one when the observations are in fact i.i.d. draws from a super-population (see for instance the discussion following Theorem 5 in Li and Ding, 2017). With this in mind, finite-population results about the limiting distribution of $\\hat{\\Delta}_{n}$ can often be conceptualized as first imagining a collection of i.i.d. draws $W^{(N)}=\\left\\{\\left(Y_{i}(1), Y_{i}(0), X_{i}\\right): 1 \\leq i \\leq N\\right\\}$ from some distribution $P$, sampling a subset of $n$ units from $W^{(N)}$, and then deriving the limiting distribution of $\\hat{\\Delta}_{n}$ conditional on $W^{(N)}$ :\n\n$$\n\\sqrt{n}\\left(\\hat{\\Delta}_{n}-\\Delta_{N}\\right) \\mid W^{(N)} \\xrightarrow{d} N\\left(0, \\sigma_{1, \\lambda}^{2}\\left(W^{(\\infty)}\\right)\\right)\n$$\n\nwhere $W^{(\\infty)}=\\left\\{W_{i}: i \\geq 1\\right\\}$ and $n / N \\rightarrow \\lambda \\in[0,1]$. Formally, this conditional convergence can be defined as\n\n$$\n\\sup _{t \\in \\mathbf{R}}\\left|P\\left\\{\\sqrt{n}\\left(\\hat{\\Delta}_{n}-\\Delta_{N}\\right) \\leq t \\mid W^{(N)}\\right\\}-\\Phi\\left(t / \\sigma_{1, \\lambda}\\left(W^{\\infty}\\right)\\right)\\right| \\xrightarrow{P} 0\n$$\n\nWe now relate (22) to the unconditional limiting distribution of $\\hat{\\Delta}_{n}$ is a super-population analysis. First, we note that while in principle $\\sigma_{1, \\lambda}\\left(W^{(\\infty)}\\right)$ could vary with different realizations of the sequence of finite populations $W^{(\\infty)}$, if $W^{(\\infty)}$ in fact arise from a super-population, then $\\sigma_{1}\\left(W^{(\\infty)}\\right)$ will often be the same for (almost all) realizations of $W^{(\\infty)}$. As an example, if we consider complete randomization as defined in Section 2 and if $W^{(\\infty)}$ is drawn from a super-population $P$, then it follows from the strong law of large numbers that for almost every sequence $W^{(\\infty)}$, as $n \\rightarrow \\infty$,\n\n$$\nn \\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid W^{(N)}\\right] \\rightarrow \\frac{\\operatorname{Var}_{P}\\left[Y_{i}(1)\\right]}{\\pi}+\\frac{\\operatorname{Var}_{P}\\left[Y_{i}(0)\\right]}{1-\\pi}-\\lambda \\operatorname{Var}_{P}\\left[Y_{i}(1)-Y_{i}(0)\\right]\n$$\n\nwhere $n / N \\rightarrow \\lambda \\in[0,1]$ (we illustrate a similar property for stratified block randomization at the end of this section). Note that it follows immediately by the central limit theorem that $\\sqrt{n}\\left(\\Delta_{N}-\\Delta\\right) \\xrightarrow{d} N\\left(0, \\lambda \\sigma_{2}^{2}\\right)$, where $\\sigma_{2}^{2}=\\operatorname{Var}_{P}\\left[Y_{i}(1)-Y_{i}(0)\\right]$. Because $\\Delta_{N}$ is a function of $W^{(\\infty)}$, if (22) holds with $\\sigma_{1, \\lambda}^{2}\\left(W^{(\\infty)}\\right) \\equiv \\sigma_{1, \\lambda}^{2}$, then it follows from Lemma S.1.2 in Bai et al. (2022) that the distributions of $\\sqrt{n}\\left(\\hat{\\Delta}_{n}-\\Delta_{N}\\right) \\mid W^{(N)}$ and $\\sqrt{n}\\left(\\Delta_{N}-\\Delta\\right)$ are \"asymptotically independent\" and moreover that\n\n$$\n\\sqrt{n}\\left(\\hat{\\Delta}_{n}-\\Delta\\right) \\xrightarrow{d} N\\left(0, \\sigma_{1, \\lambda}^{2}+\\lambda \\sigma_{2}^{2}\\right)\n$$\n\nIn other words, the super-population variance given in (24) and the finite-population variance given in\n\n(22) differ by $\\lambda \\operatorname{Var}_{P}\\left[Y_{i}(1)-Y_{i}(0)\\right]$ in the limit, which is the fraction of people sampled from the finite population times the variance of the individual level treatment effects. When $n / N \\rightarrow \\lambda=0$, so that we sample a negligible fraction of the finite population in the limit, the difference is zero. Indeed, we can see this for complete randomization by comparing (7) and (23). It thus follows immediately that constructing a consistent estimator of $\\sigma_{1, \\lambda}^{2}+\\lambda \\sigma_{2}^{2}$ delivers a conservative variance estimator of $\\sigma_{1, \\lambda}^{2}$ (in fact, $\\sigma_{1, \\lambda}+\\lambda \\sigma_{2}^{2}$ doesn't vary across $\\lambda$, as can be seen for example in complete randomization.)\n\nAs another example, consider stratified block randomization with discrete $x \\in\\{1,2, \\ldots, \\mathcal{X}\\}$ as defined in Section 4. Here we suppose $n=N$ so that $\\lambda=1$. For $d \\in\\{0,1\\}$ and $x \\in\\{1,2, \\ldots, \\mathcal{X}\\}$, define\n\n$$\n\\begin{aligned}\nn_{d}(x) & =\\sum_{1 \\leq i \\leq n} I\\left\\{D_{i}=d, X_{i}=x\\right\\} \\\\\nn(x) & =n_{1}(x)+n_{0}(x) \\\\\n\\bar{Y}_{n}(d, x) & =\\frac{1}{n_{d}(x)} \\sum_{1 \\leq i \\leq n} Y_{i}(d) I\\left\\{D_{i}=d, X_{i}=x\\right\\} \\\\\n\\Delta_{n}(x) & =\\bar{Y}_{n}(1, x)-\\bar{Y}_{n}(0, x)\n\\end{aligned}\n$$\n\nThe finite population variance of the fully saturated estimator $\\hat{\\Delta}_{n}^{\\text {sat }}$ is\n\n$$\n\\operatorname{Var}\\left[\\hat{\\Delta}_{n}^{\\text {sat }} \\mid W^{(N)}\\right]=\\sum_{x} \\frac{n(x)}{n}\\left(\\frac{S_{1}^{2}(x)}{n_{1}(x)}+\\frac{S_{0}^{2}(x)}{n_{0}(x)}-\\frac{S_{\\Delta}^{2}(x)}{n(x)}\\right)\n$$\n\nwhere $S_{d}^{2}(x)$ and $S_{\\Delta}^{2}(x)$ are the within-stratum counterpart of $S_{d}^{2}$ and $S_{\\Delta}^{2}$ in (2):\n\n$$\n\\begin{aligned}\n& S_{d}^{2}(x)=\\frac{1}{n_{d}(x)-1} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(d)-\\bar{Y}_{n}(d, x)\\right)^{2} I\\left\\{D_{i}=d, X_{i}=x\\right\\} \\\\\n& S_{\\Delta}^{2}(x)=\\frac{1}{n(x)-1} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)-Y_{i}(0)-\\Delta_{n}(x)\\right)^{2} I\\left\\{X_{i}=x\\right\\}\n\\end{aligned}\n$$\n\nIf the finite populations are realizations from a super-population $P$, then it can be shown that with probability one,\n\n$$\nn \\operatorname{Var}\\left[\\hat{\\Delta}_{n}^{\\text {sat }} \\mid W^{(N)}\\right] \\rightarrow \\sum_{x} p(x)\\left(\\frac{\\operatorname{Var}\\left[Y_{i}(1) \\mid X_{i}=x\\right]}{\\pi(x)}+\\frac{\\operatorname{Var}\\left[Y_{i}(0) \\mid X_{i}=x\\right]}{1-\\pi(x)}-\\operatorname{Var}\\left[Y_{i}(1)-Y_{i}(0) \\mid X_{i}=x\\right]\\right)\n$$\n\nwhere $p(x)=P\\left\\{X_{i}=x\\right\\}$.\nOn the other hand, the super-population variance satisfies\n\n$$\n\\begin{aligned}\nn \\operatorname{Var}\\left[\\hat{\\Delta}_{n}^{\\text {sat }} \\mid \\rightarrow \\sum_{x} p(x)\\left(\\frac{\\operatorname{Var}\\left[Y_{i}(1) \\mid X_{i}=x\\right]}{\\pi(x)}\\right.\\right. & \\left.+\\frac{\\operatorname{Var}\\left[Y_{i}(0) \\mid X_{i}=x\\right]}{1-\\pi(x)}\\right. \\\\\n& \\left.+\\left(E\\left[Y_{i}(1)-Y_{i}(0) \\mid X_{i}=x\\right]-E\\left[Y_{i}(1)-Y_{i}(0)\\right]\\right)^{2}\\right)\n\\end{aligned}\n$$\n\nTherefore, once again we have\n\n$$\nn \\operatorname{Var}\\left[\\hat{\\Delta}_{n}^{\\text {sat }}\\right]-n \\operatorname{Var}\\left[\\hat{\\Delta}_{n}^{\\text {sat }} \\mid W^{(N)}\\right] \\rightarrow \\operatorname{Var}\\left[Y_{i}(1)-Y_{i}(0)\\right]\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 29,
      "text": "# B Proofs of claims in main text\n## B. 1 Derivations of $E\\left[\\hat{\\Delta}_{n}\\right]$ and $\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right]$ under complete randomization in the finite population framework\n\nDefine the set $\\mathcal{C}_{n, N}$ to index the random subset of $n$ observations sampled without replacement from the population of size $N$. Consider the following expansion:\n\n$$\n\\hat{\\Delta}_{n}=A_{n}-B_{n}\n$$\n\nwhere\n\n$$\n\\begin{gathered}\nA_{n}=\\frac{1}{n_{1}} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)+Y_{i}(0) \\frac{n_{1}}{n_{0}}\\right) D_{i} \\\\\nB_{n}=\\frac{1}{n_{0}} \\sum_{1 \\leq i \\leq n} Y_{i}(0)\n\\end{gathered}\n$$\n\nIn this re-writing, we have partitioned $\\hat{\\Delta}_{n}$ into two components $A_{n}$ and $B_{n}$. Conditional on $\\mathcal{C}_{n, N}$, and given our definition of $D^{(n)}$, the first component $A_{n}$ can be interpreted as the sample average when sampling $n_{1}$ units without replacement from the finite population of observations $\\left\\{\\left(Y_{i}(1)+Y_{i}(0) \\frac{n_{1}}{n_{0}}\\right): 1 \\leq i \\leq n\\right\\}$, whose population average is given by\n\n$$\n\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)+Y_{i}(0) \\frac{n_{1}}{n_{0}}\\right)\n$$\n\nWhen viewed from this perspective, the treatment indicators $D_{i}$ are now sampling indicators which determine which of the $n_{1}$ units are sampled from our population of $n$ units. Using standard results from the literature on survey sampling (see, for instance, Theorem 2.1 in Cochran, 1977), the sample average is unbiased for the population average:\n\n$$\nE\\left[A_{n} \\mid \\mathcal{C}_{n, N}\\right]=\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)+Y_{i}(0) \\frac{n_{1}}{n_{0}}\\right)\n$$\n\nwhere we emphasize that the expectation is with respect to the sampling indicators $D^{(n)}$, which are indeed the only source of randomness once we condition on $\\mathcal{C}_{n, N}$. Combining this result with our decomposition for $\\hat{\\Delta}_{n}$, we obtain that\n\n$$\n\\begin{aligned}\nE\\left[\\hat{\\Delta}_{n} \\mid \\mathcal{C}_{n, N}\\right] & =E\\left[A_{n} \\mid \\mathcal{C}_{n, N}\\right]-E\\left[B_{n} \\mid \\mathcal{C}_{n, N}\\right] \\\\\n& =\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)+Y_{i}(0) \\frac{n_{1}}{n_{0}}\\right)-B_{n}\n\\end{aligned}\n$$\n\n$$\n=\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)-Y_{i}(0)\\right)\n$$\n\nwhere the second equality used our above derivation of $E\\left[A_{n} \\mid \\mathcal{C}_{n, N}\\right]$ and the fact that $B_{n}$ is non-random conditional on $\\mathcal{C}_{n, N}$, and the third equality follows from some additional algebra. Then by the law of iterated expectations:\n\n$$\nE\\left[\\hat{\\Delta}_{n}\\right]=E\\left[E\\left[\\hat{\\Delta}_{n} \\mid \\mathcal{C}_{n, N}\\right]\\right]=E\\left[\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)-Y_{i}(0)\\right)\\right]=\\Delta_{N}^{\\mathrm{lp}}\n$$\n\nwhere the last equality again follows from Theorem 2.1 in Cochran (1977). Using the same decomposition, we obtain that\n\n$$\n\\begin{aligned}\n\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid \\mathcal{C}_{n, N}\\right] & =\\operatorname{Var}\\left[A_{n} \\mid \\mathcal{C}_{n, N}\\right] \\\\\n& =\\operatorname{Var}\\left[\\frac{1}{n_{1}} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)+Y_{i}(0) \\frac{n_{1}}{n_{0}}\\right) D_{i}\\right] \\\\\n& =\\left(\\frac{1}{n_{1}}-\\frac{1}{n}\\right) \\frac{1}{n-1} \\sum_{1 \\leq i \\leq n}\\left[\\left(Y_{i}(1)+Y_{i}(0) \\frac{n_{1}}{n_{0}}\\right)-\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)+Y_{i}(0) \\frac{n_{1}}{n_{0}}\\right)\\right]^{2} \\\\\n& =\\frac{\\zeta_{1}^{2}}{n_{1}}+\\frac{\\zeta_{0}^{2}}{n_{0}}-\\frac{1}{n(n-1)} \\sum_{1 \\leq i \\leq n}\\left[\\left(Y_{i}(1)-\\bar{Y}_{n}(1)\\right)^{2}+\\left(Y_{i}(0)-\\bar{Y}_{n}(0)\\right)^{2}-2\\left(Y_{i}(1)-\\bar{Y}_{n}(1)\\right)\\left(Y_{i}(0)-\\bar{Y}_{n}(0)\\right)\\right] \\\\\n& =\\frac{\\zeta_{1}^{2}}{n_{1}}+\\frac{\\zeta_{0}^{2}}{n_{0}}-\\frac{\\zeta_{\\Delta}^{2}}{n}\n\\end{aligned}\n$$\n\nwith\n\n$$\n\\begin{aligned}\n& \\zeta_{d}^{2}=\\frac{1}{n-1} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(d)-\\bar{Y}_{n}(d)\\right)^{2} \\\\\n& \\zeta_{\\Delta}^{2}=\\frac{1}{n-1} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)-Y_{i}(0)-\\left(\\bar{Y}_{n}(1)-\\bar{Y}_{n}(0)\\right)\\right)^{2}\n\\end{aligned}\n$$\n\nwhere the first equality follows since $B_{n}$ is non-random conditional on $\\mathcal{C}_{n, N}$, the third equality follows from Theorem 2.2 in Cochran (1977) and the last two equalities from additional algebra. We thus obtain from the law of total variance that\n\n$$\n\\begin{aligned}\n\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right] & =E\\left[\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid \\mathcal{C}_{n, N}\\right]\\right]+\\operatorname{Var}\\left[E\\left[\\hat{\\Delta}_{n} \\mid \\mathcal{C}_{n, N}\\right]\\right] \\\\\n& =E\\left[\\frac{\\zeta_{1}^{2}}{n_{1}}+\\frac{\\zeta_{0}^{2}}{n_{0}}-\\frac{\\zeta_{\\Delta}^{2}}{n}\\right]+\\operatorname{Var}\\left[\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)-Y_{i}(0)\\right)\\right]\n\\end{aligned}\n$$\n\nRepeated applications of Theorems 2.1 and 2.2 in Cochran (1977) along with additional algebra reveals that\n\n$$\nE\\left[\\zeta_{d}^{2}\\right]=\\frac{n}{n-1}\\left(\\left(\\frac{N-1}{N}\\right) S_{d}^{2}-\\left(\\frac{1}{n}-\\frac{1}{N}\\right) S_{d}^{2}\\right)=S_{d}^{2}\n$$\n\nand similarly\n\n$$\nE\\left[\\zeta_{\\Delta}^{2}\\right]=S_{\\Delta}^{2}\n$$\n\nBy another application of Theorem 2.2 in Cochran (1977),\n\n$$\n\\operatorname{Var}\\left[\\frac{1}{n} \\sum_{1 \\leq i \\leq n}\\left(Y_{i}(1)-Y_{i}(0)\\right)\\right]=\\left(\\frac{1}{n}-\\frac{1}{N}\\right) S_{\\Delta}^{2}\n$$\n\nPutting this all together, we obtain\n\n$$\n\\begin{aligned}\n\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right] & =\\frac{S_{1}^{2}}{n_{1}}+\\frac{S_{1}^{2}}{n_{0}}-\\frac{S_{\\Delta}^{2}}{n}+\\left(\\frac{1}{n}-\\frac{1}{N}\\right) S_{\\Delta}^{2} \\\\\n& =\\frac{S_{1}^{2}}{n_{1}}+\\frac{S_{1}^{2}}{n_{0}}-\\frac{S_{\\Delta}^{2}}{N}\n\\end{aligned}\n$$\n\nas desired.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 30,
      "text": "# B. 2 Derivations of (5) and (6) \n\nFor the first claim,\n\n$$\n\\begin{aligned}\nE\\left[\\hat{\\Delta}_{n}\\right] & =E\\left[E\\left[\\hat{\\Delta}_{n} \\mid D^{(n)}\\right]\\right] \\\\\n& =E\\left[\\frac{1}{n_{1}} \\sum_{1 \\leq i \\leq n} E\\left[Y_{i}(1) \\mid D^{(n)}\\right] D_{i}-\\frac{1}{n_{0}} \\sum_{1 \\leq i \\leq n} E\\left[Y_{i}(0) \\mid D^{(n)}\\right]\\left(1-D_{i}\\right)\\right] \\\\\n& =E\\left[E\\left[Y_{i}(1)\\right]\\left(\\frac{1}{n_{1}} \\sum_{1 \\leq i \\leq n} D_{i}\\right)-E\\left[Y_{i}(0)\\right]\\left(\\frac{1}{n_{0}} \\sum_{1 \\leq i \\leq n}\\left(1-D_{i}\\right)\\right)\\right] \\\\\n& =E\\left[Y_{i}(1)-Y_{i}(0)\\right]=\\Delta\n\\end{aligned}\n$$\n\nwhere the first equality follows from the law of iterated expectations, the second from properties of conditional expectations and the definition of the observed outcome $Y_{i}$, the third from the exogeneity of treatment assignment and the fact that the sample is i.i.d., and the final equality by the definition of $n_{1}$ and $n_{0}$.\n\nFor the second claim,\n\n$$\n\\begin{aligned}\n\\operatorname{Var}\\left[\\hat{\\Delta}_{n}\\right] & =E\\left[\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid D^{(n)}\\right]\\right]+\\operatorname{Var}\\left[E\\left[\\hat{\\Delta}_{n} \\mid D^{(n)}\\right]\\right] \\\\\n& =E\\left[\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid D^{(n)}\\right]\\right]+\\operatorname{Var}[\\Delta]\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =E\\left[\\frac{1}{n_{1}^{2}} \\sum_{1 \\leq i \\leq n} \\operatorname{Var}\\left[Y_{i}(1) \\mid D^{(n)}\\right] D_{i}+\\frac{1}{n_{0}^{2}} \\sum_{1 \\leq i \\leq n} \\operatorname{Var}\\left[Y_{i}(0) \\mid D^{(n)}\\right]\\left(1-D_{i}\\right)\\right] \\\\\n& =E\\left[\\operatorname{Var}\\left[Y_{i}(1)\\right] \\frac{1}{n_{1}^{2}} \\sum_{1 \\leq i \\leq n} D_{i}+\\operatorname{Var}\\left[Y_{i}(0)\\right] \\frac{1}{n_{0}^{2}} \\sum_{1 \\leq i \\leq n}\\left(1-D_{i}\\right)\\right] \\\\\n& =\\frac{\\operatorname{Var}\\left[Y_{i}(1)\\right]}{n_{1}}+\\frac{\\operatorname{Var}\\left[Y_{i}(0)\\right]}{n_{0}}\n\\end{aligned}\n$$\n\nwhere the first equality follows from the law of total variance, the second from the derivation of the expectation, the third from the properties of conditional variances and the fact that $D_{i}$ is binary, the fourth by the exogeneity of treatment assignment and the fact that the sample is i.i.d., and the final equality by the definition of $n_{1}$ and $n_{0}$ under complete randomization.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 31,
      "text": "# B. 3 Derivation of (8) \n\nWe have\n\n$$\n\\begin{aligned}\n& E\\left[\\operatorname{Var}\\left[\\hat{\\Delta}_{n} \\mid X^{(n)}, D^{(n)}\\right] \\mid X^{(n)}\\right] \\\\\n& =E\\left[\\frac{4}{n^{2}} \\sum_{1 \\leq i \\leq n}\\left(D_{i} \\operatorname{Var}\\left[Y_{i}(1) \\mid X_{i}\\right]+\\left(1-D_{i}\\right) \\operatorname{Var}\\left[Y_{i}(0) \\mid X_{i}\\right]\\right) \\mid X^{(n)}\\right] \\\\\n& =\\frac{2}{n^{2}} \\sum_{1 \\leq i \\leq n}\\left(\\operatorname{Var}\\left[Y_{i}(1) \\mid X_{i}\\right]+\\operatorname{Var}\\left[Y_{i}(0) \\mid X_{i}\\right]\\right)\n\\end{aligned}\n$$\n\nwhere in the first equality we use the fact that the potential outcomes are independent across units conditional on $X^{(n)}$ and $D^{(n)}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 32,
      "text": "# References \n\nAdusumilli, K. (2021). Risk and optimal policies in bandit experiments. arXiv preprint arXiv:2112.06363.\nAdusumilli, K. (2023). Optimal tests following sequential experiments. arXiv preprint arXiv:2305.00403.\nAgrawal, S. and Goyal, N. (2017). Near-optimal regret bounds for thompson sampling. Journal of the $A C M(J A C M), 641-24$.\n\nAnanth, A. (2020). Optimal treatment assignment rules on networked populations. Tech. rep., working paper.\n\nAngrist, J. and Lavy, V. (2009). The Effects of High Stakes High School Achievement Awards: Evidence from a Randomized Trial. American Economic Review, 99 1384-1414.\n\nAngrist, J. D. and Pischke, J.-S. (2009). Mostly harmless econometrics: An empiricist's companion. Princeton university press.\n\nAnsel, J., Hong, H. and Jessie Li, A. (2018). Ols and 2sls in randomized and conditionally randomized experiments. Jahrb\u00fccher f\u00fcr National\u00f6konomie und Statistik, 238 243-293.\n\nArmstrong, T. B. (2022). Asymptotic Efficiency Bounds for a Class of Experimental Designs. ArXiv:2205.02726 [stat], URL http://arxiv.org/abs/2205.02726.\n\nAronow, P. M., Green, D. P. and Lee, D. K. K. (2014). Sharp Bounds on the Variance in Randomized Experiments. The Annals of Statistics, 42 850-871.\n\nAronow, P. M. and Middleton, J. A. (2013). A class of unbiased estimators of the average treatment effect in randomized experiments. Journal of Causal Inference, 1 135-154.\n\nAronow, P. M. and Samii, C. (2017). Estimating average causal effects under general interference, with application to a social network experiment.\n\nAthey, S., Eckles, D. and Imbens, G. W. (2018). Exact p-values for network interference. Journal of the American Statistical Association, 113 230-240.\n\nAthey, S. and Imbens, G. W. (2017). The Econometrics of Randomized Experiments. In Handbook of Economic Field Experiments, vol. 1. Elsevier, 73-140.\n\nAthey, S. and Wager, S. (2021). Policy learning with observational data. Econometrica, 89 133-161.\nAtkinson, A., Donev, A. and Tobias, R. (2007). Optimum experimental designs, with SAS, vol. 34. OUP Oxford.\n\nAuerbach, E. and Tabord-Meehan, M. (2021). The local approach to causal inference under network interference. arXiv preprint arXiv:2105.03810.\n\nAzevedo, E. M., Deng, A., Montiel Olea, J. L., Rao, J. and Weyl, E. G. (2020). A/b testing with fat tails. Journal of Political Economy, 128 4614-000.\n\nBai, Y. (2022). Optimality of Matched-Pair Designs in Randomized Controlled Trials. American Economic Review, 112 3911-3940.\n\nBai, Y. (2023). Why randomize? Minimax optimality under permutation invariance. Journal of Econometrics, $232565-575$.\n\nBai, Y., Guo, H., Shaikh, A. M. and Tabord-Meehan, M. (2024a). Inference in Experiments with Matched Pairs and Imperfect Compliance. Publisher: ASA Website .eprint: https://doi.org/10.1080/07350015.2024.2416972, URL https://doi.org/10.1080/07350015. 2024.2416972 .\n\nBai, Y., Hsieh, M. H., Liu, J. and Tabord-Meehan, M. (2024b). Revisiting the analysis of matched-pair and stratified experiments in the presence of attrition. Journal of Applied Econometrics, 39 256-268.\n\nBai, Y., Huang, X., Romano, J. P., Shaikh, A. M. and Tabord-Meehan, M. (2025a). A New Design-Based Variance Estimator for Finely Stratified Experiments. ArXiv:2503.10851 [econ], URL http: //arxiv.org/abs/2503.10851.\n\nBai, Y., Jiang, L., Romano, J. P., Shaikh, A. M. and Zhang, Y. (2024c). Covariate adjustment in experiments with matched pairs. Journal of Econometrics, 241105740.\n\nBai, Y., Liu, J., Shaikh, A. M. and Tabord-Meehan, M. (2024d). Inference in cluster randomized trials with matched pairs. Journal of Econometrics, 245105873.\n\nBai, Y., Liu, J., Shaikh, A. M. and Tabord-Meehan, M. (2025b). On the Efficiency of Finely Stratified Experiments. ArXiv:2307.15181 [econ], URL http://arxiv.org/abs/2307.15181.\n\nBai, Y., Liu, J. and Tabord-Meehan, M. (2024e). Inference for Matched Tuples and Fully Blocked Factorial Designs. Quantitative Economics, 15 279-330.\n\nBai, Y., Romano, J. P. and Shaikh, A. M. (2022). Inference in Experiments With Matched Pairs. Journal of the American Statistical Association, 117 1726-1737.\n\nBalke, A. and Pearl, J. (1997). Bounds on Treatment Effects from Studies with Imperfect Compliance. Journal of the American Statistical Association, 92 1171-1176.\n\nBanerjee, A., Duflo, E., Glennerster, R. and Kinnan, C. (2015). The Miracle of Microfinance? Evidence from a Randomized Evaluation. American Economic Journal: Applied Economics, 7 22-53.\n\nBasse, G., Ding, P., Feller, A. and Toulis, P. (2019a). Randomization tests for peer effects in group formation experiments. arXiv preprint arXiv:1904.02308.\n\nBasse, G. and Feller, A. (2018). Analyzing two-stage experiments in the presence of interference. Journal of the American Statistical Association, 113 41-55.\n\nBasse, G. W., Feller, A. and Toulis, P. (2019b). Randomization tests of causal effects under interference. Biometrika, 106 487-494.\n\nBerry, J., Karlan, D. and Pradhan, M. (2018). The Impact of Financial Education for Youth in Ghana. World Development, 102 71-89.\n\nBhattacharya, D. and Dupas, P. (2012). Inferring welfare maximizing treatment assignment under budget constraints. Journal of Econometrics, 167 168-196.\n\nBhattacharya, J., Shaikh, A. M. and Vytlacil, E. (2008). Treatment effect bounds under monotonicity assumptions: an application to swan-ganz catheterization. American Economic Review, 98 351-356.\n\nBhattacharya, J., Shaikh, A. M. and Vytlacil, E. (2012). Treatment effect bounds: An application to swan-ganz catheterization. Journal of Econometrics, 168 223-243.\n\nBibaut, A., Dimakopoulou, M., Kallus, N., Chambaz, A. and van Der Laan, M. (2021). Post-contextual-bandit inference. Advances in neural information processing systems, 34 28548-28559.\n\nBlackwell, D. and Girshick, M. A. (1954). Theory of games and statistical decisions. John Wiley and Sons, Inc., New York; Chapman and Hall, Ltd., London.\n\nBlackwell, M., Pashley, N. E. and Valentino, D. (2022). Batch adaptive designs to improve efficiency in social science experiments.\n\nBranson, Z., Li, X. and Ding, P. (2024). Power and sample size calculations for rerandomization. Biometrika, 111 355-363.\n\nBruhn, M., Le\u00c3o, L. d. S., Legovini, A., Marchetti, R. and Zia, B. (2016). The Impact of High School Financial Education: Evidence from a Large-Scale Evaluation in Brazil. American Economic Journal: Applied Economics, 8 256-295.\n\nBruhn, M. and McKenzie, D. (2009). In Pursuit of Balance: Randomization in Practice in Development Field Experiments. American Economic Journal: Applied Economics, 1 200-232.\n\nBubeck, S., Cesa-Bianchi, N. et al. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends\u00ae in Machine Learning, 5 1-122.\n\nBugni, F., Canay, I., Shaikh, A. and Tabord-Meehan, M. (2022). Inference for Cluster Randomized Experiments with Non-ignorable Cluster Sizes. ArXiv:2204.08356 [econ, stat], URL http://arxiv.org/ abs/2204.08356.\n\nBugni, F., Gao, M., Obradovi\u0107, F. and Velez, A. (2024). Identification and inference on treatment effects under covariate-adaptive randomization and imperfect compliance. Working Paper.\n\nBugni, F. A., Canay, I. A. and Shaikh, A. M. (2018). Inference Under Covariate-Adaptive Randomization. Journal of the American Statistical Association, 113 1784-1796.\n\nBugni, F. A., Canay, I. A. and Shaikh, A. M. (2019). Inference under covariate-adaptive randomization with multiple treatments. Quantitative Economics, 10 1747-1785.\n\nBugni, F. A. and Gao, M. (2023). Inference under covariate-adaptive randomization with imperfect compliance. Journal of Econometrics, 237105497.\n\nCai, Y. and Rafi, A. (2022). On the Performance of the Neyman Allocation with Small Pilots. ArXiv:2206.04643 [econ], URL http://arxiv.org/abs/2206.04643.\n\nCallen, M., Gulzar, S., Hasanain, A., Khan, M. Y. and Rezaee, A. (2020). Data and policy decisions: Experimental evidence from Pakistan. Journal of Development Economics, 146102523.\n\nChang, H. (2023). Design-based estimation theory for complex experiments. arXiv preprint arXiv:2311.06891.\n\nChen, J. and Andrews, I. (2023). Optimal conditional inference in adaptive experiments. arXiv preprint arXiv:2309.12162.\n\nChiang, H. D., Matsushita, Y. and Otsu, T. (2023). Regression adjustment in randomized controlled trials with many covariates. arXiv preprint arXiv:2302.00469.\n\nChung, E. and Romano, J. P. (2013). Exact and asymptotically robust permutation tests. Annals of Statistics, 41 484-507.\n\nCochran, W. G. (1977). Sampling techniques. 3rd ed. Wiley Series in Probability and Mathematical Statistics, John Wiley \\& Sons, New York-London-Sydney.\n\nCox, D. and Reid, N. (2000). The Theory of the Design of Experiments. Chapman \\& Hall/CRC Monographs on Statistics \\& Applied Probability, CRC Press.\n\nCr\u00e9pon, B., Devoto, F., Duflo, E. and Parient\u00e9, W. (2015). Estimating the Impact of Microcredit on Those Who Take It Up: Evidence from a Randomized Experiment in Morocco. American Economic Journal: Applied Economics, 7 123-150.\n\nCytrynbaum, M. (2023a). Covariate Adjustment in Stratified Experiments. ArXiv:2302.03687 [econ, stat], URL http://arxiv.org/abs/2302.03687.\n\nCytrynbaum, M. (2023b). Optimal Stratification of Survey Experiments. ArXiv:2111.08157 [econ, math, stat], URL http://arxiv.org/abs/2111.08157.\n\nCytrynbaum, M. (2025). Finely Stratified Rerandomization Designs. ArXiv:2407.03279 [econ], URL http://arxiv.org/abs/2407.03279.\n\nde Chaisemartin, C. and Ramirez-Cuellar, J. (2024). At What Level Should One Cluster Standard Errors in Paired and Small-Strata Experiments? American Economic Journal: Applied Economics, 16 $193-212$.\n\nDeheJIA, R. H. (2005). Program evaluation as a decision problem. Journal of Econometrics, 125 141-173.\nDiNardo, J., McCrary, J. and Sanbonmatsu, L. (2006). Constructive proposals for dealing with attrition: An empirical example. Tech. rep., Working paper, University of Michigan.\n\nDing, P., Avi, F. and Luke, M. (2019). Decomposing Treatment Effect Variation. Journal of the American Statistical Association, 114 304-317.\n\nDizon-Ross, R. (2019). Parents' Beliefs about Their Children's Academic Ability: Implications for Educational Investments. American Economic Review, 109 2728-2765.\n\nDonner, A. and Klar, N. (2000). Design and analysis of cluster randomization trials in health research. London: Arnold.\n\nDuflo, E., Dupas, P. and Kremer, M. (2015). Education, HIV, and Early Fertility: Experimental Evidence from Kenya. American Economic Review, 105 2757-2797.\n\nDuflo, E., Glennerster, R. and Kremer, M. (2007). Using Randomization in Development Economics Research: A Toolkit. In Handbook of Development Economics, vol. 4. Elsevier, 3895-3962.\n\nFaridani, S. and Niehaus, P. (2024). Linear estimation of global average treatment effects. Tech. rep., National Bureau of Economic Research.\n\nFisher, R. A. (1925). Theory of Statistical Estimation. Mathematical Proceedings of the Cambridge Philosophical Society, 22 700-725.\n\nFISHER, R. A. (1935). The design of experiments. The design of experiments.\nForastiere, L., Airoldi, E. M. and Mealli, F. (2021). Identification and estimation of treatment and interference effects in observational studies on networks. Journal of the American Statistical Association, 116 901-918.\n\nFreedman, D. A. (2008). On regression adjustments to experimental data. Advances in Applied Mathematics, 40 180-193.\n\nFukumoto, K. (2022). Nonignorable attrition in pairwise randomized experiments. Political Analysis, 30 $132-141$.\n\nGao, M. and Ding, P. (2023). Causal inference in network experiments: regression-based analysis and design-based properties. arXiv preprint arXiv:2309.07476.\n\nGerber, A. S. and Green, D. P. (2012). Field Experiments: Design, Analysis and Interpretation. W. W. Norton \\& Company, New York, NY.\n\nGhanem, D., Hirshleifer, S. and Ortiz-Beccera, K. (2023). Testing attrition bias in field experiments. Journal of Human Resources.\n\nGlennerster, R. and Takavarasha, K. (2013). Running Randomized Evaluations: A Practical Guide. Princeton University Press.\n\nHadad, V., Hirshberg, D. A., Zhan, R., Wager, S. and Athey, S. (2021). Confidence intervals for policy evaluation in adaptive experiments. Proceedings of the national academy of sciences, 118 e2014602118.\n\nHahn, J., Hirano, K. and Karlan, D. (2011). Adaptive Experimental Design Using the Propensity Score. Journal of Business $\\mathcal{E}$ Economic Statistics, 29 96-108.\n\nHalloran, M. E. and Struchiner, C. J. (1995). Causal inference in infectious diseases. Epidemiology $142-151$.\n\nHansen, M. H., Hurwitz, W. N. and Madow, W. G. (1953). Sample survey methods and theory. vol. i. methods and applications.\n\nHarshaw, C., Middleton, J. A. and S\u0306\u0306vie, F. (2021). Optimized variance estimation under interference and complex experimental designs. arXiv preprint arXiv:2112.01709.\n\nHeckman, J., Pinto, R. and Shaikh, A. M. (2024). Dealing with imperfect randomization: Inference for the highscope perry preschool program. Journal of Econometrics 105683.\n\nHeckman, J. J. (1979). Sample selection bias as a specification error. Econometrica: Journal of the econometric society $153-161$.\n\nHigbee, S. (2023). Policy learning with new treatments. arXiv preprint arXiv:2210.04703.\nHigbee, S. D. (2024). Experimental design for policy choice.\nHirano, K., Imbens, G. W. and Ridder, G. (2003). Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score. Econometrica, 71 1161-1189.\n\nHirano, K. and Porter, J. R. (2009). Asymptotics for statistical treatment rules. Econometrica, 77 $1683-1701$.\n\nHirano, K. and Porter, J. R. (2020). Asymptotic analysis of statistical decision rules in econometrics. In Handbook of econometrics, vol. 7. Elsevier, 283-354.\n\nHirano, K. and Porter, J. R. (2023). Asymptotic representations for sequential decisions, adaptive experiments, and batched bandits. arXiv preprint arXiv:2302.03117.\n\nHoeffding, W. (1952). The large-sample power of tests based on permutations of observations. In The Collected Works of Wassily Hoeffding. Springer, 247-271.\n\nHorowitz, J. L. and Manski, C. F. (2000). Nonparametric analysis of randomized experiments with missing covariate and outcome data. Journal of the American statistical Association, 95 77-84.\n\nHu, F. and Rosenberger, W. F. (2006). The theory of response-adaptive randomization in clinical trials. John Wiley \\& Sons.\n\nHu, Y., Li, S. and Wager, S. (2022). Average direct and indirect causal effects under interference. Biometrika, 109 1165-1172.\n\nHudgens, M. G. and Halloran, M. E. (2008). Toward causal inference with interference. Journal of the American Statistical Association, 103 832-842.\n\nImai, K., Jiang, Z. and Malani, A. (2021). Causal inference with interference and noncompliance in two-stage randomized experiments. Journal of the American Statistical Association, 116 632-644.\n\nImai, K., King, G. and Nall, C. (2009). The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation. Statistical Science, 24 29-53.\n\nImbens, G. W. and Angrist, J. D. (1994). Identification and Estimation of Local Average Treatment Effects. Econometrica, 62 467-475.\n\nImbens, G. W. and Rubin, D. B. (2015). Causal Inference in Statistics, Social, and Biomedical Sciences. Cambridge University Press.\n\nJiang, L., Liu, X., Phillips, P. C. and Zhang, Y. (2021). Bootstrap Inference for Quantile Treatment Effects in Randomized Experiments with Matched Pairs. The Review of Economics and Statistics 1-47.\n\nJiang, L., Phillips, P. C., Tao, Y. and Zhang, Y. (2023). Regression-adjusted estimation of quantile treatment effects under covariate-adaptive randomizations. Journal of Econometrics, 234 758-776.\n\nKallus, N. (2018). Optimal a priori balance in the design of controlled experiments. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80 85-112.\n\nKallus, N. (2021). On the optimality of randomization in experimental design: How to randomize for minimax variance and design-based inference. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 83 404-409.\n\nKarlan, D. and Appel, J. (2017). Failing in the field: What we can learn when field research goes wrong. Princeton University Press.\n\nKarlan, D. and Zinman, J. (2010). Expanding Credit Access: Using Randomized Supply Decisions to Estimate the Impacts. The Review of Financial Studies, 23 433-464.\n\nKasy, M. and Sautmann, A. (2021). Adaptive treatment assignment in experiments for policy choice. Econometrica, 89 113-132.\n\nKato, M., Okumura, K., Ishihara, T. and Kitagawa, T. (2024). Contextual fixed-budget best arm identification: Adaptive experimental design with policy learning. arXiv preprint arXiv:2401.03756.\n\nKennedy, E. H. (2023). Towards optimal doubly robust estimation of heterogeneous causal effects. Electronic Journal of Statistics, 17 3008-3049.\n\nKiefer, J. (1959). Optimum experimental designs. Journal of the Royal Statistical Society: Series B (Methodological), 21 272-304.\n\nKitagawa, T. and Tetenov, A. (2018). Who should be treated? empirical welfare maximization methods for treatment choice. Econometrica, 86 591-616.\n\nKitagawa, T. and Tetenov, A. (2021). Equality-minded treatment choice. Journal of Business $\\mathcal{E}$ Economic Statistics, 39 561-574.\n\nKitagawa, T. and Wang, G. (2023). Who should get vaccinated? individualized allocation of vaccines over sir network. Journal of Econometrics, 232 109-131.\n\nKlar, N. and Donner, A. (1997). The merits of matching in community intervention trials: a cautionary tale. Statistics in medicine, 16 1753-1764.\n\n\u041a\u043e\u0441\u043a, A. B., Preinerstorfer, D. and Veliyev, B. (2023). Treatment recommendation with distributional targets. Journal of Econometrics, 234 624-646.\n\nKuang, X. and Wager, S. (2023). Weak signal asymptotics for sequentially randomized experiments. Management Science.\n\nLattimore, T. and Szepesv\u00e1ri, C. (2020). Bandit algorithms. Cambridge University Press.\nLee, S. and Shaikh, A. M. (2014). Multiple Testing and Heterogeneous Treatment Effects: Re-Evaluating the Effect of Progresa on School Enrollment. Journal of Applied Econometrics, 29 612-626.\n\nLehmann, E. and Romano, J. P. (2022). Testing Statistical Hypotheses. Springer Texts in Statistics, Springer International Publishing, Cham.\n\nLeung, M. P. (2020). Treatment and spillover effects under network interference. Review of Economics and Statistics, 102 368-380.\n\nLeung, M. P. (2022). Causal inference under approximate neighborhood interference. Econometrica, 90 267-293.\n\nLeung, M. P. (2023). Design of cluster-randomized trials with cross-cluster interference. arXiv preprint arXiv:2310.18836.\n\nLi, H. H. and Owen, A. B. (2023). Double machine learning and design in batch adaptive experiments. arXiv preprint arXiv:2309.15297.\n\nLi, K.-C. (1983). Minimaxity for Randomized Designs: Some General Results. Annals of Statistics, 11 $225-239$.\n\nLi, S. and Wager, S. (2022). Random graph asymptotics for treatment effect estimation under network interference. The Annals of Statistics, 50 2334-2358.\n\nLi, X. and Ding, P. (2017). General forms of finite population central limit theorems with applications to causal inference. Journal of the American Statistical Association, 112 1759-1769.\n\nLi, X. and Ding, P. (2020). Rerandomization and Regression Adjustment. Journal of the Royal Statistical Society Series B: Statistical Methodology, 82 241-268.\n\nLi, X., Ding, P. and Rubin, D. B. (2018). Asymptotic theory of rerandomization in treatment-control experiments. Proceedings of the National Academy of Sciences, 115 9157-9162.\n\nLi, X., Ding, P. and Rubin, D. B. (2020). Rerandomization in 2 k factorial experiments. The Annals of Statistics, $4843-63$.\n\nLieber, J. (2022). Estimating concentration parameters for bandit algorithms.\nLin, W. (2013). Agnostic notes on regression adjustments to experimental data: Reexamining Freedman's critique. Annals of Applied Statistics, 7 295-318.\n\nList, J. A. (2023). The voltage effect. Business Economics, 58 3-8.\nList, J. A., Shaikh, A. M. and Vayalinkal, A. (2023). Multiple testing with covariate adjustment in experimental economics. Journal of Applied Econometrics, 38 920-939.\n\nList, J. A., Shaikh, A. M. and Xu, Y. (2019). Multiple hypothesis testing in experimental economics. Experimental Economics, 22 773-793.\n\nLiu, H., Ren, J. and Yang, Y. (2021). Randomization-based joint central limit theorem and efficient covariate adjustment in stratified $2^{K}$ factorial experiments. arXiv preprint arXiv:2103.04050.\n\nLiU, J. (2023). Inference for two-stage experiments under covariate-adaptive randomization. arXiv preprint arXiv:2301.09016.\n\nLu, X., Liu, T., Liu, H. and Ding, P. (2023). Design-based theory for cluster rerandomization. Biometrika, $110467-483$.\n\nMa, W., Tu, F. and Liu, H. (2022). Regression analysis for covariate-adaptive randomization: A robust and efficient inference perspective. Statistics in Medicine, 41 5645-5661.\n\nMachado, C., Shaikh, A. M. and Vytlacil, E. J. (2019). Instrumental variables and the sign of the average treatment effect. Journal of Econometrics, 212 522-555.\n\nManski, C. F. (1990). Nonparametric Bounds on Treatment Effects. The American Economic Review, 80 $319-323$.\n\nManski, C. F. (2004). Statistical Treatment Rules for Heterogeneous Populations. Econometrica, 72 $1221-1246$.\n\nManski, C. F. (2013). Identification of treatment response with social interactions. The Econometrics Journal, 16 S1-S23.\n\nMbakop, E. and Tabord-Meehan, M. (2021). Model Selection for Treatment Choice: Penalized Welfare Maximization. Econometrica, 89 825-848.\n\nMiddleton, J. A. and Aronow, P. M. (2015). Unbiased estimation of the average treatment effect in cluster-randomized experiments. Statistics, Politics and Policy, 6 39-75.\n\nMorgan, K. L. and Rubin, D. B. (2012). Rerandomization to improve covariate balance in experiments. Annals of Statistics, 40 1263-1282.\n\nMunro, E., Wager, S. and Xu, K. (2021). Treatment effects in market equilibrium. arXiv preprint arXiv:2109.11647.\n\nMuralidharan, K. and Niehaus, P. (2017). Experimentation at Scale. Journal of Economic Perspectives, 31 103-124.\n\nNegi, A. and Wooldridge, J. M. (2021). Revisiting regression adjustment in experiments with heterogeneous treatment effects. Econometric Reviews, 40 504-534.\n\nNeyman, J. (1923). On the Application of Probability Theory to Agricultural Experiments. Essay on Principles. Section 9. Statistical Science, 5 465-472.\n\nPark, C. and Kang, H. (2023). Assumption-lean analysis of cluster randomized trials in infectious diseases for intent-to-treat effects and network effects. Journal of the American Statistical Association, 118 11951206 .\n\nPashley, N. E. and Miratrix, L. W. (2021). Insights on variance estimation for blocked and matched pairs designs. Journal of Educational and Behavioral Statistics, 46 271-296.\n\nPukelsheim, F. (2006). Optimal Design of Experiments. Classics in Applied Mathematics, Society for Industrial and Applied Mathematics.\n\nRafi, A. (2023). Efficient Semiparametric Estimation of Average Treatment Effects Under Covariate Adaptive Randomization. ArXiv:2305.08340 [econ] version: 1, URL http://arxiv.org/abs/2305.08340.\n\nReichardt, C. S. and Gollob, H. F. (1999). Justifying the use and increasing the power of at test for a randomized experiment with a convenience sample. Psychological methods, 4117.\n\nREn, J. (2023). Model-assisted complier average treatment effect estimates in randomized experiments with noncompliance. Journal of Business $\\mathcal{E}$ Economic Statistics 1-12.\n\nRobins, J. M., RotnitZky, A. and Zhao, L. P. (1995). Analysis of Semiparametric Regression Models for Repeated Outcomes in the Presence of Missing Data. Journal of the American Statistical Association, 90 106-121.\n\nRomano, J. P., Ritzwoller, D. and Shaikh, A. M. (2024). Randomization inference: Theory and applications.\n\nRomano, J. P., Shaikh, A. M. and Wolf, M. (2008). Formalized data snooping based on generalized error rates. Econometric Theory, 24 404-447.\n\nRomano, J. P. and Wolf, M. (2005). Exact and approximate stepdown methods for multiple hypothesis testing. Journal of the American Statistical Association, 100 94-108.\n\nRomano, J. P. and Wolf, M. (2007). Control of generalized error rates in multiple testing.\nRomano, J. P. and Wolf, M. (2010). Balanced control of generalized error rates. Annals of Statistics, 38 $598-633$.\n\nRomero, M., Sandefur, J. and Sandholtz, W. A. (2020). Outsourcing Education: Experimental Evidence from Liberia. American Economic Review, 110 364-400.\n\nRosenberger, W. F. and Lachin, J. M. (2015). Randomization in Clinical Trials: Theory and Practice. John Wiley \\& Sons.\n\nRusso, D. (2016). Simple bayesian algorithms for best arm identification. In Conference on Learning Theory. PMLR, 1417-1418.\n\nRusso, D. and Van Roy, B. (2016). An information-theoretic analysis of thompson sampling. Journal of Machine Learning Research, 17 1-30.\n\nSavage, L. J. (1951). The Theory of Statistical Decision. Journal of the American Statistical Association, 46 55-67.\n\nS\u00e4VJe, F. (2024). Causal inference with misspecified exposure mappings: separating definitions and assumptions. Biometrika, 111 1-15.\n\nS\u00e4VJe, F., Aronow, P. and Hudgens, M. (2021). Average treatment effects in the presence of unknown interference. Annals of statistics, 49673.\n\nSchochet, P. Z. (2013). Estimators for clustered education rcts using the neyman model for causal inference. Journal of Educational and Behavioral Statistics, 38 219-238.\n\nSchochet, P. Z., Pashley, N. E., Miratrix, L. W. and Kautz, T. (2021). Design-based ratio estimators and central limit theorems for clustered, blocked rcts. Journal of the American Statistical Association 1-12.\n\nStoye, J. (2009). Minimax regret treatment choice with finite samples. Journal of Econometrics, 151 $70-81$.\n\nSu, F. and Ding, P. (2021). Model-assisted analyses of cluster-randomized experiments. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83 994-1015.\n\nSun, L. (2021). Empirical welfare maximization with constraints. arXiv preprint arXiv:2103.15298, 2.\n\nTabord-Meehan, M. (2023). Stratification Trees for Adaptive Randomisation in Randomised Controlled Trials. The Review of Economic Studies, 90 2646-2673.\n\nTrifonov, J., Bai, Y., Shaikh, A. and Tabord-Meehan, M. (2025). sreg: Stratified Randomized Experiments. URL https://cran.r-project.org/web/packages/sreg/index.html.\n\nTsiatis, A. A., Davidian, M., Zhang, M. and Lu, X. (2008). Covariate adjustment for two-sample treatment comparisons in randomized clinical trials: a principled yet flexible approach. Statistics in Medicine, 27 4658-4677.\n\nTu, F., Ma, W. and Liu, H. (2023). A unified framework for covariate adjustment under stratified randomization. arXiv preprint arXiv:2312.01266.\n\nVazquez-Bare, G. (2023). Identification and estimation of spillover effects in randomized experiments. Journal of Econometrics, 237105237.\n\nViviano, D. (2019). Policy targeting under network interference. arXiv preprint arXiv:1906.10258.\n\nViviano, D. (2020). Experimental design under network interference. arXiv preprint arXiv:2003.08421.\n\nViviano, D. (2022). Policy design in experiments with unknown interference. Tech. rep., working paper.\n\nViviano, D., Lei, L., Imbens, G., Karrer, B., Schrijvers, O. and Shi, L. (2023). Causal clustering: design of cluster experiments under network interference. arXiv preprint arXiv:2310.14983.\n\nWAGER, S. and Xu, K. (2021). Experimenting in equilibrium. Management Science, 67 6694-6715.\n\nWald, A. (1949). Statistical decision functions. The Annals of Mathematical Statistics 165-205.\n\nWang, B., Park, C., Small, D. S. and Li, F. (2024). Model-robust and efficient covariate adjustment for cluster-randomized experiments. Journal of the American Statistical Association 1-13.\n\nWang, B., Susukida, R., MojtabaI, R., Amin-Esmaeili, M. and Rosenblum, M. (2023a). Modelrobust inference for clinical trials that improve precision by stratified randomization and covariate adjustment. Journal of the American Statistical Association, 118 1152-1163.\n\nWang, X., Wang, T. and Liu, H. (2023b). Rerandomization in stratified randomized experiments. Journal of the American Statistical Association, 118 1295-1304.\n\nWei, W., Ma, X. and Wang, J. (2024). Fair adaptive experiments. Advances in Neural Information Processing Systems, 36.\n\nWu, C. J. and Hamada, M. S. (2011). Experiments: planning, analysis, and optimization. John Wiley \\& Sons.\n\nWu, E. and Gagnon-Bartsch, J. A. (2018). The loop estimator: Adjusting for covariates in randomized experiments. Evaluation review, 42 458-488.\n\nWu, J. and Ding, P. (2021). Randomization tests for weak null hypotheses in randomized experiments. Journal of the American Statistical Association, 116 1898-1913.\n\nZhang, K., Janson, L. and Murphy, S. (2021). Statistical inference with m-estimators on adaptively collected data. Advances in neural information processing systems, 34 7460-7471.\n\nZhang, L. and Ma, W. (2023). Interaction tests with covariate-adaptive randomization. arXiv preprint arXiv:2311.17445.\n\nZhang, Y. and Zheng, X. (2020). Quantile treatment effects and bootstrap inference under covariateadaptive randomization. Quantitative Economics, 11 957-982.\n\nZhao, A. and Ding, P. (2021a). Covariate-adjusted Fisher randomization tests for the average treatment effect. Journal of Econometrics, 225 278-294.\n\nZhao, A. and Ding, P. (2021b). No star is good news: A unified look at rerandomization based on $p$-values from covariate balance tests. arXiv preprint arXiv:2112.10545.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2405.03910v2",
  "authors": [
    "Yuehao Bai",
    "Azeem M. Shaikh",
    "Max Tabord-Meehan"
  ],
  "categories": [
    "econ.EM",
    "stat.ME"
  ],
  "abstract": "The past two decades have witnessed a surge of new research in the analysis\nof randomized experiments. The emergence of this literature may seem surprising\ngiven the widespread use and long history of experiments as the \"gold standard\"\nin program evaluation, but this body of work has revealed many subtle aspects\nof randomized experiments that may have been previously unappreciated. This\narticle provides an overview of some of these topics, primarily focused on\nstratification, regression adjustment, and cluster randomization.",
  "updated": "2025-04-01T19:11:29Z",
  "published": "2024-05-06T23:59:24Z"
}