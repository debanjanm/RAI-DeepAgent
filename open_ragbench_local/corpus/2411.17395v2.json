{
  "title": "Asymptotics for estimating a diverging number of parameters -- with and\n  without sparsity",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nWe consider high-dimensional estimation problems where the number of parameters diverges with the sample size. General conditions are established for consistency, uniqueness, and asymptotic normality in both unpenalized and penalized estimation settings. The conditions are weak and accommodate a broad class of estimation problems, including ones with non-convex and group structured penalties. The wide applicability of the results is illustrated through diverse examples, including generalized linear models, multi-sample inference, and stepwise estimation procedures.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "## 1 Introduction\n\nIn modern applications, statisticians are facing increasingly complex and high-dimensional problems. Many data sets have a huge number of variables, calling for similarly many parameters $p$. In other scenarios, the number of variables is moderate, but adequately modeling the data requires highly complex, non-linear models with many parameters. The traditional fixed- $p$-large- $n$ paradigm is inadequate in such situations.\n\nThis article adopts an asymptotic perspective, allowing both the sample size $n$ and the number of parameters $p_{n}$ to diverge. We consider general parametric problems where the estimator $\\hat{\\boldsymbol{\\theta}}$ solves an estimating equation\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(\\boldsymbol{X}_{i} ; \\hat{\\boldsymbol{\\theta}}\\right)=\\mathbf{0} \\in \\mathbb{R}^{p_{n}}\n$$\n\nwith some function $\\phi: \\mathbb{R}^{p_{n}} \\rightarrow \\mathbb{R}^{p_{n}}$. A classical example are risk minimization problems, where $\\phi$ is the gradient of a loss function. The estimating equation framework is also suited for more complex methods, such as stepwise estimation procedures, where an optimization-based formulation is less convenient. In fact, the lack of general asymptotic theory for stepwise procedures when $p_{n} \\rightarrow \\infty$ is what originally motivated this work.\n\nThe main question we address is: under what conditions on the data-generating process, the function $\\phi$, and the growth of $p_{n}$ is an estimator solving (1) consistent and asymptotically normal? Both penalized and unpenalized estimation problems are considered in this article.\n\nUnpenalized estimation The study of this problem dates back at least to Huber (1973), who focused on $M$-estimators in linear models. Following his seminal work, Yohai and Maronna\n\n(1979), Portnoy (1984, 1985), Welsh (1989), and Mammen (1989) established consistency and asymptotic normality under various conditions; see Li et al. (2011) for a comprehensive overview. The sharpest known conditions are $p_{n} \\ln p_{n} / n \\rightarrow 0$ for consistency (Portnoy, 1984), and $\\left(p_{n} \\ln n\\right)^{3 / 2} / n \\rightarrow 0$ (Portnoy, 1985) or $p_{n}^{3 / 2} \\ln n / n \\rightarrow 0$ (Mammen, 1989) for asymptotic normality. Fan and Peng (2004) extend these results from (generalized) linear models to general maximum likelihood problems under more restrictive conditions, requiring $p_{n}^{4} / n \\rightarrow 0$ for consistency and $p_{n}^{5} / n \\rightarrow 0$ for asymptotic normality. He and Shao (2000) derived asymptotics for $M$-estimators with convex loss under a generic stochastic equicontinuity condition, though this approach provides limited insight into the settings where it applies.\n\nThe first contribution of this work is to extend existing results to much broader classes of estimation problems under verifiable but weak conditions. We provide general conditions for the existence and consistency (Theorem 1), uniqueness (Theorem 2), and asymptotic normality (Theorem 3) of the estimator $\\hat{\\boldsymbol{\\theta}}$ in (1). When specialized, these results closely align with the sharpest known conditions for (generalized) linear models and substantially improve upon those for general maximum-likelihood problems. Importantly, our conditions accommodate more complex estimation settings, including stepwise or multi-sample procedures, as we demonstrate by several corollaries.\n\nPenalized estimation Even the condition $p_{n} / n \\rightarrow 0$ is overly restrictive for many modern problems. When $p_{n}$ exceeds $n$, consistent estimation is still possible if the true parameter $\\boldsymbol{\\theta}^{*}$ is sparse, containing only a few nonzero entries. In such settings, penalized estimation offers a viable solution. Penalized estimation was initially studied in the context of linear models and its variants. Various sparsity-inducing penalties have been proposed, including the Lasso (Tibshirani, 1996), group-structured penalties like the Group Lasso (Yuan and Lin, 2006), and bias-reducing, non-convex penalties such as $\\ell_{q}$-penalties (Knight and Fu, 2000), SCAD (Fan, 1997, Fan and Li, 2001), and MCP (Zhang, 2010).\n\nA key question is when such penalized estimators are consistent, both in terms of estimation error and the identification of nonzero parameters. The developments in this area are comprehensively summarized in the recent monograph by Wainwright (2019). State-of-the-art results address general $M$-estimation problems and broad classes of penalties (Negahban et al., 2012, Lee et al., 2013, Loh and Wainwright, 2015, 2017, Loh, 2017) under a restricted strong convexity (RSC) condition on the loss. A unifying theme is the focus on deterministic bounds, where conditions are based on the realized sample rather than population-level quantities. One may then show that these conditions hold with high probability under appropriate assumptions, often at the expense of constants too large for practical inference. Although an asymptotic perspective may sometimes be misleading for finite samples, it offers two distinct advantages. First, regularity conditions are naturally formulated at the population level, providing clarity on the settings where the results are applicable. Second, it enables a precise characterization of estimation uncertainty through distributional limits.\n\nOur second contribution is to complement the existing literature with such asymptotic results while broadening their applicability. Penalized inference within the general estimating equation framework (1) can be appropriately formulated as\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(\\boldsymbol{X}_{i} ; \\hat{\\boldsymbol{\\theta}}\\right) \\in \\partial p_{\\boldsymbol{\\lambda}}(\\hat{\\boldsymbol{\\theta}})\n$$\n\nwhere $\\partial p_{\\boldsymbol{\\lambda}}(\\hat{\\boldsymbol{\\theta}})$ denotes the subdifferential of a penalty function $p_{\\boldsymbol{\\lambda}}$ at $\\hat{\\boldsymbol{\\theta}}$, and $\\boldsymbol{\\lambda}$ is a vector of tuning parameters. We derive general conditions for consistency (Theorem 4), existence (Theorem 5),\n\nand uniqueness (Theorem 6) of such penalized estimators. Our conditions imply a weak version of the RSC condition. We also show that the stronger form commonly used in the literature already fails in a simple, genuinely non-linear problem, where our weaker condition remains easy to verify (Section 4.1.2). The solution constructed in Theorem 5 is particularly appealing, because it is selection consistent. Its existence relies on a generalization of a condition known as mutual incoherence in the literature. For this solution, we also establish asymptotic normality (Theorem 7). For suitable non-convex penalties, the estimator satisfies the oracle property: it behaves as if the zero entries of $\\boldsymbol{\\theta}^{*}$ were known in advance and no penalization is needed. The assumptions on the penalty are mild. Notably, it does not need to be convex or coordinateseparable and can involve multiple tuning parameters with varying strength. We are unaware of other results that apply to such a broad class of penalties. The population-level regularity conditions are weak, permitting up to $p_{n}=e^{O(n)}$, and relatively straightforward to verify in applications, as we shall illustrate in several corollaries.\n\nOutline The remainder of this article is structured as follows. Section 2 presents the main results for unpenalized estimation. Section 3.1 explains formulation (2) for penalized estimation within general estimating equations. Our conditions on the penalty, along with several examples, are discussed in Section 3.2. Section 3.3 contains the main consistency results and a detailed discussion of the mutual incoherence condition. The asymptotic normality result is provided in Section 3.4. In Section 4, we demonstrate the wide applicability of our results through examples, including $M$-estimation (Section 4.1), multi-sample estimation problems in distributed inference and quality control (Section 4.2), and stepwise procedures in causal inference and stochastic optimization (Section 4.3). All proofs are provided in the appendix.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 2 Unpenalized Estimation\n### 2.1 Setup and Notation\n\nSuppose we observe independent random variables $\\boldsymbol{X}_{1}, \\ldots, \\boldsymbol{X}_{n} \\in \\mathcal{X}$ and want to estimate a parameter $\\boldsymbol{\\theta}=\\left(\\theta_{1}, \\ldots, \\theta_{p_{n}}\\right)^{\\top} \\in \\mathbb{R}^{p_{n}}$ with $p_{n} \\rightarrow \\infty$ as $n \\rightarrow \\infty$. We do not assume identical distributions to also cover multi-sample estimation problems. The target value $\\boldsymbol{\\theta}^{*}$ is the solution to the system of equations $\\sum_{i=1}^{n} \\mathbb{E}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]=\\mathbf{0}$, with continuous $\\phi_{i}(\\boldsymbol{\\theta})=\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right) \\in \\mathbb{R}^{p_{n}}$; for example, the gradient of a log-likelihood or loss function. The $k$-th entry of $\\phi_{i}(\\boldsymbol{\\theta})$ is denoted by $\\phi_{i}(\\boldsymbol{\\theta})_{k}, k=1, \\ldots, p_{n}$. The estimator $\\hat{\\boldsymbol{\\theta}}$ is the solution of\n\n$$\n\\Phi_{n}(\\hat{\\boldsymbol{\\theta}}):=\\frac{1}{n} \\sum_{i=1}^{n} \\phi_{i}(\\hat{\\boldsymbol{\\theta}})=\\mathbf{0}\n$$\n\nDefine the two $p_{n} \\times p_{n}$ matrices\n\n$$\nI(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{Cov}\\left[\\phi_{i}(\\boldsymbol{\\theta})\\right], \\quad J(\\boldsymbol{\\theta})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\left[\\phi_{i}(\\boldsymbol{\\theta})\\right]=\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta_{l}} \\mathbb{E}\\left[\\phi_{i}(\\boldsymbol{\\theta})_{k}\\right]\\right)_{k, l=1, \\ldots, p_{n}}\n$$\n\nThe parameter $\\boldsymbol{\\theta}^{*}$, functions $\\phi(\\boldsymbol{\\theta}), J(\\boldsymbol{\\theta}), I(\\boldsymbol{\\theta})$, and the support and distribution of the $\\boldsymbol{X}_{i}$ all depend on $n$, but we suppress this in the notation to avoid clutter.\n\nThroughout the paper, $\\|\\cdot\\|$ denotes the Euclidean norm for vectors and the spectral norm $\\|A\\|=\\sup _{\\|\\boldsymbol{x}\\|=1}\\|A \\boldsymbol{x}\\|$ for matrices. Define $r_{n}=\\sqrt{\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)\\right) / n}$ and let $\\Theta_{n} \\subset \\mathbb{R}^{p_{n}}$ be a sequence of sets with $\\Theta_{n} \\supset\\left\\{\\boldsymbol{\\theta}:\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant r_{n} C\\right\\}$ for all $C<\\infty$ and $n$ large.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 2.2 Consistency and Uniqueness \n\nOur main assumption for consistency of the estimator $\\hat{\\boldsymbol{\\theta}}$ is the following:\n(A1) There exists a sequence of symmetric, matrix-valued functions $H_{n}(\\boldsymbol{x})$ such that:\n(i) For all $\\boldsymbol{u}$ such that $\\boldsymbol{\\theta}^{*}+\\boldsymbol{u} \\in \\Theta_{n}$ and $\\boldsymbol{x} \\in \\mathcal{X}$, it holds\n\n$$\n\\boldsymbol{u}^{\\top}\\left[\\phi\\left(\\boldsymbol{x} ; \\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{x} ; \\boldsymbol{\\theta}^{*}\\right)\\right] \\leqslant \\boldsymbol{u}^{\\top} H_{n}(\\boldsymbol{x}) \\boldsymbol{u}\n$$\n\n(ii) $\\lim \\sup _{n \\rightarrow \\infty} \\lambda_{\\max }\\left(n^{-1} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right) \\leqslant-c<0$;\n(iii) For some sequence $B_{n}=o\\left(n / \\ln p_{n}\\right)$, it holds\n\n$$\n\\begin{aligned}\n& \\frac{1}{n}\\left\\|\\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)^{2} \\mathbb{1}_{\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\| \\leqslant B_{n}}\\right]\\right\\|=o\\left(n / \\ln p_{n}\\right) \\\\\n& \\frac{1}{n} \\sum_{i=1}^{n} \\int_{B_{n}}^{\\infty} \\mathbb{P}\\left(\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\|>t\\right) d t=o(1)\n\\end{aligned}\n$$\n\nBefore discussing the assumptions in detail, we state our main results.\nTheorem 1. Under assumption (A1), the following holds with probability tending to 1:\n(i) The sets $\\Theta_{n}$ contain at least one solution of the estimating equation (3).\n(ii) Every solution $\\hat{\\boldsymbol{\\theta}} \\in \\Theta_{n}$ satisfies\n\n$$\n\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{\\frac{\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)\\right)}{n}}\\right)\n$$\n\nTheorem 2. Suppose that (A1) holds, and for any $\\boldsymbol{\\theta}, \\boldsymbol{\\theta}+\\boldsymbol{u} \\in\\left\\{\\boldsymbol{\\theta}:\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant a_{n} r_{n}\\right\\}$ with $a_{n} \\rightarrow \\infty$ arbitrarily slowly and all $\\boldsymbol{x} \\in \\mathcal{X}$, it holds\n\n$$\n\\boldsymbol{u}^{\\top}[\\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta}+\\boldsymbol{u})-\\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta})] \\leqslant \\boldsymbol{u}^{\\top} H_{n}(\\boldsymbol{x}) \\boldsymbol{u}\n$$\n\nThen the solution $\\hat{\\boldsymbol{\\theta}}$ is unique on $\\Theta_{n}$ with probability tending to 1 .\nThe convergence rate $r_{n}=\\sqrt{\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)\\right) / n}$ is typically $\\sqrt{p_{n} / n}$, for example when $\\max _{i, k} \\mathbb{E}\\left[\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}^{*}\\right)_{k}^{2}\\right]=$ $O(1)$. However, the above formulation allows the entries of $\\phi$ to diverge. This is useful in settings with a slower rate of convergence, e.g., the multi-sample estimation problems discussed in Section 4.2.\n\nTo understand condition (A1), it is instructive to outline the core idea of the proof. A sufficient condition for the estimating equation (3) to have solution on a ball $\\left\\{\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant r_{n} C\\right\\}$ is (Fierro et al., 2004, Theorem 2.3):\n\n$$\n\\sup _{\\|\\boldsymbol{u}\\|=r_{n} C} \\boldsymbol{u}^{\\top} \\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right) \\leqslant 0\n$$\n\nCondition (i) allows us to majorize the left-hand side via\n\n$$\n\\boldsymbol{u}^{\\top} \\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right) \\leqslant \\boldsymbol{u}^{\\top} \\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}\\right)+\\boldsymbol{u}^{\\top}\\left(\\frac{1}{n} \\sum_{i=1}^{n} H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right) \\boldsymbol{u}\n$$\n\nStandard arguments give $\\left\\|\\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|=O_{p}\\left(r_{n}\\right)$, and the sample average in the quadratic form can be approximated by its expectation, using condition (iii) and concentration inequalities for random matrices. This expectation is negative definite by (ii), so the second term on the right is strictly negative. For $\\|\\boldsymbol{u}\\|=r_{n} C$ with sufficiently large $C$, the second term in the upper bound dominates the first. This shows that the estimating equation has a solution on $\\left\\{\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant r_{n} C\\right\\}$ with high probability. Consistency of other solutions and uniqueness follow from variations of the same argument.\n\nThe majorization approach provides uniform control over the curvature around $\\boldsymbol{\\theta}^{*}$ without relying on covering arguments. This appears essential for establishing non-restrictive conditions on the growth rate of $p_{n}$. Examples of constructing $H_{n}$ in various settings are provided in Section 4. The matrix-valued function $H_{n}$ may be interpreted as an envelope for the family of symmetrized Jacobians\n\n$$\n\\left\\{H_{\\boldsymbol{\\theta}}(\\boldsymbol{x})=\\frac{1}{2} \\nabla_{\\boldsymbol{\\theta}} \\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta})+\\frac{1}{2} \\nabla_{\\boldsymbol{\\theta}} \\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta})^{\\top}: \\boldsymbol{\\theta} \\in \\Theta_{n}\\right\\}\n$$\n\nin the positive semi-definite ordering $\\leq$ of matrices ${ }^{1}$, but condition (i) is slightly weaker. Condition (ii) is an on-average concavity constraint, but $\\boldsymbol{u} \\mapsto \\boldsymbol{u}^{\\top} H_{n}(\\boldsymbol{x}) \\boldsymbol{u}$ does not have to be concave for every $\\boldsymbol{x}$. Rate conditions as in (iii) and similar form will be used throughout this article. Lemma 1 and Lemma 2 in the appendix are useful tools for converting them into more interpretable moment conditions. For example, the second part of (iii) is satisfied if $p_{n} \\ln p_{n} / n \\rightarrow 0, \\max _{i} \\mathbb{E}\\left[\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\|\\right]=$ $O\\left(p_{n}\\right)$, and $\\max _{i} \\operatorname{Var}\\left[\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\|\\right]=O\\left(p_{n}\\right)$. Further, the first part of (iii) follows automatically from the second if all $H_{n}(\\boldsymbol{x})$ are negative semi-definite and $n^{-1} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]=O(1)$. The weakest conditions are achieved by choosing $\\Theta_{n}=\\left\\{\\boldsymbol{\\theta}:\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant r_{n} a_{n}\\right\\}$ for $a_{n} \\rightarrow \\infty$ arbitrarily slow. In this case, only the local behavior around a shrinking neighborhood of $\\boldsymbol{\\theta}^{*}$ is relevant. The conditions become stronger when considering larger sets $\\Theta_{n}$ because they potentially require larger envelopes $H_{n}$ in (A1)(i) and (4).\n\nThe assumptions of the theorems implicitly restrict the rate of growth of the number of parameters $p_{n}$. Results of this form have been studied primarily in the context of M-estimation of (generalized) linear regression models, where $\\phi_{i}(\\boldsymbol{\\theta})=\\psi\\left(Y_{i}, \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{\\theta}\\right) \\boldsymbol{X}_{i}$ for some $\\psi$. In this setting, we obtain the condition $p_{n} \\ln p_{n} / n \\rightarrow 0$ under standard assumptions. This corresponds to the best known results obtained by Portnoy (1984); see Section 4.1.1 for more details. For maximum-likelihood estimation in general non-linear models, Fan and Peng (2004) required the much stronger condition $p_{n}^{4} / n \\rightarrow 0$. Technically, we do not even require $p_{n} / n \\rightarrow 0$ unless dictated by condition (A1)(iii). This is sensible, because the vector equation (3) may be composed of entirely unrelated estimating equations for the individual parameters $\\theta_{k}, k=1, \\ldots, p_{n}$. In such cases, we may choose $H_{n}$ as a diagonal matrix. If, for example, all entries of $H_{n}$ are uniformly bounded, (A1)(iii) holds with $B_{n}=O(1)$ as long as $p_{n} \\leqslant e^{o(n)}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 2.3 Asymptotic Normality \n\nWe now turn to the asymptotic normality of the estimator. Because the dimension of the parameter space grows with the sample size, we state this in terms of convergence of finitedimensional projections. Let $A_{n} \\in \\mathbb{R}^{q \\times p_{n}}$ be some matrix.\n\n[^0]\n[^0]:    ${ }^{1} A \\leq B$ if $B-A$ is positive semi-definite or, equivalently, $\\boldsymbol{u}^{\\top} A \\boldsymbol{u}-\\boldsymbol{u}^{\\top} B \\boldsymbol{u} \\leqslant 0$ for all $\\boldsymbol{u}$.\n\n(A2) For every $C \\in(0, \\infty)$ and some sequence $D_{n}=o\\left(\\sqrt{n} /\\left(r_{n} p_{n}\\right)\\right)$, it holds\n\n$$\n\\begin{aligned}\n& \\sup _{\\|\\boldsymbol{u}\\|,\\left\\|\\boldsymbol{u}^{\\prime}\\right\\| \\leqslant r_{n} C} \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\mathbb{E}\\left[\\left\\|A_{n}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}^{\\prime}\\right)\\right]\\right\\|^{2}\\right]}{\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\|^{2}}=o\\left(\\frac{1}{r_{n}^{2} p_{n}}\\right), \\\\\n& \\sum_{i=1}^{n} \\mathbb{P}\\left(\\sup _{\\|\\boldsymbol{u}\\|,\\left\\|\\boldsymbol{u}^{\\prime}\\right\\| \\leqslant r_{n} C} \\frac{\\left\\|A_{n}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}^{\\prime}\\right)\\right]\\right\\|}{\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\|}>D_{n}\\right)=o(1), \\\\\n& \\sup _{\\|\\boldsymbol{u}\\| \\leqslant r_{n} C}\\left\\|A_{n}\\left[J\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-J\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\|=o\\left(\\frac{1}{\\sqrt{n} r_{n}}\\right) .\n\\end{aligned}\n$$\n\n(A3) It holds $\\max _{1 \\leqslant i \\leqslant n} \\mathbb{E}\\left[\\left\\|A_{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|^{4}\\right]=o(n)$.\nAssumption (A2) is a stochastic smoothness condition required to control fluctuations of the estimating equation. The moment condition (A3) typically requires $p_{n}^{2} / n \\rightarrow 0$, e.g., if $\\left\\|A_{n}\\right\\|=O(1)$ and $\\max _{i, k} \\mathbb{E}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)_{k}^{4}\\right]=O(1)$.\n\nTheorem 3. If conditions (A1)-(A3) hold for some matrix $A_{n} \\in \\mathbb{R}^{q \\times p_{n}}$ for which $\\Sigma=$ $\\lim _{n \\rightarrow \\infty} A_{n} I\\left(\\boldsymbol{\\theta}^{*}\\right) A_{n}^{\\top}$ exists, it holds\n\n$$\n\\sqrt{n} A_{n} J\\left(\\boldsymbol{\\theta}^{*}\\right)\\left(\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right) \\rightarrow_{d} \\mathcal{N}(\\mathbf{0}, \\Sigma)\n$$\n\nIn the finite-dimensional setting $\\left(p_{n}=p<\\infty\\right)$, we can choose $A_{n}=J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1}$. Then, Theorem 3 yields the typical result\n\n$$\n\\sqrt{n}\\left(\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right) \\rightarrow_{d} \\mathcal{N}\\left(\\mathbf{0}, J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1} I\\left(\\boldsymbol{\\theta}^{*}\\right) J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-\\top}\\right)\n$$\n\nThe proof of Theorem 3 uses mixed-entropy inequalities (Van der Vaart and Wellner, 2023, Section 2.14) building on Talagrand's work on generic chaining (Talagrand, 2005) to obtain weak conditions. For $M$-estimators in the linear regression model, we obtain asymptotic normality if $p_{n}^{2} \\ln n / n \\rightarrow 0$ under standard assumptions, see Corollary 1 in Section 4.1.1. Portnoy (1985) and Mammen (1989) obtain the slightly weaker condition $p_{n}^{3 / 2} \\ln n / n \\rightarrow 0$. Both authors exploit the linear model structure through fourth-order expansions and a design-dependent standardization, effectively alleviating the need for the sample Hessian to converge. Portnoy (1986) showed that a general CLT cannot hold unless $p_{n}^{2} / n \\rightarrow 0$, so our result appears to be close to optimal. Fan and Peng (2004) require the much stronger condition $p_{n}^{5} / n \\rightarrow 0$ for maximum-likelihood estimation in general non-linear models.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 3 Penalized Estimation\n### 3.1 Penalization of Estimating Equations\n\nTo motivate our formulation, consider a penalized risk minimization problem. Let $L$ be some differentiable loss function and $p_{\\lambda_{n}}$ be a function penalizing the magnitude of $\\boldsymbol{\\theta}$, e.g., the $\\ell_{1}$-penalty $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\lambda_{n}\\|\\boldsymbol{\\theta}\\|_{1}$. The estimator is given by\n\n$$\n\\tilde{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min }\\left(\\frac{1}{n} \\sum_{i=1}^{n} L\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right)+p_{\\lambda_{n}}(\\boldsymbol{\\theta})\\right)\n$$\n\nIf both penalty and loss are differentiable, the first-order condition for $\\hat{\\boldsymbol{\\theta}}$ is\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n}-\\nabla_{\\boldsymbol{\\theta}} L\\left(\\boldsymbol{X}_{i} ; \\hat{\\boldsymbol{\\theta}}\\right)-p_{\\lambda_{n}}^{\\prime}(\\hat{\\boldsymbol{\\theta}})=\\mathbf{0}\n$$\n\nwhere $p_{\\lambda_{n}}^{\\prime}(\\hat{\\boldsymbol{\\theta}})$ is the gradient of $p_{\\lambda_{n}}$ at $\\hat{\\boldsymbol{\\theta}}$. Sparsity inducing penalties are discontinuous at points where some coordinates are zero. The vector $\\hat{\\boldsymbol{\\theta}}$ is a solution to the minimization problem when the gradients cross $\\mathbf{0}$ at $\\hat{\\boldsymbol{\\theta}}$, either continuously or with a jump. This is illustrated in the left panel of Fig. 1.\n\nTo appropriately deal with the non-differentiability of the penalty, we reformulate the estimation problem. Recall from Clarke (1990, Chapter 2) that $\\boldsymbol{z} \\in \\mathbb{R}^{p}$ is a generalized gradient or subgradient of a Lipschitz function $f: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}$ at $\\boldsymbol{\\theta}$, if $f(\\boldsymbol{\\theta}+\\boldsymbol{\\Delta})-f(\\boldsymbol{\\theta}) \\geqslant\\langle\\boldsymbol{z}, \\boldsymbol{\\Delta}\\rangle$, for all $\\boldsymbol{\\Delta}$ in a neighborhood of $\\mathbf{0}$. It coincides with the usual derivative if the function is differentiable. The collection of all subgradients of $f$ at $\\boldsymbol{\\theta}$ is called subdifferential and denoted by $\\partial f(\\boldsymbol{\\theta})$. For example, $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\lambda_{n}\\|\\boldsymbol{\\theta}\\|_{1}$ has subdifferentials $\\partial p_{\\lambda_{n}}(\\boldsymbol{\\theta})_{k}=\\left\\{\\lambda_{n} \\operatorname{sign}\\left(\\theta_{k}\\right)\\right\\}$, where $\\operatorname{sign}(0)$ is allowed to be any number in $[-1,1]$. With this notation, the estimator $\\hat{\\boldsymbol{\\theta}}$ solves\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n}-\\nabla_{\\boldsymbol{\\theta}} L\\left(\\boldsymbol{X}_{i} ; \\hat{\\boldsymbol{\\theta}}\\right) \\in \\partial p_{\\lambda_{n}}(\\hat{\\boldsymbol{\\theta}})\n$$\n\nMore generally, we define a penalized estimator $\\hat{\\boldsymbol{\\theta}}$ as a solution to\n\n$$\n\\Phi_{n}(\\boldsymbol{\\theta}):=\\frac{1}{n} \\sum_{i=1}^{n} \\phi_{i}(\\boldsymbol{\\theta}) \\in \\partial p_{\\boldsymbol{\\lambda}_{n}}(\\boldsymbol{\\theta})\n$$\n\nWe allow for a vector of tuning parameters $\\boldsymbol{\\lambda}_{n}=\\left(\\lambda_{n, 1}, \\ldots, \\lambda_{n, p_{n}}\\right)$. This encompasses, for example, penalties of the form $p_{\\boldsymbol{\\lambda}_{n}}(\\boldsymbol{\\theta})=\\sum_{k=1}^{p_{n}} \\lambda_{n, k} p\\left(\\theta_{k}\\right)$, which can be useful when the estimator $\\hat{\\boldsymbol{\\theta}}$ is composed of solutions to sub-problems of different dimensionality or sample size. The target parameter $\\boldsymbol{\\theta}^{*}$ still solves the unpenalized population equation $\\sum_{i=1}^{n} \\mathbb{E}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]=\\mathbf{0}$. In what follows, we assume without loss of generality that the true $\\boldsymbol{\\theta}^{*}$ can be written as $\\boldsymbol{\\theta}^{*}=\\left(\\boldsymbol{\\theta}_{(1)}^{*}, \\boldsymbol{\\theta}_{(2)}^{*}\\right)$ with $\\boldsymbol{\\theta}_{(1)}^{*} \\in \\mathbb{R}^{s_{n}}$ and $\\boldsymbol{\\theta}_{(2)}^{*}=\\mathbf{0} \\in \\mathbb{R}^{p_{n}-s_{n}}$. Similarly, write $\\boldsymbol{v}_{(1)}=\\left(v_{1}, \\ldots, v_{s_{n}}\\right), \\boldsymbol{v}_{(2)}=\\left(v_{s_{n}+1}, \\ldots, v_{p_{n}}\\right)$ for any vector $\\boldsymbol{v} \\in \\mathbb{R}^{p_{n}}$.\n\nA penalty that is not differentiable at $\\mathbf{0}$ can induce sparsity. To illustrate this, consider the Lasso penalty $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\lambda_{n}\\|\\boldsymbol{\\theta}\\|_{1}$ with scalar $\\lambda_{n}$. Recall that the subdifferential of the Lasso penalty $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\lambda_{n}\\|\\boldsymbol{\\theta}\\|$ at $\\mathbf{0}$ is the set $\\partial p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\left[-\\lambda_{n}, \\lambda_{n}\\right]^{p_{n}}$. More generally, sparsity-inducing penalties typically satisfy\n\n$$\n\\partial p_{\\lambda_{n}}(\\boldsymbol{\\theta})_{(2)} \\supseteq\\left[-\\lambda_{n}, \\lambda_{n}\\right]^{p_{n}-s_{n}}\n$$\n\nfor all $\\boldsymbol{\\theta}$ with $\\boldsymbol{\\theta}_{(2)}=\\mathbf{0}$. The subdifferential collapses to the gradient if it exists, i.e., $\\partial_{\\theta_{k}} p_{\\lambda_{n}}(\\boldsymbol{\\theta})=$ $\\left\\{\\lambda_{n} \\operatorname{sign}\\left(\\theta_{k}\\right)\\right\\}$ is a singleton for $\\theta_{k} \\neq 0$ for the Lasso. This means that a solution with $\\hat{\\theta}_{k}>0$ must satisfy $\\Phi_{n}(\\hat{\\boldsymbol{\\theta}})_{k}=\\lambda_{n}$, and one with $\\hat{\\theta}_{k}<0$ must satisfy $\\Phi_{n}(\\hat{\\boldsymbol{\\theta}})_{k}=-\\lambda_{n}$. This biases the estimating equation to favor parameters with smaller magnitude. For $\\hat{\\boldsymbol{\\theta}}$ with $\\hat{\\theta}_{k}=0$ to be a solution, we only need $\\Phi_{n}(\\hat{\\boldsymbol{\\theta}})_{k} \\in\\left[-\\lambda_{n}, \\lambda_{n}\\right]$. Here, we give the criterion $\\Phi_{n}(\\hat{\\boldsymbol{\\theta}})_{k}$ extra wiggle room to make it more likely that the penalized criterion has solutions with $\\hat{\\theta}_{k}=0$.\n\nThis principle is illustrated in Fig. 1 for a one-dimensional $\\theta$. In the left column, the dashed lines show two possible realizations of $\\Phi_{n}(\\theta)$. Its root, the unpenalized estimator, is denoted by $\\hat{\\theta}_{u}$. Now we add a Lasso penalty with $\\lambda=1$. The solid lines show $\\Phi_{n}(\\theta)-p_{\\lambda}^{\\prime}(\\theta)$, whose root is the penalized estimator $\\hat{\\theta}$. The Lasso is not differentiable at zero, so $\\Phi_{n}(\\theta)-p_{\\lambda}^{\\prime}(\\theta)$ is not continuous and has a jump of size $2 \\lambda$ at $\\theta=0$. If this jump crosses 0 , then $\\hat{\\theta}=0$, which is the\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Two views on sparsity-inducing penalties: Left column: realization of $\\Phi_{n}(\\theta)$ (dashed line) and Lasso-penalized version (solid line). Right column: Penalized estimator as solution to $\\Phi_{n}(\\theta) \\in \\partial p_{\\lambda}(\\theta)$.\ncase in the lower left corner. The right column illustrates the view described above. Here, the solid line denotes $\\Phi_{n}(\\theta)$. A negative $\\hat{\\theta}$ must satisfy $\\Phi_{n}(\\hat{\\theta})=-\\lambda$, which is the case in the upper right corner. The penalty shrinks the coefficient, i.e., $|\\hat{\\theta}|<\\left|\\hat{\\theta}_{u}\\right|$. The penalized estimator is zero if $\\Phi_{n}(0) \\in[-\\lambda, \\lambda]$, which is shown in the lower right corner.\n\nMore generally, we may define the penalized estimator $\\hat{\\boldsymbol{\\theta}}$ as a solution to $\\Phi_{n}(\\hat{\\boldsymbol{\\theta}}) \\in \\mathcal{I}_{n}(\\hat{\\boldsymbol{\\theta}})$ with some function $\\mathcal{I}_{n}$ that maps $\\boldsymbol{\\theta}$ to $p_{n}$-dimensional rectangles. We shall continue with the penalty formulation for better interpretability. Adapting the following results to more general interval-valued functions $\\mathcal{I}_{n}(\\boldsymbol{\\theta})$ is straightforward.",
      "tables": {},
      "images": {
        "img-0.jpeg": "data:image/jpeg;base64,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"
      }
    },
    {
      "section_id": 6,
      "text": "# 3.2 Examples and Conditions for the Penalty \n\nSome common penalty functions covered by our framework are given in the following examples.\nExample 3.1 (Lasso). The $\\ell_{1}$ penalty (Tibshirani, 1996) $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\lambda_{n}\\|\\boldsymbol{\\theta}\\|_{1}$ is not differentiable at 0 and the subgradient is given by $p_{\\lambda_{n}}^{\\prime}(\\boldsymbol{\\theta})_{k}=\\lambda_{n} \\operatorname{sign}\\left(\\theta_{k}\\right)$, where $\\operatorname{sign}(0)$ is allowed to be any number in $[-1,1]$.\nExample 3.2 (Elastic Net). The elastic net penalty (Zou and Hastie, 2005) is given by $p_{\\lambda_{n, 1}, \\lambda_{n, 2}}(\\boldsymbol{\\theta})=\\lambda_{n, 1}\\|\\boldsymbol{\\theta}\\|_{1}+\\lambda_{n, 2}\\|\\boldsymbol{\\theta}\\|_{2}^{2}$. It holds $p_{\\lambda_{n}}^{\\prime}(\\boldsymbol{\\theta})_{k}=\\lambda_{n, 1} \\operatorname{sign}\\left(\\theta_{k}\\right)+2 \\lambda_{n, 2} \\theta_{k}$. For $\\lambda_{n, 1}>0$, the elastic net can induce sparsity.\nExample 3.3 (Group Lasso). The Group Lasso penalty (Yuan and Lin, 2006) is given by $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\lambda_{n} \\sum_{i=1}^{K}\\left\\|\\boldsymbol{\\theta}_{G_{i}}\\right\\|_{2}$ with groups $G_{1}, \\ldots, G_{K} \\subset\\left\\{1, \\ldots, p_{n}\\right\\}$. In this case, $\\boldsymbol{\\theta}_{(1)}$ contains all $\\theta_{k}$ belonging to groups $G_{i}$ with at least one non-zero $\\theta_{k}^{*}$ and $\\boldsymbol{\\theta}_{(2)}$ contains all $\\theta_{k}$ belonging to at least one group $G_{i}$ with $\\boldsymbol{\\theta}_{G_{i}}^{*}=\\mathbf{0}$. The subgradient is given by\n\n$$\np_{\\lambda_{n}}^{\\prime}(\\boldsymbol{\\theta})_{k}=\\lambda_{n} \\sum_{i=1}^{K} \\mathbb{1}\\left\\{k \\in G_{i}\\right\\}\\left[\\mathbb{1}\\left\\{\\left\\|\\boldsymbol{\\theta}_{G_{i}}\\right\\|_{2} \\neq 0\\right\\} \\frac{\\theta_{k}}{\\left\\|\\boldsymbol{\\theta}_{G_{i}}\\right\\|_{2}}+\\mathbb{1}\\left\\{\\left\\|\\boldsymbol{\\theta}_{G_{i}}\\right\\|_{2}=0\\right\\} \\operatorname{sign}\\left(\\theta_{k}\\right)\\right]\n$$\n\nThe Group Lasso is non-differentiable at points where $\\left\\|\\boldsymbol{\\theta}_{G_{i}}\\right\\|_{2}=0$ for some of the groups.\n\nExample 3.4 ( $\\ell_{q}$ penalty). The $\\ell_{q}$ penalty (Frank and Friedman, 1993) $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\lambda_{n}\\|\\boldsymbol{\\theta}\\|_{q}^{q}$ with $q \\in(0,1]$ is not differentiable at 0 and can therefore induce sparsity. The $\\ell_{q}$ penalty is also not Lipschitz around 0 for $q<1$, but the definition of a subdifferential can be extended to non-smooth functions as in Clarke (1990, Section 2.4). This leads to subdifferentials of the form $\\partial_{\\theta_{k}} p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\left\\{q \\operatorname{sign}\\left(\\theta_{k}\\right)\\left|\\theta_{k}\\right|^{q-1}\\right\\}$ for $\\theta_{k} \\neq 0$ and $\\partial_{\\theta_{k}} p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\mathbb{R}$ for $\\theta_{k}=0$. The $\\ell_{q}$ penalty with $q<1$ is special in the sense that $\\boldsymbol{\\theta}=\\mathbf{0}$ is always a solution to the penalized estimating equation.\n\nExample 3.5 (SCAD). The SCAD penalty (smoothly clipped absolute deviation) (Fan, 1997, Fan and Li, 2001) was developed with the aim of obtaining a penalized estimator that is unbiased for large parameters. For a single parameter, the SCAD penalty and its derivative are defined as\n\n$$\np_{\\lambda}(\\theta)= \\begin{cases}\\lambda|\\theta| & \\text { if }|\\theta| \\leqslant \\lambda \\\\ \\frac{2 a \\lambda|\\theta|-\\theta^{2}-\\lambda^{2}}{2(a-1)} & \\text { if } \\lambda<|\\theta| \\leqslant a \\lambda \\\\ \\frac{(a+1) \\lambda^{2}}{2} & \\text { if }|\\theta|>a \\lambda\\end{cases} \\quad p_{\\lambda}^{\\prime}(\\theta)=\\left\\{\\begin{array}{ll}\n\\lambda \\operatorname{sign}(\\theta) & \\text { if }|\\theta| \\leqslant \\lambda \\\\\n\\frac{\\operatorname{sign}(\\theta)(a \\lambda-|\\theta|)}{(a-1)} & \\text { if } \\lambda<|\\theta| \\leqslant a \\lambda \\\\\n0 & \\text { if }|\\theta|>a \\lambda\\end{array}\\right.\n$$\n\nfor some $a>2$. For multiple parameters, the SCAD penalty is used componentwise as $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=$ $\\sum_{k=1}^{p_{n}} p_{\\lambda_{n}}\\left(\\theta_{k}\\right)$. Around the origin, SCAD coincides with the Lasso penalty, so it can induce sparsity. Since the derivative $p_{\\lambda}^{\\prime}(\\theta)$ is zero for all $|\\theta| \\geqslant a \\lambda$, it leads to an unbiased estimating equation for large parameters.\n\nExample 3.6 (MCP). A penalty that is unbiased for large coefficients (i.e., $p_{\\lambda}^{\\prime}(|\\theta|)=0$ for $|\\theta| \\geqslant a \\lambda$ with some $a>0$ ) and induces sparsity (i.e., $\\lim _{\\theta \\downarrow 0} p_{\\lambda}^{\\prime}(\\theta)=\\lambda$ ) must be nonconvex. The minimax concave penalty (MCP) (Zhang, 2010) is the \"most convex\" penalty among the penalties satisfying unbiasedness and sparsity, i.e., it minimizes the maximum concavity. For a single parameter, it is given by\n\n$$\np_{\\lambda}(\\theta)=\\mathbb{1}\\{|\\theta| \\leqslant a \\lambda\\}\\left(\\lambda|\\theta|-\\frac{\\theta^{2}}{2 a}\\right)+\\mathbb{1}\\{|\\theta|>a \\lambda\\} \\frac{a \\lambda^{2}}{2}\n$$\n\nwith derivative\n\n$$\np_{\\lambda}^{\\prime}(\\theta)=\\mathbb{1}\\{|\\theta| \\leqslant a \\lambda\\} \\operatorname{sign}(\\theta) \\frac{(a \\lambda-|\\theta|)}{a}\n$$\n\nfor some $a>0$ (Fan and Lv, 2010). The penalty shares a similar behavior to SCAD in that it is equivalent to Lasso around $\\theta=0$ and leads to unbiased estimating equations for $|\\theta|$ large.\n\nExample 3.7 (Fusion penalty). The aim of a fusion penalty is to reduce the number of different coefficients, which is of particular interest for categorical data (Tibshirani et al., 2005). This is achieved by penalizing differences between coefficients, e.g., $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\lambda_{n} \\sum_{k=1}^{p_{n}-1}\\left|\\theta_{k+1}-\\theta_{k}\\right|$. This penalty fits in the presented framework using a reparametrization, i.e., by defining $\\beta_{1}=\\theta_{1}$ and $\\beta_{k}=\\theta_{k}-\\theta_{k-1}$ and adapting the estimation function $\\phi$ accordingly. Then, zero-entries of $\\boldsymbol{\\beta}_{n}$ correspond to parameters being \"fused\".\n\nAll these penalties can be used with a vector of tuning parameters $\\boldsymbol{\\lambda}_{n}$, i.e., specific penalty parameters $\\lambda_{n, k}$ for each $\\theta_{k}$. Most penalties can then be expressed as $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\sum_{k=1}^{p_{n}} p_{\\lambda_{n, k}}\\left(\\theta_{k}\\right)$. For Group Lasso, a group-specific $\\lambda_{n, G_{i}}$ can be introduced for each group $G_{i}$. Different $\\lambda_{n, k}$ are especially desirable in settings of grouped or stepwise estimation of parameters. The examples show a wide range of different behaviors. In particular, we do not assume that the penalties are convex or decomposable into a sum of component penalties.\n\nTo state our conditions, let $\\tilde{r}_{n}$ be the target rate of convergence defined in Section 3.3.1 ahead. Recall that $\\boldsymbol{\\theta}=\\left(\\boldsymbol{\\theta}_{(1)}, \\boldsymbol{\\theta}_{(2)}\\right) \\in \\mathbb{R}^{p_{n}}$ with $\\boldsymbol{\\theta}_{(1)} \\in \\mathbb{R}^{s_{n}}$ and $\\boldsymbol{\\theta}^{*}$ satisfies $\\boldsymbol{\\theta}_{(2)}^{*}=\\mathbf{0}$. Define $\\Theta_{n}^{\\prime}=\\left\\{\\left(\\boldsymbol{\\theta}_{(1)}, \\mathbf{0}\\right):\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant \\tilde{r}_{n} a_{n}\\right\\}$ with $a_{n} \\rightarrow \\infty$ arbitrarily slowly, and let $\\Theta_{n} \\supset \\Theta_{n}^{\\prime}$ be sets specified further in the theorems to follow.\n\n(P1) The penalty $p_{\\boldsymbol{\\lambda}_{n}}$ is twice continuously differentiable with respect to $\\boldsymbol{\\theta}_{(1)}$ at $\\boldsymbol{\\theta} \\in \\Theta_{n}^{\\prime}$ with\n\n$$\n\\sup _{\\boldsymbol{\\theta} \\in \\Theta_{n}^{\\prime}}\\left\\|\\nabla_{\\boldsymbol{\\theta}_{(1)}}^{2} p_{\\boldsymbol{\\lambda}_{n}}(\\boldsymbol{\\theta})\\right\\|=o(1)\n$$\n\n(P2) The subdifferential satisfies\n\n$$\n\\partial p_{\\boldsymbol{\\lambda}_{n}}(\\boldsymbol{\\theta})_{(2)} \\supseteq\\left[-\\lambda_{n, s_{n}+1}, \\lambda_{n, s_{n}+1}\\right] \\times \\ldots \\times\\left[-\\lambda_{n, p_{n}}, \\lambda_{n, p_{n}}\\right] \\quad \\text { for all } \\boldsymbol{\\theta} \\in \\Theta_{n}^{\\prime}\n$$\n\n(P3) There is $\\mu_{n} \\geqslant 0$ such that for any $\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\prime} \\in \\Theta_{n}$ and valid subgradients $p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\boldsymbol{\\theta}), p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)$,\n\n$$\n\\left\\langle\\boldsymbol{\\theta}^{\\prime}-\\boldsymbol{\\theta}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)-p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\boldsymbol{\\theta})\\right\\rangle \\geqslant-\\frac{1}{2} \\mu_{n}\\left\\|\\boldsymbol{\\theta}^{\\prime}-\\boldsymbol{\\theta}\\right\\|^{2}\n$$\n\n(P4) The Hessian matrix of $p_{\\boldsymbol{\\lambda}_{n}}$ with respect to $\\boldsymbol{\\theta}_{(1)}$ satisfies\n\n$$\n\\sup _{\\boldsymbol{\\theta} \\in \\Theta_{n}^{\\prime}}\\left\\|\\nabla_{\\boldsymbol{\\theta}_{(1)}}^{2} p_{\\boldsymbol{\\lambda}_{n}}(\\boldsymbol{\\theta})\\right\\|=o\\left(\\frac{1}{\\sqrt{n} \\tilde{r}_{n}}\\right)\n$$\n\nFor the Lasso and SCAD, differentiability in (P1) requires $\\min _{1 \\leqslant k \\leqslant s_{n}}\\left|\\theta_{k}^{*}\\right| / \\tilde{r}_{n} \\rightarrow \\infty$ : to remain identifiable under penalization, the nonzero parameters $\\theta_{1}^{*}, \\ldots, \\theta_{s_{n}}^{*}$ are not allowed to vanish too fast. For the Group Lasso, the maximal coefficient of each group with at least one $\\theta_{k}^{*} \\neq 0$ must not decay too fast. The Hessian condition is mild and typically implied by $\\max _{k} \\lambda_{n, k} \\rightarrow 0$. For Lasso, Group Lasso, SCAD, and MCP penalties, condition (P2) holds with equality instead of $\\supseteq$, but using $\\supseteq$ also allows for $\\ell_{q}$ penalties with $q<1$ or asymmetric penalties of the form $p_{\\lambda_{n}}(\\boldsymbol{\\theta})_{k}=\\lambda_{n}\\left(c_{1}\\left|\\theta_{k}\\right| \\cdot \\mathbb{1}\\left\\{\\theta_{k} \\geqslant 0\\right\\}+c_{2}\\left|\\theta_{k}\\right| \\cdot \\mathbb{1}\\left\\{\\theta_{k}<0\\right\\}\\right)$. Assumption (P2) ensures that the penalty induces sparsity. Assumption (P3) limits the degree of non-convexity of the penalty and is required to guarantee uniqueness. For convex penalties, the assumption is always true with $\\mu_{n}=0$. It also holds, e.g., for the non-convex SCAD and MLP penalties with $\\mu_{n}=(a-1)^{-1}$ and $\\mu_{n}=a^{-1}$, respectively. It fails for $\\ell_{q}$-penalties with $q<1$, which cannot lead to unique solutions unless $\\boldsymbol{\\theta}^{*}=\\mathbf{0}$. The condition is implied by, but weaker than the $\\mu$-amenability condition in Loh and Wainwright (2017) and Loh (2017). Assumption (P4) is a refinement of (P1) and only required for asymptotic normality. For most penalties (including SCAD and Lasso), (P4) is trivial since $\\left\\|\\nabla_{\\boldsymbol{\\theta}_{(1)}}^{2} p_{\\lambda_{n}}(\\boldsymbol{\\theta})\\right\\|=\\mathbf{0}$ for small enough $\\lambda_{n}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 3.3 Estimation and selection consistency\n### 3.3.1 Notation and Assumptions\n\nTo simplify some conditions, we shall assume from now on that $p_{n} \\geqslant n^{a}$ for some $a>0$. This only excludes uninteresting edge cases where $p_{n}$ is effectively constant. To state the required assumptions, let\n\n$$\nb_{n}^{*}=\\left\\|p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right\\|_{\\infty}, \\quad \\tilde{r}_{n}=\\sqrt{\\frac{\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right)}{n}}+\\sqrt{s_{n}} b_{n}^{*}\n$$\n\nand recall $\\Theta_{n}^{\\prime}=\\left\\{\\left(\\boldsymbol{\\theta}_{(1)}, \\mathbf{0}\\right):\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant \\tilde{r}_{n} a_{n}\\right\\}$ with $a_{n} \\rightarrow \\infty$ arbitrarily slowly and $\\boldsymbol{\\theta}_{(1)} \\in \\mathbb{R}^{s_{n}}$. Further, define the cones\n\n$$\n\\Theta^{*}\\left(\\nu_{n}\\right)=\\left\\{\\boldsymbol{\\theta}:\\left\\|\\boldsymbol{\\theta}^{*}-\\boldsymbol{\\theta}\\right\\|_{1} \\leqslant \\sqrt{\\nu_{n}}\\left\\|\\boldsymbol{\\theta}^{*}-\\boldsymbol{\\theta}\\right\\|_{2}\\right\\}\n$$\n\nand suppose that $\\Theta_{n}$ are sets such that $\\Theta^{\\prime} \\subseteq \\Theta_{n} \\subseteq \\Theta^{*}\\left(\\nu_{n}\\right)$ for some $\\nu_{n}$. Define\n\n$$\n\\begin{aligned}\nJ(\\boldsymbol{\\theta})_{(1)} & =\\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\theta}_{(1)}} \\mathbb{E}\\left[\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right)_{(1)}\\right] \\in \\mathbb{R}^{s_{n} \\times s_{n}} \\\\\nJ(\\boldsymbol{\\theta})_{(2,1)} & =\\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\theta}_{(1)}} \\mathbb{E}\\left[\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right)_{(2)}\\right] \\in \\mathbb{R}^{\\left(p_{n}-s_{n}\\right) \\times s_{n}} \\\\\nJ(\\boldsymbol{\\theta})_{k,(1)} & =\\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\theta}_{(1)}}^{*} \\mathbb{E}\\left[\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right)_{k}\\right] \\in \\mathbb{R}^{1 \\times s_{n}} \\\\\nI(\\boldsymbol{\\theta})_{(1)} & =\\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{Cov}\\left[\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right)_{(1)}\\right] \\in \\mathbb{R}^{s_{n} \\times s_{n}} \\\\\n\\tilde{b}_{n} & =\\sup _{\\boldsymbol{v} \\in \\partial p_{\\boldsymbol{\\lambda}_{n}}(\\boldsymbol{\\theta}), \\boldsymbol{\\theta} \\in \\Theta_{n}}\\|\\boldsymbol{v}\\|_{\\infty}\n\\end{aligned}\n$$\n\nand let $\\eta_{n}$ be any sequence such that\n\n$$\n\\eta_{n} \\geqslant 2 \\sigma_{n} \\sqrt{\\frac{\\ln p_{n}}{n}}, \\quad \\text { where } \\quad \\sigma_{n}=\\max _{1 \\leqslant k \\leqslant p_{n}} \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)_{k}^{2}\\right]}\n$$\n\nOur main regularity conditions for penalized estimation are as follows:\n(A4) There exists a sequence of symmetric, matrix-valued functions $H_{n}(\\boldsymbol{x})$ such that:\n(i) For all $\\boldsymbol{\\theta}^{*}+\\boldsymbol{u} \\in \\Theta_{n}$ and $\\boldsymbol{x} \\in \\mathcal{X}$, it holds\n\n$$\n\\boldsymbol{u}^{\\top}\\left[\\phi\\left(\\boldsymbol{x} ; \\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{x} ; \\boldsymbol{\\theta}^{*}\\right)\\right] \\leqslant \\boldsymbol{u}^{\\top} H_{n}(\\boldsymbol{x}) \\boldsymbol{u}\n$$\n\n(ii) $\\lim \\sup _{n \\rightarrow \\infty} \\lambda_{\\max }\\left(n^{-1} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right) \\leqslant-c<0$;\n(iii) For some sequence $\\tilde{B}_{n}=o\\left(n /\\left(\\nu_{n} \\ln p_{n}\\right)\\right)$, it holds\n\n$$\n\\begin{aligned}\n\\max _{1 \\leqslant j, k \\leqslant p_{n}} \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)_{j, k}^{2}\\right] & =o\\left(\\frac{n}{\\nu_{n}^{2} \\ln p_{n}}\\right) \\\\\n\\sum_{i=1}^{n} \\mathbb{P}\\left(\\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|H_{n}\\left(\\boldsymbol{X}_{i}\\right)_{j, k}\\right|>\\tilde{B}_{n}\\right) & =o(1)\n\\end{aligned}\n$$\n\n(A5) It holds $\\sum_{i=1}^{n} \\mathbb{P}\\left(\\left\\|\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|_{\\infty}>\\sigma_{n} \\sqrt{\\frac{n}{4 \\ln p_{n}}}\\right)=o(1)$.\n(A6) There is $\\alpha \\in[0,1)$ such that\n\n$$\n\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} J(\\boldsymbol{\\theta})_{(2,1)} J^{-1}(\\boldsymbol{\\theta})_{(1)} p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\tilde{\\boldsymbol{\\theta}}\\right)_{(1)}\\right\\|_{\\infty} \\leqslant \\alpha\n$$\n\nfor all $\\boldsymbol{\\theta}, \\tilde{\\boldsymbol{\\theta}} \\in \\Theta^{\\prime}$.\n(A7) The subvector $\\boldsymbol{\\lambda}_{n(2)}$ must fulfill\n\n$$\n\\lambda_{n, k} \\geqslant \\frac{4}{1-\\alpha} J_{n, k} \\eta_{n} \\quad \\text { for all } k=s_{n}+1, \\ldots, p_{n}\n$$\n\nwhere $\\alpha$ is defined in (A6), $\\eta_{n}$ is defined in (8) and\n\n$$\nJ_{n, k}=\\max \\left\\{1, \\sup _{\\boldsymbol{\\theta} \\in \\Theta^{\\prime}}\\left\\|\\left(J(\\boldsymbol{\\theta})_{k,(1)} J^{-1}(\\boldsymbol{\\theta})_{(1)}\\right)^{\\top}\\right\\|_{1}\\right\\}\n$$\n\n(A8) It holds\n\n$$\n\\begin{aligned}\n& \\max _{1 \\leqslant k \\leqslant p_{n}} \\sup _{\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\prime} \\in \\Theta^{\\prime}} \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\mathbb{E}\\left[\\left|\\phi_{i}(\\boldsymbol{\\theta})_{k}-\\phi_{i}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)_{k}\\right|^{2}\\right]}{\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}\\right\\|^{2}}=o\\left(\\frac{n \\eta_{n}^{2}}{\\tilde{r}_{n}^{2}\\left(s_{n}+\\ln p_{n}\\right)}\\right) \\\\\n& \\sum_{i=1}^{n} \\mathbb{P}\\left(\\sup _{\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\prime} \\in \\Theta^{\\prime}} \\frac{\\left\\|\\phi_{i}(\\boldsymbol{\\theta})-\\phi_{i}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)\\right\\|_{\\infty}}{\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}\\right\\|}>\\tilde{D}_{n}\\right)=o(1)\n\\end{aligned}\n$$\n\nwith $\\tilde{r}_{n}$ and $\\eta_{n}$ as defined in (6) and (8) and some sequence $\\tilde{D}_{n}=o\\left(n \\eta_{n} /\\left(\\tilde{r}_{n} s_{n}+\\tilde{r}_{n} \\ln p_{n}\\right)\\right)$.\nAssumption (A4) is a variant of (A1) that guarantees a restricted form of concavity with high probability; see the following section for further comments. Assumption (A5) is a tail condition trading off the moments of the estimating equation with the number of parameters $p_{n}$. For example, if $\\sigma_{n}=O(1)$ and all $\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}^{*}\\right)_{k}$ have sub-Gaussian tails, we may take $p_{n}$ as large as $p_{n} \\sim e^{a^{a}}$ for some $a \\in(0,1 / 2)$. The remaining conditions are only required for constructing an explicit solution that is selection consistent. Assumption (A6) generalizes the mutual incoherence or irrepresentable conditions (Wainwright, 2019, Zhao and Yu, 2006, B\u00fchlmann and van de Geer, 2011) and is discussed in more detail in Section 3.3.3. Assumption (A7) requires the penalty parameters $\\lambda_{n, k}$ to be large enough for obtaining sparse solutions. In the recent literature (Negahban et al., 2012, Loh and Wainwright, 2015), such conditions are often stated with reference to the realization of $n^{-1}\\left\\|\\sum_{i=1}^{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|_{\\infty}$, which we replace by a population bound $\\eta_{n}$. The final condition (A8) guarantees sufficiently fast convergence of the estimating equation to its population counterpart. It leads to growth restrictions on $p_{n}$ analogous to our discussion of (A5). The condition becomes weaker if we make $\\eta_{n}$ in (8) large, but this requires us to use a larger penalty parameter $\\boldsymbol{\\lambda}_{n}$ in (A7). This is less problematic for non-convex penalties, see the discussion following Theorem 5 .",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "# 3.3.2 Main results \n\nWe will provide three main results for penalized estimators. The first guarantees that any sparse solution to the penalized estimating equation is close to the true parameter vector.\n\nTheorem 4. Suppose that (A4) and (A5) holds. Then any solution $\\hat{\\boldsymbol{\\theta}} \\in \\Theta_{n} \\subseteq \\Theta_{n}^{*}\\left(\\nu_{n}\\right)$ satisfies\n\n$$\n\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{\\nu_{n}}\\left(\\eta_{n}+\\bar{b}_{n}\\right)\\right)\n$$\n\nwith $\\eta_{n}$ and $\\bar{b}_{n}$ as defined in (8) and (7).\nThe optimal rate of convergence $\\sqrt{s_{n}} \\eta_{n}$ is attained if $\\nu_{n}=O\\left(s_{n}\\right)$ and $\\bar{b}_{n}=O\\left(\\eta_{n}\\right)$ which typically holds if $\\left\\|\\boldsymbol{\\lambda}_{n}\\right\\|_{\\infty}=O\\left(\\eta_{n}\\right)$. In most classical applications, this rate simplifies to the usual convergence rate $\\sqrt{s_{n} \\ln p_{n} / n}$. A similar result (with the same rate) was obtained by Loh and Wainwright (2015) under a restricted strong convexity (RSC) condition. The RSC condition makes assumptions about the realization of a sample, while Theorem 4 gives conditions on the population level. In the proof, we see that (A4) implies that an RSC-type condition holds with high probability. Specifically, in most standard cases we have\n\n$$\n\\left\\langle\\boldsymbol{u}, \\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}\\right)-\\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right\\rangle \\geqslant c\\|\\boldsymbol{u}\\|^{2}-c_{1}\\left\\|\\boldsymbol{u}\\right\\|_{1}^{2} \\eta_{n}\n$$\n\nfor some constant $c_{1} \\geqslant 0$. A second difference is that we restrict the statement to cones $\\Theta^{*}\\left(\\nu_{n}\\right)$, while Loh and Wainwright (2015), Negahban et al. (2012) and Wainwright (2019, Chapter 9)\n\nallow for larger sets of the form $\\left\\{\\|\\boldsymbol{\\theta}\\|_{1} \\leqslant k_{n}\\right\\}$. They facilitate this by a stronger RSC condition of the form\n\n$$\n\\left\\langle\\boldsymbol{u}, \\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}\\right)-\\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right\\rangle \\geqslant c\\|\\boldsymbol{u}\\|^{2}-c_{1}\\|\\boldsymbol{u}\\|_{1}^{2} \\eta_{n}^{2}\n$$\n\nrequired to hold with high probability for all $\\boldsymbol{u}$ with sufficiently small norm. Under mild assumptions on the penalty and tuning parameter, this stronger condition implies that $\\hat{\\boldsymbol{\\theta}} \\in \\Theta_{n}^{*}\\left(\\nu_{n}\\right)$ with $\\nu_{n}=O\\left(s_{n}\\right)$, as shown in Lemma 3 in the appendix. To the best of our knowledge, the stronger condition (10) has only been verified for variations of the linear model, where the fact that $H_{n}$ is negative semi-definite and rank- 1 can be exploited. Establishing this for general nonlinear problems appears much harder. In fact, it already fails to hold in simple problems not exhibiting this specific structure, see Section 4.1.2. The preemptive restriction to cones is a way to circumvent this issue with little practical consequence. Whenever a $\\nu_{n}$-sparse solution has been found, we also know that it belongs to $\\Theta^{*}\\left(\\nu_{n}\\right)$ and Theorem 4 applies.\n\nTheorem 4 does not say anything about existence and selection consistency, which also explains why it does not require a lower bound on the tuning parameters. The next theorem shows that there is an estimation and selection consistent solution to the penalized equation with high probability. Specifically, we show that, with high probability, the reduced problem\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\phi_{i}\\left(\\left(\\boldsymbol{\\theta}_{(1)}, \\mathbf{0}\\right)\\right)_{(1)} \\in \\widehat{c} p_{\\boldsymbol{\\lambda}_{n}}\\left(\\left(\\boldsymbol{\\theta}_{(1)}, \\mathbf{0}\\right)\\right)_{(1)}\n$$\n\nhas a solution $\\hat{\\boldsymbol{\\theta}}_{(1)}$ close to $\\boldsymbol{\\theta}_{(1)}^{*}$, and that $\\left(\\hat{\\boldsymbol{\\theta}}_{(1)}, \\mathbf{0}\\right)$ is also a solution to the full problem (5).\nTheorem 5. Suppose the reduced problem (11) satisfies (A1) on sets $\\left\\{\\boldsymbol{\\theta}_{(1)}: \\boldsymbol{\\theta} \\in \\Theta^{\\prime}\\right\\}$. Suppose further that (P1), (P2) and (A5)-(A8) hold. Then, with probability tending to 1 , the sets $\\Theta_{n}^{\\prime}$ contain a solution $\\hat{\\boldsymbol{\\theta}}$ of the penalized estimating equation (5) such that\n\n$$\n\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{\\frac{\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right)}{n}}+\\sqrt{s_{n}} b_{n}^{*}\\right), \\quad \\hat{\\boldsymbol{\\theta}}_{(2)}=\\mathbf{0}\n$$\n\nTo simplify the discussion, suppose $\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right)=O\\left(s_{n}\\right)$, which is the most common situation. The rate of convergence depends on the number of non-zero coefficients $s_{n}$ of $\\boldsymbol{\\theta}^{*}$ and the bias $b_{n}^{*}$ (defined in (6)) induced by the penalty. For example, we have $b_{n}^{*}=\\lambda_{n}$ for the Lasso, and $b_{n}^{*}=0$ for SCAD for $n$ large enough if $\\lambda_{n} \\rightarrow 0$. For a Lasso-type penalty with different $\\lambda_{n, k}$, i.e., $p_{\\boldsymbol{\\lambda}_{n}}(\\boldsymbol{\\theta})=\\sum_{k=1}^{p_{n}} \\lambda_{n, k}\\left|\\theta_{k}\\right|$, we obtain $b_{n}^{*}=\\max _{k=1, \\ldots, s_{n}} \\lambda_{n, k}$. The rate of convergence does not explicitly involve the overall number of parameters $p_{n}$, although it may enter through the bias $b_{n}^{*}$ (typically logarithmically). Similarly, the regularity conditions above usually depend on $p_{n}$ only logarithmically. This shows that estimation of $\\boldsymbol{\\theta}^{*}$ is possible even if the total number of parameters $p_{n}$ is much larger than the sample size $n$. The non-asymptotic results of Negahban et al. (2012) and Loh and Wainwright (2015) imply a rate of $O_{p}\\left(\\sqrt{s_{n} \\ln p_{n} / n}\\right)$. For the SCAD and similar penalties, this is still suboptimal as Theorem 5 gives the rate $O_{p}\\left(\\sqrt{s_{n} / n}\\right)$.\n\nIn practice, we do not know which parameters $\\theta_{k}$ are 0 , so it is unclear whether the solution from Theorem 5 can be found. Our final theorem gives conditions under which the solution is unique.\n\nTheorem 6. Suppose that the conditions of Theorem 4 are satisfied and (P3) holds with $\\lim \\sup _{n \\rightarrow \\infty} \\mu_{n}<2 c$, where $c$ is defined in (A4). Suppose further that for all $\\boldsymbol{\\theta}, \\boldsymbol{\\theta}+\\boldsymbol{u} \\in$\n\n$\\Theta_{n} \\cap\\left\\{\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant a_{n} \\sqrt{\\nu_{n}}\\left(\\eta_{n}+\\bar{b}_{n}\\right)\\right\\}$ with $a_{n} \\rightarrow \\infty$ arbitrarily slowly and all $\\boldsymbol{x} \\in \\mathcal{X}$, it holds\n\n$$\n\\boldsymbol{u}^{\\top}\\left[\\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta}+\\boldsymbol{u})-\\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta})\\right] \\leqslant \\boldsymbol{u}^{\\top} H_{n}(\\boldsymbol{x}) \\boldsymbol{u}\n$$\n\nThen, with probability tending to 1, there is at most one solution in $\\Theta_{n}$.\nTheorem 6 implies the uniqueness of any solution in $\\Theta_{n}$. If the conditions of Theorem 5 hold, the selection consistent estimator $\\widehat{\\boldsymbol{\\theta}}$ is unique on $\\Theta_{n}$ with probability tending to 1 .",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 3.3.3 Mutual incoherence \\& irrepresentable condition \n\nThe condition on $\\boldsymbol{\\lambda}_{n}$ in (A6) requires the existence of some $\\alpha \\in[0,1)$ such that\n\n$$\n\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} J(\\boldsymbol{\\theta})_{(2,1)} J^{-1}(\\boldsymbol{\\theta})_{(1)} p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\widehat{\\boldsymbol{\\theta}})_{(1)}\\right\\|_{\\infty} \\leqslant \\alpha\n$$\n\nfor all $\\boldsymbol{\\theta}, \\widehat{\\boldsymbol{\\theta}} \\in \\Theta^{\\prime}$. For penalties such as SCAD or MCP that are unbiased for large coefficients if $\\lambda_{n} \\rightarrow 0$, this condition is always satisfied with $\\alpha=0$ since $p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\boldsymbol{\\theta})_{(1)}=\\mathbf{0}$. For the Lasso and a scalar $\\lambda_{n}$, the terms involving $\\lambda_{n}$ cancel and the condition can be simplified to\n\n$$\n\\left\\|J(\\boldsymbol{\\theta})_{(2,1)} J^{-1}(\\boldsymbol{\\theta})_{(1)}\\right\\|_{\\infty} \\leqslant \\alpha\n$$\n\nThis condition is independent of $\\lambda_{n}$ and therefore a characteristic of the population equation. The $\\left(p_{n}-s_{n}\\right) \\times s_{n}$ matrix $J(\\boldsymbol{\\theta})_{(2,1)}$ is the expected gradient of $\\phi(\\boldsymbol{\\theta})_{(2)}$ with respect to $\\boldsymbol{\\theta}_{(1)}$ and describes the average effect of errors in $\\boldsymbol{\\theta}_{(1)}$ on $\\phi(\\boldsymbol{\\theta})_{(2)}$. The matrix $J^{-1}(\\boldsymbol{\\theta})_{(1)}$ can be seen a normalization term. Condition (A6) requires that this effect must not be too big.\n\nFor the linear model, condition (13) has an even nicer interpretation: In this case, one obtains $J(\\boldsymbol{\\theta})_{(2,1)}=\\mathbb{E}\\left[\\boldsymbol{X}_{(2)} \\boldsymbol{X}_{(1)}^{\\top}\\right]$ and $J(\\boldsymbol{\\theta})_{(1)}=\\mathbb{E}\\left[\\boldsymbol{X}_{(1)} \\boldsymbol{X}_{(1)}^{\\top}\\right]$. Assuming without loss of generality that $\\mathbb{E}[\\boldsymbol{X}]=0$, these matrices correspond to covariances between the covariates. If the covariates are correlated too strongly, the Lasso cannot select the correct variables. This is a population version of the well-known mutual incoherence condition Wainwright (2019, Section 7.5.1), which requires some sort of approximate orthogonality. Zhao and Yu (2006) and B\u00fchlmann and van de Geer (2011) obtain a similar condition under the name irrepresentable condition, as this means that the $X_{j}, j=s_{n}+1, \\ldots, p_{n}$ are not represented \"too well\" by the covariates $\\boldsymbol{X}_{(1)}$ of the true model. Zhao and Yu (2006) and Zou (2006) also show that the condition in Eq. (13) is necessary for variable selection consistency of the Lasso in the linear model. Similar to our results, Loh and Wainwright (2017) show that such incoherence conditions are not necessary for the SCAD, MCP, and similar non-convex penalties.\n\nTo better understand the condition, consider the Lasso. Assume an iid problem with twodimensional parameter $\\left(\\theta_{1}, \\theta_{2}\\right)$ with $\\theta_{1}^{*} \\neq 0$ and $\\theta_{2}^{*}=0$. The zero $\\theta_{2}^{(0)}$ of $\\theta_{2} \\mapsto \\mathbb{E}\\left[\\phi\\left(\\left(\\theta_{1}^{*}, \\theta_{2}\\right)\\right)\\right]$ is $\\theta_{2}^{*}=0$. However, if some $\\hat{\\theta}_{1}$ is plugged in instead of $\\theta_{1}^{*}$, then $\\theta_{2}^{(0)}$ is not necessarily equal to $\\theta_{2}^{*}=0$. The more $\\theta_{1}$ affects $\\phi(\\boldsymbol{\\theta})_{2}$, the further away $\\theta_{2}^{(0)}$ is from 0 . In general, biased estimation of $\\boldsymbol{\\theta}_{(1)}$ implies biased estimation of $\\boldsymbol{\\theta}_{(2)}$. If the influence of $\\boldsymbol{\\theta}_{(1)}$ on the estimation of $\\boldsymbol{\\theta}_{(2)}$ is too big, $\\hat{\\boldsymbol{\\theta}}_{(2)}$ is too large to be shrunk to $\\mathbf{0}$. Choosing a larger $\\lambda_{n}$ does not help, since this also increases the bias of $\\hat{\\boldsymbol{\\theta}}_{(1)}$. If the estimation of $\\boldsymbol{\\theta}_{(1)}$ does not affect the estimation of $\\boldsymbol{\\theta}_{(2)}$, e.g., if the $k$-th entry of $\\phi(\\boldsymbol{\\theta})$ only depends on $\\theta_{k}$ for all $k$, then $J(\\boldsymbol{\\theta})_{(2,1)}=0$ and $\\alpha=0$.\n\nThe definition of the penalty in Section 3.1 also allows for vectors $\\boldsymbol{\\lambda}_{n}=\\left(\\lambda_{n, 1}, \\ldots, \\lambda_{n, p_{n}}\\right)$, e.g., a weighted Lasso penalty $p_{\\boldsymbol{\\lambda}_{n}}(\\boldsymbol{\\theta})=\\sum_{k=1}^{p_{n}} \\lambda_{n, k}\\left|\\theta_{k}\\right|$. Here, the condition characterizes a trade-off between the magnitudes of the entries of $\\boldsymbol{\\lambda}_{n}$, depending on how sensitive the equation is to errors\n\nin the respective parameters. Assuming $J(\\boldsymbol{\\theta})_{(1)}=I_{s_{n}}$ for simplicity, condition (A6) becomes\n\n$$\n\\frac{1}{\\lambda_{n, j}} \\sum_{k=1}^{s_{n}} \\lambda_{n, k}\\left|\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\theta_{k}} \\phi(\\boldsymbol{\\theta})_{j}\\right]\\right| \\leqslant \\alpha<1 \\quad \\text { for all } j=s_{n}+1, \\ldots, p_{n}\n$$\n\nRecall from above that we need this assumption because a biased estimation of $\\boldsymbol{\\theta}_{(1)}$ leads to a bias in $\\tilde{\\boldsymbol{\\theta}}_{(2)}$. If the effect of errors in $\\theta_{k}, k \\in\\left\\{1, \\ldots, s_{n}\\right\\}$ on the estimating equation for $\\theta_{j}, j \\in\\left\\{s_{n}+1, \\ldots, p_{n}\\right\\}$ are large, we must either choose $\\lambda_{n, k}$ small or $\\lambda_{n, j}$ large. Since we do not know the true support of $\\boldsymbol{\\theta}^{*}$ in advance, tuning $\\lambda_{n, k}$ differently depending on whether it belongs to $\\boldsymbol{\\lambda}_{n,(1)}$ or $\\boldsymbol{\\lambda}_{n(2)}$ is not practical. However, it is sometimes possible to gauge the magnitudes of $\\mathbb{E}\\left[\\nabla_{\\boldsymbol{\\theta}} \\phi(\\boldsymbol{\\theta})_{j}\\right]$ and choose $\\lambda_{n, j}$ large if this magnitude is high. One example is if $\\phi_{i}(\\boldsymbol{\\theta})_{j}$ only depends on a subset of $\\boldsymbol{\\theta}$, as is often the case in step-wise estimation procedures.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# 3.4 Asymptotic Normality and the Oracle Property \n\nFinally, we establish asymptotic normality of the estimator $\\tilde{\\boldsymbol{\\theta}}_{(1)}$.\nTheorem 7. Suppose that (P4) holds and that for some matrix $A_{n} \\in \\mathbb{R}^{q \\times s_{n}}$ with $\\left\\|A_{n}\\right\\|=O(1)$, the reduced problem (11) satisfies the conditions of Theorem 3 with $r_{n}$ replaced by $\\tilde{r}_{n}$ as defined in (6). Then, the estimator $\\tilde{\\boldsymbol{\\theta}}_{(1)}$ in Theorem 5 is asymptotically normal with\n\n$$\n\\sqrt{n} A_{n}\\left[J\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}\\right)-p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right] \\rightarrow_{d} \\mathcal{N}(0, \\Sigma)\n$$\n\nwhere $\\Sigma=\\lim _{n \\rightarrow \\infty} A_{n} I\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)} A_{n}^{\\top}$.\nRecall that $\\tilde{\\boldsymbol{\\theta}}$ has the oracle property if it behaves like the hypothetical oracle estimator that knows $\\boldsymbol{\\theta}_{(2)}^{*}=\\mathbf{0}$ in advance. This is the case if $\\tilde{\\boldsymbol{\\theta}}_{(2)}=\\mathbf{0}$ and $p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}=o(1 / \\sqrt{n})$. For SCAD and MCP with $\\lambda_{n} \\rightarrow 0$, one obtains $b_{n}^{*}=0$, so the rate of convergence is the same as if $\\boldsymbol{\\theta}_{(2)}^{*}=\\mathbf{0}$ is known in advance. Since $p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}=\\mathbf{0}$, the SCAD and MCP-penalized estimators have the same efficiency as the oracle estimator. As long as $\\boldsymbol{\\lambda}_{n}$ is small enough that $p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}=\\mathbf{0}$ asymptotically, it can be chosen large enough such that (A8) is fulfilled.\n\nFor the Lasso, we have $b_{n}^{*}=\\lambda_{n}$ and $p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}=O\\left(\\sqrt{s_{n}} \\lambda_{n}\\right)$. For the oracle property to hold, we would need $\\lambda_{n}=o\\left(1 / \\sqrt{s_{n} n}\\right)$. This condition cannot be satisfied, because (A6) requires $\\lambda_{n} \\geqslant \\eta_{n} \\approx \\sqrt{\\ln p_{n} / n}$. This is in line with the results from Zou (2006), who shows that in the linear model, the Lasso is only variable selection consistent at the cost of a slower rate of convergence.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "## 4 Applications\n\nOur general results are stated under rather abstract regularity conditions to keep them widely applicable. In the following, we provide several examples of how these conditions simplify in specific applications. While some of these examples can be analyzed using less sophisticated approaches tailored to the specific problem, this section illustrates the wide applicability of our framework.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# 4.1 $M$-Estimation\n### 4.1.1 Generalized Linear Models\n\nLet $\\boldsymbol{X}_{1}, \\ldots, \\boldsymbol{X}_{n} \\in \\mathbb{R}^{p_{n}}$ be iid and consider an estimator $\\hat{\\boldsymbol{\\theta}}$ that satisfies\n\n$$\n\\sum_{i=1}^{n} \\phi_{i}(\\hat{\\boldsymbol{\\theta}})=\\sum_{i=1}^{n} \\psi\\left(Y_{i}, \\boldsymbol{X}_{i}^{\\top} \\hat{\\boldsymbol{\\theta}}\\right) \\boldsymbol{X}_{i}=\\mathbf{0}\n$$\n\nwith some function $\\psi$. This includes the least squares estimator and other $M$-estimators in the linear model studied by Huber (1973), Portnoy (1984, 1985) and Mammen (1989) as well as likelihood inference in generalized linear models as special cases. If the function $\\psi$ is smooth with nonpositive derivative, the matrix $H_{n}$ in (A1) can be constructed as\n\n$$\nH_{n}(\\boldsymbol{x})=-\\inf _{\\boldsymbol{\\theta} \\in \\Theta_{n}}\\left|\\psi^{\\prime}\\left(Y_{i}, \\boldsymbol{x}^{\\top} \\boldsymbol{\\theta}\\right)\\right| \\boldsymbol{x} \\boldsymbol{x}^{\\top}\n$$\n\nwhere $\\psi^{\\prime}\\left(Y_{i}, \\eta\\right)=\\frac{\\hat{\\sigma}}{\\hat{\\sigma} \\eta} \\psi\\left(Y_{i}, \\eta\\right)$.\nCorollary 1. Let $\\hat{\\boldsymbol{\\theta}}$ solve (14) and let $\\psi^{\\prime}$ be nonpositive, Lipschitz in $\\eta$ and uniformly bounded. Suppose that $\\max _{k} \\mathbb{E}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)_{k}^{4}\\right]=O(1)$ and for all $\\boldsymbol{a} \\in \\mathbb{R}^{p_{n}}$ with $\\|\\boldsymbol{a}\\|=O(1)$,\n\n$$\n\\begin{array}{ll}\n\\left\\|\\mathbb{E}\\left[\\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top}\\right]\\right\\|=O(1), & \\mathbb{E}\\left[\\left|\\boldsymbol{a}^{\\top} \\boldsymbol{X}_{i}\\right|^{4}\\right]=O\\left(\\|\\boldsymbol{a}\\|^{4}\\right), \\quad \\mathbb{E}\\left[\\rho\\left(\\left|\\boldsymbol{a}^{\\top} \\boldsymbol{X}_{i}\\right|^{2}\\right)\\right]=O(1) \\\\\n\\operatorname{Var}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|^{2}\\right]=O\\left(p_{n}\\right), & \\mathbb{E}\\left[\\rho\\left(\\left|\\left\\|\\boldsymbol{X}_{i}\\right\\|-\\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right]\\right|^{2}\\right)\\right]=O(1)\n\\end{array}\n$$\n\nwith some increasing and strictly convex function $\\rho:(0, \\infty) \\rightarrow(0, \\infty)$. Suppose further that\n\n$$\n\\lambda_{\\min }\\left(\\mathbb{E}\\left[\\inf _{\\boldsymbol{\\theta} \\in \\Theta_{n}}\\left|\\psi^{\\prime}\\left(Y_{i}, \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{\\theta}\\right)\\right| \\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top}\\right]\\right) \\geqslant c\n$$\n\nholds with some $c>0$ and sets $\\Theta_{n} \\supset\\left\\{\\boldsymbol{\\theta}:\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant r_{n} C\\right\\}$ for all $C<\\infty$ and $n$ large.\n(i) If $p_{n} \\ln p_{n} / n \\rightarrow 0, \\hat{\\boldsymbol{\\theta}}$ is a $\\sqrt{n / p_{n}}$-consistent estimator of $\\boldsymbol{\\theta}^{*}$ and unique on $\\Theta_{n}$ with probability tending to 1 .\n(ii) If the matrix $A_{n}$ in Theorem 3 satisfies $\\left\\|A_{n}\\right\\|=O(1)$, and $p_{n}^{2} \\rho^{-1}(n) / n \\rightarrow 0, \\hat{\\boldsymbol{\\theta}}$ is asymptotically normal.\n\nThe design conditions (15) are relatively mild and only the first and fourth are required for consistency. The second requires a weak form of isotropy. The third is a tail condition. The fourth and fifth require some form of concentration. All conditions are easily satisfied for, e.g., independent sub-Gaussian variables, for which $\\rho(x)=\\exp (x)$ works, and the growth bound for asymptotic normality becomes $p_{n}^{2} \\ln n / n \\rightarrow 0$. In contrast to the results of Portnoy (1985), the corollary also applies to covariates with heavier tails. For example, taking $\\rho(x)=x^{2}$ (which gives a fourth moment condition), the estimator is asymptotically normal as long as $p_{n}^{4} / n \\rightarrow 0$. For the least squares estimator, $\\psi^{\\prime}$ is constant, so the eigenvalue condition (16) simplifies to $\\lambda_{\\min }\\left(\\mathbb{E}\\left[\\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top}\\right]\\right) \\geqslant c$. This simplification applies more generally if $\\psi^{\\prime}$ is uniformly bounded away from zero.\n\nNow we turn to penalized estimation. We consider the Group Lasso penalty from Example 3.3, noting that this includes the usual Lasso as a special case. Now, $\\boldsymbol{\\theta}_{(2)}$ consists of all groups $G_{k}$ with $\\boldsymbol{\\theta}_{G_{k}}^{*}=0$.\n\nCorollary 2. Suppose the reduced problem (11) satisfies the conditions from Corollary 1 with some function $\\rho$ with $\\rho(x)=O(\\exp (x))$, and\n\n$$\n\\max _{1 \\leqslant k \\leqslant p_{n}} \\mathbb{E}\\left[\\rho\\left(X_{i, k}^{2} \\psi\\left(Y_{i}, \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{\\theta}^{*}\\right)^{2}\\right)\\right]=O(1)\n$$\n\nLet\n\n$$\n\\sup _{\\boldsymbol{\\theta} \\in \\mathcal{G}^{\\prime}} \\mid \\mathbb{E}\\left[\\psi^{\\prime}\\left(Y_{i}, \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{\\theta}\\right) \\boldsymbol{X}_{(2)} \\boldsymbol{X}_{(1)}^{\\top}\\right] \\mathbb{E}\\left[\\psi^{\\prime}\\left(Y_{i}, \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{\\theta}\\right) \\boldsymbol{X}_{(1)} \\boldsymbol{X}_{(1)}^{\\top}\\right]^{-1} \\|_{x} \\leqslant \\alpha\n$$\n\nfor some $\\alpha \\in[0,1)$, suppose that $\\sigma^{2}=\\max _{1 \\leqslant k \\leqslant p_{n}} \\mathbb{E}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)_{k}^{2}\\right]$ is bounded away from zero and infinity, either $\\boldsymbol{\\theta}_{G_{i}}^{*}=\\mathbf{0}$ or $\\left\\|\\boldsymbol{\\theta}_{G_{i}}^{*}\\right\\| / \\tilde{r}_{n} \\rightarrow \\infty$ with $\\tilde{r}_{n}=\\sqrt{s_{n} \\ln p_{n} / n}$ and\n\n$$\n\\lambda_{n} \\geqslant \\frac{8}{1-\\alpha} \\sqrt{\\frac{\\sigma^{2} \\ln p_{n}}{n}}\n$$\n\nThen, if\n\n$$\ns_{n} \\ln p_{n}=o(\\sqrt{n}), \\quad p_{n}=o\\left(\\frac{\\rho\\left(n /\\left(s_{n} \\ln p_{n}\\right)^{2}\\right)}{n}\\right)\n$$\n\nthe Group-Lasso-penalized equation has a solution $\\hat{\\boldsymbol{\\theta}}$ with $\\hat{\\boldsymbol{\\theta}}_{(2)}=\\mathbf{0}$ with probability tending to 1, $\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{s_{n} \\ln p_{n} / n}\\right)$, and $\\hat{\\boldsymbol{\\theta}}_{(1)}$ is asymptotically normal (with $\\left\\|A_{n}\\right\\|=O(1)$ ). If $\\left|\\psi^{\\prime}\\right|$ is uniformly bounded away from 0 and $\\lambda_{\\min }\\left(\\mathbb{E}\\left[\\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top}\\right]\\right) \\geqslant c>0$, this solution is unique with probability tending to 1 .\n\nThe moment conditions in (15) and (17) constrain the growth of $p_{n}$ through the function $\\rho$. Assuming $s_{n}=O(1)$ for simplicity, the choice $\\rho(x)=\\exp (x)$ allows $p_{n} \\sim \\exp \\left(n^{1 / 2-\\varepsilon}\\right)$ for any $\\varepsilon>0$. Similarly, polynomial moment bounds translate into polynomial growth conditions on $p_{n}$. For example, the choice $\\rho(x)=x^{3}$ requires sixth moments and allows $p_{n} \\sim n^{2-\\varepsilon}$.\n\nThe two corollaries also apply to non-parametric regression problems, in which $\\boldsymbol{X}_{i}$ consists of appropriate basis functions. For example, Corollary 2 implies consistency of Group-Lasso-assisted variable selection in high-dimensional nonparametric additive models (Huang et al., 2010) under appropriate conditions.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# 4.1.2 A problem where the classical RSC condition fails \n\nIn Section 3.3.2 we argued that the typical RSC condition (10) is too strong for general non-linear models. We now provide a simple example where the RSC condition fails, but the conditions of our theorems are easy to verify.\n\nLet $\\boldsymbol{X}_{1}, \\ldots, \\boldsymbol{X}_{n} \\in \\mathbb{R}^{p_{n} \\times p_{n}}$ be iid random matrices, each having independent Gaussian entries with $\\mathbb{E}\\left[\\boldsymbol{X}_{i}\\right]=I_{p_{n}}, \\operatorname{Var}\\left[X_{i, j, k}\\right]=1,1 \\leqslant j, k \\leqslant p_{n}$. Consider the estimator\n\n$$\n\\hat{\\boldsymbol{\\theta}}=\\underset{\\boldsymbol{\\theta}}{\\arg \\min } \\boldsymbol{\\theta}^{\\top}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{X}_{i}\\right) \\boldsymbol{\\theta}+\\lambda_{n}\\|\\boldsymbol{\\theta}\\|_{1}\n$$\n\nIgnoring the penalty, the loss function is a quadratic form, but convex only in expectation. The corresponding identifying function in our framework is $\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)=\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right)=-\\boldsymbol{X}_{i} \\boldsymbol{\\theta}$, so $\\Phi_{n}(\\boldsymbol{\\theta})=-\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{X}_{i} \\boldsymbol{\\theta}$. The unique solution to the population equation $\\mathbb{E}\\left[\\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]=\\mathbf{0}$ is $\\boldsymbol{\\theta}^{*}=\\mathbf{0}$. Condition (A4) is straightforward to establish for $H_{n}\\left(\\boldsymbol{X}_{i}\\right)=-\\boldsymbol{X}_{i}$ using standard results for independent Gaussian random variables. Condition (A5) is trivial since $\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)=\\mathbf{0}$. This implies the following result.\n\nCorollary 3. Consider the setting above with $\\eta_{n}=\\sqrt{\\ln p_{n} / n}, \\ln p_{n}=o(\\sqrt{n}), \\lambda_{n}=O\\left(\\eta_{n}\\right)$, and let $\\nu_{n}=O\\left(\\left(n / \\ln p_{n}\\right)^{1 / 2-\\delta}\\right)$ for some $\\delta>0$. With probability tending to 1 , the cones $\\Theta^{*}\\left(\\nu_{n}\\right)=\\left\\{\\boldsymbol{\\theta}:\\|\\boldsymbol{\\theta}\\|_{1} \\leqslant \\sqrt{\\nu_{n}}\\|\\boldsymbol{\\theta}\\|_{2}\\right\\}$ contain a unique solution $\\hat{\\boldsymbol{\\theta}}$ to the penalized estimation problem satisfying\n\n$$\n\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{\\nu_{n}} \\eta_{n}\\right)\n$$\n\nOn the other hand, the RSC condition (10) usually made in the literature can only hold with vanishing probability.\n\nProposition 1. Let $\\eta_{n}=\\sqrt{\\ln p_{n} / n}, p_{n} \\geqslant n, \\ln p_{n}=o(\\sqrt{n})$. Then, for any $c, c_{1}>0$, the event\n\n$$\n\\left\\langle\\boldsymbol{u}, \\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}\\right)-\\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right\\rangle \\geqslant c\\|\\boldsymbol{u}\\|_{2}^{2}-c_{1}\\|\\boldsymbol{u}\\|_{1}^{2} \\eta_{n}^{2}, \\quad \\forall \\boldsymbol{u} \\text { with }\\|\\boldsymbol{u}\\|_{1} \\leqslant 1\n$$\n\nhas probability tending to zero as $n \\rightarrow \\infty$.\nConsequently, the theory developed in, e.g., Negahban et al. (2012), Loh and Wainwright (2015), Wainwright (2019) does not apply. The estimation problem (20) is still quite simple, but it lacks the semidefinite, low-rank structure of the Hessian typically exploited in proofs of the RSC condition (10). We should therefore expect the classical RSC condition to fail in many non-linear settings not exhibiting this structure.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# 4.2 Multi-Sample Estimation \n\nConsider a multi-sample estimation problem: The data are given by $\\left(k_{1}, \\boldsymbol{X}_{1}\\right), \\ldots,\\left(k_{n}, \\boldsymbol{X}_{n}\\right)$, where $k_{i} \\in\\left\\{1, \\ldots, K_{n}\\right\\}$ indicates to which of the $K_{n}$ samples $\\boldsymbol{X}_{i}$ belongs. Assume for simplicity that $p_{n}=K_{n}$ and that each $\\theta_{k}$ is estimated using only the $k$-th sample. The estimation function can be written as\n\n$$\n\\phi\\left(k_{i}, \\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right)=\\left(\\begin{array}{c}\n\\mathbb{1}\\left\\{k_{i}=1\\right\\} \\frac{n}{n_{1}} \\phi_{1}\\left(\\boldsymbol{X}_{i} ; \\theta_{1}\\right) \\\\\n\\vdots \\\\\n\\mathbb{1}\\left\\{k_{i}=K_{n}\\right\\} \\frac{n}{n_{K_{n}}} \\phi_{K_{n}}\\left(\\boldsymbol{X}_{i} ; \\theta_{K_{n}}\\right)\n\\end{array}\\right)\n$$\n\nwhere $n_{k}=\\sum_{i=1}^{n} \\mathbb{1}\\left\\{k_{i}=k\\right\\}$ denotes the sample size of the $k$-th sample. The standardization $n / n_{k}$ is necessary to ensure that the eigenvalues of $n^{-1} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]$ are bounded away from 0 . The Jacobian of $\\phi$ is a diagonal matrix given by\n\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\phi\\left(k_{i}, \\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right)=\\operatorname{diag}\\left(\\mathbb{1}\\left\\{k_{i}=k\\right\\} \\frac{n}{n_{k}} \\phi_{k}^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}\\right)\\right)_{k=1, \\ldots, K_{n}}\n$$\n\nwhere $\\phi_{k}^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{k}\\right)=\\frac{\\varepsilon}{\\varepsilon \\theta_{k}} \\phi_{k}\\left(\\boldsymbol{X} ; \\theta_{k}\\right)$. Straightforward computations give\n\n$$\nI\\left(\\boldsymbol{\\theta}^{*}\\right)=\\operatorname{diag}\\left(\\frac{n}{n_{k}} \\mathbb{E}\\left[\\phi_{k}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)^{2}\\right]\\right)_{k=1, \\ldots, K_{n}}\n$$\n\nwhich yields $r_{n}=O\\left(\\left(\\sum_{k=1}^{K_{n}} n_{k}^{-1}\\right)^{1 / 2}\\right)$ assuming that $\\mathbb{E}\\left[\\phi_{k}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)^{2}\\right]$ is bounded for each $n$ and $k$. This framework can easily be extended to multiple parameters $\\boldsymbol{\\theta}_{k} \\in \\mathbb{R}^{p_{k}}$, potentially shared across subsamples. In the following examples, we stick to the single-parameter case for simplicity.\n\nRemark 8. There is an alternative framework for modelling multi-sample problems. One could model the data as iid draws from a mixture distribution with $K_{n}$ components, where the $k$-th component has weight $\\pi_{k}=n_{k} / n$. The main difference in this approach is that the sizes of the sub-samples are random, but it otherwise leads to essentially the same results.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "# 4.2.1 Example: Distributed Inference \n\nAn interesting application arises in distributed inference. Here, iid data is distributed over $K_{n}$ different locations, and the goal is to estimate a parameter $\\theta^{*} \\in \\mathbb{R}$ from the distributed data. This is a common setup in federated learning, where data is distributed over different devices, and the goal is to estimate a common model. Our setup further allows for differing sample sizes and population characteristics between locations. This may happen if, for example, the data is collected over hospitals who may share their estimate but not the data for privacy reasons. The distributed estimates $\\hat{\\theta}_{k}$ can be reconciled into a global estimate through averaging $\\hat{\\theta}_{K_{n}+1}=K_{n}^{-1} \\sum_{k=1}^{K_{n}} \\hat{\\theta}_{k}$. To put this in our framework, we stack the individual estimating equations $\\phi_{k}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}\\right)=\\psi\\left(\\boldsymbol{X}_{i} ; \\theta_{k}\\right)$ as above, and append the reconciliation function\n\n$$\n\\phi_{K_{n}+1}\\left(\\boldsymbol{X}_{i} ; \\theta\\right)=\\frac{1}{K_{n}} \\sum_{k=1}^{K_{n}} \\theta_{k}-\\theta_{K_{n}+1}\n$$\n\nCorollary 4. Let $\\Theta_{0} \\subseteq \\mathbb{R}$ and $n_{1}=\\cdots=n_{K_{n}}=n / K_{n}$. Suppose that, for $\\theta \\in \\Theta_{0}, \\psi(\\boldsymbol{x} ; \\theta)$ is uniformly bounded and $\\psi^{\\prime}(\\boldsymbol{x} ; \\theta)=\\widehat{c}_{\\theta} \\psi(\\boldsymbol{x} ; \\theta)$ is negative, uniformly bounded away from 0 and $-\\infty$ and Lipschitz in $\\theta$. Then, if $K_{n}^{3} / n \\rightarrow 0$, it holds\n\n$$\n\\begin{aligned}\n\\sqrt{n / K_{n}}\\left(\\hat{\\theta}_{k}-\\theta_{k}^{*}\\right) & \\rightarrow N\\left(0, \\sigma_{k}^{2}\\right), \\quad k=1, \\ldots, K_{n} \\\\\n\\text { and } \\quad \\sqrt{n}\\left(\\hat{\\theta}_{K_{n}+1}-\\theta^{*}\\right) & \\rightarrow N\\left(0, \\sigma^{2}\\right)\n\\end{aligned}\n$$\n\nwith\n\n$$\n\\sigma_{k}^{2}=\\frac{\\mathbb{E}\\left[\\psi\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)^{2}\\right]}{\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)\\right]^{2}}, \\quad \\sigma^{2}=\\lim _{n \\rightarrow \\infty} \\frac{1}{K_{n}} \\sum_{k=1}^{K_{n}} \\sigma_{k}^{2}\n$$\n\nIf the samples have identical distributions, $\\sigma_{1}=\\cdots=\\sigma_{K_{n}}$, the averaged estimator is as efficient as one computed from the pooled sample. If the samples have different distributions, however, there might be a loss in efficiency.\n\nWhile for finite $K_{n}$, one would easily obtain the asymptotic normality of $\\hat{\\theta}_{K_{n}+1}$ from the asymptotic normality of the independent $\\hat{\\theta}_{k}$, our results provide conditions on the rate of growth of $K_{n}$ when $K_{n} \\rightarrow \\infty$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "### 4.2.2 Example: Quality Control\n\nTo give an example where our penalized results are useful, consider the following statistical quality control problem. We have $K_{n}$ machines producing items, and we want to ensure that the items maintain a prescribed quality level. The quality of an item $i$ produced by machine $k_{i}$ is given by $q_{k_{i}}\\left(\\boldsymbol{X}_{i}\\right)$. A machine is considered OK if\n\n$$\n\\mathbb{E}\\left[q_{k}\\left(\\boldsymbol{X}_{i}\\right) \\mid k_{i}=k\\right]=a\n$$\n\nwhere $a$ is the targeted quality. To detect potentially faulty machines, define parameters $\\theta_{k}^{*}$ such that\n\n$$\n\\theta_{k}^{*}=\\mathbb{E}\\left[q_{k}\\left(\\boldsymbol{X}_{i}\\right) \\mid k_{i}=k\\right]-a\n$$\n\nwhich can be estimated using $\\phi_{k}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}\\right)=q_{k}\\left(\\boldsymbol{X}_{i}\\right)-a-\\theta_{k}$ in the above setup. To only detect faulty machines, we can penalize the equation using, e.g., the Lasso penalty $p_{\\lambda_{n}}(\\boldsymbol{\\theta})=\\lambda_{n} \\sum_{k=1}^{K_{n}}\\left|\\theta_{k}\\right|$. The following corollary guarantees that the penalized estimator $\\hat{\\boldsymbol{\\theta}}$ detects all faulty machines with probability tending to 1 .\n\nCorollary 5. In the above quality control example, suppose that (A5) holds and\n\n$$\n\\lambda_{n} \\geqslant 8 \\max _{1 \\leqslant k \\leqslant K_{n}} \\max _{1 \\leqslant i \\leqslant n} \\sqrt{\\frac{\\mathbb{E}\\left[\\left(q_{k}\\left(\\boldsymbol{X}_{i}\\right)-a-\\theta_{k}^{*}\\right)^{2}\\right] \\ln K_{n}}{n_{k}}}\n$$\n\nSuppose further that $s_{n}^{2} \\ln K_{n} / \\min _{k} n_{k} \\rightarrow 0$ and either $\\theta_{k}^{*}=0$ or $\\theta_{k}^{*} / \\tilde{r}_{n} \\rightarrow \\infty$ with $\\tilde{r}_{n}=\\sqrt{s_{n}} \\lambda_{n}$ for all $k$. Then\n\n$$\n\\mathbb{P}\\left(\\left\\{k: \\hat{\\theta}_{k} \\neq 0\\right\\}=\\left\\{k: \\theta_{k}^{*} \\neq 0\\right\\}\\right) \\rightarrow 1\n$$\n\nNote that we do not require the samples from the machines to be identically distributed, so the result also applies to situations where different types of machines are used or the machine only fails after some time.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# 4.3 Stepwise Estimation \n\nAnother setting that shows the flexibility of our results is stepwise estimation. Assume the parameter vector can be grouped as $\\boldsymbol{\\theta}=\\left(\\boldsymbol{\\theta}_{1}, \\ldots, \\boldsymbol{\\theta}_{K_{n}}\\right)$. The parameters are estimated sequentially using the estimates $\\hat{\\boldsymbol{\\theta}}_{1}, \\ldots, \\hat{\\boldsymbol{\\theta}}_{k-1}$ from previous iterations:\n\n$$\n\\hat{\\boldsymbol{\\theta}}_{k}=\\underset{\\theta_{k}}{\\arg \\max } \\sum_{i=1}^{n} f_{k}\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}_{k}, \\hat{\\boldsymbol{\\theta}}_{1}, \\ldots, \\hat{\\boldsymbol{\\theta}}_{k-1}\\right), \\quad \\boldsymbol{\\theta}_{k}^{*}=\\underset{\\boldsymbol{\\theta}_{k}}{\\arg \\max } \\mathbb{E}\\left[f_{k}\\left(\\boldsymbol{X} ; \\boldsymbol{\\theta}_{k}, \\boldsymbol{\\theta}_{1}^{*}, \\ldots, \\boldsymbol{\\theta}_{k-1}^{*}\\right)\\right]\n$$\n\nfor some functions $f_{k}$. Denote $\\phi_{k}\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}_{k}, \\boldsymbol{\\theta}_{1}, \\ldots, \\boldsymbol{\\theta}_{k-1}\\right)=\\nabla_{\\boldsymbol{\\theta}_{k}} f_{k}\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}_{k}, \\boldsymbol{\\theta}_{1}, \\ldots, \\boldsymbol{\\theta}_{k-1}\\right)$. Then, the sequential estimator $\\hat{\\boldsymbol{\\theta}}$ can be expressed as the solution of $\\sum_{i=1}^{n} \\phi\\left(\\boldsymbol{X}_{i} ; \\hat{\\boldsymbol{\\theta}}\\right)=\\mathbf{0}$ with\n\n$$\n\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}\\right)=\\left(\\begin{array}{c}\n\\phi_{1}\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}_{1}\\right) \\\\\n\\phi_{2}\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}_{2}, \\boldsymbol{\\theta}_{1}\\right) \\\\\n\\vdots \\\\\n\\phi_{K_{n}}\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}_{K_{n}}, \\boldsymbol{\\theta}_{1}, \\ldots, \\boldsymbol{\\theta}_{K_{n}-1}\\right)\n\\end{array}\\right)\n$$\n\nand $\\boldsymbol{\\theta}^{*}$ is the solution of $\\mathbb{E}\\left[\\phi\\left(\\boldsymbol{X} ; \\boldsymbol{\\theta}^{*}\\right)\\right]=\\mathbf{0}$.\nAn interesting application appears in vine copula models, which are hierarchical decompositions of multivariate densities (Czado, 2019, Czado and Nagler, 2022). For a $d_{n}$-variate density, there are $K_{n}=O\\left(d_{n}^{2}\\right)$ building blocks in the decomposition, whose parameters are typically estimated sequentially (Aas et al., 2009, Haff, 2013). To best of our knowledge, there is no asymptotic theory available for $d_{n} \\rightarrow \\infty$. Due to the complexity of these models, we leave a detailed treatment to a follow-up paper.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "### 4.3.1 Example: Causal Inference\n\nAs a concrete example, suppose we want to estimate the causal effect of some covariates $\\boldsymbol{Z}$ on an outcome $Y$ in the presence of confounders $\\boldsymbol{C}$ from iid observational data. Part of the population has received a treatment, which we indicate by the binary treatment indicator $T$. Under the usual conditions for no unmeasured confounding, the conditional average treatment effect (CATE) can be defined as\n\n$$\n\\operatorname{CATE}(\\boldsymbol{z})=\\mathbb{E}\\left[\\frac{Y T}{\\mathbb{P}(T=1 \\mid \\boldsymbol{W})}-\\frac{Y(1-T)}{\\mathbb{P}(T=0 \\mid \\boldsymbol{W})} \\mid \\boldsymbol{Z}=\\boldsymbol{z}\\right]\n$$\n\nwhere $\\boldsymbol{W}=(\\boldsymbol{Z}, \\boldsymbol{C})$; see for example Huber (2023). Now we model the treatment probabilities and CATE by\n\n$$\n\\mathbb{P}(T=1 \\mid \\boldsymbol{w})=\\sigma\\left(\\boldsymbol{w}^{\\top} \\boldsymbol{\\theta}_{1}\\right), \\quad \\operatorname{CATE}(\\boldsymbol{z})=\\boldsymbol{z}^{\\top} \\boldsymbol{\\theta}_{2}\n$$\n\nwhere $\\sigma$ is an appropriate link function. The parameters can be estimated by first estimating $\\boldsymbol{\\theta}_{1}$ using maximum-likelihood, and then estimating $\\boldsymbol{\\theta}_{2}$ by the plug-in least squares estimator\n\n$$\n\\hat{\\boldsymbol{\\theta}}_{2}=\\arg \\min _{\\boldsymbol{\\theta}_{2}} \\sum_{i=1}^{n}\\left[\\frac{Y_{i} T_{i}}{\\sigma\\left(\\boldsymbol{W}_{i}^{\\top} \\hat{\\boldsymbol{\\theta}}_{1}\\right)}-\\frac{Y_{i}\\left(1-T_{i}\\right)}{1-\\sigma\\left(\\boldsymbol{W}_{i}^{\\top} \\hat{\\boldsymbol{\\theta}}_{1}\\right)}-\\boldsymbol{Z}_{i}^{\\top} \\boldsymbol{\\theta}_{2}\\right]^{2}\n$$\n\nThis step-wise procedure can be reformulated as solving the estimating equation\n\n$$\n\\sum_{i=1}^{n} \\phi\\left(Y_{i}, T_{i}, \\boldsymbol{Z}_{i}, \\boldsymbol{W}_{i} ; \\hat{\\boldsymbol{\\theta}}\\right)=\\mathbf{0}\n$$\n\nwith\n\n$$\n\\phi\\left(Y_{i}, T_{i}, \\boldsymbol{Z}_{i}, \\boldsymbol{W}_{i} ; \\boldsymbol{\\theta}\\right)=\\binom{\\nabla_{\\boldsymbol{\\theta}_{1}}\\left[T_{i}\\left(\\ln \\sigma\\left(\\boldsymbol{W}_{i}^{\\top} \\boldsymbol{\\theta}_{1}\\right)+\\left(1-T_{i}\\right) \\ln \\left[1-\\sigma\\left(\\boldsymbol{W}_{i}^{\\top} \\boldsymbol{\\theta}_{1}\\right)\\right]\\right]}{-\\left[\\frac{Y_{i} T_{i}}{\\sigma\\left(\\boldsymbol{W}_{i}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}-\\frac{Y_{i}\\left(1-T_{i}\\right)}{1-\\sigma\\left(\\boldsymbol{W}_{i}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}-\\boldsymbol{Z}_{i}^{\\top} \\boldsymbol{\\theta}_{2}\\right] \\boldsymbol{Z}_{i}}\n$$\n\nCorollary 6. Suppose that $\\sigma$ is bounded away from zero and 1 and twice continuously differentiable with uniformly bounded derivatives. Suppose further that $|Y| \\leqslant 1$ and $\\boldsymbol{W} \\in \\mathbb{R}^{p_{n}}$ satisfies the design conditions from (15). Let $\\bar{\\sigma}=1-\\sigma$ and define\n\n$$\n\\begin{aligned}\n\\alpha_{1}(T, \\boldsymbol{W}) & =\\sup _{\\boldsymbol{\\theta} \\in \\Theta_{n}}\\left[T(\\ln \\sigma)^{\\prime \\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)+(1-T)(\\ln \\bar{\\sigma})^{\\prime \\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)\\right] \\\\\n\\alpha_{2}(T, Y, \\boldsymbol{W}) & =\\sup _{\\boldsymbol{\\theta} \\in \\Theta_{n}}\\left|\\frac{T Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}{2 \\sigma\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}}\\right|+\\left|\\frac{(1-T) Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}{2 \\bar{\\sigma}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}}\\right|-1\n\\end{aligned}\n$$\n\nand suppose there is $c>0$ such that\n\n$$\n\\lambda_{\\min }\\left(\\mathbb{E}\\left[\\alpha_{1}(T, \\boldsymbol{W}) \\boldsymbol{W} \\boldsymbol{W}^{\\top}\\right]\\right) \\leqslant-c, \\quad \\lambda_{\\min }\\left(\\mathbb{E}\\left[\\alpha_{2}(T, Y, \\boldsymbol{W}) \\boldsymbol{Z} \\boldsymbol{Z}^{\\top}\\right]\\right) \\leqslant-c\n$$\n\nThen, if $p_{n} \\ln p_{n} / n \\rightarrow 0$, the estimating equation has a unique solution $\\hat{\\boldsymbol{\\theta}}$ on $\\Theta_{n}$ with\n\n$$\n\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{\\frac{p_{n}}{n}}\\right)\n$$\n\nIf $p_{n}^{2} \\rho^{-1}(n) / n \\rightarrow 0$ (with $\\rho$ as defined in (15)), $\\hat{\\boldsymbol{\\theta}}$ is also asymptotically normal.\nThe matrices inside the expectations of the eigenvalue condition are the blocks of a block-diagonal matrix $H_{n}$ constructed in the proof. The condition is easiest to verify if $\\ln \\sigma$ is concave, which is the case for the logistic and probit link functions.\n\nIf the number of covariates or confounders is large, we may want to add a sparsity penalty $p_{\\lambda_{n}}(\\boldsymbol{\\theta})$. For simplicity, suppose that the parameters are reordered such that $\\boldsymbol{\\theta}^{*}=\\left(\\boldsymbol{\\theta}_{(1)}^{*}, \\mathbf{0}\\right)$. The following corollary guarantees that the SCAD-penalized estimator $\\hat{\\boldsymbol{\\theta}}$ is consistent and asymptotically normal.\n\nCorollary 7. Suppose that the regularity conditions from Corollary 6 hold, $\\boldsymbol{W}$ satisfies the design conditions (17) (with $|\\psi|$ uniformly bounded), $\\sqrt{n / s_{n}} \\min _{1 \\leqslant k \\leqslant s_{n}}\\left|\\theta_{k}^{*}\\right| \\rightarrow \\infty$, and that the SCAD penalty is used with $a>1+\\frac{1}{2 c}$ and\n\n$$\n\\lambda_{n} \\geqslant 8 \\sigma_{n} \\sqrt{\\frac{\\ln p_{n}}{n}}\n$$\n\nSuppose that $s_{n} \\ln p_{n}=o(\\sqrt{n})$ and $p_{n}=o\\left(\\rho\\left(n /\\left(s_{n} \\ln p_{n}\\right)^{2}\\right) / n\\right)$. Then, with probability tending to 1 , the penalized equation has a unique solution $\\hat{\\boldsymbol{\\theta}}$ on $\\Theta_{n}$ with $\\hat{\\boldsymbol{\\theta}}_{(2)}=\\mathbf{0}$ and $\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{s_{n} / n}\\right)$. Additionally, $\\hat{\\boldsymbol{\\theta}}_{(1)}$ is asymptotically normal and as efficient as the oracle solution.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 19,
      "text": "# 4.3.2 Example: Stochastic Optimization \n\nOur general results also apply to cases where $K_{n} \\rightarrow \\infty$. Such situations arise frequently in the analysis of iterative procedures, such as gradient descent or boosting algorithms. To illustrate this, suppose we want to learn a parameter $\\theta_{\\infty}^{*} \\in \\mathbb{R}$ solving\n\n$$\n\\mathbb{E}\\left[f\\left(\\boldsymbol{X} ; \\theta_{\\infty}^{*}\\right)\\right]=0\n$$\n\nLet $\\theta_{0}^{*} \\in \\mathbb{R}$ be an initial value and define the iterative solutions\n\n$$\n\\theta_{k}^{*}=\\theta_{k-1}^{*}-\\alpha \\mathbb{E}\\left[f\\left(\\boldsymbol{X} ; \\theta_{k-1}^{*}\\right)\\right]\n$$\n\nUnder appropriate conditions on the learning rate $\\alpha$ and smoothness of $f$, the sequence $\\theta_{k}^{*}$ can be shown to converge geometrically fast to $\\theta_{\\infty}^{*}$ as $k \\rightarrow \\infty$. Now define $\\hat{\\theta}_{0}=\\theta_{0}^{*}$ and $\\hat{\\theta}_{k}, 1 \\leqslant k \\leqslant K_{n}$, as the solutions of the batched sample equation\n\n$$\n\\hat{\\theta}_{k}=\\hat{\\theta}_{k-1}-\\alpha \\frac{K_{n}}{n} \\sum_{i \\in \\mathcal{B}_{k}} f\\left(\\boldsymbol{X}_{i} ; \\hat{\\theta}_{k-1}\\right)\n$$\n\nwhere $\\boldsymbol{X}_{1}, \\ldots, \\boldsymbol{X}_{n}$ are iid samples from the distribution of $\\boldsymbol{X}$, and $\\mathcal{B}_{1} \\cup \\cdots \\cup \\mathcal{B}_{K_{n}}=\\{1, \\ldots, n\\}$ with $\\left|\\mathcal{B}_{k}\\right|=n / K_{n}$ is a partition of the sample indices. We can define the entire iteration path $\\hat{\\boldsymbol{\\theta}}$ as the solution of a single estimating equation $n^{-1} \\sum_{i=1}^{n} \\phi\\left(\\boldsymbol{X}_{i} ; \\hat{\\boldsymbol{\\theta}}\\right)=\\mathbf{0}$ with\n\n$$\n\\phi(\\boldsymbol{X} ; \\boldsymbol{\\theta})=\\left(\\begin{array}{c}\nK_{n} \\mathbb{1}_{i \\in \\mathcal{B}_{1}}\\left[\\theta_{0}^{*}-\\theta_{1}-\\alpha f\\left(\\boldsymbol{X}_{i} ; \\theta_{0}^{*}\\right)\\right] \\\\\n\\vdots \\\\\nK_{n} \\mathbb{1}_{i \\in \\mathcal{B}_{K_{n}}}\\left[\\theta_{K_{n}-1}-\\theta_{K_{n}}-\\alpha f\\left(\\boldsymbol{X}_{i} ; \\theta_{K_{n}-1}\\right)\\right]\n\\end{array}\\right)\n$$\n\nand similarly for the population version. The following is a possible result under simple conditions.\nCorollary 8. Let $f^{\\prime}(\\boldsymbol{x} ; \\theta)=\\partial_{\\theta} f(\\boldsymbol{x} ; \\theta)$. Suppose that $K_{n}^{3} / n \\rightarrow 0,\\left\\|\\boldsymbol{\\theta}^{*}\\right\\|_{\\infty}=O(1), \\sup _{\\theta \\in \\Theta_{n}} \\mathbb{E}\\left[f(\\boldsymbol{X} ; \\theta)^{4}\\right]=$ $O(1), f^{\\prime} \\in[\\kappa, L]$ and $\\left|f^{\\prime \\prime}\\right| \\leqslant L$ for some $\\kappa, L \\in(0, \\infty)$, and that $0<\\alpha \\leqslant 1 / L$. Then\n\n$$\n\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\frac{K_{n}}{\\sqrt{n}}\\right)\n$$\n\nand $\\hat{\\boldsymbol{\\theta}}$ is unique with probability tending to 1 , and for any $A_{n} \\in \\mathbb{R}^{q \\times K_{n}}$ with $\\left\\|A_{n}\\right\\|=O(1)$, we have\n\n$$\n\\sqrt{n / K_{n}} A_{n}\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right) \\rightarrow_{d} \\mathcal{N}\\left(0, \\lim _{n \\rightarrow \\infty} A_{n} \\Sigma_{n} A_{n}^{\\top}\\right)\n$$\n\nwhere $\\Sigma_{n}$ is symmetric and, with the convention $\\prod_{j=i}^{i-1} a_{j}=1$, it holds for $i \\leqslant j$,\n\n$$\n\\Sigma_{i, j}=\\alpha^{2} \\sum_{k=1}^{i} \\operatorname{Var}\\left[f\\left(\\boldsymbol{X} ; \\theta_{k-1}^{s}\\right)\\right]\\left[\\prod_{m=k}^{i-1}\\left(1-\\alpha \\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{m}^{s}\\right)\\right]\\right)\\right]^{2}\\left[\\prod_{m=i}^{j-1}\\left(1-\\alpha \\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{m}^{s}\\right)\\right]\\right)\\right]\n$$\n\nThe corollary couples the iterates on the sample equation to the iterates on the population equation. The first part shows that the iteration paths are globally close to one another. The second part shows that finite-dimensional linear summaries of the solution path converge to a Gaussian limit. Several interesting special cases arise from particular choices of the sequence $A_{n}$. For example, $A_{n}=(0, \\ldots, 0,1)$ implies $\\sqrt{n / K_{n}}$-convergence of the final iterate and $A_{n}=$ $\\left(1 / \\sqrt{K_{n}}, \\ldots, 1 / \\sqrt{K_{n}}\\right)$ gives $\\sqrt{n}$-convergence of the averaged iterate. Another interesting choice is $A_{n}=\\left(\\boldsymbol{e}_{\\left[K_{n} / q\\right]}, \\boldsymbol{e}_{\\left[2 K_{n} / q\\right]}, \\ldots, \\boldsymbol{e}_{\\left[q K_{n} / q\\right]}\\right)^{\\top}$, where $\\boldsymbol{e}_{k}$ is the $k$ th standard unit vector. This corresponds to a discretized approximation of the process $\\hat{\\theta}(t)=\\hat{\\theta}_{\\left[t K_{n}\\right]}, t \\in[0,1]$. One can verify that for $n, K_{n} \\rightarrow \\infty$,\n\n$$\n\\sqrt{n / K_{n}} A_{n}\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right) \\rightarrow_{d} \\mathcal{N}(0, V)\n$$\n\nwith diagonal matrix $V$. This suggests that the solution path $\\hat{\\theta}(t)$ behaves like a white noise process around the population path $\\theta^{*}(t)$. The assumptions of the corollary can be relaxed in various ways using more sophisticated assumptions and arguments in the proof. For example, we may let the learning rate $\\alpha$ decay slowly or impose only probabilistic bounds on $f^{\\prime}$. Other iterative algorithms and multivariate versions can be handled similarly.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "# References \n\nAas, K., Czado, C., Frigessi, A., and Bakken, H. (2009). Pair-copula constructions of multiple dependence. Insurance: Mathematics and Economics, 44(2):182-198.\n\nB\u00fchlmann, P. and van de Geer, S. (2011). Statistics for High-Dimensional Data: Methods, Theory and Applications. Springer Series in Statistics. Springer Berlin Heidelberg.\n\nClarke, F. H. (1990). Optimization and Nonsmooth Analysis. SIAM.\nCzado, C. (2019). Analyzing Dependent Data with Vine Copulas: A Practical Guide With R. Lecture Notes in Statistics. Springer International Publishing.\n\nCzado, C. and Nagler, T. (2022). Vine copula based modeling. Annual Review of Statistics and Its Application, 9(1):453-477.\n\nFan, J. (1997). Comments on \"Wavelets in statistics: A review\" by A. Antoniadis. Journal of the Italian Statistical Society, 6(2):131.\n\nFan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348-1360.\n\nFan, J. and Lv, J. (2010). A selective overview of variable selection in high dimensional feature space. Statistica Sinica, 20(1):101-148. PMC3092303.\n\nFan, J. and Peng, H. (2004). Nonconcave penalized likelihood with a diverging number of parameters. The Annals of Statistics, 32(3):928 - 961.\n\nFierro, R., Ya\u00f1ez, C., and Morales, C. (2004). The aftermath of the intermediate value theorem. Fixed Point Theory and Applications, 2004.\n\nFrank, 1. E. and Friedman, J. H. (1993). A statistical view of some chemometrics regression tools. Technometrics, 35(2):109-135.\n\nHaff, I. H. (2013). Parameter estimation for pair-copula constructions. Bernoulli, 19(2):462 491 .\n\nHe, X. and Shao, Q.-M. (2000). On parameters of increasing dimensions. Journal of Multivariate Analysis, 73(1):120-135.\n\nHuang, J., Horowitz, J. L., and Wei, F. (2010). Variable selection in nonparametric additive models. The Annals of Statistics, 38(4):2282 - 2313.\n\nHuber, M. (2023). Causal Analysis: Impact Evaluation and Causal Machine Learning with Applications in R. MIT Press.\n\nHuber, P. J. (1973). Robust Regression: Asymptotics, Conjectures and Monte Carlo. The Annals of Statistics, 1(5):799 - 821.\n\nKnight, K. and Fu, W. (2000). Asymptotics for Lasso-type estimators. The Annals of Statistics, 28(5):1356 - 1378.\n\nLee, J. D., Sun, Y., and Taylor, J. E. (2013). On model selection consistency of penalized M-estimators: a geometric theory. Advances in Neural Information Processing Systems, 26.\n\nLi, G., Peng, H., and Zhu, L. (2011). Nonconcave penalized m-estimation with a diverging number of parameters. Statistica Sinica, 21(1):391-419.\n\nLoh, P.-L. (2017). Statistical consistency and asymptotic normality for high-dimensional robust M-estimators. The Annals of Statistics, 45(2):866-896.\n\nLoh, P.-L. and Wainwright, M. J. (2015). Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima. Journal of Machine Learning Research, 16(19):559616 .\n\nLoh, P.-L. and Wainwright, M. J. (2017). Support recovery without incoherence: A case for nonconvex regularization. The Annals of Statistics, 45(6):2455 - 2482.\n\nMammen, E. (1989). Asymptotics with Increasing Dimension for Robust Regression with Applications to the Bootstrap. The Annals of Statistics, 17(1):382 - 400.\n\nNegahban, S. N., Ravikumar, P., Wainwright, M. J., and Yu, B. (2012). A unified framework for high-dimensional analysis of $M$-estimators with decomposable regularizers. Statistical Science, 27(4).\n\nPortnoy, S. (1984). Asymptotic Behavior of $M$-Estimators of $p$ Regression Parameters when $p^{2} / n$ is Large. I. Consistency. The Annals of Statistics, 12(4):1298 - 1309.\n\nPortnoy, S. (1985). Asymptotic Behavior of $M$ Estimators of $p$ Regression Parameters when $p^{2} / n$ is Large; II. Normal Approximation. The Annals of Statistics, 13(4):1403 - 1417.\n\nPortnoy, S. (1986). On the central limit theorem in $\\mathbb{R}^{p}$ when $p \\rightarrow \\infty$. Probability Theory and Related Fields, 73(4):571-583.\n\nTalagrand, M. (2005). The Generic Chaining: Upper and Lower Bounds of Stochastic Processes. Springer Science \\& Business Media.\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267-288.\n\nTibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K. (2005). Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), $67(1): 91-108$.\n\nVan der Vaart, A. and Wellner, J. A. (2023). Weak Convergence and Empirical Processes: With Applications to Statistics. Springer Nature.\nvan der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.\n\nWainwright, M. J. (2019). High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.\n\nWelsh, A. H. (1989). On M-Processes and M-Estimation. The Annals of Statistics, 17(1):337 361 .\n\nYohai, V. J. and Maronna, R. A. (1979). Asymptotic Behavior of $M$-Estimators for the Linear Model. The Annals of Statistics, 7(2):258 - 268.\n\nYuan, M. and Lin, Y. (2006). Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67.\n\nZhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894 - 942.\n\nZhao, P. and Yu, B. (2006). On model selection consistency of lasso. Journal of Machine Learning Research, 7(90):2541-2563.\n\nZou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association, 101(476):1418-1429.\n\nZou, H. and Hastie, T. (2005). Regularization and Variable Selection Via the Elastic Net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301-320.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "# A Proofs of Main Results \n\nTo simplify the notation in the following proofs and results, we shall use the following notation:\n\n$$\n\\mathbb{P}_{n} f=\\frac{1}{n} \\sum_{i=1}^{n} f\\left(\\boldsymbol{X}_{i}\\right), \\quad P f=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[f\\left(\\boldsymbol{X}_{i}\\right)\\right]\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 22,
      "text": "## A. 1 Proof of Theorem 1\n\nWe first show that the sets $\\Theta_{n}$ contain a solution of the estimating equation (3) with probability tending to 1. From an extension of the intermediate value theorem in Fierro et al. (2004, Theorem 2.3), it follows that if\n\n$$\n\\sup _{\\|\\boldsymbol{u}\\|=1}\\left\\langle r_{n} C \\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+r_{n} C \\boldsymbol{u}\\right)\\right\\rangle \\leqslant 0\n$$\n\nholds, there is a solution $\\hat{\\boldsymbol{\\theta}}$ of $\\mathbb{P}_{n} \\phi(\\boldsymbol{\\theta})=\\mathbf{0}$ that satisfies $\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant r_{n} C$. We show that by choosing $C$ large enough, the probability that\n\n$$\n\\left(r_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1}\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+r_{n} C \\boldsymbol{u}\\right)\\right\\rangle \\leqslant-c<0\n$$\n\nholds for some $c>0$ becomes arbitrarily close to 1 . We have\n\n$$\n\\begin{aligned}\n& \\left(r_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1}\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+r_{n} C \\boldsymbol{u}\\right)\\right\\rangle \\\\\n\\leqslant & \\left(r_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1}\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\rangle+\\left(r_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1}\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+r_{n} C \\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\rangle\n\\end{aligned}\n$$\n\nThe first term is of order $O_{p}(1 / C)$, since\n\n$$\n\\sup _{\\|\\boldsymbol{u}\\|=1}\\left|\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\rangle\\right|=\\left\\|\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|=O_{p}\\left(n^{-1 / 2} \\sqrt{\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)\\right)}\\right)=O_{p}\\left(r_{n}\\right)\n$$\n\nby Lemma 5. Choosing $C$ large enough, it suffices that the second term remains below some $-c$ with probability going to 1 . It holds\n\n$$\n\\left(r_{n} C\\right)^{-1}\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+r_{n} C \\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\rangle \\leqslant \\mathbb{P}_{n}\\left[\\boldsymbol{u}^{\\top} H_{n} \\boldsymbol{u}\\right]=P\\left[\\boldsymbol{u}^{\\top} H_{n} \\boldsymbol{u}\\right]+\\left(\\mathbb{P}_{n}-P\\right)\\left[\\boldsymbol{u}^{\\top} H_{n} \\boldsymbol{u}\\right]\n$$\n\nwith $H_{n}$ as defined in assumption (A1)(i). Hence,\n\n$$\n\\left(r_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1}\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+r_{n} C \\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\rangle \\leqslant \\lambda_{\\max }\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right)+\\left\\|\\left(\\mathbb{P}_{n}-P\\right) H_{n}\\right\\|\n$$\n\nBy assumption (A1)(ii), we have $\\lambda_{\\max }\\left(n^{-1} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right) \\leqslant-c$ for some $c>0$ and large enough $n$, and Lemma 7 gives $\\left\\|\\left(\\mathbb{P}_{n}-P\\right) H_{n}\\right\\|=o_{p}(1)$. This proves (22).\n\nWe now show that every solution in $\\Theta_{n}$ must satisfy $\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant C r_{n}$ for some $C<\\infty$, with probability tending to 1 . Suppose this is not the case and define $C_{n}^{-1}=r_{n} /\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=o_{p}(1)$. Then, we can write $\\hat{\\boldsymbol{\\theta}}=\\boldsymbol{\\theta}^{*}+r_{n} C_{n} \\hat{\\boldsymbol{u}}$ with $\\|\\hat{\\boldsymbol{u}}\\|=1$. It holds\n\n$$\n\\begin{aligned}\n0 & =\\left(r_{n} C_{n}\\right)^{-1}\\left\\langle\\hat{\\boldsymbol{u}}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+r_{n} C_{n} \\hat{\\boldsymbol{u}}\\right)\\right\\rangle \\leqslant \\sup _{\\|\\boldsymbol{u}\\|=1}\\left(r_{n} C_{n}\\right)^{-1}\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+r_{n} C_{n} \\boldsymbol{u}\\right)\\right\\rangle \\\\\n& \\leqslant O_{p}\\left(C_{n}^{-1}\\right)-c+o_{p}(1)=-c+o_{p}(1)\n\\end{aligned}\n$$\n\nwhere the second inequality follows from the above arguments. Hence, $\\hat{\\boldsymbol{\\theta}}$ cannot be a solution with probability tending to 1 .",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 23,
      "text": "# A. 2 Proof of Theorem 2 \n\nThe claim is trivial when no solution exists. Otherwise, let $\\hat{\\boldsymbol{\\theta}}$ be any two solution to the estimating equation (1). By Theorem 1, we may assume that $\\left\\|\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant r_{n} C$ for some $C \\in(0, \\infty)$ and $n$ large enough. Suppose there is another solution $\\hat{\\boldsymbol{\\theta}}+\\boldsymbol{u}$. By the strengthened assumption (4),\n\n$$\n\\begin{aligned}\n\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi(\\hat{\\boldsymbol{\\theta}}+\\boldsymbol{u})\\right\\rangle=\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi(\\hat{\\boldsymbol{\\theta}}+\\boldsymbol{u})\\right\\rangle-\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi(\\hat{\\boldsymbol{\\theta}})\\right\\rangle & \\leqslant \\mathbb{P}_{n}\\left[\\boldsymbol{u}^{\\top} H_{n} \\boldsymbol{u}\\right] \\\\\n& =P\\left[\\boldsymbol{u}^{\\top} H_{n} \\boldsymbol{u}\\right]+\\left(\\mathbb{P}_{n}-P\\right)\\left[\\boldsymbol{u}^{\\top} H_{n} \\boldsymbol{u}\\right] \\\\\n& \\leqslant\\|\\boldsymbol{u}\\|^{2}\\left(-c+o_{p}(1)\\right)\n\\end{aligned}\n$$\n\nuniformly on the set $\\left\\{\\boldsymbol{u}:\\left\\|\\hat{\\boldsymbol{\\theta}}+\\boldsymbol{u}-\\boldsymbol{\\theta}^{*}\\right\\| \\leqslant r_{n} C\\right\\}$ using (A1) and Lemma 7. The right-hand side is strictly negative on the subset where $\\boldsymbol{u} \\neq \\mathbf{0}$ with probability tending to 1 , so it must hold $\\boldsymbol{u}=\\mathbf{0}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 24,
      "text": "## A. 3 Proof of Theorem 3\n\nWe have\n\n$$\n\\begin{aligned}\n\\mathbf{0} & =\\mathbb{P}_{n} \\phi(\\hat{\\boldsymbol{\\theta}})=\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)+\\mathbb{P}_{n}\\left[\\phi(\\hat{\\boldsymbol{\\theta}})-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right] \\\\\n& =\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)+P\\left[\\phi(\\hat{\\boldsymbol{\\theta}})-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]+\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\hat{\\boldsymbol{\\theta}})-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right] \\\\\n& =\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)+\\nabla_{\\boldsymbol{\\theta}} P \\phi(\\hat{\\boldsymbol{\\theta}})\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right)+\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\hat{\\boldsymbol{\\theta}})-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\n\\end{aligned}\n$$\n\nwith some $\\hat{\\boldsymbol{\\theta}}$ between $\\hat{\\boldsymbol{\\theta}}$ and $\\boldsymbol{\\theta}^{*}$. We have $\\nabla_{\\boldsymbol{\\theta}} P \\phi(\\hat{\\boldsymbol{\\theta}})=J(\\hat{\\boldsymbol{\\theta}})$, so\n\n$$\n-J\\left(\\boldsymbol{\\theta}^{*}\\right)\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right)=\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)+\\left[J(\\hat{\\boldsymbol{\\theta}})-J\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right)+\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\hat{\\boldsymbol{\\theta}})-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\n$$\n\nand\n\n$$\n\\begin{aligned}\n-\\sqrt{n} A_{n} J\\left(\\boldsymbol{\\theta}^{*}\\right)\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right)= & \\sqrt{n} A_{n} \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right) \\\\\n& +\\sqrt{n} A_{n}\\left[J(\\hat{\\boldsymbol{\\theta}})-J\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right) \\\\\n& +\\sqrt{n}\\left(\\mathbb{P}_{n}-P\\right) A_{n}\\left[\\phi(\\hat{\\boldsymbol{\\theta}})-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\n\\end{aligned}\n$$\n\nThe second and the third term are negligible, since\n\n$$\n\\sqrt{n} A_{n}\\left[J(\\hat{\\boldsymbol{\\theta}})-J\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\left(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right)=o\\left(\\sqrt{n} \\frac{1}{\\sqrt{n} r_{n}} r_{n}\\right)=o(1)\n$$\n\nby assumption (A2), and Lemma 10 yields\n\n$$\n\\sqrt{n}\\left(\\mathbb{P}_{n}-P\\right) A_{n}\\left[\\phi(\\hat{\\boldsymbol{\\theta}})-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]=o_{p}(1)\n$$\n\nIt remains to show a central limit theorem for\n\n$$\n\\sqrt{n} A_{n} \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)=\\sum_{i=1}^{n} \\frac{1}{\\sqrt{n}} A_{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right) \\vDash \\sum_{i=1}^{n} \\boldsymbol{Y}_{i}\n$$\n\nSince\n\n$$\n\\sum_{i=1}^{n} \\mathbb{E}\\left[\\left\\|\\boldsymbol{Y}_{i}\\right\\|^{2} \\mathbb{1}\\left\\{\\left\\|\\boldsymbol{Y}_{i}\\right\\|>\\varepsilon\\right\\}\\right] \\leqslant \\sum_{i=1}^{n} \\mathbb{E}\\left[\\left\\|\\boldsymbol{Y}_{i}\\right\\|^{2} \\mathbb{1}\\left\\{\\left\\|\\boldsymbol{Y}_{i}\\right\\|>\\varepsilon\\right\\}\\left\\|\\boldsymbol{Y}_{i}\\right\\|^{2} / \\varepsilon^{2}\\right] \\leqslant \\sum_{i=1}^{n} \\mathbb{E}\\left[\\left\\|\\boldsymbol{Y}_{i}\\right\\|^{4}\\right] / \\varepsilon^{2}\n$$\n\nand $\\mathbb{E}\\left[\\left\\|\\boldsymbol{Y}_{i}\\right\\|^{4}\\right]=n^{-2} \\mathbb{E}\\left[\\left\\|A_{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|^{4}\\right]=o\\left(n^{-1}\\right)$ for all $i=1, \\ldots, n$, by (A3), we have\n\n$$\n\\sum_{i=1}^{n} \\mathbb{E}\\left[\\left\\|\\boldsymbol{Y}_{i}\\right\\|^{2} \\mathbb{1}\\left\\{\\left\\|\\boldsymbol{Y}_{i}\\right\\|>\\varepsilon\\right\\}\\right] \\rightarrow 0 \\text { for every } \\varepsilon>0\n$$\n\nSince $\\mathbb{E}\\left[\\boldsymbol{Y}_{i}\\right]=\\mathbf{0}$ for all $i=1, \\ldots, n$ and\n\n$$\n\\sum_{i=1}^{n} \\operatorname{Cov}\\left(\\boldsymbol{Y}_{i}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{Cov}\\left[A_{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]=\\frac{1}{n} A_{n} \\sum_{i=1}^{n} \\operatorname{Cov}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right] A_{n}^{\\top}=A_{n} I\\left(\\boldsymbol{\\theta}^{*}\\right) A_{n}^{\\top} \\rightarrow \\Sigma\n$$\n\nthe conditions of the Lindeberg-Feller central limit theorem (van der Vaart, 1998, Section 2.8) are satisfied, and we obtain\n\n$$\n\\sqrt{n} A_{n} J\\left(\\boldsymbol{\\theta}^{*}\\right)\\left(\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right) \\rightarrow_{d} \\mathcal{N}(\\mathbf{0}, \\Sigma)\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 25,
      "text": "# A. 4 Proof of Theorem 4 \n\nLet $\\tilde{\\boldsymbol{\\theta}} \\in \\Theta_{n}$ be any solution of (5), which we write as $\\tilde{\\boldsymbol{\\theta}}=\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}$. It holds\n\n$$\n\\begin{aligned}\n0 & =\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-p_{\\mathbf{A}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right\\rangle \\\\\n& \\leqslant\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\rangle+\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} H_{n} \\boldsymbol{u}\\right\\rangle-\\left\\langle\\boldsymbol{u}, p_{\\mathbf{A}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right\\rangle\n\\end{aligned}\n$$\n\nUsing H\u00f6lder's inequality and Lemma 13, the first term in (23) can be bounded by\n\n$$\n\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\rangle \\leqslant\\|\\boldsymbol{u}\\|_{1} \\eta_{n} \\leqslant \\sqrt{\\nu_{n}}\\|\\boldsymbol{u}\\|_{2} \\eta_{n}\n$$\n\nwith probability tending to 1 . For the second term in (23), (A4) and H\u00f6lder's inequality yield\n\n$$\n\\begin{aligned}\n\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} H_{n} \\boldsymbol{u}\\right\\rangle & =\\left\\langle\\boldsymbol{u}, P H_{n} \\boldsymbol{u}\\right\\rangle+\\left\\langle\\boldsymbol{u},\\left(\\mathbb{P}_{n}-P\\right) H_{n} \\boldsymbol{u}\\right\\rangle \\\\\n& \\leqslant-c\\|\\boldsymbol{u}\\|^{2}+\\nu_{n}\\|\\boldsymbol{u}\\|^{2} \\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) H_{n, j, k}\\right| \\\\\n& =-c\\|\\boldsymbol{u}\\|^{2}+\\|\\boldsymbol{u}\\|^{2} o_{p}(1)\n\\end{aligned}\n$$\n\nwhere the last step follows from Lemma 8. For the third term in (23), H\u00f6lder's inequality gives\n\n$$\n-\\left\\langle\\boldsymbol{u}, p_{\\mathbf{A}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right\\rangle \\leqslant \\sqrt{\\nu_{n}}\\|\\boldsymbol{u}\\|_{2} \\bar{b}_{n}\n$$\n\nAltogether, we have shown\n\n$$\n0 \\leqslant\\|\\boldsymbol{u}\\|_{2} \\sqrt{\\nu_{n}}\\left(\\eta_{n}+\\bar{b}_{n}\\right)-\\|\\boldsymbol{u}\\|_{2}^{2}\\left[c+o_{p}(1)\\right]\n$$\n\nRearranging terms gives\n\n$$\n\\left\\|\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|_{2}=\\|\\boldsymbol{u}\\|_{2} \\leqslant \\frac{\\sqrt{\\nu_{n}}\\left(\\eta_{n}+\\bar{b}_{n}\\right)}{c+o_{p}(1)}=O_{p}\\left(\\sqrt{\\nu_{n}}\\left(\\eta_{n}+\\bar{b}_{n}\\right)\\right)\n$$\n\nas claimed.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 26,
      "text": "# A. 5 Proof of Theorem 5 \n\nThe proof is split in two steps:\n\n1. Show that there is a solution $\\hat{\\boldsymbol{\\theta}}_{(1)}$ to\n\n$$\n\\Phi_{n}\\left(\\left(\\boldsymbol{\\theta}_{(1)}, \\mathbf{0}\\right)\\right)_{(1)} \\in \\partial_{\\boldsymbol{\\theta}_{(1)}} p_{\\boldsymbol{\\lambda}_{n}}\\left(\\left(\\boldsymbol{\\theta}_{(1)}, \\mathbf{0}\\right)\\right) \\in \\mathbb{R}^{s_{n}}\n$$\n\nwith $\\left\\|\\hat{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}\\right\\|=O_{p}\\left(\\tilde{r}_{n}\\right)$.\n2. Show that $\\hat{\\boldsymbol{\\theta}}=\\left(\\hat{\\boldsymbol{\\theta}}_{(1)}, \\mathbf{0}\\right)$ is also a valid solution to\n\n$$\n\\Phi_{n}(\\hat{\\boldsymbol{\\theta}})_{(2)} \\in \\partial_{\\boldsymbol{\\theta}_{(2)}} p_{\\boldsymbol{\\lambda}_{n}}(\\hat{\\boldsymbol{\\theta}})\n$$\n\nSince\n\n$$\n\\partial p_{\\boldsymbol{\\lambda}_{n}}(\\hat{\\boldsymbol{\\theta}})_{(2)} \\supseteq\\left[-\\lambda_{n, s_{n}+1}, \\lambda_{n, s_{n}+1}\\right] \\times \\ldots \\times\\left[-\\lambda_{n, p_{n}}, \\lambda_{n, p_{n}}\\right]\n$$\n\nby (P2), it suffices to verify $\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} \\Phi_{n}(\\hat{\\boldsymbol{\\theta}})_{(2)}\\right\\|_{\\infty} \\leqslant 1$.\nTogether this implies that $\\hat{\\boldsymbol{\\theta}}$ is a valid solution to the full problem (5).\nStep 1 Observe that (P1) implies that $p_{\\boldsymbol{\\lambda}_{n}}(\\boldsymbol{\\theta})$ is differentiable on $\\Theta_{n}^{\\prime}$. Similar as in the proof of Theorem 1, it therefore suffices to show that\n\n$$\n\\left(\\tilde{r}_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1, \\boldsymbol{u}_{(2)}=\\mathbf{0}}\\left\\langle\\boldsymbol{u}_{(1)}, \\Phi_{n}\\left(\\boldsymbol{\\theta}^{*}+\\tilde{r}_{n} C \\boldsymbol{u}\\right)_{(1)}-p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\tilde{r}_{n} C \\boldsymbol{u}\\right)_{(1)}\\right\\rangle \\leqslant-c<0\n$$\n\nfor large enough $C$ and high probability. The left-hand side of (24) is upper bounded by\n\n$$\n\\left(\\tilde{r}_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1, \\boldsymbol{u}_{(2)}=\\mathbf{0}}\\left\\langle\\boldsymbol{u}_{(1)}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+\\tilde{r}_{n} C \\boldsymbol{u}\\right)_{(1)}\\right\\rangle-\\left(\\tilde{r}_{n} C\\right)^{-1} \\inf _{\\|\\boldsymbol{u}\\|=1, \\boldsymbol{u}_{(2)}=\\mathbf{0}}\\left\\langle\\boldsymbol{u}_{(1)}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\tilde{r}_{n} C \\boldsymbol{u}\\right)_{(1)}\\right\\rangle\n$$\n\nFor the first term, one can proceed similarly to the proof of Theorem 1 and show that, using (A1) and choosing $C$ large enough,\n\n$$\n\\left(\\tilde{r}_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1, \\boldsymbol{u}_{(2)}=\\mathbf{0}}\\left\\langle\\boldsymbol{u}_{(1)}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+\\tilde{r}_{n} C \\boldsymbol{u}\\right)_{(1)}\\right\\rangle \\leqslant-c<0\n$$\n\nholds with arbitrarily high probability. It remains to show that the second term in (25) is sufficiently small. A Taylor expansion yields\n\n$$\n\\begin{aligned}\n& \\left(\\tilde{r}_{n} C\\right)^{-1}\\left\\langle\\boldsymbol{u}_{(1)}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\tilde{r}_{n} C \\boldsymbol{u}\\right)_{(1)}\\right\\rangle \\\\\n& =\\left(\\tilde{r}_{n} C\\right)^{-1}\\left\\langle\\boldsymbol{u}_{(1)}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right\\rangle+\\left\\langle\\boldsymbol{u}_{(1)}, \\nabla_{\\boldsymbol{\\theta}_{(1)}}^{2} p_{\\boldsymbol{\\lambda}_{n}}\\left(\\boldsymbol{\\theta}^{*}+\\tilde{r}_{n} C \\boldsymbol{u}^{\\prime}\\right) \\boldsymbol{u}_{(1)}\\right\\rangle\n\\end{aligned}\n$$\n\nwith some $\\boldsymbol{u}^{\\prime}$ with $\\left\\|\\boldsymbol{u}^{\\prime}\\right\\| \\leqslant 1, \\boldsymbol{u}_{(2)}^{\\prime}=\\mathbf{0}$. Using the Cauchy-Schwarz inequality, we obtain\n\n$$\n\\begin{aligned}\n\\left(\\tilde{r}_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1, \\boldsymbol{u}_{(2)}=\\mathbf{0}}\\left|\\left\\langle\\boldsymbol{u}_{(1)}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right\\rangle\\right| & \\leqslant\\left(\\tilde{r}_{n} C\\right)^{-1} \\sup _{\\|\\boldsymbol{u}\\|=1, \\boldsymbol{u}_{(2)}=\\mathbf{0}}\\left\\|p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right\\|\\|\\boldsymbol{u}\\| \\\\\n& \\leqslant\\left(\\tilde{r}_{n} C\\right)^{-1} \\sqrt{s_{n}} \\max _{k=1, \\ldots, s_{n}}\\left\\{\\left|p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{k}\\right|\\right\\} \\\\\n& =\\left(\\tilde{r}_{n} C\\right)^{-1} \\sqrt{s_{n}} b_{n}^{*}=O(1 / C)\n\\end{aligned}\n$$\n\nwhich becomes negligible by choosing $C$ large enough. For the second term, (P1) gives\n\n$$\n\\sup _{\\|\\boldsymbol{u}\\|=1, \\boldsymbol{u}_{(2)}=\\mathbf{0}}\\left\\langle\\boldsymbol{u}_{(1)}, \\nabla_{\\boldsymbol{\\theta}_{(1)}}^{2} p_{\\boldsymbol{\\lambda}_{n}}\\left(\\boldsymbol{\\theta}^{*}+\\tilde{r}_{n} C \\boldsymbol{u}^{\\prime}\\right) \\boldsymbol{u}_{(1)}\\right\\rangle=o(1)\n$$\n\nwhich proves (24).\n\nStep 2 Lemma 11 yields\n\n$$\n\\begin{aligned}\n& \\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} \\Phi_{n}(\\tilde{\\boldsymbol{\\theta}})_{(2)}\\right\\|_{\\infty} \\\\\n& \\leqslant \\quad\\underbrace{\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} J(\\tilde{\\boldsymbol{\\theta}})_{(2,1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)} p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\right\\|_{\\infty}}_{\\approx v_{1}} \\\\\n& +\\underbrace{\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1}\\left(\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}-J(\\tilde{\\boldsymbol{\\theta}})_{(2,1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)} \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right\\|_{\\infty}}_{\\approx v_{2}} \\\\\n& +o_{p}(1)\n\\end{aligned}\n$$\n\nwith some $\\tilde{\\boldsymbol{\\theta}}$ between $\\boldsymbol{\\theta}^{*}$ and $\\tilde{\\boldsymbol{\\theta}}$. It holds $\\left\\|\\boldsymbol{v}_{1}\\right\\|_{\\infty} \\leqslant \\alpha \\in[0,1)$ by the definition of $\\alpha$ in (A6). Further, Lemma 13 implies $\\left\\|\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|_{\\infty} \\leqslant \\eta_{n}$ with probability tending to 1 . On this event, the definitions of $\\eta_{n}$ in (8) and of $\\boldsymbol{\\lambda}_{n(2)}$ in (A7) give\n\n$$\n\\begin{aligned}\n\\left\\|\\boldsymbol{v}_{2}\\right\\|_{\\infty} & \\leqslant \\frac{1-\\alpha}{4} \\frac{\\left\\|\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}\\right\\|_{\\infty}}{\\eta_{n}}+\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} J(\\tilde{\\boldsymbol{\\theta}})_{(2,1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\right\\|_{\\infty}\\left\\|\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right\\|_{\\infty} \\\\\n& \\leqslant \\frac{1-\\alpha}{4}+\\frac{1-\\alpha}{4} \\frac{\\max _{k=s_{n}+1, \\ldots, p_{n}} \\frac{1}{J_{n, k}}\\left\\|\\left(J(\\tilde{\\boldsymbol{\\theta}})_{k,(1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\right)^{\\top}\\right\\|_{1}\\left\\|\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right\\|_{\\infty}}{\\eta_{n}} \\\\\n& \\leqslant \\frac{1}{2}(1-\\alpha)\n\\end{aligned}\n$$\n\nTogether, we have shown\n\n$$\n\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} \\Phi_{n}(\\tilde{\\boldsymbol{\\theta}})_{(2)}\\right\\|_{\\infty} \\leqslant \\alpha+\\frac{1}{2}(1-\\alpha)+o_{p}(1)=\\frac{1}{2}(1+\\alpha)+o_{p}(1)<1\n$$\n\nwith probability going to 1 .",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 27,
      "text": "# A. 6 Proof of Theorem 6 \n\nSuppose there is a solution $\\tilde{\\boldsymbol{\\theta}} \\in \\Theta_{n}$ and a further solution $\\tilde{\\boldsymbol{\\theta}}=\\tilde{\\boldsymbol{\\theta}}+\\tilde{\\boldsymbol{u}} \\in \\Theta_{n}$. From Theorem 4 we know that $\\left\\|\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{\\nu_{n}}\\left(\\eta_{n}+\\left\\|\\boldsymbol{\\lambda}_{n}\\right\\|_{\\infty}\\right)\\right)$ and $\\left\\|\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{\\nu_{n}}\\left(\\eta_{n}+\\left\\|\\boldsymbol{\\lambda}_{n}\\right\\|_{\\infty}\\right)\\right)$. Similar to the proofs of Theorem 2 and Theorem 4, it must hold\n\n$$\n\\begin{aligned}\n0 & =\\langle\\tilde{\\boldsymbol{u}}, \\mathbb{P}_{n} \\phi(\\tilde{\\boldsymbol{\\theta}})-p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\tilde{\\boldsymbol{\\theta}})\\rangle \\\\\n& =\\left\\langle\\tilde{\\boldsymbol{u}}, \\mathbb{P}_{n}[\\phi(\\tilde{\\boldsymbol{\\theta}})-\\phi(\\tilde{\\boldsymbol{\\theta}})]\\right\\rangle-\\left\\langle\\tilde{\\boldsymbol{u}}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\tilde{\\boldsymbol{\\theta}})-p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\tilde{\\boldsymbol{\\theta}})\\right\\rangle \\\\\n& \\leqslant-c\\|\\tilde{\\boldsymbol{u}}\\|_{2}^{2}+\\nu_{n}\\|\\tilde{\\boldsymbol{u}}\\|_{2}^{2} \\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) H_{j, k}\\right|+\\frac{1}{2} \\mu_{n}\\|\\tilde{\\boldsymbol{u}}\\|_{2}^{2} \\\\\n& \\leqslant\\|\\tilde{\\boldsymbol{u}}\\|_{2}^{2}\\left[-c+\\frac{1}{2} \\mu_{n}+o_{p}(1)\\right]\n\\end{aligned}\n$$\n\nwhere we used (A4) and (P3) in the first inequality, and Lemma 8 in the second. Because $\\mu_{n}<2 c$ asymptotically, it must hold $\\|\\tilde{\\boldsymbol{u}}\\|_{2}=0$ or, equivalently, $\\tilde{\\boldsymbol{\\theta}}=\\tilde{\\boldsymbol{\\theta}}$ with probability tending to 1 .",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 28,
      "text": "## A. 7 Proof of Theorem 7\n\nSimilar to the proof of Theorem 3, we obtain\n\n$$\n\\begin{aligned}\n\\mathbf{0}= & \\mathbb{P}_{n} \\phi(\\tilde{\\boldsymbol{\\theta}})_{(1)}-p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\tilde{\\boldsymbol{\\theta}})_{(1)} \\\\\n= & \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}+J(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}\\right)+\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(1)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right] \\\\\n& -p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}-\\nabla_{\\tilde{\\boldsymbol{\\theta}}_{(1)}}^{*} p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}\\right)\n\\end{aligned}\n$$\n\nfor and some $\\tilde{\\boldsymbol{\\theta}}$ on the line segment from $\\tilde{\\boldsymbol{\\theta}}$ to $\\boldsymbol{\\theta}^{*}$. It then holds\n\n$$\n\\begin{aligned}\n\\sqrt{n} A_{n}\\left[-J\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}\\right)+p_{\\boldsymbol{\\Lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right]= & \\sqrt{n} A_{n} \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)} \\\\\n& +\\sqrt{n} A_{n}[J(\\tilde{\\boldsymbol{\\theta}})_{(1)}-J\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}]\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}\\right) \\\\\n& +\\sqrt{n} A_{n}\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(1)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right] \\\\\n& -\\sqrt{n} A_{n} \\nabla_{\\boldsymbol{\\theta}_{(1)}}^{2} p_{\\boldsymbol{\\Lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}\\right)\n\\end{aligned}\n$$\n\nAdapting the proof of Theorem 3, one can show a central limit theorem for the first term and that the second and third term are of order $o_{p}(1)$ by (A2) and Lemma 10. For the last term, we have\n\n$$\n\\sqrt{n} A_{n} \\nabla_{\\boldsymbol{\\theta}_{(1)}}^{2} p_{\\boldsymbol{\\Lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}\\right)=o_{p}(1)\n$$\n\nby assumption (P4) and $\\left\\|A_{n}\\right\\|=O(1)$, which concludes the proof.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 29,
      "text": "# A. 8 Proof of Proposition 1 \n\nLet $k_{n}=\\lceil\\sqrt{n}\\rceil$ and consider the set of perturbations $\\mathcal{U}_{n}=\\left\\{\\boldsymbol{u} \\in \\mathbb{R}^{p_{n}}:\\|\\boldsymbol{u}\\|_{1}=1,\\|\\boldsymbol{u}\\|_{2} \\leqslant 1 / \\sqrt{k_{n}}\\right\\}$. Define $G_{n}(\\boldsymbol{u})=\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{u}^{\\top}\\left(\\boldsymbol{X}_{i}-I_{p_{n}}\\right) \\boldsymbol{u}$. For $\\boldsymbol{u} \\in \\mathcal{U}_{n}$, the RSC condition becomes\n\n$$\n\\|\\boldsymbol{u}\\|_{2}^{2}-G_{n}(\\boldsymbol{u}) \\geqslant c\\|\\boldsymbol{u}\\|_{2}^{2}-c_{1} \\eta_{n}^{2}\n$$\n\nwhich implies\n\n$$\n\\sup _{\\boldsymbol{u} \\in \\mathcal{U}_{n}} G_{n}(\\boldsymbol{u}) \\leqslant \\frac{(1-c)}{k_{n}}+c_{1} \\frac{\\ln p_{n}}{n} \\leqslant \\frac{1}{\\sqrt{n}}+c_{1} \\frac{\\ln p_{n}}{n}\n$$\n\nWe will show that this event has probability tending to zero.\nThe random matrix $\\boldsymbol{Z}=\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{X}_{i}-I_{p_{n}}$ has independent Gaussian entries, each with variance $1 / n$. Thus, $G_{n}(\\boldsymbol{u})$ is a Gaussian process with\n\n$$\n\\sup _{\\boldsymbol{u} \\in \\mathcal{U}_{n}} \\operatorname{Var}\\left[G_{n}(\\boldsymbol{u})\\right]=\\sup _{\\boldsymbol{u} \\in \\mathcal{U}_{n}} \\sum_{i=1}^{p_{n}} \\sum_{j=1}^{p_{n}} u_{i}^{2} u_{j}^{2} \\operatorname{Var}\\left[Z_{i, j}\\right] \\leqslant \\frac{\\sup _{\\boldsymbol{u} \\in \\mathcal{U}_{n}}\\|\\boldsymbol{u}\\|_{2}^{4}}{n} \\leqslant \\frac{1}{n k_{n}^{2}}\n$$\n\nThe Gaussian concentration inequality (e.g., Talagrand, 2005, Lemma 2.1.3) now yields\n\n$$\n\\sup _{\\boldsymbol{u} \\in \\mathcal{U}_{n}} G_{n}(\\boldsymbol{u}) \\geqslant \\mathbb{E}\\left[\\sup _{\\boldsymbol{u} \\in \\mathcal{U}_{n}} G_{n}(\\boldsymbol{u})\\right]+O_{p}\\left(\\frac{1}{\\sqrt{n} k_{n}}\\right)=\\mathbb{E}\\left[\\sup _{\\boldsymbol{u} \\in \\mathcal{U}_{n}} G_{n}(\\boldsymbol{u})\\right]+O_{p}\\left(\\frac{1}{n}\\right)\n$$\n\nTo lower bound the expectation on the right, consider the set\n\n$$\n\\overline{\\mathcal{U}}_{n}=\\left\\{(\\boldsymbol{s}, 1, \\mathbf{0}) / k_{n} \\in \\mathbb{R}^{p_{n}}: \\boldsymbol{s} \\in\\left\\{-1,1\\right\\}^{k_{n}-1}\\right\\} \\subset \\mathcal{U}_{n}\n$$\n\nwhich has cardinality $\\left|\\overline{\\mathcal{U}}_{n}\\right|=2^{k_{n}-1}$. Sudakov's minoration inequality (e.g., Wainwright, 2019, Theorem 5.30) gives\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\sup _{\\boldsymbol{u} \\in \\overline{\\mathcal{U}}_{n}} G_{n}(\\boldsymbol{u})\\right]^{2} & \\geqslant \\frac{1}{4} \\ln \\left|\\overline{\\mathcal{U}}_{n}\\right| \\min _{\\boldsymbol{u}, \\boldsymbol{u}^{\\prime} \\in \\overline{\\mathcal{U}}_{n}, \\boldsymbol{u} \\neq \\boldsymbol{u}^{\\prime}} \\operatorname{Var}\\left[G_{n}(\\boldsymbol{u})-G_{n}\\left(\\boldsymbol{u}^{\\prime}\\right)\\right] \\\\\n& \\geqslant \\frac{\\ln 2}{4}\\left(k_{n}-1\\right) \\min _{\\boldsymbol{u}, \\boldsymbol{u}^{\\prime} \\in \\overline{\\mathcal{U}}_{n}, \\boldsymbol{u} \\neq \\boldsymbol{u}^{\\prime}} \\operatorname{Var}\\left[G_{n}(\\boldsymbol{u})-G_{n}\\left(\\boldsymbol{u}^{\\prime}\\right)\\right]\n\\end{aligned}\n$$\n\nWe have\n\n$$\n\\operatorname{Var}\\left[G_{n}(\\boldsymbol{u})-G_{n}\\left(\\boldsymbol{u}^{\\prime}\\right)\\right]=\\operatorname{Var}\\left[\\sum_{i=1}^{p_{n}} \\sum_{j=1}^{p_{n}}\\left(u_{i} u_{j}-u_{i}^{\\prime} u_{j}^{\\prime}\\right) Z_{i, j}\\right]=\\frac{1}{n} \\sum_{i=1}^{p_{n}} \\sum_{j=1}^{p_{n}}\\left(u_{i} u_{j}-u_{i}^{\\prime} u_{j}^{\\prime}\\right)^{2}\n$$\n\nDefining $s_{i}=u_{i} k_{n}$ and $s_{i}^{\\prime}=u_{i}^{\\prime} k_{n}$ and using $\\boldsymbol{s} \\neq \\boldsymbol{s}^{\\prime}$ and $\\boldsymbol{s} \\neq-\\boldsymbol{s}^{\\prime}$ since $s_{k_{n}}=s_{k_{n}}^{\\prime}=1$, Lemma 14 gives\n\n$$\n\\sum_{i=1}^{p_{n}} \\sum_{j=1}^{p_{n}}\\left(u_{i} u_{j}-u_{i}^{\\prime} u_{j}^{\\prime}\\right)^{2}=\\frac{1}{k_{n}^{2}} \\sum_{i=1}^{k_{n}} \\sum_{j=1}^{k_{n}}\\left(s_{i} s_{j}-s_{i}^{\\prime} s_{j}^{\\prime}\\right)^{2} \\geqslant \\frac{8\\left(k_{n}-1\\right)}{k_{n}^{2}}\n$$\n\nWe have shown that\n\n$$\n\\mathbb{E}\\left[\\sup _{\\boldsymbol{u} \\in \\mathcal{U}_{n}} G_{n}(\\boldsymbol{u})\\right]^{2} \\geqslant 2 \\ln 2 \\frac{\\left(k_{n}-1\\right)^{2}}{n k_{n}^{2}} \\geqslant \\frac{1.2}{n}\n$$\n\nfor $n$ large enough. Altogether this implies\n\n$$\n\\sup _{\\boldsymbol{u} \\in \\mathcal{U}_{n}} G_{n}(\\boldsymbol{u}) \\geqslant \\frac{\\sqrt{1.2}}{\\sqrt{n}}+O_{p}\\left(\\frac{1}{n}\\right)\n$$\n\nso that the event (26) holds with probability tending to zero as $n \\rightarrow \\infty$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 30,
      "text": "# B Proofs of Corollaries\n## B. 1 Proof of Corollary 1\n\nConsistency For (A1), we can choose\n\n$$\nH_{n}(\\boldsymbol{X})=-\\inf _{\\boldsymbol{\\theta} \\in \\Theta_{n}}\\left|\\psi^{\\prime}\\left(Y_{i}, \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{\\theta}\\right)\\right| \\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top}\n$$\n\nThen, (A1)(i) is fulfilled by the definition of $H_{n}$ and (ii) follows from (16). For (iii), $\\inf _{\\boldsymbol{\\theta} \\in \\Theta_{n}}\\left|\\psi^{\\prime}\\left(Y_{i}, \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{\\theta}\\right)\\right|$ is negligible as this term is bounded, so it remains to verify\n\n$$\n\\frac{1}{n}\\left\\|\\sum_{i=1}^{n} \\mathbb{E}\\left[\\left(\\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top}\\right)^{2} \\mathbb{1}_{\\left|\\boldsymbol{X}_{i}\\right|^{2} \\leqslant B_{n}}\\right]\\right\\|=o\\left(n / \\ln p_{n}\\right) \\quad \\text { and } \\quad \\int_{B_{n}}^{\\infty} \\mathbb{P}\\left(\\left\\|\\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top}\\right\\|>t\\right)=o(1)\n$$\n\nThe second condition holds for $B_{n}=p_{n} a_{n}$ with $a_{n} \\rightarrow \\infty$ arbitrarily slowly by Lemma 1 with $\\beta(x)=x$ and $F_{n}(\\boldsymbol{X})=\\left\\|\\boldsymbol{X} \\boldsymbol{X}^{\\top}\\right\\|$. This a valid $B_{n}$ since $p_{n} \\ln p_{n} / n \\rightarrow 0$ and the second condition follows from Lemma 2. Now the consistency result follows from Theorem 1. Since our choice of $H_{n}$ does not rely on $\\boldsymbol{\\theta}^{*}$, (A1)(i) holds with $\\boldsymbol{\\theta}^{*}$ replaced by any $\\boldsymbol{\\theta} \\in \\Theta_{n}$, and the resulting estimator is unique by Theorem 2. The rate of convergence is $\\sqrt{p_{n} / n}$, asinces $\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)\\right)=O\\left(p_{n}\\right)$ follows from $\\mathbb{E}\\left[\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}^{*}\\right)_{k}^{4}\\right]=O(1)$.\n\nAsymptotic normality It suffices to verify (A2) for each row $\\boldsymbol{a}_{n}$ of $A_{n}$, as $A_{n} \\phi_{i}(\\boldsymbol{\\theta})$ is a finite dimensional vector. Boundedness of $A_{n}$ implies $\\left\\|\\boldsymbol{a}_{n}\\right\\|=O(1)$. Using a Taylor expansion and boundedness of $\\psi^{\\prime}\\left(Y_{i}, \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{\\theta}\\right)$, it suffices to use $\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top} \\tilde{\\boldsymbol{u}}$ with $\\tilde{\\boldsymbol{u}} \\equiv \\boldsymbol{u}-\\boldsymbol{u}^{\\prime}$ instead of $\\boldsymbol{a}_{n}^{\\top}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}^{\\prime}\\right)\\right]$. We obtain\n\n$$\n\\mathbb{E}\\left[\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top} \\tilde{\\boldsymbol{u}}\\right|^{2}\\right]=\\mathbb{E}\\left[\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right|^{2}\\left|\\boldsymbol{X}_{i}^{\\top} \\tilde{\\boldsymbol{u}}\\right|^{2}\\right] \\leqslant \\sqrt{\\mathbb{E}\\left[\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right|^{4}\\right] \\mathbb{E}\\left[\\left|\\tilde{\\boldsymbol{u}}^{\\top} \\boldsymbol{X}_{i}\\right|^{4}\\right]}=O\\left(\\|\\tilde{\\boldsymbol{u}}\\|^{2}\\right)\n$$\n\nverifying the first condition in (A2) (with $p_{n}^{2} / n=o(1)$ ). For the second, note that the first condition in (15) implies $\\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right]=O\\left(\\sqrt{p_{n}}\\right)$ since $\\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right] \\leqslant \\sqrt{\\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|^{2}\\right]}=\\sqrt{\\sum_{k=1}^{p_{n}} \\mathbb{E}\\left[X_{i, k}^{2}\\right]}=$ $O\\left(\\sqrt{p_{n}}\\right)$. Set $D_{n}=\\rho^{-1}\\left(n \\omega_{n}\\right) \\sqrt{p_{n}}$ with $\\omega_{n} \\rightarrow \\infty$ arbitrarily slowly. This is a valid choice since\n\n$$\n\\frac{D_{n} r_{n} p_{n}}{\\sqrt{n}}=\\frac{\\rho^{-1}\\left(n \\omega_{n}\\right) p_{n}^{2}}{n}=o(1)\n$$\n\nby assumption. We have\n\n$$\n\\begin{aligned}\n& \\sum_{i=1}^{n} \\mathbb{P}\\left(\\sup _{\\|\\boldsymbol{u}\\| \\leqslant r_{n} C} \\frac{\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{u}\\right|}{\\|\\boldsymbol{u}\\|}>D_{n}\\right) \\\\\n\\leqslant & n \\mathbb{P}\\left(\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right|\\left\\|\\boldsymbol{X}_{i}\\right\\|>D_{n}\\right) \\\\\n\\leqslant & n \\mathbb{P}\\left(\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right| \\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right]>D_{n}\\right)+n \\mathbb{P}\\left(\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right|\\left(\\left\\|\\boldsymbol{X}_{i}\\right\\|-\\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right]\\right)>D_{n}\\right) \\\\\n\\leqslant & n \\mathbb{P}\\left(\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right| \\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right]>D_{n}\\right)+n \\mathbb{P}\\left(\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right|^{2}>D_{n}\\right)+n \\mathbb{P}\\left(\\left|\\left\\|\\boldsymbol{X}_{i}\\right\\|-\\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right]\\right|^{2}>D_{n}\\right) \\\\\n\\leqslant & \\frac{n \\mathbb{E}\\left[\\rho\\left(\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right|\\right)\\right]}{\\rho\\left(D_{n} / \\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right]\\right)}+\\frac{n \\mathbb{E}\\left[\\rho\\left(\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right|^{2}\\right)\\right]}{\\rho\\left(D_{n}\\right)}+\\frac{n \\mathbb{E}\\left[\\rho\\left(\\left|\\left\\|\\boldsymbol{X}_{i}\\right\\|-\\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right]\\right|^{2}\\right)\\right]}{\\rho\\left(D_{n}\\right)} \\\\\n= & o(1)\n\\end{aligned}\n$$\n\nsince all expectations are bounded by assumption, $\\rho(x)$ increasing and $D_{n} / \\mathbb{E}\\left[\\left\\|\\boldsymbol{X}_{i}\\right\\|\\right] \\geqslant \\rho^{-1}\\left(n \\omega_{n}\\right)$ for $n$ large enough.\n\nBecause $\\psi^{\\prime}$ is Lipschitz, we further have\n\n$$\n\\begin{aligned}\n\\left\\|\\boldsymbol{a}_{n}^{\\top}\\left[J\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-J\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\| & \\leqslant\\left\\|\\mathbb{E}\\left[\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i} \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{X}_{i}^{\\top} \\boldsymbol{u}\\right]\\right\\| \\\\\n& =\\sup _{\\left\\|\\boldsymbol{u}^{\\prime}\\right\\|=1}\\left|\\mathbb{E}\\left[\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i} \\boldsymbol{u}^{\\prime \\top} \\boldsymbol{X}_{i} \\boldsymbol{u}^{\\top} \\boldsymbol{X}_{i}\\right]\\right| \\\\\n& \\leqslant\\left|\\mathbb{E}\\left[\\left|\\boldsymbol{a}_{n}^{\\top} \\boldsymbol{X}_{i}\\right|^{3}\\right]^{1 / 3} \\mathbb{E}\\left[\\left|\\boldsymbol{u}^{\\prime \\top} \\boldsymbol{X}_{i}\\right|^{3}\\right]^{1 / 3} \\mathbb{E}\\left[\\left|\\boldsymbol{u}^{\\top} \\boldsymbol{X}_{i}\\right|^{3}\\right]^{1 / 3}\\right] \\\\\n& =O\\left(\\left\\|\\boldsymbol{a}_{n}\\right\\|\\|\\boldsymbol{u}\\|\\|\\boldsymbol{\\tilde { u }}\\|\\right)=O\\left(\\sqrt{p_{n} / n}\\right)=o\\left(1 / \\sqrt{p_{n}}\\right)\n\\end{aligned}\n$$\n\nsince $p_{n}^{2} / n=o_{1} 1$ ), verifying the third condition in (A2).\nFinally, since $\\left\\|A_{n} \\phi_{i}(\\boldsymbol{\\theta})\\right\\|^{4} \\leqslant\\left\\|A_{n}\\right\\|^{4}\\left\\|\\phi_{i}(\\boldsymbol{\\theta})\\right\\|^{4}$,\n\n$$\n\\left\\|\\phi_{i}(\\boldsymbol{\\theta})\\right\\|^{4}=\\left(\\sum_{k=1}^{p_{n}} \\phi_{i}(\\boldsymbol{\\theta})_{k}^{2}\\right)^{2}=\\sum_{k=1}^{p_{n}} \\sum_{k^{\\prime}=1}^{p_{n}}\\left(\\phi_{i}(\\boldsymbol{\\theta})_{k} \\phi_{i}(\\boldsymbol{\\theta})_{k^{\\prime}}\\right)^{2}\n$$\n\nand $\\max _{k} \\mathbb{E}\\left[\\phi\\left(\\boldsymbol{X}_{i} ; \\boldsymbol{\\theta}^{*}\\right)_{k}^{4}\\right]=O(1)$, we have $\\mathbb{E}\\left[\\left\\|A_{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|^{4}\\right]=O\\left(p_{n}^{2}\\right)=o(n)$, since $p_{n}^{2} / n=o(1)$, verifying (A3). We have checked all conditions of Theorem 3, which yields the claim.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 31,
      "text": "# B. 2 Proof of Corollary 2 \n\nWe apply Theorems 5-7. The rate of convergence in Theorem 5 is $\\tilde{r}_{n}=\\sqrt{s_{n} \\ln p_{n} / n}$, since $a_{n}=\\lambda_{n}=O\\left(\\eta_{n} J_{n, k}\\right), \\eta_{n}=O\\left(\\sqrt{\\ln p_{n} / n}\\right)$ and $J_{n, k} \\leqslant \\alpha$ by (A7), see also Section 3.3.3.\n\n- The conditions on the penalty (P1)-(P3) are satisfied for the Group Lasso by the assumptions on $\\left\\|\\boldsymbol{\\theta}_{t i}^{*}\\right\\|$. (P4) follows from the assumptions on $s_{n}$ and $p_{n}$.\n- That the reduced problem (11) satisfies (A1) follows from the proof of Corollary 1.\n\n- For (A5), our assumptions give $\\sigma_{n}=\\sigma$ which is bounded away from zero and infinity. Further the union bound, Markov's inequality, (17), and (19) imply\n\n$$\n\\begin{aligned}\n\\sum_{i=1}^{n} \\mathbb{P}\\left(\\left\\|\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|_{\\infty}>\\sqrt{n \\sigma^{2} / 4 \\ln p_{n}}\\right) & \\leqslant n p_{n} \\max _{k} \\mathbb{P}\\left(\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)_{k}^{2}>n \\sigma^{2} / 4 \\ln p_{n}\\right) \\\\\n& \\leqslant \\frac{n p_{n} \\max _{k} \\mathbb{E}\\left[\\rho\\left(\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)_{k}^{2}\\right)\\right]}{\\rho\\left(n \\sigma^{2} / 4 \\ln p_{n}\\right)} \\\\\n& =O\\left(\\frac{n p_{n}}{\\rho\\left(n / \\ln p_{n}\\right)}\\right) \\\\\n& =o(1)\n\\end{aligned}\n$$\n\n- Eq. (18) implies (A6), and (A7) holds with the proposed choice of $\\lambda_{n}$.\n- Observe that for $\\left|\\psi^{\\prime}\\right| \\leqslant K$ and $\\boldsymbol{e}_{k}$ the $k$ th unit vector, we have\n\n$$\n\\left|\\phi_{i}(\\boldsymbol{\\theta})_{k}-\\phi_{i}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)_{k}\\right| \\leqslant K\\left|\\boldsymbol{e}_{k}^{\\top} \\boldsymbol{X}_{i}\\right|\\left|\\boldsymbol{X}_{i(1)}^{\\top}\\left(\\boldsymbol{\\theta}_{(1)}-\\boldsymbol{\\theta}_{(1)}\\right)\\right|\n$$\n\nusing that $\\boldsymbol{\\theta}_{(2)}=\\mathbf{0}$ for $\\boldsymbol{\\theta} \\in \\Theta_{n}^{\\prime}$. By our design conditions, we get\n\n$$\n\\max _{1 \\leqslant k \\leqslant p_{n}} \\mathbb{E}\\left[\\left|\\phi_{i}(\\boldsymbol{\\theta})_{k}-\\phi_{i}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)_{k}\\right|^{2}\\right]=O\\left(\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}\\right\\|^{2}\\right)\n$$\n\nas in the proof of Corollary 1, so the first condition of (A8) holds because $\\left(s_{n}^{2}+s_{n} \\ln p_{n}\\right) / n=$ $o(1)$. For the second condition, choose $\\tilde{D}_{n}=K \\sqrt{s_{n}} \\rho^{-1}\\left(n p_{n} \\omega_{n}\\right)$ with $\\omega_{n} \\rightarrow \\infty$ arbitrarily slowly. It holds\n\n$$\n\\begin{aligned}\n\\sum_{i=1}^{n} \\mathbb{P}\\left(\\sup _{\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\prime} \\in \\Theta^{\\prime}} \\frac{\\left\\|\\phi_{i}(\\boldsymbol{\\theta})-\\phi_{i}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)\\right\\|_{\\infty}}{\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}\\right\\|}>\\tilde{D}_{n}\\right) & \\leqslant n \\mathbb{P}\\left(K\\left\\|\\boldsymbol{X}_{i}\\right\\|_{\\infty}\\left\\|\\boldsymbol{X}_{i(1)}\\right\\|>\\tilde{D}_{n}\\right) \\\\\n& \\leqslant n \\mathbb{P}\\left(K \\sqrt{s_{n}}\\left\\|\\boldsymbol{X}_{i}\\right\\|_{\\infty}^{2}>\\tilde{D}_{n}\\right) \\\\\n& \\leqslant n p_{n} \\max _{1 \\leqslant k \\leqslant p_{n}} \\mathbb{P}\\left(\\left|X_{i, k}\\right|^{2}>\\tilde{D}_{n} /\\left(K \\sqrt{s_{n}}\\right)\\right) \\\\\n& \\leqslant O\\left(\\frac{n p_{n}}{\\rho\\left(\\tilde{D}_{n} /\\left(K \\sqrt{s_{n}}\\right)\\right)}\\right) \\\\\n& =O\\left(\\frac{n p_{n}}{\\rho\\left(\\rho^{-1}\\left(n p_{n} \\omega_{n}\\right)\\right)}\\right)=o(1)\n\\end{aligned}\n$$\n\nThis choice satisfies\n\n$$\n\\frac{\\tilde{D}_{n} r_{n}\\left(s_{n}+\\ln p_{n}\\right)}{n \\eta_{n}} \\leqslant \\frac{\\tilde{D}_{n} \\sqrt{s_{n}}\\left(s_{n}+\\ln p_{n}\\right)}{2 \\sigma n}=\\frac{K}{2 \\sigma} \\frac{\\left(s_{n}^{2}+s_{n} \\ln p_{n}\\right) \\rho^{-1}\\left(n p_{n} \\omega_{n}\\right)}{n}=o(1)\n$$\n\nby (19) as required.\n\n- The first two conditions in (A2) can be verified as in the proof of Corollary 1 by the choice $D_{n}=\\rho^{-1}\\left(n \\omega_{n}\\right) \\sqrt{s_{n}}$ and $\\omega_{n} \\rightarrow \\infty$ arbitrarily slowly. This is a valid choice because\n\n$$\n\\frac{\\rho^{-1}\\left(n \\omega_{n}\\right) \\sqrt{s_{n}} \\bar{r}_{n} s_{n}}{\\sqrt{n}}=\\frac{\\rho^{-1}\\left(n \\omega_{n}\\right) s_{n}^{2} \\sqrt{\\ln p_{n}}}{n} \\leqslant \\frac{\\rho^{-1}\\left(n p_{n}\\right) s_{n}^{2} \\sqrt{\\ln p_{n}}}{n}=o(1)\n$$\n\nby (19). Similarly, it follows\n\n$$\n\\sup _{\\|\\boldsymbol{u}\\| \\leqslant r_{n} C}\\left\\|A_{n}\\left[J\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-J\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\|=O\\left(r_{n}\\right)=o\\left(\\frac{1}{\\sqrt{n r_{n}}}\\right)\n$$\n\nbecause\n\n$$\n\\tilde{r}_{n}^{2} \\sqrt{n}=\\frac{s_{n} \\ln p_{n}}{\\sqrt{n}}=o(1)\n$$\n\nby (19). Assumptions (A3) is verified exactly as in Corollary 1, so asymptotic normality of $\\tilde{\\boldsymbol{\\theta}}_{(1)}$ follows from Theorem 7.\n\n- To verify the conditions of Theorem 6, we can choose $H_{n}$ as in Corollary 1. This construction is independent of $\\boldsymbol{\\theta}^{*}$. Since $\\left|\\psi^{\\prime}\\right|$ is bounded away from zero, the eigenvalue condition implies that (A4)(i) and (ii) hold for all $\\boldsymbol{\\theta} \\in \\Theta_{n}$. It remains to verify (A4)(iii) with $\\nu_{n}=s_{n}$. As $\\max _{1 \\leqslant k \\leqslant p_{n}} \\mathbb{E}\\left[X_{i, k}^{4}\\right]=O(1)$ by (15), the first condition holds since $s_{n}^{2} \\ln p_{n} / n=o(1)$. For the second condition, choose $\\tilde{B}_{n}=\\rho^{-1}\\left(n p_{n} \\omega_{n}\\right)$ with $\\omega_{n} \\rightarrow \\infty$ arbitrarily slowly. Then\n\n$$\n\\begin{aligned}\n\\sum_{i=1}^{n} \\mathbb{P}\\left(\\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|X_{i, j} X_{i, k}\\right|>\\tilde{B}_{n}\\right) & \\leqslant \\sum_{i=1}^{n} \\mathbb{P}\\left(\\max _{1 \\leqslant k \\leqslant p_{n}} X_{i, k}^{2}>\\tilde{B}_{n}\\right) \\\\\n& \\leqslant n p_{n} \\max _{1 \\leqslant j, k \\leqslant p_{n}} \\mathbb{P}\\left(X_{i, k}^{2}>\\tilde{B}_{n}\\right) \\\\\n& \\leqslant \\frac{n p_{n} \\max _{1 \\leqslant j, k \\leqslant p_{n}} \\mathbb{E}\\left[\\rho\\left(X_{i, k}^{2}\\right)\\right]}{\\rho\\left(\\tilde{B}_{n}\\right)} \\\\\n& =O\\left(\\frac{n p_{n}}{n p_{n} \\omega_{n}}\\right)=o(1)\n\\end{aligned}\n$$\n\nusing (17). This choice satisfies $\\tilde{B}_{n}=o\\left(n /\\left(s_{n} \\ln p_{n}\\right)\\right)$ since $\\rho^{-1}\\left(n p_{n}\\right) s_{n} \\ln p_{n} / n=o(1)$ by (19).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 32,
      "text": "# B. 3 Proof of Corollary 4 \n\nWe first verify (A1). Since\n\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\phi_{i}(\\boldsymbol{\\theta})=\\left(\\begin{array}{cc}\n\\operatorname{diag}\\left(K_{n} \\mathbb{1}\\left\\{k_{i}=k\\right\\} \\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}\\right)\\right)_{k=1, \\ldots, K_{n}} & \\mathbf{0} \\\\\n\\frac{1}{K_{n}} \\mathbf{1}^{\\top} & -1\n\\end{array}\\right)\n$$\n\nwe can choose\n\n$$\nH_{n}\\left(\\boldsymbol{x}_{i}, k_{i}\\right)=\\left(\\begin{array}{cc}\n\\operatorname{diag}\\left(K_{n} \\mathbb{1}\\left\\{k_{i}=k\\right\\}\\left(-\\inf _{\\theta_{k} \\in \\Theta^{0}}\\left|\\psi^{\\prime}\\left(\\boldsymbol{x}_{i} ; \\theta_{k}\\right)\\right|\\right)\\right)_{k=1, \\ldots, K_{n}}+I_{K_{n}} / \\sqrt{4 K_{n}} & \\mathbf{0} \\\\\n\\mathbf{0} & -1+1 / \\sqrt{4 K_{n}}\n\\end{array}\\right)\n$$\n\nby Lemma 4. Then\n$\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}, k_{i}\\right)\\right]=\\left(\\begin{array}{cc}\\operatorname{diag}\\left(\\mathbb{E}\\left[-\\inf _{\\theta_{k} \\in \\Theta^{0}}\\left|\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}\\right)\\right|\\right]\\right)_{k=1, \\ldots, K_{n}}+I_{K_{n}} / \\sqrt{4 K_{n}} & \\mathbf{0} \\\\ \\mathbf{0} & -1+1 / \\sqrt{4 K_{n}} \\end{array}\\right)$,\nso $\\lim \\sup _{n \\rightarrow \\infty} \\lambda_{\\max }\\left(n^{-1} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right)<0$ because $\\psi^{\\prime}$ is negative and bounded away from 0 . Further,\n\n$$\n\\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}, k_{i}\\right)^{2}\\right]=\\left(\\begin{array}{cc}\n\\operatorname{diag}\\left(K_{n}^{2} \\mathbb{1}\\left\\{k_{i}=k\\right\\} \\mathbb{E}\\left[\\inf _{\\theta_{k} \\in \\Theta^{0}}\\left|\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}\\right)\\right|^{2}\\right]\\right)_{k=1, \\ldots, K_{n}} & \\mathbf{0} \\\\\n\\mathbf{0} & \\left(1+1 / \\sqrt{4 K_{n}}\\right)^{2}\n\\end{array}\\right)\n$$\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}, k_{i}\\right)^{2}\\right]=\\binom{\\operatorname{diag}\\left(K_{n} \\mathbb{E}\\left[\\inf _{\\theta_{k} \\in \\Theta^{0}}\\left|\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}\\right)\\right|^{2}\\right]\\right)_{k=1, \\ldots, K_{n}}}{\\mathbf{0}} \\cdot \\begin{array}{c}\n\\mathbf{0} \\\\\n\\left.1-1 / \\sqrt{4 K_{n}}\\right)^{2}\n\\end{array}\n$$\n\nusing $n_{k} / n=1 / K_{n}$. This gives $n^{-1}\\left\\|\\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)^{2}\\right]\\right\\|=O\\left(K_{n}\\right)$ because $\\left|\\psi^{\\prime}\\left(\\boldsymbol{x}_{i} ; \\theta\\right)\\right|$ is bounded. Since $\\max _{1 \\leqslant i \\leqslant n}\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\|=O\\left(K_{n}\\right)$, we can choose $B_{n}=K_{n} C$ in (A1) with some large enough $C$. Then, (A1) is satisfied since $K_{n} \\ln K_{n} / n \\rightarrow 0$. Because the given $H_{n}\\left(\\boldsymbol{x}_{i}, k_{i}\\right)$ is valid for all $\\boldsymbol{\\theta} \\in \\Theta_{0}^{K_{n}+1}$, the solution is also unique by Theorem 2. Next, we have\n\n$$\nI\\left(\\boldsymbol{\\theta}^{*}\\right)=K_{n} \\operatorname{diag}\\left(\\mathbb{E}\\left[\\psi\\left(\\boldsymbol{X}_{i} ; \\theta_{1}^{*}\\right)^{2}\\right], \\ldots, \\mathbb{E}\\left[\\psi\\left(\\boldsymbol{X}_{i} ; \\theta_{K_{n}}^{*}\\right)^{2}\\right], 0\\right)\n$$\n\nwhich implies that the convergence rate of the stacked parameter vector $\\tilde{\\boldsymbol{\\theta}}$ is $\\sqrt{\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)\\right) / n}=$ $\\sqrt{K_{n}^{2} / n}=\\sqrt{K_{n} / n_{1}}$.\n\nWe now verify (A2) and (A3). It holds\n\n$$\nJ\\left(\\boldsymbol{\\theta}^{*}\\right)=\\left(\\begin{array}{cc}\n\\operatorname{diag}\\left(\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{1}^{*}\\right)\\right], \\ldots, \\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{K_{n}}^{*}\\right)\\right]\\right) & \\mathbf{0} \\\\\n\\frac{1}{K_{n}} \\mathbf{1}^{\\top} & -1\n\\end{array}\\right)\n$$\n\nfor which the block inversion formula yields\n\n$$\nJ\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1}=\\left(\\begin{array}{cc}\n\\operatorname{diag}\\left(\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{1}^{*}\\right)\\right]^{-1}, \\ldots, \\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{K_{n}}^{*}\\right)\\right]^{-1}\\right) & \\mathbf{0} \\\\\n-\\frac{1}{K_{n}} \\operatorname{vec}\\left(\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{1}^{*}\\right)\\right]^{-1}, \\ldots, \\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{K_{n}}^{*}\\right)\\right]^{-1}\\right) & -1\n\\end{array}\\right)\n$$\n\nChoosing $A_{n}=\\boldsymbol{a}_{n}^{\\top} J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1} \\in \\mathbb{R}^{1 \\times p_{n}}$ with $\\boldsymbol{a}_{n}^{\\top}=(0, \\ldots, 0,1)$ gives the statement for $\\hat{\\theta}_{K_{n}+1}$, as\n\n$$\nA_{n}=\\boldsymbol{a}_{n}^{\\top} J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1}=\\operatorname{vec}\\left(-\\frac{1}{K_{n}} \\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{1}^{*}\\right)\\right]^{-1}, \\ldots,-\\frac{1}{K_{n}} \\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{K_{n}}^{*}\\right)\\right]^{-1},-1\\right)\n$$\n\nand, therefore,\n\n$$\n\\boldsymbol{a}_{n}^{\\top} J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1} I\\left(\\boldsymbol{\\theta}^{*}\\right) J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-\\top} \\boldsymbol{a}_{n}=\\frac{1}{K_{n}} \\sum_{k=1}^{K_{n}} \\frac{\\mathbb{E}\\left[\\psi\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)^{2}\\right]}{\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)\\right]^{2}}\n$$\n\nWe have\n\n$$\n\\begin{aligned}\n& A_{n}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}^{\\prime}\\right)\\right] \\\\\n& =u_{K_{n}+1}-u_{K_{n}+1}^{\\prime}-\\frac{1}{K_{n}} \\sum_{k=1}^{K_{n}}\\left(u_{k}-u_{k}^{\\prime}\\right)-\\sum_{k=1}^{K_{n}} \\frac{\\mathbb{1}\\left\\{k_{i}=k\\right\\} \\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}+\\tilde{u}_{k}\\right)\\left(u_{k}-u_{k}^{\\prime}\\right)}{\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\theta_{k}^{*}\\right)\\right]}=O\\left(\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\|\\right)\n\\end{aligned}\n$$\n\nsince $\\psi^{\\prime}$ is bounded away from 0 and $-\\infty$. Choosing $D_{n}=C$ with some large enough $C$, the first two conditions in (A2) are satisfied since $r_{n}^{2} p_{n}=K_{n}^{3} / n \\rightarrow 0$. Further\n\n$$\n\\left.A_{n}\\left[J\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-J\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]=-\\frac{1}{K_{n}} \\operatorname{vec}\\left(\\frac{\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\theta_{1}^{*}+u_{1}\\right)-\\psi^{\\prime}\\left(\\theta_{1}^{*}\\right)\\right]}{\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\theta_{1}^{*}\\right)\\right]}, \\ldots, \\frac{\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\theta_{K_{n}}^{*}+u_{K_{n}}\\right)-\\psi^{\\prime}\\left(\\theta_{K_{n}}^{*}\\right)\\right]}{\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\theta_{K_{n}}^{*}\\right)\\right]}, 0\\right)\\right.\n$$\n\nSince $\\psi^{\\prime}$ is Lipschitz and $\\left\\|\\boldsymbol{a}^{\\top}\\right\\| \\leqslant \\sqrt{K_{n}} \\max _{i}\\left|a_{i}\\right|$, we have $\\left\\|A_{n}\\left[J\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-J\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\| /\\|\\boldsymbol{u}\\|=$ $O\\left(K_{n}^{-1 / 2}\\right)$, so the third condition in (A2) holds since $K_{n}^{3} / n \\rightarrow 0$ We further have\n\n$$\nA_{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)=-\\sum_{k=1}^{K_{n}} \\frac{\\mathbb{1}\\left\\{k_{i}=k\\right\\} \\psi\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)}{\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)\\right]}-\\frac{1}{K_{n}} \\sum_{k=1}^{K_{n}} \\theta_{k}^{*}-\\theta_{K_{n}+1}^{*}=-\\sum_{k=1}^{K_{n}} \\frac{\\mathbb{1}\\left\\{k_{i}=k\\right\\} \\psi\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)}{\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)\\right]}\n$$\n\nThis implies that $\\max _{1 \\leqslant i \\leqslant n} \\mathbb{E}\\left[\\left\\|A_{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|^{4}\\right]=O(1)$ using that $\\max _{i, k} \\mathbb{E}\\left[\\psi\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)^{4}\\right]=O(1)$ since $\\psi$ is bounded and $\\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)\\right]$ is bounded away from 0 . This verifies (A3).\n\nTo obtain the asymptotic distribution of $\\sqrt{n / K_{n}}\\left(\\hat{\\theta}_{k}-\\theta_{k}^{*}\\right)$, choose $\\boldsymbol{a}_{n}^{\\top}=K_{n}^{-1 / 2} \\boldsymbol{e}_{k}^{\\top}$ and\n\n$$\nA_{n}=\\boldsymbol{a}_{n}^{\\top} J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1}=K_{n}^{-1 / 2} \\mathbb{E}\\left[\\psi^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)\\right]^{-1} \\boldsymbol{e}_{k}^{\\top}\n$$\n\nSimple calculations yield $O(1), D_{n}=\\sqrt{K_{n}} C$ and $O\\left(K_{n}^{-1 / 2}\\right)$ for the three quantities in (A2), so the conditions of Theorem 3 are satisfied since $K_{n}^{3} / n \\rightarrow 0$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 33,
      "text": "# B. 4 Proof of Corollary 5 \n\nDue to simplicity of the estimating equation, most regularity conditions are automatic. In particular, $J(\\boldsymbol{\\theta})=-I_{p_{n}}$ so that conditions (A6) becomes void and (A7) holds with $J_{n, k}=1$ for all $k$ and\n\n$$\n\\eta_{n}=2 \\max _{1 \\leqslant k \\leqslant K_{n}} \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\frac{n^{2}}{n_{k}^{2}} \\mathbb{E}\\left[\\phi_{k}\\left(\\boldsymbol{X}_{i}, \\theta_{k}^{*}\\right)^{2}\\right] \\ln K_{n}}{n}}=2 \\max _{1 \\leqslant k \\leqslant K_{n}} \\sqrt{\\frac{\\mathbb{E}\\left[\\left(q_{k}\\left(\\boldsymbol{X}_{i}\\right)-a-\\theta_{k}^{*}\\right)^{2}\\right] \\ln K_{n}}{n_{k}}}\n$$\n\nSince $\\phi_{i}(\\boldsymbol{\\theta})_{k}-\\phi_{i}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)_{k}=\\mathbb{1}\\left\\{k_{i}=k\\right\\} \\frac{n}{n_{k}}\\left(\\theta_{k^{\\prime}}-\\theta_{k}\\right)$, (A8) can be verified using and $s_{n}^{2} \\ln K_{n} / \\min _{k} n_{k} \\rightarrow$ 0 . Further, (P1) is satisfied because either $\\theta_{k}^{*}=0$ or $\\theta_{k}^{*} / \\bar{r}_{n} \\rightarrow \\infty$, (P2) always holds for the Lasso and (P3) is easily verified with $\\mu_{n}=0$. As\n\n$$\n\\left\\langle\\boldsymbol{u}, \\phi_{i}(\\boldsymbol{\\theta}+\\boldsymbol{u})-\\phi_{i}(\\boldsymbol{\\theta})\\right\\rangle=-\\sum_{k=1}^{K_{n}} \\mathbb{1}\\left\\{k_{i}=k\\right\\} \\frac{n}{n_{k}} u_{k}^{2}\n$$\n\nthe $s_{n}$-dimensional subproblem satisfies (A1) with $H_{n}\\left(k_{i}, \\boldsymbol{X}_{i}\\right)=-\\operatorname{diag}\\left(\\mathbb{1}\\left\\{k_{i}=k\\right\\} \\frac{n}{n_{k}}\\right)_{k=1, \\ldots, s_{n}}$ and $\\ln K_{n} / \\min _{k} n_{k} \\rightarrow 0$. Existence of a solution with the claimed detection property now follows from Theorem 5. The additional conditions in Theorem 6 also hold with $H_{n}=-\\frac{n}{\\min _{k} n_{k}} I_{p_{n}}$ since $s_{n}^{2} \\ln K_{n} / \\min _{k} n_{k} \\rightarrow 0$, which guarantees uniqueness of the solution.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 34,
      "text": "## B. 5 Proof of Corollary 6\n\nFirst note that we can multiply the first block of $\\phi$ with any $\\kappa>0$ without changing the solution. Denote $\\boldsymbol{X}=(Y, T, \\boldsymbol{Z}, \\boldsymbol{W})$. By the mean value theorem, for any $\\boldsymbol{u}$, there exists $s \\in(0,1)$ such that\n\n$$\n\\langle\\boldsymbol{u}, \\phi(\\boldsymbol{X} ; \\boldsymbol{\\theta}+\\boldsymbol{u})-\\phi(\\boldsymbol{X} ; \\boldsymbol{\\theta})\\rangle=\\boldsymbol{u}^{\\top} \\nabla_{\\boldsymbol{\\theta}} \\phi(\\boldsymbol{X} ; \\boldsymbol{\\theta}+s \\boldsymbol{u}) \\boldsymbol{u}\n$$\n\nwhere\n\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\phi(\\boldsymbol{X} ; \\boldsymbol{\\theta})=\\left(\\begin{array}{cc}\n\\kappa\\left[T\\left(\\ln \\sigma)^{\\prime \\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)+(1-T)\\left(\\ln \\bar{\\sigma}\\right)^{\\prime \\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)\\right] \\boldsymbol{W} \\boldsymbol{W}^{\\top}\\right. & 0 \\\\\n\\left[\\frac{Y T}{\\sigma\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}}+\\frac{Y(1-T)}{\\bar{\\sigma}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}}\\right] \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right) \\boldsymbol{Z} \\boldsymbol{W}^{\\top} & -\\boldsymbol{Z} \\boldsymbol{Z}^{\\top}\n\\end{array}\\right)\n$$\n\nTo obtain suitable matrices $H_{n}(\\boldsymbol{X})$, we distinguish the two cases $T=0,1$. Consider first the case $T=1$, in which we can simplify\n\n$$\n\\begin{aligned}\n& \\boldsymbol{u}^{\\top} \\nabla_{\\boldsymbol{\\theta}} \\phi(\\boldsymbol{X} ; \\boldsymbol{\\theta}) \\boldsymbol{u} \\\\\n= & \\boldsymbol{u}^{\\top}\\left(\\begin{array}{cc}\n\\kappa(\\ln \\sigma)^{\\sigma}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right) \\boldsymbol{W} \\boldsymbol{W}^{\\top} & 0 \\\\\n\\frac{Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}{\\sigma\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}} \\boldsymbol{Z} \\boldsymbol{W}^{\\top} & -\\boldsymbol{Z} \\boldsymbol{Z}^{\\top}\n\\end{array}\\right) \\boldsymbol{u} \\\\\n= & \\left(\\begin{array}{l}\n\\boldsymbol{u}_{1}^{\\top} \\boldsymbol{W} \\\\\n\\boldsymbol{u}_{2}^{\\top} \\boldsymbol{Z}\n\\end{array}\\right)\\left(\\begin{array}{cc}\n\\kappa(\\ln \\sigma)^{\\sigma}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right) & 0 \\\\\n\\frac{Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}{\\sigma\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}} & -1\n\\end{array}\\right)\\binom{\\boldsymbol{W}^{\\top} \\boldsymbol{u}_{1}}{\\boldsymbol{Z}^{\\top} \\boldsymbol{u}_{2}} \\\\\n\\leqslant & \\left(\\begin{array}{l}\n\\boldsymbol{u}_{1}^{\\top} \\boldsymbol{W} \\\\\n\\boldsymbol{u}_{2}^{\\top} \\boldsymbol{Z}\n\\end{array}\\right)\\left(\\begin{array}{cc}\n\\kappa(\\ln \\sigma)^{\\sigma}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)+ & \\left|\\frac{Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}{2 \\sigma\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}}\\right| & 0 \\\\\n0 & -1+\\left|\\frac{Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}{2 \\sigma\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}}\\right|\n\\end{array}\\right)\\binom{\\boldsymbol{W}^{\\top} \\boldsymbol{u}_{1}}{\\boldsymbol{Z}^{\\top} \\boldsymbol{u}_{2}}\n\\end{aligned}\n$$\n\nusing Lemma 4. By the same arguments, we get a similar result for $T=0$. Denoting $\\bar{\\sigma}=1-\\sigma$, we obtain\n\n$$\n\\begin{aligned}\n& \\boldsymbol{u}^{\\top} \\nabla_{\\boldsymbol{\\theta}} \\phi(\\boldsymbol{X} ; \\boldsymbol{\\theta}) \\boldsymbol{u} \\\\\n\\leqslant & \\left(\\begin{array}{l}\n\\boldsymbol{u}_{1}^{\\top} \\boldsymbol{W} \\\\\n\\boldsymbol{u}_{2}^{\\top} \\boldsymbol{Z}\n\\end{array}\\right)\\left(\\begin{array}{cc}\n\\kappa(\\ln \\bar{\\sigma})^{\\sigma}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)+\\left|\\frac{Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}{2 \\bar{\\sigma}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}}\\right| & 0 \\\\\n0 & -1+\\left|\\frac{Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)}{2 \\bar{\\sigma}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}}\\right|\n\\end{array}\\right)\\binom{\\boldsymbol{W}^{\\top} \\boldsymbol{u}_{1}}{\\boldsymbol{Z}^{\\top} \\boldsymbol{u}_{2}}\n\\end{aligned}\n$$\n\nBy the assumptions on $\\boldsymbol{W}, \\sigma$ and $Y$, there is $K \\in(0, \\infty)$ such that\n\n$$\n\\left|Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right) /\\left(2 \\sigma\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}\\right)\\right| \\leqslant K, \\quad\\left|Y \\sigma^{\\prime}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right) /\\left(2 \\bar{\\sigma}\\left(\\boldsymbol{W}^{\\top} \\boldsymbol{\\theta}_{1}\\right)^{2}\\right)\\right| \\leqslant K, \\quad\\left\\|\\mathbb{E}\\left[\\boldsymbol{W} \\boldsymbol{W}^{\\top}\\right]\\right\\| \\leqslant K\n$$\n\nNow the matrix\n\n$$\nH_{n}(T, Y, \\boldsymbol{W})=\\left(\\begin{array}{cc}\n{\\left[\\kappa \\alpha_{1}(T, \\boldsymbol{W})+K\\right] \\boldsymbol{W} \\boldsymbol{W}^{\\top}} & 0 \\\\\n0 & \\alpha_{2}(T, Y, \\boldsymbol{W}) \\boldsymbol{Z} \\boldsymbol{Z}^{\\top}\n\\end{array}\\right)\n$$\n\nsatisfies (A1)(i)-(ii) with $\\kappa \\geqslant 2 K^{2} / c$. Finally, because $\\alpha_{1}$ and $\\alpha_{2}$ are uniformly bounded, (A1)(iii), (A2) and (A3) can be verified exactly as for the generalized linear model (Corollary 1).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 35,
      "text": "# B. 6 Proof of Corollary 7 \n\nConditions (P1), (P2) (since $\\sqrt{n / s_{n}} \\min _{1 \\leqslant k \\leqslant s_{n}} \\theta_{k}^{*} \\rightarrow \\infty$ ) and (P4) are satisfied by the SCAD penalty. Consistency with $\\tilde{r}_{n}=\\sqrt{s_{n} / n}$ and asymptotic normality of $\\tilde{\\boldsymbol{\\theta}}_{(1)}$ as well as its oracle property (efficiency) follow from Corollary $6, \\rho^{-1}(n) s_{n}^{2} / n \\rightarrow 0$ and the properties of the SCAD penalty with $\\lambda_{n} \\rightarrow 0$.\n(A5) follows from the assumptions on $p_{n}$, see the proof of Corollary 2. (A6) always holds for SCAD. The proposed choice of $\\lambda_{n}$ satisfies (A7) and (A8) can be verified as in the proof of Corollary 2, so $\\tilde{\\boldsymbol{\\theta}}_{(2)}=\\mathbf{0}$ with probability tending to 1 . (A4) follows from the assumptions on $p_{n}$. The assumption on $a$, which determines the non-convexity of SCAD, implies that (P3) holds with $\\mu<2 c$, so $\\tilde{\\boldsymbol{\\theta}}$ is unique with probability tending to 1 by Theorem 6 .",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 36,
      "text": "# B. 7 Proof of Corollary 8 \n\nThe Jacobian of $\\phi$ is a bidiagonal matrix with\n\nWe have\n\n$$\n\\begin{aligned}\n& K_{n}^{-1} \\boldsymbol{u}^{\\top} \\nabla \\phi(\\boldsymbol{X} ; \\boldsymbol{\\theta}) \\boldsymbol{u} \\\\\n& =-\\sum_{k=1}^{K_{n}} u_{k}^{2} \\mathbb{1}_{i \\in \\mathcal{B}_{k}}+\\sum_{k=1}^{K_{n}-1} u_{k} u_{k+1}\\left[1-\\alpha f^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}\\right)\\right] \\mathbb{1}_{i \\in \\mathcal{B}_{k+1}} \\\\\n& \\leqslant-\\sum_{k=1}^{K_{n}} u_{k}^{2} \\mathbb{1}_{i \\in \\mathcal{B}_{k}}+(1-\\alpha \\kappa) \\sum_{k=1}^{K_{n}-1} \\sqrt{u_{k}^{2} u_{k+1}^{2}} \\mathbb{1}_{i \\in \\mathcal{B}_{k+1}} \\\\\n& \\leqslant-\\sum_{k=1}^{K_{n}} u_{k}^{2} \\mathbb{1}_{i \\in \\mathcal{B}_{k}}+(1-\\alpha \\kappa) \\sum_{k=1}^{K_{n}-1}\\left(u_{k}^{2}+u_{k+1}^{2}\\right) \\mathbb{1}_{i \\in \\mathcal{B}_{k+1}} \\\\\n& \\leqslant-\\sum_{k=1}^{K_{n}} u_{k}^{2} \\mathbb{1}_{i \\in \\mathcal{B}_{k}}+(1-\\alpha \\kappa) \\sum_{k=1}^{K_{n}} u_{k}^{2} \\mathbb{1}_{i \\in \\mathcal{B}_{k}} \\\\\n& \\leqslant-\\alpha \\kappa \\sum_{k=1}^{K_{n}} u_{k}^{2} \\mathbb{1}_{i \\in \\mathcal{B}_{k}}\n\\end{aligned}\n$$\n\nHence, (A1) is satisfied with\n\n$$\nH_{n}(\\boldsymbol{x})=-\\alpha \\kappa K_{n} \\operatorname{diag}\\left(\\mathbb{1}_{i \\in \\mathcal{B}_{k}}\\right)_{k=1, \\ldots, K_{n}}\n$$\n\n$c=\\alpha \\kappa, B_{n}=K_{n} \\ln n$ and $K_{n}^{3} / n \\rightarrow 0$. Further, (A1)(i) is valid for all $\\boldsymbol{\\theta} \\in \\mathbb{R}^{K_{n}}$. Finally,\n\n$$\nI\\left(\\boldsymbol{\\theta}^{*}\\right)=\\operatorname{Cov}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]=\\alpha^{2} K_{n} \\operatorname{diag}\\left(\\operatorname{Var}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{k-1}^{*}\\right)\\right]\\right)_{k=1, \\ldots, K_{n}}=\\alpha^{2} K_{n} \\Gamma\n$$\n\nso $\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)\\right)=O\\left(K_{n}^{2}\\right)$ and $r_{n}=O\\left(\\sqrt{K_{n}^{2} / n}\\right)$. Now Theorem 1 and Theorem 2 show that, with probability tending to 1 , a unique solution path $\\tilde{\\boldsymbol{\\theta}}$ exists and satisfies $\\left\\|\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|=O_{p}\\left(\\sqrt{K_{n}^{2} / n}\\right)$.\n\nWe now verify the conditions of Theorem 3 for a matrix $\\tilde{A}_{n}$ such that $\\left\\|\\tilde{A}_{n}\\right\\|=1 / \\sqrt{K_{n}}$ to be chosen later. It holds\n\n$$\n\\left\\|\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}^{\\prime}\\right)\\right\\| \\leqslant(2+\\alpha L) K_{n}\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\| \\leqslant 3 K_{n}\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\|\n$$\n\nso the first two conditions of (A2) are satisfied for any matrix $\\tilde{A}_{n}$ with $\\left\\|\\tilde{A}_{n}\\right\\|=O\\left(1 / \\sqrt{K_{n}}\\right)$ and $D_{n}=K_{n}$ since $K_{n}^{3} / n \\rightarrow 0$. Next,\n\n$$\nJ(\\boldsymbol{\\theta})=\\mathbb{E}[\\nabla \\phi(\\boldsymbol{X} ; \\boldsymbol{\\theta})]=\\left(\\begin{array}{cccc}\n-1 & & & \\\\\n1-\\alpha \\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{1}\\right)\\right] & -1 & & \\\\\n& \\ddots & & \\\\\n& & 1-\\alpha \\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{K_{n}-1}\\right)\\right] & -1\n\\end{array}\\right)\n$$\n\nWe have\n\n$$\n\\begin{aligned}\n\\left\\|\\tilde{A}_{n} J\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\tilde{A}_{n} J\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}^{\\prime}\\right)\\right\\| & \\leqslant \\alpha\\left\\|\\tilde{A}_{n}\\right\\| \\sqrt{\\sum_{j=1}^{K_{n}-1}\\left(\\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{k}^{*}+u_{k}\\right)\\right]-\\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{k}^{*}+u_{k}^{\\prime}\\right)\\right]\\right)^{2}} \\\\\n& \\leqslant \\alpha\\left\\|\\tilde{A}_{n}\\right\\| L\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\|=O\\left(\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\| / \\sqrt{K_{n}}\\right) \\\\\n& =o\\left(\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\| /\\left(\\sqrt{n} r_{n}^{2}\\right)\\right)\n\\end{aligned}\n$$\n\nwhere we used $r_{n}^{2}=K_{n}^{2} / n$ and $K_{n}^{3} / n \\rightarrow 0$. This verifies the third condition in (A2). Assumption (A3) also holds because only one entry of $\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)$ can be non-zero at a time and, thus,\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\left\\|\\tilde{A}_{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|^{4}\\right] \\\\\n= & O\\left(1 / K_{n}^{2}\\right) \\times \\mathbb{E}\\left[\\left\\|\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|^{4}\\right] \\\\\n= & O\\left(1 / K_{n}^{2}\\right) \\times K_{n}^{4} \\sum_{k=1}^{K_{n}} \\sum_{j=1}^{K_{n}} \\mathbb{E}\\left[\\left(\\theta_{k-1}^{*}-\\theta_{k}^{*}-f^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)\\right)^{2}\\left(\\theta_{j-1}^{*}-\\theta_{j}^{*}-f^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{j}^{*}\\right)\\right)^{2} \\mathbb{1}_{i \\in \\mathcal{B}_{k} \\cap \\mathcal{B}_{j}}\\right] \\\\\n= & O\\left(K_{n}^{2}\\right) \\times \\sum_{k=1}^{K_{n}} \\mathbb{E}\\left[\\left(\\theta_{k-1}^{*}-\\theta_{k}^{*}-f^{\\prime}\\left(\\boldsymbol{X}_{i} ; \\theta_{k}^{*}\\right)\\right)^{4} \\mathbb{1}_{i \\in \\mathcal{B}_{k}}\\right] \\\\\n= & O\\left(K_{n}^{2}\\right)=o(n)\n\\end{aligned}\n$$\n\nagain using $K_{n}^{3} / n \\rightarrow 0$.\nWe can now apply Theorem 3 and it remains to verify the asymptotic covariance structure. By the inversion formula for bidiagonal matrices,\n\n$$\n\\left(J(\\boldsymbol{\\theta})^{-1}\\right)_{i, j}= \\begin{cases}-1, & i=j \\\\ -\\prod_{\\ell=j}^{i-1}\\left(1-\\alpha \\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{\\ell}\\right)\\right]\\right), & i>j \\\\ 0, & i<j\\end{cases}\n$$\n\nLet $\\tilde{A}_{n}=A_{n} J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1} / \\sqrt{K_{n}}=O\\left(1 / \\sqrt{K_{n}}\\right)$, so that\n\n$$\n\\tilde{A}_{n} I\\left(\\boldsymbol{\\theta}^{*}\\right) \\tilde{A}_{n}^{\\top}=\\frac{1}{K_{n}} A_{n} J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1} I\\left(\\boldsymbol{\\theta}^{*}\\right) J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-\\top} A_{n}^{\\top}\n$$\n\nSince $I\\left(\\boldsymbol{\\theta}^{*}\\right)=\\alpha^{2} K_{n} \\Gamma$ with $\\Gamma$ diagonal, we have for $i \\leqslant j$,\n\n$$\n\\begin{aligned}\n\\Sigma_{n} & :=\\frac{1}{K_{n}}\\left(J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-1} I\\left(\\boldsymbol{\\theta}^{*}\\right) J\\left(\\boldsymbol{\\theta}^{*}\\right)^{-\\top}\\right)_{i, j} \\\\\n& =\\alpha^{2} \\sum_{\\ell=1}^{K_{n}} \\sum_{k=1}^{K_{n}}\\left(J(\\boldsymbol{\\theta})^{-1}\\right)_{i, \\ell} \\Gamma_{\\ell, k}\\left(J(\\boldsymbol{\\theta})^{-1}\\right)_{j, k} \\\\\n& =\\alpha^{2} \\sum_{\\ell=1}^{i} \\sum_{k=1}^{j}\\left(J(\\boldsymbol{\\theta})^{-1}\\right)_{i, \\ell} \\Gamma_{\\ell, k}\\left(J(\\boldsymbol{\\theta})^{-1}\\right)_{j, k} \\\\\n& =\\alpha^{2} \\sum_{k=1}^{i} \\Gamma_{k, k}\\left(J(\\boldsymbol{\\theta})^{-1}\\right)_{i, k}\\left(J(\\boldsymbol{\\theta})^{-1}\\right)_{j, k} \\\\\n& =\\alpha^{2} \\sum_{k=1}^{i} \\Gamma_{k, k}\\left[\\prod_{m=k}^{i-1}\\left(1-\\alpha \\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{m}^{*}\\right)\\right]\\right)\\right]\\left[\\prod_{m=k}^{j-1}\\left(1-\\alpha \\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{m}^{*}\\right)\\right]\\right)\\right] \\\\\n& =\\alpha^{2} \\sum_{k=1}^{i} \\Gamma_{k, k}\\left[\\prod_{m=k}^{i-1}\\left(1-\\alpha \\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{m}^{*}\\right)\\right]\\right)\\right]^{2}\\left[\\prod_{m=i}^{j-1}\\left(1-\\alpha \\mathbb{E}\\left[f^{\\prime}\\left(\\boldsymbol{X} ; \\theta_{m}^{*}\\right)\\right]\\right)\\right]\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 37,
      "text": "# C Lemmas \n\nFor any strictly increasing, convex function $\\beta:(0, \\infty) \\rightarrow(0, \\infty)$ and any positive function $g$, define the norm-like quantity\n\n$$\n\\|g\\|_{\\beta}=\\max _{1 \\leqslant i \\leqslant n} \\mathbb{E}\\left[g\\left(\\boldsymbol{X}_{i}\\right)\\right]+\\frac{1}{n} \\sum_{i=1}^{n} \\beta^{-1}\\left(\\mathbb{E}\\left[\\tilde{\\beta}\\left(\\left|g\\left(\\boldsymbol{X}_{i}\\right)-\\mathbb{E}\\left[g\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right|\\right)\\right]\\right)\n$$\n\nwhere $\\tilde{\\beta}(x)=x \\beta(x)$. This 'norm' measures both the absolute size of $g(\\boldsymbol{X})$ and its concentration. For example, the choice $\\beta(x)=x$ corresponds to\n\n$$\n\\|g\\|_{\\beta}=\\max _{1 \\leqslant i \\leqslant n} \\mathbb{E}\\left[g\\left(\\boldsymbol{X}_{i}\\right)\\right]+\\frac{1}{n} \\sum_{i=1}^{n} \\operatorname{Var}\\left[g\\left(\\boldsymbol{X}_{i}\\right)\\right]\n$$\n\nAnother important example is $\\beta(x)=\\exp (x)$ for exponential concentration inequalities.\nLemma 1. Let $F_{n}$ be a positive function and $\\beta:(0, \\infty) \\rightarrow(0, \\infty)$ strictly increasing and convex. For any $B_{n}$ such that $B_{n} /\\left\\|F_{n}\\right\\|_{\\beta} \\rightarrow \\infty$, it holds\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\int_{B_{n}}^{\\infty} \\mathbb{P}\\left(F_{n}\\left(\\boldsymbol{X}_{i}\\right)>t\\right) d t=o(1)\n$$\n\nProof. Since $B_{n} /\\left\\|F_{n}\\right\\|_{\\beta} \\rightarrow \\infty$ and the convexity of $\\beta$ imply $B_{n} / \\max _{1 \\leqslant i \\leqslant n} \\mathbb{E}\\left[F_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right] \\rightarrow \\infty$, it holds $\\max _{1 \\leqslant i \\leqslant n} \\mathbb{E}\\left[F_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right] \\leqslant B_{n} / 2$ for $n$ large enough. We have\n\n$$\n\\begin{aligned}\n\\frac{1}{n} \\sum_{i=1}^{n} \\int_{B_{n}}^{\\infty} \\mathbb{P}\\left(F_{n}\\left(\\boldsymbol{X}_{i}\\right)>t\\right) d t & \\leqslant \\frac{1}{n} \\sum_{i=1}^{n} \\int_{B_{n}}^{\\infty} \\mathbb{P}\\left(\\left|F_{n}\\left(\\boldsymbol{X}_{i}\\right)-\\mathbb{E}\\left[F_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right|>t-B_{n} / 2\\right) d t \\\\\n& =\\frac{1}{n} \\sum_{i=1}^{n} \\int_{B_{n} / 2}^{\\infty} \\mathbb{P}\\left(\\left|F_{n}\\left(\\boldsymbol{X}_{i}\\right)-\\mathbb{E}\\left[F_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right|>t\\right) d t \\\\\n& =\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[\\left|F_{n}\\left(\\boldsymbol{X}_{i}\\right)-\\mathbb{E}\\left[F_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right| \\mathbb{1}_{F_{n}\\left(\\boldsymbol{X}_{i}\\right)>B_{n} / 2}\\right] \\\\\n& \\stackrel{(*)}{\\leqslant} \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\mathbb{E}\\left[\\tilde{\\beta}\\left(\\left|F_{n}\\left(\\boldsymbol{X}_{i}\\right)-\\mathbb{E}\\left[F_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]\\right)\\right)\\right]}{\\beta\\left(B_{n} / 2\\right)} \\\\\n& \\leqslant \\frac{\\beta\\left(\\left\\|F_{n}\\right\\|_{\\beta}\\right)}{\\beta\\left(B_{n} / 2\\right)}=o(1)\n\\end{aligned}\n$$\n\nsince $B_{n} /\\left\\|F_{n}\\right\\|_{\\beta} \\rightarrow \\infty$. In $(*)$ we use that for a positive random variable $Y$ it holds\n\n$$\n\\mathbb{E}\\left[Y \\mathbb{1}_{Y>B}\\right] \\leqslant \\mathbb{E}\\left[\\frac{Y \\beta(Y)}{\\beta(B)}\\right]\n$$\n\nLemma 2. Suppose that $H_{n}(\\boldsymbol{x})$ is negative semi-definite for all $\\boldsymbol{x} \\in \\mathcal{X}, \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right]=$ $O(1)$, and $B_{n}=o\\left(n / \\ln p_{n}\\right)$. Then\n\n$$\n\\frac{1}{n}\\left\\|\\sum_{i=1}^{n} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)^{2} \\mathbb{1}_{\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\| \\leqslant B_{n}}\\right]\\right\\|=o\\left(n / \\ln p_{n}\\right)\n$$\n\nProof. Using $\\operatorname{tr}(A B) \\leqslant|\\operatorname{tr}(A)| \\| B \\rrbracket$, we have\n\n$$\n\\begin{aligned}\n\\boldsymbol{u}^{\\top} H_{n}\\left(\\boldsymbol{X}_{i}\\right)^{2} \\boldsymbol{u} \\mathbb{1}_{\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\| \\leqslant B_{n}} & =\\operatorname{tr}\\left(\\boldsymbol{u}^{\\top} H_{n}\\left(\\boldsymbol{X}_{i}\\right)^{2} \\boldsymbol{u}\\right) \\mathbb{1}_{\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\| \\leqslant B_{n}} \\\\\n& =\\operatorname{tr}\\left(\\boldsymbol{u} \\boldsymbol{u}^{\\top} H_{n}\\left(\\boldsymbol{X}_{i}\\right)^{2}\\right) \\mathbb{1}_{\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\| \\leqslant B_{n}} \\\\\n& \\leqslant\\left|\\operatorname{tr}\\left(\\boldsymbol{u} \\boldsymbol{u}^{\\top} H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right)\\right|\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\| \\mathbb{1}_{\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\| \\leqslant B_{n}} \\\\\n& \\leqslant\\left|\\boldsymbol{u}^{\\top} H_{n}\\left(\\boldsymbol{X}_{i}\\right) \\boldsymbol{u}\\right| B_{n} \\\\\n& =-\\boldsymbol{u}^{\\top} H_{n}\\left(\\boldsymbol{X}_{i}\\right) \\boldsymbol{u} B_{n}\n\\end{aligned}\n$$\n\nThus,\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{u}^{\\top} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)^{2} \\mathbb{1}_{\\left\\|H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right\\| \\leqslant B_{n}}\\right] \\boldsymbol{u} \\leqslant-\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{u}^{\\top} \\mathbb{E}\\left[H_{n}\\left(\\boldsymbol{X}_{i}\\right)\\right] \\boldsymbol{u} B_{n}\n$$\n\nwhich implies the claim.\nLemma 3. Let $\\boldsymbol{\\lambda}_{n}=\\lambda_{n} \\mathbf{1}$ with $\\lambda_{n} \\rightarrow 0$ and $\\lambda_{n} \\geqslant 2 \\eta_{n}$. Denote $\\tilde{\\Theta}_{n}=\\left\\{\\boldsymbol{\\theta}:\\|\\boldsymbol{\\theta}\\|_{1} \\leqslant k_{n}\\right\\}$ with some $k_{n}=o\\left(\\eta_{n}^{-1}\\right)$ such that $\\boldsymbol{\\theta}^{*} \\in \\tilde{\\Theta}_{n}$. Suppose that (P3) and (A5) hold,\n\n$$\n\\sup _{\\boldsymbol{\\theta} \\in \\tilde{\\Theta}_{n}}\\left\\|p_{\\lambda_{n}}^{\\prime}(\\boldsymbol{\\theta})\\right\\|_{\\infty}=O\\left(\\lambda_{n}\\right)\n$$\n\nand for all $\\|\\boldsymbol{u}\\|=o\\left(\\lambda_{n}\\right)$ and large enough $n$,\n\n$$\n\\left\\langle\\boldsymbol{u}_{(2)}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)_{(2)}\\right\\rangle \\geqslant \\lambda_{n}\\left\\|\\boldsymbol{u}_{(2)}\\right\\|_{1}\n$$\n\nThen if\n\n$$\n\\left\\langle\\boldsymbol{u}, \\frac{1}{n} \\sum_{i=1}^{n}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)-\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right]\\right\\rangle \\geqslant c\\|\\boldsymbol{u}\\|^{2}-c_{1}\\|\\boldsymbol{u}\\|_{1}^{2} \\eta_{n}^{2}\n$$\n\nholds with $\\lim \\sup _{n \\rightarrow \\infty} \\mu_{n}<2 c$, any solution $\\tilde{\\boldsymbol{\\theta}} \\in \\tilde{\\Theta}_{n}$ satisfies\n\n$$\n\\left\\|\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|_{1} \\leqslant \\sqrt{\\nu_{n}}\\left\\|\\tilde{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}^{*}\\right\\|\n$$\n\nfor some $\\nu_{n}=O\\left(s_{n}\\right)$.\n\nProof. Write $\\tilde{\\boldsymbol{\\theta}}=\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}$ with $\\|\\boldsymbol{u}\\|_{1} \\leqslant 2 k_{n}$. We will show that $\\|\\boldsymbol{u}\\|_{1} \\leqslant O\\left(\\sqrt{s_{n}}\\right)\\|\\boldsymbol{u}\\|$. Let $t_{n}=o(1)$ fast enough such that $\\left\\|t_{n} \\boldsymbol{u}\\right\\|=o\\left(\\lambda_{n}\\right)$. It holds,\n\n$$\n\\begin{aligned}\n\\left\\langle\\boldsymbol{u}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right\\rangle & =\\left\\langle\\boldsymbol{u}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+t_{n} \\boldsymbol{u}\\right)\\right\\rangle+\\left\\langle\\boldsymbol{u}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+t_{n} \\boldsymbol{u}\\right)\\right\\rangle \\\\\n& =\\frac{1}{1-t_{n}}\\left\\langle\\boldsymbol{u}\\left(1-t_{n}\\right), p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+t_{n} \\boldsymbol{u}\\right)\\right\\rangle+\\left\\langle\\boldsymbol{u}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+t_{n} \\boldsymbol{u}\\right)\\right\\rangle \\\\\n& \\geqslant-\\frac{\\mu_{n}}{2}\\|\\boldsymbol{u}\\|^{2}+\\left\\langle\\boldsymbol{u}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+t_{n} \\boldsymbol{u}\\right)\\right\\rangle \\\\\n& =-\\frac{\\mu_{n}}{2}\\|\\boldsymbol{u}\\|^{2}+\\left\\langle\\boldsymbol{u}_{(1)}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+t_{n} \\boldsymbol{u}\\right)_{(1)}\\right\\rangle+\\frac{1}{t_{n}}\\left\\langle t_{n} \\boldsymbol{u}_{(2)}, p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+t_{n} \\boldsymbol{u}\\right)_{(2)}\\right\\rangle \\\\\n& \\geqslant-\\frac{\\mu_{n}}{2}\\|\\boldsymbol{u}\\|^{2}-O\\left(\\lambda_{n}\\right)\\left\\|\\boldsymbol{u}_{(1)}\\right\\|_{1}+\\lambda_{n}\\left\\|\\boldsymbol{u}_{(2)}\\right\\|_{1}\n\\end{aligned}\n$$\n\nusing (P3) and $\\left|1-t_{n}\\right| \\leqslant 1$ in the first, and (27)-(28) in the second inequality. Furthermore, by (29), (A5) and Lemma 13,\n\n$$\n\\begin{aligned}\n\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right\\rangle & =\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\rangle+\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\rangle \\\\\n& \\leqslant-c\\|\\boldsymbol{u}\\|^{2}+c_{1} \\eta_{n}^{2}\\|\\boldsymbol{u}\\|_{1}^{2}+\\eta_{n}\\|\\boldsymbol{u}\\|_{1} \\\\\n& \\leqslant-c\\|\\boldsymbol{u}\\|^{2}+[1+o(1)] \\eta_{n}\\|\\boldsymbol{u}\\|_{1} \\\\\n& =-c\\|\\boldsymbol{u}\\|^{2}+O\\left(\\lambda_{n}\\right)\\left\\|\\boldsymbol{u}_{(1)}\\right\\|_{1}+[1+o(1)] \\eta_{n}\\left\\|\\boldsymbol{u}_{(2)}\\right\\|_{1}\n\\end{aligned}\n$$\n\nwhere we have used $\\eta_{n}\\|\\boldsymbol{u}\\|_{1} \\leqslant \\eta_{n} k_{n}=o(1)$ in the third and $\\lambda_{n} \\geqslant 2 \\eta_{n}$ in the last step. Together this yields\n\n$$\n\\begin{aligned}\n0 & =\\left\\langle\\boldsymbol{u}, \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)\\right\\rangle \\\\\n& \\leqslant-\\left(c-\\mu_{n} / 2\\right)\\|\\boldsymbol{u}\\|^{2}+O\\left(\\lambda_{n}\\right)\\left\\|\\boldsymbol{u}_{(1)}\\right\\|_{1}-\\frac{\\lambda_{n}}{2}[1+o(1)]\\left\\|\\boldsymbol{u}_{(2)}\\right\\|_{1}\n\\end{aligned}\n$$\n\nas $\\eta_{n}-\\lambda_{n} \\leqslant-\\lambda_{n} / 2$ by assumption. Since $c-\\mu_{n} / 2$ is strictly positive asymptotically, it must hold\n\n$$\n\\left\\|\\boldsymbol{u}_{(2)}\\right\\|_{1} \\leqslant O(1)\\left\\|\\boldsymbol{u}_{(1)}\\right\\|_{1}\n$$\n\nNow the claim follows from\n\n$$\n\\|\\boldsymbol{u}\\|_{1}=\\left\\|\\boldsymbol{u}_{(1)}\\right\\|_{1}+\\left\\|\\boldsymbol{u}_{(2)}\\right\\|_{1} \\leqslant O(1)\\left\\|\\boldsymbol{u}_{(1)}\\right\\|_{1} \\leqslant O\\left(\\sqrt{s_{n}}\\right)\\left\\|\\boldsymbol{u}_{(1)}\\right\\| \\leqslant O\\left(\\sqrt{s_{n}}\\right)\\|\\boldsymbol{u}\\|\n$$\n\nLemma 4. For any $A \\in \\mathbb{R}^{q_{1} \\times q_{1}}, B \\in \\mathbb{R}^{q_{1} \\times q_{2}}, C \\in \\mathbb{R}^{q_{2} \\times q_{1}}, D \\in \\mathbb{R}^{q_{2} \\times q_{2}}, q_{1}, q_{2} \\in \\mathbb{N}$, it holds\n\n$$\n\\left(\\begin{array}{cc}\nA & B \\\\\nC & D\n\\end{array}\\right) \\leq\\left(\\begin{array}{cc}\nA+I_{q_{1}}(\\|B\\|+\\|C\\|) / 2 & 0 \\\\\n0 & D+I_{q_{2}}(\\|B\\|+\\|C\\|) / 2\n\\end{array}\\right)\n$$\n\nProof. Let $\\boldsymbol{y}_{1} \\in \\mathbb{R}^{q_{1}}$ and $\\boldsymbol{y}_{2} \\in \\mathbb{R}^{q_{2}}$ arbitrary. Then using sub-multiplicativity of the norm (first step) and the AM-GM inequality (second step), we get\n\n$$\n\\begin{aligned}\n& \\left(\\boldsymbol{y}_{1}^{\\top} \\quad \\boldsymbol{y}_{2}^{\\top}\\right)\\left(\\begin{array}{cc}\nA & B \\\\\nC & D\n\\end{array}\\right)\\binom{\\boldsymbol{y}_{1}}{\\boldsymbol{y}_{2}} \\\\\n= & \\boldsymbol{y}_{1}^{\\top} A \\boldsymbol{y}_{1}+\\boldsymbol{y}_{1}^{\\top} B \\boldsymbol{y}_{2}+\\boldsymbol{y}_{2}^{\\top} C \\boldsymbol{y}_{1}+\\boldsymbol{y}_{2}^{\\top} D \\boldsymbol{y}_{2} \\\\\n\\leqslant & \\boldsymbol{y}_{1}^{\\top} A \\boldsymbol{y}_{1}+\\left\\|\\boldsymbol{y}_{1}\\right\\|\\left\\|\\boldsymbol{y}_{2}\\right\\|(\\|B\\|+\\|C\\|)+\\boldsymbol{y}_{2}^{\\top} D \\boldsymbol{y}_{2} \\\\\n\\leqslant & \\boldsymbol{y}_{1}^{\\top} A \\boldsymbol{y}_{1}+\\frac{1}{2}\\left(\\left\\|\\boldsymbol{y}_{1}\\right\\|^{2}+\\left\\|\\boldsymbol{y}_{2}\\right\\|^{2}\\right)(\\|B\\|+\\|C\\|)+\\boldsymbol{y}_{2}^{\\top} D \\boldsymbol{y}_{2} \\\\\n= & \\left(\\boldsymbol{y}_{1}^{\\top} \\quad \\boldsymbol{y}_{2}^{\\top}\\right)\\left(\\begin{array}{cc}\nA+I_{q_{1}}(\\|B\\|+\\|C\\|) / 2 & 0 \\\\\n0 & D+I_{q_{2}}(\\|B\\|+\\|C\\|) / 2\n\\end{array}\\right)\\binom{\\boldsymbol{y}_{1}}{\\boldsymbol{y}_{2}}\n\\end{aligned}\n$$\n\nLemma 5. It holds\n\n$$\n\\left\\|\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|=O_{p}\\left(\\sqrt{\\frac{\\operatorname{tr}\\left(I\\left(\\boldsymbol{\\theta}^{*}\\right)\\right)}{n}}\\right)\n$$\n\nProof. Using $Y=O_{p}\\left(\\mathbb{E}\\left(Y^{2}\\right)^{1 / 2}\\right)$ and $\\|\\boldsymbol{u}\\|=\\sqrt{\\operatorname{tr}\\left(\\boldsymbol{u} \\boldsymbol{u}^{\\top}\\right)}$, we obtain\n\n$$\n\\left\\|\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|=O_{p}\\left(\\sqrt{\\mathbb{E}\\left[\\operatorname{tr}\\left(\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right) \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)^{\\top}\\right)\\right]}\\right)=O_{p}\\left(\\sqrt{\\operatorname{tr}\\left(\\mathbb{E}\\left[\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right) \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)^{\\top}\\right]\\right)}\\right)\n$$\n\nand\n\n$$\n\\mathbb{E}\\left[\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right) \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)^{\\top}\\right]=\\operatorname{Cov}\\left[\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{Cov}\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]=\\frac{1}{n} f\\left(\\boldsymbol{\\theta}^{*}\\right)\n$$\n\nLemma 6. Let $\\mathcal{F}_{n}$ be classes of real-valued functions from $\\mathcal{X}$ to $\\mathbb{R}, F_{n}$ be any function with $\\sup _{f \\in \\mathcal{F}_{n}}|f| \\leqslant F_{n}$, and $B_{n}$ be any sequence.\n(i) It holds\n\n$$\n\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f\\right| \\leqslant \\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{F_{n} \\leqslant B_{n}}\\right|+O_{p}\\left(\\int_{B_{n}}^{\\infty} P \\mathbb{1}_{F_{n}>t} d t,\\right)\n$$\n\n(ii) If $P \\mathbb{1}_{F_{n}>B_{n}}=o(1 / n)$, we have\n\n$$\n\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f\\right| \\leqslant \\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{F_{n} \\leqslant B_{n}}\\right|+o_{p}\\left(\\sqrt{\\frac{\\sup _{f \\in \\mathcal{F}_{n}} P f^{2}}{n}}\\right)\n$$\n\nProof. It holds\n\n$$\n\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f\\right| \\leqslant \\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{F_{n} \\leqslant B_{n}}\\right|+\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{F_{n}>B_{n}}\\right|\n$$\n\nWe prove two different bounds for the second term on the right.\n(i) We have\n\n$$\n\\mathbb{E}\\left[\\sup _{f \\in \\mathcal{F}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{F_{n}>B_{n}}\\right|\\right] \\leqslant \\mathbb{E}\\left[\\mathbb{P}_{n} F_{n} \\mathbb{1}_{F_{n}>B_{n}}\\right]+P F_{n} \\mathbb{1}_{F_{n}>B_{n}} \\leqslant 2 P F_{n} \\mathbb{1}_{F_{n}>B_{n}}\n$$\n\nNoting that $P F_{n} \\mathbb{1}_{F_{n}>B_{n}}=\\int_{B_{n}}^{\\infty} P \\mathbb{1}_{F_{n}>t} d t$ and Markov's inequality give\n\n$$\n\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{F_{n}>B_{n}}\\right|=O_{p}\\left(\\int_{B_{n}}^{\\infty} P \\mathbb{1}_{F_{n}>t} d t\\right)\n$$\n\n(ii) Decompose\n\n$$\n\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{F_{n}>B_{n}}\\right| \\leqslant\\left|\\mathbb{P}_{n} F_{n} \\mathbb{1}_{F_{n}>B_{n}}\\right|+\\sup _{f \\in \\mathcal{F}}\\left|P f \\mathbb{1}_{F_{n}>B_{n}}\\right|\n$$\n\nSince $P \\mathbb{1}_{F_{n}>B_{n}}=o(1 / n)$ it holds\n\n$$\n\\mathbb{P}\\left(\\exists i: F_{n}\\left(\\boldsymbol{X}_{i}\\right)>B_{n}\\right) \\leqslant n P \\mathbb{1}_{F_{n}>B_{n}}=o(1)\n$$\n\nso $\\mathbb{P}_{n} F_{n} \\mathbb{1}_{F_{n}>B_{n}}=0$ with probability tending to 1 . Now the claim follows from the Cauchy Schwarz inequality:\n\n$$\n\\sup _{f \\in \\mathcal{F}}\\left|P f \\mathbb{1}_{F_{n}>B_{n}}\\right| \\leqslant \\sqrt{\\sup _{f \\in \\mathcal{F}} P f^{2}} \\sqrt{P \\mathbb{1}_{F_{n}>B_{n}}}=o_{p}\\left(\\sqrt{\\frac{\\sup _{f \\in \\mathcal{F}_{n}} P f^{2}}{n}}\\right)\n$$\n\nLemma 7. Under assumption (A1)(iii), it holds\n\n$$\n\\left\\|\\left(\\mathbb{P}_{n}-P\\right) H_{n}\\right\\|=o_{p}(1)\n$$\n\nProof. Let $S_{n}=\\left\\{\\boldsymbol{x}:\\left\\|H_{n}(\\boldsymbol{x})\\right\\| \\leqslant B_{n}\\right\\}$ with $B_{n}$ as in (A1)(iii). Applying Lemma 6 (i), we obtain\n\n$$\n\\left\\|\\left(\\mathbb{P}_{n}-P\\right) H_{n}\\right\\| \\leqslant\\left\\|\\left(\\mathbb{P}_{n}-P\\right) H_{n} \\mathbb{1}_{S_{n}}\\right\\|+o_{p}(1)\n$$\n\nby (A1)(iii). Defining $M_{n}^{2}=P H_{n}^{2} \\mathbb{1}_{S_{n}}$, the Bernstein inequality for random matrices (Wainwright, 2019, Theorem 6.17) yields\n\n$$\n\\left\\|\\left(\\mathbb{P}_{n}-P\\right) H_{n} \\mathbb{1}_{S_{n}}\\right\\|=O_{p}\\left(\\sqrt{\\frac{M_{n}^{2} \\ln p_{n}}{n}}+\\frac{B_{n} \\ln p_{n}}{n}\\right)=o_{p}(1)\n$$\n\nLemma 8. Under assumption (A4), it holds\n\n$$\n\\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) H_{n, j, k}\\right|=o_{p}\\left(1 / \\nu_{n}\\right)\n$$\n\nProof. Define\n\n$$\nM_{n}^{2}=\\max _{1 \\leqslant j, k \\leqslant p_{n}} P H_{n, j, k}^{2}, \\quad S_{n}=\\left\\{\\boldsymbol{x}: \\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|H_{n}(\\boldsymbol{x})_{n, j, k}\\right| \\leqslant \\tilde{B}_{n}\\right\\}\n$$\n\nLemma 6 (ii) and (A4) give\n\n$$\n\\begin{aligned}\n\\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) H_{n, j, k}\\right| & \\leqslant \\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) H_{n, j, k} \\mathbb{1}_{S_{n}}\\right|+o_{p}\\left(\\sqrt{M_{n}^{2} / n}\\right) \\\\\n& =\\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) H_{n, j, k} \\mathbb{1}_{S_{n}}\\right|+o_{p}\\left(1 / \\nu_{n}\\right)\n\\end{aligned}\n$$\n\nwith probability tending to 1 . The union bound and Bernstein's inequality give\n\n$$\n\\mathbb{P}\\left(\\max _{1 \\leqslant j, k \\leqslant p_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) H_{n, j, k} \\mathbb{1}_{S_{n}}\\right|>\\varepsilon\\right) \\leqslant 2 p_{n}^{2} \\exp \\left(-\\frac{n \\varepsilon^{2}}{2 M_{n}^{2}+\\tilde{B}_{n} \\varepsilon}\\right)\n$$\n\nThe claim follows upon choosing\n\n$$\n\\varepsilon=C \\sqrt{\\frac{M_{n}^{2} \\ln p_{n}}{n}}+C \\frac{\\tilde{B}_{n} \\ln p_{n}}{n}\n$$\n\nwith some large enough constant $C$ and the assumptions on the growth of $M_{n}$ and $\\tilde{B}_{n}$ from (A4).\n\nLemma 9. For some $c_{n} \\in(0, \\infty), d_{n}, K_{n} \\in \\mathbb{N}$, let\n\n$$\n\\mathcal{F}_{n}=\\left\\{f_{\\boldsymbol{u}, k}: \\boldsymbol{u} \\in \\mathbb{R}^{d_{n}},\\|\\boldsymbol{u}\\| \\leqslant c_{n}, f_{\\mathbf{0}, k} \\equiv 0, k=1, \\ldots, K_{n}\\right\\}\n$$\n\nbe classes of functions such that\n\n$$\n\\begin{aligned}\n& \\max _{1 \\leqslant k \\leqslant K_{n}} \\sup _{\\|\\boldsymbol{u}\\|,\\left\\|\\boldsymbol{u}^{\\prime}\\right\\| \\leqslant c_{n}} \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\mathbb{E}\\left[\\left|f_{\\boldsymbol{u}, k}\\left(\\boldsymbol{X}_{i}\\right)-f_{\\boldsymbol{u}^{\\prime}, k}\\left(\\boldsymbol{X}_{i}\\right)\\right|^{2}\\right]}{\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\|^{2}} \\leqslant M_{n}^{2} \\\\\n& \\sum_{i=1}^{n} \\mathbb{P}\\left(\\max _{1 \\leqslant k \\leqslant K_{n}} \\sup _{\\|\\boldsymbol{u}\\|,\\left\\|\\boldsymbol{u}^{\\prime}\\right\\| \\leqslant c_{n}} \\frac{\\left|f_{\\boldsymbol{u}, k}\\left(\\boldsymbol{X}_{i}\\right)-f_{\\boldsymbol{u}^{\\prime}, k}\\left(\\boldsymbol{X}_{i}\\right)\\right|}{\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\|}>D_{n}\\right)=o(1)\n\\end{aligned}\n$$\n\nThen\n\n$$\n\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f\\right|=O_{p}\\left(\\sqrt{\\frac{M_{n}^{2} c_{n}^{2}\\left(d_{n}+\\ln K_{n}\\right)}{n}}+\\frac{D_{n} c_{n}\\left(d_{n}+\\ln K_{n}\\right)}{n}\\right)\n$$\n\nProof. We proceed in three steps.\nStep 1: Truncation We start with a truncation argument. Let\n\n$$\nF_{n}(\\boldsymbol{x})=\\max _{1 \\leqslant k \\leqslant K_{n}} \\sup _{\\|\\boldsymbol{u}\\|,\\left\\|\\boldsymbol{u}^{\\prime}\\right\\| \\leqslant c_{n}} \\frac{\\left|f_{\\boldsymbol{u}, k}(\\boldsymbol{x})-f_{\\boldsymbol{u}^{\\prime}, k}(\\boldsymbol{x})\\right|}{\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{\\prime}\\right\\|}\n$$\n\nSince $f_{k, \\boldsymbol{0}} \\equiv 0$ by assumption, $F_{n}$ is an envelope for the functions in $c_{n}^{-1} \\mathcal{F}_{n}$ :\n\n$$\n\\sup _{f \\in \\mathcal{F}_{n}} c_{n}^{-1}|f| \\leqslant \\max _{1 \\leqslant k \\leqslant K_{n}} \\sup _{\\|\\boldsymbol{u}\\| \\leqslant c_{n}} c_{n}^{-1}\\|\\boldsymbol{u}\\| \\frac{\\left|f_{\\boldsymbol{u}, k}(\\boldsymbol{x})-0\\right|}{\\|\\boldsymbol{u}-\\mathbf{0}\\|} \\leqslant F_{n}(\\boldsymbol{x})\n$$\n\nNow Lemma 6 (ii) and our assumptions give\n\n$$\nc_{n}^{-1} \\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f\\right| \\leqslant c_{n}^{-1} \\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{F_{n} \\leqslant D_{n}}\\right|+o_{p}\\left(M_{n} / \\sqrt{n}\\right)\n$$\n\nso\n\n$$\n\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f\\right| \\leqslant \\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{F_{n} \\leqslant D_{n}}\\right|+o_{p}\\left(\\sqrt{M_{n}^{2} c_{n}^{2} / n}\\right)\n$$\n\nStep 2: Bounding covering numbers Let $\\mathcal{A}$ be some set equipped with a norm $\\|\\cdot\\|$. A collection of $N$ balls $B\\left(a_{k}, \\epsilon\\right)=\\left\\{a \\in \\mathcal{A}:\\left\\|a-a_{k}\\right\\| \\leqslant \\epsilon\\right\\}$ is called an $\\epsilon$-covering of $\\mathcal{A}$ with respect to the norm $\\|\\cdot\\|$ if $\\mathcal{A} \\subset \\bigcup_{k=1}^{N} B\\left(a_{k}, \\epsilon\\right)$. The minimal number of balls of radius $\\epsilon$ needed to cover $\\mathcal{A}$ is the covering number $N(\\epsilon, \\mathcal{A},\\|\\cdot\\|)$.\n\nFix $k$ and consider $\\mathcal{F}_{n}^{(k)}=\\left\\{f_{\\boldsymbol{u}, k} \\mathbb{1}_{S_{n}}: \\boldsymbol{u} \\in \\mathbb{R}^{p_{n}},\\|\\boldsymbol{u}\\| \\leqslant c_{n}\\right\\}$. Recall that by our definition of $P f$, the $L_{2}(P)$-norm is defined as $\\|f-g\\|_{L_{2}(P)}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[\\left|f\\left(X_{i}\\right)-g\\left(X_{i}\\right)\\right|^{2}\\right]$. We will show that\n\n$$\n\\ln N\\left(\\varepsilon, \\mathcal{F}_{n}^{(k)}, L_{2}(P)\\right) \\lesssim d_{n} \\ln \\left(3 M_{n} c_{n} / \\varepsilon\\right), \\quad \\ln N\\left(\\varepsilon, \\mathcal{F}_{n}^{(k)},\\|\\cdot\\|_{\\infty}\\right) \\lesssim d_{n} \\ln \\left(3 D_{n} c_{n} / \\varepsilon\\right)\n$$\n\nwhere $\\lesssim$ means \"bounded up to a universal constant\". Let $\\boldsymbol{u}_{1}, \\ldots, \\boldsymbol{u}_{N}$ be the centers of an $\\eta$ covering of $\\left\\{\\boldsymbol{u} \\in \\mathbb{R}^{d_{n}}:\\|\\boldsymbol{u}\\| \\leqslant c_{n}\\right\\}$, which we can find with $N=\\left(3 c_{n} / \\eta\\right)^{d_{n}}$. Then, by the definitions of $M_{n}$ and $D_{n}$, the functions $f_{\\boldsymbol{u}_{1}, k}, \\ldots, f_{\\boldsymbol{u}_{N}, k}$ are centers of a $M_{n} \\eta$-covering of $\\mathcal{F}_{n}^{(k)}$ in $L_{2}(P)$, and a $D_{n} \\eta$ covering for $\\|\\cdot\\|_{\\infty}$, respectively. Choosing $\\eta=\\varepsilon / M_{n}$ and $\\eta=\\varepsilon / D_{n}$, respectively, gives (30). Now we can take a union over the coverings of all $\\mathcal{F}_{n}^{(k)}$ to find a covering of $\\mathcal{F}_{n}$, which gives\n\n$$\n\\begin{aligned}\n\\ln N\\left(\\varepsilon, \\mathcal{F}_{n}, L_{2}(P)\\right) & \\lesssim \\ln K_{n}+d_{n} \\ln \\left(3 M_{n} c_{n} / \\varepsilon\\right) \\\\\n\\ln N\\left(\\varepsilon, \\mathcal{F}_{n},\\|\\cdot\\|_{\\infty}\\right) & \\lesssim \\ln K_{n}+d_{n} \\ln \\left(3 D_{n} c_{n} / \\varepsilon\\right)\n\\end{aligned}\n$$\n\nStep 3: Bounding the truncated process Denote $S_{n}=\\left\\{\\boldsymbol{x}: F_{n}(\\boldsymbol{x}) \\leqslant D_{n}\\right\\}$. Theorem 2.14.21 of Van der Vaart and Wellner (2023) gives\n$\\mathbb{E}\\left[\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{S_{n}}\\right|\\right] \\leqslant \\frac{\\int_{0}^{M_{n} c_{n}} \\sqrt{1+\\ln N\\left(\\epsilon, \\mathcal{F}_{n}, L_{2}(P)\\right)} d \\epsilon}{\\sqrt{n}}+\\frac{\\int_{0}^{D_{n} c_{n}}\\left[1+\\ln N\\left(\\epsilon, \\mathcal{F}_{n},\\|\\cdot\\|_{\\infty}\\right)\\right] d \\epsilon}{n}$.\nSubstituting the covering number bounds (31) and the changes of variables $t=\\epsilon / M_{n} c_{n}$ and $t=\\epsilon / D_{n} c_{n}$, respectively, gives\n\n$$\n\\mathbb{E}\\left[\\sup _{f \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f \\mathbb{1}_{S_{n}}\\right|\\right] \\leqslant \\frac{M_{n} c_{n} \\sqrt{d_{n}+\\ln K_{n}}}{\\sqrt{n}}+\\frac{D_{n} c_{n}\\left(d_{n}+\\ln K_{n}\\right)}{n}\n$$\n\nNow the result follows from Markov's inequality.\nLemma 10. Under assumption (A2), it holds for all $C<\\infty$,\n\n$$\n\\sup _{\\|\\boldsymbol{u}\\| \\leqslant r_{n} C}\\left\\|\\left(\\mathbb{P}_{n}-P\\right) A_{n}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\|=o_{p}(1 / \\sqrt{n})\n$$\n\nProof. We show that for each row $\\boldsymbol{a}_{n}$ from $A_{n} \\in \\mathbb{R}^{q \\times p_{n}}$, it holds\n\n$$\n\\sup _{\\|\\boldsymbol{u}\\| \\leqslant r_{n} C}\\left|\\left(\\mathbb{P}_{n}-P\\right) \\boldsymbol{a}_{n}^{\\top}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right|=o_{p}(1 / \\sqrt{n})\n$$\n\nSince $A_{n}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]$ is a finite dimensional vector, this implies the claim. Let $\\boldsymbol{a}_{n}$ be some row of $A_{n}$. We have\n\n$$\n\\sup _{\\|\\boldsymbol{u}\\| \\leqslant r_{n} C}\\left|\\left(\\mathbb{P}_{n}-P\\right) \\boldsymbol{a}_{n}^{\\top}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right|=\\sup _{f_{\\boldsymbol{u}} \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f_{\\boldsymbol{u}}\\right|\n$$\n\nwith $\\mathcal{F}_{n}=\\left\\{\\boldsymbol{a}_{n}^{\\top}\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]:\\|\\boldsymbol{u}\\| \\leqslant r_{n} C\\right\\}$. Now apply Lemma 9 with $d_{n}=p_{n}, c_{n}=r_{n} C$, $K_{n}=1$. This gives\n\n$$\n\\sup _{f_{\\boldsymbol{u}} \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f_{\\boldsymbol{u}}\\right|=o_{p}\\left(\\frac{1}{\\sqrt{n}} M_{n} r_{n} \\sqrt{p_{n}}+\\frac{1}{\\sqrt{n}} \\frac{D_{n} r_{n} p_{n}}{\\sqrt{n}}\\right)=o_{p}(1 / \\sqrt{n})\n$$\n\nsince $M_{n}=o\\left(1 /\\left(r_{n} \\sqrt{p_{n}}\\right)\\right)$ and $D_{n}=o\\left(\\sqrt{n} /\\left(r_{n} p_{n}\\right)\\right)$ by (A2).\nLemma 11. Let $\\tilde{\\boldsymbol{\\theta}}=\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}, \\mathbf{0}\\right)$ be a solution to $\\Phi_{n}\\left(\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}, \\mathbf{0}\\right)\\right)_{(1)} \\in \\partial_{\\boldsymbol{\\theta}_{(1)}} p_{\\boldsymbol{\\lambda}_{n}}\\left(\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}, \\mathbf{0}\\right)\\right)$. Under assumptions (A7) and (A8), it holds\n\n$$\n\\begin{aligned}\n& \\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} \\mathbb{P}_{n} \\phi(\\tilde{\\boldsymbol{\\theta}})_{(2)}\\right\\|_{\\infty} \\\\\n\\leqslant & \\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} J(\\tilde{\\boldsymbol{\\theta}})_{(2,1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)} p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\right\\|_{\\infty} \\\\\n& +\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1}\\left(\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}-J(\\tilde{\\boldsymbol{\\theta}})_{(2,1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)} \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right)\\right\\|_{\\infty}+o_{p}(1)\n\\end{aligned}\n$$\n\nwith some $\\tilde{\\boldsymbol{\\theta}}$ on the line segment from $\\boldsymbol{\\theta}^{*}$ to $\\tilde{\\boldsymbol{\\theta}}$.\n\nProof. We have\n\n$$\n\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} \\mathbb{P}_{n} \\phi(\\tilde{\\boldsymbol{\\theta}})_{(2)}=\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}+\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} \\mathbb{P}_{n}\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(2)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}\\right]\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\mathbb{P}_{n}\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(2)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}\\right] & =P\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(2)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}\\right]+\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(2)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}\\right] \\\\\n& =J(\\tilde{\\boldsymbol{\\theta}})_{(2,1)}\\left(\\tilde{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}\\right)+\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(2)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}\\right]\n\\end{aligned}\n$$\n\nwith some $\\tilde{\\boldsymbol{\\theta}}$ between $\\tilde{\\boldsymbol{\\theta}}$ and $\\boldsymbol{\\theta}^{*}$ and $J(\\boldsymbol{\\theta})_{(2,1)}$ as defined in Section 3.3.1. Similar to the proof of Theorem 3, one obtains\n\n$$\n\\tilde{\\boldsymbol{\\theta}}_{(1)}-\\boldsymbol{\\theta}_{(1)}^{*}=J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\left[-\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}-\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(1)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right]+p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\right]\n$$\n\nwith $J(\\boldsymbol{\\theta})_{(1)}$ as defined in Section 3.3.1. We may take the same $\\tilde{\\boldsymbol{\\theta}}$ as above here since we are using the same expansion for the term $P \\phi(\\tilde{\\boldsymbol{\\theta}})-P \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)$ in both arguments. The three last displays together yield\n\n$$\n\\begin{aligned}\n& \\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} \\mathbb{P}_{n} \\phi(\\tilde{\\boldsymbol{\\theta}})_{(2)} \\\\\n= & \\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} \\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)} \\\\\n& +\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1}\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(2)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(2)}\\right] \\\\\n& +\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} J(\\tilde{\\boldsymbol{\\theta}})_{(2,1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\left[-\\mathbb{P}_{n} \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}+p_{\\boldsymbol{\\lambda}_{n}}^{\\prime}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\right] \\\\\n& +\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} J(\\tilde{\\boldsymbol{\\theta}})_{(2,1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\left[-\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})_{(1)}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{(1)}\\right]\\right]\n\\end{aligned}\n$$\n\nThe claim follows if we show that the the second and fourth terms are negligible (with respect to the $\\|\\cdot\\|_{\\infty}$-norm). With $\\eta_{n}$ as defined in (A7), it holds\n\n$$\n\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1}\\right\\|_{\\infty} \\leqslant \\frac{1}{\\min _{k=s_{n}+1, \\ldots, p_{n}} \\lambda_{n, k}} \\leqslant \\frac{1}{\\eta_{n}}\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\left\\|\\operatorname{diag}\\left(\\boldsymbol{\\lambda}_{n(2)}\\right)^{-1} J(\\tilde{\\boldsymbol{\\theta}})_{(2,1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\right\\|_{\\infty} & \\leqslant \\frac{1}{\\eta_{n}} \\max _{k=s_{n}+1, \\ldots, p_{n}} \\frac{1}{J_{n, k}}\\left\\|\\left(J(\\tilde{\\boldsymbol{\\theta}})_{k,(1)} J^{-1}(\\tilde{\\boldsymbol{\\theta}})_{(1)}\\right)^{\\top}\\right\\|_{1} \\\\\n& \\leqslant \\frac{1}{\\eta_{n}}\n\\end{aligned}\n$$\n\nFurther, Lemma 12 gives\n\n$$\n\\left.\\left\\|_{\\infty}\\right|_{\\infty}\\right|_{\\infty}-\\left.P\\right)\\left[\\phi(\\tilde{\\boldsymbol{\\theta}})-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right|_{\\infty}=o_{p}\\left(\\eta_{n}\\right)\n$$\n\nNow the claim follows from combining the previous displays.\nLemma 12. Under (A8), it holds\n\n$$\n\\sup _{\\substack{\\boldsymbol{u} \\in \\mathbb{R}^{p n}, \\boldsymbol{u}_{(2)}=\\mathbf{0} \\\\\\|\\boldsymbol{u}\\| \\leqslant \\ell_{n} C}}\\left\\|\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\|_{\\infty}=o_{p}\\left(\\eta_{n}\\right)\n$$\n\nfor every $C<\\infty$.\n\nProof. We have\n\n$$\n\\sup _{\\substack{\\boldsymbol{u} \\in \\mathbb{R}^{p_{n}}, \\boldsymbol{u}_{(2)}=\\mathbf{0} \\\\\\|\\boldsymbol{u}\\| \\leqslant \\tilde{r}_{n} C}}\\left\\|\\left(\\mathbb{P}_{n}-P\\right)\\left[\\phi\\left(\\boldsymbol{\\theta}^{*}+\\boldsymbol{u}\\right)-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)\\right]\\right\\|_{\\infty}=\\sup _{f_{\\boldsymbol{u}, k} \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f_{\\boldsymbol{u}, k}\\right|\n$$\n\nwith\n\n$$\n\\mathcal{F}_{n}=\\left\\{\\phi\\left(\\boldsymbol{\\theta}^{*}+\\left(\\boldsymbol{u}_{(1)}, \\mathbf{0}\\right)\\right)_{k}-\\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{k}: \\boldsymbol{u}_{(1)} \\in \\mathbb{R}^{s_{n}}, \\boldsymbol{u}_{(2)}=\\mathbf{0},\\|\\boldsymbol{u}\\| \\leqslant \\tilde{r}_{n} C, k=1, \\ldots p_{n}\\right\\}\n$$\n\nDefine\n\n$$\n\\tilde{M}_{n}^{2}=\\max _{1 \\leqslant k \\leqslant p_{n}, \\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{\\prime} \\in \\Theta^{\\prime}} \\frac{P\\left|\\phi_{i}(\\boldsymbol{\\theta})_{k}-\\phi_{i}\\left(\\boldsymbol{\\theta}^{\\prime}\\right)_{k}\\right|^{2}}{\\left\\|\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^{\\prime}\\right\\|^{2}}\n$$\n\nBy assumption (A8), we can apply Lemma 9 with $d_{n}=s_{n}, K_{n}=p_{n}, c_{n}=\\tilde{r}_{n} C, M_{n}=\\tilde{M}_{n}$ and $D_{n}=\\tilde{D}_{n}$. This gives\n\n$$\n\\mathbb{E}\\left(\\sup _{f_{\\boldsymbol{u}, k} \\in \\mathcal{F}_{n}}\\left|\\left(\\mathbb{P}_{n}-P\\right) f_{\\boldsymbol{u}, k}\\right|\\right)=O\\left(\\sqrt{\\frac{\\tilde{M}_{n}^{2} \\tilde{r}_{n}^{2} C^{2}\\left(s_{n}+\\ln p_{n}\\right)}{n}}+\\frac{\\tilde{D}_{n} \\tilde{r}_{n} C\\left(s_{n}+\\ln p_{n}\\right)}{n}\\right)=o\\left(\\eta_{n}\\right)\n$$\n\nwhere we used the growth bounds from (A8) in the last step. Now the claim follows from Markov's inequality.\n\nLemma 13. Under assumption (A5) and $p_{n} \\rightarrow \\infty$, it holds\n\n$$\n\\mathbb{P}\\left(\\left\\|\\frac{1}{n} \\sum_{i=1}^{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|_{\\infty} \\leqslant 2 \\sigma_{n} \\sqrt{\\frac{\\ln p_{n}}{n}}\\right) \\rightarrow 1\n$$\n\nProof. Let\n\n$$\n\\eta_{n}=2 \\sigma_{n} \\sqrt{\\frac{\\ln p_{n}}{n}}, \\quad B_{n}=\\sqrt{\\frac{n}{4 \\ln p_{n}}}\n$$\n\nUsing Lemma 6 (ii), (A5), and $\\sqrt{\\max _{k} P \\phi\\left(\\boldsymbol{\\theta}^{*}\\right)_{k}^{2}} \\leqslant \\sqrt{n} \\eta_{n}$, we get\n\n$$\n\\left\\|\\mathbb{P}_{n} \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|_{\\infty} \\leqslant\\left\\|\\left(\\mathbb{P}_{n}-P\\right) \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right) \\mathbb{1}_{\\left[\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|_{\\infty} \\leqslant B_{n}}\\right\\|_{\\infty}+o_{p}\\left(\\eta_{n}\\right)\n$$\n\nFurther, the union bound and Bernstein's inequality give\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(\\left\\|\\left(\\mathbb{P}_{n}-P\\right) \\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right) \\mathbb{1}_{\\left\\|\\phi_{i}\\left(\\boldsymbol{\\theta}^{*}\\right)\\right\\|_{\\infty} \\leqslant B_{n}}\\right\\|_{\\infty}>\\eta_{n}\\right) & \\leqslant 2 p_{n} \\max _{1 \\leqslant k \\leqslant p_{n}} \\exp \\left(-\\frac{\\frac{1}{2} \\eta_{n}^{2}}{\\frac{1}{n} \\sigma_{n}^{2}+\\frac{1}{3} \\eta_{n} B_{n} / n}\\right) \\\\\n& \\leqslant 2 \\exp \\left(\\ln p_{n}-\\frac{\\eta_{n}^{2} n}{2 \\sigma_{n}^{2}+\\eta_{n} B_{n}}\\right) \\\\\n& \\leqslant 2 \\exp \\left(\\ln p_{n}-\\frac{\\eta_{n}^{2} n}{3 \\sigma_{n}^{2}}\\right) \\\\\n& =2 \\exp \\left(\\ln p_{n}-\\frac{4}{3} \\ln p_{n}\\right) \\\\\n& =o(1)\n\\end{aligned}\n$$\n\nLemma 14. Let $\\boldsymbol{s}, \\boldsymbol{s}^{\\prime} \\in\\{-1,1\\}^{k}, k \\geqslant 2$, be such that $\\boldsymbol{s} \\neq \\boldsymbol{s}^{\\prime}$ and $\\boldsymbol{s} \\neq-\\boldsymbol{s}^{\\prime}$. Then,\n\n$$\n\\sum_{i=1}^{k} \\sum_{j=1}^{k}\\left(s_{i} s_{j}-s_{i}^{\\prime} s_{j}^{\\prime}\\right)^{2} \\geqslant 8(k-1)\n$$\n\nProof. Denoting the Frobenius norm by $\\|\\cdot\\|_{F}$, we have\n\n$$\n\\sum_{i, j=1}^{k}\\left(s_{i} s_{j}-s_{i}^{\\prime} s_{j}^{\\prime}\\right)^{2}=\\left\\|\\boldsymbol{s} \\boldsymbol{s}^{\\top}-\\boldsymbol{s}^{\\prime} \\boldsymbol{s}^{\\prime \\top}\\right\\|_{F}^{2}\n$$\n\nSince $\\boldsymbol{s}, \\boldsymbol{s}^{\\prime} \\in\\{-1,1\\}^{k}$, we have $\\left\\|\\boldsymbol{s} \\boldsymbol{s}^{\\top}\\right\\|_{F}^{2}=\\left\\|\\boldsymbol{s}^{\\prime} \\boldsymbol{s}^{\\prime \\top}\\right\\|_{F}^{2}=k^{2}$. Therefore,\n\n$$\n\\left\\|\\boldsymbol{s} \\boldsymbol{s}^{\\top}-\\boldsymbol{s}^{\\prime} \\boldsymbol{s}^{\\prime \\top}\\right\\|_{F}^{2}=\\left\\langle\\boldsymbol{s} \\boldsymbol{s}^{\\top}-\\boldsymbol{s}^{\\prime} \\boldsymbol{s}^{\\prime \\top}, \\boldsymbol{s} \\boldsymbol{s}^{\\top}-\\boldsymbol{s}^{\\prime} \\boldsymbol{s}^{\\prime \\top}\\right\\rangle=2 k^{2}-2\\left\\langle\\boldsymbol{s} \\boldsymbol{s}^{\\top}, \\boldsymbol{s}^{\\prime} \\boldsymbol{s}^{\\prime \\top}\\right\\rangle=2 k^{2}-2\\left(\\boldsymbol{s}^{\\top} \\boldsymbol{s}^{\\prime}\\right)^{2}\n$$\n\nsince\n\n$$\n\\left\\langle\\boldsymbol{s} \\boldsymbol{s}^{\\top}, \\boldsymbol{s}^{\\prime} \\boldsymbol{s}^{\\prime \\top}\\right\\rangle_{F}=\\sum_{i=1}^{k} \\sum_{j=1}^{k}\\left(s_{i} s_{j}\\right)\\left(s_{i}^{\\prime} s_{j}^{\\prime}\\right)=\\left(\\sum_{i=1}^{k} s_{i} s_{i}^{\\prime}\\right)\\left(\\sum_{j=1}^{k} s_{j} s_{j}^{\\prime}\\right)=\\left(\\boldsymbol{s}^{\\top} \\boldsymbol{s}^{\\prime}\\right)^{2}\n$$\n\nIt holds $\\boldsymbol{s}^{\\top} \\boldsymbol{s}^{\\prime}=k-\\left\\|\\boldsymbol{s}-\\boldsymbol{s}^{\\prime}\\right\\|_{1}$, so that\n\n$$\n\\left(\\boldsymbol{s}^{\\top} \\boldsymbol{s}^{\\prime}\\right)^{2}=\\left(k-\\left\\|\\boldsymbol{s}-\\boldsymbol{s}^{\\prime}\\right\\|_{1}\\right)^{2} \\leqslant(k-2)^{2}\n$$\n\nsince $\\boldsymbol{s} \\neq \\boldsymbol{s}^{\\prime}$ and $\\boldsymbol{s} \\neq-\\boldsymbol{s}^{\\prime}$. Substituting this, we obtain\n\n$$\n\\left\\|\\boldsymbol{s} \\boldsymbol{s}^{\\top}-\\boldsymbol{s}^{\\prime} \\boldsymbol{s}^{\\prime \\top}\\right\\|_{F}^{2} \\geqslant 2 k^{2}-2(k-2)^{2}=8(k-1)\n$$\n\nas required.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2411.17395v2",
  "authors": [
    "Jana Gauss",
    "Thomas Nagler"
  ],
  "categories": [
    "math.ST",
    "stat.ME",
    "stat.TH"
  ],
  "abstract": "We consider high-dimensional estimation problems where the number of\nparameters diverges with the sample size. General conditions are established\nfor consistency, uniqueness, and asymptotic normality in both unpenalized and\npenalized estimation settings. The conditions are weak and accommodate a broad\nclass of estimation problems, including ones with non-convex and group\nstructured penalties. The wide applicability of the results is illustrated\nthrough diverse examples, including generalized linear models, multi-sample\ninference, and stepwise estimation procedures.",
  "updated": "2025-04-07T15:50:55Z",
  "published": "2024-11-26T12:55:51Z"
}