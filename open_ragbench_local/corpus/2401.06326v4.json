{
  "title": "Optimal linear prediction with functional observations: Why you can use\n  a simple post-dimension reduction estimator",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nWe study the optimal linear prediction of a random function that takes values in an infinite dimensional Hilbert space. We begin by characterizing the mean square prediction error (MSPE) associated with a linear predictor and discussing the minimal achievable MSPE. This analysis reveals that, in general, there are multiple non-unique linear predictors that minimize the MSPE, and even if a unique solution exists, consistently estimating it from finite samples is generally impossible. Nevertheless, we can define asymptotically optimal linear operators whose empirical MSPEs approach the minimal achievable level as the sample size increases. We show that, interestingly, standard post-dimension reduction estimators, which have been widely used in the literature, attain such asymptotic optimality under minimal conditions.\n\n\nMSC2020 subject classifications: Primary 60G25; secondary 62J99.\nKeywords and phrases: linear prediction, functional data, functional linear models, regularization.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "## Contents\n\n1 Introduction ..... 2\n2 Optimal linear prediction in Hilbert space ..... 3\n2.1 Preliminaries ..... 3\n2.2 Linear prediction in $\\mathcal{H}$ ..... 4\n3 Estimation ..... 6\n4 Discussions and extensions ..... 9\n4.1 A more general result ..... 9\n4.2 Misspecified functional linear models and OLPO ..... 9\n4.3 Requirement of sufficient dimension reduction ..... 10\n4.4 Dimension reduction of the target variable ..... 10\n5 Simulation ..... 11\n6 Concluding remarks ..... 13\nA Proofs ..... 15\nA. 1 Useful lemmas ..... 15\nA. 2 Proofs of the theoretical results ..... 15\n\n[^0]\n[^0]:    *Won-Ki Seo is the corresponding author.\n\nB Additional simulation results for alternative estimators . . . . . . . . . 20 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\ngeneral. As will be detailed in Example 1, this is because, it is not possible to estimate $a_{j}$ for $j>T$ from $T$ observations unless (i) a simplifying condition on $A$, such as $a_{j}=a$ for all $j \\geq m$ for some finite $m$, holds and (ii) researchers are aware of this condition and use it appropriately. Even in the simple case where $Y_{t}=\\bar{A}_{o} X_{t}+\\varepsilon_{t}$ with $\\bar{A}_{o}=a I$ ( $I$ denotes the identity map) for $a \\in \\mathbb{R}$, consistent estimation of $\\bar{A}_{o}$ is impossible for the same reason if we do not know such a simple structure of $\\bar{A}_{o}$ and hence allow a more general case given in (1.2) (note that, $\\bar{A}_{o}=a I$ is a special case of $A_{o}$ in (1.2) with $a_{j}=a$ ).\n\nDoes this mean that it is impossible to statistically solve the optimal linear prediction problem in this general setting? The answer is no. We will demonstrate that, under mild conditions, there exists the minimum MSPE achievable by a linear predictor and it is feasible to construct a possibly inconsistent estimator $\\widehat{A}$ such that the empirical MSPE, computed as $T^{-1} \\sum_{t=1}^{T}\\left\\|Y_{t}-\\widehat{A} X_{t}\\right\\|^{2}$, converges to the minimum MSPE. Particularly, we show that a standard post dimension-reduction estimator, obtained by (i) reducing the dimensionality of the predictive variable $X_{t}$ using the principal directions of its sample covariance and then (ii) applying the least squares method to estimate the linear relationship between the resulting lower dimensional predictive variable and $Y_{t}$, is effective for linear prediction. This post dimension-reduction estimator has been widely used due to its simplicity. Its statistical properties have been studied under technical assumptions, which are challenging to verify, such as those concerning the eigenstructure of the covariance of $X_{t}$; the reader is referred to, e.g., [13] and [26], where the assumptions of [12] are adopted for function-onfunction regression models. We show that, without such assumptions, a naive use of this simple post dimension-reduction estimator can be justified as a way to obtain a solution which asymptotically minimizes the MSPE. We also extend this finding to show that similar estimators, which involve further dimension reduction of $Y_{t}$, also possess this desirable property under mild conditions.\n\nThe paper proceeds as follows: Section 2 characterizes the minimal achievable MSPE by a linear predictor, followed by a discussion on the estimation of an asymptotically optimal predictor in Section 3. Further discussions and extensions are given in Section 4. Section 5 provides simulation evidence of our theoretical findings, and Section 6 contains concluding remarks. The appendix includes mathematical proofs of the theoretical results and some additional simulation results.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 2. Optimal linear prediction in Hilbert space\n### 2.1. Preliminaries\n\nFor the subsequent discussion, we introduce notation. Let $\\mathcal{H}$ be a separable Hilbert space with inner product $\\langle\\cdot, \\cdot\\rangle$ and norm $\\|\\cdot\\|$. For $V \\subset \\mathcal{H}$, let $V^{\\perp}$ be the orthogonal complement to $V$. We let $\\mathcal{L}_{\\infty}$ be the set of continuous linear operators, and let $\\|\\mathcal{T}\\|_{\\infty}, \\mathcal{T}^{*}, \\operatorname{ran} \\mathcal{T}$, and $\\operatorname{ker} \\mathcal{T}$ denote the operator norm, adjoint, range, and kernel of $\\mathcal{T}$, respectively. $\\mathcal{T}$ is self-adjoint if $\\mathcal{T}=\\mathcal{T}^{*} . \\mathcal{T}$ is\n\ncalled nonnegative if $\\langle\\mathcal{T} x, x\\rangle \\geq 0$ for any $x \\in \\mathcal{H}$, and positive if also $\\langle\\mathcal{T} x, x\\rangle \\neq 0$ for any $x \\in \\mathcal{H} \\backslash\\{0\\}$. For $x, y \\in \\mathcal{H}$, we let $x \\otimes y$ be the operator given by $z \\mapsto\\langle x, z\\rangle y$ for $z \\in \\mathcal{H} . \\mathcal{T} \\in \\mathcal{L}_{\\infty}$ is compact if $\\mathcal{T}=\\sum_{j \\geq 1} a_{j} v_{j} \\otimes w_{j}$ for some orthonormal bases $\\left\\{v_{j}\\right\\}_{j \\geq 1}$ and $\\left\\{w_{j}\\right\\}_{j \\geq 1}$ and a sequence of nonnegative numbers $\\left\\{a_{j}\\right\\}_{j \\geq 1}$ tending to zero; if $\\mathcal{T}$ is also self-adjoint and nonnegative (see [7], p. 35), we may assume that $v_{j}=w_{j}$. For any compact $\\mathcal{T} \\in \\mathcal{L}_{\\infty}$ and $p \\in \\mathbb{N}$, let $\\|\\mathcal{T}\\|_{\\mathcal{S}_{p}}$ be defined by $\\|\\mathcal{T}\\|_{\\mathcal{S}_{p}}^{\\nu}=\\sum_{j=1}^{\\infty} a_{j}^{p}$ and let $\\mathcal{S}_{p}$ be the set of compact operators $\\mathcal{T}$ with $\\|\\mathcal{T}\\|_{\\mathcal{S}_{p}}<\\infty ; \\mathcal{S}_{p}$ is called the Schatten $p$-class. $\\mathcal{S}_{1}$ (resp. $\\mathcal{S}_{2}$ ) is also referred to the trace (resp. Hilbert-Schmidt) class. It is known that the following hold: $\\|\\mathcal{T}\\|_{\\infty} \\leq\\|\\mathcal{T}\\|_{\\mathcal{S}_{2}} \\leq\\|\\mathcal{T}\\|_{\\mathcal{S}_{1}}$ and $\\|\\mathcal{T}\\|_{\\mathcal{S}_{2}}^{2}=\\sum_{j=1}^{\\infty}\\left\\|\\mathcal{T} w_{j}\\right\\|^{2}$ for any orthonormal basis $\\left\\{w_{j}\\right\\}_{j \\geq 1}$. For any $\\mathcal{H}$-valued mean-zero random elements $Z$ and $\\widetilde{Z}$ with $\\mathbb{E}\\|Z\\|^{2}<\\infty$ and $\\mathbb{E}\\|\\widetilde{Z}\\|^{2}<\\infty$, their cross-covariance $C_{Z \\tilde{Z}}=\\mathbb{E}[Z \\otimes \\tilde{Z}]$ is a Schatten 1-class operator; if $Z=\\tilde{Z}$, it reduces to the covariance of $Z$ and $\\mathbb{E}\\|Z\\|^{2}=\\left\\|C_{Z Z}\\right\\|_{\\mathcal{S}_{1}}$ holds.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 2.2. Linear prediction in $\\mathcal{H}$ \n\nConsider a weakly stationary sequence $\\left\\{Y_{t}, X_{t}\\right\\}_{t \\geq 1}$ with nonzero covariances $C_{Y Y}=\\mathbb{E}\\left[Y_{t} \\otimes Y_{t}\\right]$ and $C_{X X}=\\mathbb{E}\\left[X_{t} \\otimes X_{t}\\right]$, along with the cross-covariance $C_{X Y}=\\mathbb{E}\\left[X_{t} \\otimes Y_{t}\\right]$ (or equivalently $C_{Y X}^{*}$ ). We hereafter write $C_{Y Y}$ and $C_{X X}$ as their spectral representations as follows, if necessary:\n\n$$\nC_{Y Y}=\\sum_{j \\geq 1} \\kappa_{j} u_{j} \\otimes u_{j}, \\quad C_{X X}=\\sum_{j \\geq 1} \\lambda_{j} v_{j} \\otimes v_{j}\n$$\n\nwhere $\\kappa_{1} \\geq \\kappa_{2} \\geq \\ldots \\geq 0, \\lambda_{1} \\geq \\lambda_{2} \\geq \\ldots \\geq 0$, and $\\left\\{u_{j}\\right\\}_{j \\geq 1}$ and $\\left\\{v_{j}\\right\\}_{j \\geq 1}$ are orthonormal sets of $\\mathcal{H}$. Unless otherwise stated, we assume that $C_{Y Y}$ and $C_{X X}$ are not finite rank operators and thus there are infinitely many nonzero eigenvalues in (2.1), which is as usually assumed for covariances of Hilbert-valued random elements in the literature on functional data analysis.\n\nNote first that, for any $B \\in \\mathcal{L}_{\\infty}, \\mathbb{E}\\left\\|Y_{t}-B X_{t}\\right\\|^{2}=\\left\\|\\mathbb{E}\\left[\\left(Y_{t}-B X_{t}\\right) \\otimes\\left(Y_{t}-\\right.\\right.\\right.$ $\\left.\\left.B X_{t}\\right)\\right]\\left\\|_{\\mathcal{S}_{1}}\\right.$ and hence the MSPE associated with $B$ can be written as follows:\n\n$$\n\\mathbb{E}\\left\\|Y_{t}-B X_{t}\\right\\|^{2}=\\left\\|C_{Y Y}-C_{X Y} B^{*}-B C_{X Y}^{*}+B C_{X X} B^{*}\\right\\|_{\\mathcal{S}_{1}}\n$$\n\nNext, we provide the main result of this section, which not only gives us a useful characterization of the MSPE in (2.2), but also provides essential preliminary results for the subsequent discussion.\n\nProposition 2.1. For any $B \\in \\mathcal{L}_{\\infty}$, there exists a unique element $R_{X Y} \\in \\mathcal{L}_{\\infty}$ such that $C_{X Y}=C_{Y Y}^{1 / 2} R_{X Y} C_{X X}^{1 / 2}$ and\n\n$$\n\\mathbb{E}\\left\\|Y_{t}-B X_{t}\\right\\|^{2}=\\left\\|C_{Y Y}-C_{Y Y}^{1 / 2} R_{X Y} R_{X Y}^{*} C_{Y Y}^{1 / 2}\\right\\|_{\\mathcal{S}_{1}}+\\left\\|B C_{X X}^{1 / 2}-C_{Y Y}^{1 / 2} R_{X Y}\\right\\|_{\\mathcal{S}_{2}}^{2}\n$$\n\nProposition 2.1 shows that the MSPE associated with $B \\in \\mathcal{L}_{\\infty}$ is the sum of the Schatten 1- and 2-norms of specific operators dependent on $C_{Y Y}, C_{X X}$,\n\n$R_{X Y}$ and $B$; notably, only the latter term $\\left(\\left\\|B C_{X X}^{1 / 2}-C_{Y Y}^{1 / 2} R_{X Y}\\right\\|_{\\mathcal{S}_{2}}^{2}\\right)$ in (2.3) depends on $B$. Thus, the former term $\\left(\\left\\|C_{Y Y}-C_{Y Y}^{1 / 2} R_{X Y} R_{X Y}^{*} C_{Y Y}^{1 / 2}\\right\\|_{\\mathcal{S}_{1}}\\right)$ represents the minimal achievable MSPE by a linear predictor, while the latter can be understood as a measure of the inadequacy of $B$ as a linear predictor. If $\\left\\{w_{j}\\right\\}_{j \\geq 1}$ is an orthonormal basis of $\\mathcal{H}$, this inadequacy becomes zero if and only if\n\n$$\n\\left\\|\\left(B C_{X X}^{1 / 2}-C_{Y Y}^{1 / 2} R_{X Y}\\right) w_{j}\\right\\|^{2}=0 \\quad \\text { for all } j \\geq 1\n$$\n\nBased on these findings, we obtain the following two characterizations of an OLPO in Corollary 2.1: the first is a direct consequence of (2.4) and the HahnBanach extension theorem (see, e.g., Theorem 1.9.1 of [21]), which, in turn, implies the second due to the fact that $C_{X Y}=C_{Y Y}^{1 / 2} R_{X Y} C_{X X}^{1 / 2}$ as observed in Proposition 2.1.\nCorollary 2.1. $A$ is an OLPO if and only if any of the following equivalent conditions holds: (a) $A C_{X X}^{1 / 2}=C_{Y Y}^{1 / 2} R_{X Y}$ and (b) $A C_{X X}=C_{X Y}$.\n\nCondition (b) follows directly from condition (a) and Proposition 2.1, and it is notably align with the characterization of an OLPO provided by [8]; Remark 2.1 outlines the distinctions between our findings and the existing result in more detail. From Corollary 2.1, we find that the minimum MSPE is attained by $A \\in \\mathcal{L}_{\\infty}$ satisfying $A C_{X X}=C_{X Y}$ (or $A C_{X X}^{1 / 2}=C_{Y Y}^{1 / 2} R_{X Y}$ ). However, such an operator $A$ is not uniquely determined; particularly, the equation does not specify how $A$ acts on $\\operatorname{ker} C_{X X}$, allowing $A$ to agree with any arbitrary element in $\\mathcal{L}_{\\infty}$ on $\\operatorname{ker} C_{X X}$ (see Remark 2.2). When $C_{X X}$ is not injective, there are multiple choices of $A$ that achieve the minimum MSPE. In infinite dimensional settings, the injectivity of $C_{X X}$ is a stringent assumption, and verifying this condition from finite observations is impractical. Thus, pursuing prediction under the existence of the unique OLPO, as in the standard univariate or multivariate prediction, is restrictive. Even if a different setup is considered with a different purpose, similar concerns about the requirements for unique identification were recently raised by [3], and the enthusiastic reader is referred to their paper for more detailed discussion on the topic.\n\nRemark 2.1. Condition (b) in Proposition 2.1 was earlier obtained as the requirement for $A \\in \\mathcal{L}_{\\infty}$ to be an OLPO by Propositions 2.2-2.3 of [8]. Compared with this earlier result, Proposition 2.1 not only provides more general results, such as the explicit expression of the gap between the minimal attainable MSPE and the MSPE associated with any $B \\in \\mathcal{L}_{\\infty}$, but it also employs a distinct approach. The result of [8] relies on the notion of a linearly closed subspace and an extension of the standard projection theorem, while Proposition 2.1 is established by an algebraic proof based on the representation of cross-covariance operators in $[4]$.\n\nRemark 2.2. By invoking the Hahn-Banach extension theorem (Theorem 1.9.1 of [21]), we may assume that $A$ satisfying $A C_{X X}=C_{X Y}$ is a unique continuous linear map defined on the closure of $\\operatorname{ran} C_{X X}$, which is not equal to $\\mathcal{H}$ if $C_{X X}$ is not injective. Thus, if there exists another continuous linear operator $\\widehat{A}$ which\n\nagrees with $A$ on the closure of $\\operatorname{ran} C_{X X}$ but not on $\\left[\\operatorname{ran} C_{X X}\\right]^{\\perp}$, then $\\widehat{A}$ also satisfies that $\\mathbb{E}\\left\\|Y_{t}-\\widehat{A} X_{t}\\right\\|^{2}=\\left\\|C_{Y Y}-C_{Y Y}^{1 / 2} R_{X Y} R_{X Y}^{*} C_{Y Y}^{1 / 2}\\right\\|_{\\mathcal{S}_{1}}$.\n\nThe results given in Proposition 2.1 and Remark 2.2 imply that, particularly when the predictive variable is function-valued, there may be multiple OLPOs that satisfy (1.1). Furthermore, even if a unique OLPO exists, it may not be consistently estimable; a more detailed discussion is given in Example 1 below. This means that we are in a somewhat different situation from the previous simple univariate case discussed in Section 1, where we can estimate the OLP by consistently estimating the OLPO.\n\nExample 1. Suppose that $\\left\\{Y_{t}, X_{t}\\right\\}_{t \\geq 1}$ satisfies (1.2), $X_{t}$ has a positive covariance $C_{X X}$, and $\\varepsilon_{t}$ is serially independent and also independent of $X_{s}$ for any $s$. In this case, $A_{\\mathrm{o}} C_{X X}=C_{X Y}$, making $A_{\\mathrm{o}}$ an OLPO. Since $C_{X X}$ is injective, any continuous linear operator $\\widehat{A}$ agrees with $A_{\\mathrm{o}}$ on the closure of $\\operatorname{ran} C_{X X}$ also agrees with $A_{\\mathrm{o}}$ on $\\mathcal{H}$ (see Remark 2.2 and note that the closure of $\\operatorname{ran} C_{X X}$ is $\\mathcal{H}$ in this case). However, consistently estimating $A_{\\mathrm{o}}$ without any further assumptions is impossible. To illustrate this, we may consider the case where $\\left\\{f_{j}\\right\\}_{j \\geq 1}$ in (1.2) are known for simplicity. Even in this simplified scenario, there are infinitely many unknown parameters $\\left\\{a_{j}\\right\\}_{j \\geq 1}$ to be estimated from only $T$ samples, necessitating additional assumptions on $\\left\\{a_{j}\\right\\}_{j \\geq 1}$ for consistent estimation.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 3. Estimation \n\nWe observed that in a general Hilbert space setting, there can be not only multiple OLPOs but also instances where, even if a unique OLPO exists, consistent estimation of it is impossible. Nevertheless, under mild conditions, we may construct a predictor using a standard post dimension-reduction estimator in such a way that the associated empirical MSPE converges to the minimum MPSE as in the simple univariate case. To propose such a predictor, let\n\n$$\n\\widehat{C}_{X Y}=\\frac{1}{T} \\sum_{t=1}^{T} X_{t} \\otimes Y_{t}, \\quad \\widehat{C}_{X X, k_{T}}=\\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j} \\hat{v}_{j} \\otimes \\hat{v}_{j}, \\quad \\widehat{C}_{X X, k_{T}}^{-1}=\\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j}^{-1} \\hat{v}_{j} \\otimes \\hat{v}_{j}\n$$\n\nwhere $k_{T}$ is an integer satisfying the following assumption: below, we let $a_{1} \\wedge a_{2}=$ $\\min \\left\\{a_{1}, a_{2}\\right\\}$ for $a_{1}, a_{2} \\in \\mathbb{R}$ and assume that $\\max _{j \\geq 1}\\left\\{j: E_{j}\\right\\}=1$ if the condition $E_{j}$ is not satisfied for all $j \\geq 1$.\n\nAssumption 1 (Elbow-like rule). $k_{T}$ in (3.1) is given by\n\n$$\nk_{T}=\\max _{j \\geq 1}\\left\\{j: \\hat{\\lambda}_{j} \\geq \\hat{\\lambda}_{j+1}+\\tau_{T}\\right\\} \\wedge v_{T}^{-1}\n$$\n\nwhere $\\tau_{T}$ and $v_{T}$ are user-specific choices of positive constants decaying to 0 as $T \\rightarrow \\infty$, and both $\\tau_{T}^{-1}$ and $v_{T}^{-1}$ are bounded above by $\\gamma_{0} T^{\\gamma_{1}}$ for some $\\gamma_{0}>0$ and $\\gamma_{1} \\in(0,1 / 2)$.\n\n$\\widehat{C}_{X X, k_{T}}^{-1}$ in (3.1) is understood as the inverse of $\\widehat{C}_{X X}$ viewed as a map acting on the restricted subspace $\\operatorname{span}\\left\\{\\hat{v}_{j}\\right\\}_{j=1}^{k_{T}}$ and $k_{T}$ in Assumption 1 is a random integer by its construction (see Remark 3.1 below). By including $v_{T}$ in Assumption 1, we ensure that $k_{T}$ becomes $o_{p}\\left(T^{1 / 2}\\right)$, which facilitates our theoretical analysis. Even if the choice of $k_{T}$ depends on various contexts requiring a regularized inverse of $\\widehat{C}_{X X}$, it is commonly set to a much smaller number than $T$, and thus this condition does not impose any practical restrictions. A practically more meaningful decision is made by the first component, $\\max _{j \\geq 1}\\left\\{j: \\hat{\\lambda}_{j} \\geq\\right.$ $\\left.\\hat{\\lambda}_{j+1}+\\tau_{T}\\right\\}$ in Assumption 1. Firstly, given that $\\hat{\\lambda}_{k_{T}+1}>0$, this condition implies that $\\hat{\\lambda}_{k_{T}}^{-1}<\\tau_{T}^{-1}$, and thus $\\left\\|\\widehat{C}_{X X, k_{T}}^{-1}\\right\\|_{\\infty} \\leq \\tau_{T}^{-1}$, where $\\tau_{T}^{-1}$ diverges slowly compared to $T$; clearly, this is one of the essential requirements for $k_{T}$ (as the rank of a regularized inverse of $\\widehat{C}_{X X}$ ) to satisfy in the literature employing similar regularized inverses. Secondly, $k_{T}$ is determined near the point where the gap $\\hat{\\lambda}_{j}-\\hat{\\lambda}_{j+1}$ is no longer smaller than a specified threshold $\\tau_{T}$ for the last time. This approach is, in fact, analogous to the standard elbow rule used to determine the number of principal components in multivariate analysis based on the scree plot. Thus, even if Assumption 1 details some specific mathematical requirements necessary for our asymptotic analysis, these seem to closely align with existing practical rules for selecting $k_{T}$, commonly employed in current practice; for example, see [10].\n\nUsing the regularized inverse $\\widehat{C}_{X X, k_{T}}^{-1}$, the proposed predictor is constructed as follows:\n\n$$\n\\widehat{Y}_{t}=\\widehat{A} X_{t}, \\quad \\text { where } \\quad \\widehat{A}(\\cdot)=\\widehat{C}_{X Y} \\widehat{C}_{X X, k_{T}}^{-1}(\\cdot)=\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j}^{-1}\\left\\langle\\hat{v}_{j}, \\cdot\\right\\rangle\\left\\langle\\hat{v}_{j}, X_{t}\\right\\rangle Y_{t}\n$$\n\nThe above predictor is standard in the literature on functional data analysis as the least squares predictor of $Y_{t}$ given the projection of $X_{t}$ onto the space spanned by the eigenvectors corresponding to the first $k_{T}$ largest eigenvalues; a similar estimator was earlier considered by [23]. The predictor described in (3.2) and its modifications (such as those that will be considered in Section 4.4) have been widely studied in the literature and adapted to various contexts; see e.g., $[1],[2],[7],[17],[20],[27]$ and $[28]$.\nRemark 3.1. A significant difference in $\\widehat{A}$ compared to most of the existing estimators, lies in the choice of $k_{T}$. In many earlier articles, $k_{T}$ is directly chosen by researchers and thus regarded as deterministic. However, as pointed out by [26], even in this case, it is generally not recommended to choose $k_{T}$ arbitrarily without taking the eigenvalues $\\hat{\\lambda}_{j}$ into account. Therefore, it is natural to view $k_{T}$ as random from a practical point of view.\n\nTo establish consistency of $\\widehat{A}$ in (3.2), certain assumptions have been employed in the aforementioned literature, particularly concerning the eigenstructure of $C_{X X}$ and the unique identification of the target estimator $A$. However, as illustrated by Example 1 and Remark 2.2, these assumptions are not guaranteed to\n\nhold even in cases where there exists a well defined OLPO. Thus, in this paper, we do not make such assumptions for consistency, but allow $\\widehat{A}$ to potentially be inconsistent. Our main result in this section is that, despite not relying on the typical assumptions for consistency, the predictor $\\widehat{Y}_{t}$ asymptotically minimizes the MSPE, which only requires mild conditions on the sample (cross-)covariance operators.\n\nFor the subsequent discussion, we define the following: for any $B \\in \\mathcal{L}_{\\infty}$,\n\n$$\n\\Sigma(B)=\\frac{1}{T} \\sum_{t=1}^{T}\\left(Y_{t}-B X_{t}\\right) \\otimes\\left(Y_{t}-B X_{t}\\right)\n$$\n\nNote that $\\|\\Sigma(B)\\|_{\\mathcal{S}_{1}}$ is equivalent to the empirical MSPE, given by $T^{-1} \\sum_{t=1}^{T}\\left\\|Y_{t}-\\right.$ $\\left.B X_{t}\\right\\|^{2}$. We then define an asymptotically OLPO as a random bounded linear operator producing a predictor that asymptotically minimizes the MSPE in the following sense:\n\nDefinition 1. Any random bounded linear operator $\\widehat{B}$ is called an asymptotically OLPO if\n\n$$\n\\|\\Sigma(\\widehat{B})\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} \\Sigma_{\\min } \\quad \\text { as } T \\rightarrow \\infty\n$$\n\nwhere $\\Sigma_{\\min }=\\left\\|C_{Y Y}-C_{Y Y}^{1 / 2} R_{X Y} R_{X Y}^{*} C_{Y Y}^{1 / 2}\\right\\|_{\\mathcal{S}_{1}}$, which is the minimum MSPE that can be achieved by a linear predictor, as defined in Proposition 2.1.\n\nWe will employ the following assumption:\nAssumption 2 (Standard rate of convergence). $\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\infty},\\left\\|\\widehat{C}_{X X}-\\right.$ $\\left.C_{X X}\\right\\|_{\\infty}$, and $\\left\\|\\widehat{C}_{X Y}-C_{X Y}\\right\\|_{\\infty}$ are $O_{p}\\left(T^{-1 / 2}\\right)$.\n\nWe observe that $\\left\\{Y_{t} \\otimes Y_{t}-C_{Y Y}\\right\\}_{t \\geq 1},\\left\\{X_{t} \\otimes X_{t}-C_{X X}\\right\\}_{t \\geq 1}$, and $\\left\\{X_{t} \\otimes\\right.$ $\\left.Y_{t}-C_{X Y}\\right\\}_{t \\geq 1}$ are stationary sequences of Schatten 2-class operators. These operator-valued sequences may also be understood as stationary sequences in a separable Hilbert space ([7], p. 34). Then, Assumption 2 is satisfied under some non-restrictive regularity conditions; see e.g., Theorems 2.16-2.18 of [7] concerning the central limit theorems for Hilbert-valued random elements. Given that we are dealing with a weakly stationary sequence $\\left\\{Y_{t}, X_{t}\\right\\}_{t \\geq 1}$, Assumption 2 appears to be standard. Furthermore, it can be relaxed to a weaker requirement by imposing stricter conditions on $\\tau_{T}$ and $v_{T}$ in Assumption 1; this will be detailed in Section 4.1.\n\nWe now present the main result of this paper.\nTheorem 3.1. Under Assumptions 1-2, $\\widehat{A}$ is an asymptotically OLPO, i.e., $\\|\\Sigma(\\widehat{A})\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} \\Sigma_{\\min }$\n\nAs stated, an appropriate growth rate of $k_{T}$, detailed in Assumption 1, and the standard rate of convergence of the sample (cross-)covariance operators (Assumption 2) are all that we need in order to demonstrate that the proposed predictor in (3.2) is an asymptotically OLPO. As discussed earlier, since the choice rule in Assumption 1 is practically similar to existing rules for selecting $k_{T}$, a naive use of the simple post-dimension reduction estimator $\\widehat{A}$ in (3.2)\n\nwithout the usual assumptions for the unique identification and/or consistency, which are widely employed but challenging to verity, can still be justified as a way to asymptotically minimize the MSPE (see Section 4.2). Some discussions and extensions on the above theorem are given in the next section. We also consider the case where $\\widehat{A}$ is replaced by another estimator employing a different regularization scheme (Sections 4.3-4.4).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 4. Discussions and extensions\n### 4.1. A more general result\n\nIn Assumption 2, we assumed that the sample (cross-)covariances converge to the population counterparts with $\\sqrt{T}$-rate. However, the exact $\\sqrt{T}$-rate is not mandatory for the desired result and can be relaxed if we make appropriate adjustments on $\\tau_{T}$ and $v_{T}$ in Assumption 1 as follows:\nCorollary 4.1. Suppose that, for $\\beta \\in(0,1 / 2],\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\infty},\\left\\|\\widehat{C}_{X X}-\\right.$ $\\left.C_{X X}\\right\\|_{\\infty}$, and $\\left\\|\\widehat{C}_{X Y}-C_{X Y}\\right\\|_{\\infty}$ are $O_{p}\\left(T^{-\\beta}\\right)$. If Assumption 1 holds for $\\gamma_{1} \\in$ $(0, \\beta)$, then $\\widehat{A}$ is an asymptotically OLPO.\n\nThat is, $\\widehat{A}$ is an asymptotically OLPO under weaker assumptions on the sample (cross-)covariance operators if we impose stricter conditions on the decay rates of $\\tau_{T}$ and $v_{T} . \\tau_{T}$ and $v_{T}$ are user-specific choices dependent on $T$, so researchers can easily manipulate their decay rates. Corollary 4.1, thus, tells us that we can make $\\widehat{A}$ an asymptotically OLPO under more general scenarios by simply reducing the decay rates of $\\tau_{T}$ and $v_{T}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "### 4.2. Misspecified functional linear models and OLPO\n\nConsider the standard functional linear model\n\n$$\nY_{t}=A X_{t}+\\varepsilon_{t}, \\quad \\mathbb{E}\\left[X_{t} \\otimes \\varepsilon_{t}\\right]=0\n$$\n\nwhere $A$ is typically assumed to satisfy the following two conditions: (i) $A$ is Hilbert-Schmidt (i.e., $A \\in \\mathcal{S}_{2}$ ) and (ii) $A$ is uniquely identified (in $\\mathcal{S}_{2}$ ). A common assumption employed for the unique identification is that $\\operatorname{ran} C_{X X}$ is dense in $\\mathcal{H}$ (see Remark 2.2). Under additional technical assumptions on the eigenstructure of $C_{X X}$, such as those on the spectral gap $\\left(\\lambda_{j}-\\lambda_{j-1}\\right)$ as in [12], the proposed estimator $\\widehat{A}$ in (3.2) turns out to be consistent if $k_{T}$ grows appropriately. Of course, some alternative estimators, such as the least squares estimator with Tikhonov regularization (see, e.g., [6]), do not require any assumptions on the spectral gap, which is a well known advantage of such methods. However, they still require condition (i), the Hilbert-Schmidt property of $A$, with some additional regularity assumptions on $A$, and also impose assumptions for condition (ii) when discussing the consistency of those estimators.\n\nIt is crucial to have conditions (i) and (ii) together with the model (4.1), since a violation of either of these can easily lead to inconsistency (see Example 1). While these requirements are standard for estimation, they exclude many natural data generating mechanisms in $\\mathcal{H}$ (see Examples 1-2). Consequently, the model may suffer from misspecification issues. However, even in such cases, our results show that $\\widehat{A}$ attains the minimum MSPE asymptotically if $k_{T}$ grows at an appropriate rate, and thus affirm the potential use of the standard postdimension reduction estimator in practice without a careful examination of various technical conditions.\n\nExample 2. Similar to the example given in Section 5 of [5], suppose that $X_{t}=Y_{t-1}$ and $Y_{t}$ satisfies the functional $\\operatorname{AR}(1)$ law of motion: $Y_{t}=A Y_{t-1}+\\varepsilon_{t}$ with $A=\\sum_{j=1}^{\\infty} a_{j} w_{j} \\otimes w_{j}$ for some orthonormal basis $\\left\\{w_{j}\\right\\}_{j \\geq 1}$ and iid sequence $\\left\\{\\varepsilon_{t}\\right\\}_{t \\in \\mathbb{Z}}$ (see also [2]). This leads to the following pointwise $\\operatorname{AR}(1)$ model: $\\left\\langle Y_{t}, w_{j}\\right\\rangle=a_{j}\\left\\langle Y_{t-1}, w_{j}\\right\\rangle+\\left\\langle\\varepsilon_{t}, w_{j}\\right\\rangle$. This time series is guaranteed to be stationary if $\\sup _{j}\\left|a_{j}\\right|<1$ (this can be demonstrated with only a slight and obvious modification of Theorem 3.1 of [7] or Proposition 3.2 of [25]). On the other hand, for $A$ to be a Hilbert-Schmidt operator, a much more stringent condition $\\sum_{j=1}^{\\infty}\\left|a_{j}\\right|^{2}<\\infty$ is required, and, as a consequence, $\\left\\langle Y_{t}, w_{j}\\right\\rangle$ and $\\left\\langle Y_{t-1}, w_{j}\\right\\rangle$ need to be nearly uncorrelated for large $j$ while they can be arbitrarily correlated under the aforementioned condition for weak stationarity.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 4.3. Requirement of sufficient dimension reduction \n\nIn our proof of Theorem 3.1 (resp. Corollary 4.1), it is crucial to have a regularized inverse of $\\widehat{C}_{X X}$, denoted as $\\widehat{C}_{X X, k_{T}}^{-1}$, whose rank $k_{T}$ grows at a sufficiently slower rate than $T$; see e.g., (A.10) and (A.11) in Appendix A. Due to this requirement, our arguments for proving the main results (Theorem 3.1 and Corollary 4.1) are not straightforwardly extended to other popular estimators without dimension reduction, such as least squares-type estimators with ridge or Tikhonov regularization (see, e.g., [6]). Of course, this does not mean that these estimators cannot achieve prediction results similar to those in Theorem 3.1 and Corollary 4.1 without requiring the standard assumptions of HilbertSchmidtness and the unique identification of $A$. Rather, it simply suggests that, for estimators computed without dimension reduction, alternative approaches may be needed under different sets of assumptions. This could be a potential direction for future research.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "### 4.4. Dimension reduction of the target variable\n\nThe proposed estimator $\\widehat{A}$ is commonly used as a standard estimator of the functional linear model (4.1). Note that, in our construction of $\\widehat{A}$, the target variable $Y_{t}$ is used as is, without any dimension reduction. In the literature, estimators similar to $\\widehat{A}$ in (3.2), but where $Y_{t}$ is replaced with its version obtained through dimension reduction, also appear to be popular and are used in practice (see\n\ne.g., [27]). As noted in recent articles, dimension reduction of the target variable not only is generally non-essential for establishing certain key asymptotic properties (such as consistency) of the estimator but may also lead to a less optimal estimator (see Remark 1 of [13]).\n\nConsider the following predictor and estimator, constructed using a version of $Y_{t}$ with reduced dimension:\n\n$$\n\\widetilde{Y}_{t}=\\widetilde{A} X_{t}\n$$\n\nwhere\n\n$$\n\\widetilde{A}(\\cdot)=\\widehat{\\Pi}_{Y, \\ell_{T}} \\widehat{C}_{X Y} \\widehat{C}_{X X, k_{T}}^{-1}(\\cdot)=\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j}^{-1}\\left\\langle\\hat{v}_{j}, \\cdot\\right\\rangle\\left\\langle\\hat{v}_{j}, X_{t}\\right\\rangle \\widehat{\\Pi}_{Y, \\ell_{T}} Y_{t}\n$$\n\nand $\\widehat{\\Pi}_{Y, \\ell_{T}}=\\sum_{j=1}^{\\ell_{T}} \\hat{w}_{j} \\otimes \\hat{w}_{j}$ for some orthonormal basis $\\left\\{\\hat{w}_{j}\\right\\}_{j \\geq 1}$ and $\\ell_{T}$ growing as $T$ increases; often, $\\hat{w}_{j}$ is set to the eigenvector of $\\widehat{C}_{Y Y}$ or $\\widehat{C}_{X X}$ corresponding to the $j$-th largest eigenvalue, but our subsequent analysis is not restricted to these specific cases. Even if the additional dimension reduction applied to $Y_{t}$ introduces some complications in our theoretical analysis, it can also be shown that $\\widetilde{A}$ is an asymptotically OLPO under an additional mild condition.\n\nCorollary 4.2. Suppose that Assumptions 1-2 hold and there exists a sequence $m_{T}$ tending to infinity as $T \\rightarrow \\infty$ such that $m_{T}\\left\\|\\widehat{\\Pi}_{Y, \\ell_{T}} C_{Y Y}^{1 / 2}-C_{Y Y}^{1 / 2}\\right\\|_{\\infty} \\rightarrow_{p} 0$ and $m_{T} \\leq \\ell_{T}$ eventually. Then, $\\widetilde{A}$ is an asymptotically OLPO.\n\nGiven that $\\widehat{\\Pi}_{Y, \\ell_{T}}$ is the orthogonal projection with a growing rank, the condition given in Corollary 4.2 becomes easier to be satisfied if $\\ell_{T}$ grows more rapidly. Viewed in this light, the scenario in Theorem 3.1 can be seen as the limiting case where $\\widehat{\\Pi}_{Y, \\ell_{T}}=I$, indicating no dimension reduction applied to $Y_{t}$. Corollary 4.2 tells us that estimators obtained by reducing the dimensionality of $Y_{t}$ tend to be asymptotically OLPOs under non-restrictive conditions. Given that $C_{Y Y}^{1 / 2}$ and $C_{Y Y}$ share the same eigenvectors, one may conjecture that satisfying the requirement in Corollary 4.2 could be easier if $\\widehat{w}_{j}$ is an eigenvector of $\\widehat{C}_{Y Y}$. Theoretical justification of this conjecture may require further assumptions on the eigenstructure of $C_{Y Y}$. Given the focus on optimal linear prediction $\\mathcal{H}$ under minimal conditions, we do not pursue this direction and leave it for future study.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 5. Simulation \n\nWe provide simulation evidence for our theoretical findings, focusing on cases that have not been sufficiently explored in the literature and where consistent estimation of the OLPO is impossible. In all simulation experiments, the number of replications is 1000 .\n\nLet $\\left\\{f_{j}\\right\\}_{j \\geq 1}$ be the Fourier basis of $L^{2}[0,1]$, the Hilbert space of squareintegrable functions on $[0,1]$, i.e., for $x \\in[0,1], f_{1}(x)=1$, and for $j \\geq 2$,\n\n$f_{j}=\\sqrt{2} \\sin (2 \\pi j x)$ if $j$ is even, and $f_{j}=\\sqrt{2} \\cos (2 \\pi j x)$ if $j$ is odd. We define $X_{t}$ and $Y_{t}$ as follows: for some real numbers $\\left\\{a_{j}\\right\\}_{j=1}^{101}$,\n\n$$\nY_{t}=A X_{t}+\\varepsilon_{t}, \\quad X_{t}=\\sum_{j=1}^{101} a_{j}\\left\\langle X_{t-1}, f_{j}\\right\\rangle f_{j}+e_{t}\n$$\n\nwhere $\\left\\{e_{t}\\right\\}_{t \\geq 1}$ and $\\left\\{\\varepsilon_{t}\\right\\}_{t \\geq 1}$ are assumed to be mutually and serially independent sequences of random elements. In this simulation setup, the minimal MSPE that can be achieved by a linear operator equals the Schatten 1-norm (trace norm) of the covariance operator $C_{\\varepsilon}$ of $\\varepsilon_{t}$.\n\nWe conducted experiments in three different cases. In the first case (referred to as Case BB), $e_{t}$ and $\\varepsilon_{t}$ are set to independent realizations of the standard Brownian bridge, and in the second case (referred to as Case CBM), they are set to independent realizations of the centered Brownian motion. In these first two cases, the $j$-th largest eigenvalue of $C_{\\varepsilon}$ is given by $\\pi^{-2} j^{-2}$; see [14] (p. 86) and [16] (p. 1465). From a well known result on the Riemann zeta function (see e.g., [15]), we find that these eigenvalues add up to $1 / 6$, which is the minimal achievable MSPE. In the last case (referred to as Case BM), $e_{t}$ and $\\varepsilon_{t}$ are set as realizations of the standard Brownian motion multiplied by a constant, which is properly chosen to ensure that the minimal MSPE in this scenario matches that in the previous two cases.\n\nWe will subsequently consider the following four models, depending on the specification of $a_{j}$ and $A$ for generating $X_{t}$ and $Y_{t}$ : for all $j \\geq 1$,\n\nModel 1 (M1): $a_{j} \\sim_{\\text {iid }} U(-0.1,0.25)$ and $A f_{j}=b_{0} f_{j}$, Model 2 (M2): $a_{j} \\sim_{\\text {iid }} U(-0.1,0.25)$ and $A f_{j}=b_{j} f_{j} 1\\{j \\leq 100\\}+f_{j} 1\\{j>100\\}$, Model 3 (M3): $a_{j} \\sim_{\\text {iid }} U(-0.1,0.75)$ and $A f_{j}=b_{0} f_{j}$, Model 4 (M4): $a_{j} \\sim_{\\text {iid }} U(-0.1,0.75)$ and $A f_{j}=b_{j} f_{j} 1\\{j \\leq 100\\}+f_{j} 1\\{j>100\\}$,\nwhere $b_{0} \\sim U[-2.5,2.5]$ and $b_{j} \\sim_{\\text {iid }} U[-2.5,2.5]$ for $j \\geq 1$. Note that the parameters $a_{j}, b_{0}$ and $b_{j}$ are generated differently in each simulation run; this allows us to assess the average performance of the proposed predictor across various parameter choices. In many empirical examples involving dependent sequences of functions $X_{t}$, it is often expected that $\\left\\langle X_{t}, v\\right\\rangle$ for any $v \\in \\mathcal{H}$ exhibits a positive lag-one autocorrelation, so we let $a_{j}$ tend to take positive values more frequently in our simulation settings. In any of the above cases, $A$ is non-compact and hence cannot be consistently estimated without prior knowledge on the structure of $\\left\\{A f_{j}\\right\\}_{j \\geq 1}$, which is as in the operator considered in Example 1.\n\nWe set $\\tau_{T}=0.01\\left\\|\\widehat{C}_{X X} \\mathbb{\\\\|}_{\\mathcal{S}_{1}} T^{\\gamma}\\right.$ and $v_{T}=0.5 T^{\\gamma}$ for some $\\gamma>0$, where note that $\\tau_{T}$ is designed to reflect the scale of $X_{t}$, as proposed by [26] in a similar context. We then computed the empirical MSPE associated with $\\widehat{A}$, introduced in (3.2). Table 1 reports the excess MSPE (the empirical MSPE minus 1/6) for each of the considered cases. As expected from our main theoretical results, the excess MSPE associated with $\\widehat{A}$ approaches zero as the sample size $T$ increases, even if $\\widehat{A}$ is not consistent. Notably, in Case BM, the excess MSPE tends to be\n\nTable 1\nExcess MSPE of the proposed predictor\n(a) Case BB\n\n![table_0](table_0)\n\n(b) Case CBM\n\n![table_1](table_1)\n\n(c) Case BM\n\n![table_2](table_2)\n\nNotes: The excess MSPE is calculated as the empirical MSPE minus $1 / 6$, where $1 / 6$ represents the minimal achievable MSPE by a linear predictor. $\\tau_{T}=0.01\\left\\|\\widehat{C}_{X X}\\right\\|_{\\mathcal{S}_{1}} T^{\\gamma}$ and $v_{T}=0.5 T^{\\gamma}$ for $\\gamma \\in\\{0.45,0.475\\}$. The reported MSPEs are approximately computed by (i) generating $X_{t}$ and $Y_{t}$ on a fine grid of $[0,1]$ with 200 equally spaced grid points, and then (ii) representing those with 100 cubic B-spline functions. The results exhibit little change with varying numbers of grid points and B-spline functions.\nsignificantly smaller than in the other two cases (Case BB and Case CMB); even with a moderately large number of observations, the empirical MSPE in Case BM tends to be close to the minimal MSPE. This suggests that the performance of the proposed predictor significantly depends on the specification of $\\varepsilon_{\\mathrm{f}}$. Overall, the simulation results reported in Table 1 support our theoretical finding in Section 3. We also experimented with an alternative estimator $\\widehat{A}$ which is introduced in Section 4.4 and obtained qualitatively similar supporting evidence; some of the simulation results are reported in Appendix B of the appendix.",
      "tables": {
        "table_0": "|  | $\\gamma=0.475$ |  |  |  |  | $\\gamma=0.45$ |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $T$ | 50 | 100 | 200 | 400 | 800 | 50 | 100 | 200 | 400 | 800 |\n| M1 | 0.043 | 0.035 | 0.023 | 0.018 | 0.012 | 0.072 | 0.051 | 0.031 | 0.022 | 0.014 |\n| M2 | 0.041 | 0.033 | 0.022 | 0.017 | 0.011 | 0.068 | 0.049 | 0.029 | 0.021 | 0.013 |\n| M3 | 0.056 | 0.046 | 0.031 | 0.023 | 0.016 | 0.094 | 0.066 | 0.040 | 0.028 | 0.019 |\n| M4 | 0.054 | 0.044 | 0.029 | 0.022 | 0.015 | 0.089 | 0.063 | 0.038 | 0.027 | 0.018 |",
        "table_1": "|  | $\\gamma=0.475$ |  |  |  |  | $\\gamma=0.45$ |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $T$ | 50 | 100 | 200 | 400 | 800 | 50 | 100 | 200 | 400 | 800 |\n| M1 | 0.045 | 0.036 | 0.024 | 0.018 | 0.012 | 0.074 | 0.052 | 0.031 | 0.022 | 0.014 |\n| M2 | 0.041 | 0.034 | 0.022 | 0.017 | 0.011 | 0.069 | 0.049 | 0.029 | 0.020 | 0.013 |\n| M3 | 0.059 | 0.048 | 0.032 | 0.024 | 0.016 | 0.098 | 0.068 | 0.041 | 0.029 | 0.019 |\n| M4 | 0.055 | 0.045 | 0.030 | 0.022 | 0.015 | 0.091 | 0.064 | 0.038 | 0.027 | 0.018 |",
        "table_2": "|  | $\\gamma=0.475$ |  |  |  |  | $\\gamma=0.45$ |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $T$ | 50 | 100 | 200 | 400 | 800 | 50 | 100 | 200 | 400 | 800 |\n| M1 | 0.011 | 0.010 | 0.006 | 0.004 | 0.004 | 0.026 | 0.017 | 0.009 | 0.006 | 0.004 |\n| M2 | 0.012 | 0.010 | 0.006 | 0.004 | 0.004 | 0.027 | 0.017 | 0.009 | 0.006 | 0.004 |\n| M3 | 0.018 | 0.014 | 0.009 | 0.006 | 0.005 | 0.037 | 0.024 | 0.013 | 0.008 | 0.006 |\n| M4 | 0.018 | 0.015 | 0.009 | 0.006 | 0.005 | 0.038 | 0.024 | 0.013 | 0.008 | 0.006 |"
      },
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# 6. Concluding remarks \n\nThis paper studies linear prediction in a Hilbert space, demonstrating that, under mild conditions, the empirical MSPEs associated with standard post-\n\ndimension reduction estimators approach the minimal achievable MSPE. There is ample room for future research; for example, it would be intriguing to explore whether similar prediction results can be obtained from various alternatives or modifications of the simple post-dimension reduction estimators considered in the literature.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "# Appendix A: Proofs\n## A.1. Useful lemmas\n\nLemma A.1. Let $\\Gamma$ be a nonnegative self-adjoint Schatten 1-class operator. For any $D \\in \\mathcal{L}_{\\infty}$, the following hold.\n(i) $\\Gamma^{1 / 2} D D^{*} \\Gamma^{1 / 2}$ and $D \\Gamma D^{*}$ are Schatten 1-class operators and their Schatten 1-norms are bounded above by $\\|D\\|_{\\infty}^{2}\\|\\Gamma\\|_{\\mathcal{S}_{1}}$.\n(ii) $\\Gamma^{1 / 2} D$ and $D^{*} \\Gamma^{1 / 2}$ are Schatten 2-class operators.\n\nProof. Let $\\Gamma=\\sum_{j=1}^{\\infty} c_{j} w_{j} \\otimes w_{j}$, where $c_{1} \\geq c_{2} \\geq \\ldots \\geq 0$ and $c_{j}$ may be zero. By allocating a proper vector to each zero eigenvalue, we may assume that $\\left\\{w_{j}\\right\\}_{j \\geq 1}$ is an orthonormal basis of $\\mathcal{H}$. To show (i), we note that $\\left\\langle\\Gamma^{1 / 2} D D^{*} \\Gamma^{1 / 2} w_{j}, w_{j}\\right\\rangle=$ $c_{j}\\left\\langle w_{j}, D D^{*} w_{j}\\right\\rangle \\leq c_{j}\\|D\\|_{\\infty}^{2}$, from which $\\left\\|\\Gamma^{1 / 2} D D^{*} \\Gamma^{1 / 2}\\right\\|_{\\mathcal{S}_{1}} \\leq\\|D\\|_{\\infty}^{2}\\|\\Gamma\\|_{\\mathcal{S}_{1}}$ is established. The desired result for $\\left\\|D \\Gamma D^{*}\\right\\|_{\\mathcal{S}_{1}}$ is already well known, see e.g., [11] (p. 267). To show (ii), we observe that $\\left\\|\\Gamma^{1 / 2} D\\right\\|_{\\mathcal{S}_{2}}^{2}=\\left\\|D^{*} \\Gamma^{1 / 2}\\right\\|_{\\mathcal{S}_{2}}^{2} \\leq\\|D\\|_{\\infty}^{2} \\sum_{j=1}^{\\infty}\\left\\|\\Gamma^{1 / 2} w_{j}\\right\\|^{2}=$ $\\|D\\|_{\\infty}^{2} \\sum_{j=1}^{\\infty} c_{j}<\\infty$, which establishes the desired result.\nLemma A.2. Let $\\left\\{\\Gamma_{j}\\right\\}_{j \\geq 1}$ be a sequence of random Schatten 1-class operators and let $\\Gamma$ be a self-adjoint Schatten 1-class operator. Then, for any orthonormal basis $\\left\\{w_{j}\\right\\}_{j \\geq 1}$ of $\\mathcal{H}$ and $m_{T} \\geq 0$,\n\n$$\n\\left\\|\\Gamma_{j}-\\Gamma\\right\\|_{\\mathcal{S}_{1}}=O_{p}\\left(\\left\\|\\Gamma_{j}\\right\\|_{\\mathcal{S}_{1}}-\\|\\Gamma\\|_{\\mathcal{S}_{1}}\\right)+O_{p}\\left(m_{T}\\left\\|\\Gamma_{j}-\\Gamma\\right\\|_{\\infty}+\\sum_{j=m_{T}+1}^{\\infty}\\left\\langle\\Gamma w_{j}, w_{j}\\right\\rangle\\right)\n$$\n\nMoreover, $\\left\\|\\Gamma_{j}-\\Gamma\\right\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} 0$ if $\\left\\|\\Gamma_{j}\\right\\|_{\\mathcal{S}_{1}}-\\|\\Gamma\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} 0$ and $m_{T}\\left\\|\\Gamma_{j}-\\Gamma\\right\\|_{\\infty} \\rightarrow_{p} 0$ as $m_{T} \\rightarrow \\infty$ and $T \\rightarrow \\infty$.\nProof. Equation (A.1) directly follows from Lemma 2 (and also Theorem 2) of [19]. Moreover, if $\\left\\|\\Gamma_{j}\\right\\|_{\\mathcal{S}_{1}}-\\|\\Gamma\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} 0$ and $m_{T}\\left\\|\\Gamma_{j}-\\Gamma\\right\\|_{\\infty} \\rightarrow_{p} 0$ as $m_{T} \\rightarrow \\infty$ and $T \\rightarrow \\infty$, we find that $\\left\\|\\Gamma_{j}-\\Gamma\\right\\|_{\\mathcal{S}_{1}}=O_{p}\\left(\\sum_{j=m_{T}+1}^{\\infty}\\left\\langle\\Gamma w_{j}, w_{j}\\right\\rangle\\right)$. Since $\\Gamma$ is a self-adjoint Schatten 1-class operator, we have $\\sum_{j=1}^{\\infty}\\left\\langle\\Gamma w_{j}, w_{j}\\right\\rangle \\rightarrow_{p}\\|\\Gamma\\|_{\\mathcal{S}_{1}}<\\infty$, from which we conclude that $\\sum_{j=m_{T}+1}^{\\infty}\\left\\langle\\Gamma w_{j}, w_{j}\\right\\rangle \\rightarrow_{p} 0$ as $m_{T} \\rightarrow \\infty$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "## A.2. Proofs of the theoretical results\n\nProof of Proposition 2.1. From Theorem 1 of [4], we find that $C_{X Y}$ allows the following representation: for a unique bounded linear operator $R_{X Y}$ satisfying $\\left\\|R_{X Y}\\right\\|_{\\infty} \\leq 1$\n\n$$\nC_{X Y}=C_{Y Y}^{1 / 2} R_{X Y} C_{X X}^{1 / 2}\n$$\n\nLet $C_{\\min }=C_{Y Y}-C_{Y Y}^{1 / 2} R_{X Y} R_{X Y}^{*} C_{Y Y}^{1 / 2}$, which is clearly self-adjoint. Moreover, $C_{\\min }$ is a nonnegative Schatten 1-class operator. To see this, note that for any $w \\in \\mathcal{H}$,\n\n$$\n\\left\\langle C_{\\min } w, w\\right\\rangle=\\left\\langle\\left(C_{Y Y}-C_{Y Y}^{1 / 2} R_{X Y} R_{X Y}^{*} C_{Y Y}^{1 / 2}\\right) w, w\\right\\rangle\n$$\n\n$$\n=\\left\\|C_{Y Y}^{1 / 2} w\\right\\|^{2}-\\left\\|R_{X Y}^{*} C_{Y Y}^{1 / 2} w\\right\\|^{2} \\geq 0\n$$\n\n(i.e., $C_{\\min }$ is nonnegative), which is because $\\left\\|R_{X Y}^{*} C_{Y Y}^{1 / 2} w\\right\\|^{2} \\leq\\left\\|R_{X Y}^{*}\\right\\|_{\\infty}^{2}\\left\\|C_{Y Y}^{1 / 2} w\\right\\|^{2} \\leq$ $\\left\\|C_{Y Y}^{1 / 2} w\\right\\|^{2}$. Moreover, from (A.2) and Lemma A.1(ii), we know that, for any orthonormal basis $\\left\\{w_{j}\\right\\}_{j \\geq 1},\\left\\|C_{\\min }\\right\\|_{\\mathcal{S}_{1}}=\\sum_{j=1}^{\\infty}\\left(\\left\\|C_{Y Y}^{1 / 2} w_{j}\\right\\|^{2}-\\left\\|R_{X Y}^{*} C_{Y Y}^{1 / 2} w_{j}\\right\\|^{2}\\right)<$ $\\infty$.\n\nWe then find the following holds for any $B \\in \\mathcal{L}_{\\infty}$ and $w \\in \\mathcal{H}$ :\n\n$$\n\\begin{aligned}\n& \\left\\langle\\left(C_{Y Y}-C_{X Y} B^{*}-B C_{X Y}^{*}+B C_{X X} B^{*}\\right) w, w\\right\\rangle \\\\\n& =\\left\\|C_{Y Y}^{1 / 2} w\\right\\|^{2}+\\left\\|C_{X X}^{1 / 2} B^{*} w\\right\\|^{2}-2\\left\\langle C_{X X}^{1 / 2} B^{*} w, R_{X Y}^{*} C_{Y Y}^{1 / 2} w\\right\\rangle \\\\\n& =\\left\\|C_{X X}^{1 / 2} B^{*} w-R_{X Y}^{*} C_{Y Y}^{1 / 2} w\\right\\|^{2}+\\left\\|C_{Y Y}^{1 / 2} w\\right\\|^{2}-\\left\\|R_{X Y}^{*} C_{Y Y}^{1 / 2} w\\right\\|^{2} \\\\\n& =\\left\\|C_{X X}^{1 / 2} B^{*} w-R_{X Y}^{*} C_{Y Y}^{1 / 2} w\\right\\|^{2}+\\left\\langle C_{\\min } w, w\\right\\rangle\n\\end{aligned}\n$$\n\nWe know from (A.3) that\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left\\|Y_{t}-B X_{t}\\right\\|^{2} & =\\sum_{j=1}^{\\infty}\\left\\langle\\left(C_{Y Y}-C_{X Y} B^{*}-B C_{X Y}^{*}+B C_{X X} B^{*}\\right) w_{j}, w_{j}\\right\\rangle \\\\\n& =\\sum_{j=1}^{\\infty}\\left\\|C_{X X}^{1 / 2} B^{*} w_{j}-R_{X Y}^{*} C_{Y Y}^{1 / 2} w_{j}\\right\\|^{2}+\\left\\|C_{\\min }\\right\\|_{\\mathcal{S}_{1}}\n\\end{aligned}\n$$\n\nwhere $\\left\\{w_{j}\\right\\}_{j \\geq 1}$ is any orthonormal basis of $\\mathcal{H}$. Note that $C_{X X}^{1 / 2} B^{*}-R_{X Y}^{*} C_{Y Y}^{1 / 2}$ is a Schatten 2-class operator (Lemma A.1(ii)) and thus we find that $\\sum_{j=1}^{\\infty}\\left\\|C_{X X}^{1 / 2} B^{*} w_{j}-\\right.$ $\\left.R_{X Y}^{*} C_{Y Y}^{1 / 2} w_{j}\\right\\|^{2}=\\left\\|C_{X X}^{1 / 2} B^{*}-R_{X Y}^{*} C_{Y Y}^{1 / 2}\\right\\|_{\\mathcal{S}_{2}}^{2}$. From this result combined with (A.4), the desired result immediately follows.\n\nProofs of Theorem 3.1 and Corollary 4.1. To accommodate more general cases, which are considered in Corollary 4.1, we hereafter assume that $\\tau_{T}^{-1}=O_{p}\\left(T^{\\gamma_{1}}\\right)$ and $v_{T}^{-1}=O_{p}\\left(T^{\\gamma_{1}}\\right)$ for some $\\gamma_{1} \\in(0, \\beta)$, and $\\left\\|\\widehat{C}_{X X}-C_{X X}\\right\\|_{\\infty},\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\infty}$ and $\\left\\|\\widehat{C}_{X Y}-C_{X Y}\\right\\|_{\\infty}$ are all $O_{p}\\left(T^{\\beta}\\right)$ for some $\\beta \\in(0,1 / 2]$. Our proof of Theorem 3.1 corresponds to the particular case with $\\beta=1 / 2$.\n\nNote that $\\sup _{j \\geq 1}\\left|\\hat{\\lambda}_{j}-\\lambda_{j}\\right| \\leq\\left\\|\\widehat{C}_{X X}-C_{X X}\\right\\|_{\\infty}=O_{p}\\left(T^{-\\beta}\\right)$ (Lemma 4.2 of [7]). Under Assumption 1, we have $\\hat{\\lambda}_{k_{T}}-\\hat{\\lambda}_{k_{T}+1} \\geq \\tau_{T}$ and thus\n$T^{\\beta}\\left(\\hat{\\lambda}_{k_{T}}-\\hat{\\lambda}_{k_{T}+1}\\right)=T^{\\beta}\\left(\\hat{\\lambda}_{k_{T}}-\\lambda_{k_{T}}+\\lambda_{k_{T}+1}-\\hat{\\lambda}_{k_{T}+1}\\right)+T^{\\beta}\\left(\\lambda_{k_{T}}-\\lambda_{k_{T}+1}\\right) \\geq T^{\\beta} \\tau_{T}$.\nIf $\\lambda_{k_{T}}=\\lambda_{k_{T}+1}$, (A.5) reduces to $T^{\\beta}\\left(\\hat{\\lambda}_{k_{T}}-\\hat{\\lambda}_{k_{T}+1}\\right) \\geq T^{\\beta} \\tau_{T}$. Moreover, since $T^{\\beta}\\left(\\hat{\\lambda}_{k_{T}}-\\hat{\\lambda}_{k_{T}+1}\\right)=O_{p}(1)$ and $T^{\\beta} \\tau_{T} \\rightarrow_{p} \\infty, \\mathbb{P}\\left\\{\\hat{\\lambda}_{k_{T}}-\\hat{\\lambda}_{k_{T}+1} \\geq \\tau_{T} \\mid \\lambda_{k_{T}}=\\right.$ $\\left.\\lambda_{k_{T}+1}\\right\\} \\rightarrow 0$ as $T \\rightarrow \\infty$. Using the Bayes' rule and the facts that $\\mathbb{P}\\left\\{\\hat{\\lambda}_{k_{T}}-\\hat{\\lambda}_{k_{T}+1} \\geq\\right.$ $\\left.\\tau_{T}\\right\\}=1$ and $\\mathbb{P}\\left\\{\\lambda_{k_{T}}=\\lambda_{k_{T}+1}\\right\\}=\\mathbb{P}\\left\\{\\lambda_{k_{T}}=\\lambda_{k_{T}+1} \\mid \\hat{\\lambda}_{k_{T}}-\\hat{\\lambda}_{k_{T}+1} \\geq \\tau_{T}\\right\\}$ by Assumption 1, we find that $\\mathbb{P}\\left\\{\\lambda_{k_{T}}=\\lambda_{k_{T}+1}\\right\\} \\rightarrow 0$. To establish the desired consistency, we thus may subsequently assume that $\\lambda_{k_{T}}>\\lambda_{k_{T}+1}$.\n\nLet $C_{\\min }=C_{Y Y}-C_{Y Y}^{1 / 2} R_{X Y} R_{X Y}^{*} C_{Y Y}^{1 / 2}$. Since there exists $A \\in \\mathcal{L}_{\\infty}$ such that $C_{X Y}=A C_{X X}, C_{\\min }=C_{Y Y}-A C_{X X} A^{*}$ holds (Corollary 2.1). From the triangular inequality applied to the $\\mathcal{S}_{1}$-norm, we then find that\n\n$$\n\\begin{aligned}\n\\|\\Sigma(\\widehat{A})-C_{\\min }\\|_{\\mathcal{S}_{1}} \\leq & \\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\mathcal{S}_{1}}+\\left\\|\\widehat{A} \\widehat{C}_{X X, k_{T}} \\widehat{A}^{*}-A C_{X X, k_{T}} A^{*}\\right\\|_{\\mathcal{S}_{1}} \\\\\n& +\\left\\|A C_{X X, k_{T}} A^{*}-A C_{X X} A^{*}\\right\\|_{\\mathcal{S}_{1}}\n\\end{aligned}\n$$\n\nIt suffices to show that each summand in the RHS of (A.6) is $o_{p}(1)$.\nWe will first consider the first term in the RHS of (A.6). Let $m_{T}$ be any divergent sequence (depending on $T$ ) but satisfy $T^{-\\beta} m_{T} \\rightarrow 0$. Note that $\\left\\|\\widehat{C}_{Y Y}\\right\\|_{\\mathcal{S}_{1}}-$ $\\left\\|C_{Y Y}\\right\\|_{\\mathcal{S}_{1}}=\\sum_{j=1}^{m_{T}}\\left(\\hat{\\mu}_{j}-\\mu_{j}\\right)+\\sum_{j=m_{T}+1}^{\\infty}\\left(\\hat{\\mu}_{j}-\\mu_{j}\\right) \\leq m_{T}\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\infty}+$ $\\sum_{j=m_{T}+1}^{\\infty}\\left(\\hat{\\mu}_{j}-\\mu_{j}\\right)$. For every $\\delta>0$, let $E_{\\delta}=\\left\\{\\left|\\sum_{j=m_{T}+1}^{\\infty}\\left(\\hat{\\mu}_{j}-\\mu_{j}\\right)\\right|>\\frac{\\delta}{2}\\right\\}$ and $F_{\\delta}=\\left\\{m_{T}\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\infty}>\\frac{\\delta}{2}\\right\\}$. Since $\\mathbb{P}\\left\\{m_{T}\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\infty}+\\sum_{j=m_{T}+1}^{\\infty}\\left(\\hat{\\mu}_{j}-\\right.\\right.$ $\\left.\\left.\\mu_{j}\\right)>\\delta\\right\\} \\leq \\mathbb{P}\\left\\{E_{\\delta}\\right\\}+\\mathbb{P}\\left\\{F_{\\delta}\\right\\}$, we find that\n\n$$\n\\mathbb{P}\\left\\{\\left\\|\\widehat{C}_{Y Y}\\right\\|_{\\mathcal{S}_{1}}-\\left\\|C_{Y Y}\\right\\|_{\\mathcal{S}_{1}}>\\delta\\right\\} \\leq \\mathbb{P}\\left\\{E_{\\delta}\\right\\}+\\mathbb{P}\\left\\{F_{\\delta}\\right\\}\n$$\n\nNote that $\\widehat{C}_{Y Y}$ and $C_{Y Y}$ are Schatten 1-class operators (almost surely) and also $m_{T}$ increases without bound. Moreover, $m_{T}\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\infty}=O_{p}\\left(m_{T} T^{-\\beta}\\right)=$ $o_{p}(1)$ under our assumptions. These results imply that $\\mathbb{P}\\left\\{E_{\\delta}\\right\\} \\rightarrow 0$ and $\\mathbb{P}\\left\\{F_{\\delta}\\right\\} \\rightarrow$ 0 as $T \\rightarrow \\infty$, and thus we conclude that $\\left\\|\\widehat{C}_{Y Y}\\right\\|_{\\mathcal{S}_{1}}-\\left\\|C_{Y Y}\\right\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} 0$. Combining this result with Lemma A. 2 and the fact that $m_{T}\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\infty}=o_{p}(1)$, we find that $\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} 0$ as desired.\n\nWe next consider the third term in the RHS of (A.6). We know from Lemma A. 1 that $\\left\\|A C_{X X, k_{T}} A^{*}-A C_{X X} A^{*}\\right\\|_{\\mathcal{S}_{1}} \\leq\\|A\\|_{\\infty}^{2}\\left\\|C_{X X, k_{T}}-C_{X X}\\right\\|_{\\mathcal{S}_{1}}$. Since $C_{X X}$ is a Schatten 1-class operator and $k_{T}$ grows without bound, $\\left\\|C_{X X, k_{T}}-C_{X X}\\right\\|_{\\mathcal{S}_{1}}=$ $\\sum_{j=k_{T}+1}^{\\infty} \\lambda_{j} \\rightarrow_{p} 0$ and thus $\\left\\|A C_{X X, k_{T}} A^{*}-A C_{X X} A^{*}\\right\\|_{\\mathcal{S}_{1}}=o_{p}(1)$ as desired.\n\nWe lastly consider the second term in the RHS of (A.6). Let $\\varepsilon_{t}=Y_{t}-A X_{t}$. Since $\\widehat{C}_{X Y}=A \\widehat{C}_{X X}+\\widehat{C}_{\\varepsilon X}$ and $\\widehat{A} \\widehat{C}_{X X, k_{T}} \\widehat{A}^{*}=\\widehat{C}_{X Y} \\widehat{C}_{X X, k_{T}}^{-1} \\widehat{C}_{X Y}^{*}$, we have\n\n$$\n\\begin{aligned}\n\\widehat{A} \\widehat{C}_{X X, k_{T}} \\widehat{A}^{*} & =\\left(A \\widehat{C}_{X X}+\\widehat{C}_{\\varepsilon X}\\right) \\widehat{C}_{X X, k_{T}}^{-1}\\left(A \\widehat{C}_{X X}+\\widehat{C}_{\\varepsilon X}\\right)^{*} \\\\\n& =A \\widehat{C}_{X X, k_{T}} A^{*}+A \\widehat{\\Pi}_{X, k_{T}} \\widehat{C}_{\\varepsilon X}^{*}+\\widehat{C}_{\\varepsilon X} \\widehat{\\Pi}_{X, k_{T}} A^{*}+\\widehat{C}_{\\varepsilon X} \\widehat{C}_{X X, k_{T}}^{-1} \\widehat{C}_{\\varepsilon X}^{*}\n\\end{aligned}\n$$\n\nwhere $\\widehat{\\Pi}_{X, k_{T}}=\\sum_{j=1}^{k_{T}} \\hat{v}_{j} \\otimes \\hat{v}_{j}$. Therefore, we have\n\n$$\n\\begin{aligned}\n& \\left\\|\\widehat{A} \\widehat{C}_{X X, k_{T}} \\widehat{A}^{*}-A C_{X X, k_{T}} A^{*}\\right\\|_{\\mathcal{S}_{1}} \\\\\n& \\leq\\left\\|A \\widehat{C}_{X X, k_{T}} A^{*}-A C_{X X, k_{T}} A^{*}\\right\\|_{\\mathcal{S}_{1}} \\\\\n& \\quad+\\left\\|A \\widehat{\\Pi}_{X, k_{T}} \\widehat{C}_{\\varepsilon X}^{*}+\\widehat{C}_{\\varepsilon X} \\widehat{\\Pi}_{X, k_{T}} A^{*}+\\widehat{C}_{\\varepsilon X} \\widehat{C}_{X X, k_{T}}^{-1} \\widehat{C}_{\\varepsilon X}^{*}\\right\\|_{\\mathcal{S}_{1}}\n\\end{aligned}\n$$\n\nIt will be proved later that the first term in the RHS of (A.7) is $o_{p}(1)$, i.e.,\n\n$$\n\\left\\|A \\widehat{C}_{X X, k_{T}} A^{*}-A C_{X X, k_{T}} A^{*}\\right\\|_{\\mathcal{S}_{1}}=o_{p}(1)\n$$\n\nWe deduce from Lemma A.1(i) that the second term in the RHS of (A.7), below denoted simply as $\\mathcal{D}$, satisfies\n\n$$\n\\mathcal{D} \\leq O(1)\\left\\|\\widehat{\\Pi}_{k_{T}} \\widehat{C}_{\\varepsilon X}^{*}\\right\\|_{\\mathcal{S}_{1}}+\\left\\|\\widehat{C}_{\\varepsilon X} \\widehat{C}_{X X, k_{T}}^{-1} \\widehat{C}_{\\varepsilon X}^{*}\\right\\|_{\\mathcal{S}_{1}}^{2}\n$$\n\nSince $\\widehat{C}_{X \\varepsilon}=T^{-1} \\sum_{t=1}^{T} X_{t} \\otimes Y_{t}-T^{-1} \\sum_{t=1}^{T} X_{t} \\otimes A X_{t}=C_{Y X}-A C_{X X}+$ $O_{p}\\left(T^{-\\beta}\\right)=O_{p}\\left(T^{-\\beta}\\right)$, we find that\n\n$$\n\\max \\left\\{\\left\\|\\widehat{\\Pi}_{k_{T}} \\widehat{C}_{\\varepsilon X}^{*}\\right\\|_{\\mathcal{S}_{1}},\\left\\|\\widehat{C}_{\\varepsilon X} \\widehat{\\Pi}_{k_{T}}\\right\\|_{\\mathcal{S}_{1}}\\right\\} \\leq\\left\\|\\widehat{C}_{\\varepsilon X}\\right\\|_{\\infty}\\left\\|\\widehat{\\Pi}_{k_{T}}\\right\\|_{\\mathcal{S}_{1}}=O_{p}\\left(T^{-\\beta}\\right) k_{T}=o_{p}(1)\n$$\n\nand also\n\n$$\n\\begin{aligned}\n\\left\\|\\widehat{C}_{\\varepsilon X} \\widehat{C}_{X X, k_{T}}^{-1} \\widehat{C}_{\\varepsilon X}^{*}\\right\\|_{\\mathcal{S}_{1}} & =\\sum_{\\ell=1}^{\\infty} \\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j}^{-1}\\left\\langle\\widehat{C}_{\\varepsilon X} \\hat{v}_{j}, \\hat{v}_{\\ell}\\right\\rangle^{2} \\\\\n& \\leq \\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j}^{-1}\\left\\|\\widehat{C}_{\\varepsilon X} \\hat{v}_{j}\\right\\|^{2} \\leq\\left\\|\\widehat{C}_{\\varepsilon X}\\right\\|_{\\infty}^{2} \\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j}^{-1}=o_{p}(1)\n\\end{aligned}\n$$\n\nwhere the last equality follows from the fact that $\\left\\|\\widehat{C}_{\\varepsilon X}\\right\\|_{\\infty}^{2}=O_{p}\\left(T^{-2 \\beta}\\right)$ and $\\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j}^{-1} \\leq \\tau_{T}^{-1} k_{T}=o\\left(T^{2 \\beta}\\right)$ hold under Assumptions 1 and 2. As shown by (A.9)-(A.11), the second term in the RHS of (A.7) is $o_{p}(1)$. Combining this result with (A.8), we find that $\\left\\|\\widehat{A} \\widehat{C}_{X X, k_{T}} \\widehat{A}^{*}-A C_{X X, k_{T}} A^{*}\\right\\|_{\\mathcal{S}_{1}}=o_{p}(1)$ as desired.\n\nIt remains to verify (A.8) to complete the proof. From Lemma A.1(i), we know that\n\n$$\n\\left\\|A \\widehat{C}_{X X, k_{T}} A^{*}-A C_{X X, k_{T}} A^{*}\\right\\|_{\\mathcal{S}_{1}} \\leq\\|A\\|_{\\infty}^{2}\\left\\|\\widehat{C}_{X X, k_{T}}-C_{X X, k_{T}}\\right\\|_{\\mathcal{S}_{1}}\n$$\n\nand thus it suffices to show that the RHS of (A.12) is $o_{p}(1)$. To this end, we first note that\n\n$$\n\\left\\|\\widehat{C}_{X X, k_{T}}\\right\\|_{\\mathcal{S}_{1}}-\\left\\|C_{X X, k_{T}}\\right\\|_{\\mathcal{S}_{1}}=\\sum_{j=1}^{k_{T}}\\left(\\hat{\\lambda}_{j}-\\lambda_{j}\\right) \\leq k_{T}\\left\\|\\widehat{C}_{X X}-C_{X X}\\right\\|_{\\infty} \\rightarrow_{p} 0\n$$\n\nWe then obtain an upper bound of $\\left\\|\\widehat{C}_{X X, k_{T}}-C_{X X, k_{T}}\\right\\|_{\\infty}$ as follows:\n\n$$\n\\left\\|\\widehat{C}_{X X, k_{T}}-C_{X X, k_{T}}\\right\\|_{\\infty} \\leq\\left\\|\\widehat{\\Lambda}_{k_{T}}+\\sum_{j=1}^{k_{T}}\\left(\\hat{\\lambda}_{j}-\\lambda_{j}\\right) f_{j} \\otimes f_{j}\\right\\|_{\\infty} \\leq\\left\\|\\widehat{\\Lambda}_{k_{T}}\\right\\|_{\\infty}+O_{p}\\left(T^{-\\beta}\\right)\n$$\n\nwhere $\\widehat{\\Lambda}_{k_{T}}=\\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j} \\hat{f}_{j} \\otimes \\hat{f}_{j}-\\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j} f_{j} \\otimes f_{j}$. From similar algebra used in the proof of Lemma 3.1 of [24] and the fact that $\\|\\cdot\\|_{\\infty} \\leq\\|\\cdot\\|_{\\mathcal{S}_{2}}$, we find that\n\n$$\n\\left\\|\\widehat{\\Lambda}_{k_{T}}\\right\\|_{\\infty}^{2} \\leq \\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j}^{2} \\sum_{\\ell=k_{T}+1}^{\\infty}\\left\\langle\\hat{f}_{j}, f_{\\ell}\\right\\rangle^{2}+\\sum_{\\ell=1}^{k_{T}} \\hat{\\lambda}_{\\ell}^{2} \\sum_{j=k_{T}+1}^{\\infty}\\left\\langle\\hat{f}_{j}, f_{\\ell}\\right\\rangle^{2}+\\sum_{j=1}^{k_{T}} \\sum_{\\ell=1}^{k_{T}}\\left(\\hat{\\lambda}_{j}-\\hat{\\lambda}_{\\ell}\\right)^{2}\\left\\langle\\hat{f}_{j}, f_{\\ell}\\right\\rangle^{2}\n$$\n\nObserve that, for every $\\ell=1, \\ldots, k_{T}$,\n\n$$\n\\left(\\hat{\\lambda}_{\\ell}-\\hat{\\lambda}_{\\ell+1}\\right)^{2} \\sum_{j=k_{T}+1}^{\\infty}\\left\\langle\\hat{f}_{j}, f_{\\ell}\\right\\rangle^{2} \\leq \\sum_{j=k_{T}+1}^{\\infty}\\left(\\hat{\\lambda}_{\\ell}-\\hat{\\lambda}_{j}\\right)^{2}\\left\\langle\\hat{f}_{j}, f_{\\ell}\\right\\rangle^{2}\n$$\n\n$$\n\\begin{aligned}\n& =\\sum_{j=k_{T}+1}^{\\infty}\\left(\\left\\langle\\hat{\\lambda}_{\\ell} \\hat{f}_{j}, f_{\\ell}\\right\\rangle-\\left\\langle\\widehat{C}_{X X} \\hat{f}_{j}, f_{\\ell}\\right\\rangle\\right)^{2} \\\\\n& =\\sum_{j=k_{T}+1}^{\\infty}\\left(\\left\\langle\\hat{f}_{j},\\left(\\hat{\\lambda}_{\\ell}-\\lambda_{\\ell}\\right) f_{\\ell}\\right\\rangle+\\left\\langle\\hat{f}_{j},\\left(C_{X X}-\\widehat{C}_{X X}\\right) f_{\\ell}\\right\\rangle\\right)^{2} \\\\\n& \\leq\\left\\|\\left(\\hat{\\lambda}_{\\ell}-\\lambda_{\\ell}\\right) f_{\\ell}+\\left(C_{X X}-\\widehat{C}_{X X}\\right) f_{\\ell}\\right\\|^{2}\n\\end{aligned}\n$$\n\nSince $\\sup _{\\ell \\geq 1}\\left|\\hat{\\lambda}_{\\ell}-\\lambda_{\\ell}\\right| \\leq\\left\\|\\widehat{C}_{X X}-C_{X X}\\right\\|_{\\infty}=O_{p}\\left(T^{-\\beta}\\right)$, we find that the RHS of (A.16) is $O_{p}\\left(T^{-2 \\beta}\\right)$. Using the fact that $\\tau_{T}^{-1} \\geq\\left(\\hat{\\lambda}_{\\ell}-\\hat{\\lambda}_{\\ell+1}\\right)^{-1}$ for all $\\ell=1, \\ldots, k_{T}$, the following is deduced: for some $\\delta>0$,\n\n$$\n\\sum_{\\ell=1}^{k_{T}} \\hat{\\lambda}_{\\ell}^{2} \\sum_{j=k_{T}+1}^{\\infty}\\left\\langle\\hat{f}_{j}, f_{\\ell}\\right\\rangle^{2}=\\sum_{\\ell=1}^{k_{T}} \\frac{O_{p}\\left(T^{-2 \\beta}\\right) \\hat{\\lambda}_{\\ell}^{2}}{\\left(\\hat{\\lambda}_{\\ell}-\\hat{\\lambda}_{\\ell+1}\\right)^{2}} \\leq O_{p}\\left(\\tau_{T}^{-2} T^{-2 \\beta}\\right) \\sum_{\\ell=1}^{k_{T}} \\hat{\\lambda}_{\\ell}^{2}=O_{p}\\left(T^{-\\delta}\\right)\n$$\n\nwhere the last equality is deduced from the facts that $\\sum_{\\ell=1}^{k_{T}} \\hat{\\lambda}_{\\ell}^{2}=O_{p}(1)$ and $\\tau_{T}^{-1}=O_{p}\\left(T^{\\gamma_{1}}\\right)$ for some $\\gamma_{1} \\in(0, \\beta)$ under the employed conditions. From nearly identical arguments, we also find that\n\n$$\n\\sum_{j=1}^{k_{T}} \\hat{\\lambda}_{j}^{2} \\sum_{\\ell=k_{T}+1}^{\\infty}\\left\\langle\\hat{f}_{j}, f_{\\ell}\\right\\rangle^{2}=O_{p}\\left(T^{-\\delta}\\right)\n$$\n\nMoreover, from similar algebra used in (A.16) we find that\n\n$$\n\\begin{aligned}\n& \\sum_{j=1}^{k_{T}} \\sum_{\\ell=1}^{k_{T}}\\left(\\hat{\\lambda}_{j}-\\hat{\\lambda}_{\\ell}\\right)^{2}\\left\\langle\\hat{f}_{j}, f_{\\ell}\\right\\rangle^{2} \\\\\n& =\\sum_{j=1}^{k_{T}} \\sum_{\\ell=1}^{k_{T}}\\left(\\left\\langle\\hat{f}_{j}, \\widehat{C}_{X X} f_{\\ell}\\right\\rangle-\\left\\langle\\hat{f}_{j},\\left(\\hat{\\lambda}_{\\ell}-\\lambda_{\\ell}\\right) f_{\\ell}\\right\\rangle+\\left\\langle\\hat{f}_{j}, C_{X X} f_{\\ell}\\right\\rangle\\right)^{2} \\\\\n& \\leq \\sum_{j=1}^{k_{T}}\\left\\|\\left(\\hat{\\lambda}_{\\ell}-\\lambda_{\\ell}\\right) f_{\\ell}+\\left(\\widehat{C}_{X X}-C_{X X}\\right) f_{\\ell}\\right\\|^{2}=O_{p}\\left(k_{T} T^{-2 \\beta}\\right)\n\\end{aligned}\n$$\n\nFrom (A.14)-(A.19), we know that there is a divergent sequence $m_{T}$ such that $m_{T}\\left\\|\\widehat{C}_{X X, k_{T}}-C_{X X, k_{T}}\\right\\|_{\\infty} \\rightarrow_{p} 0$. Combining this result with (A.13) and Lemma A.2, we find that the RHS of (A.12) is $o_{p}(1)$ (and thus (A.8) holds).\n\nProof of Corollary 2.1. (a) directly follows from the facts that (i) $\\|(B C_{X X}^{1 / 2}-$ $\\left.C_{Y Y}^{1 / 2} R_{X Y}\\right) w_{j} \\|^{2}=0$ for all $j \\geq 1$ if and only if $\\left\\|B C_{X X}^{1 / 2}-C_{Y Y}^{1 / 2} R_{X Y}\\right\\|_{\\mathcal{S}_{2}}^{2}=0$ and (ii) the Hahn-Banach extension theorem (see e.g., Theorem 1.9.1 of [21]). This result in turn implies (b) due to the fact that $C_{X Y}=C_{Y Y}^{1 / 2} R_{X Y} C_{X X}^{1 / 2}$ as observed in Proposition 2.1.\nProof of Corollary 4.2. Note that (A.6) holds when $\\widehat{A}$ is replaced by $\\widetilde{A}$, and $\\left\\|\\widehat{C}_{Y Y}-C_{Y Y}\\right\\|_{\\mathcal{S}_{1}}=o_{p}(1)$ and $\\left\\|A C_{X X, k_{T}} A^{*}-A C_{X X} A^{*}\\right\\|_{\\mathcal{S}_{1}}=o_{p}(1)$ can be\n\nshown as in our proof of Theorem 3.1. We thus have\n\n$$\n\\|\\Sigma(\\widetilde{A})-C_{\\min }\\|_{\\mathcal{S}_{1}} \\leq o_{p}(1)+\\left\\|\\widehat{A} \\widehat{C}_{X X, k_{T}} \\widetilde{A}^{*}-A C_{X X, k_{T}} A^{*}\\right\\|_{\\mathcal{S}_{1}}\n$$\n\nand hence it suffices to show that $\\left\\|\\widehat{A} \\widehat{C}_{X X, k_{T}} \\widetilde{A}^{*}-A C_{X X, k_{T}} A^{*}\\right\\|_{\\mathcal{S}_{1}}=o_{p}(1)$. Note that\n\n$$\n\\widehat{A} \\widehat{C}_{X X, k_{T}} \\widetilde{A}^{*}=\\widehat{\\Pi}_{Y, \\ell_{T}}\\left(A \\widehat{C}_{X X, k_{T}} A^{*}+A \\widehat{\\Pi}_{X, k_{T}} \\widehat{C}_{{\\varepsilon X}}^{*}+\\widehat{C}_{\\varepsilon X} A^{*}+\\widehat{C}_{\\varepsilon X} \\widehat{C}_{X X, k_{T}}^{-1} \\widehat{C}_{\\varepsilon X}^{*}\\right) \\widehat{\\Pi}_{Y, \\ell_{T}}\n$$\n\nUsing similar arguments used in our proof of Theorem 3.1, Lemma A. 1 and the facts that $\\left\\|\\widehat{\\Pi}_{Y, \\ell_{T}}\\right\\|_{\\infty} \\leq 1$ for any arbitrary $\\ell_{T} \\geq 1$ and $\\| A C_{X X, k_{T}} A^{*}-$ $A C_{X X} A^{*}\\left\\|_{\\mathcal{S}_{1}}=o_{p}(1)\\right.$, we find that $\\left\\|\\widehat{\\Pi}_{Y, \\ell_{T}}\\left(A \\widehat{C}_{X X, k_{T}} A^{*}-A C_{X X, k_{T}} A^{*}\\right) \\widehat{\\Pi}_{Y, \\ell_{T}}\\right\\|_{\\mathcal{S}_{1}}=$ $o_{p}(1),\\left\\|\\widehat{\\Pi}_{Y, \\ell_{T}}\\left(A C_{X X, k_{T}} A^{*}-A C_{X X} A^{*}\\right) \\widehat{\\Pi}_{Y, \\ell_{T}}\\right\\|_{\\mathcal{S}_{1}}=o_{p}(1)$, and $\\left\\|\\widehat{\\Pi}_{Y, \\ell_{T}}\\left(A \\widehat{\\Pi}_{X, k_{T}} \\widehat{C}_{\\varepsilon X}^{*}+\\right.\\right.$ $\\left.\\left.\\widehat{C}_{\\varepsilon X} A^{*}+\\widehat{C}_{\\varepsilon X} \\widehat{C}_{X X, k_{T}}^{-1} \\widehat{C}_{\\varepsilon X}^{*}\\right) \\widehat{\\Pi}_{Y, \\ell_{T}}\\right\\|_{\\mathcal{S}_{1}}=o_{p}(1)$. Therefore,\n\n$$\n\\left\\|\\widehat{A} \\widehat{C}_{X X, k_{T}} \\widetilde{A}^{*}-\\widehat{\\Pi}_{Y, \\ell_{T}} A C_{X X} A^{*} \\widehat{\\Pi}_{Y, \\ell_{T}}\\right\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} 0\n$$\n\nIf $\\left\\|\\widehat{\\Pi}_{Y, \\ell_{T}} A C_{X X} A^{*} \\widehat{\\Pi}_{Y, \\ell_{T}}-A C_{X X} A^{*}\\right\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} 0$, the desired result is established. Let $\\Upsilon=C_{Y Y}^{1 / 2} R_{X Y}$ and $\\widehat{\\Upsilon}=\\widehat{\\Pi}_{Y, \\ell_{T}} C_{Y Y}^{1 / 2} R_{X Y}$. We then have $A C_{X X} A^{*}=\\Upsilon \\Upsilon^{*}$ (see Proposition 2.1) and $\\widehat{\\Pi}_{Y, \\ell_{T}} A C_{X X} A^{*} \\widehat{\\Pi}_{Y, \\ell_{T}}=\\widehat{\\Upsilon} \\widehat{\\Upsilon}^{*}$. Observe that\n\n$$\n\\begin{aligned}\n\\left\\|\\widehat{\\Upsilon} \\widehat{\\Upsilon}^{*}-\\Upsilon \\Upsilon^{*}\\right\\|_{\\infty}=\\left\\|\\widehat{\\Upsilon}\\left(\\widehat{\\Upsilon}^{*}-\\Upsilon^{*}\\right)+(\\widehat{\\Upsilon}-\\Upsilon) \\Upsilon^{*}\\right\\|_{\\infty} & \\leq O_{p}(1)\\|\\widehat{\\Upsilon}-\\Upsilon\\|_{\\infty} \\\\\n& \\leq O_{p}(1)\\left\\|\\widehat{\\Pi}_{Y, \\ell_{T}} C_{Y Y}^{1 / 2}-C_{Y Y}^{1 / 2}\\right\\|_{\\infty}\n\\end{aligned}\n$$\n\nThis implies that $\\left\\|\\widehat{\\Upsilon} \\widehat{\\Upsilon}^{*}-\\Upsilon \\Upsilon^{*}\\right\\|_{\\infty} \\rightarrow_{p} 0$ if there exists a divergent sequence $m_{T}$ such that $m_{T}\\left\\|\\widehat{\\Pi}_{Y, \\ell_{T}} C_{Y Y}^{1 / 2}-C_{Y Y}^{1 / 2}\\right\\|_{\\infty} \\rightarrow_{p} 0$ and $m_{T} \\leq \\ell_{T}$ for large $T$. Moreover, we find the following from the fact that $\\Upsilon \\Upsilon^{*}$ is nonnegative self-adjoint and $\\left\\{\\hat{w}_{j}\\right\\}_{j=1}^{\\infty}$ is an orthonormal basis of $\\mathcal{H}$ :\n\n$$\n\\left\\|\\Upsilon \\Upsilon^{*}\\right\\|_{\\mathcal{S}_{1}}-\\|\\widehat{\\Upsilon} \\widehat{\\Upsilon}\\|_{\\mathcal{S}_{1}}=\\sum_{j=\\ell_{T}+1}^{\\infty}\\left\\langle\\Upsilon \\Upsilon^{*} \\hat{w}_{j}, \\hat{w}_{j}\\right\\rangle \\leq \\sum_{j=m_{T}+1}^{\\infty}\\left\\langle\\Upsilon \\Upsilon^{*} \\hat{w}_{j}, \\hat{w}_{j}\\right\\rangle\n$$\n\nwhich is $o_{p}(1)$ since $\\Upsilon \\Upsilon^{*}=C_{Y Y}^{1 / 2} R_{X Y} R_{X Y}^{*} C_{Y Y}^{1 / 2}$ is a Schatten 1-class (Lemma A.1(i)). We thus deduce from Lemma A. 2 that $\\left\\|\\widehat{\\Upsilon} \\widehat{\\Upsilon}^{*}-\\Upsilon \\Upsilon^{*}\\right\\|_{\\mathcal{S}_{1}} \\rightarrow_{p} 0$ as desired.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# Appendix B: Additional simulation results for alternative estimators \n\nIn this section, we experiment with an alternative estimator $\\widetilde{A}$ which is introduced in Section 4.4. We replicate the same simulation experiments conducted in Section 5 using the alternative estimator $\\widetilde{A}$ introduced in Section 4.4. We construct $\\widehat{\\Pi}_{Y, \\ell_{T}}$ using the eigenvectors $\\left\\{\\hat{u}_{j}\\right\\}_{j \\geq 1}$ of $\\widehat{C}_{Y Y}$ with $\\ell_{T}=\\left\\lfloor T^{1 / 2}\\right\\rfloor$. Overall, we found that the computed excess MSPEs tend to be similar to those obtained\n\nwith $\\widehat{A}$, providing supporting evidence for our finding in Corollary 4.2. The simulation results are reported in Table 2. We also experimented with $\\widehat{\\Pi}_{Y, \\ell_{T}}$ constructed from the eigenvectors $\\left\\{\\hat{v}_{j}\\right\\}_{j \\geq 1}$ of $\\widehat{C}_{X X}$, and the results are reported in Table 3. Even if the empirical MSPEs tend to be larger than in the previous case, we obtained overall similar simulation results.\n\nTable 2\nExcess MSPE of the alternative predictor, $\\widehat{\\Pi}_{Y, \\ell_{T}}=\\sum_{j=1}^{\\ell_{T}} \\hat{u}_{j} \\otimes \\hat{u}_{j}$\n(a) Case BB\n\n![table_3](table_3)\n\n(b) Case CBM\n\n![table_4](table_4)\n\n(c) Case BM\n\n![table_5](table_5)\n\nNotes: The excess MSPEs are calculated as in Table 1.\n\nTable 3\nExcess MSPE of the alternative predictor, $\\widehat{\\Pi}_{Y, \\hat{v}_{T}}=\\sum_{j=1}^{f_{Y}} \\hat{v}_{j} \\otimes \\hat{v}_{j}$\n(a) Case BB\n\n![table_6](table_6)\n\n(b) Case CBM\n\n![table_7](table_7)\n\n(c) Case BM\n\n![table_8](table_8)\n\nNotes: The excess MSPEs are calculated as in Table 1.",
      "tables": {
        "table_3": "|  | $\\gamma=0.475$ |  |  |  |  | $\\gamma=0.45$ |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $T$ | 50 | 100 | 200 | 400 | 800 | 50 | 100 | 200 | 400 | 800 |\n| M1 | 0.043 | 0.035 | 0.023 | 0.017 | 0.011 | 0.071 | 0.049 | 0.029 | 0.021 | 0.013 |\n| M2 | 0.045 | 0.035 | 0.023 | 0.017 | 0.012 | 0.071 | 0.051 | 0.030 | 0.021 | 0.014 |\n| M3 | 0.056 | 0.045 | 0.030 | 0.022 | 0.015 | 0.091 | 0.063 | 0.038 | 0.027 | 0.017 |\n| M4 | 0.057 | 0.046 | 0.031 | 0.023 | 0.015 | 0.092 | 0.065 | 0.039 | 0.028 | 0.018 |",
        "table_4": "|  | $\\gamma=0.475$ |  |  |  |  | $\\gamma=0.45$ |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $T$ | 50 | 100 | 200 | 400 | 800 | 50 | 100 | 200 | 400 | 800 |\n| M1 | 0.043 | 0.035 | 0.023 | 0.017 | 0.011 | 0.071 | 0.050 | 0.029 | 0.021 | 0.013 |\n| M2 | 0.044 | 0.036 | 0.023 | 0.017 | 0.012 | 0.072 | 0.051 | 0.030 | 0.021 | 0.014 |\n| M3 | 0.057 | 0.046 | 0.030 | 0.022 | 0.015 | 0.093 | 0.065 | 0.039 | 0.027 | 0.018 |\n| M4 | 0.057 | 0.047 | 0.031 | 0.023 | 0.015 | 0.094 | 0.066 | 0.039 | 0.028 | 0.018 |",
        "table_5": "|  | $\\gamma=0.475$ |  |  |  |  | $\\gamma=0.45$ |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $T$ | 50 | 100 | 200 | 400 | 800 | 50 | 100 | 200 | 400 | 800 |\n| M1 | 0.012 | 0.010 | 0.006 | 0.004 | 0.004 | 0.026 | 0.017 | 0.009 | 0.006 | 0.004 |\n| M2 | 0.012 | 0.010 | 0.006 | 0.004 | 0.004 | 0.027 | 0.018 | 0.009 | 0.006 | 0.004 |\n| M3 | 0.018 | 0.014 | 0.009 | 0.006 | 0.005 | 0.037 | 0.024 | 0.013 | 0.008 | 0.006 |\n| M4 | 0.018 | 0.015 | 0.009 | 0.006 | 0.005 | 0.038 | 0.025 | 0.013 | 0.008 | 0.006 |",
        "table_6": "|  | $\\gamma=0.475$ |  |  |  |  | $\\gamma=0.45$ |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $T$ | 50 | 100 | 200 | 400 | 800 | 50 | 100 | 200 | 400 | 800 |\n| M1 | 0.043 | 0.035 | 0.023 | 0.017 | 0.011 | 0.071 | 0.049 | 0.029 | 0.021 | 0.013 |\n| M2 | 0.066 | 0.049 | 0.032 | 0.023 | 0.015 | 0.089 | 0.063 | 0.039 | 0.026 | 0.017 |\n| M3 | 0.056 | 0.045 | 0.030 | 0.022 | 0.015 | 0.091 | 0.063 | 0.038 | 0.027 | 0.017 |\n| M4 | 0.078 | 0.057 | 0.036 | 0.025 | 0.017 | 0.109 | 0.075 | 0.044 | 0.030 | 0.019 |",
        "table_7": "|  | $\\gamma=0.475$ |  |  |  |  | $\\gamma=0.45$ |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $T$ | 50 | 100 | 200 | 400 | 800 | 50 | 100 | 200 | 400 | 800 |\n| M1 | 0.044 | 0.035 | 0.023 | 0.017 | 0.011 | 0.071 | 0.050 | 0.029 | 0.021 | 0.013 |\n| M2 | 0.075 | 0.055 | 0.036 | 0.025 | 0.016 | 0.099 | 0.069 | 0.043 | 0.029 | 0.018 |\n| M3 | 0.057 | 0.046 | 0.030 | 0.022 | 0.015 | 0.093 | 0.065 | 0.039 | 0.027 | 0.018 |\n| M4 | 0.096 | 0.071 | 0.046 | 0.032 | 0.021 | 0.127 | 0.089 | 0.055 | 0.037 | 0.023 |",
        "table_8": "|  | $\\gamma=0.475$ |  |  |  |  | $\\gamma=0.45$ |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n| $T$ | 50 | 100 | 200 | 400 | 800 | 50 | 100 | 200 | 400 | 800 |\n| M1 | 0.012 | 0.010 | 0.006 | 0.004 | 0.004 | 0.026 | 0.017 | 0.009 | 0.006 | 0.004 |\n| M2 | 0.032 | 0.022 | 0.014 | 0.009 | 0.007 | 0.046 | 0.030 | 0.017 | 0.011 | 0.008 |\n| M3 | 0.018 | 0.015 | 0.009 | 0.006 | 0.005 | 0.037 | 0.024 | 0.013 | 0.008 | 0.006 |\n| M4 | 0.040 | 0.028 | 0.017 | 0.011 | 0.008 | 0.058 | 0.037 | 0.021 | 0.013 | 0.009 |"
      },
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# References \n\n[1] Aue, A. and Klepsch, J. (2017). Estimating functional time series by moving average model fitting, arxiv:1701.00770 [stat.ME].\n[2] Aue, A., Norinho, D. D., and H\u00f6rmann, S. (2015). On the prediction of stationary functional time series. Journal of the American Statistical Association, 110(509):378-392.\n[3] Babii, A. and Florens, J.-P. (2025). Is completeness necessary? estimation in nonidentified linear models. Econometric Theory, page 1-38.\n[4] Baker, C. R. (1973). Joint measures and cross-covariance operators. Transactions of the American Mathematical Society, 186:273-289.\n[5] Beare, B. K., Seo, J., and Seo, W.-K. (2017). Cointegrated linear processes in Hilbert space. Journal of Time Series Analysis, 38(6):1010-1027.\n[6] Benatia, D., Carrasco, M., and Florens, J.-P. (2017). Functional linear regression with functional response. Journal of Econometrics, 201(2):269-291.\n[7] Bosq, D. (2000). Linear Processes in Function Spaces. Springer-Verlag New York.\n[8] Bosq, D. (2007). General linear processes in Hilbert spaces and prediction. Journal of Statistical Planning and Inference, 137(3):879-894.\n[9] Bosq, D. (2014). Computing the best linear predictor in a Hilbert space. Applications to general ARMAH processes. Journal of Multivariate Analysis, $124: 436-450$.\n[10] Chang, Y., Choi, Y., Kim, S., and Park, J. (2021). Stock market return predictability dormant in option panels. Mimeo, Department of Economics, Indiana Univeristy.\n[11] Conway, J. B. (1994). A Course in Functional Analysis. Springer.\n[12] Hall, P. and Horowitz, J. L. (2007). Methodology and convergence rates for functional linear regression. Annals of Statistics, 35(1):70 - 91.\n[13] Imaizumi, M. and Kato, K. (2018). PCA-based estimation for functional linear regression with functional responses. Journal of Multivariate Analysis, $163: 15-36$.\n[14] Jaimez, R. G. and Bonnet, M. J. V. (1987). On the Karhunen-Loeve expansion for transformed processes. Trabajos de Estadistica, 2:81-90.\n[15] Kalman, D. and McKinzie, M. (2012). Another way to sum a series: generating functions, euler, and the dilog function. The American Mathematical Monthly, 119(1):42-51.\n[16] Karol, A., Nazarov, A., and Nikitin, Y. (2008). Small ball probabilities for Gaussian random fields and tensor products of compact operators. Transactions of the American Mathematical Society, 360(3):1443-1474.\n[17] Klepsch, J., Kl\u00fcppelberg, C., and Wei, T. (2017). Prediction of functional ARMA processes with an application to traffic data. Econometrics and Statistics, 1:128-149.\n[18] Klepsch, J. and Kl\u00fcppelberg, C. (2017). An innovations algorithm for the prediction of functional linear processes. Journal of Multivariate Analysis, $155: 252-271$.\n[19] Kubrusly, C. (1985). On convergence of nuclear and correlation operators\n\nin Hilbert space. Technical report, Laboratorio de Computacao Cientifica.\n[20] Luo, R. and Qi, X. (2017). Function-on-function linear regression by signal compression. Journal of the American Statistical Association, 112(518):690705 .\n[21] Megginson, R. E. (2012). Introduction to Banach Space Theory. Springer New York.\n[22] Mollenhauer, M., M\u00fccke, N., and Sullivan, T. J. (2023). Learning linear operators: Infinite-dimensional regression as a well-behaved non-compact inverse problem.\n[23] Park, J. Y. and Qian, J. (2012). Functional regression of continuous state distributions. Journal of Econometrics, 167(2):397-412.\n[24] Reimherr, M. (2015). Functional regression with repeated eigenvalues. Statistics \\& Probability Letters, 107:62-70.\n[25] Seo, W.-K. (2023). Cointegration and representation of cointegrated autoregressive processes in Banch spaces. Econometric Theory, 39(4):737-788.\n[26] Seong, D. and Seo, W.-K. (2021). Functional instrumental variable regression with an application to estimating the impact of immigration on native wages, arXiv:2110.12722 [econ.EM].\n[27] Yao, F., M\u00fcller, H.-G., and Wang, J.-L. (2005). Functional linear regression analysis for longitudinal data. The Annals of Statistics, 33(6):2873-2903.\n[28] Zhang, X., Chiou, J.-M., and Ma, Y. (2018). Functional prediction through averaging estimated functional linear regression models. Biometrika, $105(4): 945-962$.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2401.06326v4",
  "authors": [
    "Won-Ki Seo"
  ],
  "categories": [
    "math.ST",
    "stat.TH"
  ],
  "abstract": "We study the optimal linear prediction of a random function that takes values\nin an infinite dimensional Hilbert space. We begin by characterizing the mean\nsquare prediction error (MSPE) associated with a linear predictor and\ndiscussing the minimal achievable MSPE. This analysis reveals that, in general,\nthere are multiple non-unique linear predictors that minimize the MSPE, and\neven if a unique solution exists, consistently estimating it from finite\nsamples is generally impossible. Nevertheless, we can define asymptotically\noptimal linear operators whose empirical MSPEs approach the minimal achievable\nlevel as the sample size increases. We show that, interestingly, standard\npost-dimension reduction estimators, which have been widely used in the\nliterature, attain such asymptotic optimality under minimal conditions.",
  "updated": "2025-03-04T01:55:31Z",
  "published": "2024-01-12T02:34:32Z"
}