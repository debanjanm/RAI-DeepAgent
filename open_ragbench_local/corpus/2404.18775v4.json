{"title": "Resource-rational reinforcement learning and sensorimotor causal states,\n  and resource-rational maximiners", "sections": [{"section_id": 0, "text": "#### Abstract\n\nWe propose a new computational-level objective function for theoretical biology and theoretical neuroscience that combines: reinforcement learning, the study of learning with feedback via rewards; rate-distortion theory, a branch of information theory that deals with compressing signals to retain relevant information; and computational mechanics, the study of minimal sufficient statistics of prediction also known as causal states. We highlight why this proposal is likely only an approximation, but is likely to be an interesting one, and propose a new algorithm for evaluating it to obtain the newly-coined \"reward-rate manifold\". The performance of real and artificial agents in partially observable environments can be newly benchmarked using these reward-rate manifolds. To that end, we calculate an example reward-rate manifold utilizing new equations reminiscent of the BlahutArimoto algorithm and propose a general algorithm for computing reward-rate manifolds. Finally, we describe experiments that can probe whether or not biological organisms are resource-rational reinforcement learners, using as an example maximin strategies, as bacteria have been shown to be approximate maximiners- doing their best in the worst-case environment, regardless of what is actually happening. This proposal explains why \"good-enough\" for organisms might actually be near-optimal, if viewed correctly.", "tables": {}, "images": {}}, {"section_id": 1, "text": "## I. INTRODUCTION\n\nAccording to Marr, understanding biological organisms entails uncovering three levels: the computational, the algorithmic, and the mechanistic [1, 2]. At the computational level, we ask what organisms are trying to do. What objective function might they be using? At the algorithmic level, we ask what algorithm they are using to accomplish that objective. And at the mechanistic level, we ask how they are implementing that algorithm in their wetware. None of these levels have been completely understood in theoretical neuroscience or theoretical biology, despite major advances such as the Hodgkin-Huxley model that describes how neurons behave using electrical engineering ideas and the efficient coding hypothesis [3] that describes how the brain has adapted to naturalistic stimuli.\n\nIn this manuscript, we claim that resource-rational decision making is a plausible first attempt at the computational level [4], giving an optimization approach to biology. This research program goes by the name of computational rationality [5], rational inattention [6, 7], and many other names. The basic idea behind it is that organisms endeavour to solve tasks as well as possible, but are limited in their ability to solve tasks by various resources. These resources can be time limitations, memory limitations, material limitations, or other limitations.\n\nIn this outlook, organisms that have been merely described as \"good enough\" at a task in the past are instead rationally inattending to certain bits of information, such that they are doing the best that they can with limited resources and essentially satisficing near-optimally. In other words, this research program asserts that given their resources, a wide variety of organisms are doing the best that they can at gathering reward.\n\nThere is much debate over how to implement resourcerational decision making quantitatively, but informationtheoretic codings of resources [8] and reinforcement learning-based measures of the quality of decision making [9] might be the key to understanding the full sensorimotor loop. Already, reinforcement learning has been famously used to describe dopaminergic signals [10], although there is much recent debate over whether or not that mechanistic level description is appropriate [11]. On the other hand, using information-theoretic quantities as perceptual costs has allowed researchers to explain a number of empirical findings in a wide variety of areas in the last two decades, including various aspects of macroeconomic behavior [6, 7], Shepard's universal law of generalization [12], the fuzziness of color naming systems [13], sub-optimal prediction in sequence learning [14], and a number of empirical findings on neural coding and working memory [15]. And, while not done on humans, recent work has shown that salamander retinal ganglion cells [16] and cultured cortical neurons from rats [17] both predict stimuli efficiently in an informationtheoretic sense but do not always predict well in an absolute sense. Information-theoretic costs can be justified both using material constraints [8] and nonequilibrium thermodynamics $[18,19]$.\n\nThere have been attempts to combine informationtheoretic resource constraints and reinforcement learning objectives in Refs. [20-22], but in this manuscript, we will argue that these attempts require combination to achieve the correct objective. We will give a new Blahut-Arimoto-like algorithm for calculating what we call the \"reward-rate manifold\", which describes how well an organism (real or artificial) can attain reward under the information-theoretic resource constraints. In order to provide an algorithm, we will prove that the sensorimotor\n\ncausal states of Ref. [20] can replace semi-infinite histories of observations and actions, essentially making it possible to calculate an infinite object with finite resources. As a corollary, it becomes clear that in the limit of resource constraints being inessential to functioning, sensorimotor causal states are stored by the organism. We then use a Gaussian Infomation Bottleneck-like [23] take on the equations underlying this algorithm to compute the reward-rate manifold for a simple example, showing that indeed, this computational-level objective function can be tested empirically with enough compute resources. We propose to use these kinds of analyses for more complicated systems, both computationally and experimentally, therefore testing if organisms are resource-rational decision makers, and if so, what kind.\n\nWe begin by describing the new proposed objective function and continue by providing an algorithm to efficiently calculate the newly-described reward-rate manifold and an example thereafter. We move to discussion of what else organisms might be doing- maximizing their reward in the worst-case scenario. We conclude by describing what might be done in theoretical biology and even in machine learning (for evolved and engineered organisms) with this contribution.\n\nKey to this research program is the ambitious idea that actually, organism brain and behavior all largely roughly obey the same objective function: Changes between organisms come from changes in their environments and their allowed level of resources. The exception to this comes lower-level organisms, like bacteria, that fail to have a theory of mind that would allow them to exploit the environment more intelligently and dynamically.", "tables": {}, "images": {}}, {"section_id": 2, "text": "## II. A NEW COMPUTATIONAL-LEVEL OBJECTIVE FOR THEORETICAL BIOLOGY\n\nWe start by discussing proposals for a computationallevel objective for theoretical biology in Sec. II A and move to introducing my own in Sec. II B. The environment under consideration is known in reinforcement learning [9] as a Partially Observable Markov Decision Process (POMDP), in which there is an underlying Markov state $w$ describing the environment, actions $a$ that describe what the agent can do, noisy and partial observations $o$ of the underlying world state $w$ that describe what the agent sees, a discount factor $\\gamma$ that describes how agents treat future rewards, and a reward function $r(w, a)$ that describes how much \"reward\" an agent receives when the world is in state $w$ and the agent takes action $a$. These rewards can take the form of food, shelter, sleep, and so on, and are left unspecified for the purpose of this paper. In an experiment, one might imagine giving rats sugar or humans money. Mathematically, we specify $p\\left(w_{t+1} \\mid w_{t}, a_{t}\\right)$ to be the way in which the organism's actions affect how the world evolves, and we specify $p(o \\mid w)$ to be the way in which the organism receives noisy and partial observations.", "tables": {}, "images": {}}, {"section_id": 3, "text": "## A. Attempts So Far\n\nThe first instance of such an objective function incorporating sensors and actuators is perhaps a paper by Still [20]. She imagined that an organism sees observations $o_{t}$ at time $t$, converts past actions and observations to sensory state $s_{t}$, and takes action $a_{t}$ right after based also on that history. The history of observations and actions is labeled $h_{t}$ and the future of observations is labeled $z_{t}$. She imagines that $h_{t}$ is used to inform both $s_{t}$ and $a_{t}$ separately. Still suggests that one should try to maximize $I[s, a ; z]-\\lambda I[s ; h]-\\mu I[a ; h]$ where $\\lambda, \\mu$ are Lagrange multipliers and time indices have been dropped for easier-to-read notation. In this objective, $I[s, a ; z]$ is the mutual information between the sensory state and the action relative to the future of observations; $I[s ; h]$ is the mutual information between sensory state and history; and $I[a ; h]$ is the mutual information between action and history. In Ref. [20], Still found optimal sensors to be sensorimotor causal states (described in Sec. III) in the limit that $\\lambda \\rightarrow 0$ and also identified optimal action policies in the limit that $\\mu \\rightarrow 0$.\n\nThe first term in this objective is interesting, but maximizing this term usually leads to large periodic loops when $\\lambda, \\mu$ are near enough to 0 . (Large periodic loops have a high mutual information between past and future.) That is unfortunately a limit of interest for higher-level organisms that can pick up the aforementioned sensorimotor causal states. Although some work [24] claims that these high predictive information processes correspond to processes that learn underlying parameters of the environment model, that is only true in a nonergodic case [25]. It may be possible in certain environments to see something more complex [26]. For lower-level organisms, the limit $\\lambda, \\mu \\rightarrow \\infty$ is of greater interest, but that leads to sensory states and actions that depend not at all on the history and are instead biased coin flips, by simulations not shown here. A quick theoretical argument suggests that should be the case $-I[s ; h], I[a ; h]$ can both be set to 0 if $s, a$ have no dependence on $h$, and thus the objective function is maximized by doing so.\n\nThe next instance of such an instantiation that is information-theoretic comes identically from Ref. [21] and Ref. [22]. Here, the information-theoretic term $I[s, a ; z]$ is replaced by the usual reinforcement learning term $V_{\\pi}$, the sum total of discounted rewards. Rewards depend on the underlying Markov state of the environment $w_{t}$, so that $V_{\\pi}=\\sum_{t} \\gamma^{t} r\\left(w_{t}, a_{t}\\right)$ where $\\gamma$ is a discount factor, $r$ the usual reward function [9], and $w_{t}$ and $a_{t}$ the world state and actions at time $t$. It is straightforward to generalize to continuous-time by introducing an integral. There is no cost for complicated sensory states $s$, unlike in Ref. [20]. There is only a cost on transmitting information from sensory state to actions $I[s ; a]$, the mutual information between sensory state $s$ and action $a$. As a result, the objective function reads $V_{\\pi}-\\beta I[s ; a]$. Note that here, $s$ is used to inform the action $a$ rather than the entire history $h$ being used to inform the action.\n\nThis rings more true to neuroscience, as we describe in the next section.\n\nThe work from Ref. [27] looks similar in spirit to the second of these two instantiations, but there, the rate constraint is included for a completely different reason. It encourages exploration in complex environments. The work in Ref. [28] for language also looks similar, but there, two terms exist that encourage understanding the environment- one that resembles an information bottleneck term that maximizes mutual information with the relevant variable, and one that maximizes utility, while one term remains to penalize understanding the environment.\n\nOur work is maybe closest to Ref. [29, 30] which includes both the rate constraint on communicating sensations to actions and also a sensory variable that is recursively updated as in the Recursive Information Bottleneck (RIB) [31]. The RIB constraint allows for a large value of resources when the brain is large and does not even take in any information about the stimulus, and hence seems less of a notion of memory than the information-theoretic quantity we use here. However, in their eventual algorithm, sensor and world states are collapsed into one, which we avoid as the sensory system is a bottleneck for information about the world.", "tables": {}, "images": {}}, {"section_id": 4, "text": "## B. The New Objective\n\nWe must carefully decide which terms to include in the final objective function describing an organism trying to navigate a sensorimotor feedback loop. Altogether, we would like an objective function that naturally balances exploration and exploitation, meaning that an organism should explore its environment naturally before exploiting the information it has obtained to survive; and we would like an objective function that includes as many resource constraints as possible. Exploitation naturally requires exploration, since to exploit means that one has sampled the environment enough to know which action is best, as can be seen when considering a simple multiarmed bandit. Potentially an objective function could start with more emphasis on exploration to encourage better exploitation later. A simple combination of the objective functions that exist so far as mentioned in Sec. II A yields:\n\n$$\n\\mathcal{L}=V_{\\pi}-\\beta I[s ; a]-\\lambda I[h ; s]\n$$\n\nwhere $\\beta, \\lambda$ are constants. This is really the unconstrained version of a constrained objective function:\n\n$$\nR\\left(M I_{s, a}, M I_{h, s}\\right)=\\max _{I[s ; a] \\leq M I_{s, a}, I[h ; s] \\leq M I_{h, s}} V_{\\pi}\n$$\n\nso that $\\beta, \\lambda$ are Lagrange multipliers and $M I_{s, a}$ and $M I_{h, s}$ are adjustable constants.\n\nIt is possible that this unconstrained objective function is itself more fundamental than the constrained version of the objective function, with reward being offset\nby costs. For example, the reward function is essentially equivalent to energy-gathering, while the two resource constraints linearly combined relate to energy expenditure. If so, the Lagrange multipliers may attain physical meaning that translates rates into energies, e.g. temperatures multiplied by the Boltzmann constant.\n\nWith the constrained objective function, we define the reward-rate manifold, in which $M I_{h, s}$ is on the $x$-axis, $M I_{s, a}$ is on the $y$-axis, and $V_{\\pi}$ on the $z$-axis. The manifold separates achievable combinations of informationtheoretic rates $I[h ; s], I[s ; a]$ and rewards $V_{\\pi}$ and unachievable combinations, as in rate-distortion theory [8] and predictive rate-distortion theory [32]. In other words, the reward-rate manifold defines a Pareto front.\n\nFirst, we discuss the term that allows the organism to accumulate reward. The term $V_{\\pi}$ naturally implies that we must both explore and exploit: to reap rewards, one must survey all available options (within reason) and choose the best one rather than merely sticking with the first good option that comes around. However, much effort has been spent in reinforcement learning trying to add additional terms or alter action policies so that a better balance of exploration and exploitation is achieved, e.g. as in Ref. [33].\n\nNext, we discuss the information-theoretic resource term that suggests the organism should aim for a simpler actuator. We must convey the sensory state $s$ to find the action policy $a$ using the conditional probability $\\pi(a \\mid s)$ that signifies the action policy [9]- the actuator $a$ does not have direct access to histories $h$ - and so $I[s ; a]$ is the appropriate term, as identified by Refs. [21, 22].\n\nFinally, we discuss the information-theoretic resource term that suggests the organism should aim for a simpler sensory layer [16]. If we think about the human brain, observations from the retina $o$ must combine with efference copies $a$ at the primary visual cortex V1 to give us a sensory state $s$ that can be used to determine actions. Mathematically, there is some input-dependent dynamical system that takes in information from the efference copy and the observations and turns it into something that is not quite the history $h$ written down by Still, but has information going back to the beginning of when the organism has opened its eyes. Hence we are perhaps somewhat justified in replacing this variable by $h$. This information must be communicated to the next layer in the brain, justifying $I[h ; s]$ as the next resource constraint.\n\nEvolution is not likely to directly work on this objective function, but might be subject to resource constraints that force it to essentially maximize this objective function. Essentially, the resource constraints that evolution operates on might look more like material constraints [34] or energy constraints [18, 19], both which lead to mutual informations as the natural stand-in using results from information theory or nonequilibrium thermodynamics. See App. A.", "tables": {}, "images": {}}, {"section_id": 5, "text": "## III. AN ALGORITHM TO CALCULATE USING SENSORIMOTOR CAUSAL STATES\n\nSensorimotor causal states as defined in Ref. [20] are usually also belief states of the POMDP [35]. Belief states are the probability distribution over the underlying Markov state of the environment (or more technically, of the POMDP) $w$ given the history $h$, and one uses these to \"solve\" the POMDP- to determine one's action policy $[35,36]$.\n\nThese sensorimotor causal states come from a coarsegraining relationship, as in Ref. [20, 37]. Take histories $h$ and consider two histories $h, h^{\\prime}$ equivalent if $P(w \\mid h)=P\\left(w \\mid h^{\\prime}\\right)$. Note the difference from Ref. [20]we have replaced future observations with the underlying Markov state of the POMDP. The best guide to the future of the observations is the underlying Markov state of the environment $w$. This is unobtainable directly, so in any real algorithm to ascertain sensorimotor causal states, one might use the future of observations instead. Regardless, the clusters of histories are labeled $\\sigma$, sensorimotor causal states, and the sensorimotor causal state to which history $h$ belongs is given by $\\epsilon^{+}(h)$. We define sensorimotor causal states in this modified way so that the proof of the main theorem in this paper is clear; as an added benefit, these modified sensorimotor causal states are now exactly the belief states.\n\nWith this definition in hand, we introduce our main theorem and proof that simplifies calculation of the reward-rate manifold.\n\nTheorem 1 The objective function from the previous section was $V_{\\pi}-\\beta I[s ; a]-\\lambda I[h ; s]$. We can replace histories $h$ with sensorimotor causal states $\\sigma$ if we wish to find statistics of good sensors [8] or to calculate the rewardrate manifold.\n\nTo prove this, note that there is no change to $V_{\\pi}$ or $I[s ; a]$ if sensory states $p(s \\mid h)$ are recoded as $p(s \\mid \\sigma=$ $\\epsilon^{+}(h)$ ), similar to what is true in Ref. [32]. And, as in Ref. [32], $I[s ; h]=I[s ; \\sigma]+I[s ; h \\mid \\sigma]$ only decreases with this recoding to $I[s ; \\sigma]$ since $I[s ; h \\mid \\sigma] \\geq 0$. The objective function therefore benefits from this recoding. As a result, as expected, it is optimal to pick up sensorimotor causal states using the recurrent neural network that governs the sensory layer in biological organisms.\n\nThe new insight into sensory states is that they should\npick up nothing else, however lossy; and that the objective function can be rewritten with histories $h$ replaced with sensorimotor causal states $\\sigma$.\n\nImportantly, the obtained sensor $p(s \\mid h)$ and actuator $\\pi(a \\mid s)$ from maximizing this objective might not be good sensors or actuators themselves by the original material constraints [8]. This is a common misconception for practitioners of the information bottleneck method, as this point is not stressed by the seminal work in Ref. [38]. (The information bottleneck method is a ratedistortion method with an informational distortion.) The soft clusters obtained by the Blahut-Arimoto algorithm and generalized Blahut-Arimoto algorithm are often bad lossy compressors due to the difference between $H[a]$ and $I[a ; s]$, where $I[a ; s]$ is typically considered to be the resource constraint. But it is sadly the case that $H[a]$ and not $I[s ; a]$ mirrors the expected length of the coding of the action sequence, and $H[a]$ is almost always larger, and maybe much larger. Also, as a single-symbol compression scheme, the codes revealed by these iterative algorithms are not usually optimal, except for special cases of the distortion measure [39]. This is true even when $H[a]$ replaces $I[a ; s]$ in the objective function [40]. If several symbols are used, as is more typical for good lossy compression schemes, then the statistics of a good lossy compression scheme will mirror the soft clusters obtained by the IB method [8].\n\nWe now specialize to the case of no discounting $\\gamma=1$, in which case $V_{\\pi}$ turns into a sum of rewards, for ease, with anticipated extensions later. For a POMDP, one can define a reward function on belief states $\\sigma$ and actions $a$ from the underlying reward function on underlying Markov states of the environment $w$ and actions $a$ [35], but we avoid this step. (It is not necessary for calculating the reward-rate manifold for the experiments we plan to do in the future.) Under a stationarity condition, $V_{\\pi}$ turns into $T\\langle r(w, a)\\rangle_{p(w, a)}$, where $T$ is the total number of time steps in the organism's life, and $\\langle\\cdot\\rangle$ is an expectation value, replacing what is often labeled as $E[\\cdot]$. We can ignore the additional factor of $T$ by rescaling $\\beta, \\lambda$.\n\nIn this case, from Appendix B, we can calculate the reward-rate manifold by using the iterative algorithm which updates $\\pi_{n}(a \\mid s)$ and $p_{n}(s \\mid \\sigma)$ as in the usual information bottleneck algorithm [38]:\n\n$$\n\\pi_{n+1}(a \\mid s)=\\pi_{n}(a) \\frac{\\exp \\left(\\frac{1}{\\beta} \\sum_{\\sigma, w} p_{n}(\\sigma \\mid s) p_{n}(w \\mid \\sigma) r(w, a)\\right)}{Z_{\\beta, n}(s)}\n$$\n\nwhere $Z_{\\beta, n}(a)$ is a partition function or normalization factor, similar to Refs. [20, 38], so that\n\n$$\nZ_{\\beta, n}(a)=\\sum_{a} \\pi_{n}(a) \\exp \\left(\\frac{1}{\\beta} \\sum_{\\sigma, w} p_{n}(\\sigma \\mid s) p_{n}(w \\mid \\sigma) r(w, a)\\right)\n$$\n\nSimilar manipulations for $p(s \\mid \\sigma)$ gives\n\n$$\np_{n+1}(s \\mid \\sigma)=\\frac{p_{n}(s) \\exp \\left(\\frac{1}{\\lambda} \\sum_{a, w} \\pi_{n}(a \\mid s) p_{n}(w \\mid \\sigma) r(w, a)\\right)}{Z_{\\lambda, n}(\\sigma)}\n$$\n\nwhere $Z_{\\lambda, n}(\\sigma)$ is again a partition function or normalization factor,\n\n$$\nZ_{\\lambda, n}(\\sigma)=\\sum_{s} p_{n}(s) \\exp \\left(\\frac{1}{\\lambda} \\sum_{a, w} \\pi_{n}(a \\mid s) p_{n}(w \\mid \\sigma) r(w, a)\\right)\n$$\n\nAs the action policy and sensory apparatus change with iteration, so do the sensorimotor causal states and their relationship to the underlying world states. We use a combination of the algorithms in Refs. [41, 42] to tackle this problem, as described in Appendix B and in Algorithm 1 from $p(o \\mid w), p\\left(w_{t+1} \\mid w_{t}, a_{t}\\right), p_{n}(s \\mid \\sigma)$, and $\\pi_{n}(s \\mid a)$, where there is an adjustable parameter $N$ governing the length of the observation sequence used to estimate $p_{n+1}(w \\mid \\sigma)$ and $p_{n+1}(\\sigma)$. Interestingly, this aspect of the algorithm is missing in Ref. [20]'s variational treatment, since that treatment does not take into account the fact that her $P(a \\mid h)$ affects her $P(z)$ in unanticipated ways due to sensorimotor feedback, for example- the action policy affects all future observations not just via a marginalization over one time step, but all time steps.\n\n```\nAlgorithm 1 The sensorimotor causal states algorithm to find the reward-rate manifold\n    Input world characteristics \\(p(o \\mid w)\\) and \\(p\\left(w_{t+1} \\mid a_{t}, w_{t}\\right)\\), and organism's relationship to the environment \\(r(w, a)\\).\n    while \\(\\beta, \\lambda\\) run through a list of possible \\(\\beta\\) 's, \\(\\lambda\\) 's that trace out the manifold do\n        Initialize \\(p(s \\mid \\sigma), p(a \\mid s)\\).\n        Calculate the corresponding \\(p(w)\\) and then use the mixed state presentation to find the causal states. \\(\\triangleright\\) The length of\n        observation sequences \\(N\\) and the resolution of the simplex \\(\\epsilon\\) are hyperparameters. \\(\\epsilon\\) should be as small as possible and \\(N\\) as\n        large as possible without sacrificing computational efficiency for consistency.\n        From the causal states, find \\(p(s \\mid \\sigma)\\) by averaging the \\(p(s \\mid h)\\) for those \\(h\\) in the same sensorimotor causal state.\n        Run Eqs. \\(9-6\\) to convergence.\n        Collect \\(I[s ; a]\\) and \\(I[s ; \\sigma]\\) and \\(\\langle r(w, a)\\rangle\\) for that \\(\\beta, \\lambda\\).\n    end while\n    Parametrically plot the reward-rate manifold.\n```\n\nAs $\\lambda, \\beta$ change from 0 to $\\infty$, we can trace out the entire two-dimensional reward-rate manifold. Because the objective function is convex in the sensor description $p(s \\mid \\sigma)$ and actuator description $\\pi(a \\mid s)$, this generalized BlahutArimoto algorithm will converge roughly to the global optimum as $n \\rightarrow \\infty$, with the caveat that $N, \\epsilon$ controls the quality of convergence. We wish to make $N$ as large as possible and the coarse-graining of the simplex $\\epsilon$ as small as possible, but also require compute efficiency.\n\nTheorem 2 The objective function, in the limit $\\lambda \\rightarrow 0$, finds that the sensor states should be sensorimotor causal states, and in the limit $\\beta \\rightarrow 0$, finds that the action policy should be deterministic.\n\nAs in Ref. [20], in the limit that $\\lambda \\rightarrow 0$, we find that $s$ recovers exactly the sensorimotor causal states $\\sigma$ and in the limit that $\\beta \\rightarrow 0$, we find a deterministic action policy. To see this, we can simply stare at the objective function and note that as these Lagrange multipliers tend to 0 , our goal is to maximize reward and we do not care about the rates, which is accomplished when you store as much information about the environment as possible in your sensor and have a one-to-one mapping\nfrom sensory states to actions. This lossless limit is likely nearby to what humans or very complex organisms experience. Then, in the limit that $\\beta, \\lambda$ are large, we find that the sensor picks up no information about the causal state and that the actuator is completely stochastic and does not depend on sensor state, simply from glancing at the importance of the mutual informations in the objective function in this limit. This lossy limit is likely close to what small fish or other simple organisms experience. However, again, the goal here is not necessarily to find sensors or actuators- though by conjecture statistics of good ones can be obtained from this algorithm [8]- but to calculate a reward-rate manifold so as to benchmark how well biological and artificial agents reap reward under resource constraints in POMDP environments.\n\nNote that before this theorem, operating on long histories to calculate the reward-rate manifold would encounter two curses of dimensionality based on the length of the history. We have replaced histories with sensorimotor causal states, bypassing one curse of dimensionality [43], as in Ref. [32]. Still, a curse of dimensionality is encountered from the algorithm in Ref. [42]. One can calculate, in theory, the reward-rate manifold from an\n\nalgorithm like that of Algorithm 1, with more algorithms to come in future work.", "tables": {}, "images": {}}, {"section_id": 6, "text": "## IV. AN EXAMPLE REWARD-RATE MANIFOLD\n\nInterestingly, $\\epsilon$ and $L$ need be so small and large for simple POMDPs that we readily encounter a curse of dimensionality with the algorithm as written. Improvements will need to be made before it can be used with confidence. But that does not mean we cannot use the variational equations derived above to produce an example reward-rate manifold.\n\nWe start with what the environment provides, decided arbitrarily for this example to be:\n\n$$\n\\begin{aligned}\nw_{t+1} & =w_{t}-a_{t}+\\eta_{w_{t}} \\\\\no_{t} & =w_{t}+.01 \\eta_{o_{t}} \\\\\nr\\left(w_{t}, a_{t}\\right) & =-\\left\\langle\\left(a_{t}-w_{t}\\right)^{2}\\right\\rangle\n\\end{aligned}\n$$\n\nHere, $\\eta_{w_{t}}, \\eta_{o_{t}}$ are zero-mean, unit-variance Gaussian noise. The first of these equations, Eq. B22, describes how the environment evolves with sensorimotor feedback, and the second, Eq. B23, describes how the observation is the world state corrupted with a tiny bit of noise. Because the noise is so tiny, roughly speaking, $\\sigma$ is $w$. Thus, the objective function becomes\n\n$$\n\\mathcal{L}=-\\left\\langle(a-w)^{2}\\right\\rangle-\\beta I[s ; a]-\\lambda I[w ; s]\n$$\n\nThe last environmental setup equation, Eq. 9 , is a reward function that wishes for $a_{t}$ and $w_{t}$ to be as different as possible.\n\nIt turns out that linear dynamical systems with Gaussian noise the entire way through- for how $s$ relates to $o$ and for how $a$ relates to $s$ - solve the generalized BlahutArimoto equations in Sec. III. Therefore, we can write that to solve the objective function,\n\n$$\n\\begin{aligned}\ns_{t} & =m_{s, o} o_{t}+\\sigma_{s, o} \\eta_{s_{t}} \\\\\na_{t} & =m_{a, s} s_{t}+\\sigma_{a, s} \\eta_{a_{t}}\n\\end{aligned}\n$$\n\nwhere $\\eta_{s_{t}}, \\eta_{a_{t}}$ are again zero-mean, unit-variance Gaussian noise. We avoid additive constants to avoid a trivial solution in which the reward is maximized by simply making additive constants as large as possible, though we leave a full discussion of this phenomenon for later work.\n\nWe must solve for $m_{s, o}, m_{a, s}, \\sigma_{s, o}, \\sigma_{a, s}$ to maximize $\\mathcal{L}$, which we do numerically. We just need slightly simpler expressions for all the mutual informations and the expected reward, which we find in Appendix D. The resultant approximate reward-rate manifold is shown in Fig. 1 .\n![img-0.jpeg](img-0.jpeg)\n\nFIG. 1. For the environment described in Sec. IV, an approximate reward-rate manifold constructed using Appendix D's equations leading to numerical maximization of the objective function in Sec. III in Mathematica for several $\\beta, \\lambda$ between 0 and 1000. As expected, we see a surface that approximately (within numerical error) monotonically increases the reward as both rates increase. The surface resembles the strange information curve behavior of the Even Process in Ref. [32].", "tables": {}, "images": {"img-0.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 7, "text": "## V. FOR WHICH ORGANISMS MIGHT RESOURCE-RATIONAL REINFORCEMENT LEARNING FAIL?\n\nFrom Ref. [44], it is clear that bacteria are maximiners that choose a strategy that works best for the worst-case scenario rather than operating on a discounted sum of rewards. Surprisingly, this means that bacteria do not actually seek to climb chemoattractant gradients unlike as stated in many references, including even Ref. [45]. This is despite the fact that climbing chemoattractant gradients would be an obvious strategy for thriving. Instead, bacteria live in environments in which chemoattractant gradients leave about as quickly as they come, as every other bacterium also tries to climb and eat the chemoattractant. To operate well, they assume a worstcase environment, and do the best job they can given that pessimistic assumption.\n\nThat is only one data point, but it is enough to make us pause. We have ignored these lower-level organisms for the purpose of this paper so far, but we shouldn't. They are an important part of biophysics. We understand them far better than we understand humans, simply due to the lower complexity of the bacterium. In this section, we argue that there is likely a phase transition as organisms increase in complexity from the maximin behavior of the bacterium to the complicated reinforcement learning strategies of humans.\n\nIn particular, we conjecture that for organisms of low enough complexity, you see such maximin behavior only because the organism lacks a theory of mind- an inability to understand environments that are actually other reinforcement learning agents with desires of their own. When an organism can understand other organism's de-\n\nsires, they have an ability to exploit complicated environments that have agency simply because they are partly composed of other organisms [46], and therefore can operate by maximizing a discounted sum of rewards. Otherwise, we would argue, the organism in question does its best job by assuming a worst-case scenario, or assuming an adversarial environment, like the bacterium faces. Therefore, there could be a phase transition in Marr's computational level objective as organisms increase in complexity from those that lack of theory of mind to those that possess one, based simply on the size of the corresponding brain region. The theory of mind may even be quite simple and quite implicit, as C. elegans may have enough of a theory of mind to be better described by the resource-rational reinforcement learning described so far. The question we must essentially ask to see if the organism falls on one or the other side of the phase transition is: does the organism have anything resembling mirror neurons?\n\nIt could also be the case that higher-level organisms look as though they are maximizing something like a discounted sum of rewards, but that actually, they have found the best behavior for the worst-case environment. Perhaps we have only tested higher-level organisms in environments where the maximin behavior is surprisingly close to the policies that you would get from standard reinforcement learning.\n\nHow would we know the difference between a phase transition to resource-rational reinforcement learning and maximin behavior that looks like resource-rational reinforcement learning? There is a simple way to test whether or not a higher-level organism is accomplishing resource-rational reinforcement learning or simply using the maximin action policy, and it involves finding an environment in which these two are very different. To show that this is possible, we now- for the random environment described later- place the maximin behavior relative to the reward-rate manifold. To find the maximin behavior, we allow for the organism's sensory system to store the sensorimotor causal state just so that we can get some insight into the maximin action policy relative to the reward-rate manifold. We then look for an action policy $\\pi(a \\mid \\sigma)$ that solves\n\n$$\n\\pi_{\\text {minimax }}(a \\mid \\sigma)=\\arg \\max _{\\pi(a \\mid \\sigma)} \\min _{p(w \\mid \\sigma)}\\langle r(w, a)\\rangle\n$$\n\nAs the environment changes, $p(w \\mid \\sigma)$ morphs, and so\nwe assume for these lower-level organisms that they are assuming pessimistically an environment that has the worst possible $p(w \\mid \\sigma)$ imaginable. It may not be possible to achieve this particular worst-case scenario given $p\\left(o_{t} \\mid w_{t}\\right)$ and $p\\left(w_{t+1} \\mid a_{t}, w_{t}\\right)$ and yet we assume this to make progress. Note that it is probably unreasonable to assume a lower-level organism can store sensorimotor causal states rather than lossy sensorimotor causal states, but this optimistically gives us our best shot at reaching the reward-rate manifold to see whether or not we can spot the difference between the two objectives even in simple random environments. See Appendix C for an approximate solution to this maximin objective based simply on multivariable calculus. See Algorithm 2. One simply calculates the reward and rates of the maximin strategy and compares to the relevant point for the iterative algorithm- one where $\\lambda, \\beta \\rightarrow 0$.\n\n```\nAlgorithm 2 The approximate maximin solution\n    Input \\(r(w, a)\\).\n    Use Eqs. C10-C13 to calculate an approximate maximin\n    solution.\n```\n\nThis, incidentally, illustrates how one can test if an organism is a resource-rational reinforcement learner. One simply measures the organism's behavior and sensory states using some sort of neural or other readout and calculates the relevant rates, $I[s ; h]$ and $I[s ; a]$, and reward, $\\langle r\\rangle$. Then this point is compared to the reward-rate manifold, finding like rewards and comparing rates or finding like rates and comparing rewards. Like in rate-distortion theory, if this point is close to the reward-rate manifold, we deem the organism a nearly optimal resource-rational reinforcement learning, as in Refs. $[16,47]$ for resourcerational prediction. If this point is not close, perhaps relative to a null model of some kind, then the organism is not a resource-rational reinforcement learner by the end of the experiment.\n\nIn general, lower-level organisms are unlikely to be able to pick up on the full sensorimotor causal state. (We simply assumed they could for illustrative purposes.) Perhaps instead, we can view lower-level organisms as having resource constraints that force $p(s \\mid h)$ to fall into a certain parameterized family $\\mathcal{F}$ and that force $\\pi(a \\mid s)$ to fall into another parameterized family $\\mathcal{G}$. A resource-rational maximiner, then, would take the form\n\n$$\n\\pi_{\\text {maximin }}(a \\mid s), p_{\\text {maximin }}(s \\mid h)=\\arg \\max _{\\pi(a \\mid s) \\in \\mathcal{G}, p(s \\mid h) \\in \\mathcal{F}} \\min _{\\pi\\left(w_{t+1} \\mid a_{t}, w_{t}\\right)}\\langle r(w, a)\\rangle\n$$\n\nwith $\\mathcal{F}, \\mathcal{G}$ to depend on mechanistic details of the organism. We leave this as an intriguing proposal for what a lower-level organism might be trying to do and also leaving experimentalists to test whether or not higher-\nlevel organisms are resource-rational maximiners instead of resource-rational reinforcement learners.", "tables": {}, "images": {}}, {"section_id": 8, "text": "## VI. CONCLUSION\n\nIn this manuscript, we have proposed a new computational-level objective function for theoretical biology and theoretical neuroscience that combines: reinforcement learning [9], the study of learning with feedback via rewards; rate-distortion theory, a branch of information theory $[8,38]$ that deals with compressing signals to retain relevant information; and computational mechanics, the study of minimal sufficient statistics of prediction also known as causal states [20, 37]. We have highlighted why this proposal is likely only an approximation, but is likely to be an interesting one, and proposed a new algorithm for evaluating it to obtain the newly-coined \"reward-rate manifold\".\n\nThe reward-rate manifold is like a rate-distortion function, but in a system where there is both feedback and memory (an underexplored area in information theory) and with one additional rate so that not just the sensor is considered, but the actuator too. Due to the difficulty of analyzing memoryful communication channels with feedback and memoryful input in information theory, we have merely conjectured that this reward-rate manifold might provide a guide to how biological organisms function, in the same way that the predictive rate-distortion function provided insight into the salamander retina [16] and cultured neurons [47].\nIt is important to stress that biological organisms are likely not operating directly on this objective function. Rather, they are naturally subject to resource constraints that lead to them naturally maximizing this objective function. Nor are the sensors and actuators revealed by this objective function likely to be the actual sensors and actuators used-famously, the sensors and actuators that are revealed only provide statistics that describe the true sensors and actuators that do well on the objective function [8].\nIn order to calculate this reward-rate manifold, it will usually be necessary to use the sensorimotor causal states first proposed in Ref. [20], although the algorithm implemented here in Appendix B still encounters a curse of dimensionality, unfortunately. One might reasonably ask why the organism should have access to the sensorimotor causal states. Rather, the organism is likely trying to infer sensorimotor causal states using some algorithm that we have not yet determined [36, 43]. As in Refs. $[14,16,17]$, we envision a raft of experiments that involve the experimentalist knowing the environmental statistics with which the organisms are probed and using their knowledge of sensorimotor causal states to calculate the reward-rate manifold, calculate the reward and rates of the organism from behavioral and neural data, and then place the organism's operation relative to the reward-rate manifold as is common in rate-distortion theory [8]. This will enable a stringent test of whether or not the organism really is maximizing expected reward subject to information-theoretic rate constraints, as we have done here with approximations to the maximiner\n(bacteria-like) strategy.\nAt this point, it is crucial to note that the iterative algorithms used to find the reward-rate manifold and the brute force algorithm used to find the maximiner strategy should be improved upon. The reward-rate manifold's iterative algorithm derived in Appendix B is elegant enough when considering updates for the sensor and the actuator, but due to feedback, it is complicated to find the new sensorimotor causal states. For that, we used the algorithm in Ref. [42], which encounters a curse of dimensionality. This is hard to avoid, as typically, there are an uncountable infinity of sensorimotor causal states, and we merely approximate them with a partition of the belief state space. We envision improvements might come from a variational algorithm using neural networks like that of Ref. [48] or like that of Refs. [49, 50], or potentially using a Gaussian Information Bottleneck-like algorithm as in Ref. [23]. A Gaussian Information Bottleneck-like approach, based on the iterative equations proving that a self-consistent solution was Gaussian, was used in Section IV to find an approximate example reward-rate manifold. In Appendix C, the maximiner strategy assumed that an optimal sensorimotor causal state distribution could be obtained, but this is likely not true in general, and while the foundations for finding the correct maximiner strategy are in this paper, the algorithm is not.\n\nFuture work will center on calculating this reward-rate manifold for various environments and placing organism brain recordings and behavioral assays relative to the reward-rate manifold.\n\nThis is likely only a first approximation to the true computational-level objective. The most important objection we have comes from what we consider \"memory\" to be, which is, at present, a nonlinear correlation coefficient between stimulus past and sensory brain state [51]. This is correlated with working memory in one experiment to date [14], but this is just one experiment. Plus, memory is quite complicated and extends far beyond working memory [52].\n\nIn fact, it is not even clear that memory is the right resource to look at. What about a thermodynamic constraint like entropy production rate, which is lowerbounded [53] (sometimes loosely [54]) by nonpredictive information rate? Or are energetics irrelevant as resource constraints for a system of this size and processing power, despite some beautiful experiments on lower-level situations that are more amenable to mechanistic analysis [18]? Future efforts might focus on including time, as much effort has been spent understanding the speed-accuracy-energy tradeoff in nonequilibrium thermodynamics [55], or notions of processing power and computability. In other work, minimum description length might even replace mutual informations [56]. This reward-rate manifold is just the start to what might appear, as more \"rates\" are added that may not even be mutual informations. The point of this paper is to propose the idea of a reward-rate manifold, which allows di-\n\nrect testing of all of these normative principles for brain and behavior of organisms simply by plotting where the neural recordings and behavior lie relative to a rewardrate manifold.\n\nThis proposal does not solve at all the algorithmic or mechanistic level, although ideas about the mechanistic level have informed the very foundations of this computational-level objective via resource constraints. However, this computational-level objective and the algorithm used to find its associated sensors and actuators cannot be compared to the algorithmic and mechanistic levels, for interesting reasons rooted in rate-distortion theory [8]. Thus, those algorithmic and mechanistic details are left to methods such as maximum likelihood determination of the true sensory and actuator strategies [57, 58]. Still, we hope that this contribution allows for the development of a research program that will finally unfurl the computational level of theoretical biology and theoretical neuroscience.\n\nAnd really, the aim is quite ambitious, as we wish to describe all organisms- not just humans- with a theory of mind by one objective function that is altered to the specifics of the organism's situation only by a change in the POMDP and the Lagrange multipliers for the resource constraints (or equivalently, the level of resources\nthemselves). There may be variation in a population as to how close to the Pareto front organisms are or their individual level of allowed resources for a particular computation as in prior work [14, 47, 59] with a tendency to dot the Pareto front [60], but we expect that humans as a group have a strikingly different level of allowed resources than mice or fish in general that will depend on how much the organism cares about the specific task being tested.\n\nLooking to the future, we can of course not rule the possibility that some objective functions might explain biological data well [14, 16, 17, 21] but be later superseded, as one instantiation of the efficient coding hypothesis [61] was later replaced by another [62]. But we do hope that this objective function and others of its ilk provide a start towards testing if organisms are \"good enough\" or actually resource-rational decision makers.", "tables": {}, "images": {}}, {"section_id": 9, "text": "## ACKNOWLEDGMENTS\n\nI would like to thank Dmitri Chklovskii and Rainer Engelken for inspiring conversations, and I would like to thank Lav Varshney, Artemy Kolchinsky, and Joshua Shaevitz for very helpful comments. Thank you also to anonymous referees for comments.", "tables": {}, "images": {}}, {"section_id": 10, "text": "# Appendix A: Reasoning for Mutual Informations From the Rate-Distortion Theorem \n\nBefore we describe the resource constraints for this POMDP, let us describe the rate-distortion theorem [8]. It will justify why material constraints can likely be replaced by mutual informations.\n\nIn the classic rate-distortion setup, one sends a sequence of $n$ letters $x_{0: n}$ to an encoder that chooses one of $M$ words for those $n$ letters and then sends that word to a decoder which produces a guess as to what those letters were, $\\hat{x}_{0: n}$. The material constraint is actually $\\log M / n$, not a mutual information. This corresponds to a more intuitive notion of resource constraints in the biological sense- number of molecules or number of neurons, normalized by \"blocklength\" $n$. Some distortion measure is defined, $d(x, \\hat{x})$, which could be generalized to a distortion of the entire block $x_{0: n}$ relative to $\\hat{x}_{0: n}$ rather than letter-by-letter, also called an $n$-letter extension. There are some rates $\\log M / n$ and distortions $\\sum d\\left(x_{i}, \\hat{x}_{i}\\right) / n$ that are achievable and some that are unachievable given any combination of encoder and decoder. A theorem shows that the curve separating achievable from unachievable is given by replacing the rate $\\log M / n$ with a mutual information $I[X ; \\bar{X}]$ and the average distortion with an expected distortion if all is memoryless. This curve is accurate in the limit that blocklength $n$ goes to infinity. Otherwise, the rate-distortion curve that separates achievable from unachievable is given by $R_{n}(D)$ rather than $R(D)$, and $R_{n}(D)$ is horribly difficult to calculate [8], but see Ref. [63]. In essence, what we will try to argue is that biological organisms operate in the limit of very large $n$ sometimes, and so it is okay to use mutual informations to calculate the \"reward-rate manifold\"- the two-dimensional manifold that separates allowable from unallowable combinations of the two rates to be discussed and the reward $V_{\\pi}$. Otherwise, $R_{n}(D)$ places an upper bound on $R(D)$, and since the reward is the flip of the distortion, the corresponding logic is that $R_{n}\\left(M I_{s, a}, M I_{b, s}\\right)$ places a lower bound on $R\\left(M I_{s, a}, M I_{b, s}\\right)$.\n\nThe key material constraint that we wish to think about is the number of neurons, either in the sensory layer or in the actuator layer. If there is a combinatorial code, then the number of words $M$ is equivalent to $2^{n u m}$ where num is the number of neurons. A resource constraint that is reasonable is therefore $\\log M$. This must be modulated by a blocklength- some sense of timescales. The NMJ (neuromuscular junction, or actuator layer) is thought to operate by a rate code, while the sensory layers are thought to operate on sub-millisecond timescales [64] and the environment is thought to operate on extremely large timescales given that naturalistic video is described by power laws. Given all this, the effective blocklength for the actuators is likely to be very high, so that $I[s ; a]$ is justified. Then again, blocklengths are costly for reward reasons [65], but we leave the question of why this mutual information constraint appears to explain biological data to some extent in reinforcement learning experiments [59] as an anomaly to be\n\nfigured out by future practitioners. And, $I[h ; s]$ provides us with a lower bound on the reward-rate function and appears to correlate with working memory in at least one study [14] and that explains biological data in other studies $[16,17]$.\n\nA complication exists with what seems to be an exquisite theoretical justification from information theory: the environment is memoryful, and so are the sensors and actuators. The rate-distortion theorem does extend to stationary, ergodic processes. However, memoryful processes have much harder-to-calculate objectives because the entire sequence of inputs and outputs is considered in the rate constraint [8], though see Ref. [66] for algorithms to compute the rate. As a result, we replace material constraints with mutual informations by conjecture as an approximation to what is likely true.\n\nIn a thermodynamic direction, Landauer-like bounds suggest that mutual informations might lower-bound dissipated work $[53,67]$.\n\nEven if none of these information-theoretic reasons explain why these constraints appear to work, mutual informations are excellent nonlinear correlation coefficients [51], and it could be that high correlations are costly as some sort of intuitive memory cost.", "tables": {}, "images": {}}, {"section_id": 11, "text": "# Appendix B: Derivation of a Generalized Blahut-Arimoto Algorithm \n\nWe start with the unconstrained objective function\n\n$$\n\\mathcal{L}=\\left\\langle r\\left(w_{t}, a_{t}\\right)\\right\\rangle-\\beta I\\left[s_{t} ; a_{t}\\right]-\\lambda I\\left[\\sigma_{t} ; s_{t}\\right]-\\gamma_{s} \\sum p\\left(\\sigma_{t}\\right) p\\left(s_{t} \\mid \\sigma_{t}\\right)-\\gamma_{a} \\sum p\\left(s_{t}\\right) p\\left(a_{t} \\mid s_{t}\\right)\n$$\n\nfor discrete state spaces. We take partial derivatives with respect to $p\\left(a_{t} \\mid s_{t}\\right)$ and set them equal to 0 . First:\n\n$$\n\\begin{aligned}\n\\frac{\\partial\\left\\langle r\\left(w_{t}, a_{t}\\right)\\right\\rangle}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} & =\\frac{\\partial}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} \\sum p\\left(w_{t}, a_{t}\\right) r\\left(w_{t}, a_{t}\\right) \\\\\n& =\\frac{\\partial}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} \\sum p\\left(w_{t}, a_{t}, s_{t}, \\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right) \\\\\n& =\\frac{\\partial}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} \\sum p\\left(a_{t} \\mid s_{t}\\right) p\\left(s_{t} \\mid \\sigma_{t}\\right) p\\left(w_{t} \\mid \\sigma_{t}\\right) p\\left(\\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right) \\\\\n& =\\sum p\\left(\\sigma_{t}\\right) p\\left(s_{t} \\mid \\sigma_{t}\\right) p\\left(w_{t} \\mid \\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right)\n\\end{aligned}\n$$\n\nSecond:\n\n$$\n\\frac{\\partial I\\left[s_{t} ; a_{t}\\right]}{\\partial p\\left(a_{t} \\mid s_{t}\\right)}=\\frac{\\partial}{\\partial p\\left(a_{t} \\mid s_{t}\\right)}\\left(H\\left[a_{t}\\right]-H\\left[a_{t} \\mid s_{t}\\right]\\right)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\frac{\\partial H\\left[a_{t} \\mid s_{t}\\right]}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} & =-\\frac{\\partial}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} \\sum p\\left(s_{t}\\right) p\\left(a_{t} \\mid s_{t}\\right) \\log p\\left(a_{t} \\mid s_{t}\\right) \\\\\n& =-p\\left(s_{t}\\right)\\left(1+\\log p\\left(a_{t} \\mid s_{t}\\right)\\right)\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\frac{\\partial H\\left[a_{t}\\right]}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} & =-\\frac{\\partial}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} \\sum p\\left(a_{t}\\right) \\log p\\left(a_{t}\\right) \\\\\n& =-\\sum(1+\\log p(a)) \\frac{\\partial p(a)}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} \\\\\n& =-\\sum \\delta_{a, a_{t}} p\\left(s_{t}\\right)(1+\\log p(a)) \\\\\n& =-p\\left(s_{t}\\right)\\left(1+\\log p\\left(a_{t}\\right)\\right)\n\\end{aligned}\n$$\n\nwhich means\n\n$$\n\\begin{aligned}\n\\frac{\\partial I\\left[s_{t} ; a_{t}\\right]}{\\partial p\\left(a_{t} \\mid s_{t}\\right)} & =-p\\left(s_{t}\\right)\\left(1+\\log p\\left(a_{t}\\right)\\right)+p\\left(s_{t}\\right)\\left(1+\\log p\\left(a_{t} \\mid s_{t}\\right)\\right) \\\\\n& =p\\left(s_{t}\\right) \\log \\frac{p\\left(a_{t} \\mid s_{t}\\right)}{p\\left(a_{t}\\right)}\n\\end{aligned}\n$$\n\nThird:\n\n$$\n\\frac{\\partial I\\left[s_{t} ; \\sigma_{t}\\right]}{\\partial p\\left(a_{t} \\mid s_{t}\\right)}=0\n$$\n\nFourth:\n\n$$\n\\frac{\\partial \\sum p\\left(a_{t} \\mid s_{t}\\right)}{\\partial p\\left(a_{t} \\mid s_{t}\\right)}=1\n$$\n\nand finally the last partial derivative is 0 . This gives\n\n$$\n\\begin{aligned}\n0 & =\\sum_{\\sigma_{t}, w_{t}} p\\left(\\sigma_{t}\\right) p\\left(s_{t} \\mid \\sigma_{t}\\right) p\\left(w_{t} \\mid \\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right)-\\beta p\\left(s_{t}\\right) \\log \\frac{p\\left(a_{t} \\mid s_{t}\\right)}{p\\left(a_{t}\\right)}-\\gamma_{a} p\\left(s_{t}\\right) \\\\\n\\beta p\\left(s_{t}\\right) \\log \\frac{p\\left(a_{t} \\mid s_{t}\\right)}{p\\left(a_{t}\\right)} & =\\sum_{\\sigma_{t}, w_{t}} p\\left(\\sigma_{t}\\right) p\\left(s_{t} \\mid \\sigma_{t}\\right) p\\left(w_{t} \\mid \\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right)-\\gamma_{a} p\\left(s_{t}\\right) \\\\\n\\log \\frac{p\\left(a_{t} \\mid s_{t}\\right)}{p\\left(a_{t}\\right)} & =\\frac{1}{\\beta p\\left(s_{t}\\right)} \\sum_{\\sigma_{t}, w_{t}} p\\left(\\sigma_{t}\\right) p\\left(s_{t} \\mid \\sigma_{t}\\right) p\\left(w_{t} \\mid \\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right)-\\frac{\\gamma_{a}}{\\beta} \\\\\n\\frac{p\\left(a_{t} \\mid s_{t}\\right)}{p\\left(a_{t}\\right)} & =\\exp \\left(\\frac{1}{p\\left(s_{t}\\right)} \\sum_{\\sigma_{t}, w_{t}} p\\left(\\sigma_{t}\\right) p\\left(s_{t} \\mid \\sigma_{t}\\right) p\\left(w_{t} \\mid \\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right)-\\frac{\\gamma_{a}}{\\beta}\\right) \\\\\n& =\\exp \\left(\\frac{1}{\\beta} \\sum_{\\sigma_{t}, w_{t}} p\\left(\\sigma_{t} \\mid s_{t}\\right) p\\left(w_{t} \\mid \\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right)-\\frac{\\gamma_{a}}{\\beta}\\right) \\\\\np\\left(a_{t} \\mid s_{t}\\right) & =p\\left(a_{t}\\right) \\frac{\\exp \\left(\\frac{1}{\\beta} \\sum_{\\sigma_{t}, w_{t}} p\\left(\\sigma_{t} \\mid s_{t}\\right) p\\left(w_{t} \\mid \\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right)\\right)}{Z_{\\beta}\\left(s_{t}\\right)}\n\\end{aligned}\n$$\n\nwhere $Z_{\\beta}\\left(a_{t}\\right)$ is the partition function or normalization factor. Similar manipulations for $p\\left(s_{t} \\mid \\sigma_{t}\\right)$ gives\n\n$$\np\\left(s_{t} \\mid \\sigma_{t}\\right)=\\frac{p\\left(s_{t}\\right) \\exp \\left(\\frac{1}{\\lambda} \\sum_{a_{t}, w_{t}} p\\left(a_{t} \\mid s_{t}\\right) p\\left(w_{t} \\mid \\sigma_{t}\\right) r\\left(w_{t}, a_{t}\\right)\\right)}{Z_{\\lambda}\\left(\\sigma_{t}\\right)}\n$$\n\nwhere $Z_{\\lambda}\\left(\\sigma_{t}\\right)$ is the partition function or normalization factor. To retrieve the generalized Blahut-Arimoto algorithm for the two-dimensional rate-reward manifold, we simply take Eqs. B22 and B23 and iterate them.\n\nEvery single time we iterate, we have to acknowledge that $p\\left(w_{t} \\mid \\sigma_{t}\\right)$ changes, as $p\\left(a_{t} \\mid s_{t}\\right)$ and $p\\left(s_{t} \\mid \\sigma_{t}\\right)$ tell us how the action sequence changes. Hence, actually, $p(w \\mid \\sigma)$ is $p_{u}(w \\mid \\sigma)$, and $p(\\sigma)$ is $p_{u}(\\sigma)$, changing every iteration. How do we get these? A new action sequence, combined with the new observation sequence, tell us how the probability distribution over the world states changes. First note that\n\n$$\np(a \\mid \\sigma)=\\sum_{s} p(a \\mid s) p(s \\mid \\sigma)\n$$\n\nso that at each time step we have\n\n$$\n\\begin{aligned}\np\\left(w_{t+1}, o_{t} \\mid w_{t}\\right) & =\\sum_{a_{t}, \\sigma_{t}} p\\left(w_{t+1}, o_{t}, a_{t}, \\sigma_{t} \\mid w_{t}\\right) \\\\\n& =\\sum_{a_{t}, \\sigma_{t}} p\\left(o_{t} \\mid w_{t}\\right) p\\left(w_{t+1} \\mid a_{t}, w_{t}\\right) p\\left(a_{t} \\mid \\sigma_{t}\\right) p\\left(\\sigma_{t}\\right)\n\\end{aligned}\n$$\n\nwhich can be combined to make the labeled transition matrix $T^{\\left(o_{t}\\right)}$, with which we can find the approximate probability distribution over the world states via the methods of Ref. [41]:\n\n$$\np(w \\mid \\sigma)=p(w \\mid \\overleftarrow{o}, \\overleftarrow{a})=\\frac{\\prod_{t} T^{\\left(o_{t}\\right)} \\mu}{1^{\\top} \\prod_{t} T^{\\left(o_{t}\\right)} \\mu}\n$$\n\nHere, $\\mu$ is the stationary distribution over world states, or the normalized eig $\\left(\\sum_{o} T^{(o)}\\right)$. To get a rough estimate of this conditional probability distribution $p_{n+1}(w \\mid \\sigma)$ from this, we use a reasonably long observation sequence $\\overleftarrow{o}^{N}$,\n\nmaking sure that $N$ is large enough to capture interesting behavior, though this encounters a curse of dimensionality if $N$ is too large [32]. It may seem as though the benefits of coarse-graining to sensorimotor causal states are lost by this maneuver, but now we only encounter a curse of dimensionality in finding $p(w \\mid \\sigma)$ and $p(\\sigma)$ and not in finding $p(s \\mid h)$. Each observation sequence leads to a different $\\sigma$. We then use the methods of Ref. [42] to coarse-grain into approximate sensorimotor causal states to find $p_{n+1}(\\sigma)$.", "tables": {}, "images": {}}, {"section_id": 12, "text": "# Appendix C: Derivation of a minimax action policy when the lower-level organism stores sensorimotor causal states \n\nWe start with\n\n$$\n\\pi_{\\text {minimax }}(a \\mid \\sigma)=\\arg \\max _{\\pi(a \\mid \\sigma)} \\min _{p(w \\mid \\sigma)}\\langle r(w, a)\\rangle\n$$\n\nand\n\n$$\n\\langle r(w, a)\\rangle=\\sum_{a, w, \\sigma} p(\\sigma) \\pi(a \\mid \\sigma) p(w \\mid \\sigma) r(w, a)\n$$\n\nFirst we assume that $\\pi(a \\mid \\sigma)$ is fixed and find the worst possible $p(w \\mid \\sigma)$ :\n\n$$\np_{\\text {minimax }}(w \\mid \\sigma)=\\min _{p(w \\mid \\sigma)} \\sum_{a, w, \\sigma} p(\\sigma) \\pi(a \\mid \\sigma) p(w \\mid \\sigma) r(w, a)-\\sum_{w, \\sigma} \\lambda_{\\sigma} p(w \\mid \\sigma)\n$$\n\nso that\n\n$$\n0=\\sum_{a} p(\\sigma) \\pi(a \\mid \\sigma) r(w, a)-\\lambda_{\\sigma}\n$$\n\nThe linearity in this objective implies that the objective is maximized at the edges of the simplex. In other words, $p(w \\mid \\sigma)$ should be $\\delta_{w, f(\\sigma)}$ for some $f(\\sigma)$. In other words, there is a one-to-one mapping between sensorimotor causal states and hidden states, so that we might as well replace $\\sigma$ with $w$ and assume that the environment is understood in this limit. Similar logic holds for $\\pi(a \\mid \\sigma)$, so that $\\pi(a \\mid \\sigma)$ is deterministic and $\\delta_{a, g(w)}$. Altogether, this gives\n\n$$\n\\pi_{\\text {minimax }}(a \\mid \\sigma)=\\delta_{a, g(w)}\n$$\n\nin which\n\n$$\ng(w)=\\arg \\max _{g} \\sum_{w} p_{g}(w) r(w, g(w))\n$$\n\nTo find $p_{g}(w)$, we use\n\n$$\n\\begin{aligned}\np_{g}\\left(w_{t+1}\\right) & =\\sum_{a_{t}, w_{t}} p\\left(w_{t+1} \\mid a_{t}, w_{t}\\right) p\\left(a_{t} \\mid w_{t}\\right) p_{g}\\left(w_{t}\\right) \\\\\n& =\\sum_{a_{t}, w_{t}} p\\left(w_{t+1} \\mid a_{t}, w_{t}\\right) \\delta_{a_{t}, g\\left(w_{t}\\right)} p_{g}\\left(w_{t}\\right) \\\\\n& =\\sum_{w_{t}} p\\left(w_{t+1} \\mid a_{t}=g\\left(w_{t}\\right), w_{t}\\right) p_{g}\\left(w_{t}\\right)\n\\end{aligned}\n$$\n\nso that\n\n$$\np_{g}(w)=\\operatorname{eig}_{1}\\left(p\\left(w^{\\prime} \\mid a=g(w), w)\\right)\\right.\n$$\n\nWe can do a brute force search and find the appropriate $g$. This gives us the following reward and rates:\n\n$$\n\\begin{aligned}\n\\langle r\\rangle_{\\text {minimax }} & =\\max _{g} \\sum_{w} \\operatorname{eig}_{1}\\left(p\\left(w^{\\prime} \\mid a=g(w), w)\\right) r(w, g(w))\\right. \\\\\nI[s ; h] & =H[w] \\\\\nI[a ; s] & =I[w, g(w)]=H[g(w)]\n\\end{aligned}\n$$\n\nThis point can then be placed next to the reward-rate manifold.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# Appendix D: Derivation of the example POMDP objective function \n\nWe start by finding:\n\n$$\n\\begin{aligned}\ns & =m_{s, o} w+0.01 m_{s, o} \\eta_{o}+\\sigma_{s, o} \\eta_{s} \\\\\na & =m_{a, s} s+\\sigma_{a, s} \\eta_{a} \\\\\n& =m_{a, s} m_{s, o} w+0.01 m_{a, s} m_{s, o} \\eta_{o}+m_{a, s} \\sigma_{s, o} \\eta_{s}+\\sigma_{a, s} \\eta_{a}\n\\end{aligned}\n$$\n\nThis implies that\n\n$$\nw_{t+1}=w_{t}-m_{a, s} m_{s, o} w-0.01 m_{a, s} m_{s, o} \\eta_{o}-m_{a, s} \\sigma_{s, o} \\eta_{s}-\\sigma_{a, s} \\eta_{a}+\\eta_{w}\n$$\n\nwhere the noises all combine to make\n\n$$\nw_{t+1}=\\left(1-m_{a, s} m_{s, o}\\right) w_{t}+\\sqrt{0.01^{2} m_{a, s}^{2} m_{s, o}^{2}+m_{a, s}^{2} \\sigma_{s, o}^{2}+\\sigma_{a, s}^{2}+1} \\eta_{c o m b}\n$$\n\nThis implies, if we find the variance of both sides, and assuming stationary statistics, that\n\n$$\n\\sigma_{w w}=\\left(1-m_{a, s} m_{s, o}\\right)^{2} \\sigma_{w w}+\\left(0.01^{2} m_{a, s}^{2} m_{s, o}^{2}+m_{a, s}^{2} \\sigma_{s, o}^{2}+\\sigma_{a, s}^{2}+1\\right)\n$$\n\nwhich means that\n\n$$\n\\sigma_{w w}=\\frac{0.01^{2} m_{a, s}^{2} m_{s, o}^{2}+m_{a, s}^{2} \\sigma_{s, o}^{2}+\\sigma_{a, s}^{2}+1}{1-\\left(1-m_{a, s} m_{s, o}\\right)^{2}}\n$$\n\nSeveral other quantities are necessary, as we need covariances and variances of other variables. We have\n\n$$\n\\begin{aligned}\n\\sigma_{w s} & =\\left\\langle w_{t} s_{t}\\right\\rangle-\\left\\langle w_{t}\\right\\rangle\\left\\langle s_{t}\\right\\rangle \\\\\n& =m_{s, o} \\sigma_{w w}\n\\end{aligned}\n$$\n\nand similar calculations yield\n\n$$\n\\begin{aligned}\n\\sigma_{s s} & =m_{s, o}^{2} \\sigma_{w w}+0.01^{2} m_{s, o}^{2}+\\sigma_{s, o}^{2} \\\\\n\\sigma_{a s} & =m_{a, s} m_{s, o}^{2} \\sigma_{w w}+0.01^{2} m_{a, s} m_{s, o}^{2}+m_{a, s} \\sigma_{s, o}^{2} \\\\\n\\sigma_{a a} & =m_{a, s}^{2} m_{s, o}^{2} \\sigma_{w w}+0.01^{2} m_{a, s}^{2} m_{s, o}^{2}+m_{a, s}^{2} \\sigma_{s, o}^{2}+\\sigma_{a, s}^{2}\n\\end{aligned}\n$$\n\nWith the formula for the mutual information between two Gaussians, we have\n\n$$\n\\begin{aligned}\nI[s ; a] & =-\\frac{1}{2} \\log \\left(1-\\frac{\\sigma_{s a}^{2}}{\\sigma_{s s} \\sigma_{a a}}\\right) \\\\\nI[s ; w] & =-\\frac{1}{2} \\log \\left(1-\\frac{\\sigma_{s w}^{2}}{\\sigma_{s s} \\sigma_{w w}}\\right)\n\\end{aligned}\n$$\n\nAnd then,\n\n$$\n\\begin{aligned}\n\\left\\langle(w-a)^{2}\\right\\rangle & =\\left\\langle\\left(\\left(m_{a, s} m_{s, o}-1\\right) w+\\sqrt{0.01^{2} m_{a, s}^{2} m_{s, o}^{2}+m_{a, s}^{2} \\sigma_{s, o}^{2}+\\sigma_{a, s}^{2}+1} \\eta_{c o m b}\\right)^{2}\\right\\rangle \\\\\n& =\\left(m_{a, s} m_{s, o}-1\\right)^{2} \\sigma_{w w}+0.01^{2} m_{a, s}^{2} m_{s, o}^{2}+m_{a, s}^{2} \\sigma_{s, o}^{2}+\\sigma_{a, s}^{2}+1\n\\end{aligned}\n$$\n\nThe entire expression was loaded into Mathematica and numerically maximized for $\\beta, \\lambda$ ranging from 0 to 1000 with constraints that all variables were between 0 and 1 to avoid a nonstationarity that led to an unphysically negative variance for $w_{t}$.\n[1] D. Marr, Vision new york: Freeman, (1982).\n[2] D. Levenstein, V. A. Alvarez, A. Amarasingham,\n\nH. Azab, Z. S. Chen, R. C. Gerkin, A. Hasenstaub, R. Iyer, R. B. Jolivet, S. Marzen, et al., On the role of theory and modeling in neuroscience, Journal of Neuroscience 43, 1074 (2023).\n[3] H. B. Barlow et al., Possible principles underlying the transformation of sensory messages, Sensory communication 1, 217 (1961).\n[4] T. F. Icard, Resource rationality, (2023).\n[5] S. J. Gershman, E. J. Horvitz, and J. B. Tenenbaum, Computational rationality: A converging paradigm for intelligence in brains, minds, and machines, Science 349, 273 (2015).\n[6] C. A. Sims, Implications of rational inattention, Journal of monetary Economics 50, 665 (2003).\n[7] C. A. Sims, Rational inattention: Beyond the linearquadratic case, American Economic Review 96, 158 (2006).\n[8] T. Berger, Rate distortion theory: A mathematical basis for data compression (Prentice-Hall, Inc., 1971).\n[9] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (MIT press, 2018).\n[10] W. Schultz, P. Dayan, and P. R. Montague, A neural substrate of prediction and reward, Science 275, 1593 (1997).\n[11] H. Jeong, A. Taylor, J. R. Floeder, M. Lohmann, S. Mihalas, B. Wu, M. Zhou, D. A. Burke, and V. M. K. Namboodiri, Mesolimbic dopamine release conveys causal associations, Science 378, eabq6740 (2022).\n[12] C. R. Sims, Efficient coding explains the universal law of generalization in human perception, Science 360, 652 (2018).\n[13] N. Zaslavsky, C. Kemp, T. Regier, and N. Tishby, Efficient compression in color naming and its evolution, Proceedings of the National Academy of Sciences 115, 7937 (2018).\n[14] V. Ferdinand, A. Yu, and S. Marzen, Humans are resource-rational predictors in a sequence learning task, bioRxiv, 2024 (2024).\n[15] A. M. Jakob and S. J. Gershman, Rate-distortion theory of neural coding and its implications for working memory, Elife 12, e79450 (2023).\n[16] S. E. Palmer, O. Marre, M. J. Berry, and W. Bialek, Predictive information in a sensory population, Proceedings of the National Academy of Sciences 112, 6908 (2015).\n[17] M. Lamberti, S. Tripathi, M. J. A. M. van Putten, S. Marzen, and J. le Feber, Prediction in cultured cortical neural networks, PNAS Nexus 2 (2023).\n[18] A. Hasenstaub, S. Otte, E. Callaway, and T. J. Sejnowski, Metabolic cost as a unifying principle governing neuronal biophysics, Proceedings of the National Academy of Sciences 107, 12329 (2010).\n[19] P. Mehta and D. J. Schwab, Energetic costs of cellular computation, Proceedings of the National Academy of Sciences 109, 17978 (2012).\n[20] S. Still, Information-theoretic approach to interactive learning, Europhysics Letters 85, 28005 (2009).\n[21] L. Lai and S. J. Gershman, Human decision making balances reward maximization and policy compression, (2023).\n[22] T. Malloy, C. R. Sims, T. Klinger, M. Liu, M. Riemer, and G. Tesauro, Capacity-limited decentralized actorcritic for multi-agent games, in 2021 IEEE Conference on Games (CoG) (IEEE, 2021) pp. 1-8.\n[23] G. Chechik, A. Globerson, N. Tishby, and Y. Weiss, In-\nformation bottleneck for gaussian variables, Advances in Neural Information Processing Systems 16 (2003).\n[24] W. Bialek, I. Nemenman, and N. Tishby, Predictability, complexity, and learning, Neural computation 13, 2409 (2001).\n[25] J. P. Crutchfield and S. Marzen, Signatures of infinity: Nonergodicity and resource scaling in prediction, complexity, and learning, Physical Review E 91, 050106 (2015).\n[26] S. E. Marzen and J. P. Crutchfield, Statistical signatures of structural organization: The case of long memory in renewal processes, Physics Letters A 380, 1517 (2016).\n[27] D. Arumugam, M. K. Ho, N. D. Goodman, and B. Van Roy, Bayesian reinforcement learning with limited cognitive load, Open Mind 8, 395 (2024).\n[28] M. Tucker, R. Levy, J. A. Shah, and N. Zaslavsky, Trading off utility, informativeness, and complexity in emergent communication, in Advances in Neural Information Processing Systems, Vol. 35, edited by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Curran Associates, Inc., 2022) pp. 22214-22228.\n[29] N. Tishby and D. Polani, Information theory of decisions and actions, in Perception-action cycle: Models, architectures, and hardware (Springer, 2010) pp. 601-636.\n[30] S. G. Van Dijk and D. Polani, Informational drives for sensor evolution, Artificial Life 13 (2012).\n[31] S. Still, Information bottleneck approach to predictive inference, Entropy 16, 968 (2014).\n[32] S. E. Marzen and J. P. Crutchfield, Predictive ratedistortion for infinite-order markov processes, Journal of Statistical Physics 163, 1312 (2016).\n[33] A. N. Burnetas and M. N. Katehakis, Optimal adaptive policies for sequential allocation problems, Advances in Applied Mathematics 17, 122 (1996).\n[34] D. B. Chklovskii and A. A. Koulakov, Maps in the brain: what can we learn from them?, Annu. Rev. Neurosci. 27, 369 (2004).\n[35] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, Planning and acting in partially observable stochastic domains, Artificial intelligence 101, 99 (1998).\n[36] F. Doshi-Velez, D. Pfau, F. Wood, and N. Roy, Bayesian nonparametric methods for partially-observable reinforcement learning, IEEE transactions on pattern analysis and machine intelligence 37, 394 (2013).\n[37] C. R. Shalizi and J. P. Crutchfield, Computational mechanics: Pattern and prediction, structure and simplicity, Journal of statistical physics 104, 817 (2001).\n[38] N. Tishby, F. C. Pereira, and W. Bialek, The information bottleneck method, arXiv preprint physics/0004057 (2000).\n[39] A. S. Moffett and A. W. Eckford, To code, or not to code, at the racetrack: Kelly betting and single-letter codes, arXiv preprint arXiv:2104.14277 (2021).\n[40] S. Marzen, Comment on deterministic information bottleneck, arXiv preprint arXiv:2407.01786 (2024).\n[41] J. P. Crutchfield, C. J. Ellison, and P. M. Riechers, Exact complexity: The spectral decomposition of intrinsic computation, Physics Letters A 380, 998 (2016).\n[42] S. E. Marzen and J. P. Crutchfield, Nearly maximally predictive features and their dimensions, Physical Review E 95, 051301 (2017).\n[43] N. Barnett and J. P. Crutchfield, Computational mechanics of input-output processes: Structured transformations and the epsilon-transducer, Journal of Statistical\n\nPhysics 161, 404 (2015).\n[44] A. Celani and M. Vergassola, Bacterial strategies for chemotaxis response, Proceedings of the National Academy of Sciences 107, 1391 (2010).\n[45] H. Mattingly, K. Kamino, B. Machta, and T. Emonet, Escherichia coli chemotaxis is information limited, Nature physics 17, 1426 (2021).\n[46] G. Seifert, A. Sealander, S. Marzen, and M. Levin, From reinforcement learning to agency: Frameworks for understanding basal cognition, Biosystems 235, 105107 (2024).\n[47] M. Lamberti, S. Tripathi, M. J. van Putten, S. Marzen, and J. le Feber, Prediction in cultured cortical neural networks, PNAS nexus 2, pgad188 (2023).\n[48] M. Hahn and R. Futrell, Estimating predictive ratedistortion curves via neural variational inference, Entropy 21, 640 (2019).\n[49] A. A. Alemi, Variational predictive information bottleneck, in Symposium on Advances in Approximate Bayesian Inference (PMLR, 2020) pp. 1-6.\n[50] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, Deep variational information bottleneck, arXiv preprint arXiv:1612.00410 (2016).\n[51] J. B. Kinney and G. S. Atwal, Equitability, mutual information, and the maximal information coefficient, Proceedings of the National Academy of Sciences 111, 3354 (2014).\n[52] S. Sridhar, A. Khamaj, and M. K. Asthana, Cognitive neuroscience perspective on memory: overview and summary, Frontiers in Human Neuroscience 17, 1217093 (2023).\n[53] S. Still, D. A. Sivak, A. J. Bell, and G. E. Crooks, Thermodynamics of prediction, Physical review letters 109, 120604 (2012).\n[54] S. E. Marzen and J. P. Crutchfield, Prediction and dissipation in nonequilibrium molecular sensors: Conditionally markovian channels driven by memoryful environments, Bulletin of mathematical biology 82, 25 (2020).\n[55] G. Lan, P. Sartori, S. Neumann, V. Sourjik, and Y. Tu, The energy-speed-accuracy trade-off in sensory adaptation, Nature physics 8, 422 (2012).\n[56] T. Moskovitz, K. J. Miller, M. Sahani, and M. M. Botvinick, Understanding dual process cognition via the minimum description length principle, PLOS Computational Biology 20, e1012383 (2024).\n[57] A. Uppal, V. Ferdinand, and S. Marzen, Inferring an observer's prediction strategy in sequence learning experiments, Entropy 22, 896 (2020).\n[58] N. D. Daw et al., Trial-by-trial data analysis using computational models, Decision making, affect, and learning: Attention and performance XXIII 23 (2011).\n[59] L. Lai and S. J. Gershman, Human decision making balances reward maximization and policy compression, PLOS Computational Biology 20, e1012057 (2024).\n[60] G. Tka\u010dik and P. R. t. Wolde, Information processing in biochemical networks, Annual Review of Biophysics 54 (2025).\n[61] S. Laughlin, A simple coding procedure enhances a neuron's information capacity, Zeitschrift f\u00fcr Naturforschung c 36, 910 (1981).\n[62] I. M. Park and J. W. Pillow, Bayesian efficient coding, BioRxiv, 178418 (2017).\n[63] V. Kostina and S. Verd\u00fa, Fixed-length lossy compression in the finite blocklength regime, IEEE Transactions on Information Theory 58, 3309 (2012).\n[64] I. Nemenman, G. D. Lewen, W. Bialek, and R. R. de Ruyter van Steveninck, Neural coding of natural stimuli: information at sub-millisecond resolution, PLoS computational biology 4, e1000025 (2008).\n[65] Y. Sawaya, G. Issa, and S. E. Marzen, Framework for solving time-delayed markov decision processes, Physical Review Research 5, 033034 (2023).\n[66] D.-M. Arnold, H.-A. Loeliger, P. O. Vontobel, A. Kavcic, and W. Zeng, Simulation-based computation of information rates for channels with memory, IEEE Transactions on information theory 52, 3498 (2006).\n[67] T. Sagawa and M. Ueda, Minimal energy cost for thermodynamic information processing: measurement and information erasure, Physical review letters 102, 250602 (2009).", "tables": {}, "images": {}}], "id": "2404.18775v4", "authors": ["Sarah Marzen"], "categories": ["q-bio.NC"], "abstract": "We propose a new computational-level objective function for theoretical\nbiology and theoretical neuroscience that combines: reinforcement learning, the\nstudy of learning with feedback via rewards; rate-distortion theory, a branch\nof information theory that deals with compressing signals to retain relevant\ninformation; and computational mechanics, the study of minimal sufficient\nstatistics of prediction also known as causal states. We highlight why this\nproposal is likely only an approximation, but is likely to be an interesting\none, and propose a new algorithm for evaluating it to obtain the newly-coined\n``reward-rate manifold''. The performance of real and artificial agents in\npartially observable environments can be newly benchmarked using these\nreward-rate manifolds. Finally, we describe experiments that can probe whether\nor not biological organisms are resource-rational reinforcement learners, using\nas an example maximin strategies, as bacteria have been shown to be approximate\nmaximiners -- doing their best in the worst-case environment, regardless of\nwhat is actually happening.", "updated": "2025-03-19T21:39:09Z", "published": "2024-04-29T15:10:02Z"}