{"title": "Asymptotic independence in higher dimensions and its implications on\n  risk management", "sections": [{"section_id": 0, "text": "#### Abstract\n\nIn the study of extremes, the presence of asymptotic independence signifies that extreme events across multiple variables are probably less likely to occur together. Although well-understood in a bivariate context, the concept remains relatively unexplored when addressing the nuances of joint occurrence of extremes in higher dimensions. In this paper, we propose a notion of mutual asymptotic independence to capture the behavior of joint extremes in dimensions larger than two and contrast it with the classical notion of (pairwise) asymptotic independence. Additionally, we define $k$-wise asymptotic independence, which captures the tail dependence between pairwise and mutual asymptotic independence. The concepts are compared using examples of Archimedean, Gaussian, and Marshall-Olkin copulas among others. Notably, for the popular Gaussian copula, we provide explicit conditions on the correlation matrix for mutual asymptotic independence and $k$-wise asymptotic independence to hold; moreover, we are able to compute exact tail orders for various tail events. Beside that, we compare and discuss the implications of these new notions of asymptotic independence on assessing the risk of complex systems under distributional ambiguity.\n\n\n1. Introduction. In many multivariate models, we observe that the likelihood of joint occurrence of extreme values in two or more variables is negligible in comparison to the occurrence of an extreme value in one variable. In this context, the notion of asymptotic independence looms large in the study of joint extreme values in probability distributions, although mostly restricted to the bivariate set-up. A random vector $\\left(Z_{1}, Z_{2}\\right) \\in \\mathbb{R}^{2}$ with identically distributed marginals is asymptotically (right-tail/upper-tail) independent if\n\n$$\n\\mathbb{P}\\left(Z_{1}>t, Z_{2}>t\\right)=o\\left(\\mathbb{P}\\left(Z_{1}>t\\right)\\right), \\quad t \\rightarrow \\infty\n$$\n\nor equivalently $\\mathbb{P}\\left(Z_{1}>t \\mid Z_{2}>t\\right) \\rightarrow 0$ as $t \\rightarrow \\infty$. For the rest of this paper, we focus only on extremes in the non-negative quadrant and drop the terms right/upper-tail for convenience.\n\nOften called Sibuya's condition, (1.1) was exhibited by Sibuya [38] for bivariate normal random vectors with any correlation $\\rho<1$. Such a limit behavior has also been found to hold for bivariate distributions with arbitrary choice of marginals possessing a variety of dependence structures, including, Frank copula, Ali-Mikhail-Haq copula, Gaussian copula, Farlie-Gumbel-Morgenstern copula, and, more; see [5, 21, 28, 29]. It is widely believed that the presence of asymptotic independence hinders the computation of joint tail probabilities, and has led to a variety of techniques for modeling and estimating rare tail probabilities when such a property is present; see [7, 9, 28, 30, 36, 37]. Nevertheless, for random vectors in dimensions higher than two, limited expositions are available, and multivariate asymptotic independence is often understood to be (1.1) holding for all pairs of variables, which we call pairwise asymptotic independence.\n\n[^0]\n[^0]:    AMS 2000 subject classifications: Primary 62H05, 62H20, 62G32; Secondary 62G32, 62P05.\n    Keywords and phrases: asymptotic independence, copula models, Gaussian copula, multivariate extremes, risk contagion.\n\nAsymptotic independence for bivariate joint tails is also popularly understood using the coefficient of tail dependence $\\eta$ defined in Ledford and Tawn [28]. If $Z_{1}, Z_{2}$ are identically unit Fr\u00e9chet distributed with distribution function $F(z)=\\mathrm{e}^{-1 / z}, z>0$, and\n\n$$\n\\mathbb{P}\\left(Z_{1}>t, Z_{2}>t\\right)=t^{-1 / \\eta} \\ell(t), \\quad t \\rightarrow \\infty\n$$\n\nwhere $1 / 2 \\leq \\eta<1$ and $\\ell$ is slowly varying at infinity (i.e., $\\ell(t z) / \\ell(z) \\rightarrow 1$ as $t \\rightarrow \\infty, \\forall z>$ 0 ), then $\\eta$ represents this coefficient of tail dependence. According to Ledford and Tawn [28], (i) $\\eta=1 / 2$ and $\\ell(t) \\geq 1$ signifies near independence, (ii) $\\eta=1$ and $\\ell(t) \\nrightarrow 0$ as $t \\rightarrow \\infty$ signifies upper tail dependence, and finally, (iii) either $1 / 2<\\eta<1$, or $\\eta=1$ and $\\ell(t) \\rightarrow 0$ as $t \\rightarrow \\infty$ signifies positive association.\n\nThe coefficient of tail dependence is a 2-dimensional concept and has been extended to $d$-dimensions as upper tail order by Hua and Joe [23] through the survival copula. Prior to further discussions, we recall the notions of copula and survival copula.\n\nA copula $C:[0,1]^{d} \\rightarrow[0,1]$ is a multivariate distribution function with identical uniform $[0,1]$ margins. From Sklar's Theorem $[14,34,39]$ we know that for any $d$-dimensional random vector $\\boldsymbol{Z}=\\left(Z_{1}, \\ldots, Z_{d}\\right)$ with distribution function $F$ and marginal distributions $F_{1}, \\ldots, F_{d}$ there exists a copula $C:[0,1]^{d} \\rightarrow[0,1]$ such that\n\n$$\nF\\left(z_{1}, \\ldots, z_{d}\\right)=C\\left(F_{1}\\left(z_{1}\\right), \\ldots, F_{d}\\left(z_{d}\\right)\\right)\n$$\n\nfor $\\left(z_{1}, \\ldots, z_{d}\\right) \\in \\mathbb{R}^{d}$, and if the margins are continuous, the copula is uniquely given by\n\n$$\nC\\left(u_{1}, \\ldots, u_{d}\\right)=F\\left(F_{1}^{\\leftarrow}\\left(u_{1}\\right), \\ldots, F_{d}^{\\leftarrow}\\left(u_{d}\\right)\\right)\n$$\n\nfor $0<u_{1}, \\ldots, u_{d}<1$, where\n\n$$\nF_{j}^{\\leftarrow}\\left(u_{j}\\right):=\\inf \\left\\{z \\in \\mathbb{R}: F_{j}(z) \\geq u_{j}\\right\\}\n$$\n\nis the generalized inverse of $F_{j}$ for $j=1, \\ldots, d$. In this paper, we are particularly concerned with the probability of joint extremes where the survival copula $\\widehat{C}:[0,1]^{d} \\rightarrow[0,1]$, which is also a copula, plays an important role. The survival copula $\\widehat{C}$ satisfies\n\n$$\n\\mathbb{P}\\left(Z_{1}>z_{1}, \\ldots, Z_{d}>z_{d}\\right)=\\widehat{C}\\left(\\bar{F}_{1}\\left(z_{1}\\right), \\ldots, \\bar{F}_{d}\\left(z_{d}\\right)\\right)\n$$\n\nfor $\\left(z_{1}, \\ldots, z_{d}\\right) \\in \\mathbb{R}^{d}$, where $\\bar{F}_{j}=1-F_{j}$ is the tail function of $F_{j}$ for $j=1, \\ldots, d$. Of course, the survival copula and the copula are directly related through\n\n$$\n\\widehat{C}\\left(u_{1}, \\ldots, u_{d}\\right)=1+\\sum_{\\substack{s \\in(1, \\ldots, d) \\\\ S \\neq \\emptyset}}(-1)^{|S|} C_{S}\\left(1-u_{j}: j \\in S\\right)\n$$\n\nfor $0 \\leq u_{1}, \\ldots, u_{d} \\leq 1$, where $|S|$ is the cardinality of the set $S$ and $C_{S}$ is the appropriate $|S|$-dimensional marginal copula of $C$. In dimension $d=2$, this reduces to\n\n$$\n\\widehat{C}\\left(u_{1}, u_{2}\\right)=u_{1}+u_{2}-1+C\\left(1-u_{2}, 1-u_{2}\\right)\n$$\n\nfor $0 \\leq u_{1}, u_{2} \\leq 1$.\nReturning back to notions of tail dependence, if a $d$-dimensional survival copula $\\widehat{C}$ satisfies\n\n$$\n\\widehat{C}(u, \\ldots, u)=u^{\\kappa} \\ell(u), \\quad 0 \\leq u \\leq 1\n$$\n\nfor some slowly varying function $\\ell$ at 0 and some constant $\\kappa>0$, then $\\kappa$ is called the upper tail order. Here, (i) the case $\\kappa=d$ signifies near (asymptotic) independence (for $d=2$, we have $\\kappa=1 / \\eta$ ), (ii) the case $\\kappa=1$ and $\\ell(u) \\nrightarrow 0$ as $u \\downarrow 0$ signifies (asymptotic) upper tail dependence, and, (iii) the case where $1<\\kappa<d$ is called upper intermediate tail dependence in Hua and Joe [23]. From the definition of tail order, we can see that for $d=2$, the survival\n\ncopulas in both the cases of \"near independence\" and \"upper intermediate tail dependence\" exhibit asymptotic independence in the sense of (1.1); in this paper, we gain a better understanding of these ideas when $d>2$.\n\nNote that for independence of multiple random variables, it is well-known that \"pairwise independence\" for all pairs of random variables is not equivalent to their \"mutual independence\" (cf. [22, Chapter 2]). In the same vein, we propose here the concepts of pairwise asymptotic independence in Section 2 and mutual asymptotic independence in Section 3. With the new notion of mutual asymptotic independence, we explore the ideas of \"near independence\" and \"intermediate upper tail dependence\" through all subsequent dimensions $2,3, \\ldots, d$ going beyond just the $d$-dimensional characterization as given in (1.2). For models that lie between pairwise and mutually asymptotically independent models, we introduce the concept of $k$-wise asymptotic independence for $k \\in\\{2, \\ldots, d\\}$ in Section 4. In particular, we investigate and compare the various notions of asymptotic independence and illustrate them using popular copula models. Moreover, we obtain the following three key results for the popular Gaussian copula, which have broader theoretical and practical implications:\n(i) a formulation of precise necessary and sufficient conditions for mutual asymptotic independence to hold,\n(ii) a derivation of the correct tail orders for various tail events, and,\n(iii) the existence of Gaussian copula models exhibiting $k$-wise asymptotic independence but not $(k+1)$-wise asymptotic independence.\n\nBesides the Gaussian copula, we also provide examples to exhibit the breadth of asymptotic (in)dependence behavior using the Archimedean copula family. We apply the new notions of asymptotic independence in Section 5 to show its implications on assessing the risk of complex systems under distributional ambiguity. The different notions of asymptotic independence influence risk contagion in financial systems differently and hence may lead to an underestimation or overestimation of risk if applied improperly. In particular, we exhibit this phenomenon using two pertinent conditional risk measures, namely, conditional tail probability and Contagion value-at-risk or CoVaR in dimensions $d>2$. Finally, in Section 6, we conclude with some broader implications of interpreting asymptotic independence in this new light. All proofs for the results presented in this paper are provided in Appendix A.\n\nNotations. We denote by $\\mathbb{I}_{d}=\\{1, \\ldots, d\\}$ an index set with $d$ elements and the cardinality of a set $S \\subseteq \\mathbb{I}_{d}$ is denoted by $|S|$. For a random vector $\\boldsymbol{Z}=\\left(Z_{1}, \\ldots, Z_{d}\\right)$, we write $\\boldsymbol{Z} \\sim F$ if $\\boldsymbol{Z}$ has distribution function $F$; moreover, we understand that marginally $Z_{j} \\sim F_{j}$ for $j \\in \\mathbb{I}_{d}$. For any non-empty sets $S \\subseteq \\mathbb{I}_{d}$, the copula and survival copula of the corresponding $|S|$ dimensional marginal are denoted by $C_{S}$ and $\\widehat{C}_{S}$, respectively. Moreover, if $d=1$ we have $C_{S}(u)=\\widehat{C}_{S}(u)=u$ for $0 \\leq u \\leq 1$. For a given vector $\\boldsymbol{z} \\in \\mathbb{R}^{d}$ and $S \\subseteq \\mathbb{I}_{d}$, we denote by $\\boldsymbol{z}^{\\top}$ the transpose of $\\boldsymbol{z}$ and by $\\boldsymbol{z}_{S} \\in \\mathbb{R}^{|S|}$ the vector obtained by deleting the components of $\\boldsymbol{z}$ in $\\mathbb{I}_{d} \\backslash S$. Similarly, for non-empty $S \\subseteq \\mathbb{I}_{d}, \\Sigma_{S}$ denotes the appropriate sub-matrix of a given matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$ after removing all rows and columns with indices in $\\mathbb{I}_{d} \\backslash S$. Furthermore, $\\mathbf{0}_{d}=(0, \\ldots, 0)^{\\top}$ and $\\mathbf{1}_{d}=(1, \\ldots, 1)^{\\top}$ are vectors in $\\mathbb{R}^{d}$, and $\\boldsymbol{I}_{d}$ is the identity matrix in $\\mathbb{R}^{d \\times d}$; subscripts are dropped when evident from the context. Vector operations are understood component-wise, e.g., for vectors $\\boldsymbol{z}=\\left(z_{1}, \\ldots, z_{d}\\right)$ and $\\boldsymbol{y}=\\left(y_{1}, \\ldots, y_{d}\\right), \\boldsymbol{z} \\leq \\boldsymbol{y}$ means $z_{j} \\leq y_{j}, \\forall j \\in \\mathbb{I}_{d}$. For functions $f, g:(0, \\infty) \\rightarrow(0, \\infty)$, we write $f(u) \\sim g(u)$ as $u \\downarrow 0$ if $\\lim _{u \\downarrow 0} f(u) / g(u)=1$. Moreover, a function $\\ell:(0, \\infty) \\rightarrow(0, \\infty)$ is called a slowly varying at 0 , if $\\lim _{u \\downarrow 0} \\ell(u z) / \\ell(u)=1, \\forall z>0$.\n\n2. Pairwise asymptotic independence. Note that the definition in (1.1) can be easily generalized to distributions with potentially unequal margins; any random vector $\\left(Z_{1}, Z_{2}\\right)$ with continuous marginals $Z_{j} \\sim F_{j}, j=1,2$ is asymptotically independent if\n\n$$\n\\widehat{C}(u, u)=\\mathbb{P}\\left(F_{1}\\left(Z_{1}\\right)>1-u, F_{2}\\left(Z_{2}\\right)>1-u\\right)=o(u), \\quad u \\downarrow 0\n$$\n\nwhere $\\widehat{C}$ is the survival copula of $F$. Note that limit properties in (1.1) and (2.1) remain equivalent when the marginals of $\\left(Z_{1}, Z_{2}\\right)$ are completely tail equivalent, i.e., $\\mathbb{P}\\left(Z_{1}>t\\right) / \\mathbb{P}\\left(Z_{2}>\\right.$ $t) \\rightarrow 1$ as $t \\rightarrow \\infty$. Although not all extreme sets are of this form, this definition has been a key concept in the modeling of joint extremes.\n\nAn interesting feature of this definition of asymptotic independence is that it is based on tail sets tethered along the main diagonal $(t, t)$ (in (1.1)) or $(1-u, 1-u)$ (in (2.1)). It is easy to check that (2.1) is equivalent to\n\n$$\n\\widehat{C}(a u, b u)=o(u), \\quad u \\downarrow 0\n$$\n\nfor some $a, b>0$ (cf. [2, Theorem 2]). Curiously, an equivalent result for the distribution function of a bivariate random vector with survival function $\\widehat{C}$ does not hold: even if (1.1) holds it does not necessarily hold for diagonals of the form $(a t, b t)$ for any $a, b>0$; see Das and Fasen-Hartmann [11, Proposition 3.9] for an example with normally distributed marginals $\\left(Z_{1}, Z_{2}\\right)$ where $\\mathbb{P}\\left(Z_{1}>a t, Z_{2}>t\\right)=O\\left(\\mathbb{P}\\left(Z_{2}>t\\right)\\right)$, as $t \\rightarrow \\infty$.\n\nAlthough (1.1) and (2.1) are widely applied for bivariate random vectors, a proper multivariate characterization of asymptotic independence has been relatively scarce. A definition often used and based on all pairwise comparisons following (2.1) is given next.\n\nDefinition 2.1 (Pairwise asymptotic independence). A random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ with survival copula $\\widehat{C}$ is pairwise asymptotically independent if $\\forall j, \\ell \\in \\mathbb{I}_{d}, j \\neq \\ell$,\n\n$$\n\\widehat{C}_{j, \\ell}(u, u)=o(u), \\quad u \\downarrow 0\n$$\n\nIf $\\boldsymbol{Z} \\sim F$ has copula $C$, we interchangeably say $\\boldsymbol{Z}, F, C$ or $\\widehat{C}$ exhibits pairwise asymptotic independence.\n\nREMARK 2.2. Although we assumed continuous marginals for stating the condition in (2.1), in this paper, we will be primarily concerned with the asymptotic behavior of the survival copula, hence the marginal behavior and its continuity will not be relevant henceforth.\n\nREMARK 2.3. Note that in contrast to asymptotic independence, asymptotic upper tail dependence occurs for a $d$-dimensional survival copula $\\widehat{C}$ with $d \\geq 2$ if\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widehat{C}(u, \\ldots, u)}{u}=K \\in(0, \\infty)\n$$\n\nObviously, (2.3) implies that (2.2) cannot hold. With respect to the upper tail order defined in (1.2), (2.3) is equivalent to saying $\\kappa=1$ and $\\ell(u) \\nrightarrow 0$ as $u \\downarrow 0$.\n2.1. Examples. Pairwise asymptotic independence exists in many multivariate distributions. We note a few examples here.\n\nExample 2.4 (Independence). If all components of a random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ are independent, then of course,\n\n$$\n\\mathbb{P}\\left(Z_{1}>z_{1}, \\ldots, Z_{d}>z_{d}\\right)=\\prod_{j=1}^{d} \\bar{F}_{j}\\left(z_{j}\\right)=\\widehat{C}^{\\mathrm{ind}}\\left(\\bar{F}_{1}\\left(z_{1}\\right), \\ldots, \\bar{F}\\left(z_{d}\\right)\\right)\n$$\n\nfor $\\left(z_{1}, \\ldots, z_{d}\\right) \\in \\mathbb{R}^{d}$, where $C^{\\text {ind }}:[0,1]^{d} \\rightarrow[0,1]^{d}$ is the independence copula given by\n\n$$\nC^{\\mathrm{ind}}\\left(u_{1}, \\ldots, u_{d}\\right)=\\prod_{j=1}^{d} u_{j}\n$$\n\nfor $0 \\leq u_{1}, \\ldots, u_{d} \\leq 1$ with survival copula $\\widehat{C}^{\\text {ind }}\\left(u_{1}, \\ldots, u_{d}\\right)=C^{\\text {ind }}\\left(u_{1}, \\ldots, u_{d}\\right)$. For any distinct $j, \\ell \\in \\mathbb{I}_{d}$ the $(j, l)$ marginal survival copula is as well\n\n$$\n\\widehat{C}_{j, \\ell}^{\\mathrm{ind}}\\left(u_{1}, u_{2}\\right)=u_{1} u_{2}, \\quad 0 \\leq u_{1}, u_{2} \\leq 1\n$$\n\nThus, clearly, (2.2) holds, and hence, the independence copula exhibits pairwise asymptotic independence.\n\nExample 2.5 (Marshall-Olkin dependence). The Marshall-Olkin distribution is used in reliability theory to capture the failure of subsystems in a networked system. Here we consider a particular Marshall-Olkin dependence; cf. [10, 31]. Assume that for every nonempty set $S \\subseteq \\mathbb{I}_{d}$ there exists a parameter $\\lambda_{S}>0$ and $\\Lambda:=\\left\\{\\lambda_{S}: \\emptyset \\neq S \\subseteq \\mathbb{I}_{d}\\right\\}$. Then the generalized Marshall-Olkin (MO) survival copula with rate parameter $\\Lambda$ is given by\n\n$$\n\\widehat{C}^{\\mathrm{MO}(\\Lambda)}\\left(u_{1}, \\ldots, u_{d}\\right)=\\prod_{i=1}^{d} \\prod_{|S|=i} \\bigwedge_{j \\in S} u_{j}^{\\eta_{j}^{S}}\n$$\n\nfor $0 \\leq u_{1}, \\ldots, u_{d} \\leq 1$, where\n\n$$\n\\eta_{j}^{S}=\\lambda_{S} /\\left(\\sum_{J \\supseteq\\{j\\}} \\lambda_{J}\\right), \\quad j \\in S \\subseteq \\mathbb{I}_{d}\n$$\n\nIn particular, for any distinct $j, \\ell \\in \\mathbb{I}_{d}$, we have\n\n$$\n\\widehat{C}_{\\{i, j\\}}^{\\mathrm{MO}(\\Lambda)}(u, u)=u^{\\eta^{*}}\n$$\n\nwith\n\n$$\n\\eta^{*}=\\sum_{\\substack{S \\subseteq I_{d} \\\\ 1 \\in S, 2 \\in S}} \\eta_{1}^{S}+\\sum_{\\substack{S \\subseteq I_{d} \\\\ 1 \\notin S, 2 \\in S}} \\eta_{2}^{S}+\\sum_{\\substack{S \\subseteq I_{d} \\\\ 1,2 \\in S}} \\max \\left\\{\\eta_{1}^{S}, \\eta_{2}^{S}\\right\\}\n$$\n\nProposition 2.6. The generalized Marshall-Olkin dependence as given by the survival copula $\\widehat{C}^{\\mathrm{MO}(\\Lambda)}$ in (2.5) possesses pairwise asymptotic independence for any choice of $\\Lambda$.\n\nAlthough $\\lambda_{S}$ is allowed to take any positive value for non-empty $S \\subset \\mathbb{I}_{d}$, we discuss below two particular interesting choices of the parameters, cf. Das and Fasen-Hartmann [10, Example 2.14].\n(a) Equal parameter for all sets: Here, $\\lambda_{S}=\\lambda$ for all non-empty $S \\subseteq \\mathbb{I}_{d}$ where $\\lambda>0$ and we denote the survival copula by $\\widehat{C}^{\\mathrm{MO}^{\\infty}}$. We can check from (2.6) that the value of $\\lambda$ is irrelevant here. Clearly in this case $\\eta_{j}^{S}=1 / 2^{d-1}$, for all $j \\in S$ and non-empty $S \\subset \\mathbb{I}_{d}$. Hence we can compute the value of $\\eta^{*}$ defined in (2.7) as\n\n$$\n\\eta^{*}=\\sum_{\\substack{S \\subseteq I_{d} \\\\ 1 \\in S}} \\frac{1}{2^{d-1}}+\\sum_{\\substack{S \\subseteq I_{d} \\\\ 1 \\notin S, 2 \\in S}} \\frac{1}{2^{d-1}}=\\frac{2^{d-1}+2^{d-2}}{2^{d-1}}=\\frac{3}{2}\n$$\n\nTherefore, for all $j, \\ell \\in S$ with $j \\neq \\ell$,\n\n$$\n\\widehat{C}_{j, \\ell}^{\\mathrm{MO}^{\\infty}}(u, u)=u^{3 / 2}, \\quad 0 \\leq u \\leq 1\n$$\n\n(b) Parameters proportional to the cardinality of the sets: Here, $\\lambda_{S}=|S| \\lambda$ for all non-empty $S \\subseteq \\mathbb{I}_{d}$ where $\\lambda>0$ and we denote the survival copula by $\\widehat{C}^{\\mathrm{MO}^{\\infty}}$. As well the value of $\\lambda$ is irrelevant and for all $j \\in S$ and non-empty subset $S \\subset \\mathbb{I}_{d}$ we have\n\n$$\n\\eta_{j}^{S}=\\frac{|S|}{(d+1) 2^{d-2}}\n$$\n\nWe compute again the value of $\\eta^{*}$ defined in (2.7) as\n\n$$\n\\begin{aligned}\n\\eta^{*} & =\\sum_{\\substack{S \\subseteq \\mathbb{I}_{d} \\\\\n1 \\in S}} \\frac{|S|}{(d+1) 2^{d-2}}+\\sum_{\\substack{S \\subseteq \\mathbb{I}_{d} \\\\\n1 \\notin S, 2 \\in S}} \\frac{|S|}{(d+1) 2^{d-2}} \\\\\n& =\\frac{(d+1) 2^{d-2}+d 2^{d-3}}{(d+1) 2^{d-2}}=1+\\frac{d}{2(d+1)}\n\\end{aligned}\n$$\n\nTherefore, for all $j, \\ell \\in S$ with $j \\neq \\ell$,\n\n$$\n\\widehat{C}_{j, \\ell}^{\\mathrm{MO}^{\\infty}}(u, u)=u^{1+d /(2(d+1))}, \\quad 0 \\leq u \\leq 1\n$$\n\nThe generalized MO copulas with these particular choices of parameters as in (a) and (b) are also known as Caudras-Auge copulas [6] and have been used in L\u00e9vy frailty models for survival analysis. Moreover, if the marginals are identically distributed, then the associated random vector turns out to be exchangeable [15].\n\nExample 2.7 (Archimedean copula). A useful family of copula models for multivariate distributions is the Archimedean copulas ([4, 25]). A $d$-dimensional copula $C^{\\phi}$ is Archimedean if\n\n$$\nC^{\\phi}\\left(u_{1}, \\ldots, u_{d}\\right):=\\phi^{\\leftarrow}\\left(\\phi\\left(u_{1}\\right)+\\ldots+\\phi\\left(u_{d}\\right)\\right)\n$$\n\nfor $0 \\leq u_{1}, \\ldots, u_{d} \\leq 1$, where the generator function $\\phi:[0,1] \\rightarrow[0, \\infty]$ is convex, decreasing, with $\\phi(1)=0$ and $\\phi^{\\leftarrow}(y)=\\inf \\{u \\in[0,1]: \\phi(u) \\leq y\\}$ for $y \\in(0, \\infty)$. Necessary and sufficient conditions on the function $\\phi$ such that $C^{\\phi}$ in (2.8) is a copula is given in [33]; note that the survival copula $\\widehat{C}^{\\phi}$ of an Archimedean copula $C^{\\phi}$ is in general not Archimedean. A popular choice of $\\phi$ is the Laplace transform of any positive random variable.\n\nTail dependence in such copulas has been studied in [4, 23], and sufficient conditions to obtain pairwise independence exist. Suppose the random vector $\\boldsymbol{Z}$ having an Archimedean copula $C^{\\phi}$ has a generator $\\phi$ satisfying\n\n$$\n\\lim _{u \\downarrow 0} \\frac{u \\phi^{\\prime}(1-u)}{\\phi(1-u)}=1\n$$\n\nthen we may conclude from Charpentier and Segers [4, Theorem 4.1 and equation (4.4)] that $\\boldsymbol{Z}$ is pairwise asymptotically independent. In contrast, if the limit\n\n$$\n\\theta_{1}:=\\lim _{u \\downarrow 0} \\frac{u \\phi^{\\prime}(1-u)}{\\phi(1-u)} \\in(1, \\infty)\n$$\n\nexists and is larger than 1 then we have asymptotic upper tail dependence, i.e, $\\widehat{C}^{\\phi}$ satisfies (2.3). We observe from Charpentier and Segers [4, Table 1] that for many Archimedean copulas, we have $\\theta_{1}=1$ and thus they are pairwise asymptotically independent; this includes Frank copula, Clayton copula, Ali-Mikhail-Haq copula and so on; see also Nelsen [34, Table 4.1] for further details.\n\nEXAMPLE 2.8 (Gaussian copula). The Gaussian dependence structure is perhaps the most popular one used in practice. Let $\\Phi_{\\Sigma}$ denote the distribution function of a $d$-variate normal distribution with all marginal means zero, variances one and positive-definite correlation matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$, and $\\Phi$ denote a standard normal distribution function. Then for $0<u_{1}, \\ldots, u_{d}<1$,\n\n$$\nC^{\\Sigma}\\left(u_{1}, \\ldots, u_{d}\\right)=\\Phi_{\\Sigma}\\left(\\Phi^{-1}\\left(u_{1}\\right), \\ldots, \\Phi^{-1}\\left(u_{d}\\right)\\right)\n$$\n\ndenotes the Gaussian copula with correlation matrix $\\Sigma$.\nPairwise asymptotic independence has been well-known for the bivariate normal distribution, as well as the bivariate Gaussian copula ([28, 38]). But not surprisingly, it holds as well for $d \\geq 2$ as we summarize next.\n\nProposition 2.9. If the dependence structure of a random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ is given by a Gaussian copula $C^{\\Sigma}$ with positive-definite correlation matrix $\\Sigma$, then $\\boldsymbol{Z}$ exhibits pairwise asymptotic independence.\n\nIn fact, it is possible to find the exact tail order for the Gaussian survival copula for any $S \\subseteq \\mathbb{I}$ with $|S| \\geq 2$, the precise result is given in Section 3.2.2.\n3. Mutual asymptotic independence. Pairwise asymptotic independence has often either been used as a natural extension of asymptotic independence ([2, 18]), or, taken as a consequence from other relevant properties ([13, Remark 6.2.5]), or, implicitly assumed ([26]) in a variety of works. Next, we define a notion that captures the global joint concurrent tail behavior of random vectors portrayed by many popular multivariate dependence structures, e.g., dependence defined using Gaussian, Marshall-Olkin, or various Archimedean copulas, etc., but not restricted to mere replication of pairwise comparisons of tails.\n\nDEFINITION 3.1 (Mutual asymptotic independence). A random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ with survival copula $\\widehat{C}$ is mutually asymptotically independent if for all $S \\subseteq \\mathbb{I}_{d}$ with $|S| \\geq 2$, we have\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widehat{C}_{S}(u, \\ldots, u)}{\\widehat{C}_{S \\backslash\\{\\ell\\}}(u, \\ldots, u)}=0, \\quad \\forall \\ell \\in S\n$$\n\nwhere we define $0 / 0:=0$. If $\\boldsymbol{Z} \\sim F$ has copula $C$, we interchangeably say $\\boldsymbol{Z}, F, C$ or $\\widehat{C}$ possesses mutual asymptotic independence.\n\nWhen $d=2$, both (2.2) and (3.1) boil down to (2.1) and hence, are equivalent. Assuming $d \\geq 3$ and mutual asymptotic independence, if we take all choices of $S \\subseteq \\mathbb{I}_{d}$ with $|S|=2$, then (3.1) is just a restatement of (2.2), implying pairwise asymptotic independence. We find that mutual asymptotic independence implies pairwise asymptotic independence, which we summarize next.\n\nProposition 3.2. If a random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}, d \\geq 2$ is mutual asymptotically independent, then it is also pairwise asymptotically independent.\n\nThe reverse implication of (3.2) is not necessarily true as we see in the following example, which mimics the consequences for the analogous notions of classical mutual and pairwise independence ([22]).\n\nEXAMPLE 3.3. The difference between pairwise and mutual independence can be shown using an $\\mathbb{R}^{3}$-valued random vector with Bernoulli marginals (cf. [22, Chapter 2]). We take a similar spirit using uniform marginals. Consider i.i.d. uniform [0,1] random distributed random variables $U, V, W$. Then $\\boldsymbol{Z}=(U, V, W)$ is mutually asymptotically independent (cf. Example 3.4) and hence, pairwise asymptotically independent as well. Now consider $\\boldsymbol{Z}=$ $\\left(Z_{1}, Z_{2}, Z_{3}\\right)$ such that\n\n$$\n\\boldsymbol{Z}= \\begin{cases}(U, V, \\min (U, V)), & \\text { with prob. } 1 / 3 \\\\ (U, \\min (U, V), V), & \\text { with prob. } 1 / 3 \\\\ (\\min (U, V), U, V), & \\text { with prob. } 1 / 3\\end{cases}\n$$\n\nFirst note that for $0<z<1$,\n\n$$\nF_{j}(z)=\\mathbb{P}\\left(Z_{j} \\leq z\\right)=2 z / 3+1 / 3\\left[1-(1-z)^{2}\\right]=4 z / 3-z^{2} / 3, \\quad j=1,2,3\n$$\n\nand hence, the $Z_{j}$ 's are identically distributed. If $\\widehat{C}$ denotes the survival copula of $\\boldsymbol{Z}$, then we can check that for any $\\{j, \\ell\\} \\subset\\{1,2,3\\}$,\n\n$$\n\\begin{aligned}\n\\widehat{C}_{j, \\ell}(u, u) & =\\mathbb{P}\\left(Z_{j}>2-\\sqrt{1+3 u}, Z_{\\ell}>2-\\sqrt{1+3 u}\\right) \\\\\n& =\\mathbb{P}(U>2-\\sqrt{1+3 u}, V>2-\\sqrt{1+3 u}) \\\\\n& =(\\sqrt{1+3 u}-1)^{2}=9 u^{2} / 4+o\\left(u^{2}\\right), \\quad u \\downarrow 0\n\\end{aligned}\n$$\n\nHence, $\\boldsymbol{Z}$ exhibits pairwise asymptotic independence. But\n\n$$\n\\begin{aligned}\n\\widehat{C}(u, u, u) & =\\mathbb{P}(U>2-\\sqrt{1+3 u}, V>2-\\sqrt{1+3 u}) \\\\\n& =(\\sqrt{1+3 u}-1)^{2}=9 u^{2} / 4+o\\left(u^{2}\\right), \\quad u \\downarrow 0\n\\end{aligned}\n$$\n\nimplying that $\\boldsymbol{Z}$ does not have mutual asymptotic independence.\n3.1. Examples: Part I. It is instructive to note examples of mutual asymptotic independence in various distributions.\n\nExample 3.4 (Independence). Suppose $C^{\\text {ind }}$ is the independence copula as given in (2.4), then the survival copula for any non-empty subset $S \\subset \\mathbb{I}_{d}$ satisfies\n\n$$\n\\widehat{C}_{S}^{\\text {ind }}(u, \\ldots, u)=u^{|S|}, \\quad 0 \\leq u \\leq 1\n$$\n\nThus, (3.1) holds for all such $S$ with $|S| \\geq 2$ and hence, $C^{\\text {ind }}$ exhibits mutual asymptotic independence.\n\nExample 3.5 (Marshall-Olkin dependence). In Proposition 2.6 we showed that any random vector $\\boldsymbol{Z}$ with dependence given by the generalized Marshall-Olkin survival copula $\\widehat{C}^{\\mathrm{MO}(\\Lambda)}$ as defined in (2.5) is pairwise asymptotically independent. The next proposition shows that it is indeed mutually asymptotically independent.\n\nProposition 3.6. Let the dependence structure of a random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ be given by the generalized Marshall-Olkin survival copula $\\widehat{C}^{\\mathrm{MO}(\\Lambda)}$ as in (2.5) with any choice of parameter set $\\Lambda$. Then $\\boldsymbol{Z}$ possesses mutual asymptotic independence.\n\nNote that, in light of Proposition 3.2, clearly Proposition 2.6 on pairwise asymptotic independence is a direct consequence of Proposition 3.6.\n\n3.2. Examples: Part II. In this section, we discuss examples that are pairwise asymptotically independent but sometimes are not mutually asymptotically independent. This will include a large class of examples from the Archimedean copula and as well the Gaussian copula family.\n3.2.1. Archimedean copula.. Recall the Archimedean copula $C^{\\phi}$ defined in Example 2.7. The following result provides sufficient conditions on the generator $\\phi$ for the random vector with Archimedean copula $C^{\\phi}$ to possess both pairwise and mutual asymptotic independence.\n\nTHEOREM 3.7 (Archimedean copula with mutual asymptotic independence). Let the dependence structure of a random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ be given by an Archimedean copula $C^{\\phi}$ with generator $\\phi$ as in (2.8). Suppose $\\phi^{\\leftarrow}$ is $d$-times continuously differentiable and $(-D)^{j} \\phi^{\\leftarrow}(0)<\\infty \\forall j \\in \\mathbb{I}_{d}$. Then $\\boldsymbol{Z}$ possesses both pairwise and mutual asymptotic independence.\n\nThe proof follows directly from Charpentier and Segers [4, Theorem 4.3]. The Archimedean copulas of Theorem 3.7 have the property that for any subset $S \\subset \\mathbb{I}_{d}$ with $|S| \\geq 2$, the survival copula $\\widetilde{C}_{S}^{\\phi}$ on the $|S|$-dimensional marginal behaves like the independence copula near the tails, i.e.,\n\n$$\n\\widetilde{C}_{S}^{\\phi}(u, \\ldots, u) \\sim u^{\\kappa_{S}}, \\quad u \\downarrow 0\n$$\n\nwhere the upper tail order of $C_{S}$ is $\\kappa_{S}=|S|$ (also follows from [4, Theorem 4.3]). In particular, the upper tail order for $C^{\\phi}$ is $\\kappa=\\kappa_{\\mathbb{I}_{d}}=d$ and hence, these copulas are also \"nearly independent\" (see paragraph below (1.2)); several popular Archimedean copulas models as, e.g., Frank copula, Clayton copula and Ali-Mikhail-Haq copula (cf. [4, Table 1]) fall in this class exhibiting both pairwise and mutual asymptotic independence. In contrast, there are as well Archimedean copulas exhibiting only pairwise asymptotic independence but not mutual asymptotic independence. The following result provides sufficient conditions on the generator $\\phi$ to obtain such Archimedean copulas.\n\nTHEOREM 3.8 (Archimedean copula with only pairwise asymptotic independence). Let the dependence structure of a random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ be given by an Archimedean copula $C^{\\phi}$ with generator $\\phi$ as in (2.8). Suppose $\\phi^{\\prime}(1)=0$ and\n\n$$\nL(u):=-\\phi^{\\prime}(1-u)-u^{-1} \\phi(1-u)\n$$\n\nis a positive function, which is slowly varying at 0 . Then $\\boldsymbol{Z}$ possesses pairwise asymptotic independence but does not possess mutual asymptotic independence.\n\nThe proof follows directly from Charpentier and Segers [4, Theorem 4.6 and Corollary 4.7]. Now, the Archimedean copulas of Theorem 3.8 have a different characteristic in the sense that for any subset $S \\subset \\mathbb{I}_{d}$ with $|S| \\geq 2$, the survival copula $\\widetilde{C}_{S}^{\\phi}$ on the $|S|$-dimensional marginal behaves as\n\n$$\n\\widetilde{C}_{S}^{\\phi}(u, \\ldots, u) \\sim u \\ell(u), \\quad u \\downarrow 0\n$$\n\nwhere $\\ell$ is a slowly varying function at 0 (follows from [4, Corollary 4.7]). Hence, the upper tail order of $C_{S}$ is $\\kappa_{S}=1$ for all $S \\subset \\mathbb{I}_{d}$ with $|S| \\geq 2$. To obtain an example of such a copula, take some parameter $\\theta \\in(0, \\infty)$ and define the generator\n\n$$\n\\phi_{\\theta}(u)=\\frac{1-u}{(-\\log (1-u))^{\\theta}}, \\quad 0 \\leq u \\leq 1\n$$\n\nof an Archimedean copula $C^{\\phi_{\\theta}}$. Then $C^{\\phi_{\\theta}}$ satisfies the assumptions of Theorem 3.8, resulting in an Archimedean copula with pairwise but not mutual asymptotic independence; we refer to Charpentier and Segers [4, Table 1].\n\n3.2.2. Gaussian copula.. In Example 2.8 we observed that any random vector with Gaussian copula having a positive definite correlation matrix has pairwise asymptotic independence. Interestingly, not all such models will have mutual asymptotic independence. The following theorem provides the exact condition for this.\n\nTHEOREM 3.9. Let the dependence structure of a random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ be given by a Gaussian copula $C^{\\Sigma}$ with positive-definite correlation matrix $\\Sigma$. Then $\\boldsymbol{Z}$ exhibits mutual asymptotic independence if and only if $\\Sigma_{S}^{-1} \\mathbf{1}_{|S|}>\\mathbf{0}_{|S|}$ for all non-empty sets $S \\subseteq \\mathbb{I}_{d}$.\n\nThe proof is quite involved, requiring a few auxiliary results based on the recently derived knowledge ( $[11,12]$ ) on the asymptotic behavior of tail probabilities of a multivariate distribution with identically Pareto margins and Gaussian copula $C^{\\Sigma}$. Curiously, the ingredients of the proof allow us to find the tail asymptotics of the survival copula of any $|S|$-dimensional marginal in terms of its tail order.\n\nProposition 3.10. Let $C^{\\Sigma}$ be a Gaussian copula with positive-definite correlation matrix $\\Sigma$. Then for any subset $S \\subset \\mathbb{I}_{d}$ with $|S| \\geq 2$, we have as $u \\downarrow 0$,\n\n$$\n\\widehat{C}_{S}^{\\Sigma}(u, \\ldots, u) \\sim u^{\\kappa_{S}} \\ell_{S}(u)\n$$\n\nwhere $\\ell_{S}$ is slowly varying at 0 and\n\n$$\n\\kappa_{S}=\\min _{\\left\\{z \\in \\mathbb{R}^{|S|} ; z \\geq 1_{S}\\right\\}} z^{\\top} \\Sigma_{S}^{-1} z\n$$\n\nRemark 3.11. A few interesting features of Proposition 3.10 and related results (see Appendix A) are to be noted here.\n\n1. Although Proposition 3.10 only gives the tail order of $\\widehat{C}_{S}^{\\Sigma}$, in fact, the exact tail asymptotics for $\\widehat{C}_{S}^{\\Sigma}$ is available in Proposition A.3.\n2. The upper tail order $\\kappa_{S}$ in (3.2) is obtained as a solution to a quadratic programming problem as given in Lemma A.1.\n3. With respect to (3.2), for subsets $S, T \\subset \\mathbb{I}_{d}$ with $S \\subsetneq T$ and $|S| \\geq 2$, it is possible to have (i) $\\kappa_{S}<\\kappa_{T}$, (ii) $\\kappa_{S}=\\kappa_{T}$, with $\\ell_{S}(u) \\sim c \\ell_{T}(u), u \\downarrow 0$ for $c>0$, and (iii) $\\kappa_{S}=\\kappa_{T}$, with $\\ell_{S}(u)=o\\left(\\ell_{T}(u)\\right), u \\downarrow 0$. In Example 3.12, we can observe both (i) and (ii) holding under different assumptions; an example for (iii) with Pareto margins and Gaussian copula is available in [11, Remark 5].\n\nExample 3.12. For the purpose of illustration, we provide a positive-definite correlation matrix (with $d=3$ ) for a Gaussian copula parametrized by a single parameter $\\rho$ which exhibits mutual asymptotic independence for only certain values of $\\rho$ and only pairwise asymptotic independence for other feasible values; see Das and Fasen-Hartmann [11, Ex. 1(b)] for further details. Throughout, we denote by $\\ell_{j}(u), j \\in \\mathbb{N}$, a slowly varying function at 0 . Consider the Gaussian copula $C^{\\Sigma}$ with correlation matrix\n\n$$\n\\Sigma=\\left(\\begin{array}{ccc}\n1 & \\rho & \\sqrt{2} \\rho \\\\\n\\rho & 1 & \\sqrt{2} \\rho \\\\\n\\sqrt{2} \\rho & \\sqrt{2} \\rho & 1\n\\end{array}\\right)\n$$\n\nwhere $\\rho \\in((1-\\sqrt{17}) / 8,(1+\\sqrt{17}) / 8) \\approx(-0.39,0.64)$ which ensures the positive definiteness of $\\Sigma$. Clearly, pairwise asymptotic independence holds for all such $\\rho$ values.\n\n(i) Suppose $\\rho<1 /(2 \\sqrt{2}-1) \\approx 0.55$. Then one can check that $\\Sigma^{-1} \\mathbf{1}>\\mathbf{0}$, and hence, mutual asymptotic independence holds as well. In fact, we can find the behavior of the survival copula (using Proposition A. 3 or, Das and Fasen-Hartmann [11, Example 1(b)]) as follows: as $u \\downarrow 0$,\n\n$$\n\\widehat{C}^{\\Sigma}(u, u, u) \\sim u^{\\frac{3-1+\\sqrt{2}-1) \\rho}{1+\\rho-1 \\rho^{2}}} \\ell_{1}(u)\n$$\n\nWe also find that as $u \\downarrow 0$,\n\n$$\n\\begin{aligned}\n& \\widehat{C}_{13}^{\\Sigma}(u, u)=\\widehat{C}_{23}^{\\Sigma}(u, u) \\sim u^{\\frac{2}{1+\\sqrt{2} \\rho}} \\ell_{2}(u), \\quad \\text { and } \\\\\n& \\widehat{C}_{12}^{\\Sigma}(u, u) \\sim u^{\\frac{2}{1+\\rho}} \\ell_{3}(u)\n\\end{aligned}\n$$\n\n(ii) On the other hand, if $\\rho \\geq 1 /(2 \\sqrt{2}-1)$, then $\\Sigma^{-1} \\mathbf{1} \\ngtr \\mathbf{0}$ and the copula does not have mutual asymptotic independence. Note that in this case, the behavior of the twodimensional marginal survival copulas will still be given by (3.4) and (3.5), but the tail behavior as seen in (3.3) does not hold anymore. Now, as $u \\downarrow 0$, we have\n\n$$\n\\widehat{C}^{\\Sigma}(u, u, u) \\sim u^{\\frac{2}{1+\\rho}} \\ell_{4}(u)\n$$\n\nIn fact we can check that $\\ell_{4}(u) \\sim \\beta \\ell_{3}(u)$ for some constant $\\beta>0$ (cf. [11, Example $1(b)]$ )), and hence\n\n$$\n\\widehat{C}^{\\Sigma}(u, u, u) \\sim \\beta \\widehat{C}_{12}^{\\Sigma}(u, u), \\quad u \\downarrow 0\n$$\n\nalso verifying that mutual asymptotic independence does indeed not hold here.\n4. $\\boldsymbol{k}$-wise asymptotic independence. The fact that some multivariate models exhibit pairwise asymptotic independence yet not mutual asymptotic independence naturally prompts the inquiry into the existence of models that lie in between these two notions. The following definition provides an answer.\n\nDEFINITION 4.1 ( $k$-wise asymptotic independence). A random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ with survival copula $\\widehat{C}$ is $k$-wise asymptotically independent for a fixed $k \\in\\{2, \\ldots, d\\}$, if for all $S \\subseteq \\mathbb{I}_{d}$ with $2 \\leq|S| \\leq k$, we have\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widehat{C}_{S}(u, \\ldots, u)}{\\widehat{C}_{S \\backslash\\{\\ell\\}}(u, \\ldots, u)}=0, \\quad \\forall \\ell \\in S\n$$\n\nwhere we define $0 / 0:=0$. If $\\boldsymbol{Z} \\sim F$ has copula $C$, we interchangeably say $\\boldsymbol{Z}, F, C$ or $\\widehat{C}$ possesses $k$-wise asymptotic independence.\n\nNote that for a $d$-dimensional copula, $d$-wise asymptotic independence is the same as mutual asymptotic independence (and of course 2-wise is the same as pairwise). Again, following Proposition 3.2, we may check that mutual asymptotic independence indeed implies $k$-wise asymptotic independence for all $k \\in\\{2, \\ldots, d\\}$. The reverse of the previous implication is, of course, not true; the examples in the following section also show this.\n\nObviously, an equivalent characterization of $k$-wise asymptotic independence is the following.\n\nLEMMA 4.2. A random vector $\\boldsymbol{Z}$ in $\\mathbb{R}^{d}$ is $k$-wise asymptotically independent if and only if for all $S \\subseteq \\mathbb{I}_{d}$ with $|S|=k$, the random vector $\\boldsymbol{Z}_{S}$ in $\\mathbb{R}^{k}$ is mutually asymptotically independent.\n\n4.1. Examples. Indeed, within the class of Archimedean copulas as well as the class of Gaussian copulas with dimensions $d>2$, we find examples of models which exhibit $k$ wise asymptotic independence, but not $(k+1)$-wise asymptotic independence given any $k \\in$ $\\{2, \\ldots, d-1\\}$. Consequently, these models are also not mutually asymptotically independent. Let us begin with an investigation of a particular Archimedean copula.\n\nEXAMPLE 4.3 (ACIG copula). This Archimedean copula based on the Laplace transform (LT) of an Inverse Gamma distribution, called ACIG copula in short, was introduced in Hua and Joe [23], i.e, if $Y=X^{-1}$ and $X \\sim \\operatorname{Gamma}(\\alpha, 1)$ for $\\alpha>0$, then the generator of this Archimedean copula is given by the LT of $Y$. The expression of the generator includes the Bessel function of the second kind. Closed-form expressions of the copula $C^{\\phi}$ and survival copula $\\widetilde{C}^{\\phi}$ are not easy to write down; nevertheless, from computations in Hua and Joe [23, Example 4] and Hua, Joe and Li [24, Example 4.4], we can conclude that for any $d \\geq 2$, the survival copula of the ACIG copula with parameter $\\alpha>0$ has the following asymptotic behavior:\n\n$$\n\\widetilde{C}^{\\phi}(u, \\ldots, u) \\sim \\beta_{d} u^{\\kappa_{d}}, \\quad u \\downarrow 0\n$$\n\nwhere $\\kappa_{d}=\\max \\{1, \\min \\{\\alpha, j\\}\\}$ and $\\beta_{d}>0$ is a positive constant. Here, $\\kappa_{d}$ is the tail order of the copula. Therefore, if $\\alpha \\leq 1$ then $\\kappa_{d}=1$ for all $d \\geq 2$, and if $\\alpha>1$, then $\\kappa_{d}=\\min (\\alpha, d)$. Note that by the exchangeability property of Archimedean copulas and (4.1), we know that for any $S \\subset \\mathbb{I}_{d}$ with $|S| \\geq 2$,\n\n$$\n\\widetilde{C}_{S}^{\\phi}(u, \\ldots, u) \\sim \\beta_{|S|} u^{\\kappa_{|S|}}, \\quad u \\downarrow 0\n$$\n\nThus, we may conclude that for an ACIG copula with parameter $\\alpha>0$, the following holds:\n(i) If $0<\\alpha \\leq 1$, then it exhibits asymptotic upper tail dependence.\n(ii) If $1<\\alpha \\leq d-1$, then it exhibits pairwise asymptotic independence but not mutual asymptotic independence. If additionally $k-1<\\alpha \\leq k$ for $k \\in\\{2, \\ldots, d-1\\}$, then the ACIG still exhibits $i$-wise asymptotic independence for all $i \\in\\{2, \\ldots, k\\}$, but not $(k+1)-$ wise asymptotic independence.\n(iii) If $\\alpha>d-1$, then it exhibits $k$-wise asymptotic independence for all $k \\in\\{2, \\ldots, d\\}$, and hence mutual asymptotic independence as well.\n\nExample 4.4 (Gaussian copula). The Gaussian copula has been popular in modeling dependence in a wide variety of applications. It turns out that a class of Gaussian copula models is also able to capture the presence of $k$-wise asymptotic independence and not $(k+$ 1)-wise asymptotic independence. This is demonstrated in the following result.\n\nTHEOREM 4.5. For any $k \\in\\{2, \\ldots, d-1\\}$ and $S_{1}, S_{2} \\subseteq\\{1, \\ldots, d\\}$ with $\\left|S_{1}\\right|=k$ and $\\left|S_{2}\\right|=k+1$, there exists a random vector $\\boldsymbol{Z} \\in \\mathbb{R}^{d}$ with a Gaussian copula $C^{\\Sigma}$ and a positivedefinite correlation matrix $\\Sigma$, such that $\\boldsymbol{Z}$ exhibits $k$-wise asymptotic independence but not $(k+1)$-wise asymptotic independence and for any $x>0$,\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widetilde{C}_{S_{2}}^{\\Sigma}(u, \\ldots, u, x u, u, \\ldots, u)}{\\widetilde{C}_{S_{1}}^{\\Sigma}(u, \\ldots, u)}=1\n$$\n\nwhere $x u$ is placed at the unique element in $S_{2} \\backslash S_{1}$.\nThis theorem not only provides the existence of a $k$-wise asymptotically independent Gaussian copula, but it also gives the striking feature of the copula behavior in (4.2) where, surprisingly the value of $x$ has no influence. For the derivation of worst-case measures for risk contagion under distributional ambiguity in the next section, this result turns out to be indispensable.\n\nREMARK 4.6. It is important to mention here that although most popular copula families are bivariate in nature, Joe [25, Chapter 4] lists multiple extensions of bivariate copulas to general high dimensions; many such copulas can be explored for creating models with particular levels of asymptotic independence as necessitated by the context.\n5. Implication on risk management under distributional ambiguity. In financial risk management, a variety of risk measures are used to assess the risk contagion between different financial products including stocks, bonds and equities. Such contagion or systemic risk measures are often based on conditional probabilities and range from computing regular conditional tail probabilities to CoVaR, marginal expected shortfall (MES), marginal mean excess (MME), and more; see $[1,8]$ for details.\n\nHere, we focus on two such measures of risk contagion based on specific conditional tail probabilities and conditional tail quantiles. First, recall that for a random variable $Z$, the Value-at-Risk or VaR at level $1-\\gamma \\in(0,1)$ is defined as\n\n$$\n\\operatorname{VaR}_{\\gamma}(Z):=\\inf \\{y \\in \\mathbb{R}: \\mathbb{P}(Z>y) \\leq \\gamma\\}=\\inf \\{y \\in \\mathbb{R}: \\mathbb{P}(Z \\leq y) \\geq 1-\\gamma\\}\n$$\n\nthe $(1-\\gamma)$-quantile of $Z$ where $\\inf \\emptyset:=\\infty$ (cf. [16]). If $Z \\sim F$ is continuous, then $F$ is invertible with inverse $F^{-1}$ and for $0<\\gamma<1$ we have $\\operatorname{VaR}_{\\gamma}(Z)=F^{-1}(1-\\gamma)$.\n\nConsider the returns from a portfolio of $d>1$ stocks being given by the random vector $\\boldsymbol{Z}=\\left(Z_{1}, \\ldots, Z_{d}\\right) \\sim F$. Suppose we are interested in measuring the risk of $Z_{1}$ having an extremely large value, given that all variables in some non-empty subset $J \\subset \\mathbb{I}_{d} \\backslash\\{1\\}$ with $|J|=\\ell$ are at extremely high levels. This can be captured via the following conditional tail probability\n\n$$\n\\mathbb{P}\\left(Z_{1}>t \\mid Z_{j}>t, \\forall j \\in J\\right)\n$$\n\nas $t \\rightarrow \\infty$. Alternatively, for a level $\\gamma \\in(0,1)$, we are interested in the risk measure\n\n$$\n\\mathrm{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right):=\\mathbb{P}\\left(Z_{1}>\\operatorname{VaR}_{\\gamma}\\left(Z_{1}\\right) \\mid Z_{j}>\\operatorname{VaR}_{\\gamma}\\left(Z_{j}\\right), \\forall j \\in J\\right)\n$$\n\nas $\\gamma \\rightarrow 0$. Note that (5.1) and (5.2) are equivalent if all the marginal random variables $Z_{1}, \\ldots, Z_{d}$ are identically distributed. For convenience, we will focus on the measure $\\mathrm{CTP}_{\\gamma}$ as defined in (5.2).\n\nA second measure of risk contagion we are interested in is a generalization of the VaR to the multivariate setting given by the Contagion Value-at-Risk or CoVaR at confidence level $\\left(\\gamma_{1}, \\gamma_{2}\\right)$ for $\\gamma_{1}, \\gamma_{2} \\in(0,1)$ defined as\n\n$$\n\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right):=\\inf \\left\\{z \\in \\mathbb{R}_{+}: \\mathbb{P}\\left(Z_{1}>z \\mid Z_{j}>\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{j}\\right), \\forall j \\in J\\right) \\leq \\gamma_{1}\\right\\}\n$$\n\nThe risk measure CoVaR was introduced in the bivariate setting for $J=2$ to capture risk contagion, as well as systemic risk by Adrian and Brunnermeier [1] where they used the conditioning event to be $Z_{2}=\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{2}\\right)$; this was later modified by Girardi and Erg\u00fcn [17] to $Z_{2}>\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{2}\\right)$ with the restriction that $\\gamma_{1}=\\gamma_{2}$; this latter definition has been widely used in dependence modeling ( $[12,19,32,35])$ and is generalized in our definition given in (5.3).\n\nIn risk management applications, computing quantities like CTP and CoVaR requires knowledge of the joint distribution of the risk vector $\\boldsymbol{Z}$. Even if the univariate distributions of all the marginal variables can be estimated, the joint distribution often remains unknown and relatively more involved for estimation purposes. An approach often used is to provide a worst-case value for such risk measures under certain constraints on the joint distribution of the variables. Naturally, for such tail risk measures, constraints can be provided in terms of their joint asymptotic tail behavior, including pairwise, mutual or $k$-wise asymptotic independence. It turns out that under different constraints, we may obtain different tail behavior for the worst-case measures. To further this discussion, let us define $\\mathcal{P}$ to be the class of all probability distributions in $\\mathbb{R}^{d}$ with continuous marginal distributions.\n\nDefinition 5.1. For $k \\in\\{2, \\ldots, d\\}$ define the classes of distributions\n\n$$\n\\mathcal{P}_{k}:=\\{F \\in \\mathcal{P}: F \\text { possesses } k \\text {-wise asymptotic independence }\\}\n$$\n\nand similarly the restrictions to distributions with Gaussian copulas\n\n$$\n\\mathcal{N}_{k}:=\\mathcal{P}_{k} \\cap\\left\\{F \\in \\mathcal{P}: F \\text { has a Gaussian copula } C^{\\Sigma} \\text { with } \\Sigma \\text { positive definite }\\right\\}\n$$\n\nNote that $\\mathcal{P}_{2}$ models the class of pairwise asymptotically independent random vectors, whereas $\\mathcal{P}_{d}$ models the class of mutually asymptotically independent random vectors. By Definition 4.1, it is easy to check that\n\n$$\n\\mathcal{P} \\supseteq \\mathcal{P}_{2} \\supseteq \\mathcal{P}_{3} \\supseteq \\cdots \\supseteq \\mathcal{P}_{d} \\quad \\text { and } \\quad \\mathcal{P} \\supseteq \\mathcal{N}_{2} \\supseteq \\mathcal{N}_{3} \\supseteq \\cdots \\supseteq \\mathcal{N}_{d}\n$$\n\nFurthermore, these classes are non-empty, since $\\mathcal{N}_{k} \\neq \\emptyset$ by Theorem 4.5 and $\\mathcal{N}_{k} \\subseteq \\mathcal{P}_{k}$.\nSince the joint distributions are unknown we may want to find the worst case CTP or CoVaR in such cases where $F \\in \\mathcal{P}_{k} \\subset \\mathcal{P}$ or $F \\in \\mathcal{N}_{k}, k \\in\\{2, \\ldots, d\\}$. First, we present the result for the CTP.\n\nTHEOREM 5.2. Let $\\boldsymbol{Z}=\\left(Z_{1}, \\ldots, Z_{d}\\right) \\sim F$ where $d \\geq 2$ and all the variables have continuous marginal distributions. Furthermore, suppose $J \\subset \\mathbb{I}_{d} \\backslash\\{1\\}$ with $|J|=\\ell$.\n(a) If $k \\in\\{\\ell+1, \\ldots, d\\}$, then\n\n$$\n\\sup _{F \\in \\mathcal{N}_{k}} \\lim _{\\gamma \\downarrow 0} \\operatorname{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)=\\sup _{F \\in \\mathcal{P}_{k}} \\lim _{\\gamma \\downarrow 0} \\operatorname{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)=0\n$$\n\n(b) If $k \\in\\{2, \\ldots, \\ell\\}$, then\n\n$$\n\\sup _{F \\in \\mathcal{N}_{k}} \\lim _{\\gamma \\downarrow 0} \\operatorname{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)=\\sup _{F \\in \\mathcal{P}_{k}} \\lim _{\\gamma \\downarrow 0} \\operatorname{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)=1\n$$\n\nThe results indicate a qualitatively different behavior of the worst-case CTP depending on whether the tail dependence exhibits $k$-wise asymptotic independence with $k>|J|$ vis-a-vis $k \\leq|J|$. When $k>|J|, \\operatorname{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)$ converges to 0 as $\\gamma \\downarrow 0$, suggesting that extreme large losses of $Z_{j}$ for all $j \\in J$ have a negligible influence on extreme large losses of $Z_{1}$. In contrast, when $k \\leq|J|$, there exists a $k$-wise asymptotically independent distribution function $F$, which is as well pairwise asymptotically independent, such that extreme large losses of $Z_{j}$ for all $j \\in J$ result, with a probability converging to 1 , in an extremely large loss of $Z_{1}$. In particular, for the Gaussian copula that is an astonishing result because it is in contrast to the belief that there are no joint extremes. This shows that for measuring risk contagion it is important to distinguish between these different concepts of tail independence and assuming an improper notion of asymptotic independence for our risk portfolio may lead to either underestimation or overestimation of the risk contagion.\n\nIn the following, we investigate the asymptotic behavior of the measure CoVaR. For technical reasons, we restrict the class $\\mathcal{P}_{k}$ slightly, in particular we will assume that $F_{1}$, the distribution of $Z_{1}$, is Pareto distributed, i.e., $F_{1}(z)=1-z^{-\\alpha}, z \\geq 1$, for some $\\alpha>0$.\n\nDefinition 5.3. Suppose $\\mathcal{P}^{*}:=\\left\\{F \\in \\mathcal{P}: F_{1}\\right.$ is Pareto distributed $\\}$. For $k \\in\\{2, \\ldots, d\\}$ define the classes\n\n$$\n\\mathcal{P}_{k}^{*}:=\\mathcal{P}_{k} \\cap\\left\\{F \\in \\mathcal{P}^{*}: \\sup _{\\gamma \\in\\left(0, x^{-1}\\right]} \\frac{\\widehat{C}_{S}(x \\gamma, \\gamma, \\ldots, \\gamma)}{\\widehat{C}_{S}(\\gamma, \\gamma, \\ldots, \\gamma)}<\\infty, \\forall S \\subseteq \\mathbb{I}_{d}, \\forall x \\geq 1\\right\\} \\subseteq \\mathcal{P}_{k}\n$$\n\nand\n\n$$\n\\mathcal{N}_{k}^{*}:=\\mathcal{N}_{k} \\cap \\mathcal{P}^{*} \\subseteq \\mathcal{N}_{k}\n$$\n\nREMARK 5.4. Instead of assuming that $F_{1}$ follows a Pareto distribution, it is possible to consider a broader class, allowing $F_{1}$ to have a regularly varying tail. However, this approach makes the proofs more technical without providing any further valuable insights, hence we have exhibited our results for the smaller class $\\mathcal{P}_{k}^{*}$ for the purpose of exposition.\n\nAlthough we reduce the class $\\mathcal{P}_{k}$ to $\\mathcal{P}_{k}^{*}$, it still remains quite large and contains, in particular, $k$-wise asymptotically independent Gaussian copulas (with $F_{1}$ Pareto distributed).\n\nLEMMA 5.5. $\\mathcal{N}_{k}^{*} \\subseteq \\mathcal{P}_{k}^{*}$ for $k \\in\\{2, \\ldots, d\\}$.\nBy restricting our consideration to the sets $\\mathcal{P}_{k}^{*}$ and $\\mathcal{N}_{k}^{*}$, we derive the subsequent result concerning the asymptotic behavior of the CoVaR.\n\nTHEOREM 5.6. Let $\\boldsymbol{Z}=\\left(Z_{1}, \\ldots, Z_{d}\\right) \\sim F$ where $d \\geq 2$ and all the variables have continuous marginal distributions and $F_{1}$ is a Pareto distribution. Furthermore, let $J \\subset \\mathbb{I}_{d} \\backslash\\{1\\}$ with $|J|=\\ell$.\n(a) If $k \\in\\{\\ell+1, \\ldots, d\\}$, then for any $\\gamma_{1} \\in(0,1)$,\n\n$$\n\\sup _{F \\in \\mathcal{N}_{k}^{*}} \\lim _{\\gamma_{2} \\downarrow 0} \\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)}=\\sup _{F \\in \\mathcal{P}_{k}^{*}} \\lim _{\\gamma_{2} \\downarrow 0} \\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)}=0\n$$\n\n(b) If $k \\in\\{2, \\ldots, \\ell\\}$, then for any $\\gamma_{1} \\in(0,1)$,\n\n$$\n\\sup _{F \\in \\mathcal{N}_{k}^{*}} \\lim _{\\gamma_{2} \\downarrow 0} \\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)}=\\sup _{F \\in \\mathcal{P}_{k}^{*}} \\lim _{\\gamma_{2} \\downarrow 0} \\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)}=\\infty\n$$\n\nAkin to the case of finding for the worst-case CTP, we observe that the worst-case CoVaR also has a qualitatively different behavior depending if the tail dependence exhibits $k$-wise asymptotic independence with $k>|J|$, or with $k \\leq|J|$. When $k>|J|$, the ratio $\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right) / \\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)$ converges to 0 , reflecting that $\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)$ increases at a negligible rate in comparison to $\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)$ as $\\gamma_{2} \\downarrow 0$ and that $\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)$ is relatively small, i.e, the required risk reserve capital is low. But if $k \\leq|J|$, there exists a $F \\in \\mathcal{N}_{k}^{*} \\subseteq \\mathcal{P}_{k}^{*}$ where $\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right) / \\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)$ converges to $\\infty$, so that $\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)$ may increase much faster to $\\infty$ than $\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)$ as $\\gamma_{2} \\downarrow 0$, giving a relatively high $\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)$ and a higher reserve risk capital requirement.\n\nREMARK 5.7. Computations analogous to the ones carried out in this section, can also be done for other measures of risk contagion, for example, the marginal expected shortfall (MES), or, the marginal mean excess (MME) [3, 8]; but, similar to the case of computing CoVaR, we need to restrict $\\mathcal{P}_{k}$ to smaller classes satisfying various technical conditions. We leave these pursuits for the interested researchers to explore in the future.\n6. Conclusion. In this paper, we provide a notion of truly multivariate asymptotic independence that is useful in comparing extreme events in different dimensions beyond mere pairwise comparisons. This parallels the dichotomy of mutual independence vis-a-vis pairwise independence for multivariate random vectors. We believe this new notion also provides an alternate pathway for characterizing extremal dependence for high-dimensional problems relating to tail events. We have illustrated using examples of particular copula models, including a few from the Archimedean family along with the Gaussian and the Marshall-Olkin copula. The copulas considered often exhibit at least pairwise asymptotic independence if\n\nnot mutual asymptotic independence. For both Archimedean and Gaussian copulas, we presented examples exhibiting not only mutual asymptotic independence but as well exhibiting only pairwise asymptotic independence but not mutual asymptotic independence. In particular, for the Gaussian copula, this result is quite striking since it is in contrast to the common belief that the Gaussian copula does not allow joint extremes. We have also introduced the concept of $k$-wise asymptotic independence which generalizes the two notions and brings them under the same umbrella. Here we have shown that for any $k \\in\\{2, \\ldots, d\\}$ there exists a $k$-wise asymptotic independent Gaussian copula (which is not $(k+1)$-wise asymptotically independent if $k<d$ ). Assumptions of different notions of asymptotic tail independence significantly impact measures of risk contagion within a financial system, such as conditional tail probabilities (CTP) or contagion value-at-risk (CoVaR), depending on the specific context. Overlooking these concepts and assuming only pairwise asymptotic independence for models could lead to an underestimation of risks.", "tables": {}, "images": {}}, {"section_id": 1, "text": "# REFERENCES \n\n[1] Adrian, T. and Brunnermeier, M. K. (2016). CoVaR. Am. Econom. Rev. 106 1705-41.\n[2] BalkeMA, G. and Nolde, N. (2010). Asymptotic independence for unimodal densities. Adv. in Appl. Probab. 42 411-432.\n[3] Cai, J.-J., Einmahl, J. H. J., de Haan, L. and Zhou, C. (2015). Estimation of the marginal expected shortfall: the mean when a related variable is extreme. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 77 417-442.\n[4] Charpentier, A. and Segers, J. (2009). Tails of multivariate Archimedean copulas. J. Multivariate Anal. 100 1521-1537.\n[5] Coles, S. G., Heffernan, J. E. and Tawn, J. A. (1999). Dependence measures for extreme value analyses. Extremes 2 339-365.\n[6] Cuadras, C. M. and Aug\u00e9, J. (1981). A continuous general multivariate distribution and its properties. Comm. Statist. Theory Methods 10 339-353.\n[7] Das, B., Embrechts, P. and Fasen, V. (2013). Four Theorems and a Financial Crisis. The International Journal of Approximate Reasoning 54 701-716.\n[8] Das, B. and Fasen-Hartmann, V. (2018). Risk contagion under regular variation and asymptotic tail independence. J. Multivariate Anal. 165 194-215.\n[9] Das, B., FASEN-HARTMANn, V. and Kl\u00fcPPELBERG, C. (2022). Tail probabilities of random linear functions of regularly varying random vectors. Extremes 25 721-758.\n[10] Das, B. and Fasen-Hartmann, V. (2023). Aggregating heavy-tailed random vectors: from finite sums to L\u00e9vy processes. arxiv:2301.10423.\n[11] Das, B. and FASEN-HARTMANn, V. (2024). On heavy-tailed risks under Gaussian copula: the effects of marginal transformation. J. Multivariate Anal. 202 Paper No. 105310.\n[12] Das, B. and FASEN-HARTMANn, V. (2025). Measuring risk contagion in financial networks with CoVaR. Finance and Stochastics. to appear.\n[13] de Haan, L. and Ferreira, A. (2006). Extreme Value Theory: An Introduction. Springer-Verlag, New York.\n[14] Durante, F. and Semp, C. (2016). Principles of copula theory. CRC Press, Boca Raton, FL.\n[15] DurRetT, R. (1991). Probability: Theory and Examples. Wadsworth \\& Brooks/Cole, Pacific Grove, California.\n[16] Embrechts, P., Hofert, M. and Chavez-Demoulin, V. (2024). Risk revealed-cautionary tales, understanding and communication. Cambridge University Press, Cambridge.\n[17] Girardi, G. and Erg\u00fcn, A. T. (2013). Systemic risk measurement: Multivariate GARCH estimation of CoVaR. J. Bank. Finance 37 3169-3180.\n[18] Guillou, A., Padoan, S. A. and Rizzelli, S. (2018). Inference for asymptotically independent samples of extremes. J. Multivariate Anal. 167 114-135.\n[19] H\u00e4rdle, W. K., Wang, W. and Yu, L. (2016). TENET: Tail-Event driven NETwork risk. J. Econom. 192 499-513.\n[20] Hashorva, E. and H\u00fcsler, J. (2002). On asymptotics of multivariate integrals with applications to records. Stoch. Models 18 41-69.\n[21] Heffernan, J. (2000). A directory of coefficients of tail dependence. Extremes 3 279-290.\n\n[22] HogG, R. V., McKeAn, J. W. and Craig, A. T. (2013). Introduction to Mathematical Statistics, 7 ed. Pearson Education.\n[23] HuA, L. and JoE, H. (2011). Tail order and intermediate tail dependence of multivariate copulas. J. Multivariate Anal. 102 1454-1471.\n[24] HuA, L., JoE, H. and Li, H. (2014). Relations between hidden regular variation and the tail order of copulas. J. Appl. Prob 51 37-57.\n[25] JoE, H. (2015). Dependence modeling with copulas. Monographs on Statistics and Applied Probability 134. CRC Press, Boca Raton, FL.\n[26] Lalancette, M., Engelke, S. and Volgushev, S. (2021). Rank-based estimation under asymptotic dependence and independence, with applications to spatial extremes. Ann. Statist. 492552 - 2576.\n[27] Lauritzen, S. L. (2004). Graphical models, 2. ed. Clarendon Press, Oxford.\n[28] LEdFORD, A. W. and Tawn, J. A. (1996). Statistics for near independence in multivariate extreme values. Biometrika 83 169-187.\n[29] LEdFORD, A. W. and Tawn, J. A. (1998). Concomitant tail behaviour for extremes. Adv. in Appl. Probab 30 197-215.\n[30] LehtomAa, J. and Resnick, S. I. (2020). Asymptotic independence and support detection techniques for heavy-tailed multivariate data. Insurance Math. Econom. 93 262-277.\n[31] Lin, J. and Li, X. (2014). Multivariate Generalized Marshall-Olkin Distributions and Copulas. Methodol. Comput. Appl. Probab. 53-78.\n[32] MainIK, G. and Schaanning, E. (2014). On dependence consistency of CoVaR and some other systemic risk measures. Stat. Risk Model. 31 49-77.\n[33] McNeil, A. J. and Ne\u0160LeHov\u00c1, J. (2009). Multivariate Archimedean copulas, $d$-monotone functions and $\\ell_{1}$-norm symmetric distributions. Ann. Statist. 373059 - 3097.\n[34] NelSEN, R. B. (2006). An Introduction to Copulas, Second ed. Springer, New York.\n[35] Nolde, N., Zhou, C. and Zhou, M. (2022). An extreme value approach to CoVaR estimation. Submitted. https://arxiv.org/abs/2201.00892\n[36] Ramos, A. and Ledford, A. W. (2009). A new class of models for bivariate joint tails. J. Roy. Statist. Soc. Ser. B 71 219-241.\n[37] ResniCK, S. I. (2002). Hidden regular variation, second order regular variation and asymptotic independence. Extremes 5 303-336.\n[38] Sibuya, M. (1960). Bivariate extreme statistics. Ann. Inst. Stat. Math. 11 195-210.\n[39] SkLAR, M. (1959). Fonctions de r\u00e9partition \u00e0 n dimensions et leurs marges. In Annales de l'ISUP 8 229231.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# APPENDIX A: PROOFS\n## A.1. Proofs of Section 2.\n\nProof of Proposition 2.6. Consider an MO survival copula with parameter set $\\Lambda=$ $\\left\\{\\lambda_{S}: \\emptyset \\neq S \\subseteq \\mathbb{I}_{d}\\right\\}$. We need to check (2.2) for any distinct $j, \\ell \\in \\mathbb{I}_{d}$. W.l.o.g. assume $j=$ $1, \\ell=2$. Then\n\n$$\n\\widetilde{C}_{\\{1,2\\}}^{\\mathrm{MO}(\\Lambda)}(u, u)=\\widetilde{C}^{\\mathrm{MO}(\\Lambda)}(u, u, 1, \\ldots, 1)=u^{\\eta^{*}}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\eta^{*} & =\\sum_{\\substack{\\delta \\subseteq I_{d} \\\\\n1 \\in S, 2 \\notin S}} \\eta_{1}^{S}+\\sum_{\\substack{\\delta \\subseteq I_{d} \\\\\n1 \\notin S, 2 \\in S}} \\eta_{2}^{S}+\\sum_{\\substack{\\delta \\subseteq I_{d} \\\\\n1,2 \\in S}} \\max \\left\\{\\eta_{1}^{S}, \\eta_{2}^{S}\\right\\} \\\\\n& =\\sum_{\\substack{\\delta \\subseteq I_{d} \\\\\n1 \\in S, 2 \\notin S}} \\frac{\\lambda_{S}}{\\Delta_{1}}+\\sum_{\\substack{\\delta \\subseteq I_{d} \\\\\n1 \\notin S, 2 \\in S}} \\frac{\\lambda_{S}}{\\Delta_{2}}+\\sum_{\\substack{\\delta \\subseteq I_{d} \\\\\n1,2 \\in S}} \\max \\left\\{\\frac{\\lambda_{S}}{\\Delta_{1}}, \\frac{\\lambda_{S}}{\\Delta_{2}}\\right\\}\n\\end{aligned}\n$$\n\nwith $\\Delta_{j}=\\sum_{\\substack{J \\subseteq I_{d} \\\\ j \\in J}} \\lambda_{S}$ for $j=1,2$. W.l.o.g. assume $\\Delta_{1} \\geq \\Delta_{2}$. Then\n\n$$\n\\eta^{*} \\geq \\sum_{\\substack{\\delta \\subseteq I_{d} \\\\ 1 \\in S, 2 \\notin S}} \\frac{\\lambda_{S}}{\\Delta_{1}}+\\sum_{\\substack{\\delta \\subseteq I_{d} \\\\ 1 \\notin S, 2 \\in S}} \\frac{\\lambda_{S}}{\\Delta_{1}}+\\sum_{\\substack{\\delta \\subseteq I_{d} \\\\ 1,2 \\in S}} \\frac{\\lambda_{S}}{\\Delta_{1}}\n$$\n\n$$\n\\begin{aligned}\n& =\\frac{1}{\\Delta_{1}}\\left(\\sum_{\\substack{s \\subseteq t_{d} \\\\\n1 \\in S}} \\lambda_{S}+\\sum_{\\substack{s \\subseteq t_{d} \\\\\n1 \\notin S, s \\in S}} \\lambda_{S}\\right) \\\\\n& =\\frac{1}{\\Delta_{1}}\\left(\\Delta_{1}+\\sum_{\\substack{s \\subseteq t_{d} \\\\\n1 \\notin S, s \\in S}} \\lambda_{S}\\right)>1\n\\end{aligned}\n$$\n\nsince $\\lambda_{S}>0$ for all non-empty $S$. Hence,\n\n$$\n\\widehat{C}_{1,2}^{\\mathrm{MO}(\\Lambda)}(u, u)=u^{\\eta^{*}}=o(u), \\quad u \\downarrow 0\n$$\n\nshowing (2.2).", "tables": {}, "images": {}}, {"section_id": 3, "text": "# A.2. Proofs of Section 3. \n\nProof of Proposition 3.6. Let $S \\subseteq \\mathbb{I}_{d}$ such that $|S|=k \\geq 2$. W.l.o.g. let $S=$ $\\{1, \\ldots, k\\}$. Fix any $\\ell \\in S$. For notational convenience, we will take w.l.o.g. $\\ell=k$. Then for $0 \\leq u \\leq 1$,\n\n$$\n\\begin{aligned}\n& \\overbrace{\\widehat{C}_{S \\backslash\\{k\\}}^{\\mathrm{MO}(\\Lambda)}(u, \\ldots, u)}=\\overbrace{\\widehat{C}_{S \\backslash\\{k\\}}^{\\mathrm{MO}(\\Lambda)}\\left(\\underbrace{u, \\ldots, u}_{(k-1)},\\underbrace{1, \\ldots, 1}_{(d-k+1)}\\right)}^{(d-k)} \\\\\n& =\\prod_{i=1}^{d} \\prod_{\\substack{J \\subset \\mathbb{I}_{d}: \\\\\n|J|=i, k \\in J}} \\bigwedge_{j \\in J, j \\leq k} u^{\\eta_{i}^{\\prime}} \\\\\n& =u^{\\eta_{h}^{(k)}}\\left(\\prod_{i=2}^{d} \\prod_{\\substack{J \\subset \\mathbb{I}_{d}: \\\\\n|J|=i, k \\in J}} \\bigwedge_{j \\in J, j \\leq k} u^{\\eta_{j}^{\\prime}}\\right) \\\\\n& =u^{\\eta_{h}^{(k)}} \\cdot g(u) \\quad \\text { (say) }\n\\end{aligned}\n$$\n\nThe second equality above is obtained after canceling all common terms appearing in both the numerator and denominator; these canceled terms correspond to the inner/second product not containing the $k$-th co-ordinate in the set $J$. Hence, all terms in the denominator are canceled. The third equality is obtained by accounting for $i=1$ in the outer product separately. By model definition $\\eta_{k}^{\\{k\\}}>0$ and $g(u)=u^{\\beta}$ for some $\\beta \\geq 0$. Hence\n\n$$\n\\lim _{u \\downarrow 0} \\overbrace{\\widehat{C}_{S \\backslash\\{k\\}}^{\\mathrm{MO}(\\Lambda)}(u, \\ldots, u)}^{\\widehat{C}_{S}^{\\mathrm{MO}(\\Lambda)}} \\overbrace{\\widehat{C}_{S \\backslash\\{k\\}}^{\\mathrm{MO}(\\Lambda)}(u, \\ldots, u)}^{\\lim _{u \\downarrow 0}} \\lim _{u \\downarrow 0}^{(k)}+\\beta=0\n$$\n\nshowing (3.1) and hence proving the claim.\nFirst, we present some auxiliary results required for the proof of Theorem 3.9. The following lemma is from Hashorva and H\u00fcsler [20, Proposition 2.5 and Corollary 2.7].\n\nLemma A.1. Let $\\Sigma \\in \\mathbb{R}^{d \\times d}$ be a positive-definite correlation matrix. Then for any $S \\subset$ $\\mathbb{I}_{d}$ with $|S| \\geq 2$, the quadratic programming problem\n\n$$\n\\mathcal{P}_{\\Sigma_{\\geq}^{-1}}: \\min _{\\left\\{z \\in \\mathbb{R}^{|S|}: z \\geq 1_{S}\\right\\}} z^{\\top} \\Sigma_{S}^{-1} z\n$$\n\nhas a unique solution $e^{S} \\in \\mathbb{R}^{d}$ such that\n\n$$\n\\kappa_{S}:=\\min _{\\left\\{\\boldsymbol{z} \\in \\mathbb{R}^{|S|}: \\boldsymbol{z} \\geq \\mathbf{1}_{S}\\right\\}} \\boldsymbol{z}^{\\top} \\Sigma_{S}^{-1} \\boldsymbol{z}=\\boldsymbol{e}^{S \\top} \\Sigma_{S}^{-1} \\boldsymbol{e}^{S}>1\n$$\n\nMoreover, there exists a unique non-empty index set $I_{S} \\subseteq S$ with $J_{S}:=S \\backslash I_{S}$ such that the unique solution $\\boldsymbol{e}^{S}$ is given by\n\n$$\n\\begin{aligned}\n& \\boldsymbol{e}_{I_{S}}^{S}=\\mathbf{1}_{I_{S}} \\\\\n& \\boldsymbol{e}_{J_{S}}^{S}=-\\left[\\Sigma_{S}^{-1}\\right]_{J_{S} J_{S}}^{-1}\\left[\\Sigma_{S}^{-1}\\right]_{J_{S} I_{S}} \\mathbf{1}_{I_{S}} \\geq \\mathbf{1}_{J_{S}}\n\\end{aligned}\n$$\n\nand $\\mathbf{1}_{I_{S}} \\Sigma_{I_{S}}^{-1} \\mathbf{1}_{I_{S}}=\\boldsymbol{e}^{S \\top} \\Sigma_{S}^{-1} \\boldsymbol{e}^{S}=\\kappa_{S}>1$ as well as $\\boldsymbol{z}^{\\top} \\Sigma_{S}^{-1} \\boldsymbol{e}^{S}=\\boldsymbol{z}_{I_{S}}^{\\top} \\Sigma_{I_{S}}^{-1} \\mathbf{1}_{I_{S}} \\forall \\boldsymbol{z} \\in \\mathbb{R}^{|S|}$. Also defining $h_{i}^{S}:=e_{i}^{\\top} \\Sigma_{I_{S}}^{-1} \\mathbf{1}_{I_{S}}$ for $i \\in I_{S}$ where $e_{i}$ has only one non-zero entry $I$ at the $i$-th coordinate, we have $h_{i}^{S}>0 \\forall i \\in I_{S}$.\n\nLEMMA A.2. Let $\\Sigma \\in \\mathbb{R}^{d \\times d}$ be a positive definite correlation matrix and $I:=I_{\\mathbb{I}_{d}}$ be defined as in Lemma A.1.\n(a) Suppose $\\Sigma^{-1} \\mathbf{1}>\\mathbf{0}$. Then for any $S \\subseteq \\mathbb{I}_{d}$ with $S \\neq \\mathbb{I}_{d}$, the inequality $\\kappa_{\\mathbb{I}_{d}}>\\kappa_{S}$ holds.\n(b) Suppose $\\Sigma^{-1} \\mathbf{1} \\ngtr \\mathbf{0}$. Then $I \\neq \\mathbb{I}_{d}$ and for any set $S \\neq \\mathbb{I}_{d}$ with $I \\subseteq S \\subseteq \\mathbb{I}_{d}$ the equality $\\kappa_{\\mathbb{I}_{d}}=\\kappa_{S}$ holds. For $S \\subseteq \\mathbb{I}_{d}$ with $S^{c} \\cap I \\neq \\emptyset$ we have $I=I_{S}$ and the inequality $\\kappa_{\\mathbb{I}_{d}}>\\kappa_{S}$ holds.\n\nProof. We start with some preliminary calculations. Suppose $S \\subseteq \\mathbb{I}_{d}$ with $S^{c} \\cap I \\neq \\emptyset$. Let $\\boldsymbol{e}^{*}:=\\boldsymbol{e}^{\\mathbb{I}_{d}}$ be the unique solution of the quadratic programming problem $\\mathcal{P}_{\\Sigma^{-1}}$ such that $\\kappa_{\\mathbb{I}_{d}}=\\boldsymbol{e}^{* \\top} \\Sigma^{-1} \\boldsymbol{e}^{*}, \\boldsymbol{e}^{*} \\geq \\mathbf{1}$ and $\\left[\\Sigma^{-1} \\boldsymbol{e}^{*}\\right]_{S^{c}} \\neq \\mathbf{0}_{S^{c}}$ since $\\left[\\Sigma^{-1} \\boldsymbol{e}^{*}\\right]_{I}>\\mathbf{0}_{I}$ and $S^{c} \\cap I \\neq \\emptyset$ (cf. Lemma A.1). First, define $\\widetilde{\\boldsymbol{e}}_{S}:=\\boldsymbol{e}_{S^{c}}^{*}+\\left[\\Sigma^{-1}\\right]_{S^{c}}^{-1}\\left[\\Sigma^{-1}\\right]_{S^{c} S} \\boldsymbol{e}_{S}^{*}$ and note that\n\n$$\n\\begin{aligned}\n\\widetilde{\\boldsymbol{e}}_{S} & =\\boldsymbol{e}_{S^{c}}^{*}+\\left[\\Sigma^{-1}\\right]_{S^{c}}^{-1}\\left[\\Sigma^{-1}\\right]_{S^{c} S} \\boldsymbol{e}_{S}^{*} \\\\\n& =\\left[\\Sigma^{-1}\\right]_{S^{c}}^{-1}\\left(\\left[\\Sigma^{-1}\\right]_{S^{c}} \\boldsymbol{e}_{S^{c}}^{*}+\\left[\\Sigma^{-1}\\right]_{S^{c} S} e_{S}^{*}\\right) \\\\\n& =\\left[\\Sigma^{-1}\\right]_{S^{c}}^{-1}\\left[\\Sigma^{-1} \\boldsymbol{e}^{*}\\right]_{S^{c}} \\neq \\mathbf{0}_{S^{c}}\n\\end{aligned}\n$$\n\nFinally, the Schur decomposition (see Lauritzen [27, eq. (B2)])\n\n$$\n\\left[\\Sigma^{-1}\\right]_{S}=\\Sigma_{S}^{-1}+\\left[\\Sigma^{-1}\\right]_{S S^{c}}\\left[\\Sigma^{-1}\\right]_{S^{c}}^{-1}\\left[\\Sigma^{-1}\\right]_{S^{c} S}\n$$\n\nalong with (A.1) imply that\n\n$$\n\\begin{aligned}\n\\kappa_{\\mathbb{I}_{d}} & =\\boldsymbol{e}^{* \\top} \\Sigma^{-1} \\boldsymbol{e}^{*} \\\\\n& =\\boldsymbol{e}_{S}^{* \\top} \\Sigma_{S}^{-1} \\boldsymbol{e}_{S}^{*}+\\widetilde{\\boldsymbol{e}}_{S}^{\\top}\\left[\\Sigma^{-1}\\right]_{S^{c}} \\widetilde{\\boldsymbol{e}}_{S} \\\\\n& >\\boldsymbol{e}_{S}^{* \\top} \\Sigma^{-1} \\boldsymbol{e}_{S}^{*} \\geq \\min _{\\boldsymbol{z}_{S} \\geq \\mathbf{1}_{S}} \\boldsymbol{z}_{S}^{\\top} \\Sigma_{S}^{-1} \\boldsymbol{z}_{S}=\\kappa_{S}\n\\end{aligned}\n$$\n\n(a) If $\\Sigma^{-1} \\mathbf{1}>\\mathbf{0}$ then $I=\\mathbb{I}_{d}$ and $\\boldsymbol{e}^{*}=\\mathbf{1}$; see Hashorva and H\u00fcsler [20, Proposition 2.5]. Thus, any $S \\subseteq \\mathbb{I}_{d}$ with $S \\neq \\mathbb{I}_{d}$ satisfies $S^{c} \\cap I \\neq \\emptyset$ and the result follows from (A.3).\n(b) If $\\Sigma^{-1} \\mathbf{1} \\ngtr \\mathbf{0}$ then $I \\subseteq \\mathbb{I}_{d}$ and $I \\neq \\mathbb{I}_{d}$; see Hashorva and H\u00fcsler [20, Proposition 2.5]. Hence, Lemma A. 1 and $\\Sigma_{I}^{-1} \\mathbf{1}_{I}>\\mathbf{0}_{I}$ imply that\n\n$$\n\\kappa_{\\mathbb{I}_{d}}=\\mathbf{1}_{I}^{\\top} \\Sigma_{I}^{-1} \\mathbf{1}_{I}^{\\top}=\\kappa_{I}\n$$\n\nFurther, we already know from the Schur decomposition (A.2), which is valid independent of the choice of the set $S$, that $\\kappa_{\\mathbb{I}_{d}} \\geq \\kappa_{S} \\geq \\kappa_{I}$. Hence the only possibility is $\\kappa_{\\mathbb{I}_{d}}=\\kappa_{S}=\\kappa_{I}$. The second statement was already proven in (A.3).\n\nThe next proposition provides the tail asymptotics for the Gaussian survival copula using Das and Fasen-Hartmann [11, Theorem 1].\n\nProposition A.3. Let $C^{\\Sigma}$ be a Gaussian copula with positive definite correlation matrix $\\Sigma$ and $S \\subset \\mathbb{I}_{d}$ with $|S| \\geq 2$. Let $\\kappa_{S}, I_{S}$, and $h_{s}^{S}, s \\in I_{S}$, be defined as in Lemma A.1. Now, with $\\boldsymbol{v}_{S}=\\left(v_{s}\\right)_{s \\in S}$ where $v_{s} \\in(0,1), \\forall s \\in S$, we have as $u \\downarrow 0$,\n\n$$\n\\widehat{C}_{S}^{\\Sigma}\\left(u \\boldsymbol{v}_{S}\\right)=(1+o(1)) \\Upsilon_{S}(2 \\pi)^{\\frac{\\kappa_{S}}{2}} u^{\\kappa_{S}}(-2 \\log u)^{\\frac{\\kappa_{S}-\\left|I_{S}\\right|}{2}} \\prod_{s \\in I_{S}} v_{s}^{h_{s}^{S}}\n$$\n\nwhere $\\Upsilon_{S}>0$ is a constant.\nProof. Since (A.4) is independent of the marginals of the distribution, consider a random vector $\\boldsymbol{Z} \\sim G$ in $\\mathbb{R}^{d}$ with standard Pareto marginals, i.e., $G_{j}(z)=\\mathbb{P}\\left(Z_{j} \\leq z\\right)=1-z^{-1}, z \\geq$ $1, \\forall j \\in \\mathbb{I}_{d}$, and dependence given by the Gaussian copula $C^{\\Sigma}$. Using Das and FasenHartmann [11, Theorem 1] we have that for $z_{S}=\\left(z_{s}\\right)_{s \\in S}$ with $z_{s}>0 \\forall s \\in S$, as $t \\rightarrow \\infty$,\n\n$$\n\\mathbb{P}\\left(Z_{s}>t z_{s}, \\forall s \\in S\\right)=(1+o(1)) \\Upsilon_{S}(2 \\pi)^{\\frac{\\kappa_{S}}{2}} t^{-\\kappa_{S}}(2 \\log (t))^{\\frac{\\kappa_{S}-\\left|I_{S}\\right|}{2}} \\prod_{s \\in I_{S}} z_{s}^{-h_{s}^{S}}\n$$\n\nwhere $\\Upsilon_{S}>0$ is a constant. Then\n\n$$\n\\begin{aligned}\n\\widehat{C}_{S}^{\\Sigma}\\left(u \\boldsymbol{v}_{S}\\right) & =\\mathbb{P}\\left(G_{s}\\left(Z_{s}\\right)>1-u v_{s}, \\forall s \\in S\\right) \\\\\n& =\\mathbb{P}\\left(Z_{s}>u^{-1} v_{s}^{-1}, \\forall s \\in S\\right)\n\\end{aligned}\n$$\n\nand the result follows immediately from (A.5).\nLemma A.4. Let $C^{\\Sigma}$ be a Gaussian copula with positive definite correlation matrix $\\Sigma$. Then there exists a $\\ell \\in \\mathbb{I}_{d}$ such that\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widehat{C}^{\\Sigma}(u, \\ldots, u)}{\\widehat{C}_{\\mathbb{I}_{d} \\backslash\\{\\ell\\}}^{\\Sigma}(u, \\ldots, u)}=c \\in(0,1]\n$$\n\nif and only if $\\Sigma^{-1} \\mathbf{1} \\nsupseteq \\mathbf{0}$.\nProof. $\\Leftarrow$ : Suppose $\\Sigma^{-1} \\mathbf{1} \\nsupseteq \\mathbf{0}$. From Lemma A.2(b) we already know that $I \\neq \\mathbb{I}_{d}$. Now let $\\ell \\in \\mathbb{I}_{d} \\backslash I$. For $S=\\mathbb{I}_{d} \\backslash\\{\\ell\\}$ we have $I \\subseteq S \\subseteq \\mathbb{I}_{d}$, with $I=I_{S}$ and $\\kappa_{\\mathbb{I}_{d}}=\\kappa_{S}$ (cf. proof of Lemma A.2). Now using (A.4) we have\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widehat{C}^{\\Sigma}(u, \\ldots, u)}{\\widehat{C}_{\\mathbb{I}_{d} \\backslash\\{\\ell\\}}^{\\Sigma}(u, \\ldots, u)}=\\frac{\\Upsilon_{\\mathbb{I}_{d}}}{\\Upsilon_{\\mathbb{I}_{d} \\backslash\\{\\ell\\}}}>0\n$$\n\n$\\Rightarrow$ : Suppose there exists $\\ell \\in \\mathbb{I}_{d}$ such that (A.6) holds. We prove the statement by contradiction. By way of contradiction assume $\\Sigma^{-1} \\mathbf{1}>\\mathbf{0}$ holds. Lemma A. 2 says that for any set $S \\subseteq \\mathbb{I}_{d}$ with $S \\neq \\mathbb{I}_{d}$ the inequality $\\kappa_{\\mathbb{I}_{d}}>\\kappa_{S}$ holds. Again using (A.4) we have with $\\kappa^{*}:=\\kappa_{\\mathbb{I}_{d}}-\\kappa_{\\mathbb{I}_{d} \\backslash\\{\\ell\\}}$ and $d^{*}:=d-\\left|I_{\\mathbb{I}_{d} \\backslash\\{\\ell\\}}\\right|$,\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widehat{C}^{\\Sigma}(u, \\ldots, u)}{\\widehat{C}_{\\mathbb{I}_{d} \\backslash\\{\\ell\\}}^{\\Sigma}(u, \\ldots, u)}=\\lim _{u \\downarrow 0} \\frac{\\Upsilon_{\\mathbb{I}_{d}}}{\\Upsilon_{\\mathbb{I}_{d} \\backslash\\{\\ell\\}}}(\\sqrt{2 \\pi} u)^{\\kappa^{*}}(-2 \\log u)^{\\frac{\\kappa^{*}-d^{*}}{2}}=0\n$$\n\nwhich is a contradiction to (A.6).\nProof of Theorem 3.9. The proof follows now from Lemma A. 4 by using an analogous argument as given in the proof of Proposition A.3.\n\nProof of Proposition 3.10. The proof directly follows from Proposition A. 3 where a representation for $\\ell_{S}$ is also provided.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# A.3. Proofs of Section 4. \n\nProof of Theorem 4.5. First, we define for some $\\rho \\in\\left(-\\frac{1}{\\sqrt{k}}, \\frac{1}{\\sqrt{k}}\\right)$ the $\\mathbb{R}^{(k+1) \\times(k+1)}-$ valued positive definite matrix\n\n$$\n\\Gamma_{\\rho}:=\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{k} & \\rho \\mathbf{1}_{k} \\\\\n\\rho \\mathbf{1}_{k}^{\\top} & 1\n\\end{array}\\right]\n$$\n\nwith inverse\n\n$$\n\\Gamma_{\\rho}^{-1}=\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{k}+\\frac{\\rho^{2}}{1-k \\rho^{2}} \\mathbf{1}_{k} \\mathbf{1}_{k}^{\\top} & \\frac{-\\rho}{1-k \\rho^{2}} \\mathbf{1}_{k} \\\\\n\\frac{-\\rho}{1-k \\rho^{2}} \\mathbf{1}_{k}^{\\top} & \\frac{1}{1-k \\rho^{2}}\n\\end{array}\\right]\n$$\n\nNote that\n\n$$\n\\Gamma_{\\rho}^{-1} \\mathbf{1}_{k+1}=\\left[\\frac{1-\\rho}{1-k \\rho^{2}}, \\ldots, \\frac{1-\\rho}{1-k \\rho^{2}}, \\frac{1-k \\rho}{1-k \\rho^{2}}\\right]^{\\top}\n$$\n\nIf we restrict $\\rho \\in\\left[\\frac{1}{k}, \\frac{1}{\\sqrt{k}}\\right)$ then the first $k$ components of $\\Gamma_{\\rho}^{-1} \\mathbf{1}_{k+1}$ are positive and the last component is negative resulting in $\\Gamma_{\\rho}^{-1} \\mathbf{1}_{k+1} \\ngtr \\mathbf{0}_{k+1}$, and hence, due to Theorem 3.9, a Gaussian copula $C^{\\Gamma_{\\rho}}$ with correlation matrix $\\Gamma_{\\rho}$ is not mutually asymptotically independent and thus, not $(k+1)$-wise asymptotically independent.\n\nNow suppose that $\\boldsymbol{X} \\in \\mathbb{R}^{(k+1) \\times(k+1)}$ is a random vector with Gaussian copula $C^{\\Gamma_{\\rho}}$ where $\\rho$ is further restricted to $\\rho \\in\\left(\\frac{1}{k}, \\min \\left(\\frac{1}{k-1}, \\frac{1}{\\sqrt{k}}\\right)\\right)$. Consider a subset $S \\subset\\{1, \\ldots, k+1\\}$ with $|S|=j$ such that $j \\in\\{2, \\ldots, k\\}$.\n\n- If $k+1 \\in S$, considering $k+1$ to be the final element of $S$, we have\n\n$$\n\\left[\\Gamma_{\\rho}\\right]_{S}^{-1} \\mathbf{1}_{j}=\\left[\\frac{1-\\rho}{1-(j-1) \\rho^{2}}, \\ldots, \\frac{1-\\rho}{1-(j-1) \\rho^{2}}, \\frac{1-(j-1) \\rho}{1-(j-1) \\rho^{2}}\\right]^{\\top}>\\mathbf{0}_{j}\n$$\n\n- If $k+1 \\notin S$, then $\\left[\\Gamma_{\\rho}\\right]_{S}=\\boldsymbol{I}_{j}$ and hence\n\n$$\n\\left[\\Gamma_{\\rho}\\right]_{S}^{-1} \\mathbf{1}_{j}=\\mathbf{1}_{j}>\\mathbf{0}_{j}\n$$\n\nThus Theorem 3.9 implies then that $\\boldsymbol{X}_{J}$, for any $J \\subseteq\\{1, \\ldots, k+1\\}$ with $|J| \\leq k$, is a mutually asymptotically independent random vector in $\\mathbb{R}^{J}$. Finally, a conclusion of Lemma 4.2 is that $\\boldsymbol{X}$ is $k$-wise asymptotically independent in $\\mathbb{R}^{(k+1) \\times(k+1)}$, although it is not $(k+1)$ wise asymptotically independent. From Lemma A. 1 we know that $I_{\\{1, \\ldots, k+1\\}}=\\{1, \\ldots, k\\}=$ $I_{\\{1, \\ldots, k\\}}, \\kappa_{\\{1, \\ldots, k+1\\}}=\\kappa_{\\{1, \\ldots, k\\}}=k, h_{i}^{S_{i}}=h_{i}^{S_{2}}=1$ for $i \\in\\{1, \\ldots, k\\}$ and finally, from Proposition A. 3 that\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widehat{C}_{\\{1, \\ldots, k+1\\}}^{\\Gamma_{\\rho}}(u, \\ldots, x u)}{\\widehat{C}_{\\{1, \\ldots, k\\}}^{\\Gamma_{\\rho}}(u, \\ldots, u)}=1\n$$\n\nNote that the constant $\\Upsilon_{S}$ in Proposition A. 3 is not specified in this paper, but it is given in Das and Fasen-Hartmann [11, Theorem 1], from which we obtain $\\Upsilon_{\\{1, \\ldots, k+1\\}}=\\Upsilon_{\\{1, \\ldots, k\\}}$.\n\nAfter all, define the $(d \\times d)$-dimensional correlation $\\Sigma_{\\rho}$ as a block diagonal matrix having in the first $(d-(k+1)) \\times(d-(k+1))$ block the identity matrix, zeros in the two offdiagonal blocks, and, in the last $(k+1) \\times(k+1)$ block $\\Gamma_{\\rho}$ with $\\rho \\in\\left(\\frac{1}{k}, \\min \\left(\\frac{1}{k-1}, \\frac{1}{\\sqrt{k}}\\right)\\right)$, i.e., the random vector $\\boldsymbol{Z}^{*}=\\left(Z_{1}^{*}, \\ldots, Z_{d}^{*}\\right)$ with Gaussian copula $C^{\\Sigma_{\\rho}}$ has the property that\n\n$Z_{1}^{*}, \\ldots, Z_{d-(k+1)}^{*}$ are an independent sequence which is as well independent of the random vector $\\boldsymbol{X}=\\left(Z_{d-k}^{*}, \\ldots, Z_{d}^{*}\\right)$ in $\\mathbb{R}^{(k+1) \\times(k+1)}$ with Gaussian copula $C^{\\Gamma_{\\mu}}$. Then by analogous arguments as above, $\\boldsymbol{Z}^{*}$ is a $k$-wise asymptotic independent random vector in $\\mathbb{R}^{d}$ although it is not $(k+1)$-wise asymptotic independent and\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widehat{C}_{\\left\\{d-k, \\ldots, d\\right\\}}^{\\Sigma_{\\nu}}(u, \\ldots, x u)}{\\widehat{C}_{\\left\\{d-k, \\ldots, d-1\\right\\}}^{\\Sigma_{\\nu}}(u, \\ldots, u)}=1\n$$\n\nThe $d$-dimensional random vector $\\boldsymbol{Z}$ is finally a permutation of $\\boldsymbol{Z}^{*}$ with $\\boldsymbol{Z}_{S_{2}}=\\boldsymbol{Z}_{\\{d-k, \\ldots, d\\}}^{*}$, $\\boldsymbol{Z}_{S_{1}}=\\boldsymbol{Z}_{\\{d-k, \\ldots, d-1\\}}^{*}$ and $\\boldsymbol{Z}_{\\mathbb{1}_{d} \\backslash S_{2}}=\\boldsymbol{Z}_{\\{1, \\ldots, d-k-1\\}}^{*}$ and satisfies the requirements of the theorem.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# A.4. Proofs of Section 5. \n\nProof of Theorem 5.2. For ease of notation, we define $J^{*}:=J \\cup\\{1\\}$. By definition,\n\n$$\n\\begin{aligned}\n\\operatorname{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right) & =\\mathbb{P}\\left(Z_{1}>\\operatorname{VaR}_{\\gamma}\\left(Z_{1}\\right) \\mid Z_{j}>\\operatorname{VaR}_{\\gamma}\\left(Z_{j}\\right), \\forall j \\in J\\right) \\\\\n& =\\mathbb{P}\\left(Z_{1}>F_{1}^{-1}(1-\\gamma) \\mid Z_{j}>F_{j}^{-1}(1-\\gamma), \\forall j \\in J\\right) \\\\\n& =\\frac{\\widehat{C}_{J^{*}}(\\gamma, \\ldots, \\gamma)}{\\widehat{C}_{J}(\\gamma, \\ldots, \\gamma)}\n\\end{aligned}\n$$\n\nwhich does not depend on the marginal distributions.\n(a) Since $\\mathcal{N}_{k} \\subseteq \\mathcal{P}_{k}$, and probabilities are non-negative, it is sufficient to show the statement for $\\mathcal{P}_{k}$. But for any $F \\in \\mathcal{P}_{k}$, by definition of $k$-wise asymptotic independence and $\\left|J^{*}\\right|=$ $\\ell+1 \\leq k$ we have $\\lim _{\\gamma \\downarrow 0} \\mathrm{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)=0$, and thus (a) holds.\n(b) If $d=2$, there is nothing else to prove. Hence, now assume $d \\geq 3$. Since $0 \\leq$ $\\operatorname{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right) \\leq 1$, to show (b), it is sufficient to provide an example of $F \\in \\mathcal{N}_{\\ell} \\subseteq \\mathcal{N}_{k} \\subseteq \\mathcal{P}_{k}$ for $k \\in\\{2, \\ldots, \\ell\\}$, such that for $\\boldsymbol{Z} \\sim F$, we have $\\lim _{\\gamma \\downarrow 0} \\mathrm{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)=1$. To this end, we will choose $F$ with Gaussian copula $C^{\\Sigma}$ and positive-definite correlation matrix $\\Sigma$ as identified using Theorem 4.5, such that $F$ exhibits $\\ell$-wise asymptotic independence but not $(\\ell+1)$-wise asymptotic independence and for any $x>0$,\n\n$$\n\\lim _{\\gamma \\downarrow 0} \\frac{\\widehat{C}_{J^{*}}^{\\Sigma}(x \\gamma, \\gamma, \\ldots, \\gamma)}{\\widehat{C}_{J}^{\\Sigma}(\\gamma, \\ldots, \\gamma)}=1\n$$\n\nHence, $F \\in \\mathcal{N}_{\\ell}$ and by (A.8) we have as well\n\n$$\n\\lim _{\\gamma \\downarrow 0} \\operatorname{CTP}_{\\gamma}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)=\\lim _{\\gamma \\downarrow 0} \\frac{\\widehat{C}_{J^{*}}^{\\Sigma}(\\gamma, \\ldots, \\gamma)}{\\widehat{C}_{J}^{\\Sigma}(\\gamma, \\ldots, \\gamma)}=1\n$$\n\nwhich we wanted to show.\nProof of Lemma 5.5. By definition we have the relation $\\mathcal{N}_{k}^{*} \\subseteq \\mathcal{N}_{k} \\subseteq \\mathcal{P}_{k}$. Since distributions in $\\mathcal{N}_{k}^{*}$ have a Pareto distributed margin in the first component, it remains to show that for any Gaussian copula $C^{\\Sigma}$, where $\\Sigma$ is a positive definite correlation matrix,\n\n$$\n\\sup _{\\gamma \\in\\left(0, x^{-1}\\right]} \\frac{\\widehat{C}_{S}^{\\Sigma}(x \\gamma, \\gamma, \\ldots, \\gamma)}{\\widehat{C}_{S}^{\\Sigma}(\\gamma, \\gamma, \\ldots, \\gamma)}<\\infty\n$$\n\nfor all $S \\subseteq \\mathbb{I}_{d}$, and for all $x \\geq 1$. However, a conclusion from Proposition A. 3 is that for any $S \\subseteq \\mathbb{I}_{d}$, there exists a constant $h_{1}^{S} \\geq 0$ (where $h_{1}^{S}=0$ if $1 \\notin I_{S}$ ) so that for any $x>0$,\n\n$$\n\\lim _{\\gamma \\downarrow 0} \\frac{\\widehat{C}_{S}^{\\Sigma}(x \\gamma, \\gamma, \\ldots, \\gamma)}{\\widehat{C}_{S}^{\\Sigma}(\\gamma, \\gamma, \\ldots, \\gamma)}=x^{h_{1}^{S}}\n$$\n\nimplying (A.9).\nProof of Theorem 5.6. First, note that\n\n$$\n\\begin{aligned}\n& \\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right) \\\\\n& \\quad=\\inf \\left\\{z \\in \\mathbb{R}_{+}:\\mathbb{P}\\left(Z_{1}>z \\mid Z_{j}>\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{j}\\right), \\forall j \\in J\\right) \\leq \\gamma_{1}\\right\\} \\\\\n& \\quad=\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right) \\inf \\left\\{z \\in \\mathbb{R}_{+}:\\mathbb{P}\\left(Z_{1}>z \\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right) \\mid Z_{j}>\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{j}\\right), \\forall j \\in J\\right) \\leq \\gamma_{1}\\right\\}\n\\end{aligned}\n$$\n\nSuppose $Z_{1}$ is Pareto $(\\alpha)$-distributed, $\\alpha>0$. Then the previous equation reduces to\n\n$$\n\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)=\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right) \\inf \\left\\{z \\in \\mathbb{R}_{+}:\\frac{\\widehat{C}_{J^{*}}\\left(z^{-\\frac{1}{\\alpha}} \\gamma_{2}, \\gamma_{2}, \\ldots, \\gamma_{2}\\right)}{\\widehat{C}_{J}\\left(\\gamma_{2}, \\ldots, \\gamma_{2}\\right)} \\leq \\gamma_{1}\\right\\}\n$$\n\nwhere $J^{*}=J \\cup\\{1\\}$.\n(a) Suppose $F \\in \\mathcal{P}_{k}^{*}$ and $k \\in\\{\\ell+1, \\ldots, d\\}$. Let $\\epsilon \\in\\left(0, \\gamma_{1}\\right)$ and\n\n$$\nK:=\\sup _{\\gamma \\in\\left(0, \\epsilon^{1 / \\alpha}\\right]} \\frac{\\widehat{C}_{J^{*}}\\left(\\epsilon^{-1 / \\alpha} \\gamma, \\gamma, \\ldots, \\gamma\\right)}{\\widehat{C}_{J^{*}}(\\gamma, \\gamma, \\ldots, \\gamma)}\n$$\n\nwhich is finite for $F \\in \\mathcal{P}_{k}^{*}$ by the definition of $\\mathcal{P}_{k}^{*}$. Furthermore, $F \\in \\mathcal{P}_{k}^{*} \\subseteq \\mathcal{P}_{k}$ implies that there exists a $\\gamma_{0}(\\epsilon) \\in\\left(0, \\gamma_{1}\\right)$ such that\n\n$$\n\\frac{\\widehat{C}_{J^{*}}(\\gamma, \\gamma, \\ldots, \\gamma)}{\\widehat{C}_{J}(\\gamma, \\gamma, \\ldots, \\gamma)} \\leq \\frac{\\epsilon}{K}, \\quad \\forall \\gamma \\in\\left(0, \\gamma_{0}(\\epsilon)\\right)\n$$\n\nTherefore from (A.11) and (A.12), for all $0<\\gamma_{2}<\\min \\left(\\epsilon^{1 / \\alpha}, \\gamma_{0}(\\epsilon)\\right)$ we have\n\n$$\n\\frac{\\widehat{C}_{J^{*}}\\left(\\epsilon^{-\\frac{1}{\\alpha}} \\gamma_{2}, \\gamma_{2}, \\ldots, \\gamma_{2}\\right)}{\\widehat{C}_{J}\\left(\\gamma_{2}, \\ldots, \\gamma_{2}\\right)}=\\frac{\\widehat{C}_{J^{*}}\\left(\\epsilon^{-\\frac{1}{\\alpha}} \\gamma_{2}, \\gamma_{2}, \\ldots, \\gamma_{2}\\right)}{\\widehat{C}_{J^{*}}\\left(\\gamma_{2}, \\ldots, \\gamma_{2}\\right)} \\frac{\\widehat{C}_{J^{*}}\\left(\\gamma_{2}, \\ldots, \\gamma_{2}\\right)}{\\widehat{C}_{J}\\left(\\gamma_{2}, \\ldots, \\gamma_{2}\\right)} \\leq K \\cdot \\frac{\\epsilon}{K}<\\gamma_{1}\n$$\n\nand finally, using (A.10), we get\n\n$$\n\\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)} \\leq \\epsilon\n$$\n\nSince $\\epsilon \\in\\left(0, \\gamma_{1}\\right)$ is arbitrary, this results in\n\n$$\n\\lim _{\\gamma_{2} \\downarrow 0} \\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)}=0\n$$\n\nFinally, from Lemma 5.5 we already know that $\\mathcal{N}_{k}^{*} \\subseteq \\mathcal{P}_{k}^{*}$, thus the result is true for $\\mathcal{N}_{k}^{*}$ as well.\n(b) We will construct a $\\boldsymbol{Z} \\sim F \\in \\mathcal{N}_{\\ell}^{*}$, so that\n\n$$\n\\lim _{\\gamma_{2} \\downarrow 0} \\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)}=\\infty\n$$\n\nwhich shows the statement.\n\nTo this end, we will choose $\\boldsymbol{Z} \\sim F$ which has a Gaussian copula $C^{\\Sigma}$ with positivedefinite correlation matrix $\\Sigma$ as in Theorem 4.5, such that $F$ exhibits $\\ell$-wise asymptotic independence but not $(\\ell+1)$-wise asymptotic independence and for any $x>0$,\n\n$$\n\\lim _{u \\downarrow 0} \\frac{\\widehat{C}_{J^{\\prime}}^{\\Sigma}(x u, u, \\ldots, u)}{\\widehat{C}_{J}^{\\Sigma}(u, \\ldots, u)}=1\n$$\n\nAdditionally, suppose that the margin $F_{1}$ is Pareto $(\\alpha)$-distributed. Then $F \\in \\mathcal{N}_{\\ell}^{*} \\subseteq \\mathcal{N}_{k}^{*} \\subseteq$ $\\mathcal{P}_{k}^{*}$ for $k \\in\\{2, \\ldots, \\ell\\}$. Due to (A.13), for any $M>0$ there exists an $\\gamma_{0}(M) \\in(0,1)$ such that\n\n$$\n\\frac{\\widehat{C}_{J^{\\prime}}^{\\Sigma}\\left(M^{-\\frac{1}{\\alpha}} \\gamma_{2}, \\gamma_{2}, \\ldots, \\gamma_{2}\\right)}{\\widehat{C}_{J}^{\\Sigma}\\left(\\gamma_{2}, \\ldots, \\gamma_{2}\\right)}>\\frac{\\gamma_{1}+1}{2}, \\quad \\forall \\gamma_{2} \\in\\left(0, \\gamma_{0}(M)\\right)\n$$\n\nFrom this, we get that $\\forall \\gamma_{2} \\in\\left(0, \\gamma_{0}(M)\\right)$,\n\n$$\n\\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)}=\\inf \\left\\{z \\in \\mathbb{R}_{+}:\\frac{\\widehat{C}_{J^{\\prime}}\\left(z^{-\\frac{1}{\\alpha}} \\gamma_{2}, \\gamma_{2}, \\ldots, \\gamma_{2}\\right)}{\\widehat{C}_{J}\\left(\\gamma_{2}, \\ldots, \\gamma_{2}\\right)} \\leq \\gamma_{1}\\right\\} \\geq M\n$$\n\nimplying\n\n$$\n\\liminf _{\\gamma_{2} \\downarrow 0} \\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)} \\geq M\n$$\n\nSince $M>0$ is arbitrary, we have\n\n$$\n\\lim _{\\gamma_{2} \\downarrow 0} \\frac{\\operatorname{CoVaR}_{\\gamma_{1}, \\gamma_{2}}\\left(\\boldsymbol{Z}_{1 \\mid J}\\right)}{\\operatorname{VaR}_{\\gamma_{2}}\\left(Z_{1}\\right)}=\\infty\n$$\n\nexhibiting the desired property for our chosen $F$ and, hence, proving the result.", "tables": {}, "images": {}}], "id": "2406.19186v2", "authors": ["Bikramjit Das", "Vicky Fasen-Hartmann"], "categories": ["math.ST", "stat.TH"], "abstract": "In the study of extremes, the presence of asymptotic independence signifies\nthat extreme events across multiple variables are probably less likely to occur\ntogether. Although well-understood in a bivariate context, the concept remains\nrelatively unexplored when addressing the nuances of joint occurrence of\nextremes in higher dimensions. In this paper, we propose a notion of mutual\nasymptotic independence to capture the behavior of joint extremes in dimensions\nlarger than two and contrast it with the classical notion of (pairwise)\nasymptotic independence. Additionally, we define k-wise asymptotic\nindependence, which captures the tail dependence between pairwise and mutual\nasymptotic independence. The concepts are compared using examples of\nArchimedean, Gaussian, and Marshall-Olkin copulas among others. Notably,for the\npopular Gaussian copula, we provide explicit conditions on the correlation\nmatrix for mutual asymptotic independence and k-wise asymptotic independence to\nhold; moreover, we are able to compute exact tail orders for various tail\nevents. Beside that, we compare and discuss the implications of these new\nnotions of asymptotic independence on assessing the risk of complex systems\nunder distributional ambiguity.", "updated": "2025-03-27T00:39:44Z", "published": "2024-06-27T14:05:36Z"}