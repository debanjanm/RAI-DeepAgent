{
  "title": "A Concise Mathematical Description of Active Inference in Discrete Time",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nIn this paper we present a concise mathematical description of active inference in discrete time. The main part of the paper serves as a basic introduction to the topic, including a detailed example of the action selection mechanism. The appendix discusses the more subtle mathematical details, targeting readers who have already studied the active inference literature but struggle to make sense of the mathematical details and derivations. Throughout, we emphasize precise and standard mathematical notation, ensuring consistency with existing texts and linking all equations to widely used references on active inference. Additionally, we provide Python code that implements the action selection and learning mechanisms described in this paper and is compatible with pymdp environments.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "## Introduction\n\nActive inference is a theory that describes the action selection and learning mechanisms of an agent in an environment. We aim to present a concise mathematical description of the theory so that a reader interested in the mathematical details can quickly find what they are looking for. We have paid special attention to choosing notation that is more in line with standard mathematical texts and is also descriptive, in the sense that dependencies are made explicit. Hence, the focus of this paper lies on the mathematical details and derivations rather than verbal motivations and justifications.\n\nThe paper consists of a main text and an appendix. The main text provides a clear introduction to active inference in discrete time, accessible for people new to the topic. It is divided into two parts: inference, which assumes a given generative model, and learning, which explains how the agent acquires this model. The main text concludes with a worked example of action selection. The appendix delves into finer details and derivations, catering to readers familiar with active inference who seek clarity on the mathematical aspects.\n\nTo complement our theoretical exposition, we provide a Python implementa-\n\ntion ${ }^{1}$ of the action selection and learning mechanisms described in this paper, which is compatible with pymdp environments. This code is more minimalistic, which makes it easier to understand than other implementations such as SPM and pymdp.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 1 Set-up and notation \n\nIn this paper we consider an active inference agent acting in a discrete-time setting with a finite-time horizon. This means that we consider a sequence of $T$ time steps and at every time step $\\tau$ the agent receives an observation $o_{\\tau}$, and performs an action $a_{\\tau}$. We use $\\tau$ for arbitrary time steps and the letter $t$ to denote the current time step. We use the subscript ${ }_{\\tau: \\tau^{\\prime}}$ to denote a sequence of variables, e.g. $o_{\\tau: \\tau^{\\prime}}=\\left(o_{\\tau}, \\ldots, o_{\\tau^{\\prime}}\\right)$. A sequence of (future) actions is called a policy ${ }^{2}$ and is denoted by $\\pi_{t}=a_{t: T}$, with $\\pi=\\pi_{1}$. We write $a_{1: t-1}$ for actions that were performed in the past and $\\pi_{t}$ for future actions that still need to be selected.\n\nThe agent models the dynamics of the environment using an internal generative model. This model uses a variable $s_{\\tau}$, called an internal state, to represent the state of the environment ${ }^{3}$ at time step $\\tau$. The model is given by the following probability distribution:\n\n$$\np\\left(o_{1: T}, s_{1: T} \\mid a_{1: T-1}, \\theta\\right)\n$$\n\nThis probability distribution factorizes according to the graph in Figure 1. In\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Graphical representation of the generative model\nthe first part of this paper we assume that this generative model is given and need not be learned, and we therefore suppress the dependence on the parameter $\\theta$. In the second part we discuss how the model is learned.\n\nSuppose the agent is at time step $t$. It will have received observations $o_{1: t}$ and performed actions $a_{1: t-1}$. We use $q_{t}\\left(s_{\\tau: \\tau^{\\prime}}\\right)$ to denote the (approximate) posterior\n\n[^0]\n[^0]:    ${ }^{1}$ https://github.com/jessevoostrum/active-inference\n    ${ }^{2}$ Note that in a reinforcement learning context the term \"policy\" has a different meaning.\n    ${ }^{3}$ Note that the number of possible internal states is usually much smaller than the actual number of states the environment can be in.\n\ndistribution of the generative model, $p\\left(s_{\\tau: \\tau^{\\prime}} \\mid o_{1: t}, a_{1: t-1}\\right)$, and also refer to this as the belief of the agent about the variable $s_{\\tau: \\tau^{\\prime}}$.",
      "tables": {},
      "images": {
        "img-0.jpeg": "data:image/jpeg;base64,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"
      }
    },
    {
      "section_id": 3,
      "text": "# 2 Inference\n### 2.1 Action selection according to active inference\n\nLet an agent be at time step $t$, having received observations $o_{1: t}$ and performed actions $a_{1: t-1}$. According to active inference an agent selects its next action by sampling a policy $\\pi_{t}$ from the following distribution (equation (10) in [4]):\n\n$$\n\\sigma\\left(-G\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)\\right)\n$$\n\nand selecting the action $a_{t}$ corresponding to that policy. ${ }^{4}$ The function $\\sigma$ denotes the softmax function defined in (48) and $G$ is the expected free energy function given by\n\n$$\n\\begin{aligned}\n& G\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)=-\\left(\\mathbb{E}_{q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right)}\\left[D_{\\mathrm{KL}}\\left(q_{t}\\left(s_{t+1: T} \\mid o_{t+1: T}, \\pi_{t}\\right) \\| q_{t}\\left(s_{t+1: T} \\mid \\pi_{t}\\right)\\right)\\right]\\right. \\\\\n& \\left.+\\mathbb{E}_{q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right)}\\left[\\ln p_{C}\\left(o_{t+1: T}\\right)\\right]\\right)\n\\end{aligned}\n$$\n\nNote that according to (1) the agent is more likely to sample policies $\\pi_{t}$ that have a low expected free energy $G\\left(\\pi_{t}, o_{1: t}, a_{1: t-1}\\right)$. In Section 2.2 we discuss equivalent formulations and different interpretations of the expected free energy. The distributions $q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right), q_{t}\\left(s_{t+1: T} \\mid o_{t+1: T}, \\pi_{t}\\right), q_{t}\\left(s_{t+1: T} \\mid \\pi_{t}\\right)$, needed for the calculation of $G$, are (approximate) posterior distributions of the generative model of the agent after having observed $o_{1: t}$ and performed $a_{1: t-1}$. In Section 2.3 we describe how the agent infers these posterior distributions. The distribution $p_{C}$ is a preference distribution over observations that we assume is given to the agent. This distribution is distinct from the generative model $p$.\n\nRemark 1. Note that in certain descriptions of active inference in discrete time also a variational free energy term $F$ appears in the distribution in equation (1) (e.g. equation (B.9) in [8]). This term is only relevant in specific cases that we will discuss in Remark 2 in Appendix A.2. Furthermore a habit term $E$ is sometimes included that is also considered in Appendix A.2, but discarded here for simplicity.\n\n[^0]\n[^0]:    ${ }^{4}$ Note that at every time step a new policy is sampled, only the action $a_{t}$ for the corresponding time step $t$ is executed, and the rest of the actions in $\\pi_{t}$ are discarded.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 2.2 Expected free energy \n\nRecall that equation (2) gives the following expression for the expected free energy function:\n\n$$\n\\begin{aligned}\nG\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)=- & \\left(\\mathbb{E}_{q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right)}\\left[D_{\\mathrm{KL}}\\left(q_{t}\\left(s_{t+1: T} \\mid o_{t+1: T}, \\pi_{t}\\right) \\| q_{t}\\left(s_{t+1: T} \\mid \\pi_{t}\\right)\\right)\\right] \\\\\n& +\\mathbb{E}_{q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right)}\\left[\\ln p_{C}\\left(o_{t+1: T}\\right)\\right])\n\\end{aligned}\n$$\n\nThe first term between the brackets on the RHS is called epistemic value or information gain. It measures the average change in belief about the future states $s_{t+1: T}$ due to receiving future observations $o_{t+1: T}$. The second term, known as utility, quantifies the similarity between the expected future observation distribution and the preferred observation distribution. As previously mentioned, the agent is more likely to sample policies with low expected free energy, which correspond to high information gain and utility.\n\nAn equivalent formulation of expected free energy is given by\n\n$$\n\\begin{aligned}\nG\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)= & \\mathbb{E}_{q_{t}\\left(s_{t+1: T} \\mid \\pi_{t}\\right)}\\left[\\mathrm{H}\\left[p\\left(o_{t+1: T} \\mid s_{t+1: T}\\right)\\right]\\right] \\\\\n& +D_{\\mathrm{KL}}\\left(q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right) \\| p_{C}\\left(o_{t+1: T}\\right)\\right)\n\\end{aligned}\n$$\n\nThe first term on the RHS is referred to as ambiguity. It measures the average uncertainty an agent has about its future observations given knowledge of its future states. The second term is called expected complexity or risk. It represents the divergence between expected and preferred future observations. The agent favors policies with low ambiguity and risk.\n\nIn Appendix C. 1 we show that both expressions of the expected free energy are equal.\n\nIn practice, often the following mean field approximations are made:\n\n$$\n\\begin{aligned}\nq_{t}\\left(s_{t+1: T}\\right) & =\\prod_{\\tau=t+1}^{T} q_{t}\\left(s_{\\tau}\\right) \\\\\np_{C}\\left(o_{t+1: T}\\right) & =\\prod_{\\tau=t+1}^{T} p_{C}\\left(o_{\\tau}\\right)\n\\end{aligned}\n$$\n\nEquation (2) and (3) can then be written as follows:\n\n$$\nG\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)=\\sum_{\\tau=t+1}^{T} G_{\\tau}\\left(\\pi_{t}, o_{1: t}, a_{1: t-1}\\right)\n$$\n\nwith\n\n$$\n\\begin{aligned}\nG_{\\tau}\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)=- & \\left(\\mathbb{E}_{q_{t}\\left(o_{\\tau} \\mid \\pi_{t}\\right)}\\left[D_{\\mathrm{KL}}\\left(q_{t}\\left(s_{\\tau} \\mid o_{\\tau}, \\pi_{t}\\right) \\| q_{t}\\left(s_{\\tau} \\mid \\pi_{t}\\right)\\right)\\right]\\right. \\\\\n& \\left.+\\mathbb{E}_{q_{t}\\left(o_{\\tau} \\mid \\pi_{t}\\right)}\\left[\\ln p_{C}\\left(o_{\\tau}\\right)\\right]\\right) \\\\\n= & \\mathbb{E}_{q_{t}\\left(s_{\\tau} \\mid \\pi_{t}\\right)}\\left[\\mathrm{H}\\left[p\\left(o_{\\tau} \\mid s_{\\tau}\\right)\\right]\\right]+D_{\\mathrm{KL}}\\left(q_{t}\\left(o_{\\tau} \\mid \\pi_{t}\\right) \\| p_{C}\\left(o_{\\tau}\\right)\\right)\n\\end{aligned}\n$$\n\nIt is outside the scope of this paper to further derive or motivate the expected free energy. We refer the reader to Appendix B.2.5 in [8] and [3, 10, 7] for more details.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 2.3 State inference \n\nIn this section we describe the simplest form of state inference, which is obtained by applying Bayes' rule. State inference methods as described in e.g. [8, 6, 9] can be thought of as computationally efficient approximations of what is described here. See Appendix A. 2 for more details on these methods.\n\nAbove we defined $q_{t}$ to be the (approximate) posterior of the generative model given $o_{1: t}, a_{1: t-1}$. In this section we make the conditioning variables explicit and write $q\\left(\\cdot \\mid o_{1: t}, a_{1: t-1}\\right)$ instead.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "## Current and future state inference\n\nWe start by studying the generative model that is assumed to be given to the agent. The generative model can be decomposed as follows: (see Figure 1)\n\n$$\n\\begin{aligned}\np\\left(o_{1: T}, s_{1: T} \\mid a_{1: T-1}\\right) & =p\\left(s_{1: T} \\mid a_{1: T-1}\\right) p\\left(o_{1: T} \\mid s_{1: T}\\right) \\\\\np\\left(s_{1: T} \\mid a_{1: T-1}\\right) & =p\\left(s_{1}\\right) \\prod_{\\tau=2}^{T} p\\left(s_{\\tau} \\mid s_{\\tau-1}, a_{\\tau-1}\\right) \\\\\np\\left(o_{1: T} \\mid s_{1: T}\\right) & =\\prod_{\\tau=1}^{T} p\\left(o_{\\tau} \\mid s_{\\tau}\\right)\n\\end{aligned}\n$$\n\nAt every time step the agent updates its belief about the current state it is in. Before having performed any observations, its belief about the current state is equal to the prior belief $p\\left(s_{1}\\right)$. After receiving $o_{1}$ it will update its belief using Bayes' rule:\n\n$$\nq\\left(s_{1} \\mid o_{1}\\right) \\propto p\\left(o_{1} \\mid s_{1}\\right) p\\left(s_{1}\\right)\n$$\n\nSubsequently, it will perform an action $a_{1}$ (selected as described in Section 2.1) and receive a next observation $o_{2}$. The belief about $s_{2}$ is given by\n\n$$\n\\begin{aligned}\nq\\left(s_{2} \\mid o_{1: 2}, a_{1}\\right) & \\propto p\\left(o_{2} \\mid s_{2}, o_{1}, a_{1}\\right) p\\left(s_{2} \\mid o_{1}, a_{1}\\right) \\\\\n& =p\\left(o_{2} \\mid s_{2}\\right) \\sum_{s_{1}} p\\left(s_{2} \\mid s_{1}, o_{1}, a_{1}\\right) p\\left(s_{1} \\mid a_{1}, o_{1}\\right) \\\\\n& =p\\left(o_{2} \\mid s_{2}\\right) \\sum_{s_{1}} p\\left(s_{2} \\mid s_{1}, a_{1}\\right) q\\left(s_{1} \\mid o_{1}\\right)\n\\end{aligned}\n$$\n\nFor a general $t$ the belief about $s_{t}$ is updated as follows:\n\n$$\nq\\left(s_{t} \\mid o_{1: t}, a_{1: t-1}\\right) \\propto p\\left(o_{t} \\mid s_{t}\\right) \\sum_{s_{t-1}} p\\left(s_{t} \\mid s_{t-1}, a_{t-1}\\right) q\\left(s_{t-1} \\mid o_{1: t-1}, a_{1: t-2}\\right)\n$$\n\nFor future time point $\\tau>t$, the belief about the state $s_{\\tau}$ is given by\n\n$$\nq\\left(s_{\\tau} \\mid o_{1: t}, a_{1: \\tau-1}\\right)=\\sum_{s_{\\tau-1}} p\\left(s_{\\tau} \\mid s_{\\tau-1}, a_{\\tau-1}\\right) q\\left(s_{\\tau-1} \\mid o_{1: t}, a_{1: \\tau-2}\\right)\n$$\n\nThe belief about a future state given a future observation is calculated as follows:\n\n$$\nq\\left(s_{\\tau} \\mid o_{\\tau}, o_{1: t}, a_{1: \\tau-1}\\right) \\propto p\\left(o_{\\tau} \\mid s_{\\tau}\\right) q\\left(s_{\\tau} \\mid o_{1: t}, a_{1: \\tau-1}\\right)\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# Future observation inference \n\nIn order to compute $G$ in (2), we need a posterior distribution $q\\left(o_{\\tau} \\mid o_{1: t}, a_{1: \\tau-1}\\right)$ over future observations $o_{\\tau}$. This can be computed as follows:\n\n$$\nq\\left(o_{\\tau} \\mid o_{1: t}, a_{1: \\tau-1}\\right)=\\sum_{s_{\\tau}} p\\left(o_{\\tau} \\mid s_{\\tau}\\right) q\\left(s_{\\tau} \\mid o_{1: t}, a_{1: \\tau-1}\\right)\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "## Past, current, and future state inference\n\nIn order to perform inference over states in the past, present and future (which is needed for the learning of the generative model and for the computation of the variational free energy over states), the agent can use the following formula:\n\n$$\n\\begin{aligned}\nq\\left(s_{1: T} \\mid o_{1: t}, a_{1: t-1}, \\pi_{t}\\right) & \\propto p\\left(o_{1: t} \\mid s_{1: T}, a_{1: t-1}, \\pi_{t}\\right) p\\left(s_{1: T} \\mid a_{1: t-1}, \\pi_{t}\\right) \\\\\n& =p\\left(o_{1: t} \\mid s_{1: t}\\right) p\\left(s_{1: T} \\mid a_{1: t-1}, \\pi_{t}\\right) \\\\\n& =\\prod_{\\tau=1}^{t} p\\left(o_{\\tau} \\mid s_{\\tau}\\right) p\\left(s_{1}\\right) \\prod_{\\tau=2}^{T} p\\left(s_{\\tau} \\mid s_{\\tau-1}, a_{\\tau-1}\\right)\n\\end{aligned}\n$$\n\nwhere we use $a_{\\tau-1}$ in the last term for elements of both $a_{1: t-1}$ and $\\pi_{t}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 3 Learning \n\nIn the above section, we have assumed that the agent has access to a generative model $p\\left(o_{1: T}, s_{1: T} \\mid a_{1: T-1}, \\theta\\right)$. In this section we discuss how the parameter $\\theta$ of this model is learned. The generative model consists of three (conditional) categorical distributions that are parametrized by $\\theta=\\left(\\theta^{D}, \\theta^{A}, \\theta^{B}\\right)$ in the following way:\n\n$$\n\\begin{aligned}\n& p\\left(s_{1}^{(j)} \\mid \\theta^{D}\\right)=\\theta_{j}^{D} \\\\\n& p\\left(o_{\\tau}^{(i)} \\mid s_{\\tau}^{(j)}, \\theta^{A}\\right)=\\theta_{i j}^{A} \\\\\n& p\\left(s_{\\tau+1}^{(j)} \\mid s_{\\tau}^{(k)}, a_{\\tau}^{(l)}, \\theta^{B}\\right)=\\theta_{j k l}^{B}\n\\end{aligned}\n$$\n\nwhere we have enumerated the elements of the observation, action and state space with the bracketed superscript ${ }^{(1)}$. In order to learn the parameters of the generative model, we adopt a Bayesian belief updating scheme with a Dirichlet prior. (See Appendix B for details on this.) More specifically, the prior over $\\theta$ is parametrized by $\\alpha=\\left(\\alpha^{D}, \\alpha^{A}, \\alpha^{B}\\right)$ and is given by\n\n$$\n\\begin{aligned}\np(\\theta \\mid \\alpha) & =p\\left(\\theta^{D} \\mid \\alpha^{D}\\right) \\prod_{j} p\\left(\\theta_{\\bullet j}^{A} \\mid \\alpha^{A}\\right) \\prod_{k, l} p\\left(\\theta_{\\bullet k l}^{B} \\mid \\alpha^{B}\\right) \\\\\np\\left(\\theta^{D} \\mid \\alpha^{D}\\right) & \\propto \\prod_{j}\\left(\\theta_{j}^{D}\\right)^{\\alpha_{j}^{D}-1} \\\\\np\\left(\\theta_{\\bullet j}^{A} \\mid \\alpha^{A}\\right) & \\propto \\prod_{i}\\left(\\theta_{i j}^{A}\\right)^{\\alpha_{i j}^{A}-1} \\\\\np\\left(\\theta_{\\bullet k l}^{B} \\mid \\alpha^{B}\\right) & \\propto \\prod_{j}\\left(\\theta_{j k l}^{B}\\right)^{\\alpha_{j k l}^{B}-1}\n\\end{aligned}\n$$\n\nwhere $\\theta_{\\bullet j}$ denotes the vector $\\left(\\theta_{1 j}, \\ldots, \\theta_{n j}\\right)$.\nNow after performing actions $a_{1: T-1}$ and receiving observations $o_{1: T}$ we want to update our belief about $\\theta$ according to Bayes' rule. Similar to the situation in Appendix B.3, the true posteriors over these parameters are not Dirichlet distributions. The active inference literature suggests to set the approximate posterior distribution to be a Dirichlet distribution and update the hyperparameter $\\alpha$ in the following way (equation (B.12) in [8] and equation (21), (A.6) and (A.7) in [4]):\n\n$$\n\\begin{aligned}\n\\alpha_{j}^{D^{\\prime}} & =\\alpha_{j}^{D}+q_{T}\\left(s_{1}^{(j)}\\right) \\\\\n\\alpha_{i j}^{A^{\\prime}} & =\\alpha_{i j}^{A}+\\sum_{\\tau=1}^{T} \\mathbb{1}_{a^{(i)}}\\left(a_{\\tau}\\right) q_{T}\\left(s_{\\tau}^{(j)}\\right) \\\\\n\\alpha_{j k l}^{B^{\\prime}} & =\\alpha_{j k l}^{B}+\\sum_{\\tau=1}^{T-1} q_{T}\\left(s_{\\tau+1}^{(j)}\\right) q_{T}\\left(s_{\\tau}^{(k)}\\right) \\mathbb{1}_{a^{(l)}}\\left(a_{\\tau}\\right)\n\\end{aligned}\n$$\n\nThe distributions $q_{T}\\left(s_{\\tau}\\right), \\tau \\in\\{1, \\ldots, T\\}$ are approximate posteriors obtained ${ }^{5}$ using the current version of the generative model (before $\\theta$ has been updated). In Appendix A. 2 and B. 3 we elaborate on the origin of this learning rule.\n\nNote the similarity with the standard update rule for Dirichlet priors given in (31). In the standard update rule the element $\\alpha_{i^{*}}$ of the hyperparameter corresponding to the observation $x^{\\left(i^{*}\\right)}$ is incremented by 1 , which makes this observation more likely in the updated distribution. In the updates (18)-(20) the hyperparameters are incremented by the amount of posterior belief in that state or state transition, e.g. $\\alpha_{j}^{D}$ is incremented by $q\\left(s_{1}^{(j)} \\mid o_{1: T}, a_{1: T-1}\\right)$.\n\nIn order to go from a Dirichlet distribution $p(\\theta \\mid \\alpha)$ to an actual value of the parameter that can be used for the generative model, the mean of the distribution can be used, which is given by\n\n$$\n\\begin{aligned}\n\\hat{\\theta}_{i} & =\\mathbb{E}_{p(\\theta \\mid \\alpha)}\\left[\\theta_{i}\\right] \\\\\n& =\\frac{\\alpha_{i}}{\\sum_{j} \\alpha_{j}}\n\\end{aligned}\n$$\n\nFor example, after the learning step, the new distribution over $s_{1}$ is given by\n\n$$\np\\left(s_{1}^{(j)} \\mid \\hat{\\theta}^{D}\\right)=\\frac{\\alpha_{j}^{D^{\\prime}}}{\\sum_{j} \\alpha_{k}^{D^{\\prime}}}\n$$\n\nThis concludes the discussion of learning in the context of the active inference framework.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# 4 Example: T-maze example \n\nIn this section we discuss the action selection mechanism of an active inference agent in the T-maze environment depicted in Figure 2. (See also Section 7.3 of [8] and [5].) This illustrates the theory of action selection and state inference that is presented in Section 2.\n\n[^0]\n[^0]:    ${ }^{5}$ See Section 2.3 .\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: T-maze environment",
      "tables": {},
      "images": {
        "img-1.jpeg": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAGvAnwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iisjVvFOh6FPHBquqW1pLIu9FlfBK5xn86ANeiuZ/4WJ4P/AOhisP8Av7R/wsTwf/0MVh/39oA6aiuZ/wCFieD/APoYrD/v7R/wsTwf/wBDFYf9/aAOmormf+FieD/+hisP+/tH/CxPB/8A0MVh/wB/aAOmormf+FieD/8AoYrD/v7R/wALE8H/APQxWH/f2gDpqK5n/hYng/8A6GKw/wC/tH/CxPB//QxWH/f2gDpqK5n/AIWJ4P8A+hisP+/tH/CxPB//AEMVh/39oA6aiuZ/4WJ4P/6GKw/7+0f8LE8H/wDQxWH/AH9oA6aiuZ/4WJ4P/wChisP+/tH/AAsTwf8A9DFYf9/aAOmormf+FieD/wDoYrD/AL+0f8LE8H/9DFYf9/aAOmormf8AhYng/wD6GKw/7+0f8LE8H/8AQxWH/f2gDpqK5n/hYng//oYrD/v7R/wsTwf/ANDFYf8Af2gDpqK5n/hYng//AKGKw/7+0f8ACxPB/wD0MVh/39oA6aiuY/4WJ4P/AOhisP8Av7SD4jeDyAR4i08554l7UAdRRXM/8LE8H/8AQxWH/f2j/hYng/8A6GKw/wC/tAHTUVzP/CxPB/8A0MVh/wB/aP8AhYng/wD6GKw/7+0AdNRXM/8ACxPB/wD0MVh/39o/4WJ4P/6GKw/7+0AdNRXM/wDCxPB//QxWH/f2j/hYng//AKGKw/7+0AdNRXM/8LE8H/8AQxWH/f2j/hYng/8A6GKw/wC/tAHTUVzP/CxPB/8A0MVh/wB/aP8AhYng/wD6GKw/7+0AdNRXM/8ACxPB/wD0MVh/39o/4WJ4P/6GKw/7+0AdNRXM/wDCxPB//QxWH/f2j/hYng//AKGKw/7+0AdNRXM/8LE8H/8AQxWH/f2j/hYng/8A6GKw/wC/tAHTUVzP/CxPB/8A0MVh/wB/aP8AhYng/wD6GKw/7+0AdNRXM/8ACxPB/wD0MVh/39o/4WJ4P/6GKw/7+0AdNRXM/wDCxPB//QxWH/f2j/hYng//AKGKw/7+0AdNRXM/8LE8H/8AQxWH/f2j/hYng/8A6GKw/wC/tAHTUVzP/CxPB/8A0MVh/wB/aP8AhYng/wD6GKw/7+0AdNRXM/8ACxPB/wD0MVh/39o/4WJ4P/6GKw/7+0AdNRXM/wDCxPB//QxWH/f2j/hYng//AKGKw/7+0AdNRXM/8LE8H/8AQxWH/f2j/hYng/8A6GKw/wC/tAHTUVzP/CxPB/8A0MVh/wB/aP8AhYng/wD6GKw/7+0AdNRXM/8ACxPB/wD0MVh/39o/4WJ4P/6GKw/7+0AdNRXM/wDCxPB//QxWH/f2j/hYng//AKGKw/7+0AdNRXM/8LE8H/8AQxWH/f2j/hYng/8A6GKw/wC/tAHTUVzP/CxPB/8A0MVh/wB/aP8AhYng/wD6GKw/7+0AdNRXM/8ACxPB/wD0MVh/39o/4WJ4P/6GKw/7+0AdNRXM/wDCxPB//QxWH/f2j/hYng//AKGKw/7+0AdNRXM/8LE8H/8AQxWH/f2kPxF8HgZ/4SKw/wC/tAHT0VR0rWdO1u1Nzpl5FdQK5jLxNkBh1H6ir1ABXNKP+LlH/sDj/wBHV0tc2v8AyUpv+wOP/RxoA6SiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAEI45rnfAcbxeA9CSVGSRbKIMrjDA7RnPvXRnpTEIIBUgg8gjpQA+iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKQ8DNLQelAEbzRxyRxu6q0jbUDNgucE4HqcAn8KkrnPECM3iLwmwUkLqUpYgcAfZLgZ/UV0dABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFGKKKAOa8Jf8fHiLn/mMS/8AoEddLXN+Ev8Aj48Rf9hiX/0COukoAK5tf+SlN/2Bx/6ONdJXNr/yUpv+wOP/AEcaAOkooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigBGxtOeneuc8AY/4QDQfX7FFnPXO0dfeukPSmIAAAoAA4AHSgB9FFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUHpRSHpQA0YLAkc9qfXNeIefEnhE9xqU3bt9kuP/rV0tABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHN+Ev+PjxF/2GJf8A0COukrm/CX/Hx4i/7DEv/oEddJQAVza/8lKb/sDj/wBHGukrm1/5KU3/AGBx/wCjjQB0lFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAB6VzngKR5vAWhSySNI7WURLucknaOT710TfdP9ajgiighSKKNYo1GFjUABR6UAS0UUUAFFFFABRRRQAUUUUAFFFFABRQTgUmecYoAWikDZYj096WgAooooAKKKKACiiigAooooAKD0opGOBmgBpVGZSygsOQSOhwR/LNPrndfkkTxF4URHZVfUZVcA4DD7JcHB9eQD+FdFQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBzfhL/j48Rf8AYYl/9AjrpK5vwl/x8eIv+wxL/wCgR10lABXNr/yUpv8AsDj/ANHGukrm1/5KU3/YHH/o40AdJRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAAelc/wCB7ia88D6Hc3MzzTyWcbvI7FmcleST3Nb56GoLO1gsbWK1tolighUIkajhQOwoAsUUUUAFFFFABQTjrRQeaAIJb21gmSGW4hjlkOEjeQBm+g71Nn2pk0EVxE0U0aSxtwyOoII+lZ39iW8X/HjNcWPYLBJ+7A9BGwKD8FFAGoCDS1l/8Tm2PItL5B6ZgcD8dwY/98ij+3LaIYvo57A9zcphF+sgyn/j1AGmTXh/xO+Nf9lzvo3hWSOS8jfE97gOsZH8KA8FvU9u3PIpfGD4ugLN4c8NXQO4bby8ib16xoR+rA+w748A3e2aAPrb4afFOw8b2qWl0Y7XWokzJBnCzY6vHnt7dR79a9GDZNfBNne3On3kN3aTyQXELh45I2wyMOhBr6p+GfxYsPFWkeTq9xb2er2yDzRI6oky9A6549MjsT6UAenHik3VmHV2n4srC7uP9tk8lAfq+CR7qGFIINXuT++uoLJT1S1TzH/77cY/8c/GgDTaQIpZiAoGSScAVHbXdveRebazxTxZxvjcMM/UVSXQ7DcHuEa7cHIa7cy4PqA3Cn/dArRAAxxj0oAdRRRQAUUUUAFIelLQelAEL28MssUkkSPJCxaNmUEoSCuQexwSPxqasDXLmaDXvC8UUrJHPqEqSqp4dfss7AH1GVB/Ct+gAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDm/CX/Hx4i/7DEv/AKBHXSVzfhL/AI+PEX/YYl/9AjrpKACubX/kpTf9gcf+jjXSVza/8lKb/sDj/wBHGgDpKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAQ8CsLwZe3Go+DdHvLuRpLma0R5HYYLMRya3W+6aq6dYW2l2EFjZxCK2gQRxRg52qOg55oAt0UUUAFFFFABRRRQA11VkZWAKkYIPQisW60nwxZRebd6dpECbsbpYIlG49uR1NbhGRiomgiM6zmNDKq7Q+0bgPTPp7UAc39i0i5ylh4Wtpgekk9qkEf/jy7yPdVI968u+MOu23hjT10u1t9Li1S8XcUtLOMG3iPGS5BJY8gEBfXtz7N4g1u18N6Be6vekiC1jLkA4LH+FR7kkAe5r4o8Q63eeI9bvNWv5N9zdSGRsdFHZRnsBgD6UAZhbIxSUUUAAqzZXk1lew3NuQs0Th1JAIJHYjuD0I6Gq1KDg5oA+r/AV3p/i3wzDqdnpmjXUq4jurae1SKSNx1+ZFIYHqPkH1zXTi28Owki+8PW1j6tPZR+X9fMUFB+JB9q+YvhV43fwZ4whlmcjTLwiG8UngL2f6qefpkd6+wV2vgjBGBgigChaaRoitHc2enaerY3RzQwoDj1BArTxUcUEUAIiiSME7iEUDJ9akoAKKKKACiiigAoPSikPQ0AV5LW3uJ7aWaJWktnMkLH+BipQkf8BZh+NWaw9ZvLm21vw5DBKViur6SKdcD51FtM4HTj5kU9ulblABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHN+Ev+PjxF/2GJf8A0COukrm/CX/Hx4i/7DEv/oEddJQAVza/8lKb/sDj/wBHGukrm1/5KU3/AGBx/wCjjQB0lFFFABRRRQAUUUUAFBOBRWb4h1NtF8OalqiwtO1pbSTLEvVyqkgfpQBobxShs4rjfDVta+LPDFlrU19rDyXiby32qW0IIJBxHG4ULkcdcjGSeScrwD4k1GTxp4k8IajdvfLpbh7a6k/1hjJHyOQPmIyvPU8/gAekUUUHigAPSkzxn+VITx0Irxf4lfG+20fztJ8LyR3OoD5JLzho4D3C9mb9B79KAO38bfE3QPAypHfvJcXsnK2lthpAv95skBR9ev54y/D3xv8ABuv3CWzXM+nTuQFW9jCqT/vqSo/Eivk+7vbm/u5bu7mknuJW3SSyNuZj6kmoQ3IoA++lkV9pU5VhkEU+vnf4F/Ea5j1GLwlqs5lt5QfsEkjZaNgP9Xn+6QDj0PHfj6HzzQAtFFFABRRRQAUUUUAIehrF8I6hcat4S0nULtt1zcWqSSEDALEDPH1raPQ1T0rT7fSdLtdOtAwt7aMRRhjkhQMDmgC7RRRQAUUUUAFFFFABSHpS0UAeBftGeJmSPTfDUD4Eg+2XIHccqg/PccewNfPxOe2K7T4s6qdX+J2uS7iVhn+zKD0XywEOPxUn8a4qgAooooAKKKKAFHBFfXXwW8SnxF8PrVZm3XWnn7HIT1IUAof++SBnuQa+RB1r3D9m7VGi8Q6xpRPyXFqtwM+sbbeP+/h/KgD6PooooAKKKKACiiigApD0paD0oAqT6fbXV1Z3E8e6WzkMsB3EbGKMhPv8rsOfWrdYur6jcWes+H7aFlEd7evDNkclRbTSDHp8yCtqgAooooAKKKKACiiigAPApu7249aVvunnHHUVwXhHVIfHtvqN7dSatA9veSW32ZZpLZYgpGMGNgWOMEljkHdgAYoA73dzilrzGPXL7wx8YrHwr9rubzStVszcRJdSmWS1cBycO2WZT5fQk9fbn06gAoopN2OooAU1geLPGWjeDNL+3avchA2RFCvMkzeir3+vQVy/xH+Lel+CoXsbZUvtaYcW4b5Ic9DIR0452jk+3WvlvX/EWqeJtVl1LVrp7i5k7seFHZVHYD0oA+ltI/aB8HajcLDdrfacW6SXEQZM+5Qkj8RivTrS9tr+1jurOeK4t5RujlicMrj1BHBH0r4JDc8jP416L8JviPc+Dtfhs7uYtot3IFuEc8RMePNHpjv6jPfFAH1xRSBg3I6UtABRRRQAUUUUAFFFFAHN+Ev+PjxF/wBhiX/0COukrm/CX/Hx4i/7DEv/AKBHXSUAFc2v/JSm/wCwOP8A0ca6SubX/kpTf9gcf+jjQB0lFFFABRRRQAUUUUAFB6UUHpQBheKtXn0TRHmsoBcahcSLbWcLdHmc4Xd7Dlj7KapeC/B1v4VtriWS4e81e/fzr+9k6yydcD0UZOB71panYyXmsaLcKqtDaXDzOSehMTopHr989K1h15/WgB1I33TnpS0GgDxv4kn4m+JGm0rw/oEtnpRO2SX7XAJbke/z/KhweO469cV5Gfgv8QsY/wCEdb/wLg/+Lrk/+Ei1vA/4nF/j/r5f/Gk/4SLWv+gvqH/gS/8AjQB1n/ClfiF/0Lzf+BcH/wAXR/wpb4hf9C83/gXB/wDF1yf/AAkWtf8AQX1D/wACX/xo/wCEi1r/AKC+of8AgS/+NAHd6L8JfiJpeu2GoJoLI1rcRzBvtcHG1gefn9q+sFOa+GbfxDrX2mL/AIm9/wDfH/Ly/r9a+5wOaAFooooAKKKKACiiigBD0rH8KanNrPhbS9SuQgmurZJXEYwoJHOAe1bJ6Vn6LpcWi6PZaZA8jxWkKwo0hBYgDAz+VAGhRRRQAUUUUAFFFFABQaKD0oA+G/GG8+M9cMn+sN/Pu/3vMbP61iV3Hxc0Z9G+JmtIVIjuZvtcZ7MJPmJH0YsPqK4egAooooAKKKKAAda9W/Z+Lf8ACysLnH2KXP04/rivKh1Fe5fs36O8muavrLA+XBbrbKccFnYMcfQJ/wCPUAfRlFFFABRRRQAUUUUAFI3Ck0tIelAFK602C8vdPuZd4exmaaHacDcY3j59Rtdv0q9WPqupzWGr6FaRLGY7+8eCUsOQot5ZAVweu6MfgTWxQAUUUUAFFFFABRRRQAEZFUtTvrfSNKu9Ru3KW9rC0rkDoqjJwPWrtZPiKxl1PRzaRKG8y4ty6kgZjEyM/X/ZDcUAc34Q8Kzvqz+MfESltfvY8Rwk5SwhxxEn+1g/Me5J9ye6xTEz+npT6AEY4B5rz34h6j49kgOm+DdDcl1/e6i1xCpX1EaswIP+0R9B3r0M9K+P/ibrerW/xJ1+GHU72ONLohUSdgF4HQA8UAOl+DfxFnleWXQZHkclmd7yElieSSd/Jpn/AApb4hf9C83/AIFwf/F1yf8AwkWtf9BfUP8AwJf/ABo/4SLWv+gvqH/gS/8AjQB1n/ClfiF/0Lzf+BcH/wAXQPgt8QQf+Reb/wACoP8A4uuT/wCEi1r/AKC+of8AgS/+NH/CRa1/0F7/AP8AAl/8aAPtTwrFfQeFdJg1RGTUIrSJLhSwYiQKA2SMg5Iz1rZrlvhzNJcfDrw/LNI8kjWUZZ3OSTjua6mgAooooAKKKKACiiigDm/CX/Hx4i/7DEv/AKBHXSVzfhL/AI+PEX/YYl/9AjrpKACubX/kpTf9gcf+jjXSVza/8lKb/sDj/wBHGgDpKKKKACiiigAooooAKDRRQBx3iLQ/E9/4w0PUNK10WelWrZvLQkjzhuyeBw2Rxz0689K68Dn+X0pdvuaXFABQeBRRQB8L3XhbXrO5ltp9Hv1liYqwFux5HuBUP9gaz/0CL/8A8Bn/AMK+7QMd6WgD4R/sDWf+gRf/APgM/wDhR/YGs/8AQIv/APwGf/Cvu6igD4b03wvr15qdrbw6RfGSSVVXNuwGc9yRxX3HnmlIyMUgGKAFooooAKKKKACiiigAPSsnw1qcmteG9N1SWNY3urdJWRTkKSM8VrVnaBpI0LQLDSlm84WkCw+Zt27toxnGTj86ANGiiigAooooAKKKKACkPSloIyMUAeQfHTwLJ4h0KLXNPh36hpykSqo+aWDqcepU8/QtXzAVIFffhXg9ffHevDfiR8DBqVxNq/hQRQzyEtNYMdqOfWM9FPsePTHSgD50oq9qmj6jot41pqdlPZ3C9Y50KH6jPUe/SqeznAOT6CgBtKBkgZxRj0re8OeDde8V3Qh0fTZrgbsPLt2xp/vOeB/P2oAybKxudRvIbOzhee5ncRxRIMszE4Ar7M+HnhGPwX4PtNLBVrk5mupF5DSt1we4AAUH0Arn/ht8JrDwTGL68dLzWmXBmAOyEekYP6t19MDNekBSDnNADqKKKACiiigAooooAKQ9KWigDPvdLjvr/TLt5HV7Cdp0VcYctE8eD7YkJ/CtCsrUtUew1TRLVYlddQunt2YnGwLBJKCP+/ePxrVoAKKKKACiiigAooooAQ9K5Tx5o3iLW9Ggt/DesjSrpblXkmLFd8YByMrz1wcd8YrrOtJigCK3SRIIkmcSSqoDvjG5scnHbvU1IAR3zS0ABr5D+KvhzWR8SdamXS7x4p5/NjeOFmVlIBBBA/ya+vDyKaFwetAHwn/YGs/9Ai//APAZ/wDCj+wNZ/6BF/8A+Az/AOFfd1FAHwj/AGBrP/QIv/8AwGf/AApV8P60zADSL/J6f6M/+FfdtGKAOb8A2VzpvgLQrO8haG4iso1kjbqp29D710lIFxjmloAKKKKACiiigAooooA5vwl/x8eIv+wxL/6BHXSVzfhL/j48Rf8AYYl/9AjrpKACubX/AJKU3/YHH/o410lc2v8AyUpv+wOP/RxoA6SiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACs3w/q39u+H9P1XyfI+1wLN5W/dsyM4zgZ/KtI9KyvDeltonhzTtKeUStaW6RGQDAbAxnH4UAatFFFABRRRQAUUUUAFFFFAAeaTb7kUtFAFS+02y1K3MF/aQXcB6xTxh1P4GvAPiX4W0TUPiJoXg/w5pVpZ3Vx+8vJYExtQ5wMdBtUM3vkdK+iT0rxP4eL/bnx28aazMNxsi9rGT/D83lgj0+WIj8aAPQLD4ZeC9O2GHw5YOyj700QlP1+bIrqYoIoI1jijWONRhUVQAB7CngYpaAExzS0UUAFFFFABRRRQAUUUUAFFFFAGbqGki/1HSLwzFDp1y1wF2537oZIsdeP9ZnPPTHetKs7UNWFhqGkWhhLnUbloA27GzbDJLnpz/q8Y46+1aNABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHN+Ev+PjxF/2GJf/AECOukrm/CX/AB8eIv8AsMS/+gR10lABXNr/AMlKb/sDj/0ca6SubX/kpTf9gcf+jjQB0lFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAIc7TjrVDRNUj1vRbLU4Y2jjuoVmVG6gMMjNXz0rI8LaZNovhfS9MuGjaa1tkicxklcgc4yAcUAbFFFFABRRRQAUUUUAFFFFABRRRQAHkYrxT4VN/Z/xf8f6ZLxLNcNcKp7qJW5/KVfzr2uvDvHhbwF8Z9I8YgFdO1NPs94QMgEAK2fouxgO+w0Ae45opkbq6qyMGRhlSDkEe1PoAKKKKACiiigAooooAKKKKACg0Uh6UAZepaW9/qui3Syqi6fdPcMp5LgwSRYH4yZ/CtWqF5qkVjfaZaSRuz387QIVxhSsTyZPtiMj6kVfoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5vwl/x8eIv+wxL/wCgR10lc34S/wCPjxF/2GJf/QI66SgArm1/5KU3/YHH/o410lc2v/JSm/7A4/8ARxoA6SiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAEPQ1R0jUoNY0q01G13eRcxLLGHGDtIyOKv9qxfCWnXGkeEtJ067Ci4trVIpApyAwHODQBtUUUUAFFFFABRRRQAUUUUAFFFFABXMeP8AwrF4w8GX2lFVNwU8y1Y/wzKPl/PofYmunoPSgDzP4G+JJNd8BR2l05a60uQ2rbvvFMZQn6D5f+A16ZXivwSwnjX4gRQ4+zrfLsx0x5k2MfhXtVABRRRQAUUUUAFFFFABRRRQAUh6UtIehxQBj6pps9/rGg3UTRiOwu3nlDHBKtbyxjbx13SD8M1s1SutSgsrzT7aYv5l9M0MOBwWEbyHPp8qNV2gAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDm/CX/Hx4i/7DEv/AKBHXSVzfhL/AI+PEX/YYl/9AjrpKACubX/kpTf9gcf+jjXSVza/8lKb/sDj/wBHGgDpKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAQ9KqabqFtqunW2oWb+Za3MYlifBG5SMg4PtVw9KwfBtlc6b4O0axvIjFdQWkccqZztYLyKAN6iiigAooooAKKKKACiiigAooooAKxPF3iCHwv4U1LWZuRbQlkUfxOeFH4sQK26jlhSeJ4pUR42BVlZcgg9iO4oA8t+AuhTad4Mn1e7Ui51e4M2T1Ma8Ln6nefowr1aore2itYI4II0ihiUJHGihVVQMAADgD2qWgAooooAKKKKACiiigAooooAKD0ooPSgDE1fT7i81rw7cQxgxWV6805JHCm2mQfX5nUfjW3VSe/trS6sreeTbLeSNFANpO9wjORkDj5UY846VboAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5vwl/x8eIv+wxL/wCgR10lc34S/wCPjxF/2GJf/QI66SgArm1/5KUf+wOO3/TY10h6VzOraXrf/CSR6vo0mn7zaG1kjvQ/QPuBUr9aAOmormP+K5/6l3/yNR/xXP8A1Lv/AJGoA6eiuY/4rn/qXf8AyNR/xXP/AFLv/kagDp6K5j/iuf8AqXf/ACNR/wAVz/1Lv/kagDp6K5j/AIrn/qXf/I1H/Fc/9S7/AORqAOnormP+K5/6l3/yNR/xXP8A1Lv/AJGoA6eiuY/4rn/qXf8AyNR/xXP/AFLv/kagDp6K5j/iuf8AqXf/ACNR/wAVz/1Lv/kagDp6K5j/AIrn/qXf/I1H/Fc/9S7/AORqAOnormP+K5/6l3/yNR/xXP8A1Lv/AJGoA6eiuY/4rn/qXf8AyNR/xXP/AFLv/kagDp6K5j/iuf8AqXf/ACNR/wAVz/1Lv/kagDp6K5j/AIrn/qXf/I1H/Fc/9S7/AORqAOmPTpmq9leW9/aQXdpKs1vMgeKRTwykcEfnWAT45A/5l3/yNVX4Z/2l/wAIJpP277J5X2WP7P8AZ933Mfxbu/8AjQB2OaM+1ZOrf29uh/sb+ztvPmfbd+fw20ukf29uk/tr+zsf8s/sXmf+PbqANXPpRn2rM1f+2dkf9jfYPM3fP9t34x7bai0r/hIvtEn9r/2X5Oz5Psfmbt3vu7UAbGfajNZ+rf2r9mX+x/sf2jeP+Pvds24PTbznOKqaZ/wk32wf2t/ZP2bH/Lp5m/d/wLigDcoqlqX9o/Y2/sv7L9qyMfad2zrznbz0rP07/hK/t8f9pf2L9j58z7N5vmdDjG7jrj8M0AbtGarX/wBs+xSf2f8AZ/tXGz7RnZ1Gc456ZrGtP+Ew+0xfbv7D+z5/eeT5u/HtnigDoc0ZqG6+0fZJfsnl/aNp8rzM7d3bOOcZrBh/4TTzo/tH9geVuHmeX52cd8ZoA6TNGajl3+W/lbfM2/Juz17Z74zXOp/wm29d/wDwj23cN2POzjP5ZoA6bPGaM00btoz1xzjPX/Cua/4rfP8AzL+3387OM/l0oA6fNGfao4/M8tPN279vzbc43e3euen/AOEz86Xyv+Ef8rd8nmedu29s9s0AdLmjNQ2/2j7NF9q2efsHm+Xnbu4zjvjrWJdf8Jh9ql+x/wBh/Z8/uvO83fj3xxQB0O7H/wCujPpVax+2fZE/tDyftWP3n2fds/DPNZN9/wAJX9vl/s7+xvsmRs+0+b5nQZzjjrmgDfLAdx+dHUVT03+0PsUf9qfZvteDv+y7vL68Y3c9Koal/wAJR9s/4lf9kfZdvH2rzd+f+A0AN1qxuLnXfDc8MReO2v5JZ2GPkU2s6Anv951H41vVyV/qfiewj0q1MekvqWoXrQBh5nkqiwySZPfP7vH41N/xXP8A1Lv/AJGoA6eiuY/4rn/qXf8AyNR/xXP/AFLv/kagDp6K5j/iuf8AqXf/ACNR/wAVz/1Lv/kagDp6K5j/AIrn/qXf/I1H/Fc/9S7/AORqAOnormP+K5/6l3/yNR/xXP8A1Lv/AJGoA6eiuY/4rn/qXf8AyNR/xXP/AFLv/kagDp6K5j/iuf8AqXf/ACNR/wAVz/1Lv/kagDp6K5j/AIrn/qXf/I1H/Fc/9S7/AORqAOnormP+K5/6l3/yNR/xXP8A1Lv/AJGoA6eiuY/4rn/qXf8AyNR/xXP/AFLv/kagDp6K5j/iuf8AqXf/ACNR/wAVz/1Lv/kagDp6K5j/AIrn/qXf/I1H/Fc/9S7/AORqAOnormP+K5/6l3/yNSH/AITnH/Mu/wDkagB/hMj7T4iH/UYl/wDQI66WsLwxpd/pkF82pSWz3N5ePdN9m3bFyqjA3f7v61u0AB5FIBS0UAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFc18P1H/Cv9AP/AE4xdP8Adrpa5v4f/wDJPtA/68Yv/QRQB0eKMe9LRQAmPSjHvS0UAJj3oxS0UAFFFFABRiiigBMUYpaKAExRilooATHGKMUtFACYox70tFACYoxS0UAJtz/+qjHpS0UAIVB7D8qMUtFAHN+Iv+Rk8If9hOX/ANI7iukrm/EX/IyeEP8AsJy/+kdzXSUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAJiloooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACub+H/8AyT7QP+vGL/0EV0lc38P/APkn2gf9eMX/AKCKAOkooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDm/EX/ACMnhD/sJy/+kdzXSVzfiL/kZPCH/YTl/wDSO5rpKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK5v4f8A/JPtA/68Yv8A0EV0lc38P/8Akn2gf9eMX/oIoA6SiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOb8Rf8jJ4Q/7Ccv8A6R3NdJXN+Iv+Rk8If9hOX/0jua6SgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACub+H/8AyT7QP+vGL/0EV0lc38P/APkn2gf9eMX/AKCKAOkooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDm/EX/ACMnhD/sJy/+kdzXSVzfiL/kZPCH/YTl/wDSO5rpKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK5v4f8A/JPtA/68Yv8A0EV0lc38P/8Akn2gf9eMX/oIoA6SiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOb8Rf8jJ4Q/7Ccv8A6R3NdJXN+Iv+Rk8If9hOX/0jua6SgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACub+H/8AyT7QP+vGL/0EV0lc38P/APkn2gf9eMX/AKCKAOkooooAKKKKACiiigAooooADxTdwwD2Ncx4+8Yx+CvDEmpeT9oupHWC0hzxJK2cZ9hgk/T3pll4e1EaYLvWNb1OfVWj8x/sk3lpE2M7I4x8pAPHzhs9/SgDq9w5oz7c1xHhi+1fxr4JWa5m1HRL3z3TzViRZXVSQCVdSMEEZwByDjANZ/wZ1nVdd8I38+sX0t5cxanLCJZMZ2qiYHHuTQB6NuOegx7GlzXla3WuaL8b9I0GXxDf6hpt1YyXJiuVjGGxIAMooyBtzXQeO/Fl1os+kaHpPl/2zrNwIYGkXcsKZG6QjvgHgev0wQDtC2KAc1x2taFqemeHri/0rXNRk1e0haYNcS+ZHclRko0Z+UBsEfIFIyOTWr4P8Qx+K/Cuna3EhQXUW5k5wrglXHvhlIHtQBu0UUUAFFFFABRRRQAUUUUAc34i/wCRk8If9hOX/wBI7mukrm/EX/IyeEP+wnL/AOkdzXSUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVzfw/wD+SfaB/wBeMX/oIrpK5v4f/wDJPtA/68Yv/QRQB0lFFFABRRRQAUUUUAFB6UUUAeSfHe3lTRdC1bYz2unakklwoGcKe5/EAf8AAq9WilSaJJInV0ddyODkMPXPemXllbahZzWl5Ck1vOhSWN1yHU9jXP6d4Pm0a1FjpfiLUrbTkGIrYrFL5C/3Ud0LbfQMWx2oA3re9guJriKKXe9tJ5UwwflbaGx78Mp49a81+A/y+D9WBGD/AGzPnP8AuR16TYabbaZaJbWqlUUliWJZmYnLMzHlmJyST1NYVr4Li0q+v7nRNSudOW+mM89uiJJF5p4LqGUlWP1x7UAcdq1xHL+0poKKwLQ6Q6vg/dJEpwfTgg/jVf4k2HlfFrwVqt20q6bKzWTyRyvF5chJx8ykFc7x0PRT2rpr/wCGFneeJLLXoda1S01G1QqbiJkLysc5diykEkHbjGMAAAAYrptS8PafrWiHSdWi+227IquZcBmI6PlcYbPORjB6UAQN4Z091KPNqhDDBH9rXXP/AJE6U/wxaaNYaLFaaBGsenQySoioWKhg7B8FuT827nofWq0Hhm9jgFpJ4m1WWzAA2N5YlK/3TKEDfiMN/tVuWtpBZW0VtbRLFBCgSNFGAqjoBQBNRRRQAUUUUAFFFFABRRRQBzfiL/kZPCH/AGE5f/SO5rpK5vxF/wAjJ4Q/7Ccv/pHc10lABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFc38P/wDkn2gf9eMX/oIrpK5v4f8A/JPtA/68Yv8A0EUAdJRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHN+Iv8AkZPCH/YTl/8ASO5rpK5vxF/yMnhD/sJy/wDpHc10lABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFc38P8A/kn2gf8AXjF/6CK6Sub+H/8AyT7QP+vGL/0EUAdJRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHN+Iv+Rk8If8AYTl/9I7mukrm/EX/ACMnhD/sJy/+kdzXSUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVzfw//AOSfaB/14xf+giukrm/h/wD8k+0D/rxi/wDQRQB0lFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAc34i/wCRk8If9hOX/wBI7mukrm/EX/IyeEP+wnL/AOkdzXSUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVzfw/wD+SfaB/wBeMX/oIrpK5v4f/wDJPtA/68Yv/QRQB0lFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAc34i/5GTwh/wBhOX/0jua6Sub8Rf8AIyeEP+wnL/6R3NdJQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXN/D/8A5J9oH/XjF/6CK6Sub+H/APyT7QP+vGL/ANBFAHSUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBzfiL/AJGTwh/2E5f/AEjua6Sub8Rf8jJ4Q/7Ccv8A6R3NdJQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXNfD8/wDFvtA4/wCXGP8AlXSEZGK5kfDrweP+Zd0/8IhQB09Fcz/wrvwf/wBC7Yf9+qP+Fd+D/wDoXbD/AL9UAdNRXM/8K78H/wDQu2H/AH6o/wCFd+D/APoXbD/v1QB01Fcz/wAK78H/APQu2H/fqj/hXfg//oXbD/v1QB01Fcz/AMK78H/9C7Yf9+qP+Fd+D/8AoXbD/v1QB01Fcz/wrvwf/wBC7Yf9+qP+Fd+D/wDoXbD/AL9UAdNRXM/8K78H/wDQu2H/AH6o/wCFd+D/APoXbD/v1QB01Fcz/wAK78H/APQu2H/fqj/hXfg//oXbD/v1QB01Fcz/AMK78H/9C7Yf9+qP+Fd+D/8AoXbD/v1QB01Fcz/wrvwf/wBC7Yf9+qP+Fd+D/wDoXbD/AL9UAdNRXM/8K78H/wDQu2H/AH6o/wCFd+D/APoXbD/v1QB01Fcz/wAK78H/APQu2H/fqj/hXfg//oXbD/v1QB01ITxXNf8ACu/B/wD0Lth/36pP+Fd+D/8AoXdP/wC/QoAd4hOfEnhEd/7Tl4z/ANOdxXS1g6d4L8N6TfRXtho1nb3MWfLlSPDLkFTj8CR+Nb1ABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB//2Q=="
      }
    },
    {
      "section_id": 11,
      "text": "# Description of the (internal) generative model of the agent\n## State, observation and action spaces\n\nThe internal state space of the agent ${ }^{6} \\mathcal{S}$ has two dimensions ${ }^{7}$, Location and Reward condition, and can be described as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{S} & =\\mathcal{S}^{L} \\times \\mathcal{S}^{R} \\\\\n\\mathcal{S}^{L} & =\\{\\text { center, right arm, left arm, cue location }\\} \\\\\n\\mathcal{S}^{R} & =\\{\\text { reward on right, reward on left }\\}\n\\end{aligned}\n$$\n\nA typical element of the state space is written as $s=\\left(s^{L}, s^{R}\\right)$.\nThe observation space $\\mathcal{O}$ has three dimensions ${ }^{8,9}$, Location, Reward, and Cue, and can be described as follows:\n\n$$\n\\begin{aligned}\n\\mathcal{O} & =\\mathcal{O}^{L} \\times \\mathcal{O}^{R} \\times \\mathcal{O}^{C} \\\\\n\\mathcal{O}^{L} & =\\{\\text { center, right arm, left arm, cue location }\\} \\\\\n\\mathcal{O}^{R} & =\\{\\text { no reward, reward, loss }\\} \\\\\n\\mathcal{O}^{C} & =\\{\\text { cue right, cue left }\\}\n\\end{aligned}\n$$\n\nA typical element of the observation space is written as $o=\\left(o^{L}, o^{R}, o^{C}\\right)$.\n\n[^0]\n[^0]:    ${ }^{6}$ In this example the state of the environment (generative process) is the same as the state space of the internal world model of the agent (generative model). Note that this is in general not the case. In a more realistic setting the state space of the environment will be much more complex than the internal state space.\n    ${ }^{7}$ The dimensions of the state space are sometimes referred to as state factors.\n    ${ }^{8}$ The dimensions of the observation space are sometimes referred to as observation modalities.\n    ${ }^{9}$ Note that we follow here the description from [5]. In [8] the cue observation is absorbed into the location observation.\n\nThe space of actions $\\mathcal{A}$ is described by\n\n$$\n\\begin{aligned}\n\\mathcal{A}= & \\text { \\{move to center, move to right arm, } \\\\\n& \\text { move to left arm, move to cue location }\\}\n\\end{aligned}\n$$\n\nNote that these actions are always available, independent of the current location of the agent. A typical element of the action space is written as $a$.\n\nIn the following we use [dir] as a placeholder for left and right, and [loc] as a placeholder for the four locations center, right arm, left arm and cue location.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# Observation kernel $p(o \\mid s)$ \n\nWe now specify the observation kernel $p(o \\mid s)$ of the generative model of the agent. First note that the observation dimensions are independent, that is\n\n$$\np(o \\mid s)=p\\left(o^{L} \\mid s\\right) p\\left(o^{R} \\mid s\\right)\n$$\n\nThe beliefs about the location observation given the state are modelled as follows:\n\n$$\np\\left(o^{L} \\mid s\\right)= \\begin{cases}1 & \\text { if } o^{L}=s^{L} \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nwhich implies that the location can be unambiguously inferred from the observation.\n\nThe beliefs about the reward observation given the state are modelled as follows:\n\n$$\n\\begin{aligned}\n& p\\left(o^{R}=\\text { no reward } \\mid s^{L} \\in\\{\\text { center, cue location }\\}, s^{R}\\right)=1 \\\\\n& p\\left(o^{R}=\\text { no reward } \\mid s^{L} \\notin\\{\\text { center, cue location }\\}, s^{R}\\right)=0 \\\\\n& p\\left(o^{R}=\\text { reward } \\mid s^{L}=\\text { [dir] arm, } s^{R}=\\text { reward on [dir] }\\right)=0.98 \\\\\n& p\\left(o^{R}=\\text { loss } \\mid s^{L}=\\text { right arm, } s^{R}=\\text { reward on left }\\right)=0.98 \\\\\n& p\\left(o^{R}=\\text { loss } \\mid s^{L}=\\text { left arm, } s^{R}=\\text { reward on right }\\right)=0.98\n\\end{aligned}\n$$\n\nThis implies that the agent observes no reward when it is in location center or cue location, it observes reward when it is in the same arm as specified by the reward condition with high probability, and it observes loss when it is in the opposite arm of the reward condition with high probability.\n\nThe beliefs about the cue observation given the state are modelled as follows:\n\n$$\n\\begin{aligned}\n& p\\left(o^{C}=\\text { cue [dir] } \\mid s^{L}=\\text { cue location, } s^{R}=\\text { reward on [dir] }\\right)=1 \\\\\n& p\\left(o^{C}=\\text { cue [dir] } \\mid s^{L} \\in \\mathcal{S}^{L} \\backslash\\{\\text { cue location }\\}, s^{R}\\right)=0.5\n\\end{aligned}\n$$\n\nThis implies that the cue observation is completely informative about the reward condition when the agent is at the cue location, and otherwise independent of the actual reward condition.\n\nTransition dynamics kernel $p\\left(s_{\\tau+1} \\mid s_{\\tau}, a_{\\tau}\\right)$\nWe continue by describing the transition dynamics kernel $p\\left(s_{\\tau+1} \\mid s_{\\tau}, a_{\\tau}\\right)$.\n\n$$\n\\begin{aligned}\np\\left(s_{\\tau+1}^{L}\\right. & =\\left.[\\text { loc }] \\mid s_{\\tau}^{L} \\in\\{\\text { center, cue location }\\}, s_{\\tau}^{R}, a_{\\tau}=\\text { go to }[\\text { loc }])=1\\right. \\\\\np\\left(s_{\\tau+1}^{L}\\right. & =\\left.[\\text { dir }] \\text { arm } \\mid s_{\\tau}^{L}=[\\text { dir }] \\text { arm, } s_{\\tau}^{R}, a_{\\tau}\\right)=1\n\\end{aligned}\n$$\n\nThis implies that if the agent is in center or cue location it will be in the location specified by the action in the next time step. If it is in one of the arms however, it will stay there, independent of the choice of action $a_{\\tau}$.\n\n$$\np\\left(s_{\\tau+1}^{R}=\\text { reward on }[\\text { dir }] \\mid s_{\\tau}^{R}=\\text { reward on }[\\text { dir }], s_{\\tau}^{L}, a_{\\tau}\\right)=1\n$$\n\nThis implies that the reward condition stays constant throughout the trajectory.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# Preference distribution $p_{C}$ and prior over states $p_{D}$ \n\nThe unnormalized preference distribution $p_{C}$ can be chosen to favor observations with reward and discourage observations with loss as follows:\n\n$$\n\\begin{aligned}\n& p_{C}(([\\text { loc }], \\text { no reward, cue }[\\text { dir }]))=2 \\\\\n& p_{C}(([\\text { loc }], \\text { reward, cue }[\\text { dir }]))=3 \\\\\n& p_{C}(([\\text { loc }], \\text { loss, cue }[\\text { dir }]))=1\n\\end{aligned}\n$$\n\nFinally we let the prior belief over states $p_{D}$ be uniform.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "## Action selection procedure\n\nWe will now simulate the trajectory of an agent acting according to active inference. We set the time horizon to $T=3$, which implies that the policies will have length 2 .",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "## Time step 1\n\nThe agent starts by receiving an observation $o_{1}=$ (center, no reward, cue right). It now updates its beliefs about the current state such that\n\n$$\nq_{t}\\left(s_{1}^{L}=\\text { center, } s_{1}^{R}=\\text { reward on }[\\text { dir }])\\right)=0.5\n$$\n\nSubsequently it computes its beliefs about future states and observations given a policy using equation (7), (8), (9). For example for $\\pi_{1}^{*}=$ (move to cue location, move to left arm) we have\n\n$$\n\\begin{gathered}\nq_{1}\\left(s_{2}^{L}=\\text { cue location, } s_{2}^{R}=\\text { reward on }[\\text { dir }] \\mid \\pi_{1}=\\pi_{1}^{*}\\right)=0.5 \\\\\nq_{1}\\left(s_{3}^{L}=\\text { left arm, } s_{3}^{R}=\\text { reward on }[\\text { dir }] \\mid \\pi_{1}=\\pi_{1}^{*}\\right)=0.5\n\\end{gathered}\n$$\n\nand\n\n$$\n\\begin{gathered}\nq_{1}\\left(o_{2}^{L}=\\text { cue location, } o_{2}^{R}=\\text { no reward, } o_{2}^{C}=\\text { cue }[\\text { dir }] \\mid \\pi_{1}=\\pi_{1}^{*}\\right)=0.5 \\\\\nq_{1}\\left(o_{3}^{L}=\\text { left arm, } o_{3}^{R}=\\text { reward, } o_{3}^{C}=\\text { cue }[\\text { dir }] \\mid \\pi_{1}=\\pi_{1}^{*}\\right)=0.25 \\\\\nq_{1}\\left(o_{3}^{L}=\\text { left arm, } o_{3}^{R}=\\text { loss, } o_{3}^{C}=\\text { cue }[\\text { dir }] \\mid \\pi_{1}=\\pi_{1}^{*}\\right)=0.25\n\\end{gathered}\n$$\n\nand for example for $o_{2}^{*}=$ (cue location, no reward, cue left) we have\n\n$$\nq_{1}\\left(s_{2}^{L}=\\text { cue location, } s_{2}^{R}=\\text { reward on left } \\mid o_{2}=o_{2}^{*}, \\pi_{1}=\\pi_{1}^{*}\\right)=1\n$$\n\nNote here the reduction of uncertainty about $s_{2}$ due to the observation $o_{2}^{*}$, represented by the epistemic value defined in Section 2.2 given by the KL divergence between the distributions (23) and (22).\n\nThe agent now computes $G$ and plugs this into equation (1) and gets the following posterior distribution over policies:\n\n![table_0](table_0)\n\nand in this scenario it samples a policy with as first action move to cue location with highest probability.",
      "tables": {
        "table_0": "| $a_{2} \\rightarrow$ | center | right arm | left arm | cue location |\n| :-- | :--: | :--: | :--: | :--: |\n| $a_{1} \\downarrow$ | 0.022 | 0.041 | 0.041 | 0.046 |\n| center | 0.041 | 0.075 | 0.075 | 0.083 |\n| right arm | 0.041 | 0.075 | 0.075 | 0.083 |\n| left arm | 0.046 | 0.083 | 0.083 | 0.091 |\n| cue location |  |  |  |  |"
      },
      "images": {}
    },
    {
      "section_id": 16,
      "text": "# Time step 2 \n\nAfter having performed action $a_{1}^{*}=$ move to cue location, the next observation it receives is $o_{2}^{*}=$ (cue location, no reward, cue right). Its belief about the current state is now given by\n\n$$\nq_{2}\\left(s_{2}^{L}=\\text { cue location, } s_{2}^{R}=\\text { reward on right }\\right)=1\n$$\n\nFor instance, when $\\pi_{2}^{*}=$ (move to left arm), the beliefs about future states are\n\n$$\nq_{2}\\left(s_{3}^{L}=\\text { left arm, } s_{3}^{R}=\\text { reward on right } \\mid \\pi_{2}=\\pi_{2}^{*}\\right)=1\n$$\n\nand the observation beliefs\n\n$$\n\\begin{gathered}\nq_{2}\\left(o_{3}^{L}=\\text { left arm, } o_{3}^{R}=\\text { reward, } o_{3}^{C}=\\text { cue }[\\text { dir }] \\mid \\pi_{2}=\\pi_{2}^{*}\\right)=0.01 \\\\\nq_{2}\\left(o_{3}^{L}=\\text { left arm, } o_{3}^{R}=\\text { loss, } o_{3}^{C}=\\text { cue }[\\text { dir }] \\mid \\pi_{2}=\\pi_{2}^{*}\\right)=0.49\n\\end{gathered}\n$$\n\nSince all uncertainty has already been taken away by the last observation, conditioning on $o_{3}^{*}=$ (left arm, reward, cue left) will make no difference to the belief about the state $s_{3}$, i.e.\n\n$$\n\\begin{aligned}\nq_{2}\\left(s_{3}^{L}=\\right. & \\text { left arm, } s_{3}^{R}=\\text { reward on right } \\mid o_{3}=o_{3}^{*}, \\pi_{2}=\\pi_{2}^{*}\\right)= \\\\\n& q_{2}\\left(s_{3}^{L}=\\text { left arm, } s_{3}^{R}=\\text { reward on right } \\mid \\pi_{2}=\\pi_{2}^{*}\\right)=1\n\\end{aligned}\n$$\n\nwhich will cause the epistemic value term in $G$ to be zero.\nThe agent now calculates $G$ again and obtains the following distribution over policies:\n\n![table_1](table_1)\n\nand will then sample the policy (move to right arm) with highest probability.",
      "tables": {
        "table_1": "| $a_{2}$ | center | right arm | left arm | cue location |\n| :--: | :--: | :--: | :--: | :--: |\n|  | 0.20 | 0.52 | 0.08 | 0.20 |"
      },
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# Acknowledgements \n\nThe authors would like to thank Thomas Parr, Conor Heins, Ryan Smith, Beren Millidge, Pablo Lanillos, Sean Tull, Stephen Mann, Pradeep Kumar Banerjee, Frank R\u00f6der and Lance Da Costa for helpful discussions and comments and acknowledge the support of the Deutsche Forschungsgemeinschaft Priority Programme \"The Active Self\" (SPP 2134).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "## References\n\n[1] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning. Vol. 4. 4. Springer, 2006.\n[2] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. \"Variational inference: A review for statisticians\". In: Journal of the American statistical Association 112.518 (2017), pp. 859-877.\n[3] Lancelot Da Costa et al. \"Active inference as a model of agency\". In: arXiv preprint arXiv:2401.12917 (2024).\n[4] Lancelot Da Costa et al. \"Active inference on discrete state-spaces: A synthesis\". In: Journal of Mathematical Psychology 99 (2020), p. 102447. ISSN: 0022-2496. DOI: https://doi.org/10.1016/j.jmp.2020.102447.\n[5] Conor Heins. Active Inference Demo: T-Maxe Environment. https://pymdp-rtd.readthedocs.io/en/1\n[6] Conor Heins et al. \"pymdp: A Python library for active inference in discrete state spaces\". In: The Journal of Open Source Software 7.73 (2022), p. 4098.\n[7] Beren Millidge, Alexander Tschantz, and Christopher L Buckley. \"Whence the expected free energy?\" In: Neural Computation 33.2 (2021), pp. 447482 .\n[8] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle in mind, brain, and behavior. MIT Press, 2022.\n[9] Ryan Smith, Karl J Friston, and Christopher J Whyte. \"A step-by-step tutorial on active inference and its application to empirical data\". In: Journal of mathematical psychology 107 (2022), p. 102632.\n[10] Ran Wei. \"Value of Information and Reward Specification in Active Inference and POMDPs\". In: arXiv preprint arXiv:2408.06542 (2024).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 19,
      "text": "# A Variational free energy minimization\n## A. 1 Preliminaries\n\nIn this section we use $x$ for a general observed variable and $z$ for a general latent variable.\n\nLet $p(x, z)=p(x \\mid z) p(z)$ be a generative model. In order to perform inference over the latent variables after making an observation $x$, one has to compute the posterior $p(z \\mid x)$. This posterior is often hard to compute directly. One can instead approximate this posterior by finding the distribution $q_{x}(z)$ in a family of distributions $\\mathcal{Q}$ that minimizes the following function:\n\n$$\nF(\\tilde{q} \\mid x)=\\sum_{z} \\tilde{q}(z)(\\ln \\tilde{q}(z)-\\ln p(x, z))\n$$\n\ncalled the variational free energy. Note that we distinguish notationally $\\tilde{q}$, which is a generic element of $\\mathcal{Q}$ and a variable in $F$, and $q_{x}$, which is the minimizer of $F$ for a fixed $x$, i.e.\n\n$$\nq_{x}=\\underset{\\tilde{q} \\in \\mathcal{Q}}{\\arg \\min } F(\\tilde{q} \\mid x)\n$$\n\nNote that when $\\mathcal{Q}$ is large enough, for example when $z$ is discrete and $\\mathcal{Q}$ is the set of all probability distributions over $z$, then the minimizer of the free energy is equal to the exact posterior distribution and we have\n\n$$\nq_{x}(z)=p(z \\mid x)\n$$\n\nWe can replace $\\ln p(x, z)$ in $F$ by a general function $f$, i.e.\n\n$$\nF_{f}(\\tilde{q} \\mid x)=\\sum_{z} \\tilde{q}(z)(\\ln \\tilde{q}(z)-f(x, z))\n$$\n\nThe minimizer can be found by substituting $g(x, z)=e^{f(x, z)}$ as follows:\n\n$$\n\\begin{aligned}\nF_{f}(\\tilde{q}, x) & =\\sum_{z} \\tilde{q}(z)(\\ln \\tilde{q}(z)-\\ln g(x, z)) \\\\\n& =\\left(\\sum_{z} \\tilde{q}(z)\\left(\\ln \\tilde{q}(z)-\\ln \\frac{g(x, z)}{\\sum_{z^{\\prime}} g\\left(x, z^{\\prime}\\right)}\\right)\\right)+\\ln \\sum_{z^{\\prime}} g\\left(x, z^{\\prime}\\right)\n\\end{aligned}\n$$\n\nIf $\\mathcal{Q}$ is again large enough, the minimizer is given by:\n\n$$\n\\begin{aligned}\nq_{x}(z) & =\\frac{g(x, z)}{\\sum_{z} g(x, z)} \\\\\n& =\\frac{e^{f(x, z)}}{\\sum_{z} e^{f(x, z)}} \\\\\n& =\\sigma(f(x, z))\n\\end{aligned}\n$$\n\nwhere $\\sigma$ is the softmax function defined in (48) and in the case that $f(x, z)=$ $\\ln p(x, z)$ we have\n\n$$\n\\begin{aligned}\nq_{x}(z) & =\\sigma(\\ln p(x, z)) \\\\\n& =p(z \\mid x)\n\\end{aligned}\n$$\n\nThe variational free energy can be written as follows:\n\n$$\n\\begin{aligned}\nF(\\tilde{q} \\mid x) & =D_{\\mathrm{KL}}(\\tilde{q}(z) \\| p(z \\mid x))-\\ln p(x) \\\\\n& \\geq-\\ln p(x)\n\\end{aligned}\n$$\n\nwhich shows that the negative of the variational free energy is a lower bound on the evidence (ELBO). By minimizing $F$ w.r.t. $\\tilde{q}$ we get the following approximate equality (equation (B.2) in [8]):\n\n$$\nF\\left(q_{x} \\mid x\\right) \\approx-\\ln p(x)\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "# A. 2 Variational free energy minimization in active inference \n\nActive inference adopts the perspective that perception, action selection and learning can be interpreted as minimizing one single variational free energy function $F$. In its complete form it can be written as follows:\n\n$$\nF\\left(\\tilde{q} \\mid o_{1: t}, a_{1: t-1}\\right)=\\mathbb{E}_{\\tilde{q}\\left(s_{1: T}, \\theta, \\pi_{t}\\right)}\\left[\\ln \\tilde{q}\\left(s_{1: T}, \\theta, \\pi_{t}\\right)-\\ln p\\left(s_{1: T}, o_{1: t}, \\theta, \\pi_{t} \\mid a_{1: t-1}\\right)\\right]\n$$\n\nThe distributions in the family $\\mathcal{Q}$ are assumed to factorize as follows:\n\n$$\nq\\left(s_{1: T}, \\theta, \\pi\\right)=q\\left(\\theta^{D}\\right) q\\left(\\theta^{A}\\right) q\\left(\\theta^{B}\\right) q(\\pi) \\prod_{\\tau=1}^{T} q\\left(s_{\\tau}\\right)\n$$\n\nwhich is sometimes referred to as the mean-field approximation. Now $F$ can be written as follows:\n\n$$\n\\begin{aligned}\n& F\\left(\\tilde{q} \\mid o_{1: t}, a_{1: t-1}\\right)=\\mathbb{E}_{\\tilde{q}\\left(s_{1: T}, \\theta, \\pi_{t}\\right)}\\left[\\ln \\tilde{q}\\left(\\pi_{t}\\right)+\\ln \\tilde{q}(\\theta)+\\sum_{\\tau=1}^{T} \\ln \\tilde{q}\\left(s_{\\tau}\\right)-\\ln p\\left(\\pi_{t}\\right)\\right. \\\\\n& \\left.-\\ln p(\\theta)-\\ln p\\left(s_{1} \\mid \\theta^{D}\\right)-\\sum_{\\tau=1}^{t} \\ln p\\left(o_{\\tau} \\mid s_{\\tau}, \\theta^{A}\\right)-\\sum_{\\tau=2}^{T} \\ln p\\left(s_{\\tau} \\mid s_{\\tau-1}, a_{\\tau-1}, \\theta^{B}\\right)\\right]\n\\end{aligned}\n$$\n\nwhere we use $a_{\\tau-1}$ in the last term for elements of both $a_{1: t-1}$ and $\\pi_{t}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "## Perception\n\nFor studying perception, equation (26) can be rewritten as follows:\n\n$$\nF\\left(\\tilde{q} \\mid o_{1: t}, a_{1: t-1}\\right)=\\mathbb{E}_{\\tilde{q}\\left(\\pi_{t}\\right)}\\left[F_{\\pi_{t}}\\left(\\tilde{q} \\mid o_{1: t}, a_{1: t-1}\\right)\\right]+C_{\\backslash s}\n$$\n\nwhere $C_{\\backslash s}$ is independent of $\\tilde{q}\\left(s_{1: T}\\right)$ and\n\n$$\n\\begin{aligned}\n& F_{\\pi_{t}}\\left(\\tilde{q} \\mid o_{1: t}, a_{1: t-1}\\right)= \\\\\n& \\quad \\mathbb{E}_{\\tilde{q}\\left(s_{1: T}\\right)}\\left[\\ln \\tilde{q}\\left(s_{1: T}\\right)-\\mathbb{E}_{\\tilde{q}(\\theta)}\\left[\\ln p\\left(s_{1: T}, o_{1: t} \\mid \\theta, a_{1: t-1}, \\pi_{t}\\right)\\right]\\right]\n\\end{aligned}\n$$\n\nPerception according to active inference is minimizing $F_{\\pi_{t}}$ w.r.t. $\\tilde{q}\\left(s_{1: T}\\right)$. The minimizer is written $q_{t}\\left(s_{1: T} \\mid \\pi_{t}\\right)=q\\left(s_{1: T} \\mid \\pi_{t}, o_{1: t}, a_{1: t-1}\\right)$.\nWe can use the factorizing properties to rewrite (27) as follows:\n\n$$\n\\begin{aligned}\n& F_{\\pi_{t}}\\left(\\tilde{q} \\mid o_{1: t}, a_{1: t-1}\\right)=\\sum_{\\tau=1}^{T} \\mathbb{E}_{\\tilde{q}\\left(s_{\\tau}\\right)}\\left[\\ln \\tilde{q}\\left(s_{\\tau}\\right)\\right]-\\mathbb{E}_{\\tilde{q}\\left(s_{1}, \\theta^{D}\\right)}\\left[\\ln p\\left(s_{1} \\mid \\theta^{D}\\right)\\right] \\\\\n& \\quad-\\sum_{\\tau=1}^{t} \\mathbb{E}_{\\tilde{q}\\left(s_{\\tau}, \\theta^{A}\\right)} p\\left(o_{\\tau} \\mid s_{\\tau}, \\theta^{A}\\right)-\\sum_{\\tau=2}^{T} \\mathbb{E}_{\\tilde{q}\\left(s_{\\tau}, s_{\\tau-1}, \\theta^{B}\\right)} p\\left(s_{\\tau} \\mid s_{\\tau-1}, a_{\\tau}, \\theta^{B}\\right)\n\\end{aligned}\n$$\n\nwhich is equivalent to equation (6) in [4]. ${ }^{10}$ One can also get rid of the expectations over $\\theta$ by replacing them by an estimator $\\hat{\\theta}$. Then we get\n\n$$\n\\begin{aligned}\n& F_{\\pi_{t}}\\left(\\tilde{q} \\mid o_{1: t}, a_{1: t-1}\\right)=\\sum_{\\tau=1}^{T} \\mathbb{E}_{\\tilde{q}\\left(s_{\\tau}\\right)}\\left[\\ln \\tilde{q}\\left(s_{\\tau}\\right)\\right]-\\mathbb{E}_{\\tilde{q}\\left(s_{1}\\right)}\\left[\\ln p\\left(s_{1} \\mid \\hat{\\theta}^{D}\\right)\\right] \\\\\n& \\quad-\\sum_{\\tau=1}^{t} \\mathbb{E}_{\\tilde{q}\\left(s_{\\tau}\\right)} p\\left(o_{\\tau} \\mid s_{\\tau}, \\hat{\\theta}^{A}\\right)-\\sum_{\\tau=2}^{T} \\mathbb{E}_{\\tilde{q}\\left(s_{\\tau}, s_{\\tau-1}\\right)} p\\left(s_{\\tau} \\mid s_{\\tau-1}, a_{\\tau}, \\hat{\\theta}^{B}\\right)\n\\end{aligned}\n$$\n\nwhich is equivalent to (B.4) in [8].",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 22,
      "text": "# Learning \n\nUpdating the $\\theta$ parameter (learning) happens at the end of an episode $(t=T)$. The agent has observed $o_{1: T}$ and performed $a_{1: T-1}$. The variational free energy from equation (26) can be written as follows:\n\n$$\n\\begin{aligned}\n& F\\left(\\tilde{q} \\mid o_{1: T}, a_{1: T-1}\\right)= \\\\\n& \\quad \\mathbb{E}_{\\tilde{q}(\\theta)}\\left[\\ln \\tilde{q}(\\theta)-\\mathbb{E}_{q_{T}\\left(s_{1: T}\\right)}\\left[\\ln p\\left(o_{1: T}, s_{1: T}, \\theta \\mid a_{1: T-1}\\right)\\right]\\right]+C_{\\backslash \\theta}\n\\end{aligned}\n$$\n\nwhere $C_{\\backslash \\theta}$ is independent of $\\tilde{q}(\\theta)$ and we have fixed $q_{T}\\left(s_{1: T}\\right)$ to be the approximate posterior over states, inferred using the current (not-updated) belief $p(\\theta)$. If the current belief $p(\\theta)$ is a Dirichlet distribution with hyperparameter $\\alpha$, then the minimizer $q_{T}(\\theta)$ of $F$ will also be a Dirichlet distribution with hyperparameter $\\alpha^{\\prime}$ as given in (18)-(20). In Appendix B. 3 we derive this, and relate it to a well known variational inference algorithm called coordinate ascent variational inference (CAVI).\n\n[^0]\n[^0]:    ${ }^{10}$ Note that in [4] only the parameter $\\theta^{A}$ is treated as a variable and $\\theta^{D}$ and $\\theta^{B}$ are considered fixed.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 23,
      "text": "# Action selection \n\nFinally we can also view action selection as the minimization of the variational free energy function (26). We can rewrite this function as follows:\n\n$$\n\\begin{aligned}\nF\\left(\\tilde{q} \\mid o_{1: t}\\right) & =\\mathbb{E}_{\\tilde{q}(\\pi)}\\left[\\ln \\tilde{q}(\\pi)-\\ln p(\\pi)+\\mathbb{E}_{\\tilde{q}\\left(s_{1: T}\\right)}\\left[\\ln \\tilde{q}\\left(s_{1: T}\\right)-\\ln p\\left(o_{1: t}, s_{1: T} \\mid \\pi\\right)\\right]\\right]+C_{\\backslash \\pi} \\\\\n& =\\mathbb{E}_{\\tilde{q}(\\pi)}\\left[\\ln \\tilde{q}(\\pi)-\\ln p(\\pi)+F_{\\pi}\\left(\\tilde{q}\\left(s_{1: T}\\right) \\mid o_{1: t}\\right)\\right]+C_{\\backslash \\pi}\n\\end{aligned}\n$$\n\nwhere $F_{\\pi}$ is defined in (27), $C_{\\backslash \\pi}$ is independent of $\\tilde{q}(\\pi)$, and we replaced the expectation over $\\tilde{q}(\\theta)$ by an estimator $\\hat{\\theta}$ and suppress the dependence of the generative model $p$ on $\\hat{\\theta}$ in the notation. (This is equivalent to the second line in equation (B.7) in [8].) What is important to note here, is that the agent is trying to infer an action sequence (policy) $\\pi$ of both future and past actions. We have therefore dropped the dependence on $a_{1: t-1}$ in both $F$ and $F_{\\pi}$, and instead $\\pi$ is a sequence of action starting at $\\tau=1$ instead of $\\tau=t$. In Section 2 , we always fixed the past actions to the actions that were actually performed, which is no longer the case here.\n\nWe minimize $F$ w.r.t. $\\tilde{q}(\\pi)$ and $\\tilde{q}\\left(s_{1: T}\\right)$ and using (24) we get for the minimizers respectively\n\n$$\nq_{t}(\\pi)=\\sigma\\left(-\\ln p(\\pi)+F_{\\pi}\\left(q_{t}\\left(s_{1: T}\\right) \\mid o_{1: t}\\right)\\right)\n$$\n\nand $q_{t}\\left(s_{1: T}\\right)$ is the minimizer of $F_{\\pi}$. Now we can use\n\n$$\np(\\pi)=\\sigma\\left(\\ln E(\\pi)-G\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)\\right)\n$$\n\nwhich corresponds to the last line equation (B.7) in [8]. ${ }^{11}$ The term $E(\\pi)$ is a habit term, signifying what policies the agent is usually exercising. Plugging this back into (29) gives\n\n$$\nq_{t}(\\pi)=\\sigma\\left(-\\ln E(\\pi)+G\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)+F_{\\pi}\\left(q_{t} \\mid o_{1: t}\\right)\\right)\n$$\n\nwhich corresponds to equation (B.9) in [8].\nRemark 2. We now try to interpret this derivation conceptually. Note that due to (25) we have the following approximate equality:\n\n$$\n\\begin{aligned}\nF\\left(\\tilde{q} \\mid o_{1: t}\\right) & =\\mathbb{E}_{\\tilde{q}(\\pi)}\\left[\\ln \\tilde{q}(\\pi)-\\ln p(\\pi)+F_{\\pi}\\left(\\tilde{q}\\left(s_{1: T}\\right), o_{1: t}\\right)\\right]+C_{\\backslash \\pi} \\\\\n& \\approx \\mathbb{E}_{\\tilde{q}(\\pi)}\\left[\\ln \\tilde{q}(\\pi)-\\ln p(\\pi)-\\ln p\\left(o_{1: t} \\mid \\pi\\right)\\right]+C_{\\backslash \\pi} \\\\\n& =\\mathbb{E}_{\\tilde{q}(\\pi)}\\left[\\ln \\tilde{q}(\\pi)-\\ln p\\left(o_{1: t}, \\pi\\right)\\right]+C_{\\backslash \\pi}\n\\end{aligned}\n$$\n\nwhich implies that the minimizer $q_{t}(\\pi)$ is approximately equal to the posterior $p\\left(\\pi \\mid o_{1: t}\\right)$. In other words, this says that we select the policy that is most probable given the past observations. That is, the agent forgets which past actions it has\n\n[^0]\n[^0]:    ${ }^{11}$ It can be argued that calling this a prior is incorrect, since it actually depends on the observations $o_{1: t}$.\n\nperformed, and tries to infer these based on the past observations. Then it tries to find the most likely sequence of future actions to go with this sequence of past actions. Selecting future actions in this way however only makes sense when certain past action sequences make certain future action sequences more likely. For example, let our action space consist of two actions \\{left, right\\} and policies consist of sequences of actions of length two. Now suppose that the prior over policies dictates that the agent almost certainly performs the policies (left, left) or (right, right). This implies that having inferred the first action gives the agent more information about the most likely next action. However, if both next actions are equally likely given a first action, according to the prior, then the likelihood term $p\\left(o_{1: t} \\mid \\pi\\right)$ does not have any information about the next action. Note that in the calculation of $G$ the past actions are fixed to the actions that have been performed. Therefore $G$ will not make certain future action sequences more likely based on past possible past action sequences. Therefore, the term $F_{\\pi}$ in (30) only becomes relevant when the habit term $E$ makes certain future action sequences more likely based on past action sequences.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 24,
      "text": "# B Learning Preliminaries\n## B. 1 Bayesian belief updating\n\nThe learning process of an active inference agent is formulated as Bayesian belief updating over the parameters. In general, Bayesian belief updating can be described as follows. Let $\\theta$ be the parameter of a model $p_{\\theta}$ we want to learn and $x$ the output of this model. We start with a prior belief $p(\\theta \\mid \\alpha)$ which is parametrized by the hyperparameter $\\alpha$. Now our posterior belief about $\\theta$ is given by the distribution $p(\\theta \\mid x, \\alpha)$ which is obtained by Bayes' rule. In some special cases ${ }^{12}$, the posterior distribution belongs to the same parametrized family as the prior, such that $p(\\theta \\mid x, \\alpha)=p\\left(\\theta \\mid \\alpha^{\\prime}\\right)$. Then the learning can be summarized by the update from $\\alpha$ to $\\alpha^{\\prime}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 25,
      "text": "## B. 2 Categorical model without latent variables\n\nNow we let the model be a categorical distribution over elements $\\left\\{x^{(1)}, \\ldots, x^{(n)}\\right\\}$ parametrized by $\\theta=\\left(\\theta_{1}, \\ldots, \\theta_{n}\\right)$. That is,\n\n$$\np\\left(x^{(i)} \\mid \\theta\\right)=\\theta_{i}, \\quad \\forall i \\in\\{1, \\ldots, n\\}\n$$\n\nThe prior over the parameter $\\theta$ is given by the Dirichlet distribution parametrized by the hyperparameter $\\alpha=\\left(\\alpha_{1}, \\ldots, \\alpha_{n}\\right)$. That is,\n\n$$\np(\\theta \\mid \\alpha) \\propto \\prod_{i} \\theta_{i}^{\\alpha_{i}-1}\n$$\n\n[^0]\n[^0]:    ${ }^{12}$ For details, see the theory of conjugate priors.\n\nAfter observing $x^{*}$, the posterior is given by\n\n$$\n\\begin{aligned}\np\\left(\\theta \\mid x^{*}, \\alpha\\right) & \\propto p\\left(x^{*} \\mid \\theta\\right) p(\\theta \\mid \\alpha) \\\\\n& =\\prod_{i} \\theta_{i}^{\\alpha_{i}-1+\\mathbb{1}_{x^{(i)}}\\left(x^{*}\\right)}\n\\end{aligned}\n$$\n\nNote that this is again a Dirichlet distribution with hyperparameter $\\alpha^{\\prime}$ such that\n\n$$\n\\alpha_{i}^{\\prime}=\\alpha_{i}+\\mathbb{1}_{x^{(i)}}\\left(x^{*}\\right), \\quad \\forall i \\in\\{1, \\ldots, n\\}\n$$\n\nThat is, the hyperparameter corresponding to the observation $x^{*}$ is increased by one. This will make this observation more likely in the updated distribution.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 26,
      "text": "# B. 3 Categorical model with latent variables\n## Exact posterior\n\nNow we let the model be a joint distribution over the product space of observations $x$ and latent states $z$, given by $\\left\\{x^{(1)}, \\ldots, x^{(n)}\\right\\} \\times\\left\\{z^{(1)}, \\ldots, z^{(m)}\\right\\}$. The model is parametrized by $\\theta=\\left(\\theta^{D}, \\theta^{A}\\right)$ as follows:\n\n$$\n\\begin{aligned}\np\\left(z^{(j)} \\mid \\theta^{D}\\right) & =\\theta_{j}^{D} \\\\\np\\left(x^{(i)} \\mid z^{(j)}, \\theta^{A}\\right) & =\\theta_{i j}^{A}\n\\end{aligned}\n$$\n\nThe prior over the $\\theta$ is defined as follows:\n\n$$\n\\begin{aligned}\np(\\theta \\mid \\alpha) & =p\\left(\\theta^{D} \\mid \\alpha^{D}\\right) \\prod_{j} p\\left(\\theta_{\\bullet j}^{A} \\mid \\alpha^{A}\\right) \\\\\np\\left(\\theta^{D} \\mid \\alpha^{D}\\right) & \\propto \\prod_{j}\\left(\\theta_{j}^{D}\\right)^{\\alpha_{j}^{D}-1} \\\\\np\\left(\\theta_{\\bullet j}^{A} \\mid \\alpha^{A}\\right) & \\propto \\prod_{i}\\left(\\theta_{i j}^{A}\\right)^{\\alpha_{i j}^{A}-1}\n\\end{aligned}\n$$\n\nwhere $\\theta_{\\bullet j}$ denotes the vector $\\left(\\theta_{1 j}, \\ldots, \\theta_{n j}\\right)$. After observing $x^{*}=x^{\\left(i^{*}\\right)}$, the exact posterior is given by\n\n$$\n\\begin{aligned}\np(\\theta \\mid x^{*}, \\alpha) & \\propto p\\left(x^{*} \\mid \\theta\\right) p(\\theta \\mid \\alpha) \\\\\n& =\\sum_{j} p\\left(x^{*} \\mid z^{(j)}, \\theta^{A}\\right) p\\left(z^{(j)} \\mid \\theta^{D}\\right) p\\left(\\theta^{A} \\mid \\alpha^{A}\\right) p\\left(\\theta^{D} \\mid \\alpha^{D}\\right) \\\\\n& \\propto \\sum_{j} \\theta_{i^{\\prime} j}^{A} \\theta_{j}^{D}\\left(\\prod_{j^{\\prime}} \\prod_{i^{\\prime}}\\left(\\theta_{i^{\\prime} j^{\\prime}}^{A}\\right)^{\\alpha_{i^{\\prime} j^{\\prime}}^{A}-1}\\right)\\left(\\prod_{j^{\\prime \\prime}}\\left(\\theta_{j^{\\prime}}^{D}\\right)^{\\alpha_{j^{\\prime \\prime}}^{D}-1}\\right) \\\\\n& =\\sum_{j}\\left(\\prod_{j^{\\prime}} \\prod_{i^{\\prime}}\\left(\\theta_{i^{\\prime} j^{\\prime}}^{A}\\right)^{\\alpha_{i^{\\prime} j^{\\prime}}^{A}-1+\\mathbb{1}_{i^{*}}\\left(i^{\\prime}\\right) \\mathbb{1}_{j}\\left(j^{\\prime}\\right)}\\right)\\left(\\prod_{j^{\\prime \\prime}}\\left(\\theta_{j^{\\prime \\prime}}^{D}\\right)^{\\alpha_{j^{\\prime \\prime}}^{D}-1+\\mathbb{1}_{j}\\left(j^{\\prime \\prime}\\right)}\\right)\n\\end{aligned}\n$$\n\nNote that this is no longer a Dirichlet distribution. Below we discuss how the Dirichlet distribution shows up in an algorithm for approximating the true posterior.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 27,
      "text": "# Mean-field approximation \n\nWe can also approximate the posterior over $\\theta$ by minimizing the following variational free energy function:\n\n$$\nF\\left(\\tilde{q} \\mid x^{*}\\right)=\\mathbb{E}_{\\tilde{q}(z, \\theta)}[\\ln \\tilde{q}(z, \\theta)-\\ln p\\left(x^{*}, z, \\theta \\mid \\alpha\\right)],\n$$\n\nwhere we assume the approximate posterior over both $\\theta$ and $z$ factorizes as follows:\n\n$$\nq(z, \\theta)=q(z) q(\\theta)\n$$\n\nThe coordinate ascent variational inference (CAVI) algorithm [2, 1] updates the distributions $q(z)$ and $q(\\theta)$ iteratively, each time holding one distribution fixed while updating the other. More specifically, we can initialize $q(\\theta)$ with our prior belief $p(\\theta \\mid \\alpha)$ and minimize $F$ w.r.t. $\\tilde{q}(z)$. The variational free energy now becomes\n\n$$\nF\\left(\\tilde{q} \\mid x^{*}, q(\\theta)\\right)=\\mathbb{E}_{\\tilde{q}(z)}\\left[\\ln \\tilde{q}(z)-\\mathbb{E}_{q(\\theta)}\\left[\\ln p\\left(z \\mid x^{*}, \\theta\\right)\\right]\\right]+C_{\\backslash z}\n$$\n\nwhere $C_{\\backslash z}$ is independent of $\\tilde{q}(z)$. The minimizer $q(z)$ is proportional to\n\n$$\nq(z) \\propto \\exp \\left(\\mathbb{E}_{q(\\theta)}\\left[\\ln p\\left(z \\mid x^{*}, \\theta\\right)\\right]\\right)\n$$\n\n(See equation (24).) We then fix this $q(z)$ and optimize $F$ w.r.t. $\\tilde{q}(\\theta)$ and get\n\n$$\nF\\left(\\tilde{q} \\mid x^{*}, q(z)\\right)=\\mathbb{E}_{\\tilde{q}(\\theta)}\\left[\\ln \\tilde{q}(\\theta)-\\mathbb{E}_{q(z)}\\left[\\ln p\\left(x^{*}, z, \\theta \\mid \\alpha\\right)\\right]\\right]+C_{\\backslash \\theta}\n$$\n\nwhere $C_{\\backslash \\theta}$ is independent of $\\tilde{q}(\\theta)$. The minimizer $q(\\theta)$ is proportional to\n\n$$\nq(\\theta) \\propto \\exp \\left(\\mathbb{E}_{q(z)}\\left[\\ln p\\left(x^{*}, z, \\theta \\mid \\alpha\\right)\\right]\\right)\n$$\n\nThese two steps are performed iteratively until the beliefs about $\\theta$ and $z$ have converged.\n\nNote that when $p(x, z, \\theta \\mid \\alpha)$ is a categorical model with Dirichlet priors, as defined in (32)-(36), we can rewrite equation (41) as follows:\n\n$$\n\\begin{aligned}\nq(\\theta) & \\propto \\exp \\left(\\mathbb{E}_{q(z)}\\left[\\ln p\\left(x^{*}, z, \\theta \\mid \\alpha\\right)\\right]\\right) \\\\\n& =\\exp \\left(\\mathbb{E}_{q(z)}\\left[\\ln p\\left(x^{*} \\mid z, \\theta^{A}\\right)+\\ln p\\left(z \\mid \\theta^{D}\\right)+\\ln p(\\theta \\mid \\alpha)\\right]\\right) \\\\\n& =\\exp \\left(\\sum_{j} q\\left(z^{(j)}\\right)\\left[\\ln p\\left(x^{*} \\mid z^{(j)}, \\theta^{A}\\right)+\\ln p\\left(z^{(j)} \\mid \\theta^{D}\\right)\\right]\\right) p(\\theta \\mid \\alpha) \\\\\n& =\\prod_{j}\\left(\\theta_{i^{*}}^{A}\\right)^{q\\left(z^{(j)}\\right)}\\left(\\theta_{j}^{D}\\right)^{q\\left(z^{(j)}\\right)}\\left(\\theta_{j}^{D}\\right)^{\\alpha_{j}^{D}-1} \\prod_{i}\\left(\\theta_{i j}^{A}\\right)^{\\alpha_{i j}^{A}-1} \\\\\n& =\\prod_{j}\\left(\\theta_{j}^{D}\\right)^{\\alpha_{j}^{D}+q\\left(z^{(j)}\\right)-1} \\prod_{i}\\left(\\theta_{i j}^{A}\\right)^{\\alpha_{i j}^{A}+\\mathbb{1}_{i^{*}}(i) q\\left(z^{(j)}\\right)-1}\n\\end{aligned}\n$$\n\nNote that this is again a Dirichlet distribution with updated parameter $\\alpha^{\\prime}=$ $\\left(\\alpha^{D^{\\prime}}, \\alpha^{A^{\\prime}}\\right)$ given by\n\n$$\n\\begin{aligned}\n\\alpha_{j}^{D^{\\prime}} & =\\alpha_{j}^{D}+q\\left(z^{(j)}\\right) \\\\\n\\alpha_{i j}^{A^{\\prime}} & =\\alpha_{i j}^{A}+\\mathbb{1}_{i^{*}}(i) q\\left(z^{(j)}\\right)\n\\end{aligned}\n$$\n\nUpdate (42) gives makes latent states with high $q(z)$ more likely in the updated distribution. Update (43) makes sure that latent states are more likely to generate the observation $x^{\\left(i^{*}\\right)}$, especially those with high $q(z)$. Note the similarity with the update rule (31) for categorical models without latent variables.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 28,
      "text": "# B. 4 POMDP model \n\nNow we let $p$ be the generative model from an active inference agent as described in (11)-(17). Similar to (37) the variational free energy now becomes\n\n$$\n\\begin{aligned}\n& F\\left(\\tilde{q} \\mid o_{1: T}, a_{1: T-1}\\right)= \\\\\n& \\quad \\mathbb{E}_{\\tilde{q}\\left(s_{1: T}, \\theta\\right)}\\left[\\ln \\tilde{q}\\left(s_{1: T}, \\theta\\right)-\\ln p\\left(o_{1: T}, s_{1: T}, \\theta \\mid a_{1: T-1}, \\alpha\\right)\\right]\n\\end{aligned}\n$$\n\nWe use the mean-field approximation $q\\left(s_{1: T}, \\theta\\right)=q\\left(s_{1: T}\\right) q(\\theta)$. Equivalent to (38)-(39) we can start by minimizing $F$ w.r.t. $\\tilde{q}\\left(s_{1: T}\\right)$ and get a minimizer $q_{T}\\left(s_{1: T}\\right)$ using the current belief about $\\theta$. Then, similar to (40)-(41) we can use this minimizer to write $F$ as follows:\n\n$$\n\\begin{aligned}\n& F\\left(\\tilde{q} \\mid o_{1: T}, a_{1: T-1}, q_{T}\\left(s_{1: T}\\right)\\right)= \\\\\n& \\quad \\mathbb{E}_{\\tilde{q}(\\theta)}\\left[\\ln \\tilde{q}(\\theta)-\\mathbb{E}_{q_{T}\\left(s_{1: T}\\right)}\\left[\\ln p\\left(o_{1: T}, s_{1: T}, \\theta \\mid a_{1: T-1}, \\alpha\\right)\\right]\\right]+C_{\\backslash \\theta}\n\\end{aligned}\n$$\n\nand work out the minimizer. This gives\n\n$$\n\\begin{aligned}\n& q(\\theta) \\propto \\exp \\left(\\mathbb{E}_{q_{T}\\left(s_{1: T}\\right)}\\left[\\ln p\\left(o_{1: T}, s_{1: T}, \\theta \\mid a_{1: T-1}, \\alpha\\right)\\right]\\right) \\\\\n& =\\exp \\left(\\mathbb{E}_{q_{T}\\left(s_{1: T}\\right)}\\left[\\sum_{\\tau=1}^{T} \\ln p\\left(o_{\\tau} \\mid s_{\\tau}, \\theta^{A}\\right)+\\ln p\\left(s_{1} \\mid \\theta^{D}\\right)\\right.\\right. \\\\\n& \\left.\\left.\\quad+\\sum_{\\tau=2}^{T} \\ln p\\left(s_{\\tau} \\mid s_{\\tau-1}, a_{\\tau-1}, \\theta^{B}\\right)\\right]\\right) p(\\theta \\mid \\alpha) \\\\\n& =\\exp \\left(\\sum_{s_{1: T}} q_{T}\\left(s_{1: T}\\right)\\left[\\sum_{\\tau=1}^{T} \\ln p\\left(o_{\\tau} \\mid s_{\\tau}, \\theta^{A}\\right)+\\ln p\\left(s_{1} \\mid \\theta^{D}\\right)\\right.\\right. \\\\\n& \\left.\\left.\\quad+\\sum_{\\tau=2}^{T} \\ln p\\left(s_{\\tau} \\mid s_{\\tau-1}, a_{\\tau-1}, \\theta^{B}\\right)\\right]\\right) p(\\theta \\mid \\alpha) \\\\\n& =\\prod_{j}\\left(\\theta_{j}^{D}\\right)^{q_{T}\\left(s_{1}^{(j)}\\right)} \\prod_{i}\\left(\\theta_{i j}^{A}\\right)^{\\sum_{\\tau=1}^{T} \\mathbb{1}_{a^{(i)}}\\left(o_{\\tau}\\right) q_{T}\\left(s_{\\tau}^{(j)}\\right)} \\\\\n& \\prod_{k}\\left(\\theta_{j k l}^{B}\\right)^{\\sum_{\\tau=2}^{T} q_{T}\\left(s_{v}^{(j)}\\right) q_{T}\\left(s_{v-1}^{(k)}\\right) \\mathbb{1}_{a^{(l)}}\\left(a_{\\tau-1}\\right)} p(\\theta \\mid \\alpha) \\\\\n& =\\prod_{j}\\left(\\theta_{j}^{D}\\right)^{\\alpha_{j}^{D}+q_{T}\\left(s_{1}^{(j)}\\right)-1} \\prod_{i}\\left(\\theta_{i j}^{A}\\right)^{\\alpha_{i j}^{A}+\\sum_{\\tau=1}^{T} \\mathbb{1}_{a^{(i)}}\\left(o_{\\tau}\\right) q_{T}\\left(s_{\\tau}^{(j)}\\right)-1} \\\\\n& \\prod_{k}\\left(\\theta_{j k l}^{B}\\right)^{\\alpha_{j k l}^{B}+\\sum_{\\tau=2}^{T} q_{T}\\left(s_{v}^{(j)}\\right) q_{T}\\left(s_{v-1}^{(k)}\\right) \\mathbb{1}_{a^{(l)}}\\left(a_{\\tau-1}\\right)-1}\n\\end{aligned}\n$$\n\nThis gives the update rules for $\\alpha$ given in (18)-(20).\nRemark 3. Note that these update rules are actually just the first iteration of the CAVI algorithm described above. It will therefore in general not minimize the variational free energy in equation (44). Instead it minimizes the quantity given in equation (28) and (45) where $q_{T}\\left(s_{1: T}\\right)$ is fixed. Note however that if one would assume $q_{T}\\left(s_{1: T}\\right)$ to be given, the following variational free energy would be the natural choice to minimize:\n\n$$\n\\mathbb{E}_{\\tilde{q}(\\theta)}\\left[\\ln \\tilde{q}(\\theta)-\\ln \\left(\\mathbb{E}_{q_{T}\\left(s_{1: T}\\right)}\\left[p\\left(o_{1: T}, \\theta \\mid s_{1: T}, a_{1: T-1}, \\alpha\\right)\\right]\\right)\\right]\n$$\n\nsince this has as minimizer the exact posterior distribution.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 29,
      "text": "# C Further details\n## C. 1 Equivalent formulations of expected free energy\n\nRecall that the expected free energy in equation (2) is given by\n\n$$\n\\begin{aligned}\nG\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)=- & \\left(\\mathbb{E}_{q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right)}\\left[D_{\\mathrm{KL}}\\left(q_{t}\\left(s_{t+1: T} \\mid o_{t+1: T}, \\pi_{t}\\right) \\| q_{t}\\left(s_{t+1: T} \\mid \\pi_{t}\\right)\\right)\\right] \\\\\n& +\\mathbb{E}_{q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right)}\\left[\\ln p_{C}\\left(o_{t+1: T}\\right)\\right] .\n\\end{aligned}\n$$\n\nWe can derive the equivalent formulation from equation (3) as follows. We first expand the KL divergence term to get\n\n$$\n\\begin{aligned}\nG\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)=\\mathbb{E}_{q_{t}\\left(s_{t+1: T}, o_{t+1: T} \\mid \\pi_{t}\\right)}[ & \\ln q_{t}\\left(s_{t+1: T} \\mid \\pi_{t}\\right)-\\ln q_{t}\\left(s_{t+1: T} \\mid o_{t+1: T}, \\pi_{t}\\right) \\\\\n& \\left.-\\ln p_{C}\\left(o_{t+1: T}\\right)\\right]\n\\end{aligned}\n$$\n\nUsing Bayes' rule we can rewrite\n\n$$\n\\begin{aligned}\n-\\ln q_{t}\\left(s_{t+1: T} \\mid o_{t+1: T}, \\pi_{t}\\right)= & -\\ln q_{t}\\left(o_{t+1: T} \\mid s_{t+1: T}, \\pi_{t}\\right)-\\ln q_{t}\\left(s_{t+1: T} \\mid \\pi_{t}\\right) \\\\\n& +\\ln q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right)\n\\end{aligned}\n$$\n\nPlugging this into (46) and using that $q_{t}\\left(o_{t+1: T} \\mid s_{t+1: T}, \\pi_{t}\\right)=p\\left(o_{t+1: T} \\mid s_{t+1: T}\\right)$ we get\n\n$$\n\\begin{aligned}\nG\\left(\\pi_{t} \\mid o_{1: t}, a_{1: t-1}\\right)=\\mathbb{E}_{q_{t}\\left(s_{t+1: T}, o_{t+1: T} \\mid \\pi_{t}\\right)}[ & \\ln p\\left(o_{t+1: T} \\mid s_{t+1: T}\\right)+\\ln q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right) \\\\\n& \\left.-\\ln p_{C}\\left(o_{t+1: T}\\right)\\right] \\\\\n= & \\mathbb{E}_{q_{t}\\left(s_{t+1: T} \\mid \\pi_{t}\\right)}\\left[\\mathrm{H}\\left[p\\left(o_{t+1: T} \\mid s_{t+1: T}\\right)\\right]\\right] \\\\\n& +D_{\\mathrm{KL}}\\left(q_{t}\\left(o_{t+1: T} \\mid \\pi_{t}\\right) \\| p_{C}\\left(o_{t+1: T}\\right)\\right)\n\\end{aligned}\n$$\n\nWhere the last line is equal to equation (3).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 30,
      "text": "## C. 2 Independence between state factors and observation modalities\n\nIn order to make the computation of state inference more efficient, the agent can use independencies between different state factors and observation modalities (different dimensions of state and observation space). More specifically, we assume that given a state, the different observation modalities are independent, which translates to:\n\n$$\np\\left(o_{\\tau} \\mid s_{\\tau}\\right)=\\prod_{m} p\\left(o_{\\tau}^{m} \\mid s_{\\tau}\\right)\n$$\n\nWe use superscript $m$ and $f$ to denote a specific observation modalities and state factors respectively. Furthermore we assume a certain state factor to be independent of all other state factors in the same and previous time step, given the same state factor in the previous time step and the last action, i.e.:\n\n$$\np\\left(s_{\\tau} \\mid s_{\\tau-1}, a_{\\tau-1}\\right)=\\prod_{f} p\\left(s_{\\tau}^{f} \\mid s_{\\tau-1}^{f}, a_{\\tau-1}\\right)\n$$\n\nFor a fixed state factor $f$ equation (6) and (7) now become\n\n$$\n\\begin{aligned}\nq_{t}\\left(s_{t}^{f}\\right) & \\propto p\\left(o_{t} \\mid s_{t}^{f}\\right) \\sum_{s_{t-1}^{f}} p\\left(s_{t}^{f} \\mid s_{t-1}^{f}, a_{t-1}\\right) q_{t-1}\\left(s_{t-1}^{f}\\right) \\\\\nq_{t}\\left(s_{\\tau}^{f} \\mid a_{1: \\tau-1}\\right) & =\\sum_{s_{\\tau-1}^{f}} p\\left(s_{\\tau}^{f} \\mid s_{\\tau-1}^{f}, a_{\\tau-1}\\right) q_{t}\\left(s_{\\tau-1}^{f} \\mid a_{1: \\tau-2}\\right)\n\\end{aligned}\n$$\n\nand equation (10) becomes\n\n$$\nq_{t}\\left(s_{1: T} \\mid \\pi_{t}\\right) \\propto \\prod_{\\tau=1}^{t} \\prod_{m} p\\left(o_{\\tau}^{m} \\mid s_{\\tau}\\right) p\\left(s_{1}\\right) \\prod_{\\tau=2}^{T} \\prod_{f} p\\left(s_{\\tau}^{f} \\mid s_{\\tau-1}^{f}, a_{\\tau-1}\\right)\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 31,
      "text": "# C. 3 Fixed point iteration \n\nEquation (47) involves the distribution $p\\left(o_{t} \\mid s_{t}^{f}\\right)$. We can however not access this directly. To find an approximate solution, we can use fixed point iteration as follows:\n\n$$\nq_{t}^{(i+1)}\\left(s_{t}^{f}\\right) \\propto \\sum_{s_{t}^{i j}} q_{t}^{(i)}\\left(s_{t}^{i j}\\right) p\\left(o_{t} \\mid s_{t}\\right) \\sum_{s_{t-1}^{f}} p\\left(s_{t}^{f} \\mid s_{t-1}^{f}, a_{t-1}\\right) q_{t-1}\\left(s_{t-1}^{f}\\right)\n$$\n\nwhere $\\backslash f$ denotes the set of all state factors apart from $f$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 32,
      "text": "## C. 4 Softmax function\n\nDefinition 1. Let $\\mathcal{S}=\\left\\{x^{(1)}, \\ldots, x^{(n)}\\right\\}$ be a finite set and $\\mu: \\mathcal{S} \\rightarrow \\mathbb{R}$ a function. The softmax function $\\sigma$ is given by\n\n$$\n\\sigma\\left(\\mu\\left(x^{(i)}\\right)\\right)=\\frac{e^{\\mu\\left(x^{(i)}\\right)}}{\\sum_{j} e^{\\mu\\left(x^{(j)}\\right)}}\n$$\n\nNote that $\\sigma\\left(\\mu\\left(x^{(i)}\\right)\\right)>1$ and $\\sum_{\\mathcal{S}} \\sigma\\left(\\mu\\left(x^{(i)}\\right)\\right)=1$. Therefore the softmax function can be used to turn $\\mu$ into a probability distribution.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2406.07726v3",
  "authors": [
    "Jesse van Oostrum",
    "Carlotta Langer",
    "Nihat Ay"
  ],
  "categories": [
    "cs.LG",
    "q-bio.NC"
  ],
  "abstract": "In this paper we present a concise mathematical description of active\ninference in discrete time. The main part of the paper serves as a basic\nintroduction to the topic, including a detailed example of the action selection\nmechanism. The appendix discusses the more subtle mathematical details,\ntargeting readers who have already studied the active inference literature but\nstruggle to make sense of the mathematical details and derivations. Throughout,\nwe emphasize precise and standard mathematical notation, ensuring consistency\nwith existing texts and linking all equations to widely used references on\nactive inference. Additionally, we provide Python code that implements the\naction selection and learning mechanisms described in this paper and is\ncompatible with pymdp environments.",
  "updated": "2025-04-09T17:54:25Z",
  "published": "2024-06-11T21:09:45Z"
}