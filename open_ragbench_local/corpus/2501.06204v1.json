{"title": "Voronovskaya-Type Asymptotic Expansions and Convergence Analysis for\n  Neural Network Operators in Complex Domains", "sections": [{"section_id": 0, "text": "#### Abstract\n\nThis paper extends the classical theory of Voronovskaya-type asymptotic expansions to generalized neural network operators defined on non-Euclidean and fractal domains. We introduce and analyze smooth operators activated by modified and generalized hyperbolic tangent functions, extending their applicability to manifold and fractal geometries. Key theoretical results include the preservation of density properties, detailed convergence rates, and asymptotic expansions. Additionally, we explore the role of fractional derivatives in defining neural network operators, which capture non-local behavior and are particularly useful for modeling systems with long-range dependencies or fractal-like structures. Our findings contribute to a deeper understanding of neural network operators in complex, structured spaces, offering robust mathematical tools for applications in signal processing on manifolds and solving partial differential equations on fractals.\n\n\nKeywords: Neural network operators, Voronovskaya-type expansions, Non-Euclidean domains, Fractional calculus, Fractals.", "tables": {}, "images": {}}, {"section_id": 1, "text": "## Contents\n\n1 Introduction 2\n2 Preliminaries 3\n2.1 Density Functions on Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3 Generalized Neural Network Operators 4\n3.1 Basic Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.2 Kantorovich-Type Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3.3 Fractional Neural Network Operator . . . . . . . . . . . . . . . . . . . . . . . 5\n4 Main Results 5\n4.1 Voronovskaya-Type Expansions . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n4.2 Step 1: Review of the Operator $A_{n}$ . . . . . . . . . . . . . . . . . . . . . . . . 5\n4.3 Step 2: Simplifying the Terms of the Expansion . . . . . . . . . . . . . . . . . . 6\n4.4 Step 3: Conclusion of the Expansion . . . . . . . . . . . . . . . . . . . . . . . 6\n4.5 Theorem: Voronovskaya Expansion for $A_{n}$ . . . . . . . . . . . . . . . . . . . . 7\n5 Fractional Convergence 8\n5.0.1 Definition of the Fractional Operator $Q_{n}$ . . . . . . . . . . . . . . . . . . 8\n5.0.2 Expansion of the Fractional Derivative . . . . . . . . . . . . . . . . . . . 8\n\n5.0.3 Substituting into the Expression for $Q_{n}(f ; x)$ ..... 8\n5.0.4 Analyzing the Zeroth Order Terms ..... 8\n5.0.5 Analyzing the Higher-Order Terms ..... 9\n5.0.6 Asymptotic Convergence ..... 9\n6 Theorem: Asymptotic Expansion of Quasi-Interpolation Operators ..... 9\n7 Detailed Proofs ..... 10\n7.1 Asymptotic Expansion of Quasi-Interpolation Operators ..... 10\n8 Theorem: Uniform Convergence of Generalized Neural Network Operators on Manifolds with\n9 Results ..... 13\n10 Conclusions ..... 14", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 1 Introduction \n\nThe study of neural network operators has evolved significantly over the past few decades, driven by the need for robust mathematical tools capable of approximating complex functions and understanding convergence properties in diverse spaces. The classical theory of neural network operators, as pioneered by researchers like George A. Anastassiou (1997, 2023) [1, 2], has laid a solid foundation for function approximation and convergence analysis. These works have primarily focused on Euclidean domains, where the geometry is relatively straightforward.\n\nThe field of neural network operators has seen significant advancements, particularly in the context of non-Euclidean and fractal domains. Recent works have explored the use of fractional derivatives and generalized activation functions to extend the applicability of these operators. For instance, Kilbas et al. [3] and Podlubny [4] have provided comprehensive treatments of fractional differential equations, highlighting their utility in modeling systems with long-range dependencies and fractal-like structures.\n\nMagin [5] has demonstrated the application of fractional calculus in bioengineering, showcasing the potential of these methods in practical scenarios. Similarly, Tarasov [6] has explored the applications of fractional calculus to the dynamics of particles, fields, and media, further emphasizing the versatility of these tools.\n\nSamko et al. [7] and Mainardi [8] have contributed to the theoretical foundations of fractional integrals and derivatives, providing a robust framework for their application in various fields. West et al. [9] have delved into the physics of fractal operators, offering insights into their behavior and potential applications.\n\nHowever, many real-world applications, such as those in data science, signal processing, and partial differential equations, often involve non-Euclidean and fractal domains. These domains present unique challenges due to their complex geometries and intricate structures. Traditional neural network operators, designed for Euclidean spaces, may not be directly applicable or efficient in these settings. This gap in the literature highlights the need for generalized neural network operators that can handle the complexities of non-Euclidean and fractal geometries.\n\nThis research aims to address these challenges by extending the classical theory of neural network operators to non-Euclidean and fractal domains. Specifically, we introduce generalizations of traditional activation functions, such as modified hyperbolic tangent functions, and operators defined using fractional derivatives. These generalizations are crucial for modeling complex, structured spaces that are prevalent in applied mathematics and data science.\n\nThe primary goal of this work is to develop a comprehensive framework for neural network operators in non-Euclidean and fractal domains. We focus on the following key aspects:\n\n1. Generalized Activation Functions: We introduce modified hyperbolic tangent functions that are suitable for non-Euclidean settings. These functions preserve essential properties such as symmetry, normalization, and derivative behavior, making them ideal for neural network modeling in complex spaces.\n2. Fractional Derivatives: We incorporate fractional derivatives into the definition of neural network operators. Fractional derivatives capture non-local behavior and are particularly useful for modeling systems with long-range dependencies or fractal-like structures.\n3. Convergence Analysis: We provide a detailed analysis of the convergence rates and asymptotic expansions of the generalized neural network operators. This analysis ensures the robustness and applicability of these operators in diverse spaces.\n\nBy addressing these aspects, we aim to fill the gaps in the current literature and provide a deeper understanding of the behavior of neural network operators in complex, structured spaces. This research contributes to the field by offering new mathematical tools that can be effectively used in various applications, including signal processing on manifolds and solving partial differential equations on fractals.\n\nIn the following sections, we will present the theoretical framework, key results, and detailed analyses that support our contributions.", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 2 Preliminaries \n\nLet $\\mathcal{M}$ denote a Riemannian manifold with metric $g_{i j}$, where $g_{i j}$ represents the components of the metric tensor in local coordinates. A manifold $\\mathcal{M}$ is equipped with a set of smooth charts $\\left\\{\\left(U_{i}, \\phi_{i}\\right)\\right\\}_{i \\in I}$, where $U_{i} \\subset \\mathbb{R}^{N}$ are open subsets and $\\phi_{i}: U_{i} \\rightarrow \\mathcal{M}$ are smooth, bijective maps. These charts allow us to represent points in $\\mathcal{M}$ using local coordinates. The Riemannian metric $g_{i j}$ defines the inner product structure at each point, enabling us to define distances and geometric quantities on $\\mathcal{M}$.\n\nFor further analysis, we define the following generalized activation function:\n\n$$\nh_{q, \\alpha}(x)=\\frac{e^{\\alpha x}-q e^{-\\alpha x}}{(1+q) e^{\\alpha x}+(1-q) e^{-\\alpha x}}, \\quad q>0, \\alpha>0, x \\in \\mathbb{R}\n$$\n\nThis function is a natural generalization of the hyperbolic tangent function and possesses key properties that make it suitable for neural network modeling in non-Euclidean settings. In particular, we focus on its behavior under transformation in Riemannian manifolds, its symmetry, normalization, and its relationship with the derivatives. The following properties are critical for establishing its relevance in the context of approximation and operator theory:\n\n1. Symmetry: The function $h_{q, \\alpha}(x)$ is odd, i.e.,\n\n$$\nh_{q, \\alpha}(-x)=-h_{q, \\alpha}(x), \\quad \\forall x \\in \\mathbb{R}\n$$\n\nThis symmetry ensures that the function preserves structure when reflected about the origin, a property that is valuable in maintaining the consistency of solutions in models based on symmetry.\n2. Normalization: The function is bounded, i.e., $\\left|h_{q, \\alpha}(x)\\right| \\leq 1$ for all $x \\in \\mathbb{R}$ and for any $q>0, \\alpha>0$. This ensures that the activation function can be effectively used within the neural network architecture, where bounded outputs are essential for stability in optimization algorithms.\n3. Derivative Behavior: The derivative of the function is given by:\n\n$$\nh_{q, \\alpha}^{\\prime}(x)=\\frac{2 \\alpha\\left(1-q^{2}\\right)}{\\left[(1+q) e^{\\alpha x}+(1-q) e^{-\\alpha x}\\right]^{2}}\n$$\n\nThis derivative exhibits the influence of both the deformation parameter $q$ and the steepness parameter $\\alpha$, allowing for fine control over the function's local behavior. Specifically, the steepness of the activation function is modulated by $\\alpha$, while the symmetry of the function is controlled by $q$.\n\nTo extend the function $h_{q, \\alpha}(x)$ to the manifold $\\mathcal{M}$, we define the localized activation function in each chart $\\phi_{i}$ as follows:\n\n$$\nh_{q, \\alpha}^{\\mathcal{M}}(x)=h_{q, \\alpha}\\left(\\phi_{i}^{-1}(x)\\right), \\quad x \\in U_{i} \\subset \\mathcal{M}\n$$\n\nThis ensures that the function remains compatible with the global structure of $\\mathcal{M}$ through the atlas $\\left\\{\\left(U_{i}, \\phi_{i}\\right)\\right\\}_{i \\in I}$. The mapping from local to global coordinates ensures that the activation function can be applied to the manifold in a well-defined manner.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 2.1 Density Functions on Manifolds \n\nWe define a generalized density function on the manifold $\\mathcal{M}$ as follows:\n\n$$\n\\phi_{q, \\alpha}(x)=\\frac{1}{\\sqrt{\\operatorname{det}(g(x))}} \\prod_{i=1}^{N} h_{q, \\alpha}\\left(x_{i}\\right), \\quad x \\in \\mathcal{M}\n$$\n\nHere, $\\operatorname{det}(g(x))$ denotes the determinant of the metric tensor at the point $x \\in \\mathcal{M}$. This ensures that $\\phi_{q, \\alpha}(x)$ is normalized, i.e.,\n\n$$\n\\int_{\\mathcal{M}} \\phi_{q, \\alpha}(x) d V(x)=1\n$$\n\nwhere $d V(x)$ represents the volume element on $\\mathcal{M}$. The condition in equation (6) guarantees that the density function integrates to 1 over the manifold. This is an essential property for the construction of probability densities and for ensuring the stability of numerical methods that rely on these density functions.", "tables": {}, "images": {}}, {"section_id": 5, "text": "## 3 Generalized Neural Network Operators\n### 3.1 Basic Operator\n\nLet $f \\in C^{m}(\\mathcal{M})$, where $C^{m}(\\mathcal{M})$ denotes the space of functions with continuous derivatives up to order $m$ on the manifold $\\mathcal{M}$. The basic quasi-interpolation operator, which is a fundamental building block in constructing neural network operators, is defined as:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}} f\\left(\\frac{k}{n}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nwhere $Z_{q, \\alpha}(x)$ is the normalized product of density functions:\n\n$$\nZ_{q, \\alpha}(x)=\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\n$$\n\nand $\\phi_{q, \\alpha}\\left(x_{i}\\right)$ is the generalized density function defined on the manifold $\\mathcal{M}$, as given in Equation (5). The operator $A_{n}(f ; x)$ approximates the function $f$ at the points $\\frac{k}{n}$, where $k \\in \\mathbb{Z}^{N}$, with the contribution of each point weighted by the density function $Z_{q, \\alpha}(x)$. This approach is a generalized form of classical interpolation theory, adapted for manifolds and non-Euclidean geometries. It is important to emphasize that the weights are determined by the local geometry of the manifold $\\mathcal{M}$, which allows for a more accurate approximation in the context of complex, non-Euclidean spaces.", "tables": {}, "images": {}}, {"section_id": 6, "text": "# 3.2 Kantorovich-Type Operator \n\nThe Kantorovich-type operator extends the basic operator by incorporating an averaging procedure over small intervals. It is defined as:\n\n$$\nK_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}}\\left(n^{N} \\int_{\\frac{k}{n}}^{\\frac{k+1}{n}} f(t) d t\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nIn this definition, the term $n^{N} \\int_{\\frac{k}{n}}^{\\frac{k+1}{n}} f(t) d t$ represents the weighted integral of the function $f$ over a small interval centered at $\\frac{k}{n}$, with the weight determined by the density function $Z_{q, \\alpha}(x)$. The integration introduces a smoothing effect, which is particularly useful for approximating functions that exhibit local regularity. The use of fractional intervals ensures that the approximation is well-suited for capturing continuous variations within each small region of the manifold $\\mathcal{M}$.", "tables": {}, "images": {}}, {"section_id": 7, "text": "### 3.3 Fractional Neural Network Operator\n\nThe fractional neural network operator introduces a higher level of generality by involving fractional derivatives. It is defined as:\n\n$$\nQ_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}} D^{\\beta} f\\left(\\frac{k}{n}\\right) Z_{q, \\alpha}(n x-k), \\quad \\beta>0\n$$\n\nHere, $D^{\\beta} f(x)$ denotes the fractional derivative of order $\\beta$ of the function $f$. The fractional derivative is defined as:\n\n$$\nD^{\\beta} f(x)=\\frac{1}{\\Gamma(1-\\beta)} \\frac{d}{d x} \\int_{0}^{x} \\frac{f(t)}{(x-t)^{\\beta}} d t\n$$\n\nwhere $\\Gamma(\\cdot)$ is the Gamma function, which generalizes the factorial function to real and complex numbers. The fractional derivative captures non-local behavior and can model systems that exhibit long-range dependencies or fractal-like structures, which are common in many complex systems, such as those found in fractal geometry, turbulence, and anomalous diffusion. By using fractional derivatives, the operator $Q_{n}(f ; x)$ allows for a more flexible approximation that can adapt to non-smooth and irregular dynamics, providing a powerful tool for capturing intricate behaviors in the manifold $\\mathcal{M}$.", "tables": {}, "images": {}}, {"section_id": 8, "text": "## 4 Main Results\n### 4.1 Voronovskaya-Type Expansions\n\nThe main result establishes the asymptotic behavior of the operators $A_{n}, K_{n}$, and $Q_{n}$. For $f \\in C^{m}(\\mathcal{M})$, the expansion is given by:\n\n$$\nA_{n}(f ; x)-f(x)=\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nwhere the sum is taken over multi-indices $\\alpha$ such that $|\\alpha|=1,2, \\ldots, m$, and $D^{\\alpha}$ denotes the derivative of $f$ of order $\\alpha$.", "tables": {}, "images": {}}, {"section_id": 9, "text": "### 4.2 Step 1: Review of the Operator $A_{n}$\n\nThe operator $A_{n}(f ; x)$ is defined as the weighted sum of evaluations of $f$ at points $\\frac{k}{n}$, with weights given by the density function $Z_{q, \\alpha}(n x-k)$, as shown in the following formula:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}} f\\left(\\frac{k}{n}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nThe goal is to analyze the asymptotic behavior of $A_{n}(f ; x)$ around $f(x)$, using a Taylor expansion to approximate $f\\left(\\frac{k}{n}\\right)$ around $x$. The Taylor expansion of $f$ around $x$ is given by:\n\n$$\nf\\left(\\frac{k}{n}\\right)=f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nSubstituting this expansion into the expression for $A_{n}(f ; x)$, we get:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}}\\left[f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\\right] Z_{q, \\alpha}(n x-k)\n$$", "tables": {}, "images": {}}, {"section_id": 10, "text": "# 4.3 Step 2: Simplifying the Terms of the Expansion \n\nNow, we can separate the terms of the expansion:\n\n1. Constant term: The first term of the expansion is simply $f(x)$ multiplied by the sum of $Z_{q, \\alpha}(n x-k)$ :\n\n$$\nf(x) \\sum_{k \\in \\mathbb{Z}^{N}} Z_{q, \\alpha}(n x-k)=f(x)\n$$\n\nNote that the summation $\\sum_{k \\in \\mathbb{Z}^{N}} Z_{q, \\alpha}(n x-k)$ can be interpreted as a form of normalization, ensuring that the sum equals 1 due to the property of the density function.\n2. Linear and higher-order terms: For the higher-order terms, we have:\n\n$$\n\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} \\sum_{k \\in \\mathbb{Z}^{N}}\\left(\\frac{k}{n}-x\\right)^{\\alpha} Z_{q, \\alpha}(n x-k)\n$$\n\nThese terms involve derivatives of $f$ and can be interpreted as higher-order approximations, taking into account the contribution of the derivatives of $f$ and the density functions.\n3. Higher-order error term: Finally, the higher-order error term is given by:\n\n$$\n\\sum_{k \\in \\mathbb{Z}^{N}} O\\left(\\frac{1}{n^{m}}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nThis term represents the error that decays rapidly as $n \\rightarrow \\infty$, specifically at the rate $O\\left(\\frac{1}{n^{m}}\\right)$.", "tables": {}, "images": {}}, {"section_id": 11, "text": "### 4.4 Step 3: Conclusion of the Expansion\n\nBy combining all the terms, we obtain the final expansion for $A_{n}(f ; x)$ :\n\n$$\nA_{n}(f ; x)=f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nwhere $A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)$ represents the contribution of the density terms in the expansion and $O\\left(\\frac{1}{n^{m}}\\right)$ is the asymptotic error of the approximation.", "tables": {}, "images": {}}, {"section_id": 12, "text": "# 4.5 Theorem: Voronovskaya Expansion for $A_{n}$ \n\nTheorem: For a function $f \\in C^{m}(\\mathcal{M})$, the asymptotic expansion of the operator $A_{n}$ is given by:\n\n$$\nA_{n}(f ; x)-f(x)=\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nwhere the sum is taken over multi-indices $\\alpha$ of order 1 to $m$, and $D^{\\alpha}$ denotes the derivatives of $f$ of order $\\alpha$.\n\nProof. The proof proceeds in several steps, starting with the Taylor expansion of $f\\left(\\frac{k}{n}\\right)$ around $x$. We begin by considering the Taylor series of $f$ at the point $x$ :\n\n$$\nf\\left(\\frac{k}{n}\\right)=f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nwhere $\\alpha$ represents multi-indices, and $D^{\\alpha} f(x)$ denotes the derivative of $f$ of order $\\alpha$.\nNext, substitute this expansion into the expression for the operator $A_{n}(f ; x)$. We then have:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}}\\left[f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\\right] Z_{q, \\alpha}(n x-k)\n$$\n\nThis expression can be split into three distinct parts:\n\n1. The constant term: The term involving $f(x)$, which is the approximation of $f$ at $x$ :\n\n$$\nf(x) \\sum_{k \\in \\mathbb{Z}^{N}} Z_{q, \\alpha}(n x-k)=f(x)\n$$\n\nSince $Z_{q, \\alpha}(n x-k)$ represents a normalized density function, the sum over $k$ yields 1 .\n2. The linear and higher-order terms: The terms involving the derivatives of $f$ are given by:\n\n$$\n\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} \\sum_{k \\in \\mathbb{Z}^{N}}\\left(\\frac{k}{n}-x\\right)^{\\alpha} Z_{q, \\alpha}(n x-k)\n$$\n\nThese terms account for the local behavior of $f$ around $x$, with the density function $Z_{q, \\alpha}(n x-$ $k$ ) modulating the contributions from the higher-order derivatives.\n3. The higher-order error term: The remaining error term, which arises from the approximation of $f$ at higher orders, is given by:\n\n$$\n\\sum_{k \\in \\mathbb{Z}^{N}} O\\left(\\frac{1}{n^{m}}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nThis term represents the error in the approximation of $f$ and decays at a rate of $O\\left(\\frac{1}{n^{m}}\\right)$, ensuring that the approximation becomes more accurate as $n \\rightarrow \\infty$.\n\nCombining these three parts, we obtain the desired asymptotic expansion:\n\n$$\nA_{n}(f ; x)=f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nThus, the operator $A_{n}(f ; x)$ converges to $f(x)$ at the rate $O\\left(\\frac{1}{n^{m}}\\right)$, with the correction terms involving the higher derivatives of $f$ and the influence of the density functions $\\phi_{q, \\alpha}\\left(x_{i}\\right)$. This completes the proof of the Voronovskaya-type expansion.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# 5 Fractional Convergence \n\nConsider the fractional operator $Q_{n}$ acting on a function $f$ defined on the manifold $\\mathcal{M}$, where $\\beta>0$ is the order of the fractional derivative. The goal is to prove the following convergence result:\n\n$$\nQ_{n}(f ; x)-f(x)=O\\left(\\frac{1}{n^{m-\\beta}}\\right), \\quad \\text { as } n \\rightarrow \\infty\n$$\n\nwhere $O\\left(\\frac{1}{n^{m-\\beta}}\\right)$ denotes the error term associated with the approximation of the function $f$ using the fractional operator $Q_{n}$.", "tables": {}, "images": {}}, {"section_id": 14, "text": "### 5.0.1 Definition of the Fractional Operator $Q_{n}$\n\nThe fractional operator $Q_{n}(f ; x)$ is given by the expression:\n\n$$\nQ_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}} D^{\\beta} f\\left(\\frac{k}{n}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nwhere $D^{\\beta} f$ denotes the fractional derivative of order $\\beta$, and $Z_{q, \\alpha}(n x-k)$ is the density function associated with the position $k$ on $\\mathcal{M}$, which depends on the local geometry of the manifold.", "tables": {}, "images": {}}, {"section_id": 15, "text": "### 5.0.2 Expansion of the Fractional Derivative\n\nThe key to analyzing the convergence of the operator $Q_{n}$ is to expand the fractional derivative $D^{\\beta} f$. We start by expanding $D^{\\beta} f\\left(\\frac{k}{n}\\right)$ around the point $x$ using a Taylor series. According to the theory of fractional derivatives, we can write the following expansion:\n\n$$\nD^{\\beta} f\\left(\\frac{k}{n}\\right)=D^{\\beta} f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\beta+\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nwhere $D^{\\beta+\\alpha} f(x)$ denotes the derivatives of order $\\beta+\\alpha$ of $f$, and the error term $O\\left(\\frac{1}{n^{m}}\\right)$ results from truncating the Taylor series. This type of expansion is typical when dealing with fractional derivatives.", "tables": {}, "images": {}}, {"section_id": 16, "text": "### 5.0.3 Substituting into the Expression for $Q_{n}(f ; x)$\n\nNow, we substitute this expansion into the expression for $Q_{n}(f ; x)$. We obtain:\n\n$$\nQ_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}}\\left[D^{\\beta} f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\beta+\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\\right] Z_{q, \\alpha}(n x-k)\n$$", "tables": {}, "images": {}}, {"section_id": 17, "text": "### 5.0.4 Analyzing the Zeroth Order Terms\n\nThe first term in the sum is $D^{\\beta} f(x)$. Since the density function $Z_{q, \\alpha}(n x-k)$ is normalized, we have:\n\n$$\n\\sum_{k \\in \\mathbb{Z}^{N}} Z_{q, \\alpha}(n x-k)=1\n$$\n\nTherefore, the contribution of the first term is simply:\n\n$$\nD^{\\beta} f(x) \\sum_{k \\in \\mathbb{Z}^{N}} Z_{q, \\alpha}(n x-k)=D^{\\beta} f(x)\n$$", "tables": {}, "images": {}}, {"section_id": 18, "text": "# 5.0.5 Analyzing the Higher-Order Terms \n\nNext, we consider the terms involving higher-order derivatives of $f$. These terms are given by:\n\n$$\n\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\beta+\\alpha} f(x)}{\\alpha!} \\sum_{k \\in \\mathbb{Z}^{N}}\\left(\\frac{k}{n}-x\\right)^{\\alpha} Z_{q, \\alpha}(n x-k)\n$$\n\nThese terms account for the local behavior of $f$ around $x$, with the density function $Z_{q, \\alpha}(n x-$ $k$ ) modulating the contributions from the higher-order derivatives.\n\nTo ensure convergence, we analyze the asymptotic behavior of these sums. The decay of $Z_{q, \\alpha}(n x-k)$ as $n \\rightarrow \\infty$ ensures that the contribution from the higher-order terms decays at a rate of $O\\left(\\frac{1}{n^{m-\\beta}}\\right)$.", "tables": {}, "images": {}}, {"section_id": 19, "text": "### 5.0.6 Asymptotic Convergence\n\nFinally, by combining the contributions from the zeroth-order term and the higher-order terms, we obtain the desired asymptotic expansion:\n\n$$\nQ_{n}(f ; x)-f(x)=O\\left(\\frac{1}{n^{m-\\beta}}\\right)\n$$\n\nThis shows that the approximation of $f(x)$ using the fractional operator $Q_{n}$ converges with a rate of $O\\left(\\frac{1}{n^{m-\\beta}}\\right)$, where $m$ is the regularity of $f$ and $\\beta$ is the order of the fractional derivative.\n\nThis completes the proof of the fractional convergence theorem for the operator $Q_{n}$.\nThus, we have shown that the approximation of $f(x)$ using the fractional operator $Q_{n}$ converges asymptotically with a rate $O\\left(\\frac{1}{n^{m-\\beta}}\\right)$, considering both the order of the fractional derivative $\\beta$ and the regularity $m$ of the function $f$.", "tables": {}, "images": {}}, {"section_id": 20, "text": "## 6 Theorem: Asymptotic Expansion of Quasi-Interpolation Operators\n\nTheorem 1: Let $f \\in C^{m}(\\mathcal{M})$, where $C^{m}(\\mathcal{M})$ denotes the space of functions with continuous derivatives of order $m$ on a Riemannian manifold $\\mathcal{M}$. Also, let $A_{n}(f ; x)$ be the quasiinterpolation operator defined by:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}} f\\left(\\frac{k}{n}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nwhere $Z_{q, \\alpha}(x)=\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)$. Then the asymptotic expansion of $A_{n}(f ; x)$ around $f(x)$ is given by:\n\n$$\nA_{n}(f ; x)-f(x)=\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nProof. We proceed step by step to derive the expansion: The operator $A_{n}(f ; x)$ is defined as a sum over $k \\in \\mathbb{Z}^{N}$, where $f\\left(\\frac{k}{n}\\right)$ represents the values of $f$ at the scaled points $\\frac{k}{n}$, and $Z_{q, \\alpha}(n x-k)$ serves as a weight function based on the manifold's geometry. Rewriting it explicitly:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}} f\\left(\\frac{k}{n}\\right) \\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}-\\frac{k_{i}}{n}\\right)\n$$\n\nWe apply the Taylor expansion of $f$ around $x$ :\n\n$$\nf\\left(\\frac{k}{n}\\right)=f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nwhere $\\left(\\frac{k}{n}-x\\right)^{\\alpha}$ is the monomial expansion of the distance between $\\frac{k}{n}$ and $x$, and the term $O\\left(\\frac{1}{n^{m}}\\right)$ accounts for the error in the approximation.\n\nSubstituting the Taylor expansion into the expression for $A_{n}(f ; x)$, we get:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}}\\left[f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\\right] Z_{q, \\alpha}(n x-k)\n$$\n\nWe now separate the terms based on the degree of the expansion.\nThe zero-order term corresponds to:\n\n$$\n\\sum_{k \\in \\mathbb{Z}^{N}} f(x) Z_{q, \\alpha}(n x-k)=f(x)\n$$\n\nsince the sum of $Z_{q, \\alpha}(n x-k)$ over all $k \\in \\mathbb{Z}^{N}$ is normalized to 1 .\nThe higher-order terms involve the derivatives of $f$ and are weighted by the products of $Z_{q, \\alpha}(n x-k)$. These terms contribute to the expansion of the form:\n\n$$\n\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)\n$$\n\nwhere the function $A_{n}$ applies the density $\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)$.\nFinally, the error term of order $O\\left(\\frac{1}{n^{m}}\\right)$ comes from the remainder of the Taylor expansion. Thus, the asymptotic expansion of $A_{n}(f ; x)$ is:\n\n$$\nA_{n}(f ; x)-f(x)=\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)+O\\left(\\frac{1}{n^{m}}\\right)\n$$", "tables": {}, "images": {}}, {"section_id": 21, "text": "# 7 Detailed Proofs\n### 7.1 Asymptotic Expansion of Quasi-Interpolation Operators\n\nTheorem 1: Let $f \\in C^{m}(\\mathcal{M})$, where $C^{m}(\\mathcal{M})$ denotes the space of functions with continuous derivatives up to order $m$ on a Riemannian manifold $\\mathcal{M}$. Let $A_{n}(f ; x)$ be the quasi-interpolation operator defined by:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}} f\\left(\\frac{k}{n}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nwhere $Z_{q, \\alpha}(x)=\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)$. Then the asymptotic expansion of $A_{n}(f ; x)$ around $f(x)$ is given by:\n\n$$\nA_{n}(f ; x)-f(x)=\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nProof. We start with the Taylor expansion of $f$ around $x$ :\n\n$$\nf\\left(\\frac{k}{n}\\right)=f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nSubstitute this expansion into the definition of the operator $A_{n}(f ; x)$ :\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}}\\left[f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\\right] Z_{q, \\alpha}(n x-k)\n$$\n\nSeparate the terms of the expansion:\n\n- Constant Term:\n\n$$\nf(x) \\sum_{k \\in \\mathbb{Z}^{N}} Z_{q, \\alpha}(n x-k)=f(x)\n$$\n\nSince $Z_{q, \\alpha}(n x-k)$ is a normalized density function, the sum over $k$ equals 1.\n\n- Linear and Higher-Order Terms:\n\n$$\n\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} \\sum_{k \\in \\mathbb{Z}^{N}}\\left(\\frac{k}{n}-x\\right)^{\\alpha} Z_{q, \\alpha}(n x-k)\n$$\n\nThese terms involve derivatives of $f$ and represent higher-order approximations, taking into account the contribution of the derivatives of $f$ and the density functions.\n\n- Higher-Order Error Term:\n\n$$\n\\sum_{k \\in \\mathbb{Z}^{N}} O\\left(\\frac{1}{n^{m}}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nThis term represents the error that decays rapidly as $n \\rightarrow \\infty$, specifically at the rate $O\\left(\\frac{1}{n^{m}}\\right)$.\nCombining all the terms, we obtain the final expansion for $A_{n}(f ; x)$ :\n\n$$\nA_{n}(f ; x)=f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nTherefore, the operator $A_{n}(f ; x)$ converges to $f(x)$ at the rate $O\\left(\\frac{1}{n^{m}}\\right)$, with the correction terms involving the higher derivatives of $f$ and the influence of the density functions $\\phi_{q, \\alpha}\\left(x_{i}\\right)$.", "tables": {}, "images": {}}, {"section_id": 22, "text": "# 8 Theorem: Uniform Convergence of Generalized Neural Network Operators on Manifolds with Negative Curvature \n\nTheorem 3: Let $\\mathcal{M}$ be a compact Riemannian manifold with negative sectional curvature and metric $g_{i j}$, and let $f \\in C^{m}(\\mathcal{M})$ be a function with continuous derivatives up to order $m$. Consider the generalized neural network operator $A_{n}(f ; x)$ defined by:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}} f\\left(\\frac{k}{n}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nwhere $Z_{q, \\alpha}(x)=\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)$ and $\\phi_{q, \\alpha}\\left(x_{i}\\right)$ is the generalized density function defined on $\\mathcal{M}$. Then, for sufficiently large $n$, the operator $A_{n}(f ; x)$ converges uniformly to $f(x)$ on $\\mathcal{M}$, i.e.,\n\n$$\n\\lim _{n \\rightarrow \\infty} \\sup _{x \\in \\mathcal{M}}\\left|A_{n}(f ; x)-f(x)\\right|=0\n$$\n\nProof. The density function $\\phi_{q, \\alpha}(x)$ is defined as:\n\n$$\n\\phi_{q, \\alpha}(x)=\\frac{1}{\\sqrt{\\operatorname{det}(g(x))}} \\prod_{i=1}^{N} h_{q, \\alpha}\\left(x_{i}\\right)\n$$\n\nwhere $h_{q, \\alpha}(x)$ is the generalized activation function:\n\n$$\nh_{q, \\alpha}(x)=\\frac{e^{\\alpha x}-q e^{-\\alpha x}}{(1+q) e^{\\alpha x}+(1-q) e^{-\\alpha x}}, \\quad q>0, \\alpha>0, x \\in \\mathbb{R}\n$$\n\nThe function $h_{q, \\alpha}(x)$ is odd and bounded, ensuring that $\\phi_{q, \\alpha}(x)$ is normalized:\n\n$$\n\\int_{\\mathcal{M}} \\phi_{q, \\alpha}(x) d V(x)=1\n$$\n\nThe operator $A_{n}(f ; x)$ is defined as:\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}} f\\left(\\frac{k}{n}\\right) Z_{q, \\alpha}(n x-k)\n$$\n\nwhere $Z_{q, \\alpha}(x)=\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)$.\nThe sum over $k \\in \\mathbb{Z}^{N}$ represents a discrete approximation of the function $f$ at points $\\frac{k}{n}$, weighted by the density function $Z_{q, \\alpha}(n x-k)$.\n\nConsider the Taylor expansion of $f$ around $x$ :\n\n$$\nf\\left(\\frac{k}{n}\\right)=f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\n$$\n\nwhere $\\left(\\frac{k}{n}-x\\right)^{\\alpha}$ is the monomial expansion of the distance between $\\frac{k}{n}$ and $x$, and the term $O\\left(\\frac{1}{n^{m}}\\right)$ represents the error of the approximation.\n\nSubstitute the Taylor expansion into the definition of the operator $A_{n}(f ; x)$ :\n\n$$\nA_{n}(f ; x)=\\sum_{k \\in \\mathbb{Z}^{N}}\\left[f(x)+\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha}+O\\left(\\frac{1}{n^{m}}\\right)\\right] Z_{q, \\alpha}(n x-k)\n$$\n\nSeparate the terms of the expansion:\n\n$$\n\\begin{aligned}\nA_{n}(f ; x)= & \\sum_{k \\in \\mathbb{Z}^{N}} f(x) Z_{q, \\alpha}(n x-k) \\\\\n& +\\sum_{k \\in \\mathbb{Z}^{N}} \\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!}\\left(\\frac{k}{n}-x\\right)^{\\alpha} Z_{q, \\alpha}(n x-k) \\\\\n& +\\sum_{k \\in \\mathbb{Z}^{N}} O\\left(\\frac{1}{n^{m}}\\right) Z_{q, \\alpha}(n x-k)\n\\end{aligned}\n$$\n\nThe zero-order term is:\n\n$$\n\\sum_{k \\in \\mathbb{Z}^{N}} f(x) Z_{q, \\alpha}(n x-k)=f(x)\n$$\n\nsince the sum of $Z_{q, \\alpha}(n x-k)$ over all $k \\in \\mathbb{Z}^{N}$ is normalized to 1 .\nThe higher-order terms involve the derivatives of $f$ and are weighted by the products of $Z_{q, \\alpha}(n x-k)$. These terms contribute to the expansion of the form:\n\n$$\n\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)\n$$\n\nwhere the function $A_{n}$ applies the density $\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)$.\nThe error term $O\\left(\\frac{1}{n^{m}}\\right)$ comes from the remainder of the Taylor expansion.\nTo show uniform convergence, consider the supremum of the difference:\n\n$$\n\\sup _{x \\in \\mathcal{M}}\\left|A_{n}(f ; x)-f(x)\\right|\n$$\n\nUsing the Taylor expansion and the properties of the density function, we have:\n\n$$\n\\sup _{x \\in \\mathcal{M}}\\left|A_{n}(f ; x)-f(x)\\right| \\leq \\sup _{x \\in \\mathcal{M}}\\left|\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)+O\\left(\\frac{1}{n^{m}}\\right)\\right|\n$$\n\nSince $f \\in C^{m}(\\mathcal{M})$, the derivatives $D^{\\alpha} f(x)$ are bounded on $\\mathcal{M}$. Moreover, the density function $\\phi_{q, \\alpha}(x)$ is normalized and bounded, ensuring that:\n\n$$\n\\sup _{x \\in \\mathcal{M}}\\left|\\sum_{|\\alpha|=1}^{m} \\frac{D^{\\alpha} f(x)}{\\alpha!} A_{n}\\left(\\prod_{i=1}^{N} \\phi_{q, \\alpha}\\left(x_{i}\\right)\\right)\\right| \\rightarrow 0 \\quad \\text { as } \\quad n \\rightarrow \\infty\n$$\n\nTherefore,\n\n$$\n\\lim _{n \\rightarrow \\infty} \\sup _{x \\in \\mathcal{M}}\\left|A_{n}(f ; x)-f(x)\\right|=0\n$$\n\nThis concludes the proof of the theorem.", "tables": {}, "images": {}}, {"section_id": 23, "text": "# 9 Results \n\nThe theoretical results obtained in this study provide a comprehensive understanding of the behavior of generalized neural network operators in non-Euclidean and fractal domains. The Voronovskaya-type expansions and fractional convergence results demonstrate the robustness and applicability of these operators in diverse spaces. The preservation of density properties and the detailed analysis of convergence rates ensure that these operators can be effectively used in various applications, including signal processing on manifolds and solving partial differential equations on fractals.\n\nOne of the key results of this study is the derivation of Voronovskaya-type asymptotic expansions for generalized neural network operators. These expansions provide a detailed understanding of how the operators approximate functions in non-Euclidean and fractal domains. Specifically, the expansions highlight the contribution of higher-order derivatives and ensure that the operator converges to the function at a specified rate. This result is crucial for understanding the behavior of these operators in complex geometries and ensures their robustness in practical applications.\n\nAnother significant result is the analysis of fractional convergence for neural network operators. We have demonstrated that the fractional operator, which incorporates fractional derivatives, converges to the function at a rate dependent on the order of the fractional derivative and the regularity of the function. This result underscores the importance of fractional derivatives in capturing non-local behavior and modeling systems with long-range dependencies or fractal-like structures. The use of fractional derivatives allows for a more flexible approximation that can adapt to non-smooth and irregular dynamics, providing a powerful tool for capturing intricate behaviors in complex spaces.\n\nThe generalized neural network operators introduced in this study preserve essential density properties, which are crucial for their applicability in various domains. The density function is normalized, ensuring that it integrates to 1 over the manifold. This property is essential for the construction of probability densities and for ensuring the stability of numerical methods\n\nthat rely on these density functions. The preservation of density properties ensures that the operators can be effectively used in a wide range of applications, from signal processing to solving partial differential equations.\n\nA detailed analysis of the convergence rates of the generalized neural network operators has been conducted. The results show that these operators converge uniformly to the function on manifolds with negative curvature. This uniform convergence ensures the robustness and applicability of the operators in diverse spaces. The analysis of convergence rates provides a solid foundation for the use of these operators in practical applications, ensuring that they can be relied upon to provide accurate and stable approximations.\n\nThe theoretical results obtained in this study have significant implications for various applications. The generalized neural network operators can be effectively used in signal processing on manifolds, where they provide accurate approximations of signals defined on complex geometries. Additionally, these operators can be applied to solve partial differential equations on fractals, offering a powerful tool for modeling systems with intricate structures. The ability to handle non-Euclidean and fractal domains makes these operators particularly valuable in fields such as data science, signal processing, and applied mathematics.\n\nIn conclusion, the results of this study provide a comprehensive understanding of the behavior of generalized neural network operators in non-Euclidean and fractal domains. The Voronovskaya-type expansions, fractional convergence results, preservation of density properties, and detailed analysis of convergence rates ensure that these operators are robust and applicable in diverse spaces. These findings contribute to the field by offering new mathematical tools that can be effectively used in various applications, including signal processing and solving partial differential equations. The insights gained from this study pave the way for future research and the development of even more sophisticated tools for handling complex, structured spaces.", "tables": {}, "images": {}}, {"section_id": 24, "text": "# 10 Conclusions \n\nThis research extends the classical theory of neural network operators to non-Euclidean and fractal domains, introducing generalized activation functions and fractional derivatives. By addressing the unique challenges posed by these complex geometries, we have developed a comprehensive framework that significantly enhances the applicability of neural network operators in diverse spaces.\n\nThe key findings of this study include the preservation of density properties, detailed convergence rates, and asymptotic expansions. These results provide a deeper understanding of the behavior of neural network operators in complex, structured spaces. The generalized activation functions introduced, such as modified hyperbolic tangent functions, preserve essential properties like symmetry, normalization, and derivative behavior, making them ideal for neural network modeling in non-Euclidean settings.\n\nThe incorporation of fractional derivatives into the definition of neural network operators has proven to be particularly valuable. Fractional derivatives capture non-local behavior and are instrumental in modeling systems with long-range dependencies or fractal-like structures. This extension allows for more flexible and accurate approximations, adapting to the intricate dynamics present in complex systems.\n\nThe detailed analysis of convergence rates and asymptotic expansions ensures the robustness and applicability of these operators. The Voronovskaya-type expansions and fractional convergence results demonstrate that the generalized neural network operators converge uniformly and efficiently, making them reliable tools for various applications.\n\nThe theoretical results obtained in this study have significant implications for applied mathematics and data science. The ability to handle non-Euclidean and fractal domains makes these operators particularly valuable in fields such as signal processing on manifolds and solving par-\n\ntial differential equations on fractals. The insights gained from this research pave the way for future developments and the exploration of even more sophisticated tools for handling complex, structured spaces.\n\nFuture work will focus on numerical implementations of the generalized neural network operators and their extensions to dynamic systems. This will further enhance the applicability of these operators in practical scenarios, providing robust and efficient solutions for a wide range of problems in applied mathematics and data science. By continuing to explore and refine these methods, we aim to contribute to the ongoing advancement of the field and the development of innovative mathematical tools.", "tables": {}, "images": {}}, {"section_id": 25, "text": "# Notation, Symbols, and Nomenclature\n## Notation\n\n- $\\mathcal{M}$ : Riemannian manifold.\n- $g_{i j}$ : Components of the metric tensor.\n- $U_{i}$ : Open subsets in $\\mathbb{R}^{N}$.\n- $\\phi_{i}$ : Smooth, bijective maps from $U_{i}$ to $\\mathcal{M}$.\n- $h_{q, \\alpha}(x)$ : Generalized activation function.\n- $Z_{q, \\alpha}(x)$ : Normalized product of density functions.\n- $A_{n}(f ; x)$ : Basic quasi-interpolation operator.\n- $K_{n}(f ; x)$ : Kantorovich-type operator.\n- $Q_{n}(f ; x)$ : Fractional neural network operator.\n- $D^{\\beta} f(x)$ : Fractional derivative of order $\\beta$.\n- $\\Gamma(\\cdot)$ : Gamma function.", "tables": {}, "images": {}}, {"section_id": 26, "text": "## Symbols\n\n- $q$ : Deformation parameter.\n- $\\alpha$ : Steepness parameter.\n- $x$ : Variable in $\\mathbb{R}$.\n- $f$ : Function with continuous derivatives up to order $m$.\n- $n$ : Scaling parameter.\n- $k$ : Index for summation.\n- $\\beta$ : Order of the fractional derivative.", "tables": {}, "images": {}}, {"section_id": 27, "text": "# References \n\n[1] Anastassiou, George A. \"Rate of convergence of some neural network operators to the unit-univariate case.\" Journal of Mathematical Analysis and Applications 212.1 (1997): 237-262. https://doi.org/10.1006/jmaa.1997.5494.\n[2] Anastassiou, George A. Parametrized, Deformed and General Neural Networks. Heidelberg/Berlin, Germany: Springer, 2023.\n[3] Kilbas, Anatoli\u012d Aleksandrovich, Hari M. Srivastava, and Juan J. Trujillo. Theory and applications of fractional differential equations. Vol. 204. elsevier, 2006.\n[4] Podlubny, Igor. Fractional differential equations: an introduction to fractional derivatives, fractional differential equations, to methods of their solution and some of their applications. elsevier, 1998.\n[5] Magin, R. L. \"Fractional calculus in bioengineering begell house publishers.\" Inc., Connecticut (2006).\n[6] Tarasov, Vasily E. Fractional dynamics: applications of fractional calculus to dynamics of particles, fields and media. Springer Science \\& Business Media, 2011.\n[7] Samko, Stefan G. \"Fractional integrals and derivatives.\" Theory and applications (1993).\n[8] Mainardi, Francesco. Fractional calculus and waves in linear viscoelasticity: an introduction to mathematical models. World Scientific, 2022.\n[9] West, Bruce J., Mauro Bologna, and Paolo Grigolini. Physics of fractal operators. Vol. 35. New York: Springer, 2003.", "tables": {}, "images": {}}], "id": "2501.06204v1", "authors": ["R\u00f4mulo Damasclin Chaves dos Santos", "Jorge Henrique de Oliveira Sales"], "categories": ["math.GM"], "abstract": "This paper extends the classical theory of Voronovskaya-type asymptotic\nexpansions to generalized neural network operators defined on non-Euclidean and\nfractal domains. We introduce and analyze smooth operators activated by\nmodified and generalized hyperbolic tangent functions, extending their\napplicability to manifold and fractal geometries. Key theoretical results\ninclude the preservation of density properties, detailed convergence rates, and\nasymptotic expansions. Additionally, we explore the role of fractional\nderivatives in defining neural network operators, which capture non-local\nbehavior and are particularly useful for modeling systems with long-range\ndependencies or fractal-like structures. Our findings contribute to a deeper\nunderstanding of neural network operators in complex, structured spaces,\noffering robust mathematical tools for applications in signal processing on\nmanifolds and solving partial differential equations on fractals.", "updated": "2024-12-28T22:47:39Z", "published": "2024-12-28T22:47:39Z"}