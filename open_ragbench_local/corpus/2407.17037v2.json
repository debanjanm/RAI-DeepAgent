{"title": "Robust Comparative Statics with Misspecified Bayesian Learning", "sections": [{"section_id": 0, "text": "#### Abstract\n\nWe present novel monotone comparative statics results for steady-state behavior in a dynamic optimization environment with misspecified Bayesian learning. Building on Esponda and Pouzo (2021), we analyze a Bayesian learner whose prior is over parameterized transition models but is misspecified in the sense that the true process does not belong to this set. We characterize conditions that ensure monotonicity in the steady-state distribution over states, actions, and inferred models. Additionally, we provide a new monotonicity-based proof of steady-state existence, derive an upper bound on the cost of misspecification, and illustrate the applicability of our results to several environments of general interest.\n\n\nJournal of Economic Literature Classification Numbers: C61, D83, D90\nKey Words: Berk-Nash equilibrium, monotone comparative statics, model misspecification.\n\n[^0]\n[^0]:    *I am sincerely grateful to my committee members-Robert Anderson, Ying Chen, and M. Ali Khan-for their thoughtful comments and guidance throughout this project. I also appreciate the comments of three anonymous referees. I am also thankful to Abhinav Anand, Robert Barbera, Paulo Barelli, Sarvesh Bandhu, Chris Carroll, Rama Chellappa, Yujian Chen, Finn Christensen, Liuchun Deng, Haosui Duanmu, Ignacio Esponda, Marcelo Fernandez, Mira Frick, Gagan Ghosh, Steve Hamilton, Ashwin Kambhampati, Edi Karni, Jong Jae Lee, Jason Lepore, Andrew Mackenzie, Vikram Manjunath, Arina Nikandrova, Mallesh Pai, Arthur Paul Pedersen, Kevin Reffett, Metin Uyanik, Marie-Louise Vier\u00f8, Xinyang Wang, Lukasz Wozny, and Sezer Yasar for their valuable feedback and encouragement at various stages of this work. I also appreciate the feedback received from audiences at seminars and conferences, including the 21st Annual SAET Conference, Cal Poly, IIM Bangalore, Johns Hopkins University, the XXXII European Workshop on Economic Theory at the University of Manchester, the Midwest Economic Theory Conference at the University of Rochester, and the Econometric Society Session on Misspecified Beliefs at ASSA meetings. Edited by Eduardo Azevedo.\n    ${ }^{\\dagger}$ Economics Area, Orfalea College of Business, California Polytechnic State University, San Luis Obispo, CA 93401. E-mail aghosh10@calpoly.edu.\n\nIn general, dynamic programs with learning are intractable, i.e., they are not solvable either analytically or numerically, when there is no separability of control and learning. The problem is not only whether a solution exists, but if a solution can be characterized and its properties studied.\n\nKoulovatianos, Mirman, and Santugini (2009)", "tables": {}, "images": {}}, {"section_id": 1, "text": "# 1 Introduction \n\nStemming from the foundational contributions of Arrow and Green (1973) and Nyarko (1991), model misspecification in economic environments continues to be of substantive interest for economists. It acknowledges the possibility that economic agents, perhaps due to cognitive limitations or simplified perspectives, may often not know the true model of their complex economic environment and, therefore, learn about it through a set of models that are misspecified in the sense that the true model is not contained in their set of models. A growing body of literature in economics suggests that enriching classical economic environments with misspecified economic agents aligns theoretical predictions closely with the observed behavior. ${ }^{1}$\n\nDynamic programming offers one robust framework for economists to approach discretetime economic problems. They are commonly used to analyze instances where economic agents make decisions sequentially in an uncertain environment. The framework applies to a broad spectrum of economic models, from consumption-savings decisions for individuals to investment choices for firms. However, they are known to be intractable, no closed-form solutions, when incorporated with agents who simultaneously make decisions and learn about their environment through their models, where both the decision making and the learning activity mutually affect each other. In this paper, we provide robust predictions on the monotone comparative statics properties of an agent's steady-state behavior within a dynamic programming environment, particularly when they are learning with potentially misspecified models. ${ }^{2}$\n\nConsider the following setting, conceptualized by Esponda and Pouzo (2021) (hereafter, EP), in the context of a single-agent dynamic optimization problem, a Markov Decision Process (hereafter, MDP). In each period $t=0,1,2, \\ldots$, an agent observes a state $s_{t}$ and then takes an action $x_{t}$ and receives a utility, $u\\left(s_{t}, x_{t}\\right)$. The current action and the state together determine the evolution of the state tomorrow, $s_{t+1}$, via the true transition function, $Q\\left(\\cdot \\mid s_{t}, x_{t}\\right)$. The agent chooses a sequence of actions to maximize their current and the expected discounted flow of utility. The agent doesn't know the true (objective) transition function and, therefore, chooses\n\n[^0]\n[^0]:    ${ }^{1}$ For example, Farmer, Nakamura, and Steinsson (2024) find that much of the anomalies concerned with forecasts of professional forecasters can be attributed to them not knowing the true model of the environment.\n    ${ }^{2}$ As is well-known, monotone comparative statics analysis deals with characterizing conditions for various environments, both static and dynamic, under which decision rules and solution concepts are increasing in the primitives.\n\nactions based on a set of model (subjective) transition functions, $\\left\\{Q_{\\theta}, \\theta \\in \\Theta\\right\\}$, parameterized by $\\theta$, that is misspecified in the sense that it does not contain the true transition function. The Bayesian agent has a prior $\\mu$ on the set of models $\\Theta$ and, every period updates the prior using Bayes' rule $B$, based on the current state, the chosen action, and the realized state. The agent's problem can be formulated as a Bellman equation,\n\n$$\nV(s, \\mu)=\\max _{x \\in \\mathbb{X}}\\left\\{\\int_{\\mathbb{S}}\\left\\{u\\left(s, x, s^{\\prime}\\right)+\\beta V\\left(s^{\\prime}, \\mu^{\\prime}\\right)\\right\\} \\bar{Q}_{\\mu}\\left(d s^{\\prime} \\mid s, x\\right)\\right\\}\n$$\n\nwhere $\\bar{Q}_{\\mu}=\\int_{\\Theta} Q_{\\theta} \\mu(d \\theta), \\mu^{\\prime}=B\\left(s, x, s^{\\prime}, \\mu\\right)$ is the next period's belief, ${ }^{3}$ updated using Bayes' rule, and $V: \\mathbb{S} \\times \\Delta(\\Theta) \\rightarrow \\mathbb{R}$ is the unique solution to the Bellman equation of the agent, where $\\Delta(\\Theta)$ is the set of probability distributions on the parameter space, $\\Theta$.\n\nFor a misspecified agent that follows the above setting in choosing their sequence of actions and updating their sequence of beliefs over models, EP predicts its steady-state behavior in terms of a solution concept called the Berk-Nash equilibrium. It is an equilibrium distribution over states and actions corresponding to which the misspecified agent infers the model (or a set of models) that 'best-fits' the true model. In turn, given the inferred model, the chosen actions are optimal for each state. Furthermore, the steady-state is stationary in the sense that equilibrium distribution over states and actions leads to a stationary Markov process over the states. The 'best-fit' formalization is in the sense of a minimum weighted Kullback-Leibler divergence, with weights determined by the equilibrium distribution over states and actions. ${ }^{4}$ Given this prediction of the long-run behavior of a misspecified Bayesian agent in terms of a Berk-Nash equilibrium and its corresponding inferred model, we ask the following comparative statics question: how do the equilibrium objects respond to changes in the economic primitives? Phrased differently, what are the requirements on the primitives of the economic environment so that a misspecified agent's stationary distribution and corresponding best-fit models exhibit monotone comparative statics properties?\n\nThis question is of general theoretical interest for several reasons. First, models of dynamic programming incorporated with learning, whether correctly specified or misspecified, are known to be intractable. ${ }^{5}$ In an important paper on learning in stochastic growth models, Koulovatianos, Mirman, and Santugini (2009) (hereafter, KMS) note that the intractability\n\n[^0]\n[^0]:    ${ }^{3}$ The Bayesian operator $B$, where for any $A \\subseteq \\Theta, B\\left(s, s^{\\prime}, x, \\mu\\right)(A)=\\int_{A} Q_{\\theta}\\left(s^{\\prime} \\mid s, x\\right) \\mu(d \\theta) / \\int_{\\Theta} Q_{\\theta}\\left(s^{\\prime} \\mid s, x\\right) \\mu(d \\theta)$.\n    ${ }^{4}$ The Kullback-Leibler (KL) divergence measures the difference between two probability distributions in terms of relative entropy; in our case, between the model-implied process and the true process; see Cover and Thomas (2005) for details.\n    ${ }^{5}$ With learning incorporated in dynamic programming models, the agent is an active learner and processes incoming data in order to infer the models of their unknown environment. At the same time, they also take decisions. These two functions of the agent are intertwined.\n\narises primarily for two reasons: (a) the curse of dimensionality problem, which occurs because of incorporation of beliefs about the models in the state space, thus making the state space very large and computationally cumbersome, and (b) with Bayesian updating, the prior and the posterior over models potentially belong to different families, thereby hampering analytical and computational tractability. ${ }^{6}$ By focusing on the long-run prediction for a misspecified dynamic program with a precise characterization of the distribution over states and actions, as well as the limiting posterior (beliefs) over models, we abstract away from the intractability problems and thereby make predictions about their steady-state properties even when their closed-form solutions may not exist. Thus, in this paper, we expand the theory of dynamic programming with misspecified learning, potentially rendering it applicable across a wide spectrum of economic domains.\n\nSecond, comparative statics for misspecified learning environments are typically not wellunderstood in the existing literature on misspecification in economic theory. This traces itself to the issue that with concurrent decision making and learning, there is often no separability between the two, as is illustrated in Equation (1). We tackle this problem by focusing on the monotone comparative statics properties and recognizing that steady states for such models are fixed points of a well-defined equilibrium mapping. This insight allows us to appeal to a relatively newer literature of powerful order-theoretic techniques, tailored for dealing with the monotonicity of fixed points for non-lattice spaces (Acemoglu and Jensen (2015)). A technical challenge that lies with misspecified MDPs is that the underlying spaces containing such fixed points are typically not lattices in any natural order, and therefore, a wide array of popular lattice-dependent techniques (Hopenhayn and Prescott (1992); Topkis (1998)) are not applicable. Finally, our results for comparative statics do not necessitate any specific knowledge of the functional forms of any environment primitives, and therefore are robust, and hold for a very general class of dynamic programming environments with misspecified models. ${ }^{7}$\n\nTo interpret our framing, we discuss examples that cover three important economic environments. The first example concerns Bayesian inference with misspecified $\\operatorname{AR}(1)$ models. The agent's set of models is misspecified in the sense that although the true process is $\\operatorname{AR}(1)$, the innovations of the process are non-normally distributed and, therefore, are not contained in their set of $\\operatorname{AR}(1)$ models with Gaussian innovations. We characterize the Berk-Nash equilibrium for this example, and show that despite being misspecified, the Bayesian agent correctly infers\n\n[^0]\n[^0]:    ${ }^{6}$ As KMS and EP note, the curse of dimensionality does not arise if the beliefs are over a finite parameter set. However, for parameter spaces such as the real line, it is a cause for concern. Also note that, unlike KMS, in our framework, choosing actions also affects the flow of information, thus further complicating matters.\n    ${ }^{7}$ The reader may be interested in the following discussion: https://stats.stackexchange.com/questions/274815/why-should-i-be-bayesian-when-my-model-is-wrong. Uppal and Wang (2003) discuss the contrast between Savagean and Knightian approaches to tackle model misspecification concerns. The interested reader is referred to their p. 2469 for further reading.\n\nthe persistence parameter of the true process at the steady state. The next two examples are based on EP, albeit with slight modifications for interpretation and analytical tractability. In the second example, we study a canonical problem of effort provision in a dynamic effort-task problem where the agent seeks to learn their ability while exercising control over the stochastic outcome of a task by choosing their effort levels. The agent's models are misspecified since they incorrectly postulate that tomorrow's outcome depends only on their ability and not on today's outcome. In contrary, the true dynamics are determined by both ability and current outcome. The Berk-Nash equilibrium for this case is the steady-state frequency of the outcome, either a success or a failure, along with the agent's inference of their ability at the steady state. We characterize the dependence of these equilibrium objects on the economic primitives of the environment, such as the cost of exerting higher effort or the objective probability of success, and study their associated comparative statics. The final example considers an agent that optimally chooses its consumption and savings in the presence of preference and productivity shocks. The agent seeks to learn about the return of their wealth process but does so through a set of misspecified models that do not consider the correlation between the two shocks. Here, the Berk-Nash equilibrium is the long-run perceived distribution of the wealth process along with their corresponding inferred parameter of the return on that process. However, this case has no analytical solution; therefore, making our results most valuable for such settings. ${ }^{8}$\n\nWe now turn to the specificity of our results. Theorem 1 establishes the existence of a Berk-Nash equilibrium for a misspecified MDP with compact Euclidean state and action spaces. EP and Anderson, Duanmu, Ghosh, and Khan (2024) (hereafter, ADGK) provide proofs for the existence of a Berk-Nash equilibrium with finite and infinite state and actions, respectively. ADGK applies recent advances in non-standard analysis to present existence theorems for misspecified environments with infinite action and state spaces (compact-metric) and unbounded payoff functions. In contrast, here we focus on compact Euclidean spaces, where existence also emerges as a by-product of the monotonicity assumptions. The novelty of our existence result lies in exploiting the non-lattice structure of our economic environment. In general, distributions over states and actions are not lattices under most natural orders; therefore, to show the existence of the Berk-Nash equilibrium which in turn is a distribution, we rely on a novel application of techniques tailored for non-lattice spaces. These techniques, originating in Smithson (1971), have been pioneered in Acemoglu and Jensen (2015) for establishing the existence and comparative statics results in infinite-horizon large dynamic economies.\n\nAfter furnishing the existence result for the Berk-Nash equilibrium, we utilize these tech-\n\n[^0]\n[^0]:    ${ }^{8}$ Additionally, in Remark 2 we show the applicability of our results to a dynamic version of a setting involving an over-confident agent who has a misspecified model of their ability, based on Heidhues, K\u00f6szegi, and Strack (2018).\n\nniques for Theorems 2-4 and give sufficient conditions for which the Berk-Nash equilibrium and the corresponding inferred best-fit model respond monotonically to the primitives of the environment. We formalize a notion of a positive shock for a misspecified dynamic optimization problem, defined as shocks to the primitives that increase strategies for any given beliefs over models, and under mild assumptions establish in Theorem 2 that the least and the greatest inferred best-fit model at the steady state, responds monotonically to a change in the economic primitives. Theorem 2 further provides a prediction for the monotonicity of the Berk-Nash equilibrium in the usual stochastic order, when a misspecified agent is hit with positive shocks to their primitives. In Theorem 3, we show that the least and the greatest inferred best-fit model responds monotonically to an increase in the set of models, where an increase is defined in the strong-set order in the space of parameterized models. Theorem 4 drops the usual stochastic order on the underlying space and extends Theorem 2 to spaces with increasing and convex orders. ${ }^{9}$\n\nOur final result is of a slightly different flavor from the previous results. In Theorem 5 , we perform a welfare comparison of steady-state learning in a correctly specified vis-\u00e0-vis a misspecified learning environment. While welfare under misspecified learning is always weakly lower than that under correct learning, we provide an upper bound on the cost of misspecification, formalized as the difference between the two instances and outline its dependence in terms of the primitives of the environment. ${ }^{10}$\n\nRelated literature. This paper belongs to the growing literature on learning with misspecified models and its implications on the properties of the learning behavior.\n\nWithin the dynamic programming framework, following EP and ADGK, this paper works with a general framework (Euclidean) to model MDPs with misspecified learning for infinite environments. ${ }^{11}$ MDPs are commonly used to analyze instances involving agents making decisions sequentially in an uncertain environment and, therefore, have found many applications within natural and social sciences. Within the economics literature, one of its earliest uses is traceable in Arrow, Harris, and Marschak (1951), where an inventory holder maximizes some given objective (profits, revenue, net utility) by choosing an inventory policy subject to stochastic product demand and other random fluctuations to the primitives of the problem. ${ }^{12}$ Since then, MDPs have\n\n[^0]\n[^0]:    ${ }^{9}$ As Che, Kim, and Kojima (2021) note in the context of individual choices, the strong set order proves to be an appropriate notion; see the references therein for further details.\n    ${ }^{10}$ We also remark on the potential usefulness of this bound for computational applications concerning BerkNash equilibria.\n    ${ }^{11}$ See Puterman (1994) for illustrations covering operations research, engineering, and many other allied fields. Also, see Rust (1994) for an extensive survey on MDPs and associated structural estimation methods. The results in this paper could potentially be admissible in an extended learning version of the partially observable MDP framework of Saghafian (2018); see the references therein for an overview of that literature.\n    ${ }^{12}$ This work has important precursors in Arrow, Blackwell, and Girshick (1949).\n\nspanned both micro and macro environments, covering many important economic applications; these include (i) investment with adjustment costs under uncertain demand (Lucas and Prescott (1971)), (ii) one-sector optimal economic growth with uncertainty (Brock and Mirman (1972)), (iii) equilibrium search and unemployment (Lucas and Prescott (1974)), and (iv) asset prices in a pure exchange economy (Lucas (1978)). ${ }^{13}$ In a recent paper, Saghafian (2018) presents a framework for an MDP in which agents possess a 'cloud' of models and allows for considering model ambiguity and the potential misspecification within an MDP.\n\nThis paper also contributes to the understanding of the comparative static properties of the limit posterior of a Bayesian inference process, specifically a Markov process. In his foundational paper on inference with misspecified models, devoid of any actions, Berk (1966) demonstrates that for a misspecified Bayesian agent learning about a parameter through a series of independently and identically distributed (i.i.d.) signals, the posterior concentrates asymptotically on those set of models (referred therein as the asymptotic carrier) where the Kullback-Leibler divergence is minimal with respect to the true model. ${ }^{14}$ In this paper, we address a misspecified MDP, a non-i.i.d. environment, that combines decision-making and inference but heavily relies on Berk's characterization to abstract away from the dynamics of the updating process. This approach enables us to focus on the comparative statics of the steady-state behavior for both the Berk-Nash equilibrium and the corresponding asymptotic carrier, and contribute to the inference literature by outlining the dependence of the inferred models on the primitives of the decision making and the learning environment. ${ }^{15}$\n\nThe topic of misspecification in economic environments has been a subject of active research area. ${ }^{16}$ In an influential paper, Esponda and Pouzo (2016) provides a framework for a static game-theoretic setting that relaxes the assumption that agents have a correct view of the game's environment. This formulation is further extended to the dynamic programming environment in EP for a finite (states and actions) setting and by ADGK for an infinite setting. While there has been influential work on social learning problems as in Frick, Iijima, and Ishii (2020) and Bohren and Hauser (2021), properties of asymptotic learning in a misspecified MDP\n\n[^0]\n[^0]:    ${ }^{13}$ Other prominent examples include models of portfolio choice under uncertainty (Phelps (1962); Levhari and Srinivasan (1969)) and business cycle models (Kydland and Prescott (1982); Long Jr. and Plosser (1983)).\n    ${ }^{14}$ See also Bunke and Milhaud (1998) and Shalizi (2009) for extensions to non-iid environments. The reader should note that Berk's asymptotic carrier is independent of the prior distribution.\n    ${ }^{15}$ This also has comparative statics implications for a series of papers that follow the papers of Huber (1967) and White (1982). These papers use maximum likelihood techniques for inference and show that an agent's inference converges to models minimizing the Kullback-Leibler divergence from the true distribution, consistent with the limits proposed in Berk (1966).\n    ${ }^{16}$ Among many other prominent works, some notable ones include Kirman (1975), Jehiel (2005), and Hansen and Sargent (2011). The reader is referred to the references therein for a broader view of the literature.\n\nenvironment are still not well-studied. The results in this paper contribute to the understanding of such properties, in terms of the comparative statics behavior of misspecified agents.\n\nNaturally, this work is informed by the substantive literature on monotone comparative statics. The theory of monotone comparative statics deals with characterizing conditions under which the optimizing behavior of agents leads to solution concepts (and invariant distributions) being monotonic in the primitives of the environment. ${ }^{17}$ In an influential paper, Hopenhayn and Prescott (1992) use the tools of monotone comparative statics to analyze stationary dynamic optimization problems with lattice environments. Their results turn out to be of limited use for obtaining our theorems. Our proofs instead exploit the non-lattice structure of our setting and, therefore, depend on the techniques developed in Smithson (1971) and Acemoglu and Jensen (2015). We adapt and apply these techniques in a novel way to misspecified MDPs with Bayesian learning.\n\nLastly, we calculate the cost of misspecification in terms of the discrepancy between the expected discounted welfare under correctly specified and misspecified Bayesian learning for an MDP and provide an upper bound on the discrepancy between the two quantities. We then characterize its comparative statics properties with respect to the primitives. This is partly inspired by Santos (2000)'s work on testing the accuracy of numerical solutions in dynamic models. We argue that the one-way bound on welfare that we provide is useful for computational purposes of such equilibria.\n\nOutline. The paper is organized as follows. Section 2 sets up the framework for a misspecified MDP and outlines the necessary prerequisites and techniques for the comparative statics analysis. Section 3 offers three examples to illustrate our setting. The main results of this paper are presented in Section 4, while Section 5 completes the formal analysis of the examples. Section 6 compares welfare between correctly specified and misspecified settings. Section 7 concludes by discussing how these results can be extended and related to different settings. Appendix A to this paper contains the necessary mathematical preliminaries, and the proofs of the results are presented in Appendix B.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 2 General Framework \n\nWe begin by framing the environment for a Markov Decision Process (MDP) with model misspecification. While the conceptual framework is patterned after the finite (states and actions) environment of EP, our setting is based on Euclidean spaces since it allows infinite settings\n\n[^0]\n[^0]:    ${ }^{17}$ Some important references include Milgrom and Shannon (1994), Amir (1996), Topkis (1998), Huggett (2003), Datta, Reffett, and Wo\u017any (2018), and Light (2021). Amir (2018) is a useful guide for some of the history and also of the more recent advances in the comparative statics literature.\n\nthat naturally feature many important economic applications. After outlining the contents of a misspecified MDP and the relevant Berk-Nash equilibrium concept, we provide an overview of the order-theoretic methods that are instrumental for our results. A refresher for these concepts and methods is provided in Appendix A for the reader's convenience.\n\nAt the start of each period $t=0,1,2, \\ldots$, the agent observes a state realization, $s_{t} \\in \\mathbb{S}$, and then chooses an action, $x_{t} \\in \\mathbb{X}$. Given a transition probability function $Q\\left(\\cdot \\mid s_{t}, x_{t}\\right)$, the state and action together determine the distribution of the next period state, $s_{t+1}$. The per-period payoff function is a mapping $u: \\mathbb{S} \\times \\mathbb{S} \\times \\mathbb{X} \\rightarrow \\mathbb{R}$. The agent maximizes expected discounted utility (discount factor $0<\\beta<1$ ) by choosing a feasible sequence of policy functions $\\left\\{x_{t}\\right\\}_{t=1}^{\\infty}$ that solves the following problem,\n\n$$\nV\\left(s_{0}\\right)=\\max _{\\left\\{x_{t}\\right\\}_{t=0}^{\\infty}} \\mathbb{E}_{Q}\\left[\\sum_{t=0}^{\\infty} \\beta^{t} u\\left(s_{t}, x_{t}, s_{t+1}\\right)\\right], t=0,1,2, \\ldots\n$$\n\nFollowing (2), the Bellman equation for the agents' sequential decision problem is formulated as,\n\n$$\nV(s)=\\max _{x \\in \\mathbb{X}}\\left\\{\\int_{\\mathbb{S}}\\left\\{u\\left(s, x, s^{\\prime}\\right)+\\beta V\\left(s^{\\prime}\\right)\\right\\} Q\\left(d s^{\\prime} \\mid s, x\\right)\\right\\}\n$$\n\nwhere $V$, the value function, is the unique solution to (3). Corresponding to this $V$, the optimal policy correspondence $G$ is given by,\n\n$$\nG(s) \\equiv \\underset{x \\in \\mathbb{X}}{\\operatorname{argmax}}\\left\\{\\int_{\\mathbb{S}}\\left\\{u\\left(s, x, s^{\\prime}\\right)+\\beta V\\left(s^{\\prime}\\right)\\right\\} Q\\left(\\mathrm{~d} s^{\\prime} \\mid s, x\\right)\\right\\}\n$$\n\nWe summarize this environment in the following definition of a MDP. ${ }^{18}$\nDefinition 1. A Markov Decision Process is a tuple $\\langle\\mathbb{S}, \\mathbb{X}, q_{0}, Q, u\\rangle$, where (i) the state space $\\mathbb{S} \\subseteq \\mathbb{R}^{m}$ is a compact metric space with Borel $\\sigma$-algebra $\\mathcal{B}(\\mathbb{S})$, (ii) the action space $\\mathbb{X} \\subseteq \\mathbb{R}^{n}$ is a compact metric space with Borel $\\sigma$-algebra $\\mathcal{B}(\\mathbb{X})$, (iii) the initial distribution of states $q_{0}$ is a probability measure on $\\mathbb{S}$, (iv) $Q: \\mathbb{S} \\times \\mathbb{X} \\rightarrow \\mathcal{M}_{1}(\\mathbb{S})$ is a transition probability function, where $\\mathcal{M}_{1}(\\mathbb{S})$ denote the set of probability measures on $\\mathbb{S}$, and (v) $u: \\mathbb{S} \\times \\mathbb{S} \\times \\mathbb{X} \\rightarrow \\mathbb{R}$ is the per-period payoff function. ${ }^{19}$\n\nWe next define a Subjective Markov Decision Process. It adds to the tuple, a set of subjective transition functions, $\\left\\{Q_{\\theta}\\right\\}_{\\theta \\in \\Theta}$, parameterized with $\\theta$. We refer to the parameter space $\\Theta$ as the set of models.\n\n[^0]\n[^0]:    ${ }^{18}$ The framework can be extended with minor modifications to allow for the dependence of feasible set of actions on the state variable; see the Esponda and Pouzo (2015) working paper for details.\n    ${ }^{19} \\mathcal{M}_{1}(\\mathbb{S})$ is the space of finite probability measures on $(S, \\mathbb{B}(S))$ endowed with the weak- $\\star$ topology.\n\nDefinition 2. A Subjective Markov Decision Process (hereafter, SMDP) is an MDP, $\\langle\\mathbb{S}, \\mathbb{X}, q_{0}, Q$, $u\\rangle$, and a non-empty family of transition probability functions, $\\mathcal{Q}_{\\Theta}=\\left\\{Q_{\\theta}: \\theta \\in \\Theta\\right\\}$, where each transition probability function $Q_{\\theta}: \\mathbb{S} \\times \\mathbb{X} \\rightarrow \\mathcal{M}_{1}(\\mathbb{S})$ is indexed by a parameter value $\\theta \\in \\Theta \\subseteq \\mathbb{R} .{ }^{20}$ A SMDP is said to be misspecified if $Q \\notin \\mathcal{Q}_{\\Theta}$.\n\nNotice that under this definition, an SMDP could be misspecified in several ways. For instance, the true transition function and the set of model transition functions may pertain to dissimilar families of probability distributions. Alternatively, even if the true model and set of models belong to the same family of distributions, they can be misspecified if the support, the range of possible values of the parameter, of the models is different from that of the true distribution. ${ }^{21}$ Our next definition requires the primitives of the SMDP to satisfy certain regularity conditions.\n\nDefinition 3. A regular SMDP satisfies the following conditions.\n(i) (Continuity) The mappings $(s, x) \\rightarrow Q(\\cdot \\mid s, x)$ and $(\\theta, s, x) \\rightarrow Q_{\\theta}(\\cdot \\mid s, x)$ are continuous in the Prokhorov metric, and the density function $D_{\\theta}\\left(s^{\\prime} \\mid s, x\\right)$ is jointly continuous on the set\n\n$$\n\\left\\{\\left(\\theta, s^{\\prime}, s, x\\right): Q(s, x) \\text { is dominated by } Q_{\\theta}(s, x)\\right\\}\n$$\n\nwhere $\\left(D_{\\theta}\\left(s^{\\prime} \\mid s, x\\right)\\right)$ is the Radon-Nikodym derivative of $Q$ with respect to $Q_{\\theta}, \\theta \\in \\Theta .{ }^{22}$\n(ii) (Absolute continuity) There is a dense set $\\hat{\\Theta} \\subset \\Theta$ such that $Q(\\cdot \\mid s, x)$ is absolutely continuous with respect to $Q_{\\theta}(\\cdot \\mid s, x)$ for all $\\theta \\in \\hat{\\Theta}$ and $(s, x) \\in \\mathbb{S} \\times \\mathbb{X}$.\n(iii) (Uniform integrability) For every compact set $S^{\\prime} \\subset S$, there exists some $r>0$ such that $\\left(D_{\\theta}(\\cdot \\mid s, x)\\right)^{1+r}$ is uniformly integrable with respect to $Q_{\\theta}(\\cdot \\mid s, x)$ over the set $\\hat{\\Theta} .{ }^{23}$\n(iv) (Compactness) The parameter space $\\Theta$ is a compact metric space.\n\nCondition (i) is a standard technical condition and requires the transition functions, both true and subjective, to be continuous. Condition (ii) requires that there always exists some model\n\n[^0]\n[^0]:    ${ }^{20}$ Our focus here is on an uni-dimensional parameter space $\\mathbb{R}$ to align with applications commonly found in the literature, however, the results in this paper are applicable to $\\mathbb{R}^{d}$, with slight modifications. The reader is also referred to the discussion on multi-dimensional parameter spaces in Section 7.\n    ${ }^{21}$ See Examples 1 and 2 for an instance for the former, and Example 3 for the latter. Den Haan and Drechsel (2019) explore more on forms of model misspecification and their relevance to econometric methods with macroeconomic models.\n    ${ }^{22}$ The Prokhorov metric is used to measure the closeness of probability measures based on their tightness and weak convergence. For two transition functions $Q_{1}$ and $Q_{2}$, the Prokhorov metric is defined as:\n\n    $$\n    d\\left(Q_{1}, Q_{2}\\right)=\\inf \\left\\{\\epsilon>0: Q_{1}(A) \\leqslant Q_{2}\\left(A^{\\epsilon}\\right)+\\epsilon \\text { and } Q_{2}(A) \\leqslant Q_{1}\\left(A^{\\epsilon}\\right)+\\epsilon, \\forall A \\in \\mathcal{B}(\\mathbb{X})\\right\\}\n    $$\n\n    where $A^{\\epsilon}$ is the $\\epsilon$-neighborhood of $A$, given by: $A^{\\epsilon}=\\{x \\in \\mathbb{X}: d(x, a)<\\epsilon$ for some $a \\in A\\}$. This metric is particularly well-suited for compact state spaces.\n    ${ }^{23}$ The density function $D_{\\theta}(\\cdot \\mid s, x)$ is the Radon-Nikodym derivative of $Q$ with respect to a model, $Q_{\\theta}$. The uniform integrability is satisfied if the density functions $D_{\\theta}(\\cdot \\mid s, x)$ are uniformly bounded over the set $\\{(\\theta, s, x)$ : $Q(\\cdot \\mid s, x)$ is dominated by $Q_{\\theta}(\\cdot \\mid s, x)\\}$. For example, given any two Gaussian distributions with distinct variances, the Radon-Nikodym derivative of the one with the larger variance with respect to the other is unbounded.\n\n$Q_{\\theta}$ that accounts for every observation from the true distribution, $Q$. Condition (iii) places an uniform integrability requirement to deal with distributions with infinite support. Lastly, condition (iv) requires the parameter space $\\Theta$ to be compact. This is an essential requirement for the existence of best-fit models as will be evident in the next definition. We next measure the extent of misspecification in terms of the well-known measure of relative entropy, the KullbackLeibler divergence.\n\nDefinition 4. For a given true transition function $Q$ and model transition function $Q_{\\theta}$, the Kullback-Leibler divergence, $\\mathcal{D}_{\\mathrm{KL}}$, of $Q_{\\theta}$ with respect to $Q$ is defined as,\n\n$$\n\\mathcal{D}_{\\mathrm{KL}}\\left(Q(s, x), Q_{\\theta}(s, x)\\right)=\\mathbb{E}_{Q(\\cdot \\mid s, x)}\\left[\\ln \\left(D_{\\theta}\\left(s^{\\prime} \\mid s, x\\right)\\right)\\right]\n$$\n\nThen for any distribution over states and actions, $m \\in \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})$, and model parameter, $\\theta \\in \\Theta$, the weighted Kullback-Leibler divergence is a mapping $K_{Q}: \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X}) \\times \\Theta \\rightarrow \\overline{\\mathbb{R}}_{\\geqslant 0}$ such that\n\n$$\nK_{Q}(m, \\theta)=\\int_{\\mathbb{S} \\times \\mathbb{X}} \\mathcal{D}_{\\mathrm{KL}}\\left(Q(s, x), Q_{\\theta}(s, x)\\right) m(\\mathrm{~d} s, \\mathrm{~d} x) .^{24}\n$$\n\nThe set of closest parameter values given a distribution $m \\in \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})$ and for a given true transition function $Q$ is the set, $\\Theta(m ; Q)=\\underset{\\theta \\in \\Theta}{\\operatorname{argmin}} K_{Q}(m, \\theta) .^{25}$\n\nThat is, given a distribution $m$ over states and actions and a true transition function $Q$, the best-fit set $\\Theta(m ; Q)$ is the set of those parameter values that minimize the weighted relative entropy between true transition function and the parameterized model transition functions in $\\Theta$. Our focus in this paper is on the Berk-Nash equilibrium. We now define it.\n\nDefinition 5. A probability distribution $m^{*} \\in \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})$ is a Berk-Nash equilibrium of the regular-SMDP if there exists a belief $\\mu^{*} \\in \\mathcal{M}_{1}(\\Theta)$ such that the following conditions hold.\n(a) For all states and actions, $(s, x)$, that are in the support of $m^{*}$, action $x$ is optimal given state $s$ in the $\\operatorname{MDP}\\left(\\bar{Q}_{\\mu^{*}}\\right)$, where $\\bar{Q}_{\\mu^{*}}=\\int_{\\Theta} Q_{\\theta} \\mu^{*}(\\mathrm{~d} \\theta)$.\n(b) For a given true transition function $Q$, beliefs $\\mu^{*}$ are restricted over the best-fit set of parameters, that is, $\\mu^{*} \\in \\mathcal{M}_{1}\\left(\\Theta\\left(m^{*} ; Q\\right)\\right)$.\n(c) For all $A \\in \\mathcal{B}(\\mathbb{S}), m_{\\mathbb{S}}^{*}(A)=\\int_{\\mathbb{S} \\times \\mathbb{X}} Q(A \\mid s, x) m^{*}(\\mathrm{~d} s, \\mathrm{~d} x)$, where $m_{\\mathbb{S}}^{*}$ denote the invariant marginal measure of $m^{*}$ on $\\mathbb{S}$.\n\n[^0]\n[^0]:    ${ }^{24} \\overline{\\mathbb{R}}$ denotes the extended real line, equipped with the one-point compactification topology.\n    ${ }^{25} \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})$ is the space of finite probability measures on $(\\mathbb{S} \\times \\mathbb{X}, \\mathcal{B}(\\mathbb{S} \\times \\mathbb{X}))$ endowed with the weak- $*$ topology. The density function $D_{\\theta}\\left(s^{\\prime} \\mid s, x\\right)$ is jointly continuous on the set $\\left\\{\\left(\\theta, s^{\\prime}, s, x\\right): Q(s, x)\\right.$ is dominated by $\\left.Q_{\\theta}(s, x)\\right\\}$. We follow the standard convention in that $\\ln (0) \\cdot 0=0$ and integral of infinity over a set of measure 0 is 0 .\n\nThe Berk-Nash equilibrium is a probability distribution $m^{*}$ over the states and actions, supported by equilibrium beliefs $\\mu^{*}$ over the best-fit set of parameterized models. The beliefs $\\mu^{*}$ are optimal given the data generated by the steady-state distribution over states and actions, $m^{*}$. In turn, the distribution $m^{*}$ over states and actions is such that actions, conditioned on the state, are subjectively optimal given the equilibrium beliefs $\\mu^{*}$ over the best-fit models. For instance, in the case of a correctly specified MDP, where the true transition function $Q$ is part of the set of models $\\mathcal{Q}_{\\Theta}$, the concept of Berk-Nash equilibrium implies that in the steady state for a Bayesian learner, beliefs concentrate on the true transition function, and the actions are optimal given this belief. The equilibrium concept effectively reduces to the well-known stationary solution for a $\\operatorname{MDP}(Q)$.\n\nHowever, if the SMDP is misspecified, that is, if $Q$ is not contained within the set $\\mathcal{Q}_{\\Theta}$, then in Definition (5), condition (a) asserts that action $x$ is optimal in the $\\operatorname{MDP}\\left(\\bar{Q}_{\\mu^{*}}\\right)$ when the equilibrium transition function is a weighted combination of model transition functions, with weights given by $\\mu^{*}$. Condition (b) mandates that these beliefs $\\mu^{*}$ are determined by minimizing the weighted relative entropy between the true transition function and the model transition functions, where the weights are the Berk-Nash equilibrium, $m^{*}$. Lastly, condition (c) stipulates that the marginal distributions over states, $m_{\\mathbb{S}}^{*}$ is an invariant distribution. Notice that in condition (c), the invariant distribution is dependent on the optimal actions $x$ via the true transition function, $Q$. EP show that under moderate conditions, the learning illustration with misspecified models sketched in Equation (1) converges to the Berk-Nash equilibrium. ${ }^{26}$ The equilibrium serves as a prediction of the steady-state behavior in where $m^{*}$ is the steadystate distribution over states and actions, and $\\mu^{*}$ is the limit of the sequence of posteriors of a Bayesian learner with misspecified models.\n\nOur focus is on the comparative statics of the Berk-Nash equilibrium and the associated best-fit set with respect to the primitives $P$ of the environment, where $P=<u, \\beta, Q, Q_{\\Theta}, \\Theta>$ are our objects in the collection of primitives. Given any primitive $p \\in P$, we next adjust our value function, the optimal policy correspondence, and the best-fit set to allow for their dependence on the primitive. At the steady state solution, agent has a belief $\\mu^{*}$ over their set of best-fit models and solves a stationary dynamic programming problem where the value function $V: \\mathbb{S} \\times \\mathcal{M}_{1}(\\Theta) \\times P \\rightarrow \\mathbb{R}$ is determined by the following functional equation, where\n\n[^0]\n[^0]:    ${ }^{26} \\mathrm{EP}$, in their paper, demonstrate the usefulness of their equilibrium concept by providing a learning foundation for finite spaces (Theorems 2 and 3), under either positive visitation or identification. The identification condition also holds in our setting, courtesy of Assumption 3. Anderson, Duanmu, Ghosh, and Khan (2022) have partially addressed the convergence problem for infinite spaces in an ArXiv version (v2) of their manuscript, available here: [https://arxiv.org/pdf/2206.08437v2; see Section C. 2 in Appendix, pages 48-56]. For infinite (but compact) state and action spaces, they demonstrate that convergence holds under identification, but only in the total-variation norm on the space of state and action distributions.\n\n$$\n\\begin{aligned}\n& \\bar{Q}_{\\mu^{*}}=\\int_{\\Theta} Q_{\\theta} \\mu^{*}(d \\theta) \\\\\n& \\qquad V\\left(s, \\mu^{*}, p\\right)=\\max _{x \\in \\mathbb{X}}\\left\\{\\int_{\\mathbb{S}}\\left\\{u\\left(s, x, s^{\\prime}\\right)+\\beta V\\left(s^{\\prime}, \\mu^{*}, p\\right)\\right\\} \\bar{Q}_{\\mu^{*}}\\left(d s^{\\prime} \\mid s, x\\right),\\right\\} \\text { and }\n\\end{aligned}\n$$\n\ncorresponding to $V$ and $\\mu^{*}$, the stationary optimal policy correspondence $G$ is given by,\n\n$$\nG\\left(s, \\mu^{*}, p\\right) \\equiv \\underset{x \\in \\mathbb{X}}{\\arg \\max }\\left\\{\\int_{\\mathbb{S}}\\left\\{u\\left(s, x, s^{\\prime}\\right)+\\beta V\\left(s^{\\prime}, \\mu^{*}, p\\right)\\right\\} \\bar{Q}_{\\mu^{*}}\\left(d s^{\\prime} \\mid s, x\\right)\\right\\}\n$$\n\nTherefore, in line with Definition 5, the set of Berk-Nash equilibriua for a given primitive $p$ is given by the set of fixed points of the equilibrium mapping $T$,\n\n$$\n\\Lambda(p) \\equiv\\left\\{\\left(m^{*}, \\mu^{*}\\right) \\in \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X}) \\times \\mathcal{M}_{1}(\\Theta): z \\in T(z, p)\\right\\}\n$$\n\nwhere $T: Z \\times P \\rightarrow 2^{Z}$ is a set-valued function on the space of probability measures on states and actions and the set of parameters, $Z=\\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X}) \\times \\mathcal{M}_{1}(\\Theta)$ and $P=<u, \\beta, Q, Q_{\\Theta}, \\Theta>$ are our primitives.\n\nFor inquiring the comparative statics behavior of the equilibrium objects in $\\Lambda(p)$ with respect to the primitives of the environment, we need to define corresponding orders for the equilibrium objects and the primitives. For vectors $x$ and $y$ in $\\mathbb{R}^{n}$, we use the following convention: \" $x \\geqslant y$ \" means $x_{i} \\geqslant y_{i}$ in every component, \" $x>y$ \" means $x \\geqslant y$ and $x \\neq y$, and \" $x \\gg y$ \" means $x_{i}>y_{i}$ in every component. A real-valued function, $f: X \\rightarrow \\mathbb{R}$, is said to be increasing if for $x \\geqslant y$ in the component-wise order, $f(x) \\geqslant f(y)$, and convex if its domain is a convex set and for all $x, y$ in its domain, and all $\\lambda \\in[0,1]$, we have $f(\\lambda x+(1-\\lambda) y) \\leqslant \\lambda f(x)+(1-\\lambda) f(y) .{ }^{27}$ We follow Shaked and Shanthikumar (2007) in ranking probability measures on the state and action space, and the parameter space.\n\nDefinition 6 (Shaked and Shanthikumar (2007)). Let $\\mathcal{M}(X)$ denote the space of probability measures on a compact subset $X \\subseteq \\mathbb{R}^{k}$ for some finite $k \\in \\mathbb{N}$. For any two measures, $\\mu$ and $\\nu$ in $\\mathcal{M}(X)$,\n(i) $\\mu$ usual order stochastically dominates $\\nu, \\mu \\gtrsim_{s t} \\nu$, if $\\int f(x) \\mu(\\mathrm{d} x) \\geqslant \\int f(x) \\nu(\\mathrm{d} x)$, for any measurable, bounded, and increasing real-valued function $f$.\n(ii) $\\mu$ increasing and convex-order stochastically dominates $\\nu, \\mu \\gtrsim_{i c x} \\nu$, if $\\int f(x) \\mu(\\mathrm{d} x) \\geqslant$ $\\int f(x) \\nu(\\mathrm{d} x)$, for any measurable, bounded, increasing, and convex real-valued function $f$.\n\n[^0]\n[^0]:    ${ }^{27}$ The interested reader is referred to Shaked and Shanthikumar (2007) for more details concerning univariate and multivariate orders.\n\nWe rely on well-known orders for the primitives. For instance, the discount factor $0<\\beta<$ 1 is ranked in the natural order, as in higher patience implies a higher $\\beta$. The true transition function $Q$ can be ordered in multiple ways, including the usual stochastic order, the convex order, or the increasing convex order. ${ }^{28}$ The parameter set $\\Theta$ is ranked in the strong-set order, where $\\Theta_{2}$ is greater than or equal to $\\Theta_{1}$ if, for any $\\theta$ in $\\Theta_{2}$ and any $\\theta^{\\prime}$ in $\\Theta_{1}$, the maximum of $\\left\\{\\theta, \\theta^{\\prime}\\right\\}$ belongs to $\\Theta_{2}$ and the minimum of $\\left\\{\\theta, \\theta^{\\prime}\\right\\}$ belongs to $\\Theta_{1}$. Changes in the primitives of the utility function refer to changes that impact utility levels such as variations in risk aversion or other factors, depending on the specific applications.\n\nAfter defining all the necessary prerequisites for analyzing the comparative statics of the fixed points in Equation (9), the next logical step is to apply the standard lattice theoretic methods as used in (Hopenhayn and Prescott (1992), Topkis (1998)) and derive the monotonicity of the fixed points, the Berk-Nash equilibrium and the corresponding inferred model, in terms of the primitives. However, there is a technical challenge here. Notice that for our equilibrium mapping $T$, the underlying space on which it is defined is not a lattice. That is, the space of probability measures over states and actions (and parameters) are not lattices in any natural order. ${ }^{29}$ Thus, standard lattice theoretic methods are of no use for our misspecified MDP setting. Therefore, to establish the monotonicity of the fixed points, our results rely on a rather novel application of a technical framework that originated in Smithson (1971) and was pioneered in the context of large dynamic economies by Acemoglu and Jensen (2015), for non-lattice spaces.\n\nRemark 1. Our setting differs from that of Acemoglu and Jensen (2015) in several key aspects. Their work examines the steady-state outcomes of an infinite-horizon dynamic economy with a continuum of agents experiencing idiosyncratic (exogenous) shocks, whereas we focus on the steady-state outcomes of a single agent facing an infinite-horizon dynamic problem, where shocks depend on the agent's action choices and are therefore endogenous. Furthermore, our setting features a Bayesian agent learning with misspecified models of the shock process, unlike Acemoglu and Jensen (2015), which assumes that agents know the true distribution of idiosyncratic shocks, thereby removing the need for any learning component. As a result, their techniques require adaptations to suit our framework (see Auxiliary Lemmas 1 and 2).\n\nTowards this end, we shall show (Auxiliary Lemmas 1 and 2 in Appendix B) that our equilibrium mapping $T$ is monotonic in the sense defined below.\n\n[^0]\n[^0]:    ${ }^{28}$ Convex order in this case, would refer to changes in true distribution in the sense of mean-preserving spreads. For two distributions, $Q_{1}$ and $Q_{2}$, distribution $Q_{2} \\gtrsim_{c x} Q_{1}$ iff $\\int f(s) Q_{2}(\\cdot \\mid s, x) \\geqslant \\int f(s) Q_{1}(\\cdot \\mid s, x)$ for every convex function, $f$, and for all $(s, x) \\in \\mathbb{S} \\times \\mathbb{X}$.\n    ${ }^{29}$ Let $\\mathcal{M}(X)$ denote the space of probability measures defined on a compact subset of $X \\subset \\mathbb{R}^{n}$. Although even if $X$ is a lattice, the poset $\\left(X, \\gtrsim_{s t}\\right)$ is not a lattice as pointed in Kamae, Krengel, and O'Brien (1977). For e.g., for $\\left(\\mathbb{R}^{2}, \\gtrsim_{s t}\\right)$ let $p_{1}=0.5\\left(\\epsilon_{a}+\\epsilon_{b}\\right), p_{2}=0.5\\left(\\epsilon_{a}+\\epsilon_{c}\\right), p_{3}=0.5\\left(\\epsilon_{c}+\\epsilon_{b}\\right), p_{4}=0.5\\left(\\epsilon_{a}+\\epsilon_{d}\\right)$, where $a=(0,0), b=$ $(0,1), c=(1,0), d=(1,1)$. Then, both $p_{3}$ and $p_{4}$ are supremum, which is a contradiction.\n\nDefinition 7 (Smithson (1971)). Let $X$ and $Y$ be sets equipped with some partial order $\\gtrsim$, and $P$, a partially ordered set. A correspondence $T: X \\times P \\rightarrow 2^{Y}$ is Type I monotone in $x$ for each $p$ if for all $x_{1} \\gtrsim x_{2}$ and $y_{2} \\in T\\left(x_{2}, p\\right)$, there exists $y_{1} \\in T\\left(x_{1}, p\\right)$ such that $y_{1} \\gtrsim y_{2}$, and Type II monotone if for all $x_{1} \\gtrsim x_{2}$ and $y_{1} \\in T\\left(x_{1}, p\\right)$, there exists $y_{2} \\in T\\left(x_{2}, p\\right)$ such that $y_{1} \\gtrsim y_{2}$. A correspondence $T: X \\times P \\rightarrow 2^{Y}$ is Type I monotone in $p$ for each $x$ if for all $p_{1} \\gtrsim p_{2}$ and $y_{2} \\in T\\left(p_{2}, x\\right)$, there exists $y_{1} \\in T\\left(p_{1}, x\\right)$ such that $y_{1} \\gtrsim y_{2}$, and Type II monotone if for all $p_{1} \\gtrsim p_{2}$ and $y_{1} \\in T\\left(p_{1}, x\\right)$, there exists $y_{2} \\in T\\left(p_{2}, x\\right)$ such that $y_{1} \\gtrsim y_{2}$. $T$ is Type I (Type II) monotone if it is Type I (Type II) monotone in both $X$ and $P .{ }^{30}$\n\nBy placing mild monotonicity structure on our environment, we shall show that the BerkNash equilibrium map $T$ is Type I and Type II monotone. And then by appealing to an existence result in Smithson (1971) for non-lattice spaces, we shall show that the set of fixed points is non-empty. ${ }^{31}$ Finally, the following result in Acemoglu and Jensen (2015) will play a critical role in transfer the monotonicity of the equilibrium mapping $T$ onto the set of its fixed points. We explain in more detail the applicability of these results in Section 4.\n\nTheorem (Acemoglu-Jensen (2015)). Let $X$ be a compact topological space equipped with a closed partial order $\\gtrsim, P$ a partially ordered set of primitives, and $T: X \\times P \\rightarrow 2^{X}$ be upper hemicontinuous for each $p \\in P$. Define the fixed-point correspondence,\n\n$$\n\\Lambda(p)=\\{x \\in X: x \\in T(x, p)\\}\n$$\n\nThen if $T$ is Type I (Type II) monotone in $p$, so is $\\Lambda$ in $p$.", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 3 Motivating Examples \n\nWe now present three examples of misspecified environments that illustrate the framework of this paper. Our first example is that of a misspecified $\\operatorname{AR}(1)$ inference process where actions have no role. It is purely an inference problem. ${ }^{32}$ Examples 2 and 3 are more substantive in that the choice of actions plays a role in learning, and vice-versa. They are based on EP and cover effort provision problems with misspecified effort-task dynamics and consumption-savings problem with misspecified wealth process, respectively. While Example 2 has an explicit analytical solution and is, therefore, used to outline the mechanics for comparative statics properties\n\n[^0]\n[^0]:    ${ }^{30}$ Smithson uses the term 'multifunction' in his paper. All the proofs in this section are given for Type I monotonicity. The proofs for Type II monotonicity follow analogously. It is important to point out here that while the statement for Theorem 1 is correct in Smithson (1971), its proof is wrong; the correct proofs are in H\u00f6ft (1987).\n    ${ }^{31}$ The reader may wish to pause and review the material in the mathematical preliminaries in Appendix A.\n    ${ }^{32}$ Per se, there is no decision process involved here. One could think of an economic situation where the flow of utility is constant over time, and therefore, actions play no role in either inference, nor, payoffs.\n\nof a Berk-Nash equilibrium, the solution for Example 3 is intractable and, therefore, the most substantive for the applicability of this paper's techniques. ${ }^{33}$\n\nExample 1 (Inference of an $\\mathrm{AR}(1)$ process). In this example, we characterize the Berk-Nash equilibrium of an inference process when the set of $\\mathrm{AR}(1)$ models, parameterized by $\\theta$, is misspecified. Let the state space $\\mathbb{S}=\\mathbb{R}$. Suppose the state variable $s_{t+1}$ evolves via the true transition function, $Q\\left(\\cdot \\mid s_{t}\\right)$. Let the true process $Q\\left(\\cdot \\mid s_{t}\\right)$ be an $\\operatorname{AR}(1)$ process with parameter $0<|\\rho|<1$ and with the innovations distributed as a two-component mixture normal distribution with one component $\\left(\\mu_{1}, \\sigma_{1}^{2}\\right)$ and the other component $\\left(\\mu_{2}, \\sigma_{2}^{2}\\right)$, and given by,\n\n$$\ns_{t+1}=\\rho s_{t}+\\xi_{t+1}, \\quad \\xi_{t+1} \\sim 0.5 F_{\\left(\\mu_{1}, \\sigma^{2}\\right)}+0.5 F_{\\left(\\mu_{2}, \\sigma^{2}\\right)}\n$$\n\nwhere $F$ denotes the cumulative density function for a normal distribution. The components have different means $\\left(\\mu_{1} \\neq \\mu_{2}\\right)$ but identical variances $\\left(\\sigma_{1}^{2}=\\sigma_{2}^{2}\\right)$. An agent is equipped with a compact set $\\Theta$ of parameterized $\\operatorname{AR}(1)$ models, $Q_{\\theta}\\left(\\cdot \\mid s_{t}\\right)$, with Gaussian noise, and the process,\n\n$$\ns_{t+1}=\\theta s_{t}+\\xi_{t+1}, \\quad \\xi_{t+1} \\sim N\\left(0, \\sigma^{2}\\right)\n$$\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: While the innovation process in the true distribution is mixture-normally distributed with distinct means and identical variances, resulting in a bi-modal distribution, the innovation processes in the agent's models conform to a normal distribution.\n\nThe agent's models are misspecified since the true distribution is not normally distributed, and therefore, is not in the set of models, $Q_{\\Theta}$. Following Definition (5), the Berk-Nash equilibrium is the stationary distribution $m_{S}^{*}$ over the set of states (no actions), implied by the true process, $Q\\left(\\cdot \\mid s_{t}\\right)$, with the corresponding best-fit $\\operatorname{AR}(1)$ parameter, $\\theta^{*}=\\int_{\\mathbb{S}} \\hat{\\theta}(s) m^{*}(d s)$, where,\n\n$$\n\\hat{\\theta}(s) \\equiv \\underset{\\Theta}{\\operatorname{argmin}} \\mathrm{KL}\\left(Q \\mid Q_{\\theta}\\right)=\\underset{\\Theta}{\\operatorname{argmin}} \\mathbb{E}_{Q}\\left[\\frac{\\ln Q(\\cdot \\mid s)}{\\ln Q_{\\theta}(\\cdot \\mid s)}\\right]\n$$\n\n[^0]\n[^0]:    ${ }^{33}$ We omit the full analysis of Examples 2 and 3 since they are already covered in the aforementioned papers (EP and ADGK).\n\nNow, given that the $\\operatorname{AR}(1)$ models are Gaussian and therefore log-concave, the state-dependent best-fit model $\\hat{\\theta}(s)$ is uniquely determined. ${ }^{34}$ Since the innovations in the models are Gaussian, $\\hat{\\theta}(s)$ is the least-squares minimizer with respect to the true transition $Q\\left(\\cdot \\mid s_{t}\\right)$, and solves the following,\n\n$$\n\\hat{\\theta}(s) E_{Q(\\cdot \\mid s)}\\left(s^{2}\\right)=E_{Q(\\cdot \\mid s)}\\left(s^{\\prime} s\\right)\n$$\n\nThe best-fit inferred $\\operatorname{AR}(1)$ parameter has the following form,\n\n$$\n\\theta^{*}=\\int_{\\mathbb{S}} \\hat{\\theta}(s) m_{\\mathbb{S}}^{*}=\\rho+\\int_{\\mathbb{S}} \\frac{\\left(\\mu_{1}+\\mu_{2}\\right)}{2 s} m_{\\mathbb{S}}^{*}\n$$\n\nNow, suppose $\\mu_{1}+\\mu_{2}=0$. That is, the mean of the components of the true distribution cancel each other. In this case, a Bayesian agent, starting from any prior on their models $Q_{\\Theta}$, would eventually converge in the Berk-Nash equilibrium to the true persistence parameter $\\rho$ and yet continue to remain misspecified. For this case, the comparative statics of the steady state inferred $\\theta^{*}$ with respect to the $\\operatorname{AR}(1)$ parameter of the true process is one-to-one in a trivial way. ${ }^{35}$\n\nExample 2 (Effort provision with unknown ability (EP)). In this setting of a dynamic effort provision problem, we follow EP in characterizing the Berk-Nash equilibrium. The equilibrium for this instance is a probabilistic prediction for the agent's long run outcome to be either a success or a failure, along with their inference of their ability. This example is related to the growing literature on effort provision, as modeled in Deimen and Wirtz (2022).\n\nEach period, an agent chooses either to put high $(H)$ or low $(L)$ effort, $x_{t} \\in \\mathbb{X}=\\{L, H\\}, L<$ $H$ in a task. The task then has two outcomes, either it fails ( 0 ) or succeeds (1), that is, $s_{t+1} \\in \\mathbb{S}=\\{0,1\\}$. The payoffs are as follows,\n\n$$\nu\\left(x_{t}, s_{t+1}\\right)= \\begin{cases}s_{t+1,} & x_{t}=L \\\\ s_{t+1}-c, & x_{t}=H\\end{cases}\n$$\n\nIn case the effort is low, the agent's payoff is the state realization, while if the effort is high, in addition to the state realization, they also incur a cost $c$ for their high effort. Under the true process $Q\\left(\\cdot \\mid s_{t}, x_{t}\\right)$, if the agent puts in the high effort, the probability of the task being a success\n\n[^0]\n[^0]:    ${ }^{34}$ It is worth mentioning here that Berk's characterization of the limit posterior is for an i.i.d. process. Here conditioned on the state $s$, the $\\operatorname{AR}(1)$ process behaves as i.i.d., and therefore, $\\hat{\\theta}(s)$ could be interpreted as the posterior to which the agent converges if they were to receive infinite state-realizations, all i.i.d., given the state is fixed.\n    ${ }^{35}$ For the case when $\\mu_{1}+\\mu_{2} \\neq 0$, the expression for $\\theta^{*}$ is potentially intractable since the expectation with respect to the Berk-Nash equilibrium $m^{*}$ may simply not exist. The inspiration for the mixture normal distribution comes from a paper by Csaba and Szoke (2023) who look at learning when the set of likelihoods is misspecified.\n\nis 1 , irrespective of the outcome of the task in the current state. That is, $Q\\left(1 \\mid s_{t}, H\\right)=1$. With low effort, however, the probability of success tomorrow is dependent on the outcome today; $Q(1 \\mid 0, L)=q_{0}$, if it is a failure, and $Q(1 \\mid 1, L)=q_{1}$, if it is a success. Following EP, we assume that $0<q_{0}<1-c<q_{1}<1$. From a routine calculation in EP, the optimal correctly specified policy function is the following,\n\n$$\nx\\left(s_{t}\\right)= \\begin{cases}H, & s_{t}=0 \\\\ L, & s_{t}=1\\end{cases}\n$$\n\nThat is, a failure today, pushes the agent to work harder (since $q_{0}<1-c$ ), while a success makes them put lower effort (since $1-c<q_{1}$ ).\n\nHowever, the agent has a set of misspecified models $\\mathcal{Q}_{\\Theta}=\\left\\{Q_{\\theta}\\right\\}$, parameterized by $\\theta \\in$ $\\Theta=[0,1]$, where $Q_{\\theta}(1 \\mid s, H)=1$ and $Q_{\\theta}(1 \\mid s, L)=\\theta$ for all $s \\in\\{0,1\\}$. The agent's models correctly captures the true dynamics, if they were to put in higher effort. However, under lower effort, their models imply that the chances of success are independent of the state today, and is denoted by the parameter $\\theta$. Following a growing literature on effort provision that models an agent's control over their future success, based on their effort and ability, we novelty interpret the parameter $\\theta$ as an index for the agent's ability level. That is, the agent is someone who thinks that when the effort is high, then their ability plays no role in determining the outcome tomorrow since higher effort guarantees success. Whereas, when the effort is low, it is only the ability that matters for the successful outcome, with higher ability implying higher chances of success.\n\nFor the Berk-Nash equilibrium, following EP, we shall focus on the unique mixed-strategy equilibrium wherein the choice of actions are independent of the current state, as implied by the misspecified models. A routine calculation shows that under certain parameterizations, a unique Berk-Nash equilibrium exists wherein the agent plays a mixed strategy, conditioned on the state, $m^{*}(L \\mid 0)=m^{*}(L \\mid 1)=\\frac{q_{1}-(1-c)}{c\\left(q_{1}-q_{0}\\right)}$, and correspondingly infers their ability, $\\theta^{*}=1-c .{ }^{36}$\n\nExample 3 (Savings with misperceived wealth process (EP, ADGK)). In this example, we demonstrate that there are environments wherein there is no clear analytical solution for the Berk-Nash equilibrium and its corresponding inferred model, a feature most common to dynamic programming problems.\n\nIn each period, an agent realizes wealth $y_{t}$, an i.i.d. preference shock $z_{t}$, and subsequently chooses to save $x_{t} \\in\\left[0, y_{t}\\right]=\\mathbb{X} \\subseteq \\mathbb{R}_{+}$. The utility in period $t$ is given by $u\\left(y_{t}, z_{t}, x_{t}\\right)=z_{t} \\ln \\left(y_{t}-\\right.$ $\\left.x_{t}\\right)$. The state variables, denoted as $s=(y, z)$, belong to $\\mathbb{S}=\\mathbb{R}_{+} \\times[0,1]$. The evolution of\n\n[^0]\n[^0]:    ${ }^{36}$ The steady-state marginal distribution is given by $m_{S}^{*}(1)=\\frac{1-c-q_{0}}{q_{1}-q_{0}}$, and $m_{S}^{*}(0)=1-m_{S}(1)$. Further, $\\theta_{Q}(m)=\\left(1-m_{\\mathbb{S}}(1)\\right) q_{0}+m_{\\mathbb{S}}(1) q_{1}$. The reader is referred to the EP paper (pp. 726-728) for the full numerical details.\n\nwealth $y_{t+1}$ in the following period is governed by the following process,\n\n$$\n\\ln y_{t+1}=\\alpha^{*}+\\beta^{*} \\ln x_{t}+\\varepsilon_{t}\n$$\n\nwhere the unobserved productivity shock, $\\varepsilon_{t}=\\gamma^{*} z_{t}+\\xi_{t}$, with $\\xi_{t}$ following a standard normal distribution, $\\xi_{t} \\sim \\mathcal{N}(0,1)$, and $z_{t}$ following a uniform distribution, with mean $E(z)$. Here, $\\beta^{*}$ denotes the true return on one additional unit of $\\log$ of saving in terms of the log of wealth. We assume that it lies between 0 and 1 , so that the true Markov process is stationary. Following EP, we shall assume that the preference and the productivity shocks are positively correlated, $\\gamma^{*}>0$. For an agent that knows all the primitives of the environment, solving for the Bellman as in Equation (3), the correctly specified optimal policy function is given by, $x^{*}=A_{z}\\left(\\beta^{*}\\right) y$, where $A_{z}\\left(\\beta^{*}\\right)=\\frac{\\delta \\beta^{*} E(z)}{\\left(1-\\delta \\beta^{*}\\right) z+\\delta \\beta^{*} E(z)}$, where $0<\\delta<1$ is the discount factor.\n\nHowever, the agent's set of models postulates the following wealth process, $\\ln y_{t+1}=$ $\\alpha+\\beta \\ln x_{t}+\\varepsilon_{t}$, where $\\varepsilon_{t} \\sim N(0,1)$, thus ignoring the correlation between the productivity and the preference shocks. So while the true process is $Q\\left(y^{\\prime}, z^{\\prime} \\mid y, z, x\\right)$ is such that $y^{\\prime}$ and $z^{\\prime}$ are independent, $y^{\\prime}$ has a log-normal distribution with mean $\\alpha^{*}+\\beta^{*} \\ln x+\\gamma^{*} z$ and unit variance, and $z^{\\prime} \\sim U[0,1]$, the misspecified process is $Q_{\\theta}\\left(y^{\\prime}, z^{\\prime} \\mid y, z, x\\right)$ is such that $y^{\\prime}$ and $z^{\\prime}$ are independent, $y^{\\prime}$ has a log-normal distribution with mean $\\alpha+\\beta \\ln x$ and unit variance, and $z^{\\prime} \\sim U[0,1]$.\n\nFor this case, the Berk-Nash equilibrium is characterized by an optimal policy function, $x^{m}=A_{z}\\left(\\beta^{m}\\right) y=\\frac{0.5 \\delta \\beta^{m}}{\\left(1-\\delta \\beta^{m}\\right) z+0.5 \\delta \\beta^{m}} y$, where there exists a corresponding inferred model $\\beta^{m} \\in\\left(0, \\beta^{*}\\right)$. That is, the misspecified agent underestimates the return on her saving, $\\beta^{m}<\\beta^{*}$, and therefore, undersaves in the Berk-Nash equilibrium, $x^{m}<x^{*}{ }^{37}$ Notice however that there is no closed-form expression for $\\beta^{m}$ that outlines its dependence on the environment primitives, and therefore, how the inferred return, the equilibrium policy function, and the invariant distribution respond to the primitives of the environment is an open question. ${ }^{38}$\n\n[^0]\n[^0]:    ${ }^{37}$ The intuition behind this is that a higher preference shock $z$ is associated with a lower saving proportion, $A_{z}\\left(\\beta^{m}\\right)$, since it leads to a lower inferred belief of return on saving, $\\beta^{m}$, since the preference and productivity shocks are positively correlated.\n    ${ }^{38}$ From EP: The inferred model at the stationary Berk-Nash equilibrium solves the following equation,\n\n    $$\n    \\hat{\\beta}\\left(A_{z}(\\beta)\\right)=\\beta^{*}+\\gamma^{*} \\frac{\\operatorname{Cov}\\left(z, \\ln A_{z}(\\beta)\\right)}{\\operatorname{Var}\\left(\\ln A_{z}(\\beta)\\right)+\\operatorname{Var}(\\ln Y)}\n    $$\n\n    where the covariance and variance are taken with respect to the true distribution, $Q\\left(y^{\\prime}, z^{\\prime} \\mid y, z\\right)$. Under the parametric assumptions of $\\gamma^{*}>0$, and $\\operatorname{Cov}\\left(z, A_{z}\\right)<0$, EP establish that there exists a $\\beta^{m}$, the inferred model in the Berk-Nash equilibrium. Further, in EP, notice that the inferred model depends on the proportion, $A_{z}$, instead of the distribution over states and actions, which is unlike the case required as per the conditions of Berk-Nash equilibria.", "tables": {}, "images": {"img-0.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 4, "text": "# 4 Main Results \n\nWe now turn to our main results that are on the existence and the comparative statics of Berk-Nash equilibrium for a misspecified dynamic optimization problem. We invoke techniques from the fixed point literature (Smithson (1971); Acemoglu and Jensen (2015)) and, in doing so, impose further structure on our regular SMDP. While assumptions 1 and 2 are useful for establishing increasing policy selections, assumptions 3 and 4 give the required monotonicity and identification properties for our models. Throughout, we shall assume that $Q$ is monotone. The proofs for all the results are in Appendix B.\n\nAssumption 1. The state and action spaces are lattices, and for any primitive $p \\in P$, $u\\left(s, s^{\\prime}, x, p\\right)$ is supermodular in $(s, x)$ and increasing in current state $s$ and future state $s^{\\prime}$.\n\nOur first assumption is rather standard and emphasises a lattice structure on the state, action, and parameter spaces. As evinced by the examples in this paper and in general, for many economic applications, the state, action, and parameter spaces are a subset of the real line $\\mathbb{R}$, for which the lattice assumption is trivially satisfied in the natural order. Further, we assume that the payoff function is supermodular in current state and action and is increasing in the future state variable. A payoff function $u: \\mathbb{S} \\times \\mathbb{X} \\rightarrow \\mathbb{R}$ is supermodular if, for all $\\left(s_{1}, x_{1}\\right),\\left(s_{2}, x_{2}\\right) \\in \\mathbb{S} \\times \\mathbb{X}, u\\left(\\left(s_{1}, x_{1}\\right) \\vee\\left(s_{2}, x_{2}\\right)\\right)+u\\left(\\left(s_{1}, x_{1}\\right) \\wedge\\left(s_{2}, x_{2}\\right)\\right) \\geqslant u\\left(s_{1}, x_{1}\\right)+u\\left(s_{2}, x_{2}\\right)$. This implies that states and actions exhibit complementarity in the sense that marginal contribution to the payoff of increasing the action increases with a higher state. The supermodularity in states and actions and monotononicity (in state) of the payoff function alongwith our next assumption guarantees that optimal policy correspondence is increasing in the state variable, for any given model distribution $\\mu$, and primitive $p \\in P$.\n\nAssumption 2. The following holds true for all models in the family of models, $\\mathcal{Q}_{\\Theta}=$ $\\left\\{Q_{\\theta}: \\theta \\in \\Theta\\right\\}$. For any increasing real-valued function $f(\\cdot)$,\n(i) $Q_{\\theta}$ is stochastically increasing in $(s, x)$ i.e. $\\int_{\\mathbb{S}} f\\left(s^{\\prime}\\right) Q_{\\theta}\\left(d s^{\\prime} \\mid s, x\\right)$ is increasing in $(s, x)$.\n(ii) $Q_{\\theta}$ is stochastically supermodular in $(s, x)$ i.e. $\\int_{\\mathbb{S}} f\\left(s^{\\prime}\\right) Q_{\\theta}\\left(d s^{\\prime} \\mid s, x\\right)$ is supermodular in $(s, x)$.\n\nNotice that this assumption is exclusively on the set of models. It requires the model transition functions to be stochastically increasing and stochastically supermodular in states and actions. This implies that for every model $Q_{\\theta}$, a higher current state and action increases the probability of observing a higher state in the next period and that an incremental amount of action increases this probability, the higher the state is in the current period. ${ }^{39}$ Assumptions\n\n[^0]\n[^0]:    ${ }^{39}$ For example, consider the following $\\operatorname{AR}(1)$ process, $s_{t+1}=\\theta s_{t}+\\epsilon_{t+1}$, where $\\epsilon_{t+1}$ is distributed normally with mean 0 and variance $\\sigma^{2}$. Then for every $\\theta, Q_{\\theta}$ is stochastically increasing. See Examples 2 and 3 for illustrations on stochastic supermodularity.\n\n1 and 2 are consistent with the structure one requires for a correctly specified environment and the reader is referred to Theorem 3.9.2 in Topkis (1998) for a similar set of assumptions. For a given model distribution $\\mu$, and primitive $p$, the above two assumptions guarantee an increasing optimal policy correspondence $G(s, \\mu, p)$.\n\nWe next assume that given any observable endogenous data, i.e. given any distribution $m$ over states and actions, generated by agent's endogenous learning and decision making process as in Equation (1), the best-fit parameter is point-identified. That is, given $m$, the best-fit set is singleton and therefore, the parameter is uniquely determined.\n\nAssumption 3. For any given $m \\in \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})$, we assume that the $S M D P\\left(Q, Q_{\\Theta}\\right)$ is pointidentified, meaning that $\\theta, \\theta^{\\prime} \\in \\Theta(m ; Q)$ implies $\\theta=\\theta^{\\prime}$. Moreover, we assume this holds for all $m \\in \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X}) .^{40}$\n\nThe identification assumption is satisfied for many applications in the existing misspecification literature and also plays an important role in the convergence of the steady state behavior of an agent in a SMDP to the Berk-Nash equilibrium; see examples in Esponda and Pouzo (2016), EP, and, Esponda, Pouzo, and Yamamoto (2021). For our comparative statics purposes, identification ensures that we always get a unique selection $\\theta_{Q}(m)$ from the best-fit set $\\Theta(m ; Q)$. A function $\\theta_{Q}(m)$, maps a distribution $m$ into a real number in $\\Theta$ is said to be increasing in the usual stochastic order $\\gtrsim_{\\mathrm{st}}$, if $\\theta_{Q}\\left(m_{2}\\right) \\geqslant \\theta_{Q}\\left(m_{1}\\right)$ whenever $m_{2} \\gtrsim_{\\mathrm{st}} m_{1}$. For our last assumption, we require the weighted Kullback-Leibler divergence to exhibit the single-crossing property in the parameter $\\theta$ and distribution $m$.\n\nAssumption 4. $K_{Q}(\\theta ; m)$ satisfies the single crossing property in $(\\theta ; m)$, that is, for $\\theta_{1} \\leqslant \\theta_{2}$, and for $m_{2} \\gtrsim_{s t} m_{1}, \\delta\\left(m_{1}\\right)=K_{Q}\\left(\\theta_{2}, m_{1}\\right)-K_{Q}\\left(\\theta_{1}, m_{1}\\right) \\geqslant(<) 0$ implies $\\delta\\left(m_{2}\\right)=K_{Q}\\left(\\theta_{2}, m_{2}\\right)-$ $K_{Q}\\left(\\theta_{1}, m_{2}\\right) \\geqslant(<) 0 .^{41}$\n\nThe assumption above is necessary and sufficient to imply that the best-fit set $\\Theta(m ; Q)$ (coupled with Assumption 3, the inference function $\\theta_{Q}(m)$ ), is increasing in the strong-set order on the real line. ${ }^{42}$ However, as is well-known in the comparative statics literature, it can be difficult to verify for single-crossing differences. Therefore, a sufficient and easy to check condition to\n\n[^0]\n[^0]:    ${ }^{40}$ The reader is referred here to Lewbel (2019) for an exposition on identification problem in economics.\n    ${ }^{41}$ It is important to note that as long as the divergence \"measure\" satisfies a form of single crossing, as outlined in Assumptions 4 and 5, the comparative statics results remain valid. In this regard, the KL divergence serves more as an example than a critical feature for monotonicity. However, our emphasis on the KL divergence stems from the fact that the Berk-Nash solution concept identifies it as a potential limit of the misspecified learning process. Alternatively, if another divergence measure were to characterize the limit, it would need to satisfy the single crossing property for the comparative statics results to hold. I thank a referee for this observation.\n    ${ }^{42}$ This is similar to assuming monotone Bayesian updating of the kind in Torres (2005) where stochastically higher states lead to higher posterior probabilities of the parameter. Also, since the parameter spaces are in $\\mathbb{R}$, quasi-supermodularity is trivially satisfied; see Milgrom and Shannon (1994).\n\nguarantee an increasing best-fit function or set is to require that the models follow a expected log-likelihood property.\nSufficient Condition 1: For any two models $\\theta_{1}, \\theta_{2} \\in \\Theta$, such that $\\theta_{1}<\\theta_{2}$, define the loglikelihood ratio, $L\\left(s^{\\prime} \\mid s, x\\right)=\\ln \\left(D_{\\theta_{2}}\\left(s^{\\prime} \\mid s, x\\right)\\right)-\\ln \\left(D_{\\theta_{1}}\\left(s^{\\prime} \\mid s, x\\right)\\right)$, where $D_{\\theta}\\left(s^{\\prime} \\mid s, x\\right)$ are the RadonNikodym derivatives, introduced for measuring KL divergences in (5). Then the models are said to follow the expected likelihood ratio property if the expectation of $L$ with respect to the true distribution $Q(\\cdot \\mid s, x)$ is increasing in both state and action, $(s, x) .{ }^{43}$\n\nWe can now state our first result on the existence of a Berk-Nash equilibrium for a regular SMDP with infinite states and actions.\n\nTheorem 1. Under assumptions 1-3, every regular $S M D P\\left(Q, \\mathcal{Q}_{\\Theta}\\right)$ with a bounded and continuous utility function has a Berk-Nash equilibrium and the set of such equilibria is compact.\n\nEP and ADGK provide proofs for the existence of a Berk-Nash equilibrium in an SMDP with finite and infinite environments, respectively. While the former proves the theorem for finite states and actions, ADGK takes the finite result of EP as given and extends it to more naturally appealing instances of infinite environments (compact-metric spaces), using novel tools in nonstandard analysis. In contrast, our proof of existence result relies on the assumed monotonicity and identification properties of the structure of our problem, given we are housed in the Euclidean space. It is important to mention here that we do rely on the regular SMDP definition in ADGK, most importantly on part (iv) of Definition 3, because it allows us to consider distributions that have unbounded Radon-Nikodym derivatives. This is particularly relevant when dealing with distributions over infinite spaces, such as the $\\operatorname{AR}(1)$ process. ${ }^{44}$ For unbounded state spaces, one continues to rely on ADGK's structure and existence results. Further, the set of Berk-Nash equilibria will be compact and in particular, there will always exist the least and the greatest Berk-Nash equilibrium, $m^{*}$ and corresponding beliefs $\\mu^{*}$ over the best-fit inferred models. ${ }^{45}$\n\nFollowing Acemoglu and Jensen (2015), we next define a positive shock for a SMDP. Notice that this positive shock is defined for any $p \\in P=<u, \\delta, Q, Q_{\\Theta}, \\Theta>$ and therefore, appeals to any primitive $p$ that can be considered.\n\nDefinition 8. For any given belief $\\mu$, a change in a primitive of the SMDP from $p_{1}$ to $p_{2}$ is a positive (negative) shock if $G\\left(s, \\mu, p_{2}\\right)$ is greater than $G\\left(s, \\mu, p_{1}\\right)$ in the strong set order. That\n\n[^0]\n[^0]:    ${ }^{43}$ Ross (1987) contains a useful discussion regarding the existence of the expectation of the log-likelihood ratio statistic. For instance, if the true, and the model distributions belong to the Gaussian family, this statistic would always exist.\n    ${ }^{44}$ While our examples focus on unbounded state spaces, the results in this paper apply to compact state and action spaces, which do not directly impact the comparative statics properties.\n    ${ }^{45}$ The argument for the least and greatest follows from Theorem 4 in Acemoglu and Jensen (2015) for the non-lattice case. Also, see Footnote 9 (pp. 1389) in Hopenhayn and Prescott (1992) for several instances of compact subsets of measures in economic problems.\n\nis, for all $y_{1} \\in G\\left(s, \\mu, p_{1}\\right)$ and $y_{2} \\in G\\left(s, \\mu, p_{2}\\right)$, the join $y_{1} \\vee y_{2} \\in G\\left(s, \\mu, p_{2}\\right)$ and the meet $y_{1} \\wedge y_{2} \\in G\\left(s, \\mu, p_{1}\\right)$. Further, for a given primitive $p \\in P$, a change in beliefs over models in the usual order stochastic dominance sense from $\\mu_{1}$ to $\\mu_{2}$ is a positive (negative) shock if $G(s, \\mu, p)$ is ascending in $\\mu$ from $\\mu_{1}$ to $\\mu_{2}$, that is, if $y_{1} \\vee y_{2} \\in G\\left(s, \\mu_{2}, p\\right)$ and $y_{1} \\wedge y_{2} \\in G\\left(s, \\mu_{1}, p\\right)$ for all $y_{1} \\in G\\left(s, \\mu_{1}, p\\right)$ and $y_{2} \\in G\\left(s, \\mu_{2}, p\\right)$.\n\nWe now state the main result of this paper.\nTheorem 2. Suppose assumptions 1-4 hold. Then a positive shock to the primitives of the regular SMDP will lead to an increase in the least and the greatest equilibrium best-fit models. ${ }^{46}$ Further, a positive shock to the primitives will lead to\n(a) an increase in the least and greatest Berk-Nash equilibrium in the usual stochastic order dominance if changes in beliefs over models are positive shocks, and\n(b) a decrease in the least and greatest Berk-Nash equilibrium in the usual stochastic order dominance if changes in beliefs over models are negative shocks.\n\nTheorem 2 highlights the two-way interaction between decision making and learning for monotone comparative statics behavior in misspecified dynamic optimization problems with learning. Under assumptions 1-4 on the regular SMDP, it predicts that a positive shock to any of its primitives will lead to an increase in the least and greatest equilibrium beliefs over the best-fit parameterized models. Furthermore, a positive (negative) shock will lead to an increase in the least and the greatest Berk-Nash equilibrium, the steady state distribution over states and actions, if changes in beliefs over models is a positive (negative) shock. For instance, if there is a unique Berk-Nash equilibrium, as in the examples here and in the literature, with a corresponding inferred model, then Theorem 2 gives a clear prediction for the comparative statics of the equilibrium objects.\n\nRemark 2. Heidhues, K\u00f6szegi, and Strack (2018), henceforth HKS, identify an interesting result of self-defeating learning in misspecified environments. They examine how a myopic agent, with overconfident beliefs-unrealistically high expectations-about their ability, engages in a learning process that leads them away from optimal actions. Technically, the HKS framework is not directly applicable here, as it is not a Markov Decision Process (MDP) per se. However, we adapt its basic structure to formulate it as an MDP, demonstrating how the key assumptions in HKS align with ours, and deriving the comparative statics results accordingly. ${ }^{47}$\n\nIn each period $t \\in\\{1,2,3, \\ldots\\}$, the agent produces an observable output according to the law of motion:\n\n$$\nq_{t+1}=Q\\left(x_{t}, a, \\Phi\\right)+\\varepsilon_{t+1}\n$$\n\n[^0]\n[^0]:    ${ }^{46}$ It is important to note here that we are assuming that the constraint on the parameter set is non-binding.\n    ${ }^{47} \\mathrm{I}$ am deeply grateful to an anonymous referee for urging me to make this connection.\n\nwhere $x_{t} \\in[\\underline{x}, \\bar{x}]$ is the action chosen by the agent at time $t, a \\in \\mathbb{R}$ represents the agent's constant ability, and $\\Phi \\in[\\phi, \\bar{\\phi}]$ is a fixed but unobservable fundamental parameter. The term $\\varepsilon_{t+1}$ represents random noise, which is assumed to be independent and identically distributed (i.i.d.), following the assumptions specified in HKS. The agent is assumed to be overoptimistic about his ability: although his true ability is $A$, he believes his ability is $\\tilde{a}$, where $\\tilde{a}>A$. We define the degree of overconfidence as $\\Delta=|\\tilde{a}-A|$. Given this, the agent updates his beliefs about the fundamental parameter $\\Phi$ in a Bayesian manner. He discounts future outcomes with a factor $\\delta \\in(0,1)$, and in each period, chooses a current action $x_{t}$ along with a strategy for future actions, contingent on the history of the game and his learning of the fundamental, in order to maximize his discounted expected output. The agent's objective is thus to solve the following dynamic optimization problem:\n\n$$\n\\max _{x_{t}, x_{t+1}, \\ldots} \\mathbb{E}_{t}\\left[\\sum_{t=0}^{\\infty} \\delta^{t} q_{t+1}\\right]\n$$\n\nwhere the expectation is taken with respect to the agent's beliefs about the future evolution of the state $\\Phi$ and the corresponding output $q_{t+1}$, and subject to the law of motion given by the equation above.\n\nThe steady state of this dynamic optimization problem is characterized by a Berk-Nash equilibrium, which consists of a steady-state distribution over the state variable and a corresponding action choice $x^{*}$, together with an equilibrium belief $\\Phi^{*}$ about the fundamental. In equilibrium, the agent's Bellman equation satisfies\n\n$$\nV\\left(q_{t}, a, \\Phi^{*}\\right)=\\max _{x^{*} \\in[\\underline{x}, \\bar{x}]} \\mathbb{E}\\left[q_{t+1}+\\delta V\\left(q_{t+1}, a, \\Phi^{*}\\right)\\right]\n$$\n\nwhere the expectation is taken over the distribution of $\\varepsilon_{t+1}$. Furthermore, the equilibrium belief $\\Phi^{*}$ must be consistent with the agent's subjective model. In particular, $\\Phi^{*}$ satisfies the condition\n\n$$\nQ\\left(x^{*}, A, \\Phi\\right)=Q\\left(x^{*}, \\tilde{a}, \\Phi^{*}\\right)\n$$\n\nwhich arises as the solution to the problem of minimizing the Kullback-Leibler (KL) divergence between the true and the subjective distributions. ${ }^{48}$\n\nWe prove that in the non-myopic misspecified MDP version of HKS, higher overconfidence results in lower equilibrium effort. This follows from the assumptions in HKS that correspond to our Assumptions 1-4. First, HKS's Assumption 1, $Q_{a x} \\leqslant 0$, translates in our framework to a change in the primitive (ability) that qualifies as a negative shock due to submodularity. Second, the optimal effort level is always monotonic (increasing) in the change in fundamental,\n\n[^0]\n[^0]:    ${ }^{48}$ The reader is referred to Lemma 7, page 1190 in HKS.\n\nwhich translates to a change in beliefs about the fundamental in a usual-order stochastic sense, thus a positive shock. Third, for any effort level, there is a unique fundamental consistent with the output produced. This follows from $Q_{\\phi}>0$ in HKS, satisfying Assumption 3 in our paper. Finally, since $Q_{x \\phi}>0$ in HKS, this implies that optimal action and the fundamental are monotonic in each other. ${ }^{49}$\n\nWhile the full proof of the main result is provided in the Appendix, we provide a brief proof sketch below that outlines the main points of our argument.\n\nShort proof sketch: The proof of Theorem 2 follows a three-step proof structure as in Acemoglu and Jensen (2015). The first step involves showing that under Assumptions 1 and 2 , and for any fixed model distribution $\\mu$, the set of Berk-Nash equilibriua $m$ obtained via the fixed points of the equilibrium mapping $T$ will be Type I and Type II increasing in the primitives $p$. Using the set of fixed points in the first step, the second step involves constructing a mapping $\\hat{\\theta}$ that for any model distribution $\\mu$ and primitive $p$, gives a set of model distributions. This mapping is constructed from part (b) of Definition 5 wherein for each of the Berk-Nash equilibrium $m$ obtained in step 1 , the construction gives a set of $\\mu^{\\prime} s$. It is the fixed points of this map that are our equilibrium distribution given $p$. The third and final step shows that if the least and the greatest selections of the mapping $\\hat{\\theta}$ are increasing in primitives, then the associated fixed points are increasing in $p$.\n\nRemark 3. Acemoglu and Jensen (2015) gives several sufficient conditions to identify positive shocks for their environment. All of their results (Lemmas 1-3) translate to our case, ${ }^{50}$ albeit with some moderation for the endogenous dynamic programs. For example, a change in the primitive $p$ that influences the decision problem through the utility function, such as the riskaversion parameter, is a positive shock if the utility function $u(s, x, p)$ has increasing differences in $x$ and $p$. Similarly, under Assumptions 1 and 2, an increase in discount factor $\\beta$ is a positive shock.\n\nOur next theorem is on the inference of a Bayesian learner in a misspecified MDP who undergoes an expansion of their set of models in the strong-set order.\n\nTheorem 3. Suppose the hypothesis in Theorem 1 continue to hold. If a change in beliefs over models in the usual stochastic order is a positive shock, then an increase in the parameter set under the strong set order leads to an increase in the least and the greatest equilibrium best-fit models.\n\n[^0]\n[^0]:    ${ }^{49}$ We assume, without loss of generality, that the noise $\\epsilon_{t+1}$ follows a normal distribution and, therefore, satisfies regularity. Readers are referred to page 1163 in HKS for a listing of these assumptions.\n    ${ }^{50}$ The reader is referred to pp. 604-605 in Acemoglu and Jensen (2015).\n\nOur last assumption modifies the single crossing property for instances when distributions over states and actions are ordered in the increasing and convex stochastic order. Our final theorem addresses the monotonicity of Berk-Nash equilibrium for the increasing and convex order.\n\nAssumption 5. $K_{Q}(\\theta ; m)$ satisfies the single crossing property in $(\\theta ; m)$, for increasing convex order $\\gtrsim_{i c x}$ if, for $\\theta_{1} \\leqslant \\theta_{2}$, and for $m_{2} \\geq_{i c x} m_{1}, \\delta\\left(m_{1}\\right)=K_{Q}\\left(\\theta_{2}, m_{1}\\right)-K_{Q}\\left(\\theta_{1}, m_{1}\\right) \\geqslant(<) 0$ implies $\\delta\\left(m_{2}\\right)=K_{Q}\\left(\\theta_{2}, m_{2}\\right)-K_{Q}\\left(\\theta_{1}, m_{2}\\right) \\geqslant(<) 0$.\n\nSufficient Condition 2: For any two models $\\theta_{1}, \\theta_{2} \\in \\Theta$, such that $\\theta_{1}<\\theta_{2}$, define the loglikelihood ratio, $L\\left(s^{\\prime} \\mid s, x\\right)=\\ln \\left(D_{\\theta_{2}}\\left(s^{\\prime} \\mid s, x\\right)\\right)-\\ln \\left(D_{\\theta_{1}}\\left(s^{\\prime} \\mid s, x\\right)\\right)$. Then the models are said to follow the expected likelihood ratio property in increasing and convex order if the expectation of $L$ with respect to the true distribution $Q$ is increasing and convex in state and actions, $(s, x)$.\n\nTheorem 4. Suppose assumptions 1-3 and assumption 5 holds. Then a positive (negative) shock to the primitives will lead to an increase in the least and greatest Berk-Nash equilibrium in the increasing convex order if change in beliefs in the usual stochastic order sense over models are positive (negative) shocks.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 5 Analysis of Examples \n\nIn this section, we show the applicability of our results to Examples 2 and 3. Example 2 has an explicit analytical solution and is more of an illustrative example of the structure employed in this paper. However, Example 3, like most problems in dynamic programming, does not have a closed-form solution and, therefore, is most amenable to analysis with our framework.\n\nExample 2 (contd.) (Dynamic effort with unknown ability (Esponda and Pouzo (2021))). We now verify our assumptions. The state space $\\mathbb{S}=\\{0,1\\}$ and action space $\\mathbb{X}=\\{H, L\\}$ are lattices. Further, the utility function is increasing in the state variable $s_{t+1}$ and for any given cost of effort $c$, it is also supermodular in state and actions owing to its linearity. Therefore, Assumption 1 is satisfied. To check for Assumption 2, the expectation of any increasing function with respect to model transition functions should be increasing and supermodular in states and actions. This is again satisified given the structure of the transition functions since, irrespective of the state, higher action lead to success with probability 1 in the next period, and given the action, the probability of success is independent of the current state. Since the weighted KLdivergence is strictly concave, for every given $m>0$ we have a unique minimizer, and hence, it is identified and satisfies Assumption 3. Therefore, a Berk-Nash equilibrium exists for this SMDP from Theorem 1. Notice that Assumption 4 is satisfied since the mapping $\\theta_{Q}\\left(m_{s}\\right)$ is increasing\n\nin $m_{s}$. Therefore, our results in Theorem 2 are applicable for this setting. We focus on the case with unique solution and for this example, the misspecified policy function is independent of the current state. That is,\n\n$$\ng(s, \\theta, p)= \\begin{cases}H, & \\theta<1-c \\\\ L, & \\theta \\geqslant 1-c\\end{cases}\n$$\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: An increase in the cost $c$ of high effort is a negative shock for the optimal policy correspondence, which in this case is a function.\n\nWe now verify the prediction of our results vis-\u00e0-vis the analytical solution. A fall in the cost of effort $c$ is a positive shock, and therefore, leads to a higher inference of ability, $\\theta^{*}=1-c$, in the Berk-Nash equilibrium, as predicted in Theorem 2. Similarly, a fall in the cost of the effort leads to an increase in the probability of success at the steady state $m_{s}^{*}(1)$; since an increase in the model parameter is a positive shock. Here, the stationary distribution $m_{s}^{*}$ is increasing in the usual (first) stochastic order dominance in the cost of effort.\n\nExample 3 (contd.) (Savings with misperceived wealth process (EP, ADGK)). First, we verify our assumptions. The state, action and parameter spaces are lattices and the utility function is increasing in the state variables, $y$ and $z$. Since the payoff function is concave in $y$ and $x$, and $\\frac{\\mathrm{d}^{2} u(y, z, x)}{\\mathrm{d} x \\mathrm{~d} y}>0$, it is supermodular, and hence, satisfies Assumption 1. Further, the model distributions are Gaussian with mean $\\alpha+\\beta \\ln x$, and unit variance and therefore, satisfy Assumption 2. This follows from the fact that an increase in action $x$, increases the mean, and shifts the model distributions rightward, in the usual order stochastic dominant sense. Assumption 3 is met since the Gaussian distribution is strictly log-concave, and therefore, ensures unique identification. Hence, from Theorem 1, the Berk-Nash equilibrium exists. Furthermore, Assumption 4 is satisfied since this follows from a routine verification of the sufficient condition\n\n1 for increasing best-fit models, under the usual stochastic order. ${ }^{51}$\nTherefore, from Theorem 2, we have a clear prediction about the steady-state inferred model and the Berk-Nash equilibrium. For example, an increase in the discount factor, a positive shock to the primitives, will lead to a higher inferred return $\\beta^{m}$ in the equilibrium. Similarly, an increase in the expected preference shock, another instance of a positive shock, will also lead to a higher inferred return. Moreover, since increases in beliefs about the return are a positive shock, it will perpetuate into greater savings in the steady state, and a usual order stochastic increase in the stationary wealth distribution. One advantage of our comparative statics framework is that we generate predictions about the comparative statics of the equilibria even when a closedform solution may not exist for the equilibrium objects, as is true for this case. Further, since Assumption 5 holds, the consequent implications also hold for Theorem 4.", "tables": {}, "images": {"img-1.jpeg": "data:image/jpeg;base64,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"}}, {"section_id": 6, "text": "# 6 Welfare Comparisons \n\nWe now turn towards comparing the objective welfare of an agent facing a regular SMDP, under correctly specified and misspecified Bayesian learning and characterize an upper bound on the difference between the two instances, the costs of misspecification, in terms of the primitives of the environment. For the purposes of this section, we shall assume that the utility function is bounded and strictly concave. ${ }^{52}$ Further, we shall assume that $u: \\mathbb{S} \\times \\mathbb{X} \\rightarrow \\mathbb{R}$ is continuously differentiable in actions.\n\nLet $g(s, \\bar{\\theta})$, parameterized by a parameter $\\bar{\\theta}$, denote an optimal policy function. Suppose it is well-defined under both correctly specified and misspecified environments, where $\\bar{\\theta}$ take values $\\theta^{*}$ and $\\theta_{*}$ under correctly specified and misspecified learning, respectively. We are interested in comparing the welfare of a correctly specified agent with parameter $\\theta^{*}$ to the welfare of an agent who settles at the parameter $\\theta_{*}$ asymptotically, under misspecified Bayesian learning as in Equation (1). Then, the agent's welfare under the parameter $\\bar{\\theta}$, is the objective ex-ante expected discounted payoff, $W(s, \\bar{\\theta})$ of choosing the optimal action, $g(s, \\bar{\\theta})$, and is given as,\n\n$$\nW(s, \\bar{\\theta})=\\mathbb{E}_{Q(\\cdot \\mid s, g(s, \\bar{\\theta}))}\\left[\\sum_{t=0}^{\\infty} \\beta^{t} u\\left(s_{t}, g\\left(s_{t}, \\bar{\\theta}\\right)\\right)\\right], t=0,1,2, \\ldots\n$$\n\nNotice that for objective welfare, the expectation is solely with respect to the true transition function, $Q(\\cdot \\mid s, g(s, \\bar{\\theta}))$. In Equation (16), the form of environment affects welfare through the\n\n[^0]\n[^0]:    ${ }^{51}$ The expression $\\mathbb{E}_{Q(\\cdot \\mid s, x)} L\\left(s^{\\prime} \\mid s, x\\right)$ is increasing in $(s, x)$ under the Gaussian distributions, $Q$ and $Q_{\\theta}$. For $\\left(\\alpha_{1}, \\beta_{1}\\right)<\\left(\\alpha_{2}, \\beta_{2}\\right), \\mathbb{E}_{Q(\\cdot \\mid s, x)} L\\left(s^{\\prime} \\mid s, x\\right)=\\frac{1}{2 \\sigma^{2}}\\left[\\left(\\alpha_{2}-\\alpha_{1}\\right)\\left(\\beta_{2}-\\beta_{1}\\right)\\left(\\gamma^{*} z\\right) x\\right]$ which is increasing in $x$ and $z$.\n    ${ }^{52}$ Another simplification we use here is working with payoff functions that have the current state and action as arguments.\n\noptimal policy action via two channels: first, through the per-period utility function, and second, through the true transition function, $Q$. The optimal policy action $g\\left(s, \\theta_{\\star}\\right)$ under misspecified Bayesian learning can theoretically be computed in accordance with Definition 5 and the subjective distribution $Q_{\\theta_{\\star}}$. Our next theorem compares the welfare ranking between the two instances where our choice of metric is the one induced by sup-norm in the space of functions, $W: \\mathbb{S} \\times \\Theta \\rightarrow \\mathbb{R} .{ }^{53}$\n\nTheorem 5. Welfare under correctly specified learning $W\\left(s, \\theta^{\\star}\\right)$ is weakly greater than welfare under misspecified learning, $W\\left(s, \\theta_{\\star}\\right)$. Further, if $m_{0}$ and $m_{1}$ denote the absolute upper bound on the utility function and the marginal utility function, respectively, and if $\\left\\|g\\left(s, \\theta^{\\star}\\right)-g\\left(s, \\theta_{\\star}\\right)\\right\\| \\leqslant \\gamma$, then\n\n$$\n\\left\\|W\\left(s, \\theta^{\\star}\\right)-W\\left(s, \\theta_{\\star}\\right)\\right\\| \\leqslant \\frac{2 \\beta m_{0}\\left(1-e^{-k^{\\star}}\\right)+m_{1} \\gamma}{1-\\beta}\n$$\n\nwhere $k^{\\star}$ is the upper bound on the KL divergence of $Q\\left(\\cdot \\mid s, g\\left(s, \\theta_{\\star}\\right)\\right)$ with respect to $Q\\left(\\cdot \\mid s, g\\left(s, \\theta^{\\star}\\right)\\right)$.\nWe interpret the difference $\\gamma$ between the two policy functions in the sup-norm as the approximation error in the space of policy functions. Then, for a given approximation error $\\gamma$, Equation (17) supplies an upper bound on the welfare comparison in terms of the primitives, namely the discount factor, and the absolute bounds on the utility, the marginal utility function, and the KL divergence. Notice that if $\\gamma=0$, that is, the policy functions under the two instances of learning are identical, then the two welfare quantities are equal, by construction. The upper bound in Equation (17) gives intuitive comparative statics, vis-\u00e0-vis the model primitives. For instance, the discrepancy in welfare is larger the greater the approximation error, $\\gamma$. Similarly, for a given positive approximation error, the discrepancy increases in the discount factor. That is, as the agent gets increasingly patient-reflected in a higher discount factor $\\beta$-his approximation error accumulates over the horizon, leading to a greater discrepancy in welfare. Furthermore, higher bounds on the utility function and its corresponding marginal utility function lead to a greater discrepancy in the welfare. The marginal utility function plays a role in the discrepancy by attaching itself to the approximation error. Intuitively, getting the policy wrong is more pronounced if it matters more at the margins. Notice that for the upper bound on the KL divergence, we only compare the implied distributions by the true transition function $Q$ under the two policy functions. This is because our interest lies in comparing the objective welfare under $Q$.\n\nIt is worth mentioning here that bound in Equation (17) is not specific to a misspecified MDP, rather, it holds for any MDP environment with a endogenous state evolution process, when plugged with alternate policy functions. In this sense, it is related to Lemma 3.1 in Santos (2000)\n\n[^0]\n[^0]:    ${ }^{53}$ The metric induced by sup-norm is the following: $\\left\\|W\\left(s, \\theta^{\\star}\\right)-W\\left(s, \\theta_{\\star}\\right)\\right\\|=\\sup _{\\mathrm{s}}\\left|W\\left(s, \\theta^{\\star}\\right)-W\\left(s, \\theta_{\\star}\\right)\\right|$. Similarly, for the policy function $\\left\\|g\\left(s, \\theta^{\\star}\\right)-g\\left(s, \\theta_{\\star}\\right)\\right\\|=\\sup _{\\mathrm{s}}\\left|g\\left(s, \\theta^{\\star}\\right)-g\\left(s, \\theta_{\\star}\\right)\\right|$.\n\nwhich does a similar exercise in bounding welfare discrepancies. However, the state process in there is exogeneous. Furthermore, Santos (2000) uses Euler residuals to provide upper bounds on the approximation error in terms of the primitives for both the policy and the value functions, which is useful for computing numerical approximations. Often, MDP environments do not have closed form solutions, thus making economists rely on numerical techniques to solve for them computationally. We hope that Theorem 5 is instructive in this regard and helps initiate conversations on the numerical approximation of Berk-Nash equilibria.", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 7 Concluding Remarks and Extensions \n\nModels, by their very nature, offer simplified abstractions of reality, inevitably omitting certain nuances. This paper outlines conditions under which an important qualitative property, the comparative statics of decision-making and the corresponding inference, is preserved under model misspecification. The main contribution of this paper lies in establishing monotone comparative statics results for misspecified dynamic optimization problems, by a novel application of techniques in the fixed points literature (Smithson (1971)) and first used in the context of large dynamic economies (Acemoglu and Jensen (2015)). We also illustrate the utility of these results for general interest environments. Further, we provide an upper bound on the welfare comparison between correct and misspecified learning, in terms of the primitives. We conclude by outlining several promising directions for extending our findings to more broader environments.\n\nMulti-dimensional parameter spaces. Since most of the current applications in the literature predominantly involve one-dimensional models, ${ }^{54}$ the results presented here focus on the model (parameter) set as a compact subset of the real line. However, they could be extended to multi-dimensional parameter spaces. This would involve a suitable rehabilitation of Assumptions 3 and 4 for lattice and non-lattice multi-dimensional parameter spaces. Although one limitation of these results using the current techniques is that the comparative statics results imply all inferred parameters in equilibrium must be monotonic together, which may not be ideal for specific economic applications.\n\nOther forms of updating. While our results primarily address the asymptotic behavior of Bayesian learners, it's important to note that other inference processes, such as Maximum Likelihood Estimation (MLE) and moment-based learning, can lead to the same asymptotic inferences of best-fit models. Csaba and Szoke (2023) provide an illustrative example that demonstrates the identical nature of inference between Bayesian inference and methods based on likelihoods.\n\nStatic environments and dynamic concerns. Our results are tailored for dynamic MDP\n\n[^0]\n[^0]:    ${ }^{54}$ This feature of the current literature is also noted in Esponda, Pouzo, and Yamamoto (2021).\n\nenvironments but they potentially are also applicable, subject to modifications, to static environments, such as the one in Esponda and Pouzo (2016) for static games. This also spills over to settings where agents are endogenously concerned about their misspecified models, such as those in Lanzani (2025). ${ }^{55}$\nMonotone comparative dynamics. The scope of our paper is limited to analyzing the comparative statics properties of the equilibrium objects in the steady state. An interesting question arises regarding how the dynamics of the misspecified learning process might react to changes in its primitives. Specifically, we do not know how the variations in the frequency of state-action pairs, as well as the sequence of posteriors as described in Equation (1), would be influenced because of changes in the economic primitives. There is added complexity in the dynamics due to potential complementarities between current actions and the inference process via the role of experimentation. In this regard, the tools developed in Balbus, Dziewulski, Reffett, and Wo\u017any (2022) for monotone comparative dynamics for stochastic games could be useful for further exploration.\n\nMisspecification in large economies. Our results are also potentially applicable to misspecified dynamic economies with a continuum of agents, such as the one conceived in Molavi (2019). Given that the techniques we rely on (Acemoglu and Jensen (2015)) were developed in the context of economies with a continuum of agents, the results in this paper should potentially be applicable on equilibrium concepts that Molavi (2019) develops for the boundedly rational agents working with misspecified models in macroeconomic environments.\n\n[^0]\n[^0]:    ${ }^{55}$ Corollary 1 in Lanzani (2025) presents an interesting comparative statics application to monetary policy cycles; see it for more details.", "tables": {}, "images": {}}, {"section_id": 8, "text": "# A Mathematical Preliminaries \n\nIn this section, we provide the necessary mathematical preliminaries required to go through the proofs. Let $X$ be a set. A subset $\\gtrsim$ of $X \\times X$ denotes a binary relation on $X$. A binary relation $\\gtrsim$ is a partial order if it is reflexive, transitive, and anti-symmetric. A partially ordered set, or a poset, is a pair $(X, \\gtrsim)$ that consists of a set $X$ and a partial order $\\gtrsim$. A binary relation $\\gtrsim$ is closed if the graph of $\\gtrsim$ is a closed subset of $X \\times X$. The poset $(X, \\gtrsim)$ is a lattice if for any $x, x^{\\prime} \\in X$, the greatest lower bound (infimum) $x \\wedge x^{\\prime}$ and the least upper bound (supremum) $x \\vee x^{\\prime}$ are in $X$, where $\\wedge$ and $\\vee$ denote the meet and join operations, respectively. A subset $A \\subseteq X$ is a sublattice of lattice $X$ if $A$ is a lattice that contains the meet and join (computed in $X$ ) for every pair of elements in $A$. For any subset $A$ of a poset $X$, we denote the supremum and infimum of $A$ by $\\sup A$ and $\\inf A$, respectively. That is, $\\sup A$ is the least element in $X$ such that $\\sup A \\gtrsim a$, for all $a \\in A$. Similarly, $\\inf A$ is the greatest element in $X$ such that $a \\gtrsim \\inf A$, for all $a \\in A$. A lattice $X$ is complete if both $\\inf A$ and $\\sup A$ are in $X$ for any $A \\subseteq X$. A chain is a totally ordered poset. A poset $X$ is (countably) lower chain complete if any (countable) chain $A \\subseteq X$ has its infimum in $X$. The poset is (countably) upper chain complete if any such chain has its supremum in $X$. The poset is (countably) chain complete if it is both upper and lower (countably) chain complete. Let $X$ and $Y$ be subsets of $\\mathbb{R}$. Set $Y$ dominates $X$ in the strong set order if for any $x$ in $X$ and $y$ in $Y$, we have $\\max \\{x, y\\}$ in $Y$ and $\\min \\{x, y\\}$ in $X$.\n\nLet $\\mathcal{M}(X)$ denote the space of probability measures defined on a compact subset of $X \\subset \\mathbb{R}^{n}$. Although even if $X$ is a lattice, the poset $\\left(X, \\gtrsim_{s t}\\right)$ is not a lattice as pointed in Kamae, Krengel, and O'Brien (1977). For e.g., for $\\left(\\mathbb{R}^{2}, \\gtrsim_{s t}\\right)$ let $p_{1}=0.5\\left(\\epsilon_{a}+\\epsilon_{b}\\right), p_{2}=$ $0.5\\left(\\epsilon_{a}+\\epsilon_{c}\\right), p_{3}=0.5\\left(\\epsilon_{c}+\\epsilon_{b}\\right), p_{4}=0.5\\left(\\epsilon_{a}+\\epsilon_{d}\\right)$, where $a=(0,0), b=(0,1), c=(1,0), d=(1,1)$. Then, both $p_{3}$ and $p_{4}$ are supremum, which is a contradiction. However, $\\left(\\mathbb{R}^{2}, \\gtrsim_{s t}\\right)$ is chaincomplete. ${ }^{56}$ A correspondence $T: X \\rightarrow 2^{Y}$ is upper-hemicontinuous at a point $x_{0} \\in X$ if for any sequence $\\left\\{x_{n}\\right\\}_{n \\in \\mathbb{N}}$ such that $\\left\\{x_{n}\\right\\} \\rightarrow x_{0}, y_{n} \\in T\\left(x_{n}\\right), y_{0} \\in T\\left(x_{0}\\right)$ implies $y_{n} \\rightarrow y_{0}$. We will require the following existence theorem in Smithson (1971) for chain-complete (non-lattice) spaces, and refer the interested reader to his paper for further details.\n\nTheorem (Smithson's fixed point theorem (1971)). Let $X$ be a chain-complete poset equipped with partial order $\\gtrsim$, and $T: X \\rightarrow 2^{X}$ a Type I (Type II) monotone correspondence. Suppose for any chain $C$ in $X$ and any monotone selection $f$ from the restriction of $T$ to $C, f: C \\rightarrow X$, there exists $y_{0} \\in T(\\sup C)$ such that $y_{0} \\gtrsim f(x)$ for all $x \\in C$. Then if there exists a point $e \\in X$ and a point $y \\in T(e)$ such that $y \\gtrsim e$, then $T$ has a fixed point.\n\n[^0]\n[^0]:    ${ }^{56}$ This also holds true for increasing and convex order, $\\gtrsim_{i c x}$.", "tables": {}, "images": {}}, {"section_id": 9, "text": "# B Proofs \n\nThis section is divided into four parts. Part (i) proves two auxiliary lemmas that are instrumental for our main results. Part (ii) proves Theorems 1 and 2 of the paper and part (iii) proves Theorems 3 and 4, respectively. Part (iv) does the welfare comparison.\n\nThe proofs for Theorems 1 and 2 are structured in three steps, with the proof technique similar to that in Acemoglu and Jensen (2015). For Theorem 2, the first step involves showing that for any fixed model distribution $\\mu \\in \\mathcal{M}_{1}(\\Theta)$, the set of stationary distributions on states and actions, $m^{*}$, induced by the optimal policy correspondence $G$, will be Type I (Type II) monotonic in the primitives $p$. The second step involves constructing a mapping $\\hat{\\theta}$ that for each given $\\mu$ and $p$ yields a set of model distributions, $\\mu^{\\prime} s$. It is the fixed points of this map that are the equilibrium model distributions $\\mu^{*}$, given $p$. Finally, the third step involves the least and greatest selections from this map will be increasing in $p$. This in turn leads us to give a new existence proof of Theorem 1 that relies on the monotonicity and identification properties of the equilibrium map, $T$. The rest of the proofs (Theorems 3 and 4) shall follow an analogous structure. Proof of Theorem 5 is based on the Taylor expansion of the utility function and on an application of a new result proven by Canonne (2022) on entropy bounds.\n\nThe equilibrium mapping $T$ associated with the Berk-Nash equilibrium (Definition 5) is a set-valued function on the product space of probability measure on states and actions, and parameter space, $T: W \\rightarrow 2^{W}$, where $W=\\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X}) \\times \\mathcal{M}_{1}(\\Theta)$ such that $\\mathcal{T}(m, \\mu)=$ $\\mathcal{M}(m, \\mu) \\times \\mathcal{M}\\left(\\Theta_{Q}(m)\\right)$, where\n\n$$\n(m, \\mu) \\mapsto \\mathcal{M}(m, \\mu) \\equiv\\left\\{m^{\\prime} \\in \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X}): m^{\\prime} \\in \\mathcal{F}(\\mu) \\& m_{\\mathbb{S}}^{\\prime}(\\cdot)=\\int_{\\mathbb{S} \\times \\mathbb{X}} Q(\\cdot \\mid s, x) m(\\mathrm{~d} s, \\mathrm{~d} x)\\right\\}\n$$\n\nfor any $\\mu \\in \\mathcal{M}_{1}(\\Theta), \\mathcal{F}(\\mu)$ is the set of all $m^{\\prime}$ that satisfy the conditions of optimality and stationarity as defined in Definition 5.", "tables": {}, "images": {}}, {"section_id": 10, "text": "## (i) Auxiliary Lemmas\n\nTowards this end, we begin by proving two lemmas. They adapt lemmas in Acemoglu and Jensen (2015) that are established for exogenous shock processes to our setting of endogenous Markov decision process. Lemma 1 shows that under assumptions 1 and 2, the optimal policy correspondence, defined in Eq (8), $G: \\mathbb{S} \\times P \\rightarrow 2^{X}$ is increasing in the strong-set order and the least and the greatest selection are increasing in $s$.\n\nLemma 1. Let the regular SMDP satisfy assumptions 1 \\& 2. Then for any given primitive $p$, the optimal policy correspondence $G: \\mathbb{S} \\times P \\rightarrow 2^{\\mathbb{X}}$ is increasing in the state $s$ in the strong set order. In particular, it has a least and a greatest selection, and they are increasing in state $s .{ }^{57}$\n\n[^0]\n[^0]:    ${ }^{57}$ We suppress the dependence on $\\mathcal{M}_{1}(\\Theta)$ for notational convenience. Theorem 3.9.2 in Topkis (1998) estab-\n\nProof. Given a model distribution $\\mu$, and corresponding to a Bellman solution $V$ of Eq. (5),\n\n$$\nG(s, \\mu, p)=\\underset{x \\in \\mathbb{X}}{\\operatorname{argmax}} \\int_{S}\\left\\{u\\left(s, x, s^{\\prime}\\right)+\\delta V\\left(s^{\\prime}, \\mu, p\\right)\\right\\} \\bar{Q}_{\\mu}\\left(\\mathrm{d} s^{\\prime} \\mid s, x\\right)\n$$\n\nwhere $\\bar{Q}_{\\mu}=\\int_{\\Theta} Q_{\\theta} \\mu(d \\theta)$. Given Assumptions 1 and 2, and by the application of Theorem 3.9.2 (page 165, Topkis (1998)), the expression on the right-hand side is supermodular in $(s, x)$. Therefore, the optimal policy correspondence will be increasing in $s$ in the strong-set order. Further, given the optimal correspondence, the greatest and least selection exist and are increasing in the state.\n\nThe next lemma transfers the monotonic nature of the optimal policy correspondence to its corresponding fixed point map, $T$, in terms of Type I (Type II) monotonicity.\n\nLemma 2. If the optimal policy correspondence $G: \\mathbb{S} \\rightarrow 2^{\\mathbb{X}}$ has an increasing greatest (least) selection, then the fixed point correspondence $T$ is Type I (Type II) monotone with respect to $\\gtrsim_{s t}$. Further, if $G$ depends on a primitive $p \\in P$ such that $G: \\mathbb{S} \\times P \\rightarrow 2^{\\mathbb{X}}$ and the greatest (least) selection from $G$ is increasing in $p$, then the fixed point correspondence $T$ indexed by $p$, $T_{p}$ is Type I (Type II) monotone in $\\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})$ with respect to $\\gtrsim_{s t}$.\n\nProof. We only prove the above statement for the greatest selection in the Type I case. ${ }^{58}$ Consider probability measures, $\\nu_{1}, \\nu_{2} \\in \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})$ such that $\\nu_{2} \\gtrsim_{s t} \\nu_{1}$. To prove that the fixed point correspondence $T$ is Type I monotone, we need to show that for any $\\lambda_{1} \\in T \\nu_{1}$, there exists a $\\lambda_{2} \\in T \\nu_{2}$ such that $\\lambda_{2} \\gtrsim_{s t} \\lambda_{1}$. That is, if $\\lambda_{1} \\in T \\nu_{1}$, then there exists a measurable selection $g_{1}: \\mathbb{S} \\rightarrow \\mathbb{X}$, such that, for all $A \\times B \\in \\mathcal{B}(\\mathbb{S} \\times \\mathbb{X})$,\n\n$$\n\\lambda_{1}(A, B)=\\int_{\\mathbb{S} \\times \\mathbb{X}} Q\\left(A \\mid s, g_{1}(s)\\right) \\chi_{B}\\left(g_{1}(s)\\right) \\nu_{1}(\\mathrm{~d} s, \\mathrm{~d} x)\n$$\n\nand therefore, there must exist a selection $g_{2}$ such that $\\lambda_{2} \\in T \\nu_{2}$, and $\\lambda_{2} \\gtrsim_{s t} \\lambda_{1}$. From Lemma 1 , we know that such a greatest selection exists, and therefore, for all $A \\times B \\in \\mathcal{B}(\\mathbb{S} \\times \\mathbb{X})$,\n\n$$\n\\begin{aligned}\n\\lambda_{2} \\gtrsim_{s t} \\lambda_{1} \\Longleftrightarrow \\int_{\\mathbb{S}} f\\left(s, g_{2}(s)\\right) Q\\left(\\mathrm{~d} s \\mid s, g_{2}(s)\\right) \\nu_{2}(s) & \\geqslant \\int_{\\mathbb{S}} f\\left(s, g_{2}(s)\\right) Q\\left(\\mathrm{~d} s \\mid s, g_{2}(s)\\right) \\nu_{1}(s) \\\\\n& \\geqslant \\int_{\\mathbb{S}} f\\left(s, g_{1}(s)\\right) Q\\left(\\mathrm{~d} s \\mid s, g_{1}(s)\\right) \\nu_{1}(s)\n\\end{aligned}\n$$\n\nThe first inequality follows from $\\nu_{2} \\gtrsim_{s t} \\nu_{1}$. The second inequality follows from $Q$ being monotone and $g_{2}$ being the greatest selection. With an identical argument, one can prove for\n\n[^0]\n[^0]:    lishes conditions under which finite and infinite period discounted MDPs have increasing policy selection.\n    ${ }^{58}$ The rest of the cases involving least selection and Type II monotonicity follow analogously.\n\nthe case where the greatest selection from $G$ is to be increasing in primitive $p$, and therefore, $T$ indexed by $p, T_{p}$ is Type I (Type II) monotone in $\\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})$, with respect to $\\gtrsim_{s t}$.\n\nWe are now ready to prove Theorems 1 and 2. Throughout the proof, and without loss of generality, primitive $p$ is restricted to the set $P \\equiv\\left\\{p_{1}, p_{2}\\right\\}$ ordered by $p_{2} \\gtrsim p_{1}$, where the ordering depends on the primitive being considered.\n(ii) Proofs for Theorems 1 and 2: The first step involves showing for a given model distribution $\\mu \\in \\mathcal{M}_{1}(\\Theta)$, the set of Berk-Nash equilibriua $m$ induced by the optimal policy correspondence $G$ will be Type I (Type II) increasing in the primitives $p \\in P$. To this end, we first show that under a positive shock, the fixed point correspondence $T$ is Type I (Type II) increasing in the primitives $p$. The fixed points of this correspondence are the Berk-Nash equilibrium $m$ for a given model distribution $\\mu$, and finally using a result in Acemoglu and Jensen (2015), we show that the set of Berk-Nash equilibriua for a given model distribution is Type I (Type II) increasing in the primitives.\n\nBecause of Lemma 1, the stationary optimal policy correspondence $G$ will have a least and a greatest selection that will be increasing in $s$ and therefore, by Lemma 2, for a given model distribution $\\mu$, and primitive $p, T_{\\mu, p}: \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X}) \\rightarrow 2^{\\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})}$ induced by the stationary optimal policy correspondence, $G$, defined in (8) is Type I (Type II) monotone with respect to $\\gtrsim_{s t}$. Now from a routine modification of Theorem B3 in Acemoglu and Jensen (2015), ${ }^{59}$ the set of fixed points, $F: \\mu \\times P \\rightarrow 2^{\\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X})}$, given by $F(\\mu, p)=\\left\\{m \\in \\mathcal{M}_{1}(\\mathbb{S} \\times \\mathbb{X}): m \\in T_{\\mu, p} m\\right\\}$ is non-empty valued and upper hemicontinuous. The proof of upper-hemicontinuity of $T_{\\mu, p}$ follows for the finite case follows from Claim B (page 744) in EP, while for the infinite case, specifically compact Euclidean spaces here, it follows by invoking the conditions in Definition 3 on a regular SMDP, as proved in Anderson, Duanmu, Ghosh, and Khan (2024). From Lemma 2, $T_{\\mu, p}$ is Type I (Type II) monotone in $p$, and therefore, from the monotonicity theorem of Acemoglu and Jensen (2015) in the main text (page 15), the set of fixed points $F$ will be nonempty and Type I (Type II) monotone in $p$. Hence, the set of Berk-Nash equilibriua $m$ induced by the optimal policy correspondence $G$ will be Type I (Type II) increasing in the primitives $p$. This completes the first step.\n\nThe second step constructs a mapping $\\hat{\\theta}: \\mathcal{M}_{1}(\\Theta) \\times P \\rightarrow 2^{\\mathcal{M}_{1}(\\Theta)}$ that for each given model distribution $\\mu$ and primitive $p \\in P$, yields a set of best-fit model distributions,\n\n$$\n\\hat{\\theta}(\\mu, p)=\\left\\{\\theta(m) \\equiv \\operatorname{argmin}_{\\theta \\in \\Theta} K_{Q}(m, \\theta): m \\in F(\\mu, p)\\right\\}\n$$\n\n[^0]\n[^0]:    ${ }^{59}$ Theorem B3 in Acemoglu and Jensen (2015) builts on the existence theorem in Smithson (1971). It states the following: Assume that the equilibrium mapping $T$ is either Type I or type II monotone. In addition, assume that the set of measures on state and actions has an infimum. Then $T$ has fixed point. In addition, the fixed-point correspondence is upper hemicontinuous if $T$ is upper hemicontinuous.\n\nA model distribution $\\mu^{*}$ is an equilibrium belief if and only if $\\mu^{*} \\in \\hat{\\theta}\\left(\\mu^{*}, p\\right)$. By assumption 3 of point identification and therefore, by uniqueness, for each $m$, there will be a non-empty unique $\\theta(m)$. That is, one has a Dirac measure on $\\theta(m)$. From Berge's Maximum Theorem, $\\theta$ is continuous ${ }^{60}$ and given the upper hemicontinuity of $F(\\mu, p), \\hat{\\theta}$ will be upper hemi-continuous. The rest of this step follows Acemoglu and Jensen (2015). For a fixed $\\mu$, and given $F(\\mu, \\cdot)$ is Type I and Type II monotone in $p$, and increasing $\\theta(m)$ under Assumption 4, one can use their Theorem 4 (pp. 601) to conclude that the least and greatest selections from $\\hat{\\theta}(\\mu, \\cdot)$ will be increasing in $p$ holding $\\mu$ fixed. ${ }^{61}$\n\nThe third and final step shows that if the least and the greatest selections of the upperhemicontinuous fixed point correspondence $\\hat{\\theta}$ are increasing in $p$, then the fixed points are increasing in $p$. Since under Assumption 4, $\\theta$ is a monotonic function of $m$, therefore, $\\mu_{\\min } \\equiv \\theta\\left(\\delta_{\\inf \\mathbb{S} \\times \\mathbb{X}}\\right)$ and $\\mu_{\\max } \\equiv \\theta\\left(\\delta_{\\sup \\mathbb{S} \\times \\mathbb{X}}\\right)$, where $\\delta_{\\mathbb{S} \\times \\mathbb{X}}$ denotes the degenerate measure on $\\mathbb{S} \\times \\mathbb{X}$ with its mass at $(s, x)$. Therefore, $\\mu \\gtrsim \\mu_{\\min }$ for all $\\mu \\in \\hat{\\theta}\\left(\\mu_{\\min }\\right)$ and $\\mu \\preceq \\mu_{\\max }$ for all $\\mu \\in \\hat{\\theta}\\left(\\mu_{\\max }\\right)$. Therefore, for every $p \\in P, \\hat{\\theta}(\\cdot, p):\\left[\\mu_{\\min }, \\mu_{\\max }\\right] \\rightarrow 2^{\\left[\\mu_{\\min }, \\mu_{\\max }\\right]}$. Notice in step 2 that $F(\\mu, p)$ is a convex-valued set of fixed points since $T_{\\mu, p}$ is convex-valued; the proof of convex-valuedness follows from EP. Therefore, the set of fixed points from $F$ is convex-valued and given $\\theta(m)$ is a continuous function, $\\hat{\\theta}(\\mu, p)$ is convex-valued. Hence, from Acemoglu and Jensen (2013), $\\hat{\\theta}(\\cdot, p):\\left[\\mu_{\\min }, \\mu_{\\max }\\right] \\rightarrow 2^{\\left[\\mu_{\\min }, \\mu_{\\max }\\right]}$ is upper hemicontinuous and convex valued and for each fixed value of $\\mu \\in\\left[\\mu_{\\min }, \\mu_{\\max }\\right]$, has least and greatest selections and are increasing in $p$ and from Corollary 2 in Milgrom and Roberts (1994) the least and greatest fixed points $\\mu^{*} \\in \\hat{\\theta}\\left(\\mu^{*}, p\\right)$ will be increasing in $p$. Hence, we have proven that the least and greatest inferred models are increasing in the primitives under a positive shock to a regular SMDP. Furthermore, From Theorem 2.8.3 in Topkis (1998), and by treating a change in the beliefs $\\mu$ in the usual stochastic order sense as a change in primitive as above, the remaining part of Theorem 2 follows.\n\nExistence is yielded in Step 2 by the Kakutani-Fan-Glicksberg Theorem since our map $\\hat{\\theta}(\\mu, p)$ is convex-valued, upper hemi-continuous, and as is shown in EP, $\\mathcal{M}_{1}(\\Theta)$ are locally convex Hausdorff spaces.\n(iii) Proof for Theorem 3: The parameter space $\\Theta \\subseteq \\mathbb{R}$ is one-dimensional, and hence, the weighted KL divergence satisfies quasimodularity by triviality on the parameter space, $\\Theta \\subseteq \\mathbb{R}$.\n\n[^0]\n[^0]:    ${ }^{60}$ The function $\\theta_{Q}(m)$ is continuous if it is continuous in the weak+topology on its domain.\n    ${ }^{61}$ Theorem 4 in Acemoglu and Jensen (2015) guarantees least and greatest selections from a fixed point correspondence. It may be possible to dispense with identification (Assumption 3) if we were to use more general divergence measures apart from the Kullback-Leibler divergence. However, given the criticality of the KL divergence, uniqueness is a feature we require. It is also something that the current literature has emphasized in terms of applications; see Esponda and Pouzo (2021); Esponda, Pouzo, and Yamamoto (2021).\n\nHence, from Topkis (1978) and Milgrom and Shannon (1994), given an increase in the parameter space from $\\Theta_{1}$ to $\\Theta_{2}$ in the strong-set order, the set of minimizers are increasing in the strong-set order, i.e.,\n\n$$\n\\Theta\\left(m ; \\Theta_{1}\\right) \\equiv \\underset{\\theta \\in \\Theta_{1}}{\\operatorname{argmin}} K_{Q}(m, \\theta) \\subseteq \\underset{\\theta \\in \\Theta_{2}}{\\operatorname{argmin}} K_{Q}(m, \\theta) \\equiv \\Theta\\left(m ; \\Theta_{2}\\right)\n$$\n\nTherefore, if a change in model distribution is a positive shock then the proof follows on the lines of Theorem 2. Thus, this completes the proof.\n\nProof for Theorem 4: Consider probability measures, $\\nu_{1}, \\nu_{2} \\in \\mathcal{M}(\\mathbb{S} \\times \\mathbb{X})$ such that $\\nu_{2} \\gtrsim_{i c x} \\nu_{1}$. The proof of Type I and Type II monotonicity is similar to that of Theorems 1 and 2, albeit requires that we work with increasing convex orders on the set of states and actions.\n\nTo prove that the fixed point correspondence $T$ is Type I monotone in $\\gtrsim_{i c x}$, we need to show that for any $\\lambda_{1} \\in T \\nu_{1}$, there exists a $\\lambda_{2} \\in T \\nu_{2}$ such that $\\lambda_{2} \\gtrsim_{i c x} \\lambda_{1}$, that is, if $\\lambda_{1} \\in T \\nu_{1}$, then there exists a measurable selection $g_{1}: \\mathbb{S} \\rightarrow \\mathbb{X}$, such that, for all $A \\times B \\in \\mathcal{B}(\\mathbb{S} \\times \\mathbb{X})$,\n\n$$\n\\lambda_{1}(A, B)=\\int_{\\mathbb{S} \\times \\mathbb{X}} Q\\left(A \\mid s, g_{1}(s)\\right) \\chi_{B}\\left(g_{1}(s)\\right) \\nu_{1}(\\mathrm{~d} s, \\mathrm{~d} x)\n$$\n\nand therefore, there must exist a selection $g_{2}$ such that $\\lambda_{2} \\in T \\nu_{2}$, and $\\lambda_{2} \\gtrsim_{i c x} \\lambda_{1}$. From Lemma 1 , we know that such a greatest selection exists, and therefore, for all $A \\times B \\in \\mathcal{B}(\\mathbb{S} \\times \\mathbb{X})$,\n\n$$\n\\begin{aligned}\n\\lambda_{2} \\gtrsim_{i c x} \\lambda_{1} \\Longleftrightarrow \\int_{\\mathbb{S}} f\\left(s, g_{2}(s)\\right) Q\\left(\\mathrm{~d} s \\mid s, g_{2}(s)\\right) \\nu_{2}(s) & \\geqslant \\int_{\\mathbb{S}} f\\left(s, g_{2}(s)\\right) Q\\left(\\mathrm{~d} s \\mid s, g_{2}(s)\\right) \\nu_{1}(s) \\\\\n& \\geqslant \\int_{\\mathbb{S}} f\\left(s, g_{1}(s)\\right) Q\\left(\\mathrm{~d} s \\mid s, g_{1}(s)\\right) \\nu_{1}(s)\n\\end{aligned}\n$$\n\nThe first inequality follows from $\\nu_{2} \\gtrsim_{i c x} \\nu_{1}$. The second inequality follows from $Q$ being monotone and $g_{2}$ being the greatest selection. The only adjustment in the proof that remains is that the best-fit set construction needs to incorporate Assumption 5 for it to have them monotonic, i.e., the second step constructs a mapping $\\hat{\\theta}$ that for each given model distribution $\\mu$ and primitive $p \\in P$, yields a set of model distributions,\n\n$$\n\\hat{\\theta}(\\mu, p)=\\left\\{\\theta(m) \\equiv \\underset{\\theta \\in \\Theta}{\\operatorname{argmin}} K_{Q}(m, \\theta): m \\in F(\\mu, p)\\right\\}\n$$\n\nHence, this completes the proof.\n(iv) Proof for Theorem 5: The first part follows from the optimality of $g\\left(s, \\theta^{*}\\right)$ over $g\\left(s, \\theta_{*}\\right)$, under the objective welfare function $W\\left(s, \\theta^{*}\\right)$, where the expectation is taken with respect to the true transition function, $Q$. Next, under correct learning parameterized by $\\theta^{*}$, and for an\n\ninitial state $s_{0}$, the objective welfare is given as,\n\n$$\nW\\left(s_{0}, \\theta^{*}\\right)=\\mathbb{E}_{Q\\left(\\cdot \\mid s_{t-1}, g\\left(s_{t-1}, \\theta^{*}\\right)\\right)}\\left[\\sum_{t=0}^{\\infty} \\beta^{t} u\\left(s_{t}, g\\left(s_{t}, \\theta^{*}\\right)\\right)\\right], t=0,1,2, \\ldots\n$$\n\nUnwrapping the above expression,\n\n$$\n\\begin{gathered}\nW\\left(s_{0}, \\theta^{*}\\right)=u\\left(s_{0}, g\\left(s_{0}, \\theta^{*}\\right)\\right)+\\beta \\int_{S} u\\left(s_{1}, g\\left(s_{1}, \\theta^{*}\\right)\\right) Q\\left(\\mathrm{~d} s_{1} \\mid s_{0}, g\\left(s_{0}, \\theta^{*}\\right)\\right)+\\ldots \\\\\n\\beta^{t} \\int_{S} u\\left(s_{t}, g\\left(s_{t}, \\theta^{*}\\right)\\right) Q\\left(\\mathrm{~d} s_{t} \\mid s_{t-1}, g\\left(s_{t-1}, \\theta^{*}\\right)\\right)+\\ldots\n\\end{gathered}\n$$\n\nNow, with the Berk-Nash parameter $\\theta_{*}$, the corresponding objective welfare under misspecified learning is,\n\n$$\n\\begin{gathered}\nW\\left(s_{0}, \\theta_{*}\\right)=u\\left(s_{0}, g\\left(s_{0}, \\theta_{*}\\right)\\right)+\\beta \\int_{S} u\\left(s_{1}, g\\left(s_{1}, \\theta_{*}\\right)\\right) Q\\left(\\mathrm{~d} s_{1} \\mid s_{0}, g\\left(s_{0}, \\theta_{*}\\right)\\right)+\\ldots \\\\\n\\beta^{t} \\int_{S} u\\left(s_{t}, g\\left(s_{t}, \\theta_{*}\\right)\\right) Q\\left(\\mathrm{~d} s_{t} \\mid s_{t-1}, g\\left(s_{t-1}, \\theta_{*}\\right)\\right)+\\ldots\n\\end{gathered}\n$$\n\nOur objective is to find an upper bound for $\\left\\|W\\left(s_{0}, \\theta^{*}\\right)-W\\left(s_{0}, \\theta_{*}\\right)\\right\\|$, where denotes the sup-norm in the function space. Comparing the first terms in Equations (23) and (24) and by a Taylor expansion of $u\\left(s_{0}, g\\left(s_{0}, \\theta^{*}\\right)\\right)$ on $u\\left(s_{0}, g\\left(s_{0}, \\theta_{*}\\right)\\right)$, given that we assume that it is continuously differentiable and strictly concave, we have,\n\n$$\n\\left\\|u\\left(s_{0}, g\\left(s_{0}, \\theta^{*}\\right)\\right)-u\\left(s_{0}, g\\left(s_{0}, \\theta_{*}\\right)\\right)\\right\\| \\leqslant\\left|\\frac{\\mathrm{d} u}{\\mathrm{~d} g}\\right|_{g\\left(s_{0}, \\theta_{*}\\right)}\\left\\|g\\left(s_{0}, \\theta^{*}\\right)-g\\left(s_{0}, \\theta_{*}\\right)\\right\\|\n$$\n\nThe inequality follows from the concavity of $u$ in policy function $g$ and from taking the sup-norm on both sides. Similarly, for the second term, we have,\n\n$$\n\\begin{aligned}\n& \\beta \\|\\left(\\int_{S} u\\left(s_{1}, g\\left(s_{1}, \\theta^{*}\\right)\\right) Q\\left(\\mathrm{~d} s_{1} \\mid s_{0}, g\\left(s_{0}, \\theta^{*}\\right)\\right)-\\int_{S} u\\left(s_{1}, g\\left(s_{1}, \\theta_{*}\\right)\\right) Q\\left(\\mathrm{~d} s_{1} \\mid s_{0}, g\\left(s_{0}, \\theta_{*}\\right)\\right)\\right) \\| \\\\\n& \\leqslant \\beta\\left(\\int_{S} \\mid u\\left(s_{1}, g\\left(s_{1}, \\theta_{*}\\right)\\right)\\left|\\cdot\\left\\|Q\\left(\\mathrm{~d} s_{1} \\mid s_{0}, g\\left(s_{0}, \\theta^{*}\\right)\\right)-Q\\left(\\mathrm{~d} s_{1} \\mid s_{0}, g\\left(s_{0}, \\theta_{*}\\right)\\right)\\right|+\\left|\\frac{\\mathrm{d} u}{\\mathrm{~d} g}\\right|_{g\\left(s_{1}, \\theta_{*}\\right)}\\left\\|g\\left(s_{1}, \\theta^{*}\\right)-g\\left(s_{1}, \\theta_{*}\\right)\\right\\|\\right)\n\\end{aligned}\n$$\n\nThis again follows from the strict concavity of $u$, a corresponding Taylor-expansion of the utility function $u$, and the fact that the integral of the density function (always positive) is 1 .\n\nWhile we have corresponding upper bounds for the utility function, its first derivative, and the difference in the policy functions under the two learning instances, we still need to establish a corresponding upper bound for the total-variation (TV) norm for densities $Q$. Towards this end, we rely on a lemma by Canonne (2022), that provides an almost stricter bound on the\n\nTV-norm between densities. ${ }^{62}$\nTheorem (Bretagnolle-Huber bound, Canonne (2022)). For any two probability distribution functions $Q_{1}, Q_{2}$ over $\\mathbb{R}$,\n\n$$\nd_{T V}\\left(Q_{1}, Q_{2}\\right)=\\left\\|Q_{1}-Q_{2}\\right\\|_{1} \\leqslant 2 \\sqrt{1-e^{-K L\\left(Q_{1} \\| Q_{2}\\right)}}\n$$\n\nwhere $d_{T V}$ is the total variation norm between densities $Q_{1}$ and $Q_{2}$, and $K L$ is the relative entropy between $Q_{1}$ and $Q_{2}$.\n\nTherefore, the first part of the right-hand side expression in Eq.(23) can be bounded as,\n\n$$\n\\beta\\left(\\int_{S}\\left|u\\left(s_{1}, g\\left(s_{1}, \\theta_{*}\\right)\\right)\\right| \\cdot\\left\\|\\left|Q\\left(\\mathrm{~d} s_{1} \\mid s_{0}, g\\left(s_{0}, \\theta^{*}\\right)\\right)-Q\\left(\\mathrm{~d} s_{1} \\mid s_{0}, g\\left(s_{0}, \\theta_{*}\\right)\\right)\\right\\|\\right) \\leqslant 2 \\beta m_{0} \\sqrt{1-e^{-K L(\\gamma, Q)}}\n$$\n\nwhere the KL distance, $K L_{1}(\\gamma, Q)$, depends on the approximation error $\\gamma$, and the true transition function, $Q$, and also the particular state realization. For the second part, we have,\n\n$$\n\\left|\\frac{\\mathrm{d} u}{\\mathrm{~d} g}\\right|_{g\\left(s_{1}, \\theta_{*}\\right)}\\left\\|g\\left(s_{1}, \\theta^{*}\\right)-g\\left(s_{1}, \\theta_{*}\\right)\\right\\| \\leqslant m_{1} \\gamma\n$$\n\nBy repeating the above step for all the terms in $W\\left(s, \\theta^{*}\\right)$ and $W\\left(s, \\theta_{*}\\right)$, we have,\n\n$$\n\\begin{aligned}\n\\left\\|W\\left(s, \\theta^{*}\\right)-W\\left(s, \\theta_{*}\\right)\\right\\| & \\leqslant m_{1} \\gamma+\\beta\\left(m_{1} \\gamma+2 m_{0} \\sqrt{1-e^{-K L_{1}(\\gamma, Q)}}\\right)+\\beta^{2}\\left(m_{1} \\gamma+2 m_{0} \\sqrt{1-e^{-K L_{2}(\\gamma, Q)}}\\right)+\\ldots \\\\\n& \\leqslant \\frac{m_{1} \\gamma+2 m_{0} \\beta \\sqrt{1-e^{-K L_{*}(\\gamma, Q)}}}{1-\\beta}\n\\end{aligned}\n$$\n\nwhere $K L_{*}(\\gamma, Q)$ is the upper bound on the sequence of KL distances, $\\left\\{K L_{1}, K L_{2}, \\ldots, K L_{n} \\ldots\\right\\}$. Hence, we have established our upper bound between the two welfare quantities, and this completes the proof.\n\n[^0]\n[^0]:    ${ }^{62}$ Our choice of this particular bound over the well-known Pinsker bound is that as the KL divergence gets larger, the Pinsker bound becomes vacuous and exceeds the trivial bound of 1 . The BH bound, however, never exceeds 1 ; see Canonne (2022) for further details.", "tables": {}, "images": {}}, {"section_id": 11, "text": "# References \n\nAcemoglu, D., and M. K. Jensen (2013): \"Aggregate comparative statics,\" Games and Economic Behavior, 81, 27-49.\n-_ (2015): \"Robust comparative statics in large dynamic economies,\" Journal of Political Economy, 123(3), 587-640.\nAmir, R. (1996): \"Sensitivity analysis of multisector optimal economic dynamics,\" Journal of mathematical economics, 25(1), 123-141.\n\u2014_ (2018): \"Supermodularity and monotone methods in economics,\" Economic Theory, $66,547-556$.\n\nAnderson, R. M., H. Duanmu, A. Ghosh, and M. A. Khan (2022): \"On Existence of Berk-Nash Equilibria in Misspecified Markov Decision Processes with Infinite Spaces,\" arXiv, v2 dated 12.21.2022.\n\nAnderson, R. M., H. Duanmu, A. Ghosh, and M. A. Khan (2024): \"On existence of Berk-Nash equilibria in misspecified Markov decision processes with infinite spaces,\" Journal of Economic Theory, 217, 105813.\n\nArrow, K. J., D. Blackwell, and M. A. Girshick (1949): \"Bayes and minimax solutions of sequential decision problems,\" Econometrica, pp. 213-244.\n\nArrow, K. J., and J. R. Green (1973): \"Notes on expectations equilibria in Bayesian settings,\" Institute for Mathematical Studies in the Social Sciences.\n\nArrow, K. J., T. Harris, and J. Marschak (1951): \"Optimal inventory policy,\" Econometrica, pp. 250-272.\n\nBalbus, L., P. Dziewulski, K. Reffett, and L. Wo\u017any (2022): \"Markov distributional equilibrium dynamics in games with complementarities and no aggregate risk,\" Theoretical Economics, 17(2), 725-762.\n\nBerk, R. H. (1966): \"Limiting behavior of posterior distributions when the model is incorrect,\" The Annals of Mathematical Statistics, 37(1), 51-58.\n\nBohren, J. A., and D. N. Hauser (2021): \"Learning with heterogeneous misspecified models: Characterization and robustness,\" Econometrica, 89(6), 3025-3077.\n\nBrock, W. A., and L. J. Mirman (1972): \"Optimal economic growth and uncertainty: the discounted case,\" Journal of Economic Theory, 4(3), 479-513.\n\nBunke, O., and X. Milhaud (1998): \"Asymptotic behavior of Bayes estimates under possibly incorrect models,\" The Annals of Statistics, 26(2), 617-644.\n\nCanonne, C. L. (2022): \"A short note on an inequality between KL and TV,\" arXiv preprint arXiv:2202.07198.\n\nChe, Y.-K., J. Kim, and F. Kojima (2021): \"Weak monotone comparative statics,\" arXiv preprint arXiv:1911.06442.\n\nCover, T. M., and J. Thomas (2005): Elements of information theory. John Wiley \\& Sons.\nCsaba, D., and B. Szoke (2023): \"Learning with misspecified models,\" Discussion paper, mimeo.\n\nDatta, M., K. Reffett, and \u0141. Wo\u017any (2018): \"Comparing recursive equilibrium in economies with dynamic complementarities and indeterminacy,\" Economic Theory, 66, 593626 .\n\nDeimen, I., and J. Wirtz (2022): \"Control, cost, and confidence: Perseverance and procrastination in the face of failure,\" Games and Economic Behavior, 134, 52-74.\n\nDen Haan, W. J., and T. Drechsel (2019): \"Misspecification in macroeconomics: Difficult but not impossible to deal with,\" mimeo.\n\nEsponda, I., and D. Pouzo (2015): \"Equilibrium in misspecified Markov decision processes,\" arXiv preprint arXiv:1502.06901.\n(2016): \"Berk-Nash equilibrium: A framework for modeling agents with misspecified models,\" Econometrica, 84(3), 1093-1130.\n(2021): \"Equilibrium in misspecified Markov decision processes,\" Theoretical Economics, 16, 717-757.\n\nEsponda, I., D. Pouzo, and Y. Yamamoto (2021): \"Asymptotic behavior of Bayesian learners with misspecified models,\" Journal of Economic Theory, 195, 105260.\n\nFarmer, L., E. Nakamura, and J. Steinsson (2024): \"Learning about the long run,\" Journal of Political Economy, 132(10), 3334-3377.\n\nFrick, M., R. Iljima, and Y. Ishii (2020): \"Misinterpreting others and the fragility of social learning,\" Econometrica, 88(6), 2281-2328.\n\nHansen, L. P., and T. J. Sargent (2011): Robustness. Princeton university press.\nHeidhues, P., B. K\u00f5szegi, and P. Strack (2018): \"Unrealistic expectations and misguided learning,\" Econometrica, 86(4), 1159-1214.\n\nH\u00d6FT, H. (1987): \"Order preserving selections for multifunctions,\" Contributions to General Algebra, 5.\n\nHopenhayn, H. A., and E. C. Prescott (1992): \"Stochastic monotonicity and stationary distributions for dynamic economies,\" Econometrica, pp. 1387-1406.\n\nHuber, P. J. (1967): \"The behavior of maximum likelihood estimates under nonstandard conditions,\" in Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, vol. 1, pp. 221-233. Berkeley, CA: University of California Press.\n\nHuggett, M. (2003): \"When are comparative dynamics monotone?,\" Review of Economic Dynamics, 6(1), 1-11.\n\nJehiel, P. (2005): \"Analogy-based expectation equilibrium,\" Journal of Economic Theory, $123(2), 81-104$.\n\nKamae, T., U. Krengel, and G. L. O\u2019Brien (1977): \"Stochastic inequalities on partially ordered spaces,\" The Annals of Probability, 5(6), 899-912.\nKirman, A. P. (1975): \"Learning by firms about demand conditions,\" in Adaptive economic models, pp. 137-156. Elsevier.\nKoulovatianos, C., L. J. Mirman, and M. Santugini (2009): \"Optimal growth and uncertainty: Learning,\" Journal of Economic Theory, 144(1), 280-295.\nKydland, F. E., and E. C. Prescott (1982): \"Time to build and aggregate fluctuations,\" Econometrica, pp. 1345-1370.\nLanzani, G. (2025): \"Dynamic concern for misspecification,\" forthcoming at Econometrica.\nLevhari, J. D., and T. N. Srinivasan (1969): \"Optimal savings under uncertainty,\" The Review of Economic Studies, 36(2), 153-163.\nLewbel, A. (2019): \"The identification zoo: Meanings of identification in econometrics,\" Journal of Economic Literature, 57(4), 835-903.\nLight, B. (2021): \"Stochastic Comparative Statics in Markov Decision Processes,\" Mathematics of Operations Research.\nLong Jr., J. B., and C. I. Plosser (1983): \"Real business cycles,\" Journal of Political Economy, 91(1), 39-69.\nLucas, R. E. (1978): \"Asset prices in an exchange economy,\" Econometrica, pp. 1429-1445.\nLucas, R. E., and E. C. Prescott (1971): \"Investment under uncertainty,\" Econometrica, pp. 659-681.\n(1974): \"Equilibrium search and unemployment,\" Journal of Economic Theory, 7(2), 188-209.\nMilgrom, P., and J. Roberts (1994): \"Comparing equilibria,\" The American Economic Review, pp. 441-459.\nMilgrom, P., and C. Shannon (1994): \"Monotone comparative statics,\" Econometrica, pp. $157-180$.\n\nMolavi, P. (2019): \"Macroeconomics with learning and misspecification: A general theory and applications,\" Unpublished manuscript.\nNyarko, Y. (1991): \"Learning in mis-specified models and the possibility of cycles,\" Journal of Economic Theory, 55(2), 416-427.\nPhelps, E. S. (1962): \"The new view of investment: a neoclassical analysis,\" The Quarterly Journal of Economics, 76(4), 548-567.\nPuterman, M. L. (1994): Markov decision processes: discrete stochastic dynamic programming. John Wiley \\& Sons.\nRoss, W. (1987): \"The expectation of the likelihood ratio criterion,\" International Statistical Review/Revue Internationale de Statistique, pp. 315-330.\n\nRust, J. (1994): \"Structural estimation of Markov decision processes,\" Handbook of Econometrics, 4, 3081-3143.\n\nSaghafian, S. (2018): \"Ambiguous partially observable Markov decision processes: Structural results and applications,\" Journal of Economic Theory, 178, 1-35.\nSantos, M. S. (2000): \"Accuracy of numerical solutions using the Euler equation residuals,\" Econometrica, 68(6), 1377-1402.\n\nShaked, M., and J. G. Shanthikumar (2007): Stochastic Orders. Springer.\nShalizi, C. R. (2009): \"Dynamics of Bayesian updating with dependent data and misspecified models,\" Electronic Journal of Statistics, 3, 1039-1074.\nSmithson, R. (1971): \"Fixed points of order preserving multifunctions,\" Proceedings of the American Mathematical Society, 28(1), 304-310.\nTopkis, D. M. (1978): \"Minimizing a submodular function on a lattice,\" Operations Research, 26(2), 305-321.\n(1998): Supermodularity and complementarity. Princeton university press.\nTorres, R. (2005): \"Multivariate Monotone Bayesian Updating,\" Available at SSRN 2167744.\nUppal, R., and T. Wang (2003): \"Model misspecification and underdiversification,\" The Journal of Finance, 58(6), 2465-2486.\nWhite, H. (1982): \"Maximum likelihood estimation of misspecified models,\" Econometrica, pp. 1-25.", "tables": {}, "images": {}}], "id": "2407.17037v2", "authors": ["Aniruddha Ghosh"], "categories": ["econ.TH"], "abstract": "We present novel monotone comparative statics results for steady-state\nbehavior in a dynamic optimization environment with misspecified Bayesian\nlearning. Building on \\cite{ep21a}, we analyze a Bayesian learner whose prior\nis over parameterized transition models but is misspecified in the sense that\nthe true process does not belong to this set. We characterize conditions that\nensure monotonicity in the steady-state distribution over states, actions, and\ninferred models. Additionally, we provide a new monotonicity-based proof of\nsteady-state existence, derive an upper bound on the cost of misspecification,\nand illustrate the applicability of our results to several environments of\ngeneral interest.", "updated": "2025-03-14T17:43:12Z", "published": "2024-07-24T06:50:08Z"}