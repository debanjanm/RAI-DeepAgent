{"title": "Locally Stationary Distributions: A Framework for Analyzing Slow-Mixing\n  Markov Chains", "sections": [{"section_id": 0, "text": "#### Abstract\n\nMany natural Markov chains fail to mix to their stationary distribution in polynomially many steps. Often, this slow mixing is inevitable since it is computationally intractable to sample from their stationary measure.\n\nNevertheless, Markov chains can be shown to always converge quickly to measures that are locally stationary, i.e., measures that don't change over a small number of steps. These locally stationary measures are analogous to local minima in continuous optimization, while stationary measures correspond to global minima.\n\nWhile locally stationary measures can be statistically far from stationary measures, do they enjoy provable theoretical guarantees that have algorithmic implications? We study this question in this work and demonstrate three algorithmic applications of locally stationary measures:\n\n\n1. We show that Glauber dynamics on the hardcore model can be used to find independent sets of size $\\Omega\\left(\\frac{\\log d}{d} \\cdot n\\right)$ in triangle-free graphs of degree at most $d$.\n2. Let $W$ be a symmetric real matrix with bounded spectral diameter and $v$ be a unit vector. Given the matrix $M=\\lambda v v^{\\top}+W$ with a planted rank-one spike along vector $v$, for sufficiently large constant $\\lambda$, Glauber dynamics on the Ising model defined by $M$ samples vectors $x \\in\\{ \\pm 1\\}^{n}$ that have constant correlation with the vector $v$.\n3. Let $M=A_{G}-\\frac{d}{n} \\mathbf{1} \\mathbf{1}^{\\top}$ be a centered version of the adjacency matrix where the graph $G$ is drawn from a sparse 2-community stochastic block model with signal-to-noise ratio $\\lambda$. We show that for sufficiently large constant $\\lambda$, Glauber dynamics on the Ising model defined by $M$ samples vectors $x \\in\\{ \\pm 1\\}^{n}$ that have constant correlation with the hidden community vector $\\sigma$.\nIn other words, Glauber dynamics subsumes the spectral method for spiked Wigner and community detection, by weakly recovering the planted spike.\n\n[^0]\n[^0]:    *MIT. Email: liukui@mit.edu\n    ${ }^{\\dagger}$ MIT. Email: sidm@mit.edu. Supported by CSAIL.\n    ${ }^{\\ddagger}$ UC Berkeley. Email: raghavendra@berkeley.edu. Supported by NSF CCF-2342192.\n    ${ }^{\\S}$ MIT. Email: amit_r@mit.edu. Supported by an Akamai Presidential Fellowship.\n    ${ }^{\\dagger}$ UC Berkeley. Email: david_wu@berkeley.edu. Supported by NSF GRFP DGE-2146752.", "tables": {}, "images": {}}, {"section_id": 1, "text": "# Contents \n\n1 Introduction ..... 1\n1.1 Locally stationary distributions ..... 1\n1.2 Technical overview ..... 6\n1.3 Related work ..... 8\n1.4 Open problems ..... 9\n2 Preliminaries ..... 11\n3 Properties of locally stationary distributions ..... 15\n4 Warmup: Large independent sets in triangle-free graphs ..... 18\n5 Weak recovery in spiked models ..... 21\n5.1 Entropic stability and conservation of variance ..... 23\n5.2 Restricted Gaussian dynamics achieves weak recovery ..... 25\n5.3 Glauber dynamics achieves weak recovery ..... 30", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 1 Introduction \n\nMarkov chains are a fundamental algorithmic primitive that are widely applied towards sampling and counting tasks. There is a rich body of literature devoted to understanding worst-case mixing times of Markov chains, i.e., the number of steps required for the distribution of the chain to approach its stationary measure started from an arbitrary initialization. For some highlights in this area, see, e.g., the contents and references in [MT06, Dia09, BGJM11, BGL+14, LPW17].\n\nUnfortunately, many natural Markov chains fail to mix rapidly from worst-case initializations, in that it takes a super-polynomial number of steps to reach stationarity. Structurally, this is due to the presence of cuts in the state space with very small conductance. Often, slow mixing is inevitable since sampling from the stationary measure is known to be computationally hard (say NP-hard); see, e.g., [Sly10, SS14, GV16]. Nevertheless, it has been empirically observed that certain simple and local Markov chains like Glauber dynamics succeed at optimization and inference tasks even when they are not known to mix, such as finding satisfying assignments to SAT formulas [SKC94, BIL+16], and clustering stochastic block models [MS12, GBP19]. This suggests that local Markov chains like Glauber dynamics can have algorithmic applications, even if they fail to mix rapidly, which raises our main line of inquiry.\n\nQuestion 1.1. What is the long-term behavior of Markov chains that do not mix rapidly? How can slowmixing Markov chains be harnessed for optimization and inference?\n\nTo lay out the motivation, it is useful to draw an analogy to continuous optimization. Gradient descent, the canonical algorithm in optimization, converges efficiently to a global minimum if the objective function and parameter space is convex. However, non-convex objective functions and parameter spaces come up often both in theory and practice, and finding global minima can even be provably intractable. On the other hand, gradient descent can be shown to always converge quickly to a local minimum, or more precisely, a first-order stationary point. Moreover, these local minima are useful in practice, and can admit non-trivial theoretical guarantees. See [Nes18] for a comprehensive coverage of analyzing gradient-based optimization methods, and see, e.g., [GJZ17, BLLT20, JT20, JNG+21] and the references therein for non-trivial theoretical guarantees on local minima of gradient descent in the context of machine learning.\n\nAnalogously, in the context of sampling, certain random walks can be shown to mix rapidly to their stationary measures. However, other random walks can also be shown to mix slowly from worst-case initializations; these are akin to the \"hard\" nonconvex optimization problems. Despite this, one can show that any Markov chain satisfying fairly generic conditions converges to analogs of local minima that we term locally stationary distributions (see Definition 1.2). Intuitively, a locally stationary distribution corresponds to the stationary measure conditioned on a subset of states that are sparsely connected to the rest of the state space under the Markov chain.\n\nThis raises the question of whether these locally stationary measures obey theoretical guarantees that are useful for solving problems in optimization or inference.", "tables": {}, "images": {}}, {"section_id": 3, "text": "### 1.1 Locally stationary distributions\n\nLet $P$ be the transition matrix of a time-reversible Markov chain on a state space $\\Omega$, where $P[x, y]$ denotes the transition probability from $x$ to $y$. Let $\\pi$ be a stationary distribution w.r.t. $P$.\n\nAnalogous to local minima in optimization, a locally stationary measure $v$ is one started at which the Markov chain $P$ remains nearly stationary, i.e., makes little progress. We will use the $K L$ divergence to the stationary distribution, denoted $\\operatorname{KL}(v \\| \\pi)$, as a measure of progress. This leads to the following definition of an $\\varepsilon$-locally stationary measure.\n\nDefinition 1.2. A probability measure $v$ on $\\Omega$ with density $f$ relative to $\\pi$ is said to be $\\varepsilon$-locally stationary with respect to $P$ if\n\n$$\n\\mathcal{E}(f, \\log f):=\\sum_{x, y \\in \\Omega} P[x, y] \\cdot(f(x)-f(y)) \\cdot \\log \\frac{f(x)}{f(y)} \\leqslant \\varepsilon\n$$\n\nThe Dirichlet form $\\mathcal{E}(f, \\log f)$ measures the rate at which the Markov chain progresses towards the stationary distribution. In particular, for a continuous-time version of the Markov chain $P$, if $v_{t}$ denotes the measure at time $t$, we have the following well-known fact [BT06]:\n\n$$\n\\frac{\\mathrm{d}}{\\mathrm{~d} t} \\mathrm{KL}\\left(v_{t} \\| \\pi\\right)=-\\mathcal{E}\\left(f_{t}, \\log f_{t}\\right)\n$$\n\nAs an immediate consequence, we observe that the Markov chain is typically on $\\varepsilon$-locally stationary measures over time. Formally, we have the following claim.\n\nTheorem 1.3. Fix a time-reversible Markov chain $P$ with a stationary measure $\\pi$, any starting distribution $v_{0}$, and $\\varepsilon, \\delta>0$. Let $T=\\frac{1}{\\delta \\varepsilon} \\cdot \\log \\left(\\frac{1}{\\pi_{\\min }}\\right)$. Then, for a time $\\boldsymbol{t} \\sim[0, T]$ chosen uniformly at random, the distribution $v_{t}$ at time $t$ is $\\varepsilon$-locally stationary with respect to $P$ with probability at least $1-\\delta$.\n\nThe main conceptual contribution of our work is the following meta-principle for showing that locally stationary distributions solve optimization and inference problems.\n\nProve that sampling from the true stationary distribution solves the optimization or inference problem of interest, and additionally, does so for \"local\" reasons.\n\nThis principle is best illustrated by discussing our algorithmic applications of locally stationary distributions.\n\nIndependent sets in triangle-free graphs. It is easy to see that any graph $G$ on $n$ vertices with degree bounded by $d$ has an independent set of size $\\frac{n}{d+1}$, a bound which is tight for the union of $(d+1)$-sized cliques. Ajtai, Koml\u00f3s, and Szemer\u00e9di [AKS80] showed that when $G$ is triangle-free, the size of the maximum independent set guaranteed to exist increases to $\\Omega\\left(n \\cdot \\frac{\\log d}{d}\\right)$. Shearer [She83] gave an alternate proof which pins down the leading constant to $1-o_{d}(1)$ and relaxes the assumption of bounded maximum degree to bounded average degree.\n\nIt is also well-known that a uniformly random independent set in a triangle-free graph of maximum degree $d$ has expected size at least $\\Omega\\left(n \\cdot \\frac{\\log d}{d}\\right)$; see, e.g., [AS16, Proposition 1, Page 272]. Hence, it is natural to wonder whether Glauber dynamics with respect to the uniform measure over independent sets finds such a large independent set. From a given independent set $I \\subseteq V$, the transitions of Glauber dynamics can be described as follows:\n\n1. Sample a uniformly random vertex $v \\in V$.\n\n2. If $I \\cup\\{v\\}$ is an independent set, then go to $I^{\\prime}=I \\cup\\{v\\}$ with probability $1 / 2$ and $I^{\\prime}=I \\backslash\\{v\\}$ with probability $1 / 2$.\n3. If $I \\cup\\{v\\}$ is not an independent set, go to $I^{\\prime}=I \\backslash\\{v\\}$ with probability 1 .\n\nNotably, this Markov chain requires $\\exp (\\Omega(n))$ steps to mix [MWW07] as soon as $d \\geqslant 6$. In fact, the problem of sampling a uniformly random independent set on a graph of maximum degree $d$ becomes NP-hard [Sly10, SS14] in this regime, even if triangle-freeness is assumed [G\u015aV15].\n\nDespite these hardness results for the corresponding sampling problems, we show that the above Markov chain can be used to find independent sets of size $\\Omega\\left(n \\cdot \\frac{\\log d}{d}\\right)$ in triangle-free graphs of maximum degree bounded by $d$. Specifically, we show the following result.\nTheorem 1.4. Let $G$ be a triangle-free graph on $n$ vertices with maximum degree bounded by $d$. Let $\\mathbf{I}$ be an independent set in $G$ that arises from Glauber dynamics run for $O\\left(n d^{4}\\right)$ time. Then the expected size of $\\mathbf{I}$ is at least $\\frac{1-\\sigma_{d}(1)}{4} \\cdot n \\cdot \\frac{\\log d}{d}$.\nRemark 1.5. In fact, one can prove that Glauber dynamics at \"fugacity\" $\\frac{1}{\\log d}$ finds an independent set of size $\\left(1-o_{d}(1)\\right) \\cdot n \\cdot \\frac{\\log d}{d}$ by combining our proof method with that of [DJPR18].\n\nAs mentioned before, we know that the expected size of a uniformly random independent set satisfies the above lower bound. However, the Glauber dynamics chain does not mix rapidly, and hence does not produce samples from the truly uniform distribution. Instead, it samples from a locally stationary distribution with respect to the Markov chain. Our key insight is that the same proof also goes through for an independent set sampled from a locally stationary distribution with respect to Glauber dynamics.\n\nTo give a sense of how local stationarity is used, we briefly discuss the proof. The proof from [AS16] that the expected size of a uniformly random independent set $\\Omega\\left(n \\cdot \\frac{\\log d}{d}\\right)$ argues that for any vertex $v$, and for any pinning $x_{v \\cup N(v)}$ of the independent set outside $v$ and its neighbors, either:\n\n- the uniform distribution conditioned on the pinning chooses $v$ with probability $\\gtrsim \\frac{\\log d}{d}$, or\n- it chooses $\\gtrsim \\log d$ neighbors of $v$ in expectation.\n\nThus, each vertex can be charged $\\Omega\\left(\\frac{\\log d}{d}\\right)$ vertices on average in the independent set. Observe that the above sketch of the argument goes through even if the distribution is not truly uniform but merely has conditional marginals matching the uniform distribution, which is a property we can show holds for locally stationary distributions (Lemma 3.5).\n\nUsing similar arguments, one can establish that given a triangle-free graph with maximum degree $d$, Glauber dynamics run for poly $(n)$ many steps on the antiferromagnetic Ising model on $G$ with inverse temperature $\\frac{1}{\\sqrt{d}}$ recovers a cut of relative size $\\frac{1}{2}+\\Omega\\left(\\frac{1}{\\sqrt{d}}\\right)$.\n\nWeak recovery in spiked models. Beyond independent sets, we also study the performance of Glauber dynamics for statistical inference tasks. Consider the central class of Bayesian models for principal component analysis (PCA) known as spiked random matrix models, which consist of a matrix $M \\in \\mathbb{R}^{n \\times n}$ given by\n\n$$\nM=\\underset{\\substack{\\text { signal } \\\\ \\text { strength }}}{\\lambda} \\cdot \\underset{\\text { signal }}{\\text { vo }^{\\top}}+\\underset{\\text { noise }}{\\text { noise }}\n$$\n\nThe general algorithmic question is to approximately recover the signal (a unit vector $v$ ) under appropriate assumptions about the noise $(W)$ and the signal strength $(\\lambda)$. More precisely:\n\nProblem 1.6 (Weak recovery in spiked matrix models). For a unit norm signal vector $v \\in \\mathbb{R}^{n}$, signal strength $\\lambda \\in \\mathbb{R}$, and noise matrix $W \\in \\mathbb{R}^{n \\times n}$, given $M=\\lambda \\cdot v v^{\\top}+W$, give an efficient algorithm to extract a unit norm estimate $\\tilde{v}$ such that $\\langle v, \\tilde{v}\\rangle \\geqslant \\Omega(1)$.\n\nIn the situation where $W$ is a Wigner matrix, this model, known as the spiked Wigner model, has been a subject of extensive study. The work of [BAP05] determined that once $\\lambda>1$, a spectral algorithm based on computing the top eigenvector succeeds at weak recovery. There is a fairly large body of work on the spiked Wigner model, towards characterizing optimal estimation error, efficient algorithms and its generalizations to rank larger than one [DMK ${ }^{+} 16 \\mathrm{a}, \\mathrm{DMK}^{+} 16 \\mathrm{~b}$, DAM17, EAK18, LM17, Mio17, BM19, MV21]. When the prior distribution over $v$ is the uniform distribution over $\\left\\{ \\pm \\frac{1}{\\sqrt{n}}\\right\\}^{n}$ and $\\mathrm{S}^{n-1}$, there are efficient algorithms that even achieve the maximum information-theoretically achievable correlation $|\\langle v, \\tilde{v}\\rangle|$, based on approximate message passing [FMM21, CFM23], and algorithmic stochastic localization [MW23].\n\nIn the case where the prior distribution is on the hypercube, given the matrix $M$, this posterior is described by an Ising model: a probability distribution $\\mu_{\\beta M}$ over $\\{ \\pm 1\\}^{n}$ defined by the following proportionality relation for a suitably chosen $\\beta>0$ :\n\n$$\n\\mu_{\\beta M}(x) \\propto \\exp \\left(\\frac{1}{2}\\langle x, \\beta M x\\rangle\\right) \\text { for all } x \\in\\{ \\pm 1\\}^{n}\n$$\n\nSampling from the above posterior distribution is desirable as it achieves the maximum informationtheoretically achievable correlation. The canonical algorithm for sampling is to run the Glauber dynamics Markov chain, but unfortunately, provable guarantees for Glauber dynamics are currently lacking. Thus, a natural question en route is: does Glauber dynamics for the Ising model $\\mu_{\\beta M}$ weakly recover the signal in polynomial-time?\n\nWe make progress towards answering this question affirmatively in this work by showing that Glauber dynamics at a slightly higher temperature than the posterior distribution succeeds for a broad family of settings.\n\nFormally, we show the following result:\nTheorem 1.7. Let $W$ be a matrix with $\\kappa \\preceq W \\preceq 1-\\kappa$, and $v \\in\\left\\{ \\pm \\frac{1}{\\sqrt{n}}\\right\\}^{n}$. Let $P$ denote the kernel of the Glauber dynamics chain with stationary distribution $\\mu_{W+\\lambda v v^{\\top}}$, and $x_{0}$ an arbitrary point on $\\{ \\pm 1\\}^{n}$. There exists a large enough constant $\\lambda>0$ such that for $T=\\widetilde{\\Theta}\\left(n^{5}\\right)$, and for $t \\sim[0, T]$, with probability $1-o(1)$ we have:\n\n$$\n\\mathbf{E}_{x \\sim P^{\\alpha} \\delta_{x_{0}}}[|\\langle x, v\\rangle|] \\geqslant\\left(\\kappa \\exp \\left(-\\frac{1}{\\kappa}\\right)-o(1)\\right) \\cdot \\sqrt{n}\n$$\n\nA natural approach to recover the signal $v$ from the matrix $M$ is the spectral method, which amounts to computing (even approximately) the eigenvector corresponding to the largest eigenvalue for the matrix $M$. At a high-level, the above theorem demonstrates that Glauber dynamics can simulate the spectral method in certain regimes. We further expect Glauber to achieve weak recovery when run for $T=n^{1+o(1)}$ steps, but we leave this open as a direction for future improvement.\n\nRemark 1.8. The above model of choice captures several commonly considered models of study in the algorithms and complexity of statistical inference, such as the spiked Wigner model [BAP05], and random/planted 2XOR (see, e.g., [AOW15] and the references within).\n\nRemark 1.9. In the Rademacher spiked Wigner model, where $W \\sim \\operatorname{GOE}(n)$ and $v \\sim\\left\\{ \\pm \\frac{1}{\\sqrt{n}}\\right\\}^{n}$, the posterior has the form\n\n$$\n\\operatorname{Pr}[v \\mid M] \\propto \\exp \\left(-\\frac{n}{2}\\left\\|M-\\lambda v v^{\\top}\\right\\|_{F}^{2}\\right) \\propto \\exp \\left(\\frac{\\lambda n}{2} \\cdot v^{\\top} M v\\right)\n$$\n\nThe above is an Ising model, and suggests Glauber dynamics as a natural algorithm for weak recover. We note that the Ising model we shall analyze will be a higher temperature version of the above, that is, a distribution with density proportional to $\\exp \\left(\\frac{\\beta n}{2} \\cdot v^{\\top} M v\\right)$ for some $\\beta<\\lambda$ (as opposed to the \"correct\" value $\\lambda$ ). Interestingly, such recovery guarantees were not previously known, even if one information theoretically samples from the higher temperature Ising model.\n\nStochastic block model. Another case of interest is one where the Ising model $M$ arises from a stochastic block model. To describe this result, we first begin by defining the two-community stochastic block model.\n\nDefinition 1.10 (2-community stochastic block model). Let $d, \\lambda \\in \\mathbb{R}$ be fixed parameters such that $\\lambda^{2} \\leqslant d$. The distribution $\\operatorname{SBM}(n, d, \\lambda)$ is defined over pairs $(\\sigma, G) \\in\\{ \\pm 1\\}^{n} \\times\\{0,1\\}^{n \\times n}$ generated as follows.\n\nLet $\\sigma \\in\\{ \\pm 1\\}^{n}$ be a signal vector drawn uniformly at random (i.e. the prior is uniform). Given $\\sigma$, we draw a random graph $G$ by including an edge between $u, v \\in[n]$ independently with probability $\\frac{d+\\lambda \\sqrt{d}}{n}$ if $\\sigma(u)=\\sigma(v)$, and with probability $\\frac{d-\\lambda \\sqrt{d}}{n}$ otherwise.\n\nIn a general stochastic block model, the signal vector $\\sigma$ can be over a larger finite alphabet $[q]$, and the probability of including an edge between $u, v \\in[n]$ is an arbitrary function of $\\sigma(u), \\sigma(v)$. In this work, we will use the term stochastic block model (SBM) to refer exclusively to the special case of two communities as defined above.\n\nRemark 1.11. The 2-community stochastic block model can be viewed as a special case of a spiked matrix model where $M$ is a highly sparse matrix. Due to the sparsity of $M$, this spiked matrix model falls outside the scope of Theorem 1.7, as the \"noise\" part fails to satisfy the spectral bound.\n\nThe weak recovery problem for stochastic block model is that of recovering a labelling $\\widehat{\\sigma}$ given the graph $G$ such that $\\widehat{\\sigma}$ has non-trivial correlation with the true signal $\\sigma$. More precisely, an algorithm for weak recovery is required to find a $\\widehat{\\sigma}$ such that $\\frac{1}{n}|\\langle\\widehat{\\sigma}, \\sigma\\rangle| \\geqslant \\Omega(1)$.\n\nStarting with the work of Decelle, Krzakala, Moore, \\& Zdeborova [DKMZ11] that posited broad conjectures about these models, an extensive body of work has emerged over the past decade. For the case of 2 communities, [DKMZ11] posited that weak-recovery is possible if and only if the signal strength $\\lambda^{2}>1$. This coincides with the Kesten-Stigum threshold, a threshold for broadcast processes on trees studied in the works of Kesten and Stigum [KS66, KS67]. The works of Mossel, Neeman, \\& Sly [MNS18] and Massouli\u00e9 [Mas14] confirmed the algorithmic side, namely that weak recovery can be solved efficiently above the KS threshold with a spectral algorithm, while [MNS15] showed impossibility below the threshold. We refer the reader to the survey of Abbe [Abb17] for a detailed treatment of the literature on community detection.\n\nWe show that Glauber dynamics succeeds at weak recovery when the signal strength is a constant factor above the Kesten-Stigum threshold.\n\nTheorem 1.12. There exist constants $\\lambda_{0}, \\beta, c>0$ such that for all $\\lambda$ satisfying $|\\lambda| \\geqslant \\lambda_{0}$, for $(\\boldsymbol{\\sigma}, \\boldsymbol{G}) \\sim$ $\\operatorname{SBM}(n, d, \\lambda)$, with probability $1-o(1)$ over the randomness of $(\\boldsymbol{\\sigma}, \\boldsymbol{G})$, the following holds.\n\nLet $P$ denote the kernel of Glauber dynamics with stationary distribution $\\mu_{\\frac{\\sigma}{\\sqrt{2}}}\\left(A_{G}-\\frac{d}{2} \\mathbf{1 1}^{\\top}\\right)$ and $x_{0}$ an arbitrary point on $\\{ \\pm 1\\}^{\\mathrm{n}}$. For $T=\\widetilde{\\Theta}\\left(n^{5+o_{d}(1)}\\right)$, and for $\\boldsymbol{t} \\sim[0, T]$, with probability $1-o(1)$, we have:\n\n$$\n\\mathbf{E}_{\\mathbf{x} \\sim P^{\\mathrm{n}} \\delta_{x_{0}}}[|\\langle\\mathbf{x}, \\boldsymbol{\\sigma}\\rangle|]>c n\n$$\n\nRemark 1.13. Similar to Remark 1.9, the posterior distribution $\\sigma \\mid \\boldsymbol{G}$ to solve the recovery problem in the stochastic block model is an Ising model (see, e.g., [Moo17, Eq. (7)]):\n\n$$\n\\operatorname{Pr}[\\sigma \\mid \\boldsymbol{G}] \\propto \\prod_{i j \\in E(G)}\\left(\\frac{d+\\lambda \\sqrt{d}}{d-\\lambda \\sqrt{d}}\\right)^{\\left(1+\\sigma_{i} \\sigma_{j}\\right) / 2} \\cdot \\prod_{i j \\notin E(G)}\\left(\\frac{1-\\frac{d+\\lambda \\sqrt{d}}{n}}{1-\\frac{d-\\lambda \\sqrt{d}}{n}}\\right)^{\\left(1+\\sigma_{i} \\sigma_{j}\\right) / 2}\n$$\n\nFor large $d$, the Ising model that we analyze is approximately equal to a higher temperature version of the above true posterior.\n\nAlthough spectral algorithms for weak recovery were already known in all the cases listed above, understanding the power of Glauber dynamics is interesting in its own right. It is arguably a more natural algorithm than spectral methods in the context of a Bayesian estimation problem like stochastic block models. In particular, Glauber dynamics remains locally consistent with the underlying probabilistic model at every vertex. On the other hand, a spectral algorithm that computes the top eigenvector maximizes a global objective, while crudely approximating the local features of the probabilistic model.\n\nFinally, our analysis for spiked models establishes a direct correspondence between locally stationary measures for Glauber dynamics and fixed points of a Markov chain over the onedimensional real line $\\mathbb{R}$ (related to the restricted Gaussian dynamics Markov chain). This correspondence may pave the way for a much tighter analysis to establish that Glauber dynamics achieves information theoretically optimal recovery in some of these models. To elucidate further on this correspondence, we will give a brief technical overview here.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 1.2 Technical overview \n\nIn Section 3, we derive a few basic properties of locally-stationary measures. This is followed by the result on independent sets presented in Section 4 as a warmup.\n\nIn this technical overview, we will focus on the inference problem in spiked matrix models. For a matrix $M \\in \\mathbb{R}^{n \\times n}$ and a vector $h \\in \\mathbb{R}^{n}$, we will use $\\mu_{M, h}$ to denote the distribution over $\\{ \\pm 1\\}^{n}$ defined as\n\n$$\n\\mu_{M, h}(x) \\propto \\exp \\left(\\frac{1}{2}\\langle x, M x\\rangle+\\langle h, x\\rangle\\right)\n$$\n\nand $\\mu_{M}$ to denote the distribution $\\mu_{M, 0}$.\nConsider the stationary measure $\\mu_{M}$ for a spiked matrix $M=\\lambda v v^{\\top}+W$. We outline the proofs of Theorems 1.7 and 1.12 here, which consist of two parts.\n\n- First, we show that locally stationary distributions with respect to Glauber dynamics over $\\{ \\pm 1\\}^{n}$ are also locally stationary with respect to the restricted Gaussian dynamics Markov chain.\n- Next, we show that samples from locally stationary distributions for RGD achieve weak recovery.\n\nLet us first recall the definitions of Glauber dynamics and Restricted Gaussian dynamics for Ising models.\n\nDefinition 1.14 (Glauber dynamics). Glauber dynamics with respect to a distribution $\\pi$ over $\\{ \\pm 1\\}^{n}$ is a Markov chain on $\\{ \\pm 1\\}^{n}$, where a transition from $x$ is given by the following:\n\n- Sample index $i$ uniformly from $[n]$.\n- Transition to $x^{\\oplus i}$ with probability $\\frac{\\pi\\left(x^{\\oplus i}\\right)}{S(x)+\\pi\\left(x^{\\oplus i}\\right)}$, and stay at $x$ otherwise. Here, $x^{\\oplus i}$ denotes $x$ with the $i$ th bit flipped.\n\nDefinition 1.15 (Restricted Gaussian dynamics; cf. [LST21, STL20, CE22]). Consider the joint random variable $(\\boldsymbol{x}, \\boldsymbol{z})$ where $\\boldsymbol{x} \\sim \\mu_{M}$, and $\\boldsymbol{z} \\mid \\boldsymbol{x}:=(\\lambda\\langle v, \\boldsymbol{x}\\rangle+\\sqrt{\\lambda} \\boldsymbol{g}) \\cdot v$ for $\\boldsymbol{g} \\sim \\mathcal{N}(0,1)$. Restricted Gaussian dynamics (RGD) is a Markov chain on $\\{ \\pm 1\\}^{n}$ where for any $x$, the transition to $\\boldsymbol{x}^{\\prime}$ is sampled as follows:\n\n- Sample $z \\mid x$.\n- Sample $x^{\\prime} \\mid z$.\n\nRemark 1.16. By definition, the above Markov chains are ergodic and reversible with respect to $\\pi$ and $\\mu_{M}$ respectively, and so asymptotically converge to them as their stationary distributions.\n\nRemark 1.17. We should think of $z \\mid x$ as being a noisy surrogate for how well $x$ correlates with the hidden direction $v$.\n\nInformally, we prove the following correspondence between locally stationary distributions for Glauber dynamics and locally stationary distributions for RGD. In fact, this correspondence is a consequence of a more generic statement; refer to Lemma 3.8 for details.\n\nLemma 1.18 (Informal version of Lemma 3.8). Let $v$ be a distribution over $\\{ \\pm 1\\}^{n}$ that is $\\varepsilon$-locally stationary under Glauber dynamics for $\\mu_{M}$. Suppose for every $z \\in \\mathbb{R}$, Glauber dynamics for the distribution of $\\boldsymbol{x} \\mid z$ is \"well-expanding\", and $\\log \\frac{1}{\\mu_{M}(x)} \\leqslant \\operatorname{poly}(n)$ for all $x \\in\\{ \\pm 1\\}^{n}$, then $v$ is $\\varepsilon \\cdot \\operatorname{poly}(n)$-locally stationary under restricted Gaussian dynamics.\n\nTo conclude that $v$ is locally stationary under restricted Gaussian dynamics, it suffices to verify the structural properties of $\\mu_{M}$. The lower bound on the minimum probability follows from upper and lower bounds on the values that the Hamiltonian can achieve. To show that Glauber dynamics for $\\boldsymbol{x} \\mid \\boldsymbol{z}$ is well-expanding, we must investigate the structure of this distribution further. A simple calculation reveals that the distribution of $\\boldsymbol{x} \\mid \\boldsymbol{z}$ is, in fact, the Ising model $\\mu_{W, z}$. In the setting of Theorem 1.7, where the spectral diameter of $W$ is bounded by 1, prior works [BB19, EKZ22, AJK $^{+}$21, CE22] prove that $\\mu_{W, z}$ always satisfies a \"modified log-Sobolev inequality\" (our relevant notion of \"well-expanding\"). In the setting of Theorem 1.12, where $W$ is a centered stochastic block model, a similar result is proved in a companion paper [LMRW24].\n\nRemark 1.19. This decomposition of $\\mu_{M}$ into a mixture of other Ising models is well-known in the literature by the name Hubbard-Stratonovich transformation [Hub59]; see also [KLR22].\n\nIn summary, we showed if $v$ is locally stationary with respect to Glauber dynamics, then it is also locally stationary for the RGD chain\n\n$$\nx \\rightarrow z\\left|x \\rightarrow x^{\\prime}\\right| z\n$$\n\nThus, it suffices to prove that $\\mathbf{E}_{x \\sim v}|\\langle x, v\\rangle|$ is bounded away from 0 for any distribution $v$ that is locally stationary for RGD. The two ingredients that go into proving this are:\n\n- A generic principle that says: if $v$ is locally stationary for a Markov chain $P$, then for any bounded function $f,\\left|\\mathbf{E}_{x \\sim v} f(x)-\\mathbf{E}_{x \\sim P v} f(x)\\right|$ is small (Corollary 3.3).\n- If the correlation of a distribution $v$ is too close to 0 , then a single step of RGD causes a significant boost in correlation, which means $v$ cannot be locally stationary. In particular, for $f(x)=|\\langle x, v\\rangle|$, if $\\mathbf{E}_{x \\sim v} f(x)$ is too close to 0 , then $\\mathbf{E}_{x \\sim P v} f(x)-\\mathbf{E}_{x \\sim v} f(x)$ is nontrivially large, which means any locally stationary distribution $v$ must achieve large correlation.\n\nSee Section 5.2 for the details of this argument.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 1.3 Related work \n\nMotivated by statistical physics, the phenomenon of metastability of random walks has been extensively studied. We refer the reader to the monograph by Bovier \\& Hollander [BDH16] for related literature. The notion of metastability in [BDH16] appears to be a slightly stricter notion than local stability, and thus does not generically hold for all reversible Markov chains.\n\nIn the context of sampling distributions over a continuous domain, Balasubramanian, Chewi, Erdogdu, Salim \\& Zhang [BCE+22] showed that the Langevin Monte Carlo algorithm always outputs a sample from a distribution whose relative Fisher information is small. This is the continuous sampling analog of convergence of gradient descent to approximate first-order stationary points. Building on these ideas, Cheng, Wang, Zhang \\& Zhu [CWZZ24] study the notion of conditional mixing for Langevin and Glauber dynamics and apply it to efficiently sample from Gaussian mixtures.\n\nOur analysis of Glauber dynamics borrows ideas from a recent line of works on sampling from Ising models. Glauber dynamics for an Ising model defined by a matrix $M$ was shown to mix quickly if eigenvalues of $M$ lie within an interval of length $1\\left[\\mathrm{AJK}^{+} 21\\right.$, EKZ22]. This is sharp, as evidenced by the Curie-Weiss model $M=\\frac{\\beta}{n} \\mathbf{1 1}^{\\top}$. Stronger evidence for hardness of sampling beyond this spectral criterion was recently provided by Kunisky [Kun23], based on a reduction to a certain statistical hypothesis testing problem. Koehler, Lee \\& Risteski [KLR22] devised more sophisticated algorithms based on simulated tempering and variational inference to sample from Ising models when they have constantly many eigenvalues outside an interval of length 1.\n\nBesides the question of fast mixing and metastability, the problem of how well MCMC-based algorithms perform for optimization and inference tasks was recently studied in several works. Chen, Mossel \\& Zadik [CMZ23] proved that when initialized at the empty set, natural Metropolis chains on cliques fail to find cliques of sublinear size in polynomial time, even if such a clique is planted inside the Erd\u0151s-R\u00e9nyi random graph $\\mathrm{G}(n, 1 / 2)$. This is despite there being an abundance\n\nof algorithms which can recover a planted clique of size down to $O(\\sqrt{n})$. Nevertheless, MCMCbased algorithms were redeemed in a more recent work of Gheissari, Jagannath \\& Xu [GJX23] using a more carefully designed chain and initialization. In a recent work, Sellke [Sel23] proved that low-temperature Langevin dynamics achieves the conjectured computational threshold for optimizing pure spherical spin glass models.", "tables": {}, "images": {}}, {"section_id": 6, "text": "# 1.4 Open problems \n\nWe conclude with several open directions, which we believe may be amenable to the framework of locally stationary distributions.\n\nBayesian inference via MCMC. First, there is the direction of pushing our results further in the setting of SBM. To set the scene, let $\\pi(x) \\propto \\exp (H(x))$ be the true posterior for SBM, where $H(x)$ is the SBM Hamiltonian (see Remark 1.13 for an explicit formula). It is well known that optimal recovery is achieved information theoretically by sampling from $\\pi$ (see, e.g., [Moo17, Section 4]). However, it takes exponential time to mix to $\\pi$ from a worst-case initialization. On the other hand, in Theorem 1.12, we achieve weak recovery by running Glauber dynamics on the density $\\pi_{\\beta}(x) \\propto \\exp (\\beta H(x))$ for some (constant) $\\beta$ strictly smaller than 1 . It is natural to investigate whether a sampling algorithm based on simulated annealing, i.e. running Glauber dynamics by varying the temperature over time, can succeed at sampling from $\\pi$. Our main result can be viewed as a modest step in this direction, as we show that running the chain for $\\operatorname{poly}(n)$ steps at a mismatched temperature already gives a warm start for correlation.\n\nProblem 1.20 (Optimal recovery for stochastic block model). Can an instance of simulated annealing sample from $\\pi$ ?\n\nComputationally optimal inference. The $k$-community stochastic block model is known to undergo an information-computation gap when $k \\geqslant 5$ (see, e.g., [AS16]). Specifically, for every $k \\geqslant 5$, there exists a choice of degree $d$ and signal-to-noise ratio $\\lambda$ for which weak recovery is informationtheoretically possible, but likely impossible for efficient algorithms [HS17]. This gap admits the construction of SBM instances where weak recovery is tractable, but information-theoretically optimal recovery is intractable to efficient algorithms.\n\nExample 1.21. Consider a 10-community block model obtained by taking two disjoint 5-community block model graphs, and planting a sparse bisection between them. The planted bisection should be sparse enough so it is clearly detectable to efficient algorithms. However, the parameters $d$ and $\\lambda$ for the 5 -community models are chosen to be in the intractable regime. An informationtheoretically optimal algorithm achieves weak recovery within each 5-community model. At the same time, algorithms for the 2 -community block model achieve weak recovery in the 10 community model, since they can find the planted bisection, and correctly classify vertices as belonging to either $\\{1,2,3,4,5\\}$ or $\\{6,7,8,9,10\\}$.\n\nIn settings such as the above, information-theoretically optimal inference is hard, but weak recovery is still tractable. This motivates the study of computationally optimal inference algorithms: algorithms that achieve the best guarantees possible in polynomial time. For such problems, the\n\nGlauber dynamics chain must necessarily fail to mix rapidly to the posterior distribution, but perhaps the locally stationary distribution it samples from can achieve the computationally optimal recovery guarantees?\n\nProblem 1.22 (Computationally optimal inference). Is (annealed) Glauber dynamics a computationally optimal algorithm for the $k$-community SBM and, more generally, for random CSPs with planted solutions?\n\nMetastable states. Local stationarity is a generic property of any time-reversible Markov chain, so a priori there is no reason to expect that a locally stationary distribution $v$ has any nice properties. For example, if we run the Markov chain for $T$ steps, Theorem 1.3 guarantees an $\\varepsilon$-locally stationary distribution $v$ where $\\varepsilon=O(1 / T)$, and the simple random walk on the $n$-vertex cycle graph demonstrates that this is tight if $T=o\\left(n^{2}\\right)$. This suggests the following natural questions. Suppose the stationary distribution $\\pi$ is a a Gibbs distribution on $\\{ \\pm 1\\}^{n}$. Under what additional structural assumptions on $\\pi$ can we both obtain $\\varepsilon=o(1 / T)$ for sufficiently large $T=\\operatorname{poly}(n)$ and endow $v$ with a physical or geometric interpretation?\n\nThe notion of metastable states for Gibbs distributions in statistical physics [BDH16] suggests a conceptual path forward towards this goal. In particular, one might hope to show that a locally stationary distribution is close to a metastable state, i.e. a conditional Gibbs distribution restricted to a metastable set of configurations.\n\nProblem 1.23 (Metastability). Suppose $\\pi$ is a Gibbs distribution which has a metastable subset $S$ with exponentially small conductance. Let $v$ be the locally-stationary distribution after running Glauber (or Langevin) dynamics for poly $(n)$ steps with uniform initialization in $S$. Is $v$ close to the conditional Gibbs distribution $\\pi_{S}$, e.g., $\\operatorname{KL}\\left(v \\| \\pi_{S}\\right)=o(1)$ ?\n\nFor a concrete setting, suppose $\\pi$ is a spherical spin glass in the shattering regime [EAMS23]. Can one show that in poly $(n)$ time, Langevin dynamics with uniform initialization remains stuck in the clusters identified there?\n\nCavity method. The cavity method, and the related replica method, originated in physics to predict the properties of various Gibbs distributions. Some striking achievements of this heuristic in producing accurate predictions are the Parisi formula [Par80, Tal06], and the $k$-SAT satisfiability threshold [MPZ02, MMZ06, DSS15]. It was also employed in the work of Decelle, Krazakala, Moore, \\& Zdeborov\u00e1 [DKMZ11] to conjecture the Kesten-Stigum threshold as the computational threshold for SBM.\n\nOf particular interest to us are the works of Coja-Oghlan, Krzakala, Perkins \\& Zdeborov\u00e1 [COKPZ17, COP19a, COP19b], that characterize the recovery rate that the optimal estimator, namely sampling from the Gibbs distribution, achieves for various planted constraint satisfaction problems. Their proofs use fairly minimal properties of the Gibbs distribution. More concretely, for a graph $G$ and an assignment $x$, let $H_{G}(x)$ be a Hamiltonian, and let $\\pi_{G}$ be the corresponding Gibbs distribution. Their proofs rely on the following properties satisfied by Gibbs distributions.\n\n- (Gibbs ratios) For any graph $G$ and any vertex $v$ :\n\n$$\n\\frac{\\pi_{G}(x)}{\\pi_{G \\backslash v}(x)} \\propto \\exp \\left(H_{G}(x)-H_{G \\backslash v}(x)\\right)\n$$\n\n- (Approximate independence) For random $G$ and $v$, the marginals of the neighbors of $v$ are approximately independent in $\\pi_{G \\backslash v}$.\n\nIf one can show that a family of locally stationary distributions also satisfy the Gibbs ratios up to a multiplicative error and approximate independence on a random graph $G$, then one could hope to port over the cavity method predictions and their rigorous proofs in a black-box fashion.\n\nProblem 1.24 (Cavity method). Let $\\left\\{v_{G}\\right\\}_{G \\in \\text { graphs }}$ be a family of locally stationary distributions where $v_{G}$ arises from running the Glauber dynamics for $\\pi_{G}$ for time- $T$ initialized at the uniform distribution. For a random graph $G$ and random vertex $v$, do $v_{G}$ and $v_{G \\backslash v}$ satisfy, up to small error, the Gibbs ratios and approximate independence properties?\n\nBeyond average-case models. Our work proves that Glauber dynamics recovers planted spikes when the input matrix has a clean \"signal + noise\" structure. Recently, there has been a flurry of work on inference in semirandom models; see, e.g., [BKS23, GHKM23] and the references within, where it is possible to extract the hidden signal using semidefinite programming-based algorithms. A natural direction is to investigate whether Glauber dynamics solves semirandom inference problems.\n\nProblem 1.25 (Semirandom models). Does Glauber dynamics succeed at finding solutions to semirandom planted CSPs, or large cliques in semirandom graphs with planted cliques as is done in the works of [GHKM23] and [BKS23] respectively?\n\nIn a similar vein, semidefinite programming has been phenomenally successful at solving dense CSPs, and more generally CSPs on graphs with low threshold-rank [BRS11]. A reason to believe that local algorithms perform well is Theorem 1.7, where we show that Glauber dynamics can recover rank-1 spikes in threshold-rank-1 matrices.\n\nProblem 1.26 (CSPs on low threshold-rank graphs). Does running Glauber dynamics give a PTAS for Max Cut on a dense graph?", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 2 Preliminaries \n\nWe begin by setting up some notation.\n\n- Let $P$ be the transition matrix of a time-reversible Markov chain on state space $\\Omega$ with stationary distribution $\\pi$, where $P[i, j]$ denotes the transition probability from $i$ to $j$. Let $P_{t}=\\exp (-t(I-P))$ denote the time- $t$ transition kernel.\n- For a distribution $v$ absolutely continuous with respect to $\\pi$, we use $f(x):=\\frac{\\mathrm{d} v}{\\mathrm{~d} \\pi}(x)$ to refer to its relative density to $\\pi$.\n- We use $v_{t}$ to denote $P_{t} v$, and we assume that $v_{t}$ is absolutely continuous with respect to $\\pi$ throughout. In particular, we write $f_{t}(x):=\\frac{\\mathrm{d} v_{t}}{\\mathrm{~d} \\pi}(x)$ to denote its relative density to $\\pi$;\n- We use $\\mathbf{m}(v):=\\mathbf{E}_{\\boldsymbol{x} \\sim v} \\boldsymbol{x}$ to denote the mean of $v$.\n\n- We use $y \\sim_{P} x$ when $y$ is chosen as a random neighbor of $x$ according to transition probabilities given by $P$. We drop the subscript $P$ from the $\\sim$ when the Markov chain is clear from context.\n\nRemark 2.1. The way to think of the time- $t$ transition kernel $P_{t}$ for a Markov chain with kernel $P$ on a discrete space is via the process: sample $\\boldsymbol{t} \\sim \\operatorname{Poisson}(t)$ and take $\\boldsymbol{t}$ steps using $P$.\n\nWe will require the following simple consequence of the definition of total variation distance.\nFact 2.2. For any pair of distributions $v$ and $\\pi$ on $\\Omega$, and any function $f: \\Omega \\rightarrow \\mathbb{R}$, we have\n\n$$\n\\left|\\mathbf{E}_{v} f-\\mathbf{E}_{\\pi} f\\right| \\leqslant\\left(f_{\\max }-f_{\\min }\\right) \\cdot d_{\\mathrm{TV}}(v, \\pi)\n$$\n\nDefinition 2.3 (Dirichlet form). For functions $f, g: \\Omega \\rightarrow \\mathbb{R}$, and $x, y \\in \\Omega$, the Dirichlet form of $f$ and $g$ with respect to $P$ is:\n\n$$\n\\mathcal{E}_{P}(f, g):=\\mathbf{E}_{x \\sim \\pi} \\mathbf{E}_{y \\sim \\rho x}(f(x)-f(y)) \\cdot(g(x)-g(y))\n$$\n\nWe drop the $P$ in the subscript when it is clear from context.\nRemark 2.4. When we use the Glauber dynamics chain for a distribution $\\pi$ on a hypercube, we use $\\mathcal{E}_{\\pi}$ to denote the corresponding Dirichlet form.\n\nThe Dirichlet form measures the rate at which a Markov chain makes progress towards the stationary distribution. The following is one way of articulating this notion; see, e.g., [BT06].\n\nFact 2.5. $\\frac{\\mathrm{d}}{\\mathrm{d} t} \\mathrm{KL}\\left(v_{t} \\| \\pi\\right)=-\\mathcal{E}\\left(f_{t}, \\log f_{t}\\right)=-\\mathbf{E}_{x \\sim \\pi} \\mathbf{E}_{y \\sim x}\\left(f_{t}(x)-f_{t}(y)\\right) \\cdot \\log \\frac{f_{t}(x)}{f_{t}(y)}$.\nDefinition 2.6 (Modified log-Sobolev inequality). We say $P$ satisfies a modified log-Sobolev inequality (MLSI) with constant $C$ if for any function $f: \\Omega \\rightarrow \\mathbb{R}_{>0}$,\n\n$$\n\\mathcal{E}_{P}(f, \\log f) \\geqslant C \\cdot \\operatorname{Ent}[f]\n$$\n\nHere, $\\operatorname{Ent}[f]:=\\mathbf{E}_{\\pi}[f \\log f]-\\mathbf{E}_{\\pi} f \\log \\mathbf{E}_{\\pi} f$ is the entropy functional. In particular, $C_{\\text {MLSI }}$ is the best (largest) such constant $C$.\n\nWe will need the following fact concerning the MLSI for product measures.\nFact 2.7 (see, e.g. [Goe04, Lemma 2.5]). Let $\\pi$ be a distribution over $\\{ \\pm 1\\}^{n}$ with independent coordinates. Then $C_{\\mathrm{MLSI}}(\\pi) \\geqslant 1 / n$.\n\nFinally, we need the following lemma showing that an MLSI implies concentration of Lipschitz functions, in particular linear forms.\n\nLemma 2.8 ([Goe04, Theorem 5.1]). Let $\\mu$ be an arbitrary distribution over $\\{ \\pm 1\\}^{n}$ such that $C_{\\mathrm{MLSI}}(\\mu) \\geqslant$ $\\alpha$. Then, for any function $f:\\{ \\pm 1\\}^{n} \\rightarrow \\mathbb{R}$ that is 1-Lipschitz, in that $|f(x)-f(y)| \\leqslant 1$ if $\\|x-y\\|_{1} \\leqslant 2$,\n\n$$\n\\mathbf{P r}_{x \\sim \\mu}\\left[\\left|f(x)-\\mathbf{E}_{\\mu} f\\right| \\geqslant t\\right] \\leqslant 2 \\exp \\left(-\\frac{\\alpha t^{2}}{2}\\right)\n$$\n\nRemark 2.9. It is well known that the KL divergence to the stationary distribution decays exponentially at a rate dictated by $C_{\\text {MLSI }}$ (see [BT06, Theorem 2.4] for more details):\n\n$$\n\\mathrm{KL}\\left(v_{t} \\| \\pi\\right) \\leqslant \\mathrm{KL}\\left(v_{0} \\| \\pi\\right) \\cdot \\exp \\left(-C_{\\mathrm{MLS} t} t\\right)\n$$\n\nMeasure decompositions. Some of the key properties of locally stationary distributions rely on the notion of a measure decomposition. These can be defined in great generality, but we will restrict our attention to distributions on subsets of $\\mathbb{R}^{n}$ for concreteness.\n\nDefinition 2.10 (Measure decomposition). Let $\\pi$ be a distribution on $\\mathbb{R}^{n}$. Let $\\rho$ be a mixture distribution, also on $\\mathbb{R}^{n}$, which indexes into a family of mixture components $\\left\\{\\pi_{z}\\right\\}_{z \\in \\mathbb{R}^{n}}$. We say that $\\left(\\rho, \\pi_{z}\\right)$ is a measure decomposition for $\\pi$ if\n\n$$\n\\pi=\\mathbf{E}_{z \\sim \\rho} \\pi_{z}\n$$\n\nOne should think of the mixture components $\\pi_{z}$ as being \"simpler\" distributions than the original measure $\\pi$. Not all measure decompositions are useful; there is always a trivial measure decomposition where the mixture $\\rho$ is exactly $\\pi$ and the simpler distributions $\\pi_{z}$ are just Dirac masses at $z$.\n\nAssociated to each measure decomposition is a natural Markov chain; see, e.g., [CE22, Definition 6].\n\nDefinition 2.11 (Markov chain associated to a measure decomposition). Given a measure decomposition $\\pi=\\mathbf{E}_{z \\sim \\rho} \\pi_{z}$, its associated Markov chain is defined by\n\n$$\n\\boldsymbol{x} \\rightarrow \\boldsymbol{z}\\left|\\boldsymbol{x} \\rightarrow \\boldsymbol{x}^{\\prime}\\right| \\boldsymbol{z}\n$$\n\nNotably, Glauber dynamics and restricted Gaussian dynamics can both be viewed as the associated Markov chain to certain measure decompositions. The relevant decomposition for Glauber dynamics represents $\\pi$ as the mixture of its conditional marginals.\n\nRemark 2.12. Measure decompositions constructed using stochastic localization have recently been used to prove functional inequalities for a wide class of Ising models [EKZ22, CE22, LMRW24]. We will discuss these properties in greater depth in Section 5.1.\n\nSymmetric KL divergence. A useful potential function for us is the symmetric KL divergence.\nDefinition 2.13. For a pair of distributions $\\pi$ and $v$ on $\\Omega$, we define their symmetric KL divergence as:\n\n$$\n\\operatorname{SKL}(\\pi, v):=\\operatorname{KL}(v \\| \\pi)+\\operatorname{KL}(\\pi \\| v)\n$$\n\nObservation 2.14. For any $\\pi$ and $v$, setting $f$ to be the density of $v$ with respect to $\\pi$, we have\n\n$$\n\\operatorname{SKL}(\\pi, v)=\\frac{1}{2} \\cdot \\mathbf{E}_{x, y \\sim \\pi}\\left[(f(\\boldsymbol{x})-f(\\boldsymbol{y})) \\cdot \\log \\frac{f(\\boldsymbol{x})}{f(\\boldsymbol{y})}\\right]\n$$\n\nObserve that the above quantity is the Dirichlet form for the trivial \"one-step\" Markov chain with transition matrix $\\mathbf{1} \\pi^{\\top}$ with stationary distribution $\\pi$, which we shall denote $K(\\pi)$.\n\nLemma 2.15. Let $v$ be an arbitrary distribution with density $f$ with respect to $\\pi$, and $\\tau$ such that $\\tau>$ $\\max _{x \\in \\Omega} \\log f(x)$ or $\\tau>\\max _{x \\in \\Omega} \\log \\frac{1}{f(x)}$. Then, the symmetric KL divergence can be bounded in terms of the KL divergence as follows.\n\n$$\n\\operatorname{SKL}(\\pi, v) \\leqslant(6+12 \\tau) \\cdot \\mathrm{KL}(v \\| \\pi)\n$$\n\nProof. Recall that the Hellinger distance is defined as\n\n$$\n\\mathrm{H}^{2}(v, \\pi):=\\frac{1}{2} \\cdot \\mathrm{E}_{\\underset{y \\sim \\pi}{x \\sim \\pi}}\\left[\\left(\\sqrt{f(y)}-\\sqrt{f(x)}\\right)^{2}\\right]\n$$\n\nThe symmetric KL divergence may be bounded by the Hellinger distance as follows.\n\n$$\n\\begin{aligned}\n& 2 \\cdot \\operatorname{SKL}(v, \\pi)=\\underset{y \\sim \\pi}{\\mathrm{E}_{x \\sim \\pi}}\\left[(f(y)-f(x))(\\log f(y)-\\log f(x))\\right] \\\\\n& =\\underset{y \\sim \\pi}{\\mathrm{E}_{x \\sim \\pi}}\\left[(f(y)-f(x)) \\cdot \\log \\frac{f(y)}{f(x)} \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\in\\left[\\frac{1}{2}, 2\\right]\\right]\\right] \\\\\n& +\\underset{y \\sim \\pi}{\\mathrm{E}_{x \\sim \\pi}}\\left[(f(y)-f(x)) \\cdot \\log \\frac{f(y)}{f(x)} \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\notin\\left[\\frac{1}{2}, 2\\right]\\right]\\right] \\\\\n& =\\underset{y \\sim \\pi}{\\mathrm{E}_{x \\sim \\pi}}\\left[(f(y)-f(x)) \\cdot \\log \\frac{f(y)}{f(x)} \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\in\\left[\\frac{1}{2}, 2\\right]\\right]\\right] \\\\\n& +2 \\cdot \\mathrm{E}_{y \\sim \\pi}\\left[(f(y)-f(x)) \\cdot \\log f(y) \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\notin\\left[\\frac{1}{2}, 2\\right]\\right]\\right] .\n\\end{aligned}\n$$\n\nTo control the first term, we note that for $t \\in\\left[\\frac{1}{2}, 2\\right],(t-1) \\log t \\leqslant 6(\\sqrt{t}-1)^{2}$. Consequently,\n\n$$\n\\begin{aligned}\n\\mathrm{E}_{\\underset{y \\sim \\pi}{x \\sim \\pi}}\\left[(f(y)-f(x)) \\cdot \\log \\frac{f(y)}{f(x)} \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\in\\left[\\frac{1}{2}, 2\\right]\\right]\\right] \\\\\n& \\leqslant 6 \\cdot \\mathrm{E}_{y \\sim \\pi}\\left[\\left(\\sqrt{f(y)}-\\sqrt{f(x)}\\right)^{2} \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\in\\left[\\frac{1}{2}, 2\\right]\\right]\\right] \\\\\n& \\leqslant 12 \\mathrm{H}^{2}(v, \\pi)\n\\end{aligned}\n$$\n\nTo control the second term, we first have\n\n$$\n\\mathrm{E}_{\\underset{y \\sim \\pi}{x \\sim \\pi}}\\left[(f(y)-f(x)) \\cdot \\log f(y) \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\notin\\left[\\frac{1}{2}, 2\\right]\\right]\\right] \\leqslant \\tau \\cdot \\mathrm{E}_{y \\sim \\pi}\\left[|f(y)-f(x)| \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\notin\\left[\\frac{1}{2}, 2\\right]\\right]\\right]\n$$\n\nFurthermore, for $t \\notin\\left[\\frac{1}{2}, 2\\right],|t-1| \\leqslant 6(\\sqrt{t}-1)^{2}$. Consequently,\n\n$$\n\\begin{aligned}\n\\mathrm{E}_{\\underset{y \\sim \\pi}{x \\sim \\pi}}\\left[|f(y)-f(x)| \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\notin\\left[\\frac{1}{2}, 2\\right]\\right]\\right] & \\leqslant 6 \\cdot \\mathrm{E}_{y \\sim \\pi}\\left[\\left(\\sqrt{f(y)}-\\sqrt{f(x)}\\right)^{2} \\cdot \\mathbf{1}\\left[\\frac{f(y)}{f(x)} \\notin\\left[\\frac{1}{2}, 2\\right]\\right]\\right] \\\\\n& \\leqslant 12 \\mathrm{H}^{2}(v, \\pi)\n\\end{aligned}\n$$\n\nPutting this together, we get that\n\n$$\n\\operatorname{SKL}(v, \\pi) \\leqslant(6+12 \\tau) \\cdot \\mathrm{H}^{2}(v, \\pi)\n$$\n\nTo complete the proof, we use the well-known fact that $\\mathrm{H}^{2}(v, \\pi) \\leqslant \\mathrm{KL}(v \\| \\pi)$ ([HW58], also see [SV16, Equation (16)]).", "tables": {}, "images": {}}, {"section_id": 8, "text": "# 3 Properties of locally stationary distributions \n\nWe record some useful properties of locally stationary distributions below.\nRandom walks yield locally-stationary measures at a typical time. The following is a generic statement about any time-reversible Markov chain achieving a locally stationary distribution.\n\nLemma 3.1. For any distribution $v$, any Markov chain transition kernel $P$ with stationary distribution $\\pi$ and any $T>0$, for $t$ chosen uniformly at random in $[0, T]$ :\n\n$$\n\\mathbf{E}_{t \\sim[0, T]} \\mathcal{E}_{P}\\left(f_{t}, \\log f_{t}\\right) \\leqslant \\frac{\\mathrm{KL}(v \\| \\pi)}{T} \\leqslant \\frac{\\log \\frac{1}{\\pi_{\\min }}}{T}\n$$\n\nProof. By Fact 2.5,\n\n$$\n\\begin{aligned}\n0 & \\leqslant \\mathrm{KL}\\left(v_{T} \\| \\pi\\right) \\\\\n& =\\mathrm{KL}(v \\| \\pi)-\\int_{0}^{T} \\mathcal{E}\\left(f_{t}, \\log f_{t}\\right) \\mathrm{d} t \\\\\n& =\\mathrm{KL}(v \\| \\pi)-T \\cdot \\mathbf{E}_{t \\sim[0, T]} \\mathcal{E}\\left(f_{t}, \\log f_{t}\\right) \\\\\n& \\leqslant \\log \\frac{1}{\\pi_{\\min }}-T \\cdot \\mathbf{E}_{t \\sim[0, T]} \\mathcal{E}\\left(f_{t}, \\log f_{t}\\right)\n\\end{aligned}\n$$\n\nRearranging the above gives us the desired statement.\nA simple consequence of Lemma 3.1 and Markov's inequality is that for most times in $[0, T], v_{t}$ is indeed locally stationary.\n\nTheorem 1.3. Fix a time-reversible Markov chain $P$ with a stationary measure $\\pi$, any starting distribution $v_{0}$, and $\\varepsilon, \\delta>0$. Let $T=\\frac{1}{\\delta \\varepsilon} \\cdot \\log \\left(\\frac{1}{\\pi_{\\min }}\\right)$. Then, for a time $t \\sim[0, T]$ chosen uniformly at random, the distribution $v_{t}$ at time $t$ is $\\varepsilon$-locally stationary with respect to $P$ with probability at least $1-\\delta$.\n\nStationarity over small time-scales. We will also require the observation that if the Dirichlet form at a distribution is small, so too is the total variation distance between it and the distribution obtained after one step of the Markov chain.\n\nLemma 3.2. Let $P$ be a reversible Markov chain with stationary distribution $\\pi, v$ an arbitrary distribution, and $f$ its density relative to $\\pi$. Then,\n\n$$\n\\mathcal{E}_{P}(f, \\log f) \\geqslant 2 \\cdot \\mathrm{KL}(P v \\| v) \\geqslant 4 \\cdot d_{\\mathrm{TV}}(v, P v)^{2}\n$$\n\nProof. We recall the definition of the Dirichlet form,\n\n$$\n\\begin{aligned}\n\\mathcal{E}_{P}(f, \\log f) & =\\mathbf{E}_{\\substack{x \\sim \\pi \\\\\ny \\sim p x}}(f(x)-f(y)) \\log \\frac{f(x)}{f(y)} \\\\\n& =2 \\cdot \\mathbf{E}_{\\substack{x \\sim \\pi \\\\\ny \\sim p x^{\\prime}}} f(x) \\log \\left(\\frac{f(x)}{f(y)}\\right) \\\\\n& =2 \\cdot \\mathbf{E}_{\\substack{x \\sim v \\\\\ny \\sim p x}} \\log \\left(\\frac{f(x)}{f(y)}\\right)\n\\end{aligned}\n$$\n\n(Reversibility of $P$ )\n\n$$\n=2 \\cdot\\left(\\mathbf{E}_{x \\sim v} \\log f(x)-\\mathbf{E}_{y \\sim P v} \\log f(y)\\right)\n$$\n\nAdding and subtracting a term, we may neatly express the above in terms of KL divergences as\n\n$$\n\\begin{aligned}\n\\mathcal{E}_{P}(f, \\log f) & =2 \\cdot\\left[\\left(\\mathbf{E}_{x \\sim v} \\log f(x)-\\mathbf{E}_{y \\sim P v} \\log \\frac{\\mathrm{~d} P v}{\\mathrm{~d} \\pi}(y)\\right)+\\left(\\mathbf{E}_{y \\sim P v} \\log \\frac{\\mathrm{~d} P v}{\\mathrm{~d} \\pi}(y)-\\mathbf{E}_{y \\sim P v} \\log f(y)\\right)\\right] \\\\\n& =2 \\cdot[(\\operatorname{KL}(v \\| \\pi)-\\operatorname{KL}(P v \\| \\pi))+\\operatorname{KL}(P v \\| v)] \\\\\n& \\geqslant 2 \\cdot \\operatorname{KL}(P v \\| v) \\geqslant 4 \\cdot d_{\\mathrm{TV}}(P v, v)^{2}\n\\end{aligned}\n$$\n\nas desired, where the second-to-last inequality follows from the fact that the KL divergence to the stationary distribution is non-increasing with time, and the last inequality is Pinsker's.\n\nConsequently, averages of bounded functions do not change much after one step of the Markov chain\n\nCorollary 3.3. Let $\\phi: \\Omega \\rightarrow \\mathbb{R}$ be a bounded function on the state space of a Markov chain $P$, and $v$ be an $\\varepsilon$-locally stationary measure. Then,\n\n$$\n\\left|\\mathbf{E}_{x \\sim v}[\\phi(x)]-\\mathbf{E}_{x \\sim P v}[\\phi(x)]\\right| \\leqslant\\|\\phi\\|_{\\infty} \\cdot \\sqrt{\\varepsilon}\n$$\n\nLocally stationary measures are close to stationary measures on small neighborhoods. As their name suggests, locally stationary distributions locally resemble the true stationary distribution. For example, typical samples from the locally stationary distribution approximately satisfies the detailed balance condition. Even though we do not explicitly employ this in any applications, we include it here as it gives the impression of a fundamental structural property of locally stationary distributions.\n\nLemma 3.4. For an $\\varepsilon$-locally stationary distribution $v$ with relative density $f$, and for $x \\sim v$ and $y \\sim x$, with probability at least $1-\\delta$, we have $\\frac{f(x)}{f(y)}=1 \\pm O(\\sqrt{\\frac{\\delta}{\\delta}})$, where $\\delta>2 \\varepsilon$.\n\nProof. Since $\\mathcal{E}(f, \\log f)<\\varepsilon$,\n\n$$\n\\begin{aligned}\n& \\mathbf{E}_{x \\sim \\pi} \\mathbf{E}_{y \\sim x}(f(x)-f(y)) \\cdot \\log \\frac{f(x)}{f(y)}<\\varepsilon \\\\\n& \\mathbf{E}_{x \\sim v} \\mathbf{E}_{y \\sim x}\\left(1-\\frac{f(y)}{f(x)}\\right) \\cdot \\log \\frac{f(x)}{f(y)}<\\varepsilon\n\\end{aligned}\n$$\n\nSince the random variable at hand is always nonnegative, we can apply Markov's inequality, which tells us that with probability at least $1-\\delta$ :\n\n$$\n\\left(1-\\frac{f(y)}{f(x)}\\right) \\cdot \\log \\frac{f(x)}{f(y)}<\\frac{\\varepsilon}{\\delta}\n$$\n\nThe claim then follows since the above inequality is violated if $\\frac{f(x)}{f(y)}$ deviates from 1 by more than a constant multiple of $\\sqrt{\\frac{\\varepsilon}{\\delta}}$.\n\nFormally, the following can be abstracted out of the proof of Theorem 1.4:\n\nLemma 3.5. Let $P$ be a Glauber dynamics chain for a distribution $\\mu$ on $\\{ \\pm 1\\}^{n}$, and let $\\nu$ be an $\\varepsilon$-locally stationary measure with respect to $P$ for some $\\varepsilon>0$. For a subset of coordinates $W \\subset[n]$, and an assignment $x_{\\overline{\\mathrm{W}}}$ of coordinates outside $W$, let $P_{\\mathrm{W}, x_{\\overline{\\mathrm{W}}}}$ denote the Glauber dynamics chain of $\\mu \\mid x_{\\overline{\\mathrm{W}}}$. Suppose for every choice of $W$ and $x_{\\overline{\\mathrm{W}}}$, we have $C_{\\mathrm{MLSI}}\\left(P_{\\mathrm{W}, x_{\\overline{\\mathrm{W}}}}\\right) \\geqslant C$, then:\n\n$$\n\\mathbf{E}_{x_{\\overline{\\mathrm{W}}} \\sim v}\\left[d_{\\mathrm{TV}}\\left(v_{\\mid x_{\\overline{\\mathrm{W}}}}, \\pi_{\\mid x_{\\overline{\\mathrm{W}}}}\\right)\\right] \\leqslant \\frac{1}{C} \\cdot \\sqrt{\\varepsilon}\n$$\n\nCorollary 3.6. In the setting of Lemma 3.5, suppose $\\phi:\\{0,1\\}^{W} \\rightarrow \\mathbb{R}$ is a bounded function of $x_{W}$ then,\n\n$$\n\\mathbf{E}_{x \\sim v}\\left[\\phi\\left(x_{W}\\right)\\right] \\geqslant \\mathbf{E}_{x_{\\overline{\\mathrm{W}}}} \\mathbf{E}_{x_{W} \\sim \\pi \\mid x_{\\overline{\\mathrm{W}}}}\\left[\\phi\\left(x_{W}\\right)\\right]-\\frac{1}{\\mathrm{C}} \\cdot \\sqrt{\\varepsilon} \\cdot\\|\\phi\\|_{\\infty}\n$$\n\nLocal stationarity is preserved over short time-scales of the random walk. The following lemma essentially says that if a distribution $v$ is locally stationary, then so is $P^{T} v$ for any small $T$.\n\nLemma 3.7. Let $P$ be a Markov chain with stationary distribution $\\pi$, and $v$ an arbitrary distribution with relative density $f$. Then,\n\n$$\n\\mathcal{E}_{P^{T}}(f, \\log f) \\leqslant O\\left(T^{3}\\right) \\cdot \\mathcal{E}_{P}(f, \\log f)\n$$\n\nProof. Suppose we pick $\\boldsymbol{x} \\sim \\pi, \\boldsymbol{y}_{1} \\sim_{P} \\boldsymbol{x}$, then $\\boldsymbol{y}_{2} \\sim_{P} \\boldsymbol{y}_{1}$. Then, because $\\pi$ is stationary with respect to $P$,\n\n$$\n\\mathcal{E}_{P}(f, \\log f)=\\mathbf{E}\\left[\\left(f(\\boldsymbol{x})-f\\left(\\boldsymbol{y}_{1}\\right)\\right) \\log \\frac{f(\\boldsymbol{x})}{f\\left(\\boldsymbol{y}_{1}\\right)}\\right]=\\mathbf{E}\\left[\\left(f\\left(\\boldsymbol{y}_{1}\\right)-f\\left(\\boldsymbol{y}_{2}\\right)\\right) \\log \\frac{f\\left(\\boldsymbol{y}_{1}\\right)}{f\\left(\\boldsymbol{y}_{2}\\right)}\\right]\n$$\n\nTherefore,\n\n$$\n\\begin{aligned}\n2 \\cdot \\mathcal{E}_{P}(f, \\log f) & =\\mathbf{E}\\left[\\left(f(\\boldsymbol{x})-f\\left(\\boldsymbol{y}_{1}\\right)\\right) \\log \\frac{f(\\boldsymbol{x})}{f\\left(\\boldsymbol{y}_{1}\\right)}+\\left(f\\left(\\boldsymbol{y}_{1}\\right)-f\\left(\\boldsymbol{y}_{2}\\right)\\right) \\log \\frac{f\\left(\\boldsymbol{y}_{1}\\right)}{f\\left(\\boldsymbol{y}_{2}\\right)}\\right] \\\\\n& \\geqslant \\frac{1}{4} \\cdot \\mathbf{E}\\left[\\left(f(\\boldsymbol{x})-f\\left(\\boldsymbol{y}_{2}\\right)\\right) \\log \\frac{f(\\boldsymbol{x})}{f\\left(\\boldsymbol{y}_{2}\\right)}\\right] \\\\\n& =\\frac{1}{4} \\cdot \\mathcal{E}_{P^{2}}(f, \\log f)\n\\end{aligned}\n$$\n\nHere, the inequality follows from the observation that defining $d(p, q)=(p-q) \\log (p / q), d(p, q)+$ $d(q, r) \\geqslant \\frac{1}{4} \\cdot d(p, r)$. Indeed, if $q \\notin(p, r)$, this is trivial, so assuming without loss of generality that $r \\geqslant q \\geqslant \\frac{p+r}{2} \\geqslant p$, we get that\n\n$$\nd(p, q) \\geqslant\\left(\\frac{p+r}{2}-p\\right) \\log \\left(\\frac{p+r}{2 p}\\right)=\\frac{1}{2} \\cdot(r-p) \\cdot \\log \\left(\\frac{1}{2} \\cdot\\left(1+\\frac{r}{p}\\right)\\right) \\geqslant \\frac{1}{4} \\cdot(r-p) \\log \\left(\\frac{r}{p}\\right)\n$$\n\nApplying this $\\log T$ times, it follows that\n\n$$\n\\mathcal{E}_{P^{T}}(f, \\log f) \\leqslant O\\left(T^{3}\\right) \\cdot \\mathcal{E}_{P}(f, \\log f)\n$$\n\nLocal stationarity can be transferred under component MLSI. Let $\\pi=\\mathbf{E}_{z \\sim \\rho} \\pi_{z}$ be a measure decomposition. In this section we prove that if Glauber dynamics is locally stationary, so too is the Markov chain associated with the measure decomposition, provided that the mixture components $\\pi_{z}$ all have a good MLSI constant.\n\nLemma 3.8. Let $P$ be the Markov chain associated to a measure decomposition $\\pi=\\mathbf{E}_{z \\sim \\rho} \\pi_{z}$. Let $f$ : $\\{ \\pm 1\\}^{n} \\rightarrow \\mathbb{R}_{>0}$ be any function and set $\\tau$ such that $\\min _{x \\in\\{ \\pm 1\\}^{n}} f(x)>\\exp (-\\tau)$ or $\\max _{x \\in\\{ \\pm 1\\}^{n}} f(x)<$ $\\exp (\\tau)$. For $\\delta:=\\inf _{z} \\mathrm{C}_{\\mathrm{MLSI}}\\left(\\pi_{z}\\right)$, we have\n\n$$\n\\mathcal{E}_{P}(f, \\log f) \\leqslant O\\left(\\frac{\\tau}{\\delta}\\right) \\cdot \\mathcal{E}_{\\pi}(f, \\log f)\n$$\n\nProof. We use $\\mathcal{C}_{n}$ to denote the hypercube graph on vertex set $\\{ \\pm 1\\}^{n}$ with edge set having pairs of vertices that differ in a single coordinate. For any nonnegative function $f$ and distribution $\\pi$ with $\\mathbf{E}_{\\pi} f=1$, we use $f \\cdot \\pi$ to denote the distribution $\\nu$ with $\\frac{\\mathrm{d} \\nu}{\\mathrm{~d} \\pi}(x)=f(x)$.\n\nFor any function $f$ satisfying the assumption of the statement, we have\n\n$$\n\\begin{aligned}\n\\mathcal{E}_{\\pi}(f, \\log f) & =\\sum_{\\{x, y\\} \\in \\mathcal{C}_{n}} \\frac{1}{n} \\cdot \\frac{\\pi(x) \\cdot \\pi(y)}{\\pi(x)+\\pi(y)} \\cdot(f(x)-f(y)) \\log \\frac{f(x)}{f(y)} \\\\\n& =\\sum_{\\{x, y\\} \\in \\mathcal{C}_{n}} \\frac{1}{n} \\cdot \\frac{\\mathbf{E}_{z \\sim \\rho} \\pi_{z}(x) \\cdot \\mathbf{E}_{z \\sim \\rho} \\pi_{z}(y)}{\\mathbf{E}_{z \\sim \\rho} \\pi_{z}(x)+\\mathbf{E}_{z \\sim \\rho} \\pi_{z}(y)} \\cdot(f(x)-f(y)) \\log \\frac{f(x)}{f(y)} \\\\\n& \\geqslant \\sum_{\\{x, y\\} \\in \\mathcal{C}_{n}} \\frac{1}{n} \\cdot \\mathbf{E}_{z \\sim \\rho}\\left[\\frac{\\pi_{z}(x) \\cdot \\pi_{z}(y)}{\\pi_{z}(x)+\\pi_{z}(y)}\\right] \\cdot(f(x)-f(y)) \\log \\frac{f(x)}{f(y)} \\\\\n& =\\mathbf{E}_{z \\sim \\rho}\\left[\\mathcal{E}_{\\pi_{z}}(f, \\log f)\\right]\n\\end{aligned}\n$$\n\nAbove, the inequality follows from the concavity of the function $(a, b) \\mapsto \\frac{a b}{a+b}$ in the non-negative quadrant, and all the Dirichlet forms are with respect to the Glauber dynamics chain.\n\nContinuing the above calculation,\n\n$$\n\\begin{aligned}\n\\mathbf{E}_{z \\sim \\rho}\\left[\\mathcal{E}_{\\pi_{z}}(f, \\log f)\\right] & \\geqslant \\mathbf{E}_{z \\sim \\rho}\\left[C_{\\mathrm{MLSI}}\\left(\\pi_{z}\\right) \\cdot \\mathbf{E}_{\\pi_{z}}[f] \\cdot \\mathrm{KL}\\left(\\frac{f}{\\mathbf{E}_{\\pi_{z}} f} \\cdot \\pi_{z} \\| \\pi_{z}\\right)\\right] \\\\\n& \\geqslant \\Omega\\left(\\frac{\\delta}{\\tau}\\right) \\mathbf{E}_{z \\sim \\rho}\\left[\\mathbf{E}_{\\pi_{z}}[f] \\cdot \\operatorname{SKL}\\left(\\frac{f}{\\mathbf{E}_{\\pi_{z}} f} \\cdot \\pi_{z}, \\pi_{z}\\right)\\right] \\quad \\text { (by Lemma 2.15 and MLSI) } \\\\\n& =\\Omega\\left(\\frac{\\delta}{\\tau}\\right) \\mathbf{E}_{z \\sim \\rho}\\left[\\mathbf{E}_{x, y \\sim \\pi_{z}}(f(\\boldsymbol{x})-f(\\boldsymbol{y})) \\log \\frac{f(\\boldsymbol{x})}{f(\\boldsymbol{y})}\\right] \\quad \\text { (by Observation 2.14) } \\\\\n& =\\Omega\\left(\\frac{\\delta}{\\tau}\\right) \\mathcal{E}_{P}(f, \\log f)\n\\end{aligned}\n$$\n\nThe claim follows.\nThe upshot is that we have complete freedom to select the measure decomposition, provided we can establish an MLSI for the components. This can be useful when it is easier to directly analyze the consequences of local stationarity for the associated Markov chain instead of Glauber dynamics.", "tables": {}, "images": {}}, {"section_id": 9, "text": "# 4 Warmup: Large independent sets in triangle-free graphs \n\nObserve that any graph $G$ on $n$ vertices with maximum degree $d$ has an independent set of size $\\frac{n}{d+1}$, a bound which is tight for the disjoint union of $(d+1)$-sized cliques. Ajtai, Koml\u00f3s, and Szemer\u00e9di [AKS80] showed that when $G$ is triangle-free, the size of the maximum independent set increases\n\nto $\\Omega\\left(n \\cdot \\frac{\\log d}{d}\\right)$. Shearer [She83] gave an alternate proof that shows such an independent set exists with a leading constant of 1 , even if $G$ merely has average-degree bounded by $d$. As a warmup, we prove that Glauber dynamics succeeds at finding a large independent set in $O\\left(n d^{4}\\right)$ steps.\n\nTheorem 1.4. Let $G$ be a triangle-free graph on $n$ vertices with maximum degree bounded by $d$. Let $\\boldsymbol{I}$ be an independent set in $G$ that arises from Glauber dynamics run for $O\\left(n d^{4}\\right)$ time. Then the expected size of $\\boldsymbol{I}$ is at least $\\frac{1-n_{0}(1)}{4} \\cdot n \\cdot \\frac{\\log d}{d}$.\n\nTo prove Theorem 1.4, we will need the following crude bound on the modified log-Sobolev constant for the uniform distribution over independent sets of a star. A short proof is provided at the end of this section.\n\nLemma 4.1. Let $\\pi$ denote the uniform distribution over independent sets of a star with $\\Delta$ many leaves. Then $C_{\\mathrm{MLSI}}(\\pi) \\geqslant \\exp (-O(\\Delta))$.\n\nRemark 4.2. The bound can easily be made $\\frac{1}{\\operatorname{poly}(\\Delta)}$, but we will not need this here.\nWe also leverage the following simple and well-known lemma on the local behavior of a uniformly random independent set. For completeness, we include a short proof of it at the end of this section, following the one provided in Alon \\& Spencer [AS16, Proposition 1, Page 272]. Throughout this section, we write $N(v)=\\{u \\in V: u \\sim v\\}$ for the open neighborhood of $v \\in V$, and $N[v]=N(v) \\cup\\{v\\}$ for the closed neighborhood.\n\nLemma 4.3. Let $G$ be a triangle-free graph of maximum degree $d$, and let $\\pi$ denote the uniform measure over independent sets of $G$. For every vertex $v \\in V$, define the following real-valued score function over $\\{0,1\\}^{V}:$\n\n$$\n\\phi_{v}(\\boldsymbol{x}):=d \\boldsymbol{x}_{v}+\\sum_{u \\in N(v)} \\boldsymbol{x}_{u}\n$$\n\nThen for every pinning $\\tau \\in\\{0,1\\}^{\\overline{N[v]}}$, we have\n\n$$\n\\mathbf{E}_{\\boldsymbol{x} \\sim \\pi}\\left[\\phi_{v}(\\boldsymbol{x}) \\mid \\boldsymbol{x}_{\\overline{N[v]}}=\\tau\\right] \\geqslant \\frac{\\log d}{2}\n$$\n\nThe key property of this score function is that it readily yields a lower bound on the size of an independent set $x \\in\\{0,1\\}^{V}$. This follows from the observation that\n\n$$\nn \\cdot \\mathbf{E}_{v \\sim V} \\phi_{v}(\\boldsymbol{x}) \\leqslant 2 d \\cdot \\sum_{v \\in V} \\boldsymbol{x}_{v}\n$$\n\nNote that by averaging over $\\tau \\in\\{0,1\\}^{\\overline{N[v]}}$ drawn from the marginal distribution of $\\pi$ induced on $\\overline{N[v]}$, the conclusion of Lemma 4.3 combined with Eq. (3) implies that a uniformly random independent set drawn from $\\pi$ has expected size at least $\\frac{1}{4} \\cdot n \\cdot \\frac{\\log d}{d}$. We observe that the same claim holds even if the distribution over independent sets is merely locally stationary with respect to Glauber dynamics, rather than being truly uniform.\n\nProof of Theorem 1.4. As discussed above, a direct application of the law of total expectation combined with Lemma 4.3 yields the first claim concerning the expected size of a uniformly random\n\nindependent set. We now turn to the second claim. Let $T \\geqslant 0$ be a parameter to be determined later, and for every $0 \\leqslant t \\leqslant T$, let $v_{t}$ denote the distribution over independent sets after running Glauber dynamics for time- $t$ from an arbitrary initialization. Our goal is to establish the lower bound\n\n$$\n\\mathbf{E}_{t \\sim[0, T]} \\mathbf{E}_{v_{t}} \\mathbf{E}_{v \\sim V} \\phi_{v}(\\boldsymbol{x}) \\geqslant \\frac{\\log d}{2}-\\varepsilon\n$$\n\nfor $0<\\varepsilon<o_{d}(1)$, which when combined with Eq. (3) immediately implies that the expected size of the independent set discovered by Glauber dynamics is $\\left(\\frac{1-o_{d}(1)}{4}\\right) \\cdot n \\cdot \\frac{\\log d}{d}$. For the purpose of analysis, if $v$ is any distribution over independent sets, we shall think of $\\boldsymbol{x} \\sim v$ as being sampled in the following alternate way.\n\n1. For a fixed vertex $v$, sample $x_{\\overline{N[v]}}$ from the marginal distribution induced by $v$ on $\\overline{N[v]}$. For each $w \\in N(v)$ that has a neighbor in the independent set $x_{\\overline{N[v]}}$, pin $x_{w}$ to 0 , since it is deterministically equal to 0 in the conditional measure $v \\mid x_{\\overline{N[v]}}$.\n2. If the number of unpinned vertices at this stage is strictly larger than $\\log d$, sample $x_{v}$ from its corresponding conditional marginal distribution.\n3. Let $U$ be the set of remaining unpinned vertices. Sample $x_{U} \\sim v \\mid x_{\\bar{U}}$.\n\nFor any vertex $v \\in V$, we have\n\n$$\n\\begin{aligned}\n\\mathbf{E}_{x \\sim v} \\phi_{v}(\\boldsymbol{x}) & =\\mathbf{E}_{U, x_{\\bar{\\nabla}} \\mid v} \\mathbf{E}_{v \\mid x_{\\bar{\\nabla}}} \\phi_{v}(\\boldsymbol{x}) \\\\\n& \\geqslant \\mathbf{E}_{U, x_{\\bar{\\nabla}} \\mid v}\\left[\\mathbf{E}_{\\pi \\mid x_{\\bar{\\nabla}}} \\phi_{v}(\\boldsymbol{x})-2 d \\cdot d_{\\mathrm{TV}}\\left(v \\mid x_{\\bar{\\nabla}}, \\pi \\mid x_{\\bar{\\nabla}}\\right)\\right] \\\\\n& \\geqslant \\frac{\\log d}{2}-2 d \\cdot \\mathbf{E}_{U, x_{\\bar{\\nabla}} \\mid v} d_{\\mathrm{TV}}\\left(v \\mid x_{\\bar{\\nabla}}, \\pi \\mid x_{\\bar{\\nabla}}\\right)\n\\end{aligned}\n$$\n\nNote that the random subset of vertices $U$, as well as the boundary condition $x_{\\bar{\\nabla}}$, are all drawn from the above process with respect to $v$, not $\\pi$. The first inequality follows by applying Fact 2.2 along with $2 d$-boundedness and nonnegativity of the score function $\\phi_{v}$. For the second inequality, note that if $v \\in U$, then we may invoke Lemma 4.3. Now suppose $v \\notin U$. If $v$ is pinned 1 , then $\\phi_{v}(\\boldsymbol{x})=d$. If $v$ is pinned 0 , then by triangle-freeness, $x_{u}=1$ with probability $1 / 2$ independently for all $u \\in U$. Since $|U| \\geqslant \\log d$, the lower bound follows.\n\nIn the rest of this argument, we will show that when $\\boldsymbol{t} \\sim[0, T], v$ is equal to $v_{t}$ and $v$ is chosen uniformly at random, we can achieve a strong upper bound on\n\n$$\n\\mathbf{E}_{t} \\mathbf{E}_{v} \\mathbf{E}_{U, x_{\\bar{\\nabla}} \\mid v} d_{\\mathrm{TV}}\\left(v_{t} \\mid \\boldsymbol{x}_{\\bar{\\nabla}}, \\pi \\mid \\boldsymbol{x}_{\\bar{\\nabla}}\\right)\n$$\n\nFor the rest of this proof, we shall abbreviate $v_{t} \\mid x_{\\bar{\\nabla}}$ and $\\pi \\mid x_{\\bar{\\nabla}}$ as $v_{t}^{\\prime}$ and $\\pi^{\\prime}$, respectively. Furthermore, let $f_{t}^{\\prime}$ denote the relative density of $v_{t}^{\\prime}$ with respect to $\\pi^{\\prime}$. By Pinsker's inequality, we can bound the above by:\n\n$$\n\\mathbf{E}_{t} \\mathbf{E}_{v} \\mathbf{E}_{U, x_{\\bar{\\nabla}} \\mid v} \\sqrt{\\mathrm{KL}\\left(v_{t}^{\\prime} \\| \\pi^{\\prime}\\right)} \\leqslant \\sqrt{\\mathbf{E}_{t} \\mathbf{E}_{v} \\mathbf{E}_{U, x_{\\bar{\\nabla}} \\mid v} \\mathrm{KL}\\left(v_{t}^{\\prime} \\| \\pi^{\\prime}\\right)}\n$$\n\nWe focus our attention on showing an upper bound on the term in the square-root.\n\n$$\n\\mathbf{E}_{t} \\mathbf{E}_{v \\sim V} \\mathbf{E}_{U, x_{\\bar{\\nabla}} \\mid v} \\mathrm{KL}\\left(v_{t}^{\\prime} \\| \\pi^{\\prime}\\right) \\leqslant \\mathbf{E}_{t} \\mathbf{E}_{v} \\mathbf{E}_{U, x_{\\bar{\\nabla}} \\mid v} \\frac{1}{\\mathrm{C}_{\\mathrm{MLSt}}\\left(\\pi^{\\prime}\\right)} \\mathcal{E}_{\\pi^{\\prime}}\\left(f_{t}^{\\prime}, \\log f_{t}^{\\prime}\\right)\n$$\n\n$$\n\\begin{aligned}\n& \\leqslant O(d) \\cdot \\mathbf{E}_{I} \\mathbf{E}_{v} \\mathbf{E}_{U, x_{n} \\mid v} \\mathcal{E}_{\\pi^{\\prime}}\\left(f_{I}^{\\prime}, \\log f_{I}^{\\prime}\\right) \\\\\n& \\leqslant O\\left(d^{2}\\right) \\cdot \\mathbf{E}_{I} \\mathcal{E}_{\\pi}\\left(f_{I}, \\log f_{I}\\right) \\\\\n& \\leqslant O\\left(d^{2}\\right) \\cdot \\frac{n}{T}\n\\end{aligned}\n$$\n\nIn the above, the first inequality uses the definition of $C_{\\mathrm{MLSI}}\\left(\\pi^{\\prime}\\right)$. For the second inequality, note that almost surely, either $G[U]$ is a star centered at $v$ with at most $\\log d$ many leaves, or $G[U]$ consists entirely of isolated vertices due to pinning $v$. In either case, we have $C_{\\mathrm{MLSI}}\\left(\\pi^{\\prime}\\right) \\geqslant \\Omega\\left(\\frac{1}{d}\\right)$ by appropriately applying Fact 2.7 or Lemma 4.1. The third inequality is based on comparing Dirichlet forms. The final inequality is a direct application of Lemma 3.1. Plugging in the above into (5) and setting $T=O\\left(n d^{4} / \\varepsilon^{2}\\right)$, we get Eq. (4) as desired.\n\nRemark 4.4. Using a similar argument, one can establish a similar result for Max-Cut on trianglefree graphs with maximum degree $d$. In particular, Glauber dynamics run for poly $(n)$ many steps on the antiferromagnetic Ising model on $G$ with inverse temperature $\\frac{1}{\\sqrt{d}}$ finds a cut of size $\\frac{1}{2}+$ $\\Omega\\left(\\frac{1}{\\sqrt{d}}\\right)$.\nProof of Lemma 4.3. Let $S \\subseteq N(v)$ denote the subset of neighbors of $v$ which are not adjacent to any vertex of the independent set $x_{\\overline{N[v]}}=\\tau$, and write $k=|S|$. Observe that the distribution of $x_{N[v]}$ conditioned on $x_{\\overline{N[v]}}=\\tau$ is given by choosing the singleton $\\{v\\}$ with probability $\\frac{1}{2^{k}+1}$, or a uniformly random subset of $S$ with the remaining probability. Hence,\n\n$$\n\\mathbf{E}_{x \\sim \\pi}\\left[\\phi_{v}(x) \\mid x_{\\overline{N[v]}}=\\tau\\right]=\\frac{d}{2^{k}+1}+\\frac{k}{2} \\cdot \\frac{2^{k}}{2^{k}+1}\n$$\n\nThe above expression is always at least $\\frac{\\log d}{2}$ for any choice of nonnegative integer $k$.\nProof of Lemma 4.1. By [DSC96, Corollary A.4] and [BT06, Proposition 3.6], we have that $C_{\\mathrm{MLSI}}(\\pi) \\geqslant$ $\\frac{1-2 \\pi}{\\log \\left(\\frac{1}{\\pi \\tau}-1\\right)} \\cdot \\lambda(\\pi)$, where $\\lambda(\\pi)$ denotes the spectral gap of Glauber dynamics for $\\pi$, and $\\pi_{*}=$ $\\min _{x: \\pi(x)>0} \\pi(x)=\\frac{1}{2^{\\Delta}+1}$. Hence, it suffices to establish that $\\lambda(\\pi) \\geqslant \\exp (-O(\\Delta))$. For this, we appeal to the simple fact that random walk on a connected graph with $n$ vertices has spectral gap at least $1 / \\operatorname{poly}(n)$. A comparison of Dirichlet forms between Glauber dynamics and simple random walk on the $n=2^{\\Delta}+1$ many independent sets of $G$ yields the desired lower bound.", "tables": {}, "images": {}}, {"section_id": 10, "text": "# 5 Weak recovery in spiked models \n\nIn this section, we present our main application: using Glauber dynamics on Ising models to achieve weak recovery guarantees for spiked matrix models. Let us recall the definition of an Ising model.\n\nDefinition 5.1 (Ising model). Let $J \\in \\mathbb{R}^{n \\times n}$ be a symmetric interaction matrix and $h \\in \\mathbb{R}^{n}$ an external field. The Ising model corresponding to $J$ and $h$ is the probability distribution $\\mu_{J, h}$ on $\\{ \\pm 1\\}^{n}$, where\n\n$$\n\\mu_{J, h}(x) \\propto \\exp \\left(\\frac{1}{2} x^{\\top} J x+\\langle h, x\\rangle\\right)\n$$\n\nWe drop the $h$ from the subscript when it is equal to 0 .\n\nWe now pose the general algorithmic task that we wish to solve using Glauber dynamics in polynomial time.\n\nProblem 5.2. Let $W$ be a symmetric matrix in $\\mathbb{R}^{n \\times n}$ and let $v$ be a unit vector in $\\mathbb{R}^{n}$. For $M:=$ $W+\\lambda v v^{\\top}$, what is the behavior of Glauber dynamics run for poly $(n)$ many steps for the Ising model $\\mu_{M}$ ? In particular, under what assumptions does Glauber dynamics recover a vector that is well-correlated with $v$ after $\\operatorname{poly}(n)$ time?\n\nOur main results of this section resolves Problem 5.2 affirmatively in the following concrete settings.\n\n- $M=W+\\lambda v v^{\\top}$, where the spectral diameter of $W$ is at most 1 , and $v \\in\\left\\{ \\pm \\frac{1}{\\sqrt{n}}\\right\\}^{n}$.\n- $M=A_{G}-\\frac{d}{n} \\mathbf{1 1}^{\\top}$, the degree-centered adjacency matrix of a sparse stochastic block model.\n\nIn particular, despite a failure to mix to $\\mu$, Glauber dynamics run for polynomially many steps still manages to recover nontrivial information about the planted signal $v$.\n\nTheorem 1.7. Let $W$ be a matrix with $\\kappa \\preceq W \\preceq 1-\\kappa$, and $v \\in\\left\\{ \\pm \\frac{1}{\\sqrt{n}}\\right\\}^{n}$. Let $P$ denote the kernel of the Glauber dynamics chain with stationary distribution $\\mu_{W+\\lambda v v^{\\top}}$, and $x_{0}$ an arbitrary point on $\\{ \\pm 1\\}^{n}$. There exists a large enough constant $\\lambda>0$ such that for $T=\\widetilde{\\Theta}\\left(n^{5}\\right)$, and for $t \\sim[0, T]$, with probability $1-o(1)$ we have:\n\n$$\n\\mathbf{E}_{\\boldsymbol{x} \\sim P^{\\alpha} \\delta_{x_{0}}}[|\\langle\\boldsymbol{x}, v\\rangle|] \\geqslant\\left(\\kappa \\exp \\left(-\\frac{1}{\\kappa}\\right)-o(1)\\right) \\cdot \\sqrt{n}\n$$\n\nSimilarly, we have the following result for the stochastic block model.\nTheorem 1.12. There exist constants $\\lambda_{0}, \\beta, c>0$ such that for all $\\lambda$ satisfying $|\\lambda| \\geqslant \\lambda_{0}$, for $(\\boldsymbol{\\sigma}, \\boldsymbol{G}) \\sim$ $\\operatorname{SBM}(n, d, \\lambda)$, with probability $1-o(1)$ over the randomness of $(\\boldsymbol{\\sigma}, \\boldsymbol{G})$, the following holds.\n\nLet $P$ denote the kernel of Glauber dynamics with stationary distribution $\\mu_{\\frac{\\partial}{\\sqrt{\\partial}}\\left(A_{G}-\\frac{d}{n} \\mathbf{1 1}^{\\top}\\right)}$ and $x_{0}$ an arbitrary point on $\\{ \\pm 1\\}^{n}$. For $T=\\widetilde{\\Theta}\\left(n^{5+o_{d}(1)}\\right)$, and for $t \\sim[0, T]$, with probability $1-o(1)$, we have:\n\n$$\n\\mathbf{E}_{\\boldsymbol{x} \\sim P^{\\alpha} \\delta_{x_{0}}}[|\\langle\\boldsymbol{x}, \\boldsymbol{\\sigma}\\rangle|]>c n\n$$\n\nOur strategy to study Glauber dynamics in each of these settings is to relate its behavior to that of a different Markov chain called restricted Gaussian dynamics, whose definition we recall below.\n\nDefinition 5.3 (Restricted Gaussian dynamics). For $M=W+\\lambda v v^{\\top}$, restricted Gaussian dynamics (RGD) is a Markov chain on $\\{ \\pm 1\\}^{n}$ where a transition from $x$ to $y$ is given by the following:\n\n- Sample $\\boldsymbol{g} \\sim \\mathcal{N}(0,1)$ and define $\\boldsymbol{z}:=(\\lambda\\langle v, x\\rangle+\\sqrt{\\lambda} \\boldsymbol{g}) \\cdot v$.\n- Sample $\\boldsymbol{y}$ from the Gibbs distribution $\\mu_{W, z}$.\n\nNote that the above definition specifies the full joint distribution of $(\\boldsymbol{x}, \\boldsymbol{z})$, which induces a measure decomposition $\\mu_{M}=\\mathbf{E}_{z \\sim \\rho} \\mu_{W, z}$. Further, RGD is the associated Markov chain for this decomposition.\n\nRemark 5.4. Given access to $M$, but not $W$ and $v$, it is unclear how to efficiently implement restricted Gaussian dynamics. For example, for the SBM application, one has $v=\\frac{1}{\\sqrt{n}} \\sigma$. If one could compute $v$ (or $W$ ), then the recovery task would already be solved. Nevertheless, in light of Lemma 3.8, it is useful for analysis because we can relate its behavior to that of Glauber dynamics on $M$.\n\nIn Section 5.1, we will introduce some preliminaries on Ising models that will be crucial in the analysis. Then, in Section 5.2, we show that RGD achieves weak recovery. Finally, in Section 5.3, we prove the main theorems about weak recovery using Glauber dynamics, Theorems 1.7 and 1.12.", "tables": {}, "images": {}}, {"section_id": 11, "text": "# 5.1 Entropic stability and conservation of variance \n\nRecall the definition of a measure decomposition from Section 2. In order to prove our weak recovery result, we will need more structural properties for our decompositions. We now formalize these requirements.\n\nAt a high level, we would like a measure decomposition where the components of the decomposition $\\pi_{z}$ \"inherit\" properties of $\\pi$ itself, but are also simpler at the same time. When the individual components $\\pi_{z}$ inherit the variance of $\\pi$, the mixture is said to satisfy conservation of variance.\n\nDefinition 5.5 (Conservation of variance). We say that an Ising model $\\pi$ on $\\{ \\pm 1\\}^{n}$ satisfies conservation of variance for $\\left(\\rho, \\pi_{z}\\right)$ with parameter $C_{\\text {Var }} \\in[0,1]$, if for all functions $f:\\{ \\pm 1\\}^{n} \\rightarrow \\mathbb{R}$, we have\n\n$$\n\\mathbf{E}_{z \\sim \\rho}\\left[\\operatorname{Var}_{\\pi_{z}}[f]\\right] \\geqslant C_{\\mathrm{Var}} \\cdot \\operatorname{Var}_{\\pi}[f]\n$$\n\nRemark 5.6. By the law of total variance, for any $f:\\{ \\pm 1\\}^{n} \\rightarrow \\mathbb{R}$,\n\n$$\n\\operatorname{Var}_{\\pi}[f]=\\mathbf{E}_{z \\sim \\rho}\\left[\\operatorname{Var}_{\\pi_{z}}[f]\\right]+\\operatorname{Var}\\left[\\mathbf{E}_{\\pi_{z}} f\\right] \\geqslant \\mathbf{E}_{z \\sim \\rho}\\left[\\operatorname{Var}_{\\pi_{z}}[f]\\right]\n$$\n\nThe notion of conservation of variance captures mixtures where, loosely, a reverse of the above inequality is true.\n\nTo control the mean of various Ising models, we will also use another notion, called entropic stability [CE22]. To introduce it, we need the notion of a tilt. For a measure $\\pi$ on $\\Omega \\subseteq \\mathbb{R}^{n}$ and vector $v \\in \\mathbb{R}^{n}$, we define the tilted measure $\\mathcal{T}_{v} \\pi$ on $\\Omega$ by\n\n$$\n\\frac{\\mathrm{d} \\mathcal{T}_{v} \\pi(x)}{\\mathrm{d} \\pi(x)} \\propto e^{(v, x)}\n$$\n\nDefinition 5.7 (Entropic stability [CE22]). Let $\\Omega \\subseteq \\mathbb{R}^{n}$ and $\\psi: \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\rightarrow \\mathbb{R}_{\\geqslant 0}$. For $\\alpha>0$, we say that a measure $\\pi$ on $\\Omega$ is $\\alpha$-entropically stable with respect to $\\psi$ if for all $v \\in \\mathbb{R}^{n}$,\n\n$$\n\\psi\\left(\\mathbf{m}\\left(\\mathcal{T}_{v} \\pi\\right), \\mathbf{m}(\\pi)\\right) \\leqslant \\alpha \\cdot \\mathrm{KL}\\left(\\mathcal{T}_{v} \\pi \\| \\pi\\right)\n$$\n\nwhere we recall that $\\mathbf{m}(\\pi)=\\mathbf{E}_{\\boldsymbol{x} \\sim \\pi} \\boldsymbol{x}$. We denote by $\\alpha_{\\text {Ent }}$ the best (smallest) such $\\alpha$.\nRemark 5.8. It turns out that entropic stability can be used to prove the related notion of conservation of entropy for certain measure decompositions, which in turn implies conservation of variance; see, e.g., [BT06, Proof of Proposition 3.5]. However, we will not dwell on this point and refer the interested reader to [CE22].\n\nWe now state our special measure decompositions for the two applications. For bounded spectral diameter $W$, one can decompose the Ising model into a strongly log-concave mixture of product distributions; these types of decompositions were studied in [BB19, CE22]. See [CE22, Section 5.1] for details.\n\nTheorem 5.9. Let $W$ be a symmetric matrix with $\\kappa \\preceq W \\preceq 1-\\kappa$ and $h$ be an arbitrary external field. Then there exists a (strongly log-concave) mixture $\\rho$ on $\\mathbb{R}^{n}$ such that\n\n$$\n\\mu_{W, h}=\\mathbf{E}_{z \\sim \\rho}\\left[\\mu_{0, W z+h}\\right]\n$$\n\nMoreover, the following properties hold:\n(1) $\\mu_{W, h}$ is $\\frac{1}{\\kappa}$-entropically stable with respect to $(x, y) \\mapsto\\|x-y\\|^{2}$.\n(2) $\\mathrm{C}_{\\text {Var }}\\left(\\rho, \\mu_{0, W z+h}\\right) \\geqslant \\exp (-1 / \\kappa)$.\n(3) $\\mathrm{C}_{\\mathrm{MLSI}}\\left(\\mu_{W, h}\\right) \\geqslant \\frac{1}{n} \\cdot \\frac{1}{1-\\kappa}$.\n\nRecently, a subset of the authors proved that a similar decomposition exists for an Ising model associated to the stochastic block model [LMRW24].\nTheorem 5.10. Let $(\\sigma, \\boldsymbol{G}) \\sim \\operatorname{SBM}(n, d, \\lambda)$, and let $\\overline{A_{G}}=A_{G}-\\mathbf{E}\\left[A_{G} \\mid \\sigma\\right]$ be the centered adjacency matrix. There exists some constant $\\beta>0$, a mixture distribution $\\rho$ on $\\mathbb{R}^{n}$, and function $g: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ such that for any external field $h$,\n\n$$\n\\mu_{\\bar{\\beta} \\overline{A_{G}} / \\sqrt{d}, h}=\\mathbf{E}_{z \\sim \\rho}\\left[\\mu_{H, g(z)+h}\\right]\n$$\n\nwhere $H$ is an interaction matrix supported on the edges of a forest, with at most one additional cycle per connected component. Moreover, the following properties hold:\n(1) There is a positive constant $\\alpha_{\\text {Ent }}$ such that $\\mu_{\\bar{\\beta} \\overline{A_{G}} / \\sqrt{d}, h}$ is $\\alpha_{\\text {Ent }}$-entropically stable with respect to $(x, y) \\mapsto\\left\\|I_{[n] \\backslash H}(x-y)\\right\\|^{2}$.\n(2) $\\mathrm{C}_{\\mathrm{Var}}\\left(\\mu_{\\bar{\\beta} \\overline{A_{G}} / \\sqrt{d}, h}\\right) \\geqslant \\Omega(1)$.\n(3) $\\mathrm{C}_{\\mathrm{MLSI}}\\left(\\mu_{\\bar{\\beta} \\overline{A_{G}} / \\sqrt{d}, h}\\right) \\geqslant \\frac{1}{n^{1+\\alpha_{\\beta}(1)}}$.\n(4) The number of non-isolated vertices in $H$ is at most $\\gamma n$, where $\\gamma(d)=o_{d}(1)$ is a sufficiently small constant that also shrinks with $d$. Here, a non-isolated vertex is a vertex with at least one distinct neighbor.\n\nRemark 5.11. We provide pointers for the reader interested in extracting Theorem 5.10 from [LMRW24, CE22].\n\n- Item (1) follows from [LMRW24, Theorem 5.2] and [CE22, Lemma 40] (cf. [LMRW24, Lemma 3.18]).\n- For Item (2), conservation of entropy follows from [LMRW24, Theorem 5.2, Lemma 3.18, Lemma 3.29]. This, in turn, implies conservation of variance by the generic fact that $\\lim _{c \\rightarrow \\infty} \\operatorname{Ent}\\left[(f+\\right.$ $c)^{2}]=2 \\operatorname{Var}[f]$ (see e.g., [BT06, Proof of Proposition 3.5]).\n- Item (3) is [LMRW24, Theorem 5.1].\n- Item (4) follows from [LMRW24, Lemma 6.21].", "tables": {}, "images": {}}, {"section_id": 12, "text": "# 5.2 Restricted Gaussian dynamics achieves weak recovery \n\nIn this section we prove versions of Theorems 1.7 and 1.12 for restricted Gaussian dynamics-our goal will be to show that if the Dirichlet form with respect to the restricted Gaussian dynamics Markov chain is small, then it has non-trivial correlation with the planted vector $v$. Since local stationarity with respect to Glauber dynamics transfers over to local stationarity with respect to RGD (Lemma 3.8), this suffices to complete the proof.\n\nShowing that RGD succeeds at correlating with the spike amounts to Lemma 5.12, where we show that for \"nice\" Ising models, a strong external field applied to the Ising model shows itself in its mean. In particular, if the field is aligned with $v$, then the mean is correlated with $v$. One can then show that for any distribution $v$ that is locally stationary with respect to RGD, samples from $v$ must already have good correlation with the planted $v$ (Lemma 5.17).\n\nLemma 5.12. Let $W$ be an interaction matrix such that for any external field $h$, there is a decomposition\n\n$$\n\\mu_{W, h}=\\mathbf{E}_{z \\sim \\rho}\\left[\\mu_{H, g_{b}(z)}\\right]\n$$\n\nwhere $H$ is an arbitrary interaction matrix supported on the edges of a graph that we also denote $H$, such that the following hold.\n\n1. There is a positive constant $\\alpha_{\\text {Ent }}$ such that $\\mu_{W, h}$ is $\\alpha_{\\text {Ent }}$-entropically stable with respect to $(x, y) \\mapsto$ $\\left\\|I_{[n] \\backslash H}(x-y)\\right\\|^{2}$,\n2. Variance is conserved in this decomposition with constant $C_{\\text {Var }}$.\n3. There is a constant $\\gamma<1$ such that the number of non-isolated vertices in $H$ is at most $\\gamma n$.\n\nNow, let $v \\in\\left\\{ \\pm \\frac{1}{\\sqrt{n}}\\right\\}^{n}$, and set $s>0$. Then,\n\n$$\n\\mathbf{E}_{x \\sim \\mu_{W, s o}}|\\langle x, v\\rangle| \\geqslant(1-\\gamma) \\cdot \\frac{C_{\\mathrm{Var}}}{2} \\cdot \\min \\left\\{s, 2 \\cdot \\sqrt{\\frac{n}{C_{\\mathrm{Var}} \\alpha_{\\mathrm{Ent}}}}\\right\\}\n$$\n\nRemark 5.13. The above assumptions on $H$ may be a bit confusing. However, in the case of an interaction matrix with bounded spectral diameter, we in fact have $\\gamma=0$, so $H$ is empty.\nRemark 5.14. We further remark that by [LMRW24, Lemma 3.18]\u2014a mild strengthening of [CE22, Lemma 40]-the first condition holds if for all tilts $h,\\left\\|\\operatorname{Cov}\\left(\\mu_{W, h}\\right)_{[n] \\backslash H}\\right\\| \\leqslant \\alpha_{\\text {Ent }}$. In fact, only a bound on $\\left\\|\\operatorname{Cov}\\left(\\mu_{W, t v}\\right)_{[n] \\backslash H}\\right\\|$ for \"most\" $0 \\leqslant t \\leqslant s$ is required. A technique to show the second condition, involving stochastic localization, also requires a bound on the covariance.\n\nProof. The proof strategy is to use the fact that:\n\n$$\n\\mathbf{E}_{x \\sim \\mu_{W, s o}}\\langle\\boldsymbol{x}, v\\rangle=\\int_{0}^{s} \\frac{\\mathrm{~d}}{\\mathrm{~d} t} \\mathbf{E}_{x \\sim \\mu_{W, t v}}\\langle\\boldsymbol{x}, v\\rangle \\mathrm{d} t\n$$\n\nand obtain a lower bound on the correlation by showing a lower bound on the derivative.\nThe following standard calculation gives us a formula for the derivative as a variance of the correlation.\n\n$$\n\\frac{\\mathrm{d}}{\\mathrm{~d} t} \\mathbf{E}_{x \\sim \\mu_{W, t v}}\\langle\\boldsymbol{x}, v\\rangle=\\frac{\\mathrm{d}}{\\mathrm{~d} t} \\frac{\\mathbf{E}_{x \\sim \\mu_{W, 0}} e^{t\\langle\\boldsymbol{x}, v\\rangle}\\langle\\boldsymbol{x}, v\\rangle}{\\mathbf{E}_{x \\sim \\mu_{W, 0}} e^{t\\langle\\boldsymbol{x}, v\\rangle}}\n$$\n\n$$\n=\\frac{\\mathbf{E}_{x \\sim \\mu_{W, 0}} e^{t\\langle x, v\\rangle}\\langle x, v\\rangle^{2}}{\\mathbf{E}_{x \\sim \\mu_{W, 0}} e^{t\\langle x, v\\rangle}}-\\frac{\\left(\\mathbf{E}_{x \\sim \\mu_{W, 0}} e^{t\\langle x, v\\rangle}\\langle x, v\\rangle\\right)^{2}}{\\left(\\mathbf{E}_{x \\sim \\mu_{W, 0}} e^{t\\langle x, v\\rangle}\\right)^{2}}=\\operatorname{Var}_{x \\sim \\mu_{W, t v}}\\langle x, v\\rangle\n$$\n\nNow, consider the measure decomposition provided by the assumptions, of the form\n\n$$\n\\mu_{W, t v}=\\mathbf{E}_{z \\sim \\rho}\\left[\\mu_{H, z}\\right]\n$$\n\nfor some measure $\\rho$ over external fields $z$. We have by the law of total variance that\n\n$$\n\\operatorname{Var}_{x \\sim \\mu_{W, t v}}\\langle x, v\\rangle \\geqslant \\mathbf{E}_{z \\sim \\rho} \\operatorname{Var}_{x \\sim \\mu_{H, z}}\\langle x, v\\rangle\n$$\n\nNote that we can lower bound the above variance by the contribution of the isolated vertices in $H$, which are mutually independent of all other vertices. In particular, let $S_{\\text {iso }}$ be the set of isolated vertices in $H$. Then, we have:\n\n$$\n\\begin{aligned}\n\\mathbf{E}_{z \\sim \\rho} \\operatorname{Var}_{x \\sim \\mu_{H, z}}\\langle x, v\\rangle & \\geqslant \\mathbf{E}_{z \\sim \\rho} \\operatorname{Var}_{x \\sim \\mu_{H, z}}\\left[\\sum_{i \\in S_{\\text {iso }}} x_{i} v_{i}\\right] \\\\\n& =\\mathbf{E}_{z \\sim \\rho} \\sum_{i \\in S_{\\text {iso }}} v_{i}^{2} \\cdot \\operatorname{Var}_{x \\sim \\mu_{H, z}}\\left[\\boldsymbol{x}_{i}\\right]\n\\end{aligned}\n$$\n\nBy assumption, the decomposition conserves the variance of arbitrary functions with parameter $\\mathrm{C}_{\\text {Var }}$. Hence,\n\n$$\n\\mathbf{E}_{z \\sim \\rho} \\operatorname{Var}_{x \\sim \\mu_{H, z}}\\left[\\boldsymbol{x}_{i}\\right] \\geqslant C_{\\mathrm{Var}} \\cdot \\operatorname{Var}_{x \\sim \\mu_{W, t v}}\\left[\\boldsymbol{x}_{i}\\right]=C_{\\mathrm{Var}} \\cdot\\left(1-\\mathbf{E}_{x \\sim \\mu_{W, t v}}\\left[\\boldsymbol{x}_{i}\\right]^{2}\\right)\n$$\n\nConsequently, because $\\|v\\|_{\\infty}^{2}=1 / n$,\n\n$$\n\\operatorname{Var}_{x \\sim \\mu_{W, t v}}\\langle x, v\\rangle \\geqslant C_{\\mathrm{Var}} \\cdot\\left((1-\\gamma)-\\frac{1}{n}\\left\\|I_{[n] \\backslash H} \\cdot \\mathbf{m}\\left(\\mu_{W, t v}\\right)\\right\\|^{2}\\right)\n$$\n\nBy assumption, $\\mu_{W, t v}$ is $\\alpha_{\\text {Ent }}$-entropically stable with respect to $(x, y) \\mapsto\\left\\|I_{[n] \\backslash H} \\cdot(x-y)\\right\\|^{2}$. By definition, this implies that\n\n$$\n\\begin{aligned}\n& \\left\\|I_{[n] \\backslash H}\\left(\\mathbf{m}\\left(\\mu_{W, t v}\\right)-\\mathbf{m}\\left(\\mu_{W, 0}\\right)\\right)\\right\\|^{2} \\leqslant \\alpha_{\\mathrm{Ent}} \\cdot \\mathrm{KL}\\left(\\mu_{W, t v} \\| \\mu_{W, 0}\\right) \\text { and } \\\\\n& \\left\\|I_{[n] \\backslash H}\\left(\\mathbf{m}\\left(\\mu_{W, 0}\\right)-\\mathbf{m}\\left(\\mu_{W, t v}\\right)\\right)\\right\\|^{2} \\leqslant \\alpha_{\\mathrm{Ent}} \\cdot \\mathrm{KL}\\left(\\mu_{W, 0} \\| \\mu_{W, t v}\\right)\n\\end{aligned}\n$$\n\nand since $\\mathbf{m}\\left(\\mu_{W, 0}\\right)=0$ by symmetry, we get that\n\n$$\n\\left\\|I_{[n] \\backslash H} \\mathbf{m}\\left(\\mu_{W, t v}\\right)\\right\\|^{2} \\leqslant \\frac{\\alpha_{\\mathrm{Ent}}}{2} \\cdot \\operatorname{SKL}\\left(\\mu_{W, t v}, \\mu_{W, 0}\\right)=\\frac{\\alpha_{\\mathrm{Ent}}}{2} \\cdot \\mathbf{E}_{x \\sim \\mu_{W, t v}}\\langle t v, \\boldsymbol{x}\\rangle\n$$\n\nTherefore,\n\n$$\n\\frac{\\mathrm{d}}{\\mathrm{~d} t} \\mathbf{E}_{x \\sim \\mu_{W, t v}}\\langle x, v\\rangle \\geqslant C_{\\mathrm{Var}}\\left((1-\\gamma)-\\frac{t \\alpha_{\\mathrm{Ent}}}{2 n} \\mathbf{E}_{x \\sim \\mu_{W, t v}}\\langle x, v\\rangle\\right)\n$$\n\nNow, set\n\n$$\nL:=\\frac{s C_{\\mathrm{Var}}}{1+\\frac{s^{2} C_{\\mathrm{Var}} \\alpha_{\\mathrm{Ent}}}{4 n}} \\cdot(1-\\gamma)\n$$\n\nWe claim that $\\mathbf{E}_{\\boldsymbol{x} \\sim \\mu_{W, \\infty}}\\langle\\boldsymbol{x}, v\\rangle \\geqslant L$. To prove this claim, assume for contradiction that $\\mathbf{E}_{\\boldsymbol{x} \\sim \\mu_{W, \\infty}}\\langle\\boldsymbol{x}, v\\rangle<$ $L$. Then, since $\\mathbf{E}_{\\boldsymbol{x} \\sim \\mu_{W, \\infty}}\\langle\\boldsymbol{x}, v\\rangle$ is non-decreasing with $s$ (namely, its derivative is variance), we get:\n\n$$\n\\begin{aligned}\n\\mathbf{E}_{\\boldsymbol{x} \\sim \\mu_{W, \\infty}}\\langle\\boldsymbol{x}, v\\rangle & \\geqslant \\mathrm{C}_{\\mathrm{Var}} \\int_{0}^{s}\\left((1-\\gamma)-\\frac{t L \\alpha_{\\mathrm{Ent}}}{2 n}\\right) \\mathrm{d} t \\\\\n& =\\mathrm{C}_{\\mathrm{Var}}\\left(s(1-\\gamma)-\\frac{s^{2} L \\alpha_{\\mathrm{Ent}}}{4 n}\\right)=L\n\\end{aligned}\n$$\n\nwhich contradicts the assumption that $\\mathbf{E}_{\\boldsymbol{x} \\sim \\mu_{W, \\infty}}\\langle\\boldsymbol{x}, v\\rangle<L$.\nWe conclude the proof by doing casework on $s$; to this end set $s_{*}:=2 \\cdot \\sqrt{\\frac{n}{C_{\\text {Var }} \\alpha_{\\text {Ent }}}}$. If $s \\leqslant s_{*}$, then the definition of $L$ yields $L \\geqslant \\frac{s \\mathrm{C}_{\\text {Var }}}{2} \\cdot(1-\\gamma)$. On the other hand, the case of $s \\geqslant s_{*}$ immediately reduces to the above calculation because by monotonicity,\n\n$$\n\\mathbf{E}_{\\boldsymbol{x} \\sim \\mu_{W, \\infty}}\\langle\\boldsymbol{x}, v\\rangle \\geqslant \\mathbf{E}_{\\boldsymbol{x} \\sim \\mu_{W, \\alpha, v}}\\langle\\boldsymbol{x}, v\\rangle \\geqslant(1-\\gamma) \\cdot \\frac{\\mathrm{C}_{\\mathrm{Var}}}{2} \\cdot 2 \\cdot \\sqrt{\\frac{n}{C_{\\mathrm{Var}} \\alpha_{\\mathrm{Ent}}}}\n$$\n\nRemark 5.15. The above Lemma 5.12 can easily be generalized to relax the assumptions on $v$ to having bounded $\\ell_{\\infty}$ norm or being subgaussian. However, the resulting bound is a little messier, so we omit it for the sake of readability.\n\nAs a corollary of the above and the definition of restricted Gaussian dynamics, we get the following.\n\nCorollary 5.16 (RGD boost). Given $W$ as in the previous theorem, $x$ such that $|\\langle x, v\\rangle|=r$, and $\\boldsymbol{y} \\sim p_{\\mathrm{RGD}}$ $x$, we have:\n\n$$\n\\mathbf{E}_{y}|\\langle\\boldsymbol{y}, v\\rangle| \\geqslant(1-\\gamma) \\cdot \\frac{\\mathrm{C}_{\\mathrm{Var}}}{2} \\cdot \\mathbf{E}_{y \\sim \\mathcal{N}(0,1)} \\min \\left\\{|\\lambda r+\\sqrt{\\lambda} \\boldsymbol{g}|, 2 \\sqrt{\\frac{n}{\\mathrm{C}_{\\mathrm{Var}} \\alpha_{\\mathrm{Ent}}}}\\right\\}\n$$\n\nLemma 5.17 (Correlation of locally stationary distributions under RGD). Let $W$ be an interaction matrix satisfying the conditions in Lemma 5.12, and set $M=W+\\lambda v v^{\\top}$, where $v \\in\\left\\{ \\pm \\frac{1}{\\sqrt{n}}\\right\\}^{n}$ and $\\lambda \\geqslant 1$. Additionally, suppose that the distribution $\\mu_{W, s v}$ satisfies $C_{\\mathrm{MLSt}}\\left(\\mu_{W, s v}\\right) \\geqslant n^{-1-o(1)}$ for every $s \\in \\mathbb{R}$. Set\n\n$$\n\\delta_{*}=\\delta_{*}(\\lambda):=(1-\\gamma) \\cdot \\frac{\\mathrm{C}_{\\mathrm{Var}}}{2} \\cdot \\min \\left\\{\\sqrt{\\frac{2 \\lambda}{\\pi}}, \\frac{\\lambda}{2}\\right\\}-1\n$$\n\nThen for any $\\lambda$ such that $\\delta_{*}>0$, any $\\varepsilon$-locally stationary distribution $v$ under $R G D$, and any $\\delta \\in\\left(0, \\delta_{*}\\right)$, it holds that\n\n$$\n\\mathbf{E}_{x \\sim v}|\\langle\\boldsymbol{x}, v\\rangle| \\geqslant 0.99 \\cdot(1-\\gamma) \\cdot \\sqrt{\\frac{\\mathrm{C}_{\\mathrm{Var}} n}{\\alpha_{\\mathrm{Ent}}}}-2 \\delta\n$$\n\nfor $\\varepsilon<\\delta^{4} \\cdot \\frac{\\alpha_{\\text {Ent }}^{2}}{\\mathrm{C}_{\\text {Var }}^{2}(1-\\gamma)^{4}} \\cdot \\frac{1}{n^{2}}-\\exp (-\\Omega(n))$.\nProof. Set $\\eta=(1-\\gamma) \\cdot \\sqrt{\\frac{\\mathrm{C}_{\\text {Var }}}{\\alpha_{\\text {Ent }}}}, \\bar{\\eta}=0.99 \\eta, \\eta^{\\perp}=0.995 \\eta$, and define the function $\\phi(x):=\\min \\{|\\langle x, v\\rangle|, \\eta \\sqrt{n}\\}$.\nBy Corollary 3.3, which states that bounded functions are stable under a single step of restricted Gaussian dynamics,\n\n$$\n\\mathbf{E}_{y \\sim P_{\\mathrm{RGD}} v} \\phi(y)-\\mathbf{E}_{x \\sim v} \\phi(x)<\\eta \\sqrt{\\varepsilon n}\n$$\n\nthat is, the average value of $\\phi(\\boldsymbol{x})$ cannot change much under a single step of RGD.\nAssume for contradiction that $\\mathbf{E}_{\\boldsymbol{x} \\sim v} \\phi(\\boldsymbol{x})<\\bar{\\eta} \\sqrt{n}-2 \\delta$. We will show that a single step of RGD boosts the expected correlation enough that the resulting value of $\\mathbf{E}_{\\boldsymbol{y} \\sim P_{\\text {RGD }}} \\phi(\\boldsymbol{y})$ violates (6). This follows from two claims. First, observe that by Markov's inequality:\n\n$$\n\\mathbf{P r}_{x \\sim v}\\left[|\\langle x, v\\rangle|<\\bar{\\eta} \\sqrt{n}-n^{o(1)}-\\delta\\right]=\\mathbf{P r}_{x \\sim v}\\left[\\phi(\\boldsymbol{x})<\\bar{\\eta} \\sqrt{n}-n^{o(1)}-\\delta\\right] \\geqslant \\Omega\\left(\\frac{\\delta}{\\eta \\sqrt{n}}\\right)\n$$\n\nSecond, we establish that for any $x$ such that $|\\langle x, v\\rangle|=r$ and $\\boldsymbol{y} \\sim P_{\\text {RGD }} x$,\n\n1. if $r<\\bar{\\eta} \\sqrt{n}-n^{o(1)}-\\delta$, then $\\mathbf{E}_{\\boldsymbol{y}} \\phi(\\boldsymbol{y})-r>\\delta$, and\n2. $\\mathbf{E}_{\\boldsymbol{y}} \\phi(\\boldsymbol{y})-\\min \\left\\{r, \\bar{\\eta} \\sqrt{n}-n^{o(1)}\\right\\}>-\\exp \\left(-n^{1-o(1)}\\right)$.\n\nThese two claims together tell us that:\n\n$$\n\\mathbf{E}_{x \\sim P_{\\mathrm{RGD}}} \\phi(\\boldsymbol{x})-\\mathbf{E}_{x \\sim v} \\phi(\\boldsymbol{x})>\\Omega\\left(\\frac{\\delta^{2}}{\\eta \\sqrt{n}}\\right)-\\exp \\left(-n^{1-o(1)}\\right)\n$$\n\nwhich gives the desired contradiction, as (6) and (7) cannot simultaneously be true given the assumption on the relationship between $\\delta$ and $\\varepsilon$.\n\nIt remains to prove the claimed lower bounds on $\\mathbf{E}_{\\boldsymbol{y}} \\phi(\\boldsymbol{y})-r$. Let us begin with the simpler task of proving lower bounds on $\\mathbf{E}_{\\boldsymbol{y}}|\\langle\\boldsymbol{y}, v\\rangle|-r$ : we will show that\n\n1. if $r<\\eta \\sqrt{n}-\\delta$, then $\\mathbf{E}_{\\boldsymbol{y}}|\\langle\\boldsymbol{y}, v\\rangle|-r>\\delta$, and\n2. $\\mathbf{E}_{\\boldsymbol{y}}|\\langle\\boldsymbol{y}, v\\rangle|-\\min \\{r, \\eta \\sqrt{n}\\}>-\\exp (-\\Omega(n))$.\n\nLet $x$ be an arbitrary point on the hypercube with $|\\langle x, v\\rangle|=r$. We may assume without loss of generality that $\\langle x, v\\rangle>0$.\n\nFor ease of notation, set $\\widetilde{C}=\\frac{C_{\\mathrm{NS}}}{2} \\cdot(1-\\gamma)$. We prove this lower bound by splitting into cases based on the value of $r$. By Corollary 5.16, the following lower bounds hold for $\\boldsymbol{y} \\sim P_{\\mathrm{RGD}} x$ depending on the value of $r$ :\n\nCase $r<1$. We have\n\n$$\n\\begin{aligned}\n\\mathbf{E}_{\\boldsymbol{y}}|\\langle\\boldsymbol{y}, v\\rangle| & \\geqslant \\mathbf{E}_{g \\sim \\mathcal{N}(0,1)} \\min \\left\\{\\widetilde{C} \\cdot|\\lambda r+\\sqrt{\\lambda} \\boldsymbol{g}|, \\eta \\sqrt{n}\\right\\} \\\\\n& \\geqslant \\widetilde{C} \\cdot\\left(\\mathbf{E}_{g \\sim \\mathcal{N}(0,1)}|\\lambda r+\\sqrt{\\lambda} \\boldsymbol{g}|-\\mathbf{E}_{g \\sim \\mathcal{N}(0,1)}|\\lambda r+\\sqrt{\\lambda} \\boldsymbol{g}| \\cdot \\mathbf{1}_{\\widetilde{C} \\cdot|\\lambda r+\\sqrt{\\lambda} \\boldsymbol{g}|>\\eta \\sqrt{n}}\\right) \\\\\n& \\geqslant \\widetilde{C} \\cdot\\left(\\sqrt{\\lambda} \\mathbf{E}_{g \\sim \\mathcal{N}(0,1)}|\\boldsymbol{g}|-\\exp (-\\Omega(n))\\right) \\\\\n& =\\widetilde{C}\\left(\\sqrt{\\frac{2 \\lambda}{\\pi}}-\\exp (-\\Omega(n))\\right)\n\\end{aligned}\n$$\n\nwhere the final inequality follows from the fact that $\\mathbf{E}_{g \\sim \\mathcal{N}(0,1)}|\\boldsymbol{g}+R|$ is minimized at $R=0$, and from standard Gaussian concentration.\n\nIt follows that\n\n$$\n\\mathbf{E}_{\\boldsymbol{y}}|\\langle\\boldsymbol{y}, v\\rangle|-r \\geqslant \\widetilde{C} \\cdot \\sqrt{\\frac{2 \\lambda}{\\pi}}-1-\\exp (-\\Omega(n))>\\delta\n$$\n\nCase $1 \\leqslant r \\leqslant \\frac{3 \\eta \\sqrt{n}}{2 C \\lambda}$. In this case,\n\n$$\n\\begin{aligned}\n\\mathbf{E}_{y}|\\langle y, v\\rangle| & \\geqslant \\mathbf{E}_{g \\sim \\mathcal{N}(0,1)} \\min \\left\\{\\frac{\\widetilde{C}}{2} \\cdot|\\lambda r+\\sqrt{\\lambda} g|, \\eta \\sqrt{n}\\right\\} \\\\\n& \\geqslant \\frac{\\widetilde{C}}{2} \\cdot\\left(\\mathbf{E}_{g \\sim \\mathcal{N}(0,1)}|\\lambda r+\\sqrt{\\lambda} g|-\\mathbf{E}_{g \\sim \\mathcal{N}(0,1)}|\\lambda r+\\sqrt{\\lambda} g| \\cdot \\mathbf{1}_{\\widetilde{C} \\cdot|\\lambda r+\\sqrt{\\lambda} g|>2 \\eta \\sqrt{n}}\\right) \\\\\n& \\geqslant \\frac{\\widetilde{C} \\lambda}{2} \\cdot r-\\exp (-\\Omega(n))\n\\end{aligned}\n$$\n\nObserve that\n\n$$\n\\mathbf{E}_{y}|\\langle y, v\\rangle|-r>\\left(\\frac{\\widetilde{C} \\lambda}{2}-1\\right)>\\delta\n$$\n\nCase $\\frac{3 \\eta \\sqrt{n}}{2 C \\lambda} \\leqslant r \\leqslant \\eta \\sqrt{n}-\\delta$. In this case, by standard Gaussian concentration, the value of $\\widetilde{C}$. $|\\lambda r+\\sqrt{\\lambda} g|$ exceeds $\\eta \\sqrt{n}$ with exponentially high probability, and hence $\\mathbf{E}_{y}|\\langle y, v\\rangle| \\geqslant \\eta \\sqrt{n}-$ $\\exp (-\\Omega(n))$. Thus, in this case too, we have $\\mathbf{E}_{y}|\\langle y, v\\rangle|-r>\\delta-\\exp (-\\Omega(n))$.\nCase $\\eta \\sqrt{n}-\\delta \\leqslant r$. By identical reasoning to the previous case, in this case $\\mathbf{E}_{y}|\\langle y, v\\rangle|-\\eta \\sqrt{n}>$ $-\\exp (-\\Omega(n))$.\n\nWe must now translate the above lower bounds to lower bounds on $\\mathbf{E}_{y} \\phi(y)$. To do this, we will use Lemma 2.8, which says that the correlation of a sample concentrates around its expectation. We will again consider different cases, depending on where the expectation lies. Concretely, we will prove that\n\n$$\n\\mathbf{E} \\phi(y)=\\mathbf{E} \\min \\{|\\langle y, v\\rangle|, \\eta \\sqrt{n}\\} \\geqslant \\min \\left\\{\\mathbf{E}|\\langle y, v\\rangle|, \\bar{\\eta} \\sqrt{n}-n^{o(1)}\\right\\}-\\exp \\left(-n^{1-o(1)}\\right)\n$$\n\nTowards proving this, we have\n\n$$\n\\begin{aligned}\n\\mathbf{E} \\phi(y) & =\\mathbf{E}|\\langle y, v\\rangle| \\mathbf{1}_{|\\langle y, v\\rangle| \\leqslant \\eta \\sqrt{n}}+\\eta \\sqrt{n} \\cdot \\operatorname{Pr}[|\\langle y, v\\rangle| \\geqslant \\eta \\sqrt{n}] \\\\\n& =\\mathbf{E}|\\langle y, v\\rangle|+\\mathbf{E}(\\eta \\sqrt{n}-|\\langle y, v\\rangle|) \\mathbf{1}_{|\\langle y, v\\rangle| \\geqslant \\eta \\sqrt{n}}\n\\end{aligned}\n$$\n\nLet $\\eta^{\\uparrow}=1.005 \\eta$, and recall our earlier definitions $\\bar{\\eta}=0.99 \\eta$ and $\\eta^{\\downarrow}=0.995 \\eta$.\nCase $\\mathbf{E}|\\langle y, v\\rangle|<\\eta^{\\downarrow} \\sqrt{n}$.\nHere, we have by (10) that\n\n$$\n\\mathbf{E} \\phi(y) \\geqslant \\mathbf{E}|\\langle y, v\\rangle|-\\sqrt{n} \\cdot \\operatorname{Pr}[|\\langle y, v\\rangle| \\geqslant \\eta \\sqrt{n}]\n$$\n\nLemma 2.8 allows us to bound\n\n$$\n\\operatorname{Pr}[|\\langle y, v\\rangle| \\geqslant \\eta \\sqrt{n}] \\leqslant \\exp \\left(-\\Omega\\left(\\frac{\\eta \\sqrt{n}-\\mathbf{E}|\\langle y, v\\rangle|}{n^{o(1)}}\\right)^{2}\\right)=\\exp \\left(-n^{1-o(1)}\\right)\n$$\n\nCase $\\eta^{\\downarrow} \\sqrt{n} \\leqslant \\mathbf{E}|\\langle y, v\\rangle| \\leqslant \\eta^{\\uparrow} \\sqrt{n}$.\n(10) implies that\n\n$$\n\\mathbf{E} \\phi(y) \\geqslant \\mathbf{E}|\\langle y, v\\rangle|-\\mathbf{E}|\\eta \\sqrt{n}-|\\langle y, v\\rangle|\n$$\n\n$$\n\\begin{aligned}\n& \\geqslant \\mathbf{E}|\\langle\\boldsymbol{y}, v\\rangle|-|\\eta \\sqrt{n}-\\mathbf{E}|\\langle\\boldsymbol{y}, v\\rangle||-\\mathbf{E}||\\langle\\boldsymbol{y}, v\\rangle|-\\mathbf{E}|\\langle\\boldsymbol{y}, v\\rangle|| \\\\\n& \\geqslant \\mathbf{E}|\\langle\\boldsymbol{y}, v\\rangle|-|\\eta \\sqrt{n}-\\mathbf{E}|\\langle\\boldsymbol{y}, v\\rangle||-n^{o(1)} \\\\\n& \\geqslant \\eta^{\\perp} \\sqrt{n}-\\left(\\eta-\\eta^{\\perp}\\right) \\sqrt{n}-n^{o(1)} \\\\\n& =\\bar{\\eta} \\sqrt{n}-n^{o(1)}\n\\end{aligned}\n$$\n\nwhere the third inequality uses Lemma 2.8.\nCase $\\eta^{\\uparrow} \\sqrt{n}<\\mathbf{E}|\\langle\\boldsymbol{y}, v\\rangle|$.\n(9) implies that\n\n$$\n\\mathbf{E} \\phi(\\boldsymbol{y}) \\geqslant \\eta \\sqrt{n} \\cdot \\operatorname{Pr}[|\\langle\\boldsymbol{y}, v\\rangle| \\geqslant \\eta \\sqrt{n}]\n$$\n\nWe give a lower bound for the above using Lemma 2.8 again:\n\n$$\n\\operatorname{Pr}[|\\langle\\boldsymbol{y}, v\\rangle| \\geqslant \\eta \\sqrt{n}] \\geqslant 1-\\exp \\left(-\\Omega\\left(\\frac{\\mathbf{E}|\\langle\\boldsymbol{y}, v\\rangle|-\\eta \\sqrt{n}}{n^{o(1)}}\\right)^{2}\\right)=1-\\exp \\left(-n^{1-o(1)}\\right)\n$$\n\nas desired.\nNow, let us put the pieces together to obtain a contradiction to (6). By (8), and the preceding lower bound on the shift in $\\mathbf{E}|\\langle\\boldsymbol{y}, v\\rangle|$, if $|\\langle x, v\\rangle|<\\bar{\\eta} \\sqrt{n}-\\delta-n^{o(1)}$,\n\n$$\n\\begin{aligned}\n\\mathbf{E}_{\\boldsymbol{y} \\sim P_{\\mathrm{RGD}} \\delta_{t}} \\phi(\\boldsymbol{y}) & \\geqslant \\min \\left\\{\\mathbf{E}|\\langle\\boldsymbol{y}, v\\rangle|, \\bar{\\eta} \\sqrt{n}-n^{o(1)}\\right\\}-\\exp \\left(-n^{1-o(1)}\\right) \\\\\n& \\geqslant \\min \\left\\{r+\\delta, \\bar{\\eta} \\sqrt{n}-n^{o(1)}\\right\\}-\\exp \\left(-n^{1-o(1)}\\right) \\\\\n& \\geqslant r+\\delta-\\exp \\left(-n^{1-o(1)}\\right)\n\\end{aligned}\n$$\n\nand similarly, if $|\\langle x, v\\rangle|>\\bar{\\eta} \\sqrt{n}-\\delta-n^{o(1)}$, we have\n\n$$\n\\mathbf{E}_{\\boldsymbol{y} \\sim P_{\\mathrm{RGD}} \\delta_{t}} \\phi(\\boldsymbol{y}) \\geqslant \\min \\left\\{r, \\bar{\\eta} \\sqrt{n}-n^{o(1)}\\right\\}-\\exp \\left(-n^{1-o(1)}\\right)\n$$\n\nwhich completes the proof.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# 5.3 Glauber dynamics achieves weak recovery \n\nWe are finally ready to prove the main results of this section.\nTheorem 1.7. Let $W$ be a matrix with $\\kappa \\preceq W \\preceq 1-\\kappa$, and $v \\in\\left\\{ \\pm \\frac{1}{\\sqrt{n}}\\right\\}^{n}$. Let $P$ denote the kernel of the Glauber dynamics chain with stationary distribution $\\mu_{W+\\lambda \\rho v^{T}}$, and $x_{0}$ an arbitrary point on $\\{ \\pm 1\\}^{n}$. There exists a large enough constant $\\lambda>0$ such that for $T=\\bar{\\Theta}\\left(n^{5}\\right)$, and for $\\boldsymbol{t} \\sim[0, T]$, with probability $1-o(1)$ we have:\n\n$$\n\\mathbf{E}_{\\boldsymbol{x} \\sim P^{\\prime} \\delta_{x_{0}}}[|\\langle\\boldsymbol{x}, v\\rangle|] \\geqslant\\left(\\kappa \\exp \\left(-\\frac{1}{\\kappa}\\right)-o(1)\\right) \\cdot \\sqrt{n}\n$$\n\nProof. The proof strategy is to establish that $\\nu_{t}$ is locally stationary for restricted Gaussian dynamics, and then use Lemma 5.17 to conclude. Let $\\nu_{t}$ denote the distribution after running Glauber dynamics for time $t$. For $\\boldsymbol{t} \\sim[0, T]$, by Theorem 1.3, the distribution $\\nu_{t}$ is $O\\left(\\frac{\\kappa}{\\zeta T}\\right)$-locally stationary, except with probability at most $\\zeta$. Let us assume that this event occurs.\nWe now compute the parameters we can plug into Lemma 3.8.\n\n- First, due to local stationarity, we have $\\mathcal{E}\\left(f_{t}, \\log f_{t}\\right)<O\\left(\\frac{n}{\\zeta T}\\right)$.\n- By Item (3) of Theorem 5.9, the value of $\\delta$ from Lemma 3.8 is at least $\\frac{1}{(1-\\kappa) \\cdot n}$.\n- Finally, it remains to upper bound $f_{t}(x)$ for all $x \\in\\{ \\pm 1\\}^{n}$. Indeed, since the Hamiltonian of $\\mu$ is bounded from above by $O(n)$, we may assume that $\\tau$ from Lemma 3.8 is $O(n)$.\n\nOnce we plug in these parameters, we get:\n\n$$\n\\mathcal{E}_{\\mathrm{RGD}}\\left(f_{t}, \\log f_{t}\\right)<\\widetilde{O}\\left(\\frac{n^{3}}{\\zeta T}\\right)\n$$\n\nWhen $\\zeta T \\gg n^{5}$, we get\n\n$$\n\\mathcal{E}_{\\mathrm{RGD}}\\left(f_{t}, \\log f_{t}\\right)<o\\left(\\frac{1}{n^{2}}\\right)\n$$\n\nBy Lemma 5.17 with parameters $\\varepsilon=o\\left(\\frac{1}{n^{2}}\\right)$ and $\\delta=o(1)$, and the values for $\\mathrm{C}_{\\mathrm{Var}}$ and $\\alpha_{\\text {Ent }}$ from Theorem 5.9, we get that:\n\n$$\n\\mathbf{E}_{x \\sim v_{t}}|\\langle x, v\\rangle| \\geqslant\\left(\\sqrt{\\frac{\\mathrm{C}_{\\mathrm{Var}}}{\\alpha_{\\mathrm{Ent}}}}-o(1)\\right) \\cdot \\sqrt{n} \\geqslant(\\kappa \\exp (-1 / \\kappa)-o(1)) \\sqrt{n}\n$$\n\nWe treat $\\kappa$ as a constant, and can set $\\zeta=o(1)$ and $T=\\widetilde{\\Omega}\\left(n^{5}\\right)$ to finish the proof.\nThe proof of Theorem 1.12 for SBM weak recovery is very similar, but we provide the explicit parameter dependencies for completeness. To be explicit, in the notation of Problem 5.2, for SBM recovery we have $v=\\frac{1}{\\sqrt{n}} \\sigma, M=A_{G}-\\frac{d}{n} \\mathbf{1 1}^{\\top}$, and $W=\\bar{A}_{G}:=A_{G}-\\mathbf{E}\\left[A_{G} \\mid \\sigma\\right]$. One can verify that $M=W+\\lambda \\sqrt{d} \\cdot v v^{\\top}$ by using the fact that $\\mathbf{E}\\left[A_{G} \\mid \\sigma\\right]=\\frac{d}{n} \\mathbf{1 1}^{\\top}+\\frac{\\lambda \\sqrt{d}}{n} \\sigma \\sigma^{\\top}$.\nTheorem 1.12. There exist constants $\\lambda_{0}, \\beta, c>0$ such that for all $\\lambda$ satisfying $|\\lambda| \\geqslant \\lambda_{0}$, for $(\\boldsymbol{\\sigma}, \\boldsymbol{G}) \\sim$ $\\operatorname{SBM}(n, d, \\lambda)$, with probability $1-o(1)$ over the randomness of $(\\boldsymbol{\\sigma}, \\boldsymbol{G})$, the following holds.\n\nLet $P$ denote the kernel of Glauber dynamics with stationary distribution $\\mu_{\\frac{\\sigma}{\\sqrt{d}}}\\left(A_{G}-\\frac{d}{n} \\mathbf{1 1}^{\\top}\\right)$ and $x_{0}$ an arbitrary point on $\\{ \\pm 1\\}^{n}$. For $T=\\widetilde{\\Theta}\\left(n^{5+o_{d}(1)}\\right)$, and for $t \\sim[0, T]$, with probability $1-o(1)$, we have:\n\n$$\n\\mathbf{E}_{x \\sim P^{\\alpha} \\delta_{x_{0}}}[|\\langle x, \\sigma\\rangle|]>c n\n$$\n\nProof. Again, let $\\nu_{t}$ denote the distribution after running Glauber dynamics on $\\mu_{M, 0}$ until time $t$. As before, let us assume that $\\nu_{t}$ is $O\\left(\\frac{n}{\\zeta T}\\right)$-locally stationary, which occurs with probability $1-\\zeta$. We now compute the parameters we can plug into Lemma 3.8.\n\n- By Item (3) of Theorem 5.10, the value of $\\delta$ from Lemma 3.8 is at least $\\frac{1}{n^{1+o_{d}(1)}}$.\n- To upper bound $f_{t}(x)$ for all $x \\in\\{ \\pm 1\\}^{n}$, we have $\\|W x\\|_{\\infty} \\leqslant O(\\log n)$, so that we can bound $\\tau \\leqslant O(n \\log n)$.\n\nOnce we plug in these parameters, we get:\n\n$$\n\\mathcal{E}_{\\mathrm{RGD}}\\left(f_{t}, \\log f_{t}\\right)<\\widetilde{O}\\left(\\frac{n^{3+o_{d}(1)}}{\\zeta \\cdot T}\\right)\n$$\n\nWhen $\\zeta \\cdot T \\gg n^{5+o_{\\delta}(1)}$, we get that\n\n$$\n\\mathcal{E}_{\\mathrm{RGD}}\\left(f_{\\mathrm{f}}, \\log f_{\\mathrm{f}}\\right)<o\\left(\\frac{1}{n^{2}}\\right)\n$$\n\nBy Lemma 5.17 with parameters $\\varepsilon$ as above and $\\delta=O(1)$, for some universal constant $c$,\n\n$$\n\\mathbf{E}_{x \\sim v_{\\mathrm{f}}}|\\langle x, \\sigma\\rangle| \\geqslant c n\n$$\n\nSetting $\\zeta=o(1)$ and $T=\\tilde{\\Omega}\\left(n^{5+o_{\\delta}(1)}\\right)$ completes the proof.", "tables": {}, "images": {}}, {"section_id": 14, "text": "# Acknowledgments \n\nWe would like to thank Omar Alrabiah, Sitan Chen, and Ansh Nagda for insightful discussions.", "tables": {}, "images": {}}, {"section_id": 15, "text": "## References\n\n[Abb17] Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of Machine Learning Research, 18(1):6446-6531, 2017. 5\n[AJK+21] Nima Anari, Vishesh Jain, Frederic Koehler, Huy Tuan Pham, and Thuy-Duong Vuong. Entropic independence I: Modified log-Sobolev inequalities for fractionally log-concave distributions and high-temperature ising models. arXiv preprint arXiv:2106.04105, 2021. 7, 8\n[AKS80] Mikl\u00f3s Ajtai, J\u00e1nos Koml\u00f3s, and Endre Szemer\u00e9di. A note on Ramsey numbers. Journal of Combinatorial Theory, Series A, 29(3):354-360, 1980. 2, 18\n[AOW15] Sarah R Allen, Ryan O'Donnell, and David Witmer. How to refute a random CSP. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pages 689-708. IEEE, 2015. 5\n[AS16] Noga Alon and Joel H Spencer. The Probabilistic Method. John Wiley \\& Sons, 2016. 2, $3,9,19$\n[BAP05] Jinho Baik, G\u00e9rard Ben Arous, and Sandrine P\u00e9ch\u00e9. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. The Annals of Probability, 33(5):1643 - 1697, 2005. 4, 5\n[BB19] Roland Bauerschmidt and Thierry Bodineau. A very simple proof of the LSI for high temperature spin systems. Journal of Functional Analysis, 276(8):2582-2588, 2019. 7, 24\n[BCE+22] Krishna Balasubramanian, Sinho Chewi, Murat A Erdogdu, Adil Salim, and Shunshi Zhang. Towards a theory of non-log-concave sampling: first-order stationarity guarantees for Langevin Monte Carlo. In Conference on Learning Theory, pages 2896-2923. PMLR, 2022. 8\n\n[BDH16] Anton Bovier and Frank Den Hollander. Metastability: a potential-theoretic approach, volume 351. Springer, 2016. 8, 10\n[BGJM11] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov Chain Monte Carlo. CRC Press LLC, 2011. 1\n[BGL+14] Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. Analysis and geometry of Markov diffusion operators, volume 103. Springer, 2014.1\n[BIL ${ }^{+}$16] Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina. Local entropy as a measure for sampling solutions in constraint satisfaction problems. Journal of Statistical Mechanics: Theory and Experiment, 2016(2):023301, 2016. 1\n[BKS23] Rares-Darius Buhai, Pravesh K Kothari, and David Steurer. Algorithms approaching the threshold for semi-random planted clique. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 1918-1926, 2023. 11\n[BLLT20] Peter L Bartlett, Philip M Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):3006330070, 2020.1\n[BM19] Jean Barbier and Nicolas Macris. The adaptive interpolation method: a simple scheme to prove replica formulas in Bayesian inference. Probability theory and related fields, 174:1133-1185, 2019. 4\n[BRS11] Boaz Barak, Prasad Raghavendra, and David Steurer. Rounding semidefinite programming hierarchies via global correlation. In 2011 ieee 52nd annual symposium on foundations of computer science, pages 472-481. IEEE, 2011. 11\n[BT06] Sergey G Bobkov and Prasad Tetali. Modified logarithmic Sobolev inequalities in discrete settings. Journal of Theoretical Probability, 19(2):289-336, 2006. 2, 12, 13, 21, 23, 24\n[CE22] Y. Chen and R. Eldan. Localization Schemes: A Framework for Proving Mixing Bounds for Markov Chains (extended abstract). In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 110-122, Los Alamitos, CA, USA, nov 2022. IEEE Computer Society. 7, 13, 23, 24, 25\n[CFM23] Michael Celentano, Zhou Fan, and Song Mei. Local convexity of the TAP free energy and AMP convergence for Z2-synchronization. The Annals of Statistics, 51(2):519-546, 2023. 4\n[CMZ23] Zongchen Chen, Elchanan Mossel, and Ilias Zadik. Almost-Linear Planted Cliques Elude the Metropolis Process. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 4504-4539. SIAM, 2023. 8\n[COKPZ17] Amin Coja-Oghlan, Florent Krzakala, Will Perkins, and Lenka Zdeborov\u00e1. Information-theoretic thresholds from the cavity method. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 146-157, 2017. 10\n\n[COP19a] Amin Coja-Oghlan and Will Perkins. Bethe states of random factor graphs. Communications in Mathematical Physics, 366:173-201, 2019. 10\n[COP19b] Amin Coja-Oghlan and Will Perkins. Spin systems on Bethe lattices. Communications in Mathematical Physics, 372:441-523, 2019. 10\n[CWZZ24] Xiang Cheng, Bohan Wang, Jingzhao Zhang, and Yusong Zhu. Fast Conditional Mixing of MCMC Algorithms for Non-log-concave Distributions. Advances in Neural Information Processing Systems, 36, 2024. 8\n[DAM17] Yash Deshpande, Emmanuel Abbe, and Andrea Montanari. Asymptotic mutual information for the balanced binary stochastic block model. Information and Inference: A Journal of the IMA, 6(2):125-170, 2017. 4\n[Dia09] Persi Diaconis. The Markov Chain Monte Carlo Revolution. Bull. Amer. Math. Soc., 46(2), 2009. 1\n[DJPR18] Ewan Davies, Matthew Jenssen, Will Perkins, and Barnaby Roberts. On the average size of independent sets in triangle-free graphs. Proceedings of the American Mathematical Society, 146(1):111-124, 2018.3\n[DKMZ11] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborov\u00e1. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Physical review E, 84(6):066106, 2011. 5, 10\n[DMK ${ }^{+}$16a] Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault Lesieur, Lenka Zdeborov\u00e1, et al. Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula. Advances in Neural Information Processing Systems, 29, 2016. 4\n[DMK ${ }^{+} 16 b]$ Mohamad Dia, Nicolas Macris, Florent Krzakala, Thibault Lesieur, Lenka Zdeborov\u00e1, et al. Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula. Advances in Neural Information Processing Systems, 29, 2016. 4\n[DSC96] Persi Diaconis and Laurent Saloff-Coste. Logarithmic Sobolev inequalities for finite Markov chains. The Annals of Applied Probability, 6(3), 81996.21\n[DSS15] Jian Ding, Allan Sly, and Nike Sun. Proof of the satisfiability conjecture for large k. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 59-68, 2015. 10\n[EAK18] Ahmed El Alaoui and Florent Krzakala. Estimation in the spiked wigner model: a short proof of the replica formula. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 1874-1878. IEEE, 2018. 4\n[EAMS23] Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Shattering in pure spherical spin glasses. arXiv preprint arXiv:2307.04659, 2023. 10\n[EKZ22] Ronen Eldan, Frederic Koehler, and Ofer Zeitouni. A spectral condition for spectral gap: fast mixing in high-temperature Ising models. Probability theory and related fields, 182(3):1035-1051, 2022. 7, 8, 13\n\n[FMM21] Zhou Fan, Song Mei, and Andrea Montanari. TAP free energy, spin glasses and variational inference. The Annals of Probability, 2021.4\n[GBP19] Junxian Geng, Anirban Bhattacharya, and Debdeep Pati. Probabilistic community detection with unknown number of communities. Journal of the American Statistical Association, 114(526):893-905, 2019.1\n[GHKM23] Venkatesan Guruswami, Jun-Ting Hsieh, Pravesh K Kothari, and Peter Manohar. Efficient Algorithms for Semirandom Planted CSPs at the Refutation Threshold. In 2023 IEEE 64th Annual Symposium on Foundations of Computer Science (FOCS), pages 307-327. IEEE, 2023. 11\n[GJX23] Reza Gheissari, Aukosh Jagannath, and Yiming Xu. Finding planted cliques using Markov chain Monte Carlo. arXiv preprint arXiv:2311.07540, 2023. 9\n[GJZ17] Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In International Conference on Machine Learning, pages 1233-1242. PMLR, 2017. 1\n[Goe04] Sharad Goel. Modified logarithmic Sobolev inequalities for some models of random walk. Stochastic processes and their applications, 114(1):51-79, 2004.12\n[G\u015aV15] Andreas Galanis, Daniel \u015atefankovi\u010d, and Eric Vigoda. Inapproximability for Antiferromagnetic Spin Systems in the Tree Nonuniqueness Region. J. ACM, 62(6), dec 2015. 3\n[GV16] Olivier Gu\u00e9don and Roman Vershynin. Community detection in sparse networks via Grothendieck's inequality. Probability Theory and Related Fields, 165(3-4):1025-1049, 2016. 1\n[HS17] Samuel B Hopkins and David Steurer. Efficient Bayesian estimation from few samples: community detection and related problems. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 379-390. IEEE, 2017. 9\n[Hub59] J. Hubbard. Calculation of Partition Functions. Phys. Rev. Lett., 3:77-78, Jul 1959. 8\n[HW58] Wassily Hoeffding and J Wolfowitz. Distinguishability of sets of distributions. The Annals of Mathematical Statistics, 29(3):700-718, 1958. 14\n$\\left[\\mathrm{JNG}^{+} 21\\right]$ Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points. Journal of the ACM (JACM), 68(2):1-29, 2021.1\n[JT20] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. Advances in Neural Information Processing Systems, 33:17176-17186, 2020.1\n[KLR22] Frederic Koehler, Holden Lee, and Andrej Risteski. Sampling approximately lowrank Ising models: MCMC meets variational methods. In Conference on Learning Theory, pages 4945-4988. PMLR, 2022. 8\n\n[KS66] Harry Kesten and Bernt P Stigum. Additional limit theorems for indecomposable multidimensional galton-watson processes. The Annals of Mathematical Statistics, 37(6):1463-1481, 1966.5\n[KS67] Harry Kesten and Bernt P Stigum. Limit theorems for decomposable multidimensional galton-watson processes. Journal of Mathematical Analysis and Applications, 17(2):309-338, 1967. 5\n[Kun23] Dmitriy Kunisky. Optimality of Glauber dynamics for general-purpose Ising model sampling and free energy approximation. arXiv preprint arXiv:2307.12581, 2023. 8\n[LM17] Marc Lelarge and L\u00e9o Miolane. Fundamental limits of symmetric low-rank matrix estimation. In Conference on Learning Theory, pages 1297-1301. PMLR, 2017. 4\n[LMRW24] Kuikui Liu, Sidhanth Mohanty, Amit Rajaraman, and David X Wu. Fast Mixing in Sparse Random Ising Models. arXiv preprint arXiv:2405.06616, 2024. 7, 13, 24, 25\n[LPW17] David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times. American Mathematical Society, 2 edition, 2017. 1\n[LST21] Yin Tat Lee, Ruoqi Shen, and Kevin Tian. Structured logconcave sampling with a restricted Gaussian oracle. In Conference on Learning Theory, pages 2993-3050. PMLR, 2021. 7\n[Mas14] Laurent Massouli\u00e9. Community detection thresholds and the weak Ramanujan property. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 694-703, 2014. 5\n[Mio17] L\u00e9o Miolane. Fundamental limits of low-rank matrix estimation: the non-symmetric case. arXiv preprint arXiv:1702.00473, 2017. 4\n[MMZ06] Stephan Mertens, Marc M\u00e9zard, and Riccardo Zecchina. Threshold values of random K-SAT from the cavity method. Random Structures \\& Algorithms, 28(3):340-373, 2006. 10\n[MNS15] Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition model. Probability Theory and Related Fields, 162(3-4):431-461, 2015. 5\n[MNS18] Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture. Combinatorica, 38(3):665-708, jun 2018. 5\n[Moo17] Cristopher Moore. The computer science and physics of community detection: Landscapes, phase transitions, and hardness. arXiv preprint arXiv:1702.00467, 2017. 6, 9\n[MPZ02] Marc M\u00e9zard, Giorgio Parisi, and Riccardo Zecchina. Analytic and algorithmic solution of random satisfiability problems. Science, 297(5582):812-815, 2002. 10\n[MS12] Morten M\u00f8rup and Mikkel N Schmidt. Bayesian community detection. Neural computation, 24(9):2434-2456, 2012. 1\n\n[MT06] Ravi Montenegro and Prasad Tetali. Mathematical Aspects of Mixing Times in Markov Chains. Foundations and Trends in Theoretical Computer Science, 1(3):237-354, 2006. 1\n[MV21] Andrea Montanari and Ramji Venkataramanan. Estimation of low-rank matrices via approximate message passing. The Annals of Statistics, 49(1):321 - 345, 2021. 4\n[MW23] Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via diffusion processes. arXiv preprint arXiv:2304.11449, 2023. 4\n[MWW07] Elchanan Mossel, Dror Weitz, and Nicholas C. Wormald. On the hardness of sampling independent sets beyond the tree threshold. Probability Theory and Related Fields, 143:401-439, 2007. 3\n[Nes18] Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.1\n[Par80] Giorgio Parisi. A sequence of approximated solutions to the SK model for spin glasses. Journal of Physics A: Mathematical and General, 13(4):L115, 1980. 10\n[Sel23] Mark Sellke. The threshold energy of low temperature langevin dynamics for pure spherical spin glasses. Communications on Pure and Applied Mathematics, 2023. 9\n[She83] James B Shearer. A note on the independence number of triangle-free graphs. Discrete Mathematics, 46(1):83-87, 1983. 2, 19\n[SKC94] Bart Selman, Henry A Kautz, and Bram Cohen. Noise strategies for improving local search. In $A A A I$, volume 94, pages 337-343, 1994. 1\n[Sly10] Allan Sly. Computational Transition at the Uniqueness Threshold. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 287-296, 2010. 1, 3\n[SS14] Allan Sly and Nike Sun. The Computational Hardness of Counting in Two-Spin Models on $d$-Regular Graphs. The Annals of Probability, 42(6):2383-2416, 2014. 1, 3\n[STL20] Ruoqi Shen, Kevin Tian, and Yin Tat Lee. Composite Logconcave Sampling with a Restricted Gaussian Oracle. arXiv preprint arXiv:2006.05976, 2020.7\n[SV16] Igal Sason and Sergio Verd\u00fa. $f$-divergence inequalities. IEEE Transactions on Information Theory, 62(11):5973-6006, 2016. 14\n[Tal06] Michel Talagrand. The Parisi formula. Annals of mathematics, pages 221-263, 2006. 10", "tables": {}, "images": {}}], "id": "2405.20849v3", "authors": ["Kuikui Liu", "Sidhanth Mohanty", "Prasad Raghavendra", "Amit Rajaraman", "David X. Wu"], "categories": ["cs.DS", "math.PR"], "abstract": "Many natural Markov chains fail to mix to their stationary distribution in\npolynomially many steps. Often, this slow mixing is inevitable since it is\ncomputationally intractable to sample from their stationary measure.\n  Nevertheless, Markov chains can be shown to always converge quickly to\nmeasures that are locally stationary, i.e., measures that don't change over a\nsmall number of steps. These locally stationary measures are analogous to local\nminima in continuous optimization, while stationary measures correspond to\nglobal minima.\n  While locally stationary measures can be statistically far from stationary\nmeasures, do they enjoy provable theoretical guarantees that have algorithmic\nimplications? We study this question in this work and demonstrate three\nalgorithmic applications of locally stationary measures:\n  1. We show that Glauber dynamics on the hardcore model can be used to find\nindependent sets of size $\\Omega\\left(\\frac{\\log d}{d} \\cdot n\\right)$ in\ntriangle-free graphs of degree at most $d$.\n  2. Let $W$ be a symmetric real matrix with bounded spectral diameter and $v$\nbe a unit vector. Given the matrix $M = \\lambda vv^\\top + W$ with a planted\nrank-one spike along vector $v$, for sufficiently large constant $\\lambda$,\nGlauber dynamics on the Ising model defined by $M$ samples vectors $x \\in \\{\\pm\n1\\}^n$ that have constant correlation with the vector $v$.\n  3. Let $M = A_{\\mathbf{G}} - \\frac{d}{n}\\mathbf{1}\\mathbf{1}^\\top$ be a\ncentered version of the adjacency matrix where the graph $\\mathbf{G}$ is drawn\nfrom a sparse 2-community stochastic block model. We show that for sufficiently\nlarge constant signal-to-noise ratio, Glauber dynamics on the Ising model\ndefined by $M$ samples vectors $x \\in \\{\\pm 1\\}^n$ that have constant\ncorrelation with the hidden community vector $\\mathbf{\\sigma}$.", "updated": "2025-04-08T18:10:29Z", "published": "2024-05-31T14:32:32Z"}