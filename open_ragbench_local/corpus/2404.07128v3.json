{
  "title": "Learning of deep convolutional network image classifiers via stochastic\n  gradient descent and over-parametrization",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nImage classification from independent and identically distributed random variables is considered. Image classifiers are defined which are based on a linear combination of deep convolutional networks with max-pooling layer. Here all the weights are learned by stochastic gradient descent. A general result is presented which shows that the image classifiers are able to approximate the best possible deep convolutional network. In case that the a posteriori probability satisfies a suitable hierarchical composition model it is shown that the corresponding deep convolutional neural network image classifier achieves a rate of convergence which is independent of the dimension of the images.\n\n\nAMS classification: Primary 62G08; secondary 62G20.\nKey words and phrases: Convolutional neural networks, image classification, stochastic gradient descent, over-parametrization, rate of convergence.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "## 1. Introduction\n### 1.1. Scope of the paper\n\nIn image classification the task is to learn the functional relationship between input and output, where the input consists of observed images and the output represents classes of the corresponding images that describe what kind of objects are present in the images. Since many years the most successful approaches in the area of image classification are based on deep convolutional neural networks (CNNs), see, e.g., Krizhevsky, Sutskever and Hinton (2012), LeCun, Bengio and Hinton (2015) and Rawat and Wang (2017). Recently, it has been shown that CNN image classifiers that minimize empirical risk are able to\n\n[^0]\n[^0]:    *Running title: Deep network classifiers\n    ${ }^{\\dagger}$ Corresponding author. Tel:+49 6151 16-23386\n\nachieve dimension reduction (see Kohler, Krzy\u017cak and Walter (2022), Kohler and Langer (2025), Walter (2021) and Kohler and Walter (2023)). However, in practice, it is not possible to compute the empirical risk minimizer. Instead, a gradient descent approach based on smooth surrogate losses and over-parameterized networks having many more trainable parameters than training samples is used.\n\nIn Kohler, Krzy\u017cak and Walter (2023) a plug-in classifier based on convolutional networks, which is learned by gradient descent, has been analyzed. The main result there was that this classifier achieves a dimension reduction in an average-pooling model, however in contrast to the results above for the estimates based on empirical risk minimization the model there does neither use a (more realistic) max-pooling model nor any kind of hierarchical structure.\n\nIn the present paper we consider the case of large datasets such as ImageNet, which make use of gradient descent prohibitively expensive. To be able to deal with such large data sets, we define the estimate by using stochastic gradient descent. In addition, we consider surrogate logistic loss and hierarchical models with max-pooling. Here we show dimensionality reduction and independence of rates from image dimensions.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 1.2. Pattern recognition \n\nWe study image classifiers in the context of pattern recognition. Let $d_{1}, d_{2} \\in \\mathbb{N}$ and let $(X, Y),\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right)$ be independent and identically distributed random variables with values in\n\n$$\n[0,1]^{d_{1} \\times d_{2}} \\times\\{-1,1\\}\n$$\n\nHere we use the notation\n\n$$\n[0,1]^{d_{1} \\times d_{2}}=[0,1]\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}\n$$\n\nand\n\n$$\n[0,1]^{J}=\\left\\{\\left(a_{j}\\right)_{j \\in J}: a_{j} \\in[0,1] \\quad(j \\in J)\\right\\}\n$$\n\nfor a nonempty and finite index set $J$, and we describe a (random) image from (random) class $Y \\in\\{-1,1\\}$ by a (random) matrix $X$ with $d_{1}$ columns and $d_{2}$ rows, which contains at position $(i, j)$ the grey scale value of the pixel of the image at the corresponding position. Our aim is to predict $Y$ given $X$. More precisely, given the data set\n\n$$\n\\mathcal{D}_{n}=\\left\\{\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right)\\right\\}\n$$\n\nthe goal is to construct a classifier\n\n$$\n\\hat{C}_{n}(\\cdot)=\\hat{C}_{n}\\left(\\cdot, \\mathcal{D}_{n}\\right):[0,1]^{d_{1} \\times d_{2}} \\rightarrow\\{-1,1\\}\n$$\n\nsuch that the misclassification probability\n\n$$\n\\mathbf{P}\\left\\{\\hat{C}_{n}(X) \\neq Y \\mid \\mathcal{D}_{n}\\right\\}\n$$\n\nis as small as possible.\n\nLet\n\n$$\n\\eta(x)=\\mathbf{P}\\{Y=1 \\mid X=x\\} \\quad\\left(x \\in[0,1]^{d_{1} \\times d_{2}}\\right)\n$$\n\nbe the so-called a posteriori probability of class 1 . Then\n\n$$\nf^{*}(x)= \\begin{cases}1, & \\text { if } \\eta(x) \\geq \\frac{1}{2} \\\\ -1, & \\text { elsewhere }\\end{cases}\n$$\n\nis the so-called Bayes classifier, i.e., it satisfies\n\n$$\n\\mathbf{P}\\left\\{f^{*}(X) \\neq Y\\right\\}=\\min _{f:[0,1]^{d_{1} \\times d_{2}} \\rightarrow\\{0,1\\}} \\mathbf{P}\\{f(X) \\neq Y\\}\n$$\n\n(cf., e.g., Theorem 2.1 in Devroye, Gy\u00f6rfi and Lugosi (1996)).\nIn this paper we derive upper bounds on\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\mathbf{P}\\left\\{\\hat{C}_{n}(X) \\neq Y \\mid \\mathcal{D}_{n}\\right\\}-\\mathbf{P}\\left\\{f^{*}(X) \\neq Y\\right\\}\\right\\} \\\\\n& =\\mathbf{P}\\left\\{\\hat{C}_{n}(X) \\neq Y\\right\\}-\\min _{f:[0,1]^{d_{1} \\times d_{2}} \\rightarrow\\{0,1\\}} \\mathbf{P}\\{f(X) \\neq Y\\}\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 1.3. Main results \n\nWe define deep convolutional neural network estimates by minimizing the empirical logistic loss of a linear combination of networks via stochastic gradient descent. Here we use a projection step on the weights in order to ensure that we can control the overparametrization of the estimate. We use this estimate to define an image classifier $\\hat{C}_{n}$.\n\nWe show, that in case that the a posteriori probability $\\eta(x)=\\mathbf{P}\\{Y=1 \\mid X=x\\}$ satisfies a $(p, C)$-smooth hierarchical max-pooling model of finite level $l$ and $\\operatorname{supp}\\left(\\mathbf{P}_{X}\\right) \\subseteq$ $[0,1]^{d_{1} \\times d_{2}}$, we have\n\n$$\n\\mathbf{P}\\left\\{Y \\neq \\hat{C}_{n}(X)\\right\\}-\\mathbf{P}\\left\\{Y \\neq f^{*}(X)\\right\\} \\leq c_{1} \\cdot(\\log n)^{2} \\cdot n^{-\\min \\left\\{\\frac{\\kappa}{4 p+8} \\cdot \\frac{1}{8}\\right\\}}\n$$\n\nAnd if, in addition,\n\n$$\n\\mathbf{P}\\left\\{\\max \\left\\{\\frac{\\eta(X)}{1-\\eta(X)}, \\frac{1-\\eta(X)}{\\eta(X)}\\right\\}>n^{\\frac{4}{4}}\\right\\} \\geq 1-\\frac{1}{n^{\\frac{4}{4}}} \\quad(n \\in \\mathbb{N})\n$$\n\nholds (which implies that with high probability $\\eta(X)=\\mathbf{P}\\{Y=1 \\mid X\\}$ is either close to one or close to zero), then we show that the estimates achieve the improved rate of convergence\n\n$$\n\\mathbf{P}\\left\\{Y \\neq \\hat{C}_{n}(X)\\right\\}-\\mathbf{P}\\left\\{Y \\neq f^{*}(X)\\right\\} \\leq c_{2} \\cdot(\\log n)^{4} \\cdot n^{-\\min \\left\\{\\frac{\\kappa}{2 p+4} \\cdot \\frac{1}{4}\\right\\}}\n$$\n\nIn order to prove these results we derive a general result which gives an upper bound on the expected logistic loss of an over-parametrized linear combination of deep convolutional networks learned by minimizing an empirical logistic loss via stochastic gradient descent.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 1.4. Discussion of related results \n\nStochastic gradient descent has been proposed in Robbins and Monroe (1951) and further discussed in Nemirovsky et al. (2008), Polyak and Yuditsky (1992), Spall (2003) and Kushner and Yin (2003). It is an efficient alternative to the standard batch gradient descent (GD) which has high computational complexity owing it to using all large-scale data stored in memory in each iteration and not allowing online updates. In SGD one sample is used to randomly update the gradient in each iteration, instead of directly calculating the exact value of the gradient. SGD is an unbiased estimate of the real gradient, its cost does not depend on the number of samples and it converges with sublinear rate, but achieves the optimal rate for convex problems, cf., e.g., Nemirovsky et al. (2008). SGD algorithms have been used in many classical machine learning problems such as perceptron, k-means, SVM and lasso, see Bottou (2012). Many improvements of classical SGD have been introduced over the years. They include momentum, Nesterov Accelerated GD, Adaptive Learning Rate Method, Adaptive Moment Estimation (ADAM), Stochastic Average Gradient, Stochastic Variance Reduction Gradient and Altering Direction Method of Multipliers, see Sun et al. (2019) for a comprehensive survey. Asymptotic and finite-sample properties of estimators based on stochastic gradients were investigated by Toulis and Airaldi (2017). Statistical inference for model parameters in SGD has been discussed in Chen et al. (2020). An excellent survey of optimization methods for large-scale machine learning including SGD is provided in Bottou, Curtis and Nocedal (2018).\n\nIn recent years much attention has been devoted to properties of deep neural network estimates. There exist quite a few approximation results for neural networks (cf., e.g., Yarotsky (2018), Yarotsky and Zhevnerchute (2019), Lu et al. (2020), Langer (2021) and the literature cited therein). Generalization abilities of deep neural networks can either be analyzed within the framework of the classical VC theory (using e.g. the result of Bartlett et al. (2019) to bound the VC dimension of classes of neural networks) or in case of over-parametrized deep neural networks (where the number of free parameters adjusted to the observed data set is much larger than the sample size) by using bounds on the Rademacher complexity (cf., e.g., Liang, Rakhlin and Sridharan (2015), Golowich, Rakhlin and Shamir (2019), Lin and Zhang (2019), Wang and Ma (2022) and the literature cited therein).\n\nCombining such results leads to a rich theory showing that owing to the network structure the least squares neural network estimates can achieve suitable dimension reduction in hierarchical composition models for the function to be estimated. For a simple model this was first shown by Kohler and Krzy\u017cak (2017) for H\u00f6lder smooth function and later extended to arbitrary smooth functions by Bauer and Kohler (2019). For a more complex hierarchical composition model and the ReLU activation function this was shown in Schmidt-Hieber (2020) under the assumption that the networks satisfy some sparsity constraint. Kohler and Langer (2021) showed that this also possible for fully connected neural networks, i.e., without imposing a sparsity constraint on the network. Adaptation of deep neural network to especially weak smoothness assumptions was shown in Imaizumi and Fukamizu (2018), Suzuki (2018) and Suzuki and Nitanda (2019).\n\nLess well understood is the optimization of deep neural networks. As was shown, e.g., in Zou et al. (2018), Du et al. (2019), Allen-Zhu, Li and Song (2019) and Kawaguchi and Huang (2019) the application of gradient descent to over-parameterized deep neural networks leads to a neural network which (globally) minimizes the empirical risk considered. However, as was shown in Kohler and Krzy\u017cak (2021), the corresponding estimates do not behave well on new independent data. So the main question is why gradient descent (and its variants like stochastic gradient descent) can be used to fit a neural network to observed data in such a way that the resulting estimate achieves good results on new independent data. The challenge here is not only to analyze optimization but to consider it simultaneously with approximation and generalization.\n\nIn case of shallow neural networks (i.e., neural networks with only one hidden layer) this has been done successfully in Braun et al. (2024). Here it was possible to show that the classical dimension free rate of convergence of Barron (1994) for estimation of a regression function where its Fourier transform has a finite moment can also be achieved by shallow neural networks learned by gradient descent. The main idea here is that the gradient descent selects a subset of the neural network where random initialization of the inner weights leads to values with good approximation properties, and that it adjusts the outer weights for these neurons properly. A similar idea was also applied in Gonon (2021). Kohler and Krzy\u017cak (2022) applied this idea in the context of over-parametrized deep neural networks where a linear combination of a huge number of deep neural networks of fixed size are computed in parallel. Here the gradient descent selects again a subset of the neural networks computed in parallel and chooses a proper linear combination of the networks. By using metric entropy bounds (cf., e.g., Birman and Solomnjak (1967) and Li, Gu and Ding (2021)) it is possible to control generalization of the over-parametrized neural networks, and as a result the rate of convergence of order close to $n^{-1 /(1+d)}$ (or $n^{1 /\\left(1+d^{*}\\right)}$ in case of interaction models, where it is assumed that the regression function is a sum of functions applied to only $d^{*}$ of the $d$ components of the predictor variable) can be shown for H\u00f6lder-smooth regression functions with H\u00f6lder exponent $p \\in[1 / 2,1]$. Universal consistency of such estimates for bounded $X$ was shown in Drews and Kohler (2022).\n\nIn all those results adjusting the inner weights with gradient descent is not important. In fact, Gonon (2021) does not do this at all, while Braun et al. (2024) and Kohler and Krzy\u017cak (2022) use the fact that the relevant inner weights do not move too far away from their starting values during gradient descent. Similar ideas have also been applied in Andoni et al. (2014) and Daniely (2017). This whole approach is related to random feature networks (cf., e.g., Huang, Chen and Siew (2006) and Rahimi and Recht (2008a, 2008b, 2009)), where the inner weights are chosen randomly and only the outer weights are learned during gradient descent. Yehudai and Shamir (2022) present a lower bound which implies that either the number of neurons or the absolute value of the coefficients must grow exponential in the dimension in order to learn a single ReLU neuron with random feature networks. But since Braun et al. (2024) was able to prove a useful rate of convergence result for networks similar to random feature networks, the practical relevance of this lower bound is not clear.\n\nThe estimates in Kohler and Krzy\u017cak (2022) use a $L_{2}$ regularization on the outer\n\nweights during gradient descent. As was shown in Drews and Kohler (2023), it is possible to achieve similar results without $L_{2}$ regularization.\n\nOften gradient descent in neural networks is studied in the neural tangent kernel setting proposed by Jacot, Gabriel and Hongler (2020), where instead of a neural network estimate a kernel estimate is studied and its error is used to bound the error of the neural network estimate. For further results in this context see Hanin and Nica (2019) and the literature cited therein. Suzuki and Nitanda (2019) were able to analyze the global error of an over-parametrized shallow neural network learned by gradient descent based on this approach. However, due to the use of the neural tangent kernel, also the smoothness assumption of the function to be estimated has to be defined with the aid of a norm involving the kernel, which does not lead to the classical smoothness conditions of our paper. Another approach where the estimate is studied in some asymptotically equivalent model is the mean field approach, cf., Mei, Montanari, and Nguyen (2018), Chizat and Bach (2018) or Nguyen and Pham (2020). A survey of various results on overparametrized deep neural network estimates learned by gradient descent can be found in Bartlett, Montanari and Rakhlin (2021).\n\nIn recent years deep transformer networks became very popular in research and applications. They have been introduced by Vaswani et al. (2017) and their approximation and generalization properties have been investigated by Gurevych et al. (2022). The rates of convergence of over-parametrized transformer classifiers learned by gradient descent have been studied by Kohler and Krzy\u017cak (2023).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 1.5. Notation \n\nThe sets of natural numbers, real numbers and nonnegative real numbers are denoted by $\\mathbb{N}, \\mathbb{R}$ and $\\mathbb{R}_{+}$, respectively. We define furthermore $\\overline{\\mathbb{R}}=\\mathbb{R} \\cup\\{-\\infty, \\infty\\}$. For $z \\in \\mathbb{R}$, we denote the smallest integer greater than or equal to $z$ by $\\lceil z\\rceil$, the largest integer less than or equal to $z$ by $\\lfloor z\\rfloor$, and we set $z_{+}=\\max \\{z, 0\\}$. The Euclidean norm of $x \\in \\mathbb{R}^{d}$ is denoted by $\\|x\\|$. For a closed and convex set $A \\subseteq \\mathbb{R}^{d}$ we denote by $\\operatorname{Proj}_{A} x$ that element $\\operatorname{Proj}_{A} x \\in A$ with\n\n$$\n\\left\\|x-\\operatorname{Proj}_{A} x\\right\\|=\\min _{z \\in A}\\|x-z\\| .\n$$\n\nFor $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$\n\n$$\n\\|f\\|_{\\infty}=\\sup _{x \\in \\mathbb{R}^{d}}|f(x)|\n$$\n\nis its supremum norm, and we set\n\n$$\n\\|f\\|_{\\infty, A}=\\sup _{x \\in A}|f(x)|\n$$\n\nfor $A \\subseteq \\mathbb{R}^{d}$.\nFor $\\mathbf{j}=\\left(j^{(1)}, \\ldots, j^{(d)}\\right) \\in \\mathbb{N}_{0}^{d}$ we write\n\n$$\n\\|\\mathbf{j}\\|_{1}=j^{(1)}+\\cdots+j^{(d)}\n$$\n\nand for $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ we set\n\n$$\n\\partial^{1} f=\\frac{\\partial^{\\| \\mid \\\\|_{1}} f}{\\left(\\partial x^{(1)}\\right)^{j(1)} \\ldots\\left(\\partial x^{(d)}\\right)^{(d)}}\n$$\n\nLet $\\mathcal{F}$ be a set of functions $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$, let $x_{1}, \\ldots, x_{n} \\in \\mathbb{R}^{d}$, set $x_{1}^{n}=\\left(x_{1}, \\ldots, x_{n}\\right)$ and let $p \\geq 1$. A finite collection $f_{1}, \\ldots, f_{N}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ is called an $L_{p} \\varepsilon$-packing in $\\mathcal{F}$ on $x_{1}^{n}$ if $f_{1}, \\ldots, f_{N} \\in \\mathcal{F}$ and\n\n$$\n\\min _{1 \\leq i<j \\leq N}\\left(\\frac{1}{n} \\sum_{k=1}^{n}\\left|f_{i}\\left(x_{k}\\right)-f_{j}\\left(x_{k}\\right)\\right|^{p}\\right)^{1 / p} \\geq \\varepsilon\n$$\n\nhold. The $L_{p} \\varepsilon$-packing number of $\\mathcal{F}$ on $x_{1}^{n}$ is the size $N$ of the largest $L_{p} \\varepsilon$-packing of $\\mathcal{F}$ on $x_{1}^{n}$ and is denoted by $\\mathcal{M}_{p}\\left(\\varepsilon, \\mathcal{F}, x_{1}^{n}\\right)$.\n\nFor $z \\in \\mathbb{R}$ and $\\beta>0$ we define $T_{\\beta} z=\\max \\{-\\beta, \\min \\{\\beta, z\\}\\}$. If $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ is a function then we set $\\left(T_{\\beta} f\\right)(x)=T_{\\beta}(f(x))$. And $\\operatorname{sign}(z)$ is the sign of $z \\in \\overline{\\mathbb{R}}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "# 1.6. Outline \n\nA general result, namely a bound on the logistic risk of an over-parametrized deep convolutional network fitted to data via stochastic gradient descent, is presented in Section 2. The over-parametrized deep convolutional neural network classifiers considered in this paper are introduced in Section 3 and a bound on their misclassification probability is also presented in this section. Section 4 contains the proofs.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "## 2. A general result\n\nLet $\\boldsymbol{\\Theta}$ be a closed and convex set of parameter values (weights) for a deep convolutional network of a given topology. In the sequel we assume that our aim is to learn the parameter $\\vartheta \\in \\boldsymbol{\\Theta}$ (vector of weights) for a deep convolutional network\n\n$$\nf_{\\vartheta}:[0,1]^{d_{1} \\times d_{2}} \\rightarrow \\mathbb{R}\n$$\n\nfrom the data $\\mathcal{D}_{n}$ such that\n\n$$\n\\operatorname{sign}\\left(f_{\\vartheta}(x)\\right)\n$$\n\nis a good classifier. We do this by considering linear combinations\n\n$$\nf_{(\\mathbf{w}, \\vartheta)}(x)=\\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}}\\left(f_{\\vartheta_{k}}(x)\\right)\n$$\n\nof truncated versions of deep convolutional networks $f_{\\vartheta_{k}}(x)\\left(k=1, \\ldots, K_{n}\\right)$, where $\\mathbf{w}=\\left(w_{k}\\right)_{k=1, \\ldots, K_{n}}$ satisfies\n\n$$\nw_{k} \\geq 0 \\quad\\left(k=1, \\ldots, K_{n}\\right), \\quad \\sum_{k=1}^{K_{n}} w_{k} \\leq 1 \\quad \\text { and } \\quad \\sum_{k=1}^{K_{n}} w_{k}^{2} \\leq \\alpha_{n}\n$$\n\nfor some $\\alpha_{n} \\in[0,1]$, where $\\vartheta=\\left(\\vartheta_{1}, \\ldots, \\vartheta_{K_{n}}\\right) \\in \\boldsymbol{\\Theta}^{K_{n}}$ and where $\\beta_{n}=c_{3} \\cdot \\log n$. Observe that by choosing $\\alpha_{n}=\\frac{1}{N_{n}}, w_{j}=\\frac{1}{N_{n}}\\left(j=1, \\ldots, N_{n}\\right), \\vartheta_{j}=\\vartheta\\left(j=1, \\ldots, N_{n}\\right)$ and $w_{k}=0$ for $k>N_{n}$ we get\n\n$$\nf_{(\\mathbf{w}, \\vartheta)}(x)=T_{\\beta_{n}}\\left(f_{\\vartheta}(x)\\right)\n$$\n\nand in this way we can construct an estimate which satisfies\n\n$$\n\\operatorname{sign}\\left(f_{(\\mathbf{w}, \\vartheta)}(x)\\right)=\\operatorname{sign}\\left(f_{\\vartheta}(x)\\right)\n$$\n\nfor any $\\vartheta \\in \\boldsymbol{\\Theta}$. And by choosing $K_{n}$ very large our estimate will be over-parametrized in the sense that the number of parameters of the estimate is much larger than the sample size.\n\nLet\n\n$$\n\\varphi(z)=\\log (1+\\exp (-z))\n$$\n\nbe the logistic loss (or cross entropy loss). Our aim in choosing ( $\\mathbf{w}, \\vartheta$ ) is the minimization of the logistic risk\n\n$$\nF((\\mathbf{w}, \\vartheta))=\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{(\\mathbf{w}, \\vartheta)}(X)\\right)\\right\\}\n$$\n\nIn order to achieve this, we start with a random initialization of $(\\mathbf{w}, \\vartheta)$ : We choose\n\n$$\n\\vartheta_{1}^{(0)}, \\ldots, \\vartheta_{K_{n}}^{(0)}\n$$\n\nuniformly from some closed and convex set $\\boldsymbol{\\Theta}^{0} \\subseteq \\boldsymbol{\\Theta}$ such that the random variables in (5) are independent and also independent from $(X, Y),\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right)$, and we set\n\n$$\nw_{k}^{(0)}=0 \\quad\\left(k=1, \\ldots, K_{n}\\right)\n$$\n\nThen we perform $t_{n} \\in \\mathbb{N}$ stochastic gradient descent steps starting with\n\n$$\n\\vartheta^{(0)}=\\left(\\vartheta_{1}^{(0)}, \\ldots, \\vartheta_{K_{n}}^{(0)}\\right) \\quad \\text { and } \\quad \\mathbf{w}^{(0)}=\\left(w_{1}^{(0)}, \\ldots, w_{K_{n}}^{(0)}\\right)\n$$\n\nHere we assume that $t_{n} / n$ is a natural number, and for $s \\in\\left\\{1, \\ldots, t_{n} / n\\right\\}$ we let\n\n$$\nj_{(s-1) \\cdot n}, \\ldots, j_{s \\cdot n-1}\n$$\n\nbe an arbitrary permutation of $1, \\ldots, n$, we choose a stepsize $\\lambda_{n}>0$ and we set\n\n$$\n\\begin{aligned}\n\\mathbf{w}^{(t+1)} & =\\operatorname{Proj}_{A}\\left(\\mathbf{w}^{(t)}-\\lambda_{n} \\cdot \\nabla_{\\mathbf{w}} \\varphi\\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)\\right)\\right) \\\\\n\\vartheta^{(t+1)} & =\\operatorname{Proj}_{B}\\left(\\vartheta^{(t)}-\\lambda_{n} \\cdot \\nabla_{\\vartheta} \\varphi\\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)\\right)\\right)\n\\end{aligned}\n$$\n\nfor $t=0, \\ldots, t_{n}-1$. Here $A$ is the set of all $\\mathbf{w}$ which satisfy (4), and\n\n$$\nB=\\left\\{\\vartheta \\in \\boldsymbol{\\Theta}^{K_{n}}:\\left\\|\\vartheta-\\vartheta^{(0)}\\right\\| \\leq 1\\right\\}\n$$\n\nand $\\operatorname{Proj}_{A}$ and $\\operatorname{Proj}_{B}$ is the $L_{2}$ projection on $A$ and $B$. Our estimate is then defined by\n\n$$\nf_{n}(x)=f_{\\left(\\hat{\\mathbf{w}}, \\vartheta^{\\left(t_{n}\\right)}\\right)}(x)\n$$\n\nwhere\n\n$$\n\\hat{\\mathbf{w}}=\\frac{1}{t_{n}} \\cdot \\sum_{t=0}^{t_{n}-1} \\mathbf{w}^{(t)}\n$$\n\nOur main result in this general setting is the following bound on the logistic risk of the above estimate.\n\nTheorem 1 Let $(X, Y),\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right)$ be independent and identically distributed random variables with values in $[0,1]^{d_{1} \\times d_{2}} \\times\\{-1,1\\}$. Let $N_{n}, I_{n}, t_{n} \\in \\mathbb{N}$ and let $C_{n}, D_{n} \\geq$ 0 . Set $\\beta_{n}=c_{3} \\cdot \\log n$,\n\n$$\n\\alpha_{n}=\\frac{1}{N_{n}}, \\quad \\lambda_{n}=\\frac{1}{t_{n}}, \\quad K_{n}=N_{n} \\cdot I_{n}\n$$\n\nand define the estimate $f_{n}$ as above.\nAssume that there exists $\\tilde{\\vartheta} \\in \\boldsymbol{\\Theta}^{0}$, such that $f_{\\tilde{\\vartheta}}(X)=0$, let $\\boldsymbol{\\Theta}^{*} \\subset \\boldsymbol{\\Theta}^{0}$ and set\n\n$$\n\\bar{\\Theta}=\\left\\{\\vartheta \\in \\boldsymbol{\\Theta}: \\inf _{\\tilde{\\vartheta} \\in \\boldsymbol{\\Theta}^{0}}\\|\\vartheta-\\tilde{\\vartheta}\\| \\leq 1\\right\\}\n$$\n\nAssume\n\n$$\n\\begin{gathered}\n\\left\\|f_{\\vartheta}-f_{\\tilde{\\vartheta}}\\right\\|_{\\infty, \\operatorname{supp}(X)} \\leq C_{n} \\cdot\\|\\vartheta-\\tilde{\\vartheta}\\|_{\\infty} \\quad \\text { for all } \\vartheta, \\tilde{\\vartheta} \\in \\bar{\\Theta} \\\\\n\\kappa_{n}=\\mathbf{P}\\left\\{\\vartheta_{1}^{(0)} \\in \\boldsymbol{\\Theta}^{*}\\right\\}>0 \\\\\nN_{n} \\cdot\\left(1-\\kappa_{n}\\right)^{I_{n}} \\leq \\frac{1}{n}\n\\end{gathered}\n$$\n\nand\n\n$$\n\\left\\|\\nabla_{\\mathbf{w}} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}\\right)\\right\\| \\leq D_{n}\n$$\n\nfor all $x \\in[0,1]^{d_{1} \\times d_{2}}, y \\in\\{-1,1\\}, \\mathbf{w} \\in A, \\vartheta \\in \\bar{\\Theta}, t \\in\\left\\{0, \\ldots, t_{n}-1\\right\\}$.\nThen we have\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right)\\right\\}-\\min _{f:[0,1]^{d_{1} \\times d_{2}} \\rightarrow \\mathbb{R}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\\\\n& \\leq c_{4} \\cdot\\left(\\frac{\\log n}{n}+\\beta_{n} \\cdot \\sup _{x_{1}, \\ldots, x_{n} \\in[0,1]^{d_{1} \\times d_{2}}} \\mathbf{E}\\left\\{\\left|\\sup _{\\vartheta \\in \\bar{\\Theta}} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta}\\left(x_{i}\\right)\\right|\\right\\}+\\frac{C_{n}+1}{\\sqrt{N_{n}}}+\\frac{D_{n}^{2}}{t_{n}}\\right. \\\\\n& \\left.+\\frac{n \\cdot\\left(6 \\cdot K_{n} \\cdot \\beta_{n}^{2}+4 \\cdot\\left(\\beta_{n}+1\\right) \\cdot C_{n} \\cdot \\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, y \\in\\{-1,1\\}, x \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\vartheta} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|_{\\infty}\\right)}{t_{n}}\\right) \\\\\n& \\quad+\\sup _{\\vartheta \\in \\boldsymbol{\\Theta}^{*}} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot T_{\\beta_{n}} f_{\\vartheta}(X)\\right)\\right\\}-\\min _{f:[0,1]^{d_{1} \\times d_{2}} \\rightarrow \\mathbb{R}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\}\\right)\n\\end{aligned}\n$$\n\nwhere $\\epsilon_{1}, \\ldots, \\epsilon_{n}$ are independent and uniformly distributed on $\\{-1,1\\}$ (so-called Rademacher random variables).\n\nRemark 1. In Theorem 2 we use the Rademacher complexity\n\n$$\n\\mathbf{E}\\left\\{\\sup _{\\vartheta \\in \\boldsymbol{\\Theta}}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}}\\left(f_{\\vartheta}\\left(x_{i}\\right)\\right)\\right|\\right\\}\n$$\n\nto control the generalization error of the estimate. The approximation error is measured by the term\n\n$$\n\\sup _{\\vartheta \\in \\boldsymbol{\\Theta}^{*}} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\vartheta}(X)\\right)\\right\\}-\\min _{f: \\mathbb{R}^{d} \\rightarrow \\overline{\\mathbb{R}}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\}\n$$\n\nwhich describes the maximal error occuring in the set $\\Theta^{*}$. The last three terms are used to bound the error which occurs during optimization, i.e. due to stochastic gradient descent.\n\nRemark 2. Our result above extends Theorem 2 in Kohler and Krzy\u017cak (2023) from gradient descent to the case of stochastic gradient descent. To be able to do this we need in the definition of the estimate an additional $L_{2}$ penalty on the weights in the linear combination of the networks (depending on $\\alpha_{n}$ ). Furthermore, assumption (8) is substantially stronger than the corresponding assumption in Theorem 2 in Kohler and Krzy\u017cak (2023), because there it is only required that (8) holds for networks which have good approximation properties (which is because of the maximal attention used in Transformer networks crucial for Transformer networks).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "# 3. Image classification using deep convolutional neural networks\n### 3.1. Convolutional neural network classifiers\n\nWe aim to learn feature representations of the inputs by means of $L$ (hidden) convolutional layers. Each of these $r \\in\\{1, \\ldots, L\\}$ feature maps consists of $k_{r}$ channels. The input image is considered as layer 0 with only one channel, i.e. $k_{0}=1$.\nA convolution in layer $r$ is performed by using a window of values of the previous layer $r-1$ of size $M_{r}$. The window has to fit within the dimensions of the input image, i.e. $M_{r} \\leq \\min \\left\\{d_{1}, d_{2}\\right\\}$. It relies on so-called filters, i.e. a weight matrix that determines how a neuron is computed from a weighted sum of neighboring neurons from the previous layer. The weight matrix is defined by\n\n$$\n\\mathbf{w}=\\left(w_{i, j, s_{1}, s_{2}}^{(r)}\\right)_{1 \\leq i, j \\leq M_{r}, s_{1} \\in\\left\\{1, \\ldots, k_{r-1}\\right\\}, s_{2} \\in\\left\\{1, \\ldots, k_{r}\\right\\}, r \\in\\{1, \\ldots, L\\}}\n$$\n\nFurthermore we need some weights\n\n$$\n\\mathbf{w}_{\\text {bias }}=\\left(w_{s_{2}}^{(r)}\\right)_{s_{2} \\in\\left\\{1, \\ldots, k_{r}\\right\\}, r \\in\\{1, \\ldots, L\\}}\n$$\n\nfor the bias in each channel and output weights\n\n$$\n\\mathbf{w}_{\\text {out }}=\\left(w_{s}\\right)_{s \\in\\left\\{1, \\ldots, k_{L}\\right\\}}\n$$\n\nwhich are required for the max-pooling layer defined below.\nIn the following the ReLU function $\\sigma(x)=\\max \\{x, 0\\}$ is chosen as activation function. The value of a feature map in the $s_{2}$-th channel of layer $r$ at the position $(i, j)$ is recursively defined by:\n\n$$\no_{(i, j), s_{2}}^{(r)}=\\sigma\\left(\\sum_{\\substack{k_{r-1} \\\\ s_{1}=1}}^{k_{r-1}} \\sum_{\\substack{t_{1}, t_{2} \\in\\left\\{1, \\ldots, M_{r}\\right\\} \\\\\\left(i+t_{1}-1, j+t_{2}-1\\right) \\in D}} w_{t_{1}, t_{2}, s_{1}, s_{2}}^{(r)} o_{\\left(i+t_{1}-1, j+t_{2}-1\\right), s_{1}}^{(r-1)}+w_{s_{2}}^{(r)}\\right)\n$$\n\nwhere $(i, j) \\in D=\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}, s_{2} \\in\\left\\{1, \\ldots, k_{r}\\right\\}$ and $r \\in\\{1, \\ldots, L\\}$.\nThe anchor case $r=0$ of this recursion reflects the values of the input image\n\n$$\no_{(i, j), 1}^{(0)}=x_{i, j} \\quad \\text { for } i \\in\\left\\{1, \\ldots, d_{1}\\right\\} \\text { and } j \\in\\left\\{1, \\ldots, d_{2}\\right\\}\n$$\n\nIn definition (12) above we see that weights generating the feature map $o_{(\\ldots), s 2}^{(r)}$ are shared. Weight sharing is used to reduce model complexity, thereby increasing the network's computational efficiency. In the last step a max-pooling layer is applied to the values in the $k_{L}$ channels of the last convolutional layer $L$, such that the output of the network is given by a real-valued function on $[0,1]^{\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}}$ of the form\n\n$$\n\\begin{aligned}\n& f_{\\mathbf{w}, \\mathbf{w}_{\\text {bias }}, \\mathbf{w}_{\\text {out }}}(x)=\\max \\left\\{\\sum_{s_{2}=1}^{k_{L}} w_{s_{2}} \\cdot o_{(i, j), s_{2}}^{(L)}: i \\in\\left\\{1, \\ldots, d_{1}-M_{L}+1\\right\\}\\right. \\\\\n& \\left.\\quad j \\in\\left\\{1, \\ldots, d_{2}-M_{L}+1\\right\\}\\right\\}\n\\end{aligned}\n$$\n\nOur class of convolutional neural networks with parameters $L, \\mathbf{k}=\\left(k_{1}, \\ldots, k_{L}\\right)$ and $\\mathbf{M}=\\left(M_{1}, \\ldots, M_{L}\\right)$ is defined by $\\mathcal{F}_{L, \\mathbf{k}, \\mathbf{M}^{\\prime}}^{C N N}$. As in Kohler, Krzy\u017cak and Walter (2022), the definition of the summation index over $t_{1}, t_{2} \\in\\left\\{1, \\ldots, M_{r}\\right\\}$, such that $1 \\leq i+t_{1}-1 \\leq d_{1}$ and $1 \\leq j+t_{2}-1 \\leq d_{2}$, corresponds to zero padding to the left and to the bottom of the image. Thus, the size of a channel is the same as in the previous layer (see Kohler, Krzy\u017cak and Walter (2022) for a further illustration). Our final estimate is a composition of a convolutional neural network out of the class $\\mathcal{F}_{L, \\mathbf{k}, \\mathbf{M}}^{C N N}$ and a shallow neural network, which is defined as follows: The output of this network is produced by a function $g: \\mathbb{R} \\rightarrow \\mathbb{R}$ of the form\n\n$$\ng(x)=\\sum_{i=1}^{L_{n}^{(2)}} w_{i}^{(1)} \\sigma\\left(w_{i, 1}^{(0)} \\cdot x+w_{i, 0}^{(0)}\\right)+w_{0}^{(1)}\n$$\n\nwhere $w_{0}^{(1)}, w_{1}^{(1)}, w_{1,0}^{(1)}, w_{1,1}^{(1)}, \\ldots, w_{L_{n}^{(2)}}^{(1)}, w_{L_{n}^{(2)}, 0}^{(0)}, w_{L_{n}^{(2)}, 1}^{(0)} \\in \\mathbb{R}$ denote the weights of this network and $\\sigma(z)=\\max \\{z, 0\\}$ is again the ReLU activation function. We define the function class of all real-valued functions on $\\mathbb{R}$ of the form (13) with parameter $L_{n}^{(2)}$ by $\\mathcal{F}_{L_{n}^{(2)}}^{F N N}$.\n\nOur final function class $\\mathcal{F}_{n}$ is then of the form\n\n$$\n\\mathcal{F}_{n}=\\left\\{g \\circ f: g \\in \\mathcal{F}_{L_{n}^{(2)}}^{F N N}, f \\in \\mathcal{F}_{L_{n}^{(1)}, \\mathbf{k}, \\mathbf{M}}^{C N N}\\right\\}\n$$\n\nwhich depends on the parameters\n\n$$\n\\mathbf{L}=\\left(L_{n}^{(1)}, L_{n}^{(2)}\\right), \\mathbf{k}=\\left(k_{1}, \\ldots, k_{L_{n}^{(1)}}\\right), \\mathbf{M}=\\left(M_{1}, \\ldots, M_{L_{n}^{(1)}}\\right)\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 3.2. Definition of the estimate \n\nLet $\\Theta$ be the set of all weights of the function class\n\n$$\n\\mathcal{F}_{n}=\\left\\{f_{\\vartheta}: \\theta \\in \\Theta\\right\\}\n$$\n\nintroduced in the previous subsection. In the sequel we fit a linear combination\n\n$$\nf_{(\\mathbf{w}, \\vartheta)}(x)=\\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\n$$\n\nto the data where $\\mathbf{w}$ satisfies the assumption (4) and $\\vartheta=\\left(\\theta_{1}, \\ldots, \\theta_{K_{n}}\\right) \\in \\Theta^{K_{n}}$.\nDepending on some $B_{n}>0$, which will be defined in Theorem 2 below, we choose\n\n$$\n\\vartheta_{1}^{(0)}, \\ldots, \\vartheta_{K_{n}}^{(0)}\n$$\n\nuniformly from\n\n$$\n\\boldsymbol{\\Theta}^{0}=\\left\\{\\vartheta \\in \\boldsymbol{\\Theta} \\quad: \\quad\\|\\vartheta\\|_{\\infty} \\leq B_{n}\\right\\}\n$$\n\nsuch that the random variables in (15) are independent and also independent from $(X, Y)$, $\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right)$ and we set\n\n$$\nw_{k}^{(0)}=0 \\quad\\left(k=1, \\ldots, K_{n}\\right)\n$$\n\nThen we perform $t_{n} \\in \\mathbb{N}$ stochastic gradient descent steps starting with\n\n$$\n\\vartheta^{(0)}=\\left(\\vartheta_{1}^{(0)}, \\ldots, \\vartheta_{K_{n}}^{(0)}\\right) \\quad \\text { and } \\quad \\mathbf{w}^{(0)}=\\left(w_{1}^{(0)}, \\ldots, w_{K_{n}}^{(0)}\\right)\n$$\n\nAs in the previous section we assume that $t_{n} / n$ is a natural number and for $s \\in$ $\\left\\{1, \\ldots, t_{n} / n\\right\\}$ we let\n\n$$\nj_{(s-1) \\cdot n}, \\ldots, j_{s \\cdot n-1}\n$$\n\nbe an arbitrary permutation of $1, \\ldots, n$. We choose a stepsize $\\lambda_{n}>0$ and set\n\n$$\n\\begin{aligned}\n\\mathbf{w}^{(t+1)} & =\\operatorname{Proj}_{A}\\left(\\mathbf{w}^{(t)}-\\lambda_{n} \\cdot \\nabla_{\\mathbf{w}} \\varphi\\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)\\right)\\right) \\\\\n\\vartheta^{(t+1)} & =\\operatorname{Proj}_{B}\\left(\\vartheta^{(t)}-\\lambda_{n} \\cdot \\nabla_{\\vartheta} \\varphi\\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)\\right)\\right)\n\\end{aligned}\n$$\n\nfor $t=0, \\ldots, t_{n}-1$. Here $A$ is the set of all $\\mathbf{w}$ which satisfy (4), and\n\n$$\nB=\\left\\{\\vartheta \\in\\left(\\boldsymbol{\\Theta}^{(0)}\\right)^{K_{n}}:\\left\\|\\vartheta-\\vartheta^{(0)}\\right\\| \\leq 1\\right\\}\n$$\n\nand $\\operatorname{Proj}_{A}$ and $\\operatorname{Proj}_{B}$ is the $L_{2}$ projection on $A$ and $B$. In order to compute the gradient with respect to the inner weights we use the following convention: We set\n\n$$\n\\begin{gathered}\n\\sigma^{\\prime}(z)=\\frac{\\partial}{\\partial z} \\max \\{z, 0\\}= \\begin{cases}1, & \\text { if } z \\geq 0 \\\\\n0, & \\text { else }\\end{cases} \\\\\n\\frac{\\partial}{\\partial z} T_{\\beta_{n}} z=\\frac{\\partial}{\\partial z} \\max \\left\\{-\\beta_{n}, \\min \\left\\{\\beta_{n}, z\\right\\}\\right\\}= \\begin{cases}1, & \\text { if }|z| \\leq \\beta_{n} \\\\\n0, & \\text { else }\\end{cases}\n\\end{gathered}\n$$\n\nand\n\n$$\n\\frac{\\partial}{\\partial \\vartheta_{j}} \\max \\left\\{f_{\\vartheta_{1}}(x), \\ldots, f_{\\vartheta_{L}}(x)\\right\\}=\\frac{\\partial}{\\partial \\vartheta_{j}} f_{\\vartheta_{l}}(x)\n$$\n\nwhere $l \\in\\{1, \\ldots, L\\}$ satisfies\n\n$$\n\\max \\left\\{f_{\\vartheta_{1}}(x), \\ldots, f_{\\vartheta_{L}}(x)\\right\\}=f_{\\vartheta_{l}}(x)\n$$\n\nand\n\n$$\nl=1 \\quad \\text { or } \\quad \\max \\left\\{f_{\\vartheta_{1}}(x), \\ldots, f_{\\vartheta_{l-1}}(x)\\right\\}<f_{\\vartheta_{l}}(x)\n$$\n\nOur classifier $\\hat{C}_{n}(x)$ is then defined by\n\n$$\n\\hat{C}_{n}(x)=\\operatorname{sign}\\left(f_{n}(x)\\right)\n$$\n\nwhere\n\n$$\nf_{n}(x)=f_{\\left(\\hat{\\mathbf{w}}, \\vartheta^{(t_{n})}\\right)}(x) \\quad \\text { and } \\quad \\hat{\\mathbf{w}}=\\frac{1}{t_{n}} \\cdot \\sum_{t=0}^{t_{n}-1} \\mathbf{w}^{(t)}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# 3.3. Main result \n\nIt is well known that one needs smoothness assumptions on the a posteriori probability in order to derive non-trivial rate of convergence results for the difference between the misclassification risk of any estimate and the optimal misclassification risk (cf., e.g., Cover (1968) and Section 3 in Devroye and Wagner (1982)). For this we will use our next definition.\n\nDefinition 1 Let $p=q+s$ for some $q \\in \\mathbb{N}_{0}$ and $0<s \\leq 1$. A function $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ is called $(p, C)$-smooth, if for every $\\alpha=\\left(\\alpha_{1}, \\ldots, \\alpha_{d}\\right) \\in \\mathbb{N}_{0}^{d}$ with $\\sum_{j=1}^{d} \\alpha_{j}=q$ the partial derivative $\\frac{\\partial^{q} f}{\\partial x_{1}^{\\alpha_{1}} \\ldots \\partial x_{d}^{\\alpha_{d}}}$ exists and satisfies\n\n$$\n\\left|\\frac{\\partial^{q} f}{\\partial x_{1}^{\\alpha_{1}} \\ldots \\partial x_{d}^{\\alpha_{d}}}(x)-\\frac{\\partial^{q} f}{\\partial x_{1}^{\\alpha_{1}} \\ldots \\partial x_{d}^{\\alpha_{d}}}(\\mathbf{z})\\right| \\leq C \\cdot\\|x-\\mathbf{z}\\|^{s}\n$$\n\nfor all $x, \\mathbf{z} \\in \\mathbb{R}^{d}$.\n\nFurthermore we will use a model from Kohler, Krzy\u017cak and Walter (2022) to be able to derive rates of convergence which do not depend on the dimension $d_{1} \\cdot d_{2}$ of the images. In order to be able to introduce this model, we need the following notation: For $M \\subseteq \\mathbb{R}^{d}$ and $x \\in \\mathbb{R}^{d}$ we define\n\n$$\nx+M=\\{x+\\mathbf{z}: \\mathbf{z} \\in M\\}\n$$\n\nFor $I \\subseteq\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}$ and $x=\\left(x_{i}\\right)_{i \\in\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}} \\in[0,1]^{\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}}$ we set\n\n$$\nx_{I}=\\left(x_{i}\\right)_{i \\in I}\n$$\n\nThe basic idea behind the next definition is that the a posteriori probability is a maximum of probabilities that special objects occur in subparts of the image, and that the decision about the latter events are hierarchically decided.\nThis notion is inspired by the way humans would generally proceed with a visual recognition task, specifically the task of deciding whether an image contains a certain object or not. The question whether or not an object can be detected in an image is usually solved in several sub-tasks in the sense that an individual mentally divides the image hierarchically into several parts, scanning each of them for the desired object and thereby estimating the likelihood of the object being present in each part. This idea leads to the hierarchical model introduced in part b) of the definition below.\nNaturally, this idea then leads to the assumption that the probability that an image contains the desired object is simply the maximum of the probabilities estimated in each sub-task, i.e. in each subpart of the image, which motivates the introduction the max-pooling model in part a) of definition 2 .\n\nDefinition 2 Let $d_{1}, d_{2} \\in \\mathbb{N}$ with $d_{1}, d_{2}>1$ and $m:[0,1]^{\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}} \\rightarrow \\mathbb{R}$.\na) We say that $m$ satisfies a max-pooling model with index set\n\n$$\nI \\subseteq\\left\\{0, \\ldots, d_{1}-1\\right\\} \\times\\left\\{0, \\ldots, d_{2}-1\\right\\}\n$$\n\nif there exists a function $f:[0,1]^{(1,1)+I} \\rightarrow \\mathbb{R}$ such that\n\n$$\nm(x)=\\max _{(i, j) \\in \\mathbb{Z}^{2}:(i, j)+I \\subseteq\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}} f\\left(x_{(i, j)+I}\\right) \\quad\\left(x \\in[0,1]^{\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}}\\right)\n$$\n\nb) Let $I=\\left\\{0, \\ldots, 2^{l}-1\\right\\} \\times\\left\\{0, \\ldots, 2^{l}-1\\right\\}$ for some $l \\in \\mathbb{N}$. We say that\n\n$$\nf:[0,1]^{\\left\\{1, \\ldots, 2^{l}\\right\\} \\times\\left\\{1, \\ldots, 2^{l}\\right\\}} \\rightarrow \\mathbb{R}\n$$\n\nsatisfies a hierarchical model of level $l$, if there exist functions\n\n$$\ng_{k, s}: \\mathbb{R}^{4} \\rightarrow[0,1] \\quad\\left(k=1, \\ldots, l, s=1, \\ldots, 4^{l-k}\\right)\n$$\n\nsuch that we have\n\n$$\nf=f_{l, 1}\n$$\n\nfor some $f_{k, s}:[0,1]^{\\left\\{1, \\ldots, 2^{k}\\right\\} \\times\\left\\{1, \\ldots, 2^{k}\\right\\}} \\rightarrow \\mathbb{R}$ recursively defined by\n\n$$\nf_{k, s}(x)=g_{k, s}\\left(f_{k-1,4 \\cdot(s-1)+1}\\left(x_{\\left\\{1, \\ldots, 2^{k-1}\\right\\} \\times\\left\\{1, \\ldots, 2^{k-1}\\right\\}}\\right)\\right.\n$$\n\n$$\n\\begin{aligned}\n& f_{k-1,4 \\cdot(s-1)+2}\\left(x_{\\left\\{1, \\ldots, 2^{k-1}\\right\\} \\times\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\}}\\right) \\\\\n& f_{k-1,4 \\cdot(s-1)+3}\\left(x_{\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\} \\times\\left\\{1, \\ldots, 2^{k-1}\\right\\}}\\right) \\\\\n& f_{k-1,4 \\cdot s}\\left(x_{\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\} \\times\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\}}\\right)\\right) \\\\\n& \\left(x \\in[0,1]^{\\left\\{1, \\ldots, 2^{k}\\right\\} \\times\\left\\{1, \\ldots, 2^{k}\\right\\}}\\right)\n\\end{aligned}\n$$\n\nfor $k=2, \\ldots, l, s=1, \\ldots, 4^{l-k}$, and\n\n$$\nf_{1, s}\\left(x_{1,1}, x_{1,2}, x_{2,1}, x_{2,2}\\right)=g_{1, s}\\left(x_{1,1}, x_{1,2}, x_{2,1}, x_{2,2}\\right) \\quad\\left(x_{1,1}, x_{1,2}, x_{2,1}, x_{2,2} \\in[0,1]\\right)\n$$\n\nfor $s=1, \\ldots, 4^{l-1}$.\nc) We say that $m:[0,1]^{\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}} \\rightarrow \\mathbb{R}$ satisfies a hierarchical max-pooling model of level $l$ (where $2^{l} \\leq \\min \\left\\{d_{1}, d_{2}\\right\\}$ ), if $m$ satisfies a max-pooling model with index set\n\n$$\nI=\\left\\{0, \\ldots, 2^{l}-1\\right\\} \\times\\left\\{0, \\ldots, 2^{l}-1\\right\\}\n$$\n\nand the function $f:[0,1]^{(1,1)+I} \\rightarrow \\mathbb{R}$ in the definition of this max-pooling model satisfies a hierarchical model with level $l$.\nd) We say that the hierarchical max-pooling model $m:[0,1]^{\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}} \\rightarrow \\mathbb{R}$ of level $l$ is $(p, C)$-smooth if all functions $g_{k, s}$ in the definition of the function $m$ are $(p, C)$-smooth for some $C>0$.\n\nOur main result is the following theorem, in which bounds on the difference between the misclassification probability of our classifier and the optimal misclassification probability are derived.\n\nTheorem 2 Let $(X, Y),\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right)$ be independent and identically distributed random variables with values in $[0,1]^{d_{1} \\times d_{2}} \\times\\{-1,1\\}$. Let $p \\geq 1$ and $C>0$ be arbitrary. Assume that the a posteriori probability $\\eta(x)=\\mathbf{P}\\{Y=1 \\mid X=x\\}$ satisfies a $(p, C)-$ smooth hierarchical max-pooling model of finite level $l$ and set $\\beta_{n}=c_{3} \\cdot \\log n$,\n\n$$\n\\begin{gathered}\nL_{n}^{(1)}=\\frac{4^{l}-1}{3} \\cdot\\left\\lceil c_{5} \\cdot n^{2 /(2 p+4)}\\right\\rceil+l \\quad \\text { and } \\quad L_{n}^{(2)}=\\left\\lceil c_{6} \\cdot n^{1 / 4}\\right\\rceil \\\\\nM_{s}=2^{\\pi(s)} \\quad\\left(s=1, \\ldots, L_{n}^{(1)}\\right)\n\\end{gathered}\n$$\n\nwhere the function $\\pi:\\left\\{1, \\ldots, L_{n}^{(1)}\\right\\} \\rightarrow\\{1, \\ldots, l\\}$ is defined by\n\n$$\n\\pi(s)=\\sum_{i=1}^{l} \\mathbb{1}_{\\left\\{s \\geq i+\\sum_{i=l-i+1}^{l-1} 4^{r} \\cdot\\left\\lceil c_{5} \\cdot n^{2 /(2 p+4)}\\right\\rceil\\right\\}}\n$$\n\nchoose $\\mathbf{k}=\\left(c_{7}, \\ldots, c_{7}\\right) \\in \\mathbb{N}^{L_{n}^{(1)}}$ and set\n\n$$\nB_{n}=e^{\\sqrt{n}}\n$$\n\nassume that $K_{n} \\in \\mathbb{N}$ satisfies\n\n$$\n\\frac{K_{n}}{e^{2 \\cdot n^{1.5}}} \\rightarrow \\infty \\quad(n \\rightarrow \\infty)\n$$\n\nand set\n\n$$\n\\alpha_{n}=\\frac{1}{n^{2} \\cdot e^{2 \\cdot n}} \\quad \\text { and } \\quad t_{n}=\\left\\lceil n^{2} \\cdot K_{n}\\right\\rceil\n$$\n\nDefine the classifier $\\hat{C}_{n}$ as in Section 3.2. Assume that the constants $c_{3}, c_{5}, c_{6}, c_{7}$ are sufficiently large.\na) There exists a constant $c_{8}>0$ such that we have for $n$ sufficiently large\n\n$$\n\\mathbf{P}\\left\\{Y \\neq \\hat{C}_{n}(X)\\right\\}-\\mathbf{P}\\left\\{Y \\neq f^{*}(X)\\right\\} \\leq c_{8} \\cdot(\\log n)^{2} \\cdot n^{-\\min \\left\\{\\frac{p}{4 p+8}, \\frac{1}{8}\\right\\}}\n$$\n\nb) If, in addition,\n\n$$\n\\mathbf{P}\\left\\{\\max \\left\\{\\frac{\\eta(X)}{1-\\eta(X)}, \\frac{1-\\eta(X)}{\\eta(X)}\\right\\}>n^{\\frac{1}{4}}\\right\\} \\geq 1-\\frac{1}{n^{\\frac{1}{4}}} \\quad(n \\in \\mathbb{N})\n$$\n\nholds, then there exists a constant $c_{9}>0$ such that we have for $n$ sufficiently large\n\n$$\n\\mathbf{P}\\left\\{Y \\neq \\hat{C}_{n}(X)\\right\\}-\\mathbf{P}\\left\\{Y \\neq f^{*}(X)\\right\\} \\leq c_{9} \\cdot(\\log n)^{4} \\cdot n^{-\\min \\left\\{\\frac{p}{2 p+4}, \\frac{1}{4}\\right\\}}\n$$\n\nRemark 3. The rates of convergence above do not depend on the dimension $d_{1} \\cdot d_{2}$ of the image, hence in case that the a posteriori distribution satisfies a hierarchical composition model, our estimate is able to circumvent the curse of dimensionality.\nRemark 4 In [37] the authors have shown that CNN image classifiers from the function class $\\mathcal{F}_{n}$ defined in (14) that minimize empirical risk are able to achieve dimension reduction and achieved the same convergence rates as in Theorem 2 above. However, in practice, it is not possible to compute the empirical risk minimizer. Instead, we apply stochastic gradient descent to over-parametrized linear combinations of functions from (14) and show that this estimate achieves the same rate of convergence as the empirical risk minimizer of Kohler and Langer (2025).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "# 4. Proofs\n### 4.1. Proof of Theorem 1\n\nIn the proof of Theorem 1 we will need the following auxiliary result.\nLemma 1 Let $l_{1}, l_{2}, t_{n} \\in \\mathbb{N}$, let $D_{n} \\geq 0$, let $A \\subset \\mathbb{R}^{l_{1}}$ be closed and convex, let $B \\subseteq \\mathbb{R}^{l_{2}}$ and let $F_{t}, F: \\mathbb{R}^{l_{1}} \\times \\mathbb{R}^{l_{2}} \\rightarrow \\mathbb{R}_{+}\\left(t=0, \\ldots, t_{n}-1\\right)$ be functions such that for all $t \\in$ $\\left\\{0, \\ldots, t_{n}-1\\right\\}$\n\n$$\n\\begin{gathered}\nu \\mapsto F(u, v) \\quad \\text { is differentiable and convex for all } v \\in \\mathbb{R}^{l_{2}} \\\\\nu \\mapsto F_{t}(u, v) \\quad \\text { is differentiable for all } v \\in \\mathbb{R}^{l_{2}}\n\\end{gathered}\n$$\n\nand\n\n$$\n\\left\\|\\left(\\nabla_{u} F_{t}\\right)(u, v)\\right\\| \\leq D_{n}\n$$\n\nfor all $(u, v) \\in A \\times B$. Choose $\\left(u_{0}, v_{0}\\right) \\in A \\times B$, let $v_{1}, \\ldots, v_{t_{n}} \\in B$ and set\n\n$$\nu_{t+1}=\\operatorname{Proj}_{A}\\left(u_{t}-\\lambda \\cdot\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right) \\quad\\left(t=0, \\ldots, t_{n}-1\\right)\n$$\n\nwhere\n\n$$\n\\lambda=\\frac{1}{t_{n}}\n$$\n\nLet $u^{*} \\in A$. Then it holds:\n\n$$\n\\begin{aligned}\n\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} F\\left(u_{t}, v_{t}\\right) \\leq & F\\left(u^{*}, v_{0}\\right)+\\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1}\\left|F\\left(u^{*}, v_{t}\\right)-F\\left(u^{*}, v_{0}\\right)\\right|+\\frac{\\left\\|u^{*}-u_{0}\\right\\|^{2}}{2}+\\frac{D_{n}^{2}}{2 \\cdot t_{n}} \\\\\n& +\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1}<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}>\n\\end{aligned}\n$$\n\nProof. By convexity of $u \\mapsto F\\left(u, v_{t}\\right)$ and because of $u^{*} \\in A$ we have\n\n$$\n\\begin{aligned}\n& F\\left(u_{t}, v_{t}\\right)-F\\left(u^{*}, v_{t}\\right) \\\\\n& \\leq<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}>\n& =\\frac{1}{2 \\cdot \\lambda} \\cdot 2 \\cdot<\\lambda \\cdot\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}>+<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}> \\\\\n& =\\frac{1}{2 \\cdot \\lambda} \\cdot\\left(-\\left\\|u_{t}-u^{*}-\\lambda \\cdot\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right\\|^{2}+\\left\\|u_{t}-u^{*}\\right\\|^{2}+\\left\\|\\lambda \\cdot\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right\\|^{2}\\right) \\\\\n& \\quad+<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}> \\\\\n& \\leq \\frac{1}{2 \\cdot \\lambda} \\cdot\\left(-\\left\\|\\operatorname{Proj}_{A}\\left(u_{t}-\\lambda \\cdot\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right)-u^{*}\\right\\|^{2}+\\left\\|u_{t}-u^{*}\\right\\|^{2}+\\lambda^{2} \\cdot\\left\\|\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right\\|^{2}\\right) \\\\\n& \\quad+<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}> \\\\\n& =\\frac{1}{2 \\cdot \\lambda} \\cdot\\left(\\left\\|u_{t}-u^{*}\\right\\|^{2}-\\left\\|u_{t+1}-u^{*}\\right\\|^{2}+\\lambda^{2} \\cdot\\left\\|\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right\\|^{2}\\right) \\\\\n& \\quad+<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}>\n\\end{aligned}\n$$\n\nThis implies\n\n$$\n\\begin{aligned}\n& \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} F\\left(u_{t}, v_{t}\\right)-\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} F\\left(u^{*}, v_{t}\\right) \\\\\n& =\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1}\\left(F\\left(u_{t}, v_{t}\\right)-F\\left(u^{*}, v_{t}\\right)\\right) \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\frac{1}{2 \\cdot \\lambda} \\cdot\\left(\\left\\|u_{t}-u^{*}\\right\\|^{2}-\\left\\|u_{t+1}-u^{*}\\right\\|^{2}\\right)+\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\frac{\\lambda}{2} \\cdot\\left\\|\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right\\|^{2} \\\\\n& \\quad+\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1}<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}>\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n= & \\frac{1}{2} \\cdot \\sum_{t=0}^{t_{n}-1}\\left(\\left\\|u_{t}-u^{*}\\right\\|^{2}-\\left\\|u_{t+1}-u^{*}\\right\\|^{2}\\right)+\\frac{1}{2 \\cdot t_{n}^{2}} \\sum_{t=0}^{t_{n}-1}\\left\\|\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right\\|^{2} \\\\\n& +\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1}<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}>\\ \\\\\n\\leq & \\frac{\\left\\|u_{0}-u^{*}\\right\\|^{2}}{2}+\\frac{1}{2 \\cdot t_{n}^{2}} \\sum_{t=0}^{t_{n}-1}\\left\\|\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right\\|^{2} \\\\\n& +\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1}<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}>\n\\end{aligned}\n$$\n\nUsing the above result and (19) we get\n\n$$\n\\begin{aligned}\n& \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} F\\left(u_{t}, v_{t}\\right) \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} F\\left(u^{*}, v_{t}\\right)+\\frac{\\left\\|u^{*}-u_{0}\\right\\|^{2}}{2}+\\frac{1}{2 \\cdot t_{n}^{2}} \\sum_{t=0}^{t_{n}-1}\\left\\|\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right)\\right\\|^{2} \\\\\n& \\quad+\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1}<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}>\\quad \\\\\n& \\leq F\\left(u^{*}, v_{0}\\right)+\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1}\\left|F\\left(u^{*}, v_{t}\\right)-F\\left(u^{*}, v_{0}\\right)\\right|+\\frac{\\left\\|u^{*}-u_{0}\\right\\|^{2}}{2}+\\frac{D_{n}^{2}}{2 \\cdot t_{n}} \\\\\n& \\quad+\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1}<\\left(\\nabla_{u} F\\right)\\left(u_{t}, v_{t}\\right)-\\left(\\nabla_{u} F_{t}\\right)\\left(u_{t}, v_{t}\\right), u_{t}-u^{*}>\n\\end{aligned}\n$$\n\nProof of Theorem 1. Let $E_{n}$ be the event that there exist pairwise distinct $j_{1}, \\ldots, j_{N_{n}} \\in$ $\\left\\{1, \\ldots, K_{n}\\right\\}$ such that\n\n$$\n\\vartheta_{j_{i}}^{(0)} \\in \\boldsymbol{\\Theta}^{*}\n$$\n\nholds for all $i=1, \\ldots, N_{n}$. If $E_{n}$ holds set\n\n$$\nw_{j_{i}}^{*}=\\frac{1}{N_{n}} \\quad\\left(i=1, \\ldots, N_{n}\\right) \\quad \\text { and } \\quad w_{k}^{*}=0 \\quad\\left(k \\in\\left\\{1, \\ldots, K_{n}\\right\\} \\backslash\\left\\{j_{1}, \\ldots, j_{N_{n}}\\right\\}\\right)\n$$\n\nand $\\mathbf{w}^{*}=\\left(w_{k}^{*}\\right)_{k=1, \\ldots, K_{n}}$, otherwise set $\\mathbf{w}^{*}=0$.\nWe will use the following error decomposition:\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right)\\right\\}-\\min _{f:[0,1]^{d_{1} \\times d_{2} \\rightarrow \\mathbb{R}}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\\\\n& =\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right) \\cdot 1_{E_{n}^{c}}\\right\\} \\\\\n& \\quad+\\mathbf{E}\\left\\{\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}, \\vartheta^{(n)}\\right)}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\}\n\\end{aligned}\n$$\n\n$$\n\\begin{gathered}\n-\\mathbf{E}\\left\\{\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{\\left(t_{n}\\right)}\\right.}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\} \\\\\n+\\mathbf{E}\\left\\{\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{\\left(t_{n}\\right)}\\right.}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\} \\\\\n-\\mathbf{E}\\left\\{\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right.}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\} \\\\\n+\\mathbf{E}\\left\\{\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right.}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\}-\\min _{f:[0,1]^{d_{1} \\times d_{2} \\rightarrow \\mathbb{R}}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\\\\n=: T_{1, n}+T_{2, n}+T_{3, n}+T_{4, n}\n\\end{gathered}\n$$\n\nIn the first step of the proof we show\n\n$$\n\\mathbf{P}\\left\\{E_{n}^{c}\\right\\} \\leq \\frac{1}{n}\n$$\n\nTo do this we consider a sequential choice of the initial weights $\\vartheta_{1}^{(0)}, \\ldots, \\vartheta_{K_{n}}^{(0)}$. By definition of $\\kappa_{n}$ we know that the probability that none of $\\vartheta_{1}^{(0)}, \\ldots, \\vartheta_{I_{n}}^{(0)}$ is contained in $\\boldsymbol{\\Theta}^{*}$ is given by\n\n$$\n\\left(1-\\kappa_{n}\\right)^{I_{n}}\n$$\n\nThis implies that the probability that there exists $l \\in\\left\\{1, \\ldots, N_{n}\\right\\}$ such that none of $\\vartheta_{(l-1) \\cdot I_{n}+1}^{(0)}, \\ldots, \\vartheta_{l \\cdot I_{n}}^{(0)}$ is contained in $\\boldsymbol{\\Theta}^{*}$ is upper bounded by\n\n$$\nN_{n} \\cdot\\left(1-\\kappa_{n}\\right)^{I_{n}}\n$$\n\n(10) implies\n\n$$\n\\mathbf{P}\\left\\{E_{n}^{c}\\right\\} \\leq N_{n} \\cdot\\left(1-\\kappa_{n}\\right)^{I_{n}} \\leq \\frac{1}{n}\n$$\n\nIn the second step of the proof we show\n\n$$\nT_{1, n} \\leq c_{10} \\cdot \\frac{(\\log n)}{n}\n$$\n\nTo do this, we observe that for $|z| \\leq \\beta_{n}$ we have\n$\\varphi(z)=\\log (1+\\exp (-z)) \\leq(\\log 4) \\cdot I_{\\{z>-1\\}}+\\log (2 \\cdot \\exp (-z)) \\cdot I_{\\{z \\leq-1\\}} \\leq 3+|z| \\leq c_{11} \\cdot \\log n$,\nfrom which we can conclude by the first step of the proof\n\n$$\nT_{1, n} \\leq c_{11} \\cdot(\\log n) \\cdot \\mathbf{P}\\left\\{E_{n}^{c}\\right\\} \\leq c_{11} \\cdot \\frac{\\log n}{n}\n$$\n\nIn the third step of the proof we show\n\n$$\nT_{2, n} \\leq 0\n$$\n\nThis follows from the convexity of the logistic loss, which implies\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}, \\vartheta^{(t_{n})}\\right)}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t_{n})}\\right)}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\\\\n& \\leq \\mathbf{E}\\left\\{\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t_{n})}\\right)}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\\\\n& =\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t_{n})}\\right)}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\}\n\\end{aligned}\n$$\n\nIn the fourth step of the proof we show\n\n$$\nT_{3, n} \\leq 2 \\cdot \\frac{C_{n}}{\\sqrt{N}_{n}}\n$$\n\nDue to the fact that the logistic loss is Lipschitz continuous with Lipschitz constant 1 and by assumptions (4) and (8) we have\n\n$$\n\\begin{aligned}\n& T_{3, n} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\left|\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t_{n})}\\right)}(X)\\right)-\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)\\right)\\right|\\right\\} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\left|f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t_{n})}\\right)}(X)-f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)\\right|\\right\\} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\sum_{k=1}^{K_{n}} w_{k}^{(t)} \\cdot\\left|T_{\\beta_{n}} f_{\\vartheta_{k}^{(t_{n})}}(X)-T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}(X)\\right|\\right\\} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\sum_{k=1}^{K_{n}} w_{k}^{(t)} \\cdot\\left|f_{\\vartheta_{k}^{(t_{n})}}(X)-f_{\\vartheta_{k}^{(t)}}(X)\\right|\\right\\} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\sqrt{\\sum_{k=1}^{K_{n}}\\left(w_{k}^{(t)}\\right)^{2}} \\cdot \\sqrt{\\sum_{k=1}^{K_{n}}\\left|f_{\\vartheta_{k}^{(t_{n})}}(X)-f_{\\vartheta_{k}^{(t)}}(X)\\right|^{2}}\\right\\} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\sqrt{\\alpha_{n}} \\cdot \\sqrt{\\sum_{k=1}^{K_{n}} C_{n}^{2}} \\cdot\\left\\|\\vartheta_{k}^{\\left(t_{n}\\right)}-\\vartheta_{k}^{(t)}\\right\\|_{\\infty}^{2}}\\right\\} \\\\\n& =\\frac{C_{n}}{\\sqrt{N_{n}}} \\cdot \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\sqrt{\\left\\|\\vartheta^{\\left(t_{n}\\right)}-\\vartheta^{(t)}\\right\\|^{2}}\\right\\} \\\\\n& \\leq 2 \\cdot \\frac{C_{n}}{\\sqrt{N_{n}}}\n\\end{aligned}\n$$\n\nIn the fifth step of the proof we apply Lemma 1 to $T_{4, n}$. Set\n\n$$\nF((\\mathbf{w}, \\vartheta))=\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{(\\mathbf{w}, \\vartheta)}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\text { and } F_{t}((\\mathbf{w}, \\vartheta))=\\varphi\\left(Y_{j_{t}} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j_{t}}\\right)\\right)\n$$\n\nThen Lemma 1 implies\n\n$$\n\\begin{aligned}\n& T_{4, n} \\leq \\mathbf{E}\\left\\{\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(0)}\\right)}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\}-\\min _{f:[0,1]^{d_{1}+d_{2} \\rightarrow \\mathbb{R}}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\\\\n& +\\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\left|\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(t)}\\right)}(X)\\right) \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right.\\right. \\\\\n& \\left.\\left.-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(0)}\\right)}(X)\\right) \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right] \\cdot 1_{E_{n}}\\right\\}+\\frac{1}{2} \\cdot \\frac{1}{N_{n}}+\\frac{D_{n}^{2}}{2 \\cdot t_{n}} \\\\\n& +\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\left\\langle\\left(\\nabla_{\\mathbf{w}} F\\right)\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)-\\left(\\nabla_{\\mathbf{w}} F_{t}\\right)\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right), \\mathbf{w}^{(t)}-\\mathbf{w}^{*}\\right\\rangle \\cdot 1_{E_{n}}\\right\\} .\n\\end{aligned}\n$$\n\nIn the sixth step of the proof we show\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(0)}\\right)}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\}-\\min _{f:[0,1]^{d_{1}+d_{2} \\rightarrow \\mathbb{R}}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\\\\n& \\leq \\sup _{\\vartheta \\in \\boldsymbol{\\Theta}^{*}} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot T_{\\beta_{n}} f_{\\vartheta}(X)\\right)\\right\\}-\\min _{f:[0,1]^{d_{1}+d_{2} \\rightarrow \\mathbb{R}}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\}\\right\\}\n\\end{aligned}\n$$\n\nThis follows from the convexity of the logistic loss, which implies\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(0)}\\right)}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\mathbf{E}\\left\\{\\varphi\\left(\\frac{1}{N_{n}} \\sum_{k=1}^{N_{n}} Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{d_{k}}^{(0)}}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq \\frac{1}{N_{n}} \\sum_{k=1}^{N_{n}} \\mathbf{E}\\left\\{\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{d_{k}}^{(0)}}(X)\\right) \\mid \\vartheta^{(0)}, \\mathcal{D}_{n}\\right\\} \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq \\sup _{\\vartheta \\in \\boldsymbol{\\Theta}^{*}} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot T_{\\beta_{n}} f_{\\vartheta}(X)\\right)\\right\\}\n\\end{aligned}\n$$\n\nIn the seventh step of the proof we show\n\n$$\n\\begin{aligned}\n& \\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\left|\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(t)}\\right)}(X)\\right) \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(0)}\\right)}(X)\\right) \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right| \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq \\frac{C_{n}}{\\sqrt{N_{n}}}\n\\end{aligned}\n$$\n\nThe Lipschitz continuity of the logistic loss and assumption (8) imply\n\n$$\n\\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\left|\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(t)}\\right)}(X)\\right) \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(0)}\\right)}(X)\\right) \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right| \\cdot 1_{E_{n}}\\right\\}\n$$\n\n$$\n\\begin{aligned}\n& \\leq \\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\left|\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(t)}\\right)}(X)\\right)-\\varphi\\left(Y \\cdot f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(0)}\\right)}(X)\\right)\\right|\\right\\} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\left|f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(t)}\\right)}(X)-f_{\\left(\\mathbf{w}^{*}, \\vartheta^{(0)}\\right)}(X)\\right|\\right\\} \\\\\n& =\\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\left|\\sum_{k=1}^{K_{n}} w_{k}^{*} \\cdot\\left(T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}(X)-T_{\\beta_{n}} f_{\\vartheta_{k}^{(0)}}(X)\\right)\\right|\\right\\} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\sqrt{\\sum_{k=1}^{K_{n}}\\left|w_{k}^{*}\\right|^{2}} \\cdot \\sqrt{\\sum_{k=1}^{K_{n}}\\left(T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}(X)-T_{\\beta_{n}} f_{\\vartheta_{k}^{(0)}}(X)\\right)^{2}}\\right\\} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\sqrt{\\sum_{k=1}^{K_{n}}\\left|w_{k}^{*}\\right|^{2}} \\cdot \\sqrt{\\sum_{k=1}^{K_{n}}\\left(f_{\\vartheta_{k}^{(t)}}(X)-f_{\\vartheta_{k}^{(0)}}(X)\\right)^{2}}\\right\\} \\\\\n& \\leq \\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\frac{1}{\\sqrt{N_{n}}} \\sqrt{\\sum_{k=1}^{K_{n}} C_{n}^{2}} \\cdot\\left\\|\\vartheta_{k}^{(t)}-\\vartheta_{k}^{(0)}\\right\\|_{\\infty}^{2}\\right\\} \\\\\n& =\\frac{1}{t_{n}} \\sum_{t=1}^{t_{n}-1} \\mathbf{E}\\left\\{\\frac{C_{n}}{\\sqrt{N_{n}}} \\cdot\\left\\|\\vartheta^{(t)}-\\vartheta^{(0)}\\right\\|\\right\\} \\leq \\frac{C_{n}}{\\sqrt{N_{n}}}\n\\end{aligned}\n$$\n\nLet $\\mathcal{W}$ be the set of all weight vectors $\\mathbf{w}=\\left(\\left(w_{k}\\right)_{k=1, \\ldots, K_{n}},\\left(\\vartheta_{k}\\right)_{k=1, \\ldots, K_{n}}\\right)$ which satisfy $\\vartheta=\\left(\\vartheta_{k}\\right)_{k=1, \\ldots, K_{n}} \\in \\boldsymbol{\\Theta}^{K_{n}}$ and (4). Let $\\left(X_{1}^{\\prime}, Y_{1}^{\\prime}\\right), \\ldots,\\left(X_{n}^{\\prime}, Y_{n}^{\\prime}\\right), \\epsilon_{1}, \\ldots, \\epsilon_{n}$ be random variables independent of $\\vartheta^{(0)}$ with $\\mathbf{P}\\left\\{\\epsilon_{j}=1\\right\\}=1 / 2=\\mathbf{P}\\left\\{\\epsilon_{j}=-1\\right\\}(j=1, \\ldots, n)$ such that $\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right), \\epsilon_{1}, \\ldots, \\epsilon_{n},\\left(X_{1}^{\\prime}, Y_{1}^{\\prime}\\right), \\ldots,\\left(X_{n}^{\\prime}, Y_{n}^{\\prime}\\right)$ are independent and $\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right),\\left(X_{1}^{\\prime}, Y_{1}^{\\prime}\\right), \\ldots,\\left(X_{n}^{\\prime}, Y_{n}^{\\prime}\\right)$ are identically distributed.\n\nIn the eighth step of the proof we show\n\n$$\n\\begin{aligned}\n& \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\left\\langle\\left(\\nabla_{\\mathbf{w}} F\\right)\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)-\\left(\\nabla_{\\mathbf{w}} F_{t}\\right)\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right), \\mathbf{w}^{(t)}-\\mathbf{w}^{*}\\right\\rangle \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq 4 \\cdot \\mathbf{E}\\left\\{\\sup _{\\substack{\\left.\\left.(\\mathbf{w}, \\vartheta) \\in \\mathcal{W} ;\\right.\\right.}}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& +\\frac{n \\cdot\\left(6 \\cdot K_{n} \\cdot \\beta_{n}^{2}+4 \\cdot\\left(\\beta_{n}+1\\right) \\cdot C_{n} \\cdot \\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, y \\in\\{-1,1\\}, x \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\vartheta} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|_{\\infty}\\right)}{t_{n}}\n\\end{aligned}\n$$\n\nWe have\n\n$$\n\\varphi^{\\prime}(z)=\\frac{1}{1+\\exp (-z)} \\cdot(-\\exp (-z))=\\frac{-1}{1+\\exp (z)}\n$$\n\nwhich implies\n\n$$\n\\frac{\\partial}{\\partial w_{j}} \\varphi\\left(Y \\cdot \\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)=\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{j}}(X)}{1+\\exp \\left(Y \\cdot \\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\frac{\\partial}{\\partial w_{j}} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot \\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)\\right\\} \\\\\n& =\\lim _{h \\rightarrow 0} \\mathbf{E}\\left\\{\\frac{\\varphi\\left(Y \\cdot \\sum_{k=1}^{K_{n}}\\left(w_{k}+h \\cdot I_{\\{k=j\\}}\\right) \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)-\\varphi\\left(Y \\cdot \\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)}{h}\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\lim _{h \\rightarrow 0} \\frac{\\varphi\\left(Y \\cdot \\sum_{k=1}^{K_{n}}\\left(w_{k}+h \\cdot I_{\\{k=j\\}}\\right) \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)-\\varphi\\left(Y \\cdot \\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)}{h}\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{j}}(X)}{1+\\exp \\left(Y \\cdot \\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)}\\right\\}\n\\end{aligned}\n$$\n\nwhere we have used\n\n$$\n\\begin{aligned}\n& \\left|\\frac{\\varphi\\left(Y \\cdot \\sum_{k=1}^{K_{n}}\\left(w_{k}+h \\cdot I_{\\{k=j\\}}\\right) \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)-\\varphi\\left(Y \\cdot \\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)\\right)}{h}\\right| \\\\\n& =\\beta_{n} \\cdot\\left|\\varphi^{\\prime}(\\xi)\\right| \\leq \\beta_{n}\n\\end{aligned}\n$$\n\nand the dominated convergence theorem in order to interchange limites and expectations above.\n\nConsequently,\n\n$$\n\\begin{aligned}\n& \\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\mathbf{E}\\left\\{\\left\\langle\\left(\\nabla_{\\mathbf{w}} F\\right)\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)-\\left(\\nabla_{\\mathbf{w}} F_{t}\\right)\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right), \\mathbf{w}^{(t)}-\\mathbf{w}^{*}>\\cdot 1_{E_{n}}\\right\\}\\right. \\\\\n& =\\mathbf{E}\\left\\{\\frac{1}{t_{n}} \\sum_{t=0}^{t_{n}-1} \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)\\right)} \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right.\\right. \\\\\n& \\left.\\quad-\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}\\left(X_{j_{t}}\\right)}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right) \\cdot\\left(w_{k}^{(t)}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& =\\frac{1}{t_{n} / n} \\cdot \\sum_{s=1}^{t_{n} / n} \\mathbf{E}\\left\\{\\frac{1}{n} \\sum_{t=(s-1) \\cdot n}^{s \\cdot n-1} \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)\\right)} \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right. \\\\\n& \\left.\\quad-\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}\\left(X_{j_{t}}\\right)}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right) \\cdot\\left(w_{k}^{(t)}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\}\n\\end{aligned}\n$$\n\nDuring $n$ gradient descent steps the parameter $(\\mathbf{w}, \\vartheta)$ changes in supremum norm at most by\n\n$$\nn \\cdot \\lambda_{n} \\cdot \\max \\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, y \\in\\{-1,1\\}, x \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\mathbf{w}} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|_{\\infty},\\right.\n$$\n\n$$\n\\begin{aligned}\n& \\left.\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, y \\in\\{-1,1\\}, x \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\vartheta} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|_{\\infty}\\right\\} \\\\\n& \\leq n \\cdot \\lambda_{n} \\cdot\\left(\\beta_{n}+\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, y \\in\\{-1,1\\}, x \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\vartheta} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|_{\\infty}\\right) \\\\\n& =\\frac{n \\cdot\\left(\\beta_{n}+\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, y \\in\\{-1,1\\}, x \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\vartheta} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|_{\\infty}\\right)}{t_{n}}\n\\end{aligned}\n$$\n\nUsing\n\n$$\n\\begin{aligned}\n& \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left.\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)\\right)} \\right\\rvert\\, \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right. \\\\\n& -\\mathbf{E}\\left\\{\\left.\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s n-1)}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}(X)\\right)} \\right\\rvert\\, \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\} \\\\\n& \\left.\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}\\left(X_{j_{t}}\\right)}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right. \\\\\n& \\left.+\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s n-1)}}\\left(X_{j_{t}}\\right)}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right) \\cdot\\left(w_{k}^{(t)}-w_{k}^{*}\\right) \\\\\n& \\leq \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left.\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}(X)+Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s n-1)}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)\\right.}\\right\\rvert\\,\\right. \\\\\n& +\\left|Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s n-1)}}(X)\\right| \\cdot\\left|\\frac{1}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)\\right)}\\right. \\\\\n& \\left.-\\frac{1}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}(X)\\right)}\\right\\rvert\\, \\mathcal{D}_{n}, \\vartheta^{(0)} \\right\\} \\\\\n& +\\left|\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s n-1)}}\\left(X_{j_{t}}\\right)+Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}\\left(X_{j_{t}}\\right)}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right| \\\\\n& +\\left|Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}\\left(X_{j_{t}}\\right)\\right| \\cdot\\left|\\frac{1}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right. \\\\\n& \\left.\\left.-\\frac{1}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right\\rvert\\,\\right) \\cdot\\left(w_{k}^{(t)}-w_{k}^{*}\\right)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n\\leq \\sum_{k=1}^{K_{n}}\\{ & \\mathbf{E}\\left\\{\\left|T_{\\beta_{n}} f_{\\vartheta_{k}^{(s n-1)}}(X)-T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}(X)\\right|+\\beta_{n} \\cdot\\left|f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)-f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}(X)\\right| \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\} \\\\\n& +\\left|T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}}\\left(X_{j_{t}}\\right)-T_{\\beta_{n}} f_{\\vartheta_{k}^{(s n-1)}}\\left(X_{j_{t}}\\right)\\right| \\\\\n& \\left.\\quad+\\beta_{n} \\cdot\\left|f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)-f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}\\left(X_{j_{t}}\\right)\\right|\\right) \\cdot\\left|w_{k}^{(t)}-w_{k}^{*}\\right| \\\\\n\\leq & \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left|f_{\\vartheta_{k}^{(s n-1)}}(X)-f_{\\vartheta_{k}^{(t)}}(X)\\right|+\\beta_{n} \\cdot\\left|f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)-f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}(X)\\right| \\mid \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\} \\\\\n& +\\left|f_{\\vartheta_{k}^{(t)}}\\left(X_{j_{t}}\\right)-f_{\\vartheta_{k}^{(s n-1)}}\\left(X_{j_{t}}\\right)\\right| \\\\\n& \\left.\\quad+\\beta_{n} \\cdot\\left|f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)-f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}\\left(X_{j_{t}}\\right)\\right|\\right) \\cdot\\left|w_{k}^{(t)}-w_{k}^{*}\\right| \\\\\n\\leq & 2 \\cdot \\sum_{k=1}^{K_{n}}\\left(C_{n} \\cdot\\left\\|\\vartheta_{k}^{(t)}-\\vartheta_{k}^{(s n-1)}\\right\\|_{\\infty}+\\beta_{n} \\cdot C_{n} \\cdot \\max _{j}\\left\\|\\vartheta_{j}^{(t)}-\\vartheta_{j}^{(s n-1)}\\right\\|_{\\infty}+K_{n} \\cdot \\frac{\\beta_{n}^{2} \\cdot n}{t_{n}}\\right) \\cdot\\left|w_{k}^{(t)}-w_{k}^{*}\\right| \\\\\n\\leq & 4 \\cdot C_{n} \\cdot\\left\\|\\vartheta_{k}^{(t)}-\\vartheta_{k}^{(s n-1)}\\right\\|_{\\infty}+4 \\cdot \\beta_{n} \\cdot C_{n} \\cdot \\max _{j}\\left\\|\\vartheta_{j}^{(t)}-\\vartheta_{j}^{(s n-1)}\\right\\|_{\\infty}+4 \\cdot K_{n} \\cdot \\frac{\\beta_{n}^{2} \\cdot n}{t_{n}} \\\\\n\\leq & \\frac{4 \\cdot\\left(\\beta_{n}+1\\right) \\cdot C_{n} \\cdot n \\cdot \\sup _{\\left(\\mathbf{w}, \\vartheta\\right) \\in \\mathcal{W}, y \\in\\{-1,1\\}, z \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\vartheta} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|_{\\infty}+4 \\cdot K_{n} \\cdot \\beta_{n}^{2} \\cdot n}{t_{n}}\n\\end{aligned}\n$$\n\nwhere the fourth inequality follows from\n\n$$\n\\begin{aligned}\n& \\left|f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)-f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}(X)\\right| \\\\\n& =\\left|\\sum_{k=1}^{K_{n}} w_{k}^{(t)} T_{\\beta_{n}} f_{\\vartheta^{(t)}}(X)-w_{k}^{(s n-1)} T_{\\beta_{n}} f_{\\mathbf{w}^{(s n-1)}}(X)\\right| \\\\\n& \\leq \\sum_{k=1}^{K_{n}}\\left|w_{k}^{(t)}\\right| \\cdot\\left|T_{\\beta_{n}} f_{\\vartheta^{(t)}}(X)-T_{\\beta_{n}} f_{\\mathbf{w}^{(s n-1)}}(X)\\right|+\\left|w_{k}^{(t)}-w_{k}^{(s n-1)}\\right| \\cdot\\left|T_{\\beta_{n}} f_{\\mathbf{w}^{(s n-1)}}(X)\\right| \\\\\n& \\leq C_{n} \\cdot \\max _{j}\\left\\|\\vartheta_{j}^{(t)}-\\vartheta_{j}^{(s n-1)}\\right\\|_{\\infty}+K_{n} \\cdot \\frac{n \\cdot \\beta_{n}}{t_{n}} \\cdot \\beta_{n}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left.\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s n-1)}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}(X)\\right)} \\right\\rvert\\, \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right. \\\\\n& \\left.\\quad-\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s n-1)}}\\left(X_{j_{t}}\\right)}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(s n-1)}, \\vartheta^{(s n-1)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right) \\cdot\\left(w_{k}^{(t)}-w_{k}^{(s n-1)}\\right) \\\\\n& \\leq 2 \\cdot \\beta_{n} \\cdot \\sum_{k=1}^{K_{n}}\\left|w_{k}^{(t)}-w_{k}^{(s n-1)}\\right| \\leq 2 \\cdot K_{n} \\cdot n \\cdot \\frac{\\beta_{n}^{2}}{t_{n}}\n\\end{aligned}\n$$\n\nwe can conclude\n\n$$\n\\begin{aligned}\n& \\frac{1}{t_{n} / n} \\cdot \\sum_{s=1}^{t_{n} / n} \\mathbf{E}\\left\\{\\frac{1}{n} \\sum_{t=(s-1) \\cdot n}^{s \\cdot n-1} \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left.\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}(X)\\right)} \\right\\rvert\\, \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right. \\\\\n& \\left.-\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(t)}\\left(X_{j_{t}}\\right)}}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(t)}, \\vartheta^{(t)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right) \\cdot\\left(w_{k}^{(t)}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq \\frac{1}{t_{n} / n} \\cdot \\sum_{s=1}^{t_{n} / n} \\mathbf{E}\\left\\{\\frac{1}{n} \\sum_{t=(s-1) \\cdot n}^{s \\cdot n-1} \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left.\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s \\cdot n-1)}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(s \\cdot n-1)}, \\vartheta^{(s \\cdot n-1)}\\right)}(X)\\right)} \\right\\rvert\\, \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right. \\\\\n& \\left.-\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s \\cdot n-1)}}\\left(X_{j_{t}}\\right)}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(s \\cdot n-1)}, \\vartheta^{(s \\cdot n-1)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right) \\cdot\\left(w_{k}^{(s \\cdot n-1)}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& +\\frac{n \\cdot\\left(6 \\cdot K_{n} \\cdot \\beta_{n}^{2}+4 \\cdot\\left(\\beta_{n}+1\\right) \\cdot C_{n} \\cdot \\sup _{\\left(\\mathbf{w}, \\vartheta\\right) \\in \\mathcal{W}, g \\in\\{-1,1\\}, x \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\vartheta} \\varphi\\left(y \\cdot f_{\\left(\\mathbf{w}, \\vartheta\\right)}(x)\\right)\\right\\|_{\\infty}\\right)}{t_{n}} .\n\\end{aligned}\n$$\n\nWe continue by deriving an upper bound on the first term of the sum of the right-hand side above. We have\n\n$$\n\\begin{aligned}\n& \\frac{1}{t_{n} / n} \\cdot \\sum_{s=1}^{t_{n} / n} \\mathbf{E}\\left\\{\\frac{1}{n} \\sum_{t=(s-1) \\cdot n}^{s \\cdot n-1} \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left.\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s \\cdot n-1)}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}^{(s \\cdot n-1)}, \\vartheta^{(s \\cdot n-1)}\\right)}(X)\\right)} \\right\\rvert\\, \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right. \\\\\n& \\left.-\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}^{(s \\cdot n-1)}}\\left(X_{j_{t}}\\right)}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}^{(s \\cdot n-1)}, \\vartheta^{(s \\cdot n-1)}\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right) \\cdot\\left(w_{k}^{(s \\cdot n-1)}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq \\frac{1}{t_{n} / n} \\cdot \\sum_{s=1}^{t_{n} / n} \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{t=(s-1) \\cdot n}^{s \\cdot n-1} \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left.\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}, \\vartheta\\right)}(X)\\right)}\\right\\}\\right.\\right. \\\\\n& \\left.-\\frac{-Y_{j_{t}} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j_{t}}\\right)}{1+\\exp \\left(Y_{j_{t}} \\cdot f_{\\left(\\mathbf{w}, \\vartheta\\right)}\\left(X_{j_{t}}\\right)\\right)}\\right) \\cdot\\left(w_{k}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left.\\frac{-Y \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(X)}{1+\\exp \\left(Y \\cdot f_{\\left(\\mathbf{w}, \\vartheta\\right)}(X)\\right)}\\right\\}\\right.\\right. \\\\\n& \\left.-\\frac{-Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{\\left(\\mathbf{w}, \\vartheta\\right)}\\left(X_{j}\\right)\\right)}\\right) \\cdot\\left(w_{k}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{K_{n}}\\left(\\mathbf{E}\\left\\{\\left.\\frac{-Y_{j}^{\\prime} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}^{\\prime}\\right)}{1+\\exp \\left(Y_{j}^{\\prime} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}^{\\prime}\\right)\\right)} \\right\\rvert\\, \\mathcal{D}_{n}, \\vartheta^{(0)}\\right\\}\\right. \\\\\n& \\left.-\\frac{-Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{\\left(\\mathbf{w}, \\vartheta\\right)}\\left(X_{j}\\right)\\right)}\\right) \\cdot\\left(w_{k}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& \\leq \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{K_{n}}\\left(\\frac{-Y_{j}^{\\prime} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}^{\\prime}\\right)}{1+\\exp \\left(Y_{j}^{\\prime} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}^{\\prime}\\right)\\right)}\\right.\\right. \\\\\n& \\left.\\quad-\\frac{-Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}\\right)\\right)}\\right) \\cdot\\left(w_{k}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{K_{n}} \\epsilon_{j} \\cdot\\left(\\frac{-Y_{j}^{\\prime} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}^{\\prime}\\right)}{1+\\exp \\left(Y_{j}^{\\prime} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}^{\\prime}\\right)\\right)}\\right.\\right. \\\\\n& \\left.\\quad-\\frac{-Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}\\right)\\right)}\\right) \\cdot\\left(w_{k}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{K_{n}} \\epsilon_{j} \\cdot\\left(\\frac{-Y_{j}^{\\prime} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}^{\\prime}\\right)}{1+\\exp \\left(Y_{j}^{\\prime} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}^{\\prime}\\right)\\right)}\\right) \\cdot\\left(w_{k}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& +\\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{K_{n}} \\epsilon_{j} \\cdot\\left(\\frac{Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}\\right)\\right)}\\right) \\cdot\\left(w_{k}-w_{k}^{*}\\right) \\cdot 1_{E_{n}}\\right\\} \\\\\n& =2 \\cdot \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{K_{n}} \\epsilon_{j} \\cdot \\frac{Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}\\right)\\right)} \\cdot\\left(w_{k}-w_{k}^{*}\\right)_{+} \\cdot 1_{E_{n}}\\right\\} \\\\\n& +2 \\cdot \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{K_{n}}\\left(-\\epsilon_{j}\\right) \\cdot \\frac{Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}\\right)\\right)} \\cdot\\left(w_{k}^{*}-w_{k}\\right)_{+} \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq 2 \\cdot \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\sum_{k=1}^{K_{n}} \\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n} \\sum_{j=1}^{n} \\epsilon_{j} \\cdot \\frac{Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\overline{\\mathbf{w}}, \\vartheta)}\\left(X_{j}\\right)\\right)} \\cdot\\left(w_{k}-w_{k}^{*}\\right)_{+} \\cdot 1_{E_{n}}\\right\\} \\\\\n& +2 \\cdot \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\sum_{k=1}^{K_{n}} \\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n} \\sum_{j=1}^{n} \\epsilon_{j} \\cdot \\frac{Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\overline{\\mathbf{w}}, \\vartheta)}\\left(X_{j}\\right)\\right)} \\cdot\\left(w_{k}^{\\star}-w_{k}\\right)_{+} \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq 2 \\cdot \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n} \\sum_{j=1}^{n} \\epsilon_{j} \\cdot \\frac{Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}\\right)\\right)} \\cdot \\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\sum_{k=1}^{K_{n}}\\left(w_{k}-w_{k}^{*}\\right)_{+} \\cdot 1_{E_{n}}\\right\\} \\\\\n& +2 \\cdot \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n} \\sum_{j=1}^{n}\\left(-\\epsilon_{j}\\right) \\cdot \\frac{Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}\\right)\\right)} \\cdot \\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\sum_{k=1}^{K_{n}}\\left(w_{k}^{*}-w_{k}\\right)_{+} \\cdot 1_{E_{n}}\\right\\} \\\\\n& \\leq 4 \\cdot \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n} \\sum_{j=1}^{n} \\epsilon_{j} \\cdot \\frac{Y_{j} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{j}\\right)\\right)}\\right\\} .\n\\end{aligned}\n$$\n\nWhere in the second to last inequality we have used that\n\n$$\n\\sup _{(\\widetilde{\\mathbf{w}}, \\hat{\\vartheta}) \\in \\mathcal{W}, \\widetilde{k} \\in\\left\\{1, \\ldots, K_{n}\\right\\}} \\frac{1}{n} \\sum_{j=1}^{n}\\left( \\pm \\epsilon_{j}\\right) \\cdot \\frac{Y_{j} \\cdot T_{\\beta_{n}} f_{\\hat{\\vartheta}_{k}}\\left(X_{j}\\right)}{1+\\exp \\left(Y_{j} \\cdot f_{(\\widetilde{\\mathbf{w}}, \\hat{\\vartheta})}\\left(X_{j}\\right)\\right)} \\geq 0\n$$\n\nsince there exists $(\\widetilde{\\mathbf{w}}, \\hat{\\vartheta}) \\in \\mathcal{W}$ such that $f_{\\hat{\\vartheta}}\\left(X_{j}\\right)=0$ for $j \\in\\{1, \\ldots, n\\}$.\nIn the ninth step of the proof we derive an upper bound on\n\n$$\n\\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right)\\right\\}\n$$\n\nTo do this we use a contraction style argument. Because of the independence of the random variables we can compute the expectation by first computing the expectation with respect to $\\epsilon_{1}$ and then by computing the expectation with respect to all other random variables. This yields that the last term above is equal to\n\n$$\n\\begin{aligned}\n& \\frac{1}{2} \\cdot \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n}\\left(\\sum_{i=2}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right.\\right. \\\\\n& \\left.\\left.+\\frac{1}{1+\\exp \\left(Y_{1} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{1}\\right)\\right)} \\cdot Y_{1} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{1}\\right)\\right)\\right\\} \\\\\n& +\\frac{1}{2} \\cdot \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n}\\left(\\sum_{i=2}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right.\\right. \\\\\n& \\left.\\left.-\\frac{1}{1+\\exp \\left(Y_{1} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{1}\\right)\\right)} \\cdot Y_{1} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{1}\\right)\\right)\\right\\} \\\\\n& =\\frac{1}{2} \\cdot \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta),(\\mathbf{w}, \\hat{\\vartheta}) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n}\\left(\\sum_{i=2}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right.\\right. \\\\\n& +\\frac{1}{n} \\sum_{i=2}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\hat{\\vartheta})}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\hat{\\vartheta}_{k}}\\left(X_{i}\\right) \\\\\n& \\left.+\\frac{1}{1+\\exp \\left(Y_{1} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{1}\\right)\\right)} \\cdot Y_{1} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{1}\\right)\\right. \\\\\n& \\left.\\left.-\\frac{1}{1+\\exp \\left(Y_{1} \\cdot f_{(\\mathbf{w}, \\hat{\\vartheta})}\\left(X_{1}\\right)\\right)} \\cdot Y_{1} \\cdot T_{\\beta_{n}} f_{\\hat{\\vartheta}_{k}}\\left(X_{1}\\right)\\right)\\right\\} \\\\\n& \\leq \\frac{1}{2} \\cdot \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta),(\\mathbf{w}, \\hat{\\vartheta}) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n}\\left(\\sum_{i=2}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right.\\right. \\\\\n& +\\sum_{i=2}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\hat{\\vartheta})}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\hat{\\vartheta}_{k}}\\left(X_{i}\\right)\n\\end{aligned}\n$$\n\n$$\n\\left.+\\beta_{n} \\cdot\\left|f_{(\\mathbf{w}, \\vartheta)}\\left(X_{1}\\right)-f_{\\left(\\overline{\\mathbf{w}}, \\hat{\\vartheta}\\right)}\\left(X_{1}\\right)\\right|+\\left|T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{1}\\right)-T_{\\beta_{n}} f_{\\hat{\\vartheta}_{k}}\\left(X_{1}\\right)\\right|\\right)\\right\\}\n$$\n\nThe sum inside the supremum above does not change its value if $((\\mathbf{w}, \\vartheta), k)$ is interchanged with $\\left((\\overline{\\mathbf{w}}, \\hat{\\vartheta}), \\bar{k}\\right)$. Consequently we can assume without loss of generality that $f_{(\\mathbf{w}, \\vartheta)}\\left(X_{1}\\right)-f_{\\left(\\overline{\\mathbf{w}}, \\hat{\\vartheta}\\right)}\\left(X_{1}\\right)$ is positive or that it is negative. Set $\\bar{\\epsilon}_{1}=\\bar{\\epsilon}_{1}\\left(\\epsilon_{1}, X_{1}, Y_{1}, \\ldots, X_{n}, Y_{n}\\right)=$ $\\epsilon_{1}$ if the functions $f_{(\\mathbf{w}, \\vartheta)}\\left(X_{1}\\right)-f_{(\\overline{\\mathbf{w}}, \\hat{\\vartheta})}\\left(X_{1}\\right)$ and $T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{1}\\right)-T_{\\beta_{n}} f_{\\hat{\\vartheta}_{k}}\\left(X_{1}\\right)$ which \"attain\" the above supremum have the same sign, and set it equal to $-\\epsilon_{1}$ otherwise. Then the right-hand side above is equal to\n\n$$\n\\begin{aligned}\n& \\frac{1}{2} \\cdot \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta),(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, \\\\\nk, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n}\\left(\\sum_{i=2}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right.\\right. \\\\\n& \\left.+\\sum_{i=2}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{\\left(\\overline{\\mathbf{w}}, \\hat{\\vartheta}\\right)}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\hat{\\vartheta}_{k}}\\left(X_{i}\\right)\\right. \\\\\n& \\left.\\left.+\\beta_{n} \\cdot \\epsilon_{1} \\cdot\\left(f_{(\\mathbf{w}, \\vartheta)}\\left(X_{1}\\right)-f_{\\left(\\overline{\\mathbf{w}}, \\hat{\\vartheta}\\right)}\\left(X_{1}\\right)\\right)+\\bar{\\epsilon}_{1} \\cdot\\left(T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{1}\\right)-T_{\\beta_{n}} f_{\\hat{\\vartheta}_{k}}\\left(X_{1}\\right)\\right)\\right)\\right\\} \\\\\n& \\leq \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, \\\\\nk \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n}\\left(\\sum_{i=2}^{n} \\epsilon_{i} \\cdot \\frac{1}{1+\\exp \\left(Y_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)\\right)} \\cdot Y_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right.\\right. \\\\\n& \\left.\\left.+\\beta_{n} \\cdot \\epsilon_{1} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{1}\\right)+\\bar{\\epsilon}_{1} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{1}\\right)\\right)\\right\\}\n\\end{aligned}\n$$\n\nwhere we have used that conditioned on $\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right)$ the random vector $\\left(\\epsilon_{1}, \\bar{\\epsilon}_{1}\\right)$ has the same distribution as the random vector $\\left(-\\epsilon_{1},-\\bar{\\epsilon}_{1}\\right)$.\n\nArguing in the same way for $k=2, \\ldots, n$ we see that we can upper bound the term on the right-hand side above by\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, \\\\\nk \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n} \\sum_{i=1}^{n}\\left(\\beta_{n} \\cdot \\epsilon_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)+\\bar{\\epsilon}_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right)\\right\\} \\\\\n& \\leq \\beta_{n} \\cdot \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(X_{i}\\right)\\right\\} \\\\\n& +\\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, \\\\\nk \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n} \\sum_{i=1}^{n} \\bar{\\epsilon}_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(X_{i}\\right)\\right\\} \\\\\n& \\leq \\beta_{n} \\cdot \\sup _{x_{1}, \\ldots, x_{n} \\in \\mathbb{R}^{d_{1} \\times d_{2}}} \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(x_{i}\\right)\\right\\} \\\\\n& +\\sup _{x_{1}, \\ldots, x_{n} \\in \\mathbb{R}^{d_{1} \\times d_{2}}} \\mathbf{E}\\left\\{\\sup _{\\substack{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, \\\\\nk \\in\\left\\{1, \\ldots, K_{n}\\right\\}}} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(x_{i}\\right)\\right\\}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n= & \\beta_{n} \\cdot \\sup _{x_{1}, \\ldots, x_{n} \\in \\mathbb{R}^{d_{1} \\times d_{2}}} \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(x_{i}\\right)\\right\\} \\\\\n& +\\sup _{x_{1}, \\ldots, x_{n} \\in \\mathbb{R}^{d_{1} \\times d_{2}}} \\mathbf{E}\\left\\{\\sup _{\\vartheta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta}\\left(x_{i}\\right)\\right\\}\n\\end{aligned}\n$$\n\nwhere the last equality follows from\n\n$$\n\\left\\{T_{\\beta_{n}} f_{\\vartheta_{k}}:(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}\\right\\}=\\left\\{T_{\\beta_{n}} f_{\\vartheta_{1}}:(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}\\right\\}\n$$\n\nIn the tenth step of the proof we show for $x_{1}, \\ldots, x_{n} \\in \\mathbb{R}^{d_{1} \\times d_{2}}$ arbitrary\n\n$$\n\\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(x_{i}\\right)\\right\\} \\leq \\mathbf{E}\\left\\{\\sup _{\\vartheta \\in \\Theta}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot\\left(T_{\\beta_{n}} f_{\\vartheta}\\left(x_{i}\\right)\\right)\\right|\\right\\}\n$$\n\nWe have\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot f_{(\\mathbf{w}, \\vartheta)}\\left(x_{i}\\right)\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot \\sum_{j=1}^{K_{n}} w_{j} \\cdot\\left(T_{\\beta_{n}} f_{\\vartheta_{j}}\\left(x_{i}\\right)\\right)\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\sum_{j=1}^{K_{n}} w_{j} \\cdot \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot\\left(T_{\\beta_{n}} f_{\\vartheta_{j}}\\left(x_{i}\\right)\\right)\\right\\} \\\\\n& \\leq \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\sum_{j=1}^{K_{n}}\\left|w_{j}\\right| \\cdot\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot\\left(T_{\\beta_{n}} f_{\\vartheta_{j}}\\left(x_{i}\\right)\\right)\\right|\\right\\} \\\\\n& \\leq \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}} \\sum_{j=1}^{K_{n}}\\left|w_{j}\\right| \\cdot \\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot\\left(T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(x_{i}\\right)\\right)\\right|\\right\\} \\\\\n& \\leq 1 \\cdot \\mathbf{E}\\left\\{\\sup _{(\\mathbf{w}, \\vartheta) \\in \\mathcal{W}, k \\in\\left\\{1, \\ldots, K_{n}\\right\\}}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot\\left(T_{\\beta_{n}} f_{\\vartheta_{k}}\\left(x_{i}\\right)\\right)\\right|\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\sup _{\\vartheta \\in \\Theta}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot\\left(T_{\\beta_{n}} f_{\\vartheta}\\left(x_{i}\\right)\\right)\\right|\\right\\}\n\\end{aligned}\n$$\n\nwhere the last equality followed from (21).\nSummarizing the above results, the proof is complete.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# 4.2. Proof of Theorem 2 \n\nIn the proof of Theorem 2 we will need the following auxiliary results.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# 4.2.1. Using the logistic risk for classification \n\nLemma 2 Let $\\varphi$ be the logistic loss. Let $(X, Y),\\left(X_{1}, Y_{1}\\right), \\ldots,\\left(X_{n}, Y_{n}\\right)$ and $f^{*}, \\mathcal{D}_{n}, f_{n}$ and $\\check{C}_{n}$ as in Sections 1 and 3, and set\n\n$$\nf_{\\varphi}^{*}=\\arg \\min _{f:[0,1]^{d_{1} \\times d_{2}} \\rightarrow \\mathbb{R}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\}\n$$\n\na) Then\n\n$$\n\\begin{aligned}\n& \\mathbf{P}\\left\\{Y \\neq \\check{C}_{n}(X) \\mid \\mathcal{D}_{n}\\right\\}-\\mathbf{P}\\left\\{Y \\neq f^{*}(X)\\right\\} \\\\\n& \\leq \\frac{1}{\\sqrt{2}} \\cdot\\left(\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right) \\mid \\mathcal{D}_{n}\\right\\}-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\varphi}^{*}(X)\\right)\\right\\}\\right)^{1 / 2}\n\\end{aligned}\n$$\n\nholds.\nb) Then\n\n$$\n\\begin{aligned}\n& \\mathbf{P}\\left\\{Y \\neq \\check{C}_{n}(X) \\mid \\mathcal{D}_{n}\\right\\}-\\mathbf{P}\\left\\{Y \\neq f^{*}(X)\\right\\} \\\\\n& \\leq 2 \\cdot\\left(\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right) \\mid \\mathcal{D}_{n}\\right\\}-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\varphi}^{*}(X)\\right)\\right\\}\\right)+4 \\cdot \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\varphi}^{*}(X)\\right)\\right\\}\n\\end{aligned}\n$$\n\nholds.\nc) Assume that\n\n$$\n\\mathbf{P}\\left\\{\\left|f_{\\varphi}^{*}(X)\\right|>\\tilde{F}_{n}\\right\\} \\geq 1-e^{-\\tilde{F}_{n}}\n$$\n\nfor a given sequence $\\left\\{\\tilde{F}_{n}\\right\\}_{n \\in \\mathbb{N}}$ with $\\tilde{F}_{n} \\rightarrow \\infty$. Then\n\n$$\n\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\varphi}^{*}(X)\\right)\\right\\} \\leq c_{12} \\cdot \\tilde{F}_{n} \\cdot e^{-\\tilde{F}_{n}}\n$$\n\nholds.\nProof. a) This result follows from Theorem 2.1 in Zhang (2004), where we choose $s=2$ and $c=2^{-1 / 2}$.\nb) This result follows from Lemma 1 b) in Kohler and Langer (2025).\nc) This result follows from Lemma 3 in Kim, Ohn and Kim (2019).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "### 4.2.2. Lipschitz property of the networks\n\nLemma 3 Let $\\mathcal{F}_{n}=\\left\\{f_{\\vartheta}: \\vartheta \\in \\Theta\\right\\}$ be the class of deep convolutional neural networks introduced in Subsection 3.1 (cf., (14)). Set\n\n$$\nM_{\\max }=\\max \\left\\{M_{1}, \\ldots, M_{L_{n}^{(1)}}\\right\\} \\quad \\text { and } \\quad k_{\\max }=\\max \\left\\{k_{1}, \\ldots, k_{L_{n}^{(1)}}\\right\\}\n$$\n\nLet $\\vartheta, \\bar{\\vartheta} \\in \\Theta$ such that\n\n$$\n\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty} \\leq 1\n$$\n\nholds and all weights in $f_{\\vartheta}$ are bounded in absolute value by $B_{n} \\geq 0$. Then\n\n$$\n\\left\\|f_{\\vartheta}-f_{\\bar{\\vartheta}}\\right\\|_{\\infty,[0,1]^{\\vartheta_{1} \\times d_{2}}} \\leq 7 \\cdot L_{n}^{(2)} \\cdot L_{n}^{(1)} \\cdot k_{\\max }^{L_{n}^{(1)+1}} \\cdot\\left(M_{\\max }^{2}+1\\right)^{L_{n}^{(1)}} \\cdot\\left(B_{n}+1\\right)^{L_{n}^{(1)}+3} \\cdot\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty}\n$$\n\nProof. Let $o^{(l)}, g$ and $\\bar{o}^{(l)}, \\bar{g}$ be defined as in Section 3.1 using the weights in $\\vartheta$ and $\\bar{\\vartheta}$, resp., and set\n\n$$\n\\left\\|o^{(l)}(x)\\right\\|_{\\infty}=\\max _{(i, j), s_{2}}\\left|o_{(i, j), s_{2}}^{(l)}(x)\\right|\n$$\n\nThen $|\\sigma(z)| \\leq|z|$ implies\n\n$$\n\\begin{aligned}\n\\left|o_{(i, j), s_{2}}^{(r)}(x)\\right| & \\leq k_{\\max } \\cdot\\left(M_{\\max }^{2}+1\\right) \\cdot B_{n} \\cdot \\max \\left\\{\\left\\|o^{(r-1)}(x)\\right\\|_{\\infty}, 1\\right\\} \\\\\n& \\leq k_{\\max }^{r} \\cdot\\left(M_{\\max }^{2}+1\\right)^{r} \\cdot B_{n}^{r}\n\\end{aligned}\n$$\n\nUsing this together with $\\left|\\sigma\\left(z_{1}\\right)-\\sigma\\left(z_{2}\\right)\\right| \\leq\\left|z_{1}-z_{2}\\right|$ we conclude\n\n$$\n\\begin{aligned}\n& \\left|o_{(i, j), s_{2}}^{(r)}(x)-\\bar{o}_{(i, j), s_{2}}^{(r)}(x)\\right| \\\\\n& \\leq\\left|\\sum_{s_{1}=1}^{k_{r-1}} \\sum_{\\substack{t_{1}, t_{2} \\in\\left\\{1, \\ldots, M_{r}\\right\\} \\\\\n\\left(i+t_{1}-1, j+t_{2}-1\\right)}}\\right| w_{t_{1}, t_{2}, s_{1}, s_{2}}^{(r)}\\cdot o_{i+t_{1}-1, j+t_{2}-1, s_{1}}^{(r)}(x)-\\bar{w}_{t_{1}, t_{2}, s_{1}, s_{2}}^{(r)}(x)-\\left|w_{s_{2}}^{(r)}-w_{s_{2}}^{(r)}\\right| \\\\\n& \\left.+w_{s_{2}}^{(r)}-\\bar{w}_{s_{2}}^{(r)}\\right| \\\\\n& \\leq \\sum_{s_{1}=1}^{k_{r-1}} \\sum_{\\substack{t_{1}, t_{2} \\in\\left\\{1, \\ldots, M_{r}\\right\\} \\\\\n\\left(i+t_{1}-1, j+t_{2}-1\\right)}}\\left|w_{t_{1}, t_{2}, s_{1}, s_{2}}^{(r)}-\\bar{w}_{t_{1}, t_{2}, s_{1}, s_{2}}^{(r)}\\right| \\cdot\\left|o_{i+t_{1}-1, j+t_{2}-1, s_{1}}^{(r-1)}(x)\\right|+\\left|w_{s_{2}}^{(r)}-\\bar{w}_{s_{2}}^{(r)}\\right| \\\\\n& +\\sum_{s_{1}=1}^{k_{r-1}} \\sum_{\\substack{t_{1}, t_{2} \\in\\left\\{1, \\ldots, M_{r}\\right\\} \\\\\n\\left(i+t_{1}-1, j+t_{2}-1\\right)}}\\left|\\bar{w}_{t_{1}, t_{2}, s_{1}, s_{2}}^{(r)}\\right| \\cdot\\left|o_{i+t_{1}-1, j+t_{2}-1, s_{1}}^{(r-1)}(x)-\\bar{o}_{i+t_{1}-1, j+t_{2}-1, s_{1}}^{(r-1)}(x)\\right| \\\\\n& \\leq k_{\\max } \\cdot\\left(M_{\\max }^{2}+1\\right) \\cdot\\left(\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty} \\cdot k_{\\max }^{r-1} \\cdot\\left(M_{\\max }^{2}+1\\right)^{r-1} \\cdot B_{n}^{r-1}+\\left(B_{n}+1\\right) \\cdot\\left\\|o^{(r-1)}-\\bar{o}^{(r-1)}\\right\\|_{\\infty}\\right) \\\\\n& \\leq r \\cdot k_{\\max }^{r} \\cdot\\left(M_{\\max }^{2}+1\\right)^{r} \\cdot\\left(B_{n}+1\\right)^{r-1} \\cdot\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty}\n\\end{aligned}\n$$\n\nFrom this and\n\n$$\n\\left|\\max \\left\\{a_{1}, \\ldots, a_{n}\\right\\}-\\max \\left\\{b_{1}, \\ldots, b_{n}\\right\\}\\right| \\leq \\max \\left\\{\\left|a_{1}-b_{1}\\right|, \\ldots,\\left|a_{n}-b_{n}\\right|\\right\\}\n$$\n\nwe conclude\n\n$$\n\\begin{aligned}\n& \\left|f_{\\mathbf{w}, \\mathbf{w}_{\\text {bias }}, \\mathbf{w}_{\\text {out }}}(x)-f_{\\overline{\\mathbf{w}}, \\overline{\\mathbf{w}}_{\\text {bias }}, \\mathbf{w}_{\\text {out }}}(x)\\right| \\\\\n& \\leq k_{\\max } \\cdot\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty} \\cdot B_{n} \\cdot k_{\\max }^{L_{n}^{(1)}} \\cdot\\left(M_{\\max }^{2}+1\\right)^{L_{n}^{(1)}} \\cdot B_{n}^{L_{n}^{(1)}} \\\\\n& \\quad+k_{\\max } \\cdot\\left(B_{n}+1\\right) \\cdot L_{n}^{(1)} \\cdot k_{\\max }^{L_{n}^{(1)}} \\cdot\\left(M_{\\max }^{2}+1\\right)^{L_{n}^{(1)}} \\cdot\\left(B_{n}+1\\right)^{L_{n}^{(1)}-1} \\cdot\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty} \\\\\n& \\leq\\left(L_{n}^{(1)}+1\\right) \\cdot k_{\\max }^{L_{n}^{(1)}+1} \\cdot\\left(M_{\\max }^{2}+1\\right)^{L_{n}^{(1)}} \\cdot\\left(B_{n}+1\\right)^{L_{n}^{(1)}+1} \\cdot\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty}\n\\end{aligned}\n$$\n\nThis implies\n\n$$\n|g(x)-\\bar{g}(x)|\n$$\n\n$$\n\\begin{aligned}\n\\leq & \\left(L_{n}^{(2)}+1\\right) \\cdot\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty} \\cdot 2 \\cdot B_{n} \\cdot k_{\\max } \\cdot B_{n} \\cdot k_{\\max }^{\\left(L_{n}^{(1)}\\right.} \\cdot\\left(M_{\\max }^{2}+1\\right)^{L_{n}^{(1)}} \\cdot\\left(B_{n}+1\\right)^{L_{n}^{(1)}} \\\\\n+ & L_{n}^{(2)} \\cdot\\left(B_{n}+1\\right) \\cdot\\left(2 \\cdot\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty} \\cdot k_{\\max } \\cdot B_{n} \\cdot k_{\\max }^{\\left.L_{n}^{(1)}\\right)} \\cdot\\left(M_{\\max }^{2}+1\\right)^{L_{n}^{(1)}} \\cdot\\left(B_{n}+1\\right)^{L_{n}^{(1)}} \\\\\n& \\left.+\\left(B_{n}+1\\right) \\cdot\\left(L_{n}^{(1)}+1\\right) \\cdot k_{\\max }^{\\left.L_{n}^{(1)}+1\\right)} \\cdot\\left(M_{\\max }^{2}+1\\right)^{L_{n}^{(1)}} \\cdot\\left(B_{n}+1\\right)^{L_{n}^{(1)}+1} \\cdot\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty}\\right) \\\\\n\\leq & 7 \\cdot L_{n}^{(2)} \\cdot L_{n}^{(1)} \\cdot k_{\\max }^{\\left.L_{n}^{(1)}+1\\right)} \\cdot\\left(M_{\\max }^{2}+1\\right)^{L_{n}^{(1)}} \\cdot\\left(B_{n}+1\\right)^{L_{n}^{(1)}+3} \\cdot\\|\\vartheta-\\bar{\\vartheta}\\|_{\\infty}\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "# 4.2.3. Approximation error \n\nIn our next lemma we present a bound on the error we make in case that we replace the functions $g_{k, s}$ in a hierarchical model by some approximations of them.\n\nLemma 4 Let $d_{1}, d_{2}, t \\in \\mathbb{N}$ and $l \\in \\mathbb{N}$ with $2^{l} \\leq \\min \\left\\{d_{1}, d_{2}\\right\\}$. For $a \\in\\{1, \\ldots, t\\}$, set $I=\\left\\{0,1, \\ldots, 2^{l}-1\\right\\} \\times\\left\\{0,1, \\ldots, 2^{l}-1\\right\\}$ and define\n\n$$\nm_{a}(x)=\\max _{(i, j) \\in \\mathbb{Z}^{2}:(i, j)+I \\subseteq\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}} f_{a}\\left(x_{(i, j)+I}\\right)\n$$\n\nand\n\n$$\n\\bar{m}_{a}(x)=\\max _{(i, j) \\in \\mathbb{Z}^{2}:(i, j)+I \\subseteq\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}} \\bar{f}_{a}\\left(x_{(i, j)+I}\\right)\n$$\n\nwhere $f_{a}$ and $\\bar{f}_{a}$ satisfy\n\n$$\nf_{a}=f_{l, 1}^{(a)} \\quad \\text { and } \\quad \\bar{f}_{a}=\\bar{f}_{l, 1}^{(a)}\n$$\n\nfor some $f_{k, s}^{(a)}, \\bar{f}_{k, s}^{(a)}: \\mathbb{R}^{\\left\\{1, \\ldots, 2^{k}\\right\\} \\times\\left\\{1, \\ldots, 2^{k}\\right\\}} \\rightarrow \\mathbb{R}$ recursively defined by\n\n$$\n\\begin{aligned}\n& f_{k, s}^{(a)}(x)=g_{k, s}^{(a)}\\left(f_{k-1,4 \\cdot(s-1)+1}^{(a)}\\left(x_{\\left\\{1, \\ldots, 2^{k-1}\\right\\} \\times\\left\\{1, \\ldots, 2^{k-1}\\right\\}}\\right),\\right. \\\\\n& \\left.f_{k-1,4 \\cdot(s-1)+2}^{(a)}\\left(x_{\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\} \\times\\left\\{1, \\ldots, 2^{k-1}\\right\\}}\\right),\\right. \\\\\n& \\left.f_{k-1,4 \\cdot(s-1)+3}^{(a)}\\left(x_{\\left\\{1, \\ldots, 2^{k-1}\\right\\} \\times\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\}}\\right),\\right. \\\\\n& \\left.\\left.f_{k-1,4 \\cdot s}^{(a)}\\left(x_{\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\} \\times\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\}}\\right)\\right)\\right.\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\bar{f}_{k, s}^{(a)}(x)=g_{k, s}^{(a)}\\left(f_{k-1,4 \\cdot(s-1)+1}^{(a)}\\left(x_{\\left\\{1, \\ldots, 2^{k-1}\\right\\} \\times\\left\\{1, \\ldots, 2^{k-1}\\right\\}}\\right),\\right. \\\\\n& \\left.f_{k-1,4 \\cdot(s-1)+2}^{(a)}\\left(x_{\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\} \\times\\left\\{1, \\ldots, 2^{k-1}\\right\\}}\\right),\\right. \\\\\n& \\left.\\bar{f}_{k-1,4 \\cdot(s-1)+3}^{(a)}\\left(x_{\\left\\{1, \\ldots, 2^{k-1}\\right\\} \\times\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\}}\\right),\\right. \\\\\n& \\left.\\bar{f}_{k-1,4 \\cdot s}^{(a)}\\left(x_{\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\} \\times\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\}}\\right)\\right)\n\\end{aligned}\n$$\n\nfor $k=2, \\ldots, l, s=1, \\ldots, 4^{l-k}$, and\n\n$$\nf_{1, s}^{(a)}\\left(x_{1,1}, x_{1,2}, x_{2,1}, x_{2,2}\\right)=g_{1, s}^{(a)}\\left(x_{1,1}, x_{1,2}, x_{2,1}, x_{2,2}\\right)\n$$\n\nand\n\n$$\n\\bar{f}_{1, s}^{(a)}\\left(x_{1,1}, x_{1,2}, x_{2,1}, x_{2,2}\\right)=\\bar{g}_{1, s}^{(a)}\\left(x_{1,1}, x_{1,2}, x_{2,1}, x_{2,2}\\right)\n$$\n\nfor $s=1, \\ldots, 4^{l-1}$, where\n\n$$\ng_{k, s}^{(a)}: \\mathbb{R}^{4} \\rightarrow[0,1] \\text { and } \\bar{g}_{k, s}^{(a)}: \\mathbb{R}^{4} \\rightarrow \\mathbb{R}\n$$\n\nare functions for $a \\in\\{1, \\ldots, t\\}, k \\in\\{1, \\ldots, l\\}$ and $s \\in\\left\\{1, \\ldots, 4^{l-k}\\right\\}$. Furthermore, let $g: \\mathbb{R}^{t} \\rightarrow[0,1]$ and $\\bar{g}: \\mathbb{R}^{t} \\rightarrow \\mathbb{R}$ be functions. Assume that all restrictions $g_{k, s}^{(a)}\\left\\|_{-2,2]^{4}}:\\right.$ $\\left.[-2,2]^{4} \\rightarrow[0,1]\\right.$ and $\\left.g\\right\\|_{-2,2]^{t}}:[-2,2]^{t} \\rightarrow[0,1]$ are Lipschitz continuous with respect to the Euclidean distance with Lipschitz constant $C>0$ and for all $a \\in\\{1, \\ldots, t\\}, k \\in\\{1, \\ldots, l\\}$ and $s \\in\\left\\{1, \\ldots, 4^{l-k}\\right\\}$ we assume that\n\n$$\n\\left\\|\\bar{g}_{k, s}^{(a)}\\right\\|_{[-2,2]^{4}, \\infty} \\leq 2\n$$\n\nThen for any $x \\in[0,1]^{\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}}$ it holds:\n\n$$\n\\begin{aligned}\n& \\left|g\\left(m_{1}(x), \\ldots, m_{t}(x)\\right)-\\bar{g}\\left(\\bar{m}_{1}(x), \\ldots, \\bar{m}_{t}(x)\\right)\\right| \\\\\n& \\quad \\leq \\sqrt{t} \\cdot(2 C+1)^{l} \\\\\n& \\quad \\max _{a \\in\\{1, \\ldots, t\\}, j \\in\\{1, \\ldots, l\\}, s \\in\\left\\{1, \\ldots, 4^{l-j}\\right\\}}\\left\\{\\left\\|\\bar{g}_{j, s}^{(a)}-\\bar{g}_{j, s}^{(a)}\\right\\|_{[-2,2]^{4}, \\infty},\\|g-\\bar{g}\\|_{[-2,2]^{t}, \\infty}\\right\\} .\n\\end{aligned}\n$$\n\nProof. See Lemma 1 in Kohler, Krzy\u017cak and Walter (2022).\nLemma 5 Let $d \\in \\mathbb{N}$, let $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ be $(p, C)$-smooth for some $p=q+s, q \\in \\mathbb{N}_{0}$ and $s \\in(0,1]$, and $C>0$. Let $A \\geq 1$ and $M \\in \\mathbb{N}$ sufficiently large (independent of the size of $A$, but\n\n$$\nM \\geq 2 \\text { and } M^{2 p} \\geq c_{13} \\cdot\\left(\\max \\left\\{A,\\|f\\|_{C^{q}\\left([-A, A]^{d}\\right)}\\right\\}\\right)^{4(q+1)}\n$$\n\nwhere\n\n$$\n\\|f\\|_{C^{q}\\left([-A, A]^{d}\\right)}=\\max _{\\substack{\\alpha_{1}, \\ldots \\alpha_{d} \\in \\mathbb{N}_{0} \\\\ \\alpha_{1}+\\cdots+\\alpha_{d} \\leq q}}\\left\\|\\frac{\\partial^{q} f}{\\partial x_{1}^{\\alpha_{1}} \\cdots \\partial x_{d}^{\\alpha_{d}}}\\right\\|_{\\infty,[-A, A]^{d}}\n$$\n\nmust hold for some sufficiently large constant $c_{13} \\geq 1$ ).\nLet $L, r \\in \\mathbb{N}$ be such that\n\n1. $L \\geq 5 M^{d}+\\left[\\log _{4}\\left(M^{2 p+4 \\cdot d \\cdot(q+1)} \\cdot e^{4 \\cdot(q+1) \\cdot\\left(M^{d}-1\\right)}\\right)\\right]$\n\n$$\n\\cdot\\left\\lceil\\log _{2}(\\max \\{q, d\\}+1)\\right\\rceil+\\left\\lceil\\log _{4}\\left(M^{2 p}\\right)\\right\\rceil\n$$\n\n2. $r \\geq 132 \\cdot 2^{d} \\cdot\\left\\lceil e^{d}\\right\\rceil \\cdot\\binom{d+q}{d} \\cdot \\max \\left\\{q+1, d^{2}\\right\\}$\n\nhold. There exists a feedforward neural network $f_{\\text {net,deep }}$ with ReLU activation function, $L$ hidden layers and $r$ neurons per hidden layer where all weights are bounded in absolute value by\n\n$$\ne^{c_{14} \\cdot(p+1) \\cdot M^{d}}\n$$\n\nfor some $c_{14}=c_{14}(f)>0$, such that\n\n$$\n\\left\\|f-f_{\\text {net,deep }}\\right\\|_{\\infty,[-A, A]^{d}} \\leq c_{15} \\cdot\\left(\\max \\left\\{A,\\|f\\|_{C^{q}\\left([-A, A]^{d}\\right)}\\right\\}\\right)^{4(q+1)} \\cdot M^{-2 p}\n$$\n\nholds.\nProof. This theorem is proven without the upper bound on the absolute values of the weights in Theorem 2 in Kohler and Langer (2021). It is explained in the supplement how the above upper bound on the absolute value of the weights follows from the proof given there.\n\nLemma 6 Let $d_{1}, d_{2}, l \\in \\mathbb{N}$ with $2^{l} \\leq \\min \\left\\{d_{1}, d_{2}\\right\\}$. For $k \\in\\{1, \\ldots, l\\}$ and $s \\in\\left\\{1, \\ldots, 4^{l-k}\\right\\}$ let\n\n$$\n\\bar{g}_{\\text {net }, k, s}: \\mathbb{R}^{4} \\rightarrow \\mathbb{R}\n$$\n\nbe defined by a feedforward neural network with $L_{\\text {net }} \\in \\mathbb{N}$ hidden layers and $r_{\\text {net }} \\in \\mathbb{N}$ neurons per hidden layer and ReLU activation function, where all the weights are bounded in absolute value by some $B_{n} \\geq 1$. Set\n\n$$\nI=\\left\\{0, \\ldots, 2^{l}-1\\right\\} \\times\\left\\{0, \\ldots, 2^{l}-1\\right\\}\n$$\n\nand define $\\bar{m}:[0,1]^{\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}} \\rightarrow \\mathbb{R}$ by\n\n$$\n\\bar{m}(x)=\\max _{(i, j) \\in \\mathbb{Z}^{2}:(i, j)+I \\subseteq\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}} \\bar{f}\\left(x_{(i, j)+I}\\right)\n$$\n\nwhere $\\bar{f}$ satisfies\n\n$$\n\\bar{f}=\\bar{f}_{l, 1}\n$$\n\nfor some $\\bar{f}_{k, s}:[-2,2]^{\\left\\{1, \\ldots, 2^{k}\\right\\} \\times\\left\\{1, \\ldots, 2^{k}\\right\\}} \\rightarrow \\mathbb{R}$ recursively defined by\n\n$$\n\\begin{gathered}\n\\bar{f}_{k, s}(x)=\\bar{g}_{n e t, k, s}\\left(\\bar{f}_{k-1,4 \\cdot(s-1)+1}\\left(x_{\\left\\{1, \\ldots, 2^{k-1}\\right\\} \\times\\left\\{1, \\ldots, 2^{k-1}\\right\\}}\\right)\\right. \\\\\n\\bar{f}_{k-1,4 \\cdot(s-1)+2}\\left(x_{\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\} \\times\\left\\{1, \\ldots, 2^{k-1}\\right\\}}\\right) \\\\\n\\bar{f}_{k-1,4 \\cdot(s-1)+3}\\left(x_{\\left\\{1, \\ldots, 2^{k-1}\\right\\} \\times\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\}}\\right) \\\\\n\\left.\\bar{f}_{k-1,4 \\cdot s}\\left(x_{\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\} \\times\\left\\{2^{k-1}+1, \\ldots, 2^{k}\\right\\}}\\right)\\right)\n\\end{gathered}\n$$\n\nfor $k=2, \\ldots, l, s=1, \\ldots, 4^{l-k}$, and\n\n$$\n\\bar{f}_{1, s}\\left(x_{1,1}, x_{1,2}, x_{2,1}, x_{2,2}\\right)=\\bar{g}_{n e t, 1, s}\\left(x_{1,1}, x_{1,2}, x_{2,1}, x_{2,2}\\right)\n$$\n\nfor $s=1, \\ldots, 4^{l-1}$. Set\n\n$$\nl_{\\text {net }}=\\frac{4^{l}-1}{3} \\cdot L_{\\text {net }}+l\n$$\n\n$$\nk_{s}=\\frac{2 \\cdot 4^{l}+4}{3}+r_{\\text {net }} \\quad\\left(s=1, \\ldots, l_{\\text {net }}\\right)\n$$\n\nand set\n\n$$\nM_{s}=2^{\\pi(s)} \\quad \\text { for } s \\in\\left\\{1, \\ldots, l_{\\text {net }}\\right\\}\n$$\n\nwhere the function $\\pi:\\left\\{1, \\ldots, l_{\\text {net }}\\right\\} \\rightarrow\\{1, \\ldots, l\\}$ is defined by\n\n$$\n\\pi(s)=\\sum_{i=1}^{l} \\mathbb{1}_{\\left\\{s \\geq i+\\sum_{i=l-i+1}^{l-1} 4^{r} \\cdot L_{\\text {net }}\\right\\}}\n$$\n\nThen there exists some $m_{\\text {net }} \\in \\mathcal{F}_{l_{\\text {net }}, \\mathbf{k}, \\mathbf{M}}^{C N N}$, where all the weights are bounded in absolute value by $B_{n}$, such that\n\n$$\n\\bar{m}(x)=m_{\\text {net }}(x)\n$$\n\nholds for all $x \\in[-2,2]\\left\\{1, \\ldots, d_{1}\\right\\} \\times\\left\\{1, \\ldots, d_{2}\\right\\}$.\nProof. This theorem is proven without the upper bound on the absolute values of the weights in Lemma 2 in Kohler, Krzy\u017cak and Walter (2022). In its proof the weights from the feedforward neural networks are copied in the convolutional neural network, and all other weights occurring are bounded in absolute value by 1 , therefore the bound on the absolute values of the weights holds.\n\nLemma 7 Set\n\n$$\nf(z)= \\begin{cases}\\infty & , z=1 \\\\ \\log \\frac{z}{1-z} & , 0<z<1 \\\\ -\\infty & , z=0\\end{cases}\n$$\n\nlet $K \\in \\mathbb{N}$ with $K \\geq 6$, let $m: \\mathbb{R}^{d \\cdot l} \\rightarrow[0,1]$ and let $\\bar{g}: \\mathbb{R}^{d_{1} \\times d_{2}} \\rightarrow \\mathbb{R}$ such that $\\|\\bar{g}-m\\|_{\\infty,[0,1]^{d_{1} \\times d_{2}}} \\leq \\epsilon$ for some\n\n$$\n0 \\leq \\epsilon \\leq \\frac{1}{K}\n$$\n\nThen there exists a neural network $\\bar{f}: \\mathbb{R} \\rightarrow \\mathbb{R}$ with ReLU activation function, and one hidden layer with $3 \\cdot K+9$ neurons, where all the weights are bounded in absolute value by $K$, such that for each network $\\bar{f}: \\mathbb{R} \\rightarrow \\mathbb{R}$ which has the same structure and which has weights which are in supremum norm not more than\n\n$$\n0 \\leq \\bar{\\epsilon} \\leq 1\n$$\n\naway from the weights of the above network we have that $\\bar{f} \\circ \\bar{g}$ satisfies\n\n$$\n\\|\\bar{f} \\circ \\bar{g}\\|_{\\infty,[0,1]^{d_{1} \\times d_{2}}} \\leq 132 \\cdot K^{2} \\cdot \\bar{\\epsilon}+\\log K\n$$\n\nand\n\n$$\n\\sup _{x \\in[0,1]^{d_{1} \\times d_{2}}}\\left(|m(x) \\cdot(\\varphi(\\bar{f}(\\bar{g}(x)))-\\varphi(f(m(x))))\\right|\n$$\n\n$$\n\\begin{gathered}\n+\\left|(1-m(x)) \\cdot\\left(\\varphi(-\\bar{f}(\\bar{g}(x)))-\\varphi(-f(m(x)))\\right)\\right| \\\\\n\\leq c_{16} \\cdot\\left(\\frac{\\log K}{K}+\\epsilon\\right)+132 \\cdot K^{2} \\cdot \\bar{\\epsilon}\n\\end{gathered}\n$$\n\nProof. See Lemma 13 in Kohler and Krzy\u017cak (2023).\nLemma 8 Let $p \\geq 1$ and $C>0$ be arbitrary. Assume that $\\eta: \\mathbb{R}^{d_{1} \\times d_{2}} \\rightarrow \\mathbb{R}$ satisfies a $(p, C)$-smooth hierarchical max-pooling model. Let $\\mathcal{F}_{n}$ be the set of all CNNs with ReLU activation function, which have $L_{n}^{(1)}$ convolutional layers with $c_{7}$ channels in each layer, where $c_{7}$ is sufficiently large, one max pooling layer and one additional layer with $L_{n}^{(2)}$ neurons. Furthermore assume $\\left(L_{n}^{(1)}\\right)^{2 p / 4} \\geq c_{17} \\cdot L_{n}^{(2)}$. Then there exists a network $f \\in \\mathcal{F}_{n}$ where all the weights are bounded in absolute value by\n\n$$\n\\max \\left\\{L_{n}^{(2)}, e^{c_{18}(\\eta) \\cdot(p+1) \\cdot L_{n}^{(1)}}\\right\\}\n$$\n\nsuch that\n\n$$\n\\mathbf{E}\\left\\{\\varphi(Y \\cdot f(X))\\right\\}-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\varphi}^{*}(X)\\right)\\right\\} \\leq c_{19} \\cdot\\left(\\frac{\\log L_{n}^{(2)}}{L_{n}^{(2)}}+\\frac{1}{\\left(L_{n}^{(1)}\\right)^{2 p / 4}}\\right)\n$$\n\nand such that $f$ is bounded in absolute value by $\\log L_{n}^{(2)}$.\nProof. Use Lemma 5 (applied with $d=4$ and\n\n$$\nM=\\left\\lfloor\\left(\\frac{3}{4^{l}-1} \\cdot\\left(L_{n}^{(1)}-l\\right)\\right)^{\\frac{1}{3}}\\right\\rfloor\n$$\n\nwhere $l$ is the level of the hierarchical max-pooling model for $\\eta$ ) and Lemma 6 to construct a convolutional neural network $\\bar{g}_{N N}$ built on the basis of feedforward neural networks which approximate the functions in the hierarchical model of the a posteriori probability $\\eta$ in supremum norm up to an error of order\n\n$$\n\\frac{1}{\\left(L_{n}^{(1)}\\right)^{2 p / 4}}\n$$\n\nApplication of Lemma 4 with $t=1$ and $g(x)=\\bar{g}(x)=x$ yields that $\\bar{g}_{N N}$ approximates $\\eta$ in supremum norm by an error of order (25). Next apply Lemma 7 (with $\\epsilon=c_{20} \\cdot \\frac{1}{\\left(L_{n}^{(1)}\\right)^{2 p / 4}}$ and $\\bar{\\epsilon}=0$ ) to construct a neural network $\\tilde{f}$ with one hidden layer and $L_{n}^{(2)}$ neurons which takes on function values bounded in absolute value by $\\log L_{n}^{(2)}$ and which satisfies\n\n$$\n\\sup _{x \\in[0,1]^{d_{1} \\times d_{2}}}\\left(\\left|\\eta(x) \\cdot\\left(\\varphi\\left(\\tilde{f}\\left(\\bar{g}_{N N}(x)\\right)\\right)-\\varphi(f(\\eta(x)))\\right)\\right|\\right.\n$$\n\n$$\n\\begin{gathered}\n+\\left|(1-\\eta(x)) \\cdot\\left(\\varphi\\left(-\\tilde{f}\\left(\\bar{g}_{N N}(x)\\right)\\right)-\\varphi(-f(\\eta(x)))\\right)\\right| \\\\\n\\leq c_{21} \\cdot\\left(\\frac{\\log L_{n}^{(2)}}{L_{n}^{(2)}}+\\frac{1}{\\left(L_{n}^{(1)}\\right)^{2 p / 4}}\\right)\n\\end{gathered}\n$$\n\nwhere $f$ is the function defined in Lemma 7. Because of\n\n$$\nf_{\\varphi}^{*}(x)=f(\\eta(x))\n$$\n\nthis implies\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot \\tilde{f}\\left(\\bar{g}_{N N}(X)\\right)\\right)\\right\\}-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\varphi}^{*}(X)\\right)\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\left(1_{\\{Y=1\\}}+1_{\\{Y=-1\\}}\\right) \\cdot\\left(\\varphi\\left(Y \\cdot \\tilde{f}\\left(\\bar{g}_{N N}(X)\\right)\\right)-\\varphi(Y \\cdot f(\\eta(X)))\\right)\\right\\} \\\\\n& =\\mathbf{E}\\left\\{\\eta(X) \\cdot\\left(\\varphi\\left(\\tilde{f}\\left(\\bar{g}_{N N}(X)\\right)\\right)-\\varphi(f(\\eta(X)))\\right)\\right. \\\\\n& \\left.\\quad+(1-\\eta(X)) \\cdot\\left(\\varphi\\left(-\\tilde{f}\\left(\\bar{g}_{N N}(X)\\right)\\right)-\\varphi(-f(\\eta(X)))\\right)\\right\\} \\\\\n& \\leq \\sup _{x \\in[0,1]^{d_{1} \\times d_{2}}}\\left(\\left|\\eta(x) \\cdot\\left(\\varphi\\left(\\tilde{f}\\left(\\bar{g}_{N N}(x)\\right)\\right)-\\varphi(f(\\eta(x)))\\right)\\right|\\right. \\\\\n& \\left.\\quad+\\left|(1-\\eta(x)) \\cdot\\left(\\varphi\\left(-\\tilde{f}\\left(\\bar{g}_{N N}(x)\\right)\\right)-\\varphi(-f(\\eta(x)))\\right)\\right|\\right) \\\\\n& \\leq c_{21} \\cdot\\left(\\frac{\\log L_{n}^{(2)}}{L_{n}^{(2)}}+\\frac{1}{\\left(L_{n}^{(1)}\\right)^{2 p / 4}}\\right) .\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "# 4.2.4. Generalization error \n\nLemma 9 Let $\\left\\{f_{\\vartheta}: \\vartheta \\in \\overline{\\boldsymbol{\\Theta}}\\right\\}$ be defined as in Subsection 3.2 and let $\\beta_{n}=c_{1} \\cdot \\log n$. Then we have\n\n$$\n\\begin{aligned}\n& \\sup _{x_{1}, \\ldots, x_{n} \\in[0,1]^{d_{1} \\times d_{2}}} \\mathbf{E}\\left\\{\\left|\\sup _{\\vartheta \\in \\overline{\\boldsymbol{\\Theta}}} \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}} f_{\\vartheta}\\left(x_{i}\\right)\\right|\\right\\} \\\\\n& \\leq c_{22} \\cdot(\\log n)^{2} \\cdot \\frac{\\sqrt{\\left(\\left(L_{n}^{(1)}\\right)^{2}+L_{n}^{(1)} \\cdot L_{n}^{(2)}\\right) \\cdot \\log \\left(\\max \\left\\{L_{n}^{(1)}, L_{n}^{(2)}\\right\\}\\right)}}{\\sqrt{n}}\n\\end{aligned}\n$$\n\nIn the proof of Lemma 8 we will need the following bound on the VC dimension of the class $\\left\\{f_{\\vartheta}: \\vartheta \\in \\overline{\\boldsymbol{\\Theta}}\\right\\}$ of functions.\n\nLemma 10 The VC dimension of the class $\\left\\{f_{\\vartheta}: \\vartheta \\in \\bar{\\Theta}\\right\\}$ of functions in Lemma 9 is bounded from above by\n\n$$\nc_{23} \\cdot\\left(\\left(L_{n}^{(1)}\\right)^{2}+L_{n}^{(1)} \\cdot L_{n}^{(2)}\\right) \\cdot \\log \\left(\\max \\left\\{L_{n}^{(1)}, L_{n}^{(2)}\\right\\}\\right)\n$$\n\nProof. The result follows from the proof of Lemma 7 in Kohler, Krzy\u017cak and Walter (2022).\n\nProof of Lemma 9. The results follows from Lemma 10 by an easy application of standard techniques from VC theory. For the sake of completeness we present nevertheless a detailed proof here.\n\nSet $\\mathcal{F}=\\left\\{f_{\\vartheta}: \\vartheta \\in \\bar{\\Theta}\\right\\}$. For $\\delta_{n}>0$ and $x_{1}, \\ldots, x_{n} \\in[0,1]^{d_{1} \\times d_{2}}$ we have\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\sup _{f \\in \\mathcal{F}}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}}\\left(f\\left(x_{i}\\right)\\right)\\right|\\right\\} \\\\\n& =\\int_{0}^{\\beta_{n}} \\mathbf{P}\\left\\{\\sup _{f \\in \\mathcal{F}}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}}\\left(f\\left(x_{i}\\right)\\right)\\right|>t\\right\\} d t \\\\\n& \\leq \\delta_{n}+\\int_{\\delta_{n}}^{\\beta_{n}} \\mathbf{P}\\left\\{\\sup _{f \\in \\mathcal{F}}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}}\\left(f\\left(x_{i}\\right)\\right)\\right|>t\\right\\} d t\n\\end{aligned}\n$$\n\nUsing a standard covering argument from empirical process theory we see that for any $t \\geq \\delta_{n}$ we have\n\n$$\n\\begin{aligned}\n& \\mathbf{P}\\left\\{\\sup _{f \\in \\mathcal{F}}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}}\\left(f\\left(x_{i}\\right)\\right)\\right|>t\\right\\} \\\\\n& \\leq \\mathcal{M}_{1}\\left(\\frac{\\delta_{n}}{2},\\left\\{T_{\\beta_{n}} f: f \\in \\mathcal{F}\\right\\}, x_{1}^{n}\\right) \\\\\n& \\cdot \\sup _{f \\in \\mathcal{F}} \\mathbf{P}\\left\\{\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}}\\left(f\\left(x_{i}\\right)\\right)\\right|>\\frac{t}{2}\\right\\}\n\\end{aligned}\n$$\n\nApplication of Lemma 10 and Theorem 9.4 in Gy\u00f6rfi et al. (2002) yields\n\n$$\n\\mathcal{M}_{1}\\left(\\frac{\\delta_{n}}{2},\\left\\{T_{\\beta_{n}} f: f \\in \\mathcal{F}\\right\\}, x_{1}^{n}\\right) \\leq c_{24} \\cdot\\left(\\frac{c_{25} \\cdot \\beta_{n}}{\\delta_{n}}\\right)^{c_{26} \\cdot\\left(\\left(L_{n}^{(1)}\\right)^{2}+L_{n}^{(1)} \\cdot L_{n}^{(2)}\\right) \\cdot \\log \\left(\\max \\left\\{L_{n}^{(1)}, L_{n}^{(2)}\\right\\}\\right)}\n$$\n\nBy the inequality of Hoeffding (cf., e.g., Lemma A. 3 in Gy\u00f6rfi et al. (2002)) and\n\n$$\n\\left|T_{\\beta_{n}}(f(x))\\right| \\leq \\beta_{n} \\quad\\left(x \\in \\mathbb{R}^{d}\\right)\n$$\n\nwe have for any $f \\in \\mathcal{F}$\n\n$$\n\\mathbf{P}\\left\\{\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}}\\left(f\\left(x_{i}\\right)\\right)\\right|>t\\right\\} \\leq 2 \\cdot \\exp \\left(-\\frac{2 \\cdot n \\cdot t^{2}}{4 \\cdot \\beta_{n}^{2}}\\right)\n$$\n\nHence we get\n\n$$\n\\begin{aligned}\n& \\sup _{x_{1}, \\ldots, x_{n} \\in[0,1]^{d_{1} \\times d_{2}}} \\mathbf{E}\\left\\{\\sup _{f \\in \\mathcal{F}}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_{i} \\cdot T_{\\beta_{n}}\\left(f\\left(X_{i}\\right)\\right)\\right|\\right\\} \\\\\n& \\leq \\delta_{n}+\\int_{\\delta_{n}}^{\\beta_{n}} c_{24} \\cdot\\left(\\frac{c_{25} \\cdot \\beta_{n}}{\\delta_{n}}\\right)^{c_{26} \\cdot\\left(\\left(L_{n}^{(1)}\\right)^{2}+L_{n}^{(1)} \\cdot L_{n}^{(2)}\\right) \\cdot \\log \\left(\\max \\left\\{L_{n}^{(1)}, L_{n}^{(2)}\\right\\}\\right)} \\\\\n& \\cdot 2 \\cdot \\exp \\left(-\\frac{n \\cdot \\delta_{n} \\cdot t}{2 \\cdot \\beta_{n}^{2}}\\right) d t \\\\\n& \\leq \\delta_{n}+c_{24} \\cdot\\left(\\frac{c_{25} \\cdot \\beta_{n}}{\\delta_{n}}\\right)^{c_{26} \\cdot\\left(\\left(L_{n}^{(1)}\\right)^{2}+L_{n}^{(1)} \\cdot L_{n}^{(2)}\\right) \\cdot \\log \\left(\\max \\left\\{L_{n}^{(1)}, L_{n}^{(2)}\\right\\}\\right)} \\frac{4 \\cdot \\beta_{n}^{2}}{n \\cdot \\delta_{n}} \\cdot \\exp \\left(-\\frac{n \\cdot \\delta_{n}^{2}}{2 \\cdot \\beta_{n}^{2}}\\right) .\n\\end{aligned}\n$$\n\nWith\n\n$$\n\\delta_{n}=\\sqrt{\\left(\\left(L_{n}^{(1)}\\right)^{2}+L_{n}^{(1)} \\cdot L_{n}^{(2)}\\right) \\cdot \\log \\left(\\max \\left\\{L_{n}^{(1)}, L_{n}^{(2)}\\right\\}\\right)} \\cdot \\log n \\cdot \\sqrt{\\frac{2 \\cdot \\beta_{n}^{2}}{n}}\n$$\n\nwe get the assertion.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# 4.2.5. A bound on the gradient \n\nLemma 11 Let $A, \\bar{\\Theta}$ and $f_{(\\mathbf{w}, \\vartheta)}$ be defined as in Section 3, set\n\n$$\nM_{\\max }=\\max \\left\\{M_{1}, \\ldots, M_{L_{n}^{(1)}}\\right\\} \\quad \\text { and } \\quad k_{\\max }=\\max \\left\\{k_{1}, \\ldots, k_{L_{n}^{(1)}}\\right\\}\n$$\n\nand assume that all weights in $f_{(\\mathbf{w}, \\vartheta)}$ are bounded in absolute value by $B_{n} \\geq 1$. Then\n\n$$\n\\sup _{\\mathbf{w} \\in A, \\vartheta \\in \\bar{\\Theta} K_{n}, y \\in\\{-1,1\\}, x \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\vartheta} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|_{\\infty}\n$$\n\nProof. Let\n\n$$\nf_{(\\mathbf{w}, \\vartheta)}(x)=\\sum_{k=1}^{K_{n}} w_{k} \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(x)\n$$\n\nwhere\n\n$$\n\\begin{gathered}\nf_{\\vartheta_{k}}(x)=g_{\\mathbf{w}_{k}}\\left(f_{\\mathbf{w}_{k}}(x)\\right) \\\\\ng_{\\mathbf{w}_{k}}(z)=\\sum_{i=1}^{L_{n}^{(2)}}\\left(\\mathbf{w}_{k}\\right)_{i}^{(1)} \\sigma\\left(\\left(\\mathbf{w}_{k}\\right)_{i, 1}^{(0)} \\cdot z+\\left(\\mathbf{w}_{k}\\right)_{i, 0}^{(0)}\\right)+\\left(\\mathbf{w}_{k}\\right)_{0}^{(1)} \\\\\nf_{\\mathbf{w}_{k}}(x)=\\max \\left\\{\\sum_{x_{2}=1}^{k_{L_{n}^{(1)}}}\\left(\\mathbf{w}_{k}\\right)_{x_{2}} \\cdot\\left(o_{\\left(\\mathbf{w}_{k}\\right)}\\right)_{\\left(i, j\\right), x_{2}}^{(L_{n}^{(1)})} \\vdots i \\in\\left\\{1, \\ldots, d_{1}-M_{L_{n}^{(1)}}+1\\right\\}, \\\\\n\\left.j \\in\\left\\{1, \\ldots, d_{2}-M_{L_{n}^{(1)}}+1\\right\\}\\right\\}\n\\end{gathered}\n$$\n\n$$\n\\left(o_{\\left(\\mathbf{w}_{k}\\right)}\\right)_{\\left(i, j\\right), s_{2}}^{(l)}=\\sigma\\left(\\sum_{s_{1}=1}^{k_{l-1}} \\sum_{\\substack{t_{1}, t_{2} \\in\\left\\{1, \\ldots, M_{l}\\right\\} \\\\(i+t_{1}-1, j+t_{2}-1) \\in D}}\\left(\\mathbf{w}_{k}\\right)_{t_{1}, t_{2}, s_{1}, s_{2}}^{(l)}\\left(o_{\\left(\\mathbf{w}_{k}\\right)}\\right)_{\\left(i+t_{1}-1, j+t_{2}-1\\right), s_{1}}^{(l-1)}+\\left(\\mathbf{w}_{k}\\right)_{s_{2}}^{(l)}\\right)\n$$\n\nfor $l \\in\\left\\{1, \\ldots, L_{n}^{(1)}\\right\\}$, and\n\n$$\n\\left(o_{\\left(\\mathbf{w}_{k}\\right)}\\right)_{\\left(i, j\\right), 1}^{(0)}=x_{i, j} \\quad \\text { for } i \\in\\left\\{1, \\ldots, d_{1}\\right\\} \\text { and } j \\in\\left\\{1, \\ldots, d_{2}\\right\\}\n$$\n\nBy the proof of Lemma 3 we know for any $l \\in\\left\\{1, \\ldots, L_{n}^{(1)}\\right\\}$\n\n$$\n\\left|\\left(o_{\\left(\\mathbf{w}_{k}\\right)}\\right)_{\\left(i, j\\right), s_{2}}^{(l)}(x)\\right| \\leq k_{\\max }^{L_{n}^{(1)}} \\cdot\\left(M_{\\max }^{2}+1\\right)^{L_{n}^{(1)}} \\cdot B_{n}^{L_{n}^{(1)}}\n$$\n\nUsing the chain rule we get for any $l \\in\\left\\{1, \\ldots, L_{n}^{(1)}\\right\\}$ and suitably chosen (random) $\\left(i_{0}, j_{0}\\right) \\in D$\n\n$$\n\\begin{aligned}\n& \\frac{\\partial}{\\partial\\left(\\mathbf{w}_{k}\\right)_{\\left.t_{1}, t_{2}, \\hat{s}_{1}, \\hat{s}_{2}\\right)}^{(l)}} f_{\\left(\\mathbf{w}, \\vartheta\\right)}(x) \\\\\n& =w_{k} \\cdot \\frac{\\partial}{\\partial z} T_{\\left.\\beta_{n} z\\right|_{z=f_{\\vartheta_{k}}(x)}} \\cdot \\sum_{i=1}^{L_{n}^{(2)}}\\left(\\mathbf{w}_{k}\\right)_{i}^{(1)} \\sigma^{\\prime}\\left(\\left(\\mathbf{w}_{k}\\right)_{i, 1}^{(0)} \\cdot f_{\\mathbf{w}_{k}}(x)+\\left(\\mathbf{w}_{k}\\right)_{i, 0}^{(0)}\\right) \\cdot\\left(\\mathbf{w}_{k}\\right)_{i, 1}^{(0)} \\cdot \\sum_{s_{2}=1}^{k_{L_{n}^{(1)}}^{*}}\\left(\\mathbf{w}_{k}\\right)_{s_{2}} \\\\\n& \\cdot \\sigma^{\\prime}\\left(\\sum_{s_{1}=1}^{k_{L_{n}^{(1)}-1}} \\sum_{\\substack{t_{1}, t_{2} \\in\\left\\{1, \\ldots, M_{L_{n}^{(1)}}\\right\\} \\\\\n\\left(i_{0}+t_{1}-1, j_{0}+t_{2}-1\\right) \\in D}}\\left(\\mathbf{w}_{k}\\right)_{t_{1}, t_{2}, s_{1}, s_{2}}^{(L_{n}^{(1)})}\\left(\\mathbf{w}_{k}\\right)_{t_{1}, t_{2}, s_{1}, s_{2}}^{(L_{n}^{(1)})}\\left(\\left(\\boldsymbol{w}_{k}\\right)_{t_{1}, t_{2}, s_{1}, s_{2}}^{(L_{n}^{(1)})}\\right)\\right. \\\\\n& \\cdot \\sum_{s_{1}^{(L_{n}^{(1)})}=1}^{k_{L_{n}^{(1)}-1}} \\sum_{\\substack{t_{1}^{(L_{n}^{(1)})}, t_{2}^{(L_{n}^{(1)}) \\in\\left\\{1, \\ldots, M_{L_{n}^{(1)}}\\right\\}}^{(L_{n}^{(1)})}}\\left(\\mathbf{w}_{k}\\right)_{t_{1}^{(L_{n}^{(1)})}, t_{2}^{(L_{n}^{(1)})}, s_{1}^{(L_{n}^{(1)})}, s_{2}}^{(L_{n}^{(1)})} \\\\\n& \\cdot \\sigma^{\\prime}\\left(\\sum_{s_{1}=1}^{k_{L_{n}^{(1)}-2}} \\sum_{\\substack{t_{1}, t_{2} \\in\\left\\{1, \\ldots, M_{L_{n}^{(1)}-1}\\right\\}} \\quad\\left(\\mathbf{w}_{k}\\right)_{\\left.t_{1}, t_{2}, s_{1}, s_{1}^{(L_{n}^{(1)}}\\right)}^{(L_{n}^{(1)}-1)}\\right. \\\\\n& \\cdot\\left(i_{0}+t_{1}^{(L_{n}^{(1)})}+t_{1}-2, j_{0}+t_{2}^{(L_{n}^{(1)})}+t_{2}-2\\right) \\in D \\\\\n& \\left.\\left(o_{\\left(\\mathbf{w}_{k}\\right)}\\right)_{\\left(i_{0}+t_{1}^{(L_{n}^{(1)})}\\right.}^{(L_{n}^{(1)}-2)}\\right)_{\\left(i_{0}+t_{1}^{(L_{n}^{(1)})}+t_{1}-2, j_{0}+t_{2}^{(L_{n}^{(1)})}+t_{2}-2\\right) \\in D} \\\\\n& \\cdot \\sum_{s_{1}^{(L_{n}^{(1)}-1)}=1}^{k_{L_{n}^{(1)}-2}}\\left(\\mathbf{w}_{k}\\right)_{\\left.t_{1}^{(L_{n}^{(1)}-1)}\\right)_{\\left.t_{2}^{(L_{n}^{(1)}-1)}\\right) \\in\\left\\{1, \\ldots, M_{L_{n}^{(1)}-1}\\right\\}}^{(L_{n}^{(1)}-1)} \\cdot s_{1}^{(L_{n}^{(1)}-1)} \\cdot s_{1}^{(L_{n}^{(1)})}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& \\cdot \\ldots \\cdot \\sigma\\left(\\sum_{s_{1}=1}^{k_{l-1}} \\sum_{\\substack{t_{1}, t_{2} \\in\\left\\{1, \\ldots, M_{l}\\right\\},\\left(o_{\\left(w_{l}\\right)}\\right)^{\\left(L_{n}^{(1)}\\right)},+\\cdots+t_{l}^{(l+1)}+\\bar{t}_{1}+t_{1}-\\left(L_{n}-(l-2)\\right),\\right.}}\\right. \\\\\n& \\left.\\dot{z}_{0}+t_{2}^{\\left(L_{n}^{(1)}\\right)}+\\cdots+t_{2}^{(l+1)}+\\bar{t}_{2}+t_{2}-\\left(L_{n}^{(1)}-(l-2)\\right)\\right) \\in D \\\\\n& \\left(\\mathbf{w}_{k}\\right)_{t_{1}, t_{2}, s_{1}, \\tilde{s}_{1}}^{(l-1)} \\cdot\\left(o_{\\left(\\mathbf{w}_{k}\\right)}\\right)^{(l-2)} \\underset{\\left(o_{0}+t_{1}^{\\left(L_{n}^{(1)}\\right)}\\right)+\\cdots+t_{1}^{(l+1)}+\\bar{t}_{1}+t_{1}-\\left(L_{n}^{(1)}-(l-2)\\right),}}{\\left.\\left.\\dot{z}_{0}+t_{2}^{\\left(L_{n}^{(1)}\\right)}+\\cdots+t_{2}^{(l+1)}+\\bar{t}_{2}+t_{2}-\\left(L_{n}^{(1)}-(l-2)\\right)\\right), s_{1}}\\right) \\\\\n& \\left.\\dot{z}_{0}+t_{2}^{\\left(L_{n}^{(1)}\\right)}+\\cdots+t_{2}^{(l+1)}+\\bar{t}_{2}+t_{2}-\\left(L_{n}^{(1)}-(l-2)\\right)\\right), s_{1}}\n\\end{aligned}\n$$\n\nUsing (26), $\\left|\\sigma^{\\prime}(z)\\right| \\leq 1$ and that all weights are bounded in the absolute value by $B_{n}$ we get\n\n$$\n\\left|\\frac{\\partial}{\\partial\\left(\\mathbf{w}_{k}\\right)_{t_{1}, t_{2}, s_{1}, s_{2}}^{(l)} f_{(\\mathbf{w}, \\vartheta)}(x)\\right| \\leq L_{n}^{(2)} \\cdot k_{\\max }^{2 \\cdot L_{n}^{(1)}+1} \\cdot\\left(M_{\\max }^{2}+1\\right)^{2 \\cdot L_{n}^{(1)}+2} \\cdot B_{n}^{2 \\cdot L_{n}^{(1)}+2}\n$$\n\nAnalogously we can derive bounds on all the other partial derivatives occurring in the assertion.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 18,
      "text": "# 4.2.6. Proof of Theorem 2 \n\nIt suffices to show\n\n$$\n\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right)\\right\\}-\\min _{f:[0,1]^{d_{1}=a_{2}} \\rightarrow \\mathbb{R}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\leq c_{28} \\cdot(\\log n)^{4} \\cdot n^{-\\min \\left\\{\\frac{p}{2 p+4}, \\frac{1}{4}\\right\\}}\n$$\n\nfor $n$ sufficiently large.\nThis implies the assertion, because by Lemma 2 a) we conclude from (27)\n\n$$\n\\begin{aligned}\n& \\mathbf{P}\\left\\{Y \\neq \\hat{C}_{n}(X)\\right\\}-\\mathbf{P}\\left\\{Y \\neq f^{*}(X)\\right\\} \\\\\n& \\leq \\mathbf{E}\\left\\{\\frac{1}{\\sqrt{2}} \\cdot\\left(\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right) \\mid \\mathcal{D}_{n}\\right\\}-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\varphi^{*}}(X)\\right)\\right\\}\\right)^{1 / 2}\\right\\} \\\\\n& \\leq \\frac{1}{\\sqrt{2}} \\cdot \\sqrt{\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right)\\right\\}-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\varphi^{*}}(X)\\right)\\right\\}} \\\\\n& \\leq c_{8} \\cdot(\\log n)^{2} \\cdot n^{-\\min \\left\\{\\frac{p}{2 \\cdot(2 p+4)}, \\frac{1}{8}\\right\\}}\n\\end{aligned}\n$$\n\nAnd from Lemma 2 b), (18) and Lemma 2 c) we conclude from (27)\n\n$$\n\\begin{aligned}\n& \\mathbf{P}\\left\\{Y \\neq \\hat{C}_{n}(X)\\right\\}-\\mathbf{P}\\left\\{Y \\neq f^{*}(X)\\right\\} \\\\\n& \\leq 2 \\cdot\\left(\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right)\\right\\}-\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\varphi^{*}}(X)\\right)\\right\\}\\right)+4 \\cdot \\frac{c_{29} \\cdot \\log n}{n^{1 / 4}} \\\\\n& \\leq c_{9} \\cdot(\\log n)^{4} \\cdot n^{-\\min \\left\\{\\frac{p}{2 p+4}, \\frac{1}{4}\\right\\}}\n\\end{aligned}\n$$\n\nHere we have used the fact that\n\n$$\n\\max \\left\\{\\frac{\\mathbf{P}\\{Y=1 \\mid X\\}}{1-\\mathbf{P}\\{Y=1 \\mid X\\}}, \\frac{1-\\mathbf{P}\\{Y=1 \\mid X\\}}{\\mathbf{P}\\{Y=1 \\mid X\\}}\\right\\}>n^{1 / 4}\n$$\n\nis equivalent to\n\n$$\n\\left|f_{\\varphi^{*}}(X)\\right|=\\left|\\log \\frac{\\mathbf{P}\\{Y=1 \\mid X\\}}{1-\\mathbf{P}\\{Y=1 \\mid X\\}}\\right|>\\frac{1}{4} \\cdot \\log n\n$$\n\nIn the remainder of the proof we apply Theorem 1 in order to prove (27). Here we assume throughout the proof that $n$ is sufficiently large. Let $f_{\\vartheta}$ be the network of Lemma 8 which satisfies\n\n$$\n\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{\\vartheta}(X)\\right)\\right\\}-\\min _{f:[0,1]^{d_{1} \\times d_{2} \\rightarrow \\mathbb{R}}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\leq c_{30} \\cdot\\left(\\frac{\\log L_{n}^{(2)}}{L_{n}^{(2)}}+\\frac{\\log L_{n}^{(2)}}{\\left(L_{n}^{(1)}\\right)^{2 p / 4}}\\right)\n$$\n\nSince $f_{\\vartheta}$ is bounded in supremum norm by $\\log L_{n}^{(2)} \\leq \\beta_{n}$ (for $c_{3}$ sufficiently large) this implies\n\n$$\n\\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot T_{\\beta_{n}} f_{\\vartheta}(X)\\right)\\right\\}-\\min _{f:[0,1]^{d_{1} \\times d_{2} \\rightarrow \\mathbb{R}}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\leq c_{30} \\cdot\\left(\\frac{\\log L_{n}^{(2)}}{L_{n}^{(2)}}+\\frac{\\log L_{n}^{(2)}}{\\left(L_{n}^{(1)}\\right)^{2 p / 4}}\\right)\n$$\n\nHere we have used that we can decrease the value of $L_{n}^{(2)}$ in order to ensure that the assumption $\\left(L_{n}^{(1)}\\right)^{2 p / 4} \\geq c_{17} \\cdot L_{n}^{(2)}$ (cf., Lemma 8) holds.\nSet\n\n$$\n\\delta_{n}=\\frac{1}{n \\cdot e^{n}}\n$$\n\nand choose\n\n$$\n\\Theta^{*}=\\left\\{\\vartheta^{*} \\in \\Theta:\\left\\|\\vartheta^{*}-\\vartheta\\right\\|_{\\infty} \\leq \\delta_{n}\\right\\}\n$$\n\nThen it follows from the Lipschitz continuity of the logistic loss and Lemma 3 that we have for any $\\vartheta^{*} \\in \\Theta^{*}$\n\n$$\n\\left|\\varphi\\left(Y \\cdot T_{\\beta_{n}} f_{\\vartheta^{*}}(X)\\right)-\\varphi\\left(Y \\cdot T_{\\beta_{n}} f_{\\vartheta}(X)\\right)\\right| \\leq c_{31} \\cdot e^{n} \\cdot\\left\\|\\vartheta^{*}-\\vartheta\\right\\|_{\\infty} \\leq \\frac{c_{32}}{n}\n$$\n\nfrom which we can conclude\n\n$$\n\\begin{aligned}\n& \\sup _{\\vartheta^{*} \\in \\Theta^{*}} \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot T_{\\beta_{n}} f_{\\vartheta^{*}}(X)\\right)\\right\\}-\\min _{f:[0,1]^{d_{1} \\times d_{2} \\rightarrow \\mathbb{R}}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\\\\n& \\leq c_{30} \\cdot\\left(\\frac{\\log L_{n}^{(2)}}{L_{n}^{(2)}}+\\frac{\\log L_{n}^{(2)}}{\\left(L_{n}^{(1)}\\right)^{2 p / 4}}\\right)+\\frac{c_{32}}{n}\n\\end{aligned}\n$$\n\nFurthermore the definition of $\\Theta^{*}$ implies that\n\n$$\n\\kappa_{n}=\\mathbf{P}\\left\\{\\vartheta_{1}^{(0)} \\in \\boldsymbol{\\Theta}^{*}\\right\\} \\geq\\left(\\frac{1}{n \\cdot e^{n}}\\right)^{c_{33} \\cdot\\left(L_{n}^{(1)}+L_{n}^{(2)}\\right)} \\geq e^{-n^{1.5}}\n$$\n\nSo if we choose\n\n$$\nN_{n}=n^{2} \\cdot e^{2 \\cdot n}, \\quad I_{n}=n^{2} \\cdot e^{n^{1.5}}\n$$\n\n(which is possible because of $N_{n} \\cdot I_{n} \\leq K_{n}$ ) then we have\n\n$$\nN_{n} \\cdot\\left(1-\\kappa_{n}\\right)^{I_{n}} \\leq \\frac{1}{n}\n$$\n\nso (10) holds.\nBy Lemma 3 we know that (8) is satisfied for\n\n$$\nC_{n}=c_{34} \\cdot e^{n}\n$$\n\nand because of\n\n$$\n\\left\\|\\nabla_{\\mathbf{w}} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|^{2} \\leq \\sum_{k=1}^{K_{n}}\\left|1 \\cdot T_{\\beta_{n}} f_{\\vartheta_{k}}(x)\\right|^{2} \\leq K_{n} \\cdot \\beta_{n}^{2}\n$$\n\n(11) is satisfied for\n\n$$\nD_{n}=\\sqrt{K}_{n} \\cdot \\beta_{n}\n$$\n\nBy Lemma 11 we know\n\n$$\n\\sup _{\\mathbf{w} \\in A, \\vartheta \\in \\Theta^{K_{n}}, y \\in\\{-1,1\\}, x \\in[0,1]^{d_{1} \\times d_{2}}}\\left\\|\\nabla_{\\vartheta} \\varphi\\left(y \\cdot f_{(\\mathbf{w}, \\vartheta)}(x)\\right)\\right\\|_{\\infty} \\leq e^{n}\n$$\n\nApplication of Theorem 1 together with Lemma 9 and the above results yields\n\n$$\n\\begin{aligned}\n& \\mathbf{E}\\left\\{\\varphi\\left(Y \\cdot f_{n}(X)\\right)\\right\\}-\\min _{f: \\mathbb{R}^{d_{1} \\times d_{2}} \\rightarrow \\mathbb{R}^{2}} \\mathbf{E}\\{\\varphi(Y \\cdot f(X))\\} \\\\\n& \\leq c_{35} \\cdot\\left(\\frac{\\log n}{n}+(\\log n)^{3} \\frac{\\sqrt{\\left(n^{\\frac{4}{2 p+4}}+n^{\\frac{2}{2 p+4}} \\cdot n^{\\frac{4}{4}}\\right) \\cdot \\log n}}{\\sqrt{n}}+\\frac{e^{n}}{n \\cdot e^{n}}+\\frac{K_{n} \\cdot \\beta_{n}^{2}}{t_{n}}\\right. \\\\\n& \\left.+\\frac{n \\cdot \\beta_{n}^{2} \\cdot\\left(K_{n}+e^{n} \\cdot e^{n}\\right)}{t_{n}}+\\frac{\\log L_{n}^{(2)}}{L_{n}^{(2)}}+\\frac{\\log L_{n}^{(2)}}{\\left(L_{n}^{(1)}\\right)^{2 p / 4}}\\right) \\\\\n& \\leq c_{28} \\cdot(\\log n)^{4} \\cdot n^{-\\min \\left\\{\\frac{e}{2 p+4}, \\frac{1}{4}\\right\\}}\n\\end{aligned}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 19,
      "text": "# 5. Acknowledgment \n\nThe second author would like to thank Natural Sciences and Engineering Research Council of Canada for funding this project under Grant RGPIN-2020-06793.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "## References\n\n[1] Allen-Zhu, Z., Li, Y., and Song, Z. (2019). A convergence theory for deep learning via over-parameterization. Proceedings of the 36th International Conference on Machine Learning (PMLR 2019), 97, pp. 242-252. Long Beach, California.\n[2] Andoni, A., Panigraphy, R., Valiant, G., and Zhang, L. (2014). Learning polynomials with neural networks. In International Conference on Machine Learning, pp. 19081916.\n\n[3] Barron, A. R. (1994). Approximation and estimation bounds for artificial neural networks. Machine Learning 14, pp. 115-133.\n[4] Bartlett, P., Harvey, N., Liaw, C., and Mehrabian, A. (2019). Nearly-tight VCdimension bounds for piecewise linear neural networks. Journal of Machine Learning Research 20, pp. 1-17.\n[5] Bartlett, P. L., Montanari, A., and Rakhlin, A. (2021). Deep learning: a statistical viewpoint. arXiv: 2103.09177v1.\n[6] Bauer, B., and Kohler, M. (2019). On deep learning as a remedy for the curse of dimensionality in nonparametric regression. Annals of Statistics 4, pp. 2261-2285.\n[7] Birman, M. S., and Solomjak, M. Z. (1967). Piece-wise polynomial approximations of functions in the classes $W_{p}^{\\alpha}$. Mathematics of the USSR Sbornik 73, pp. 295-317.\n[8] Bottou, L. (2012). Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade, Second Edition, Springer, pp. 421-436.\n[9] Bottou, L., Curtis, F. E., and Nocedal, J. (2018). Optimization methods for largescale machine learning. SIAM Review, 60, pp. 223-311.\n[10] Braun, A., Kohler, M., Langer, S., and Walk, H. (2024). Convergence rates for shallow neural networks learned by gradient descent. Bernoulli, 30, pp. 475-502.\n[11] Chen, X., Lee, J.D., Tong, X.T. and Zhang Y. (2020). Statistical Inference For Model Parameters In Stochastic Gradient Descent. Annals of Statistics, 48, pp. 251-273.\n[12] Chizat, L. and Bach, F. (2018). On the global convergence of gradient descent for over-parameterized models using optimal transport. Preprint, arXiv: 1805.09545.\n[13] Cover, T. M. (1968). Rates of convergence of nearest neighbor procedures. In Proceedings of the Hawaii International Conference on Systems Sciences, pp. 413-415, Honolulu, HI.\n[14] Daniely, A. (2017). SGD learns the conjugate kernel class of the network. In Advances in Neural Information Processing Systems, pp. 2422-2430.\n[15] Devroye, L., Gy\u00f6rfi, L., and Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition. Springed, New York, USA.\n[16] Devroye, L., and Wagner, T. J. (1980). Distribution-free consistency results in nonparametric discrimination and regression function estimation. Annals of Statistics, 8, pp. 231-239.\n[17] Drews, S. and Kohler, M. (2022). On the universal consistency of an over-parametrized deep neural network estimate learned by gradient descent. arXiv:2208.14283.\n\n[18] Drews, S., and Kohler, M. (2023). Analysis of the expected $L_{2}$ error of an overparametrized deep neural network estimate learned by gradient descent without regularization. Preprint.\n[19] Du, S., Lee, J., Li, H., Wang, L., und Zhai, X. (2019). Gradient descent finds global minima of deep neural networks. International Conference on Machine Learning, Preprint, arXiv: 1811.03804.\n[20] Golowich, N., Rakhlin, A., and Shamir, O. (2019). Size-Independent sample complexity of neural networks. Preprint, arXiv: 1712.06541.\n[21] Gonon, L. (2021). Random feature networks learn Black-Scholes type PDEs without curse of dimensionality. Preprint, arXiv: 2106.08900.\n[22] Gurevych, I., Kohler, M., and Sahin, G. G. (2022). On the rate of convergence of a classifier based on a Transformer encoder. IEEE Transactions on Information Theory, 68, pp. 8139-8155.\n[23] Gy\u00f6rfi, L., Kohler, M., Krzy\u017cak, A., and Walk, H. (2002). A Distribution-Free Theory of Nonparametric Regression. Springer.\n[24] Hanin, B. and Nica, M. (2019). Finite depth and width corrections to the neural tangent kernel. Preprint, arXiv: 1909.05989.\n[25] Huang, G. B., Chen, L., and Siew, C.-K. (2006). Universal approximation using incremental constructive feedforward networks with random hidden nodes. IEEE Transactions on Neural Networks 17, pp. 879-892.\n[26] Imaizumi, M., and Fukamizu, K. (2018). Deep neural networks learn non-smooth functions effectively. Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS 2019), Naha, Okinawa, Japan.\n[27] Jacot, A., Gabriel, F., und Hongler, C. (2020). Neural tangent kernel: convergence and generalization in neural networks. arXiv: 1806.07572v4.\n[28] Kawaguchi, K, and Huang, J. (2019). Gradient descent finds global minima for generalizable deep neural networks of practical sizes. arXiv: 1908.02419v1.\n[29] Kim, Y., Ohn, I. and Kim, D. (2021). Fast convergence rates of deep neural networks for classification. Neural Networks, 138, pp.179-197.\n[30] Kohler, M. (2014). Optimal global rates of convergence for noiseless regression estimation problems with adaptively chosen design. Journal of Multivariate Analysis, 132, pp. 197-208.\n[31] Kohler, M., and Krzy\u017cak, A. (2017). Nonparametric regression based on hierarchical interaction models. IEEE Transaction on Information Theory 63, pp. 1620-1630.\n\n[32] Kohler, M., and Krzy\u017cak, A. (2021). Over-parametrized deep neural networks minimizing the empirical risk do not generalize well. Bernoulli, 27, pp. 2564-2597.\n[33] Kohler, M., and Krzy\u017cak, A. (2022). Analysis of the rate of convergence of an overparametrized deep neural network estimate learned by gradient descent. Preprint, arXiv: 2210.01443.\n[34] Kohler, M., and Krzy\u017cak, A. (2023). On the rate of convergence of an overparametrized transformer classifier learned by gradient descent. Preprint, arXiv: 2312.17007.\n[35] Kohler, M., Krzy\u017cak, A., and Walter, B. (2022). On the rate of convergence of image classifiers based on convolutional neural networks. Annals of the Institute of Statistical Mathematics, 74, pp. 1085-1108.\n[36] Kohler, M., Krzy\u017cak, A., and Walter, B. (2023). Analysis of the rate of convergence of an over-parametrized convolutional neural network image classifier learned by gradient descent. Journal of Statistical Planning and Inference (to appear).\n[37] Kohler, M., and Langer, S. (2025). Statistical theory for image classification using deep convolutional neural network with cross-entropy loss under the hierarchical maxpooling model. Journal of Statistical Planning and Inference, 234. p. 106188\n[38] Kohler, M., and Langer, S. (2021). On the rate of convergence of fully connected deep neural network regression estimates using ReLU activation functions. Annals of Statistics 49, pp. 2231-2249. Preprint, arXiv: 1908.11133.\n[39] Kohler, M., and Walter, B. (2023). Analysis of convolutional neural network image classifiers in a rotationally symmetric model. IEEE Transaction on Information Theory 69, pp. 5203-5218.\n[40] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In F. Pereira et al. (Eds.), Advances In Neural Information Processing Systems 25, pp. 1097-1105. Red Hook, NY: Curran.\n[41] Kushner, J.H. and Yin, G.G. (2003). Stochastic Approximation and Recursive Algorithms and Applications, Springer.\n[42] Langer, S. (2021). Approximating smooth functions by deep neural networks with sigmoid activation function. Journal of Multivariate Analysis 182, pp. 104696.\n[43] LeCun, Y., Bengio, Y. and Hinton, G. (2015). Deep learning. Nature, 521, pp.436444 .\n[44] Liang, T., Rakhlin, A., and Sridharan, K. (2015). Learning with square loss: localization through offset Rademacher complexity. Preprint, arXiv: 1502.06134.\n[45] Li, G., Gu, Y. and Ding, J. (2021). The rate of convergence of variation-constrained deep neural networks. arXiv: 2106.12068\n\n[46] Lin, S., and Zhang, J. (2019). Generalization bounds for convolutional neural networks. Preprint, arXiv: 1910.01487.\n[47] Lu, J., Shen, Z., Yang, H., and Zhang, S. (2020). Deep network approximation for smooth functions. Preprint, arXiv: 2001.03040\n[48] Mei, S., Montanari, A. and Nguyen, P.-M. (2018). A mean field view of the landscape of two-layer neural networks. In Proceedings of the National Academy of Sciences, 115, pp. E7665-E7671.\n[49] Nemirovski, A., Juditsky, A., Lan, G. and Shapiro, A. (2008). Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19, pp. 1574-1609.\n[50] Nguyen, P.-M. and Pham, H. T. (2020). A rigorous framework for the mean field limit of multilayer neural networks Preprint, arXiv: 2001.1144.\n[51] Polyak, B. T. and Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging. SIAM Journal of Control and Optimization, 30, pp. 838-855.\n[52] Rahimi, A., and Recht, B. (2008a). Random features for large-scale kernel machines. In Advances in Neural Information Procesing Systems, pp. 1177-1184.\n[53] Rahimi, A., and Recht, B. (2008b). Uniform approximation of function with random bases. In 2008 46th Annual Allerton Conference on Communication, Control, and Computing, pp. 555-561, IEEE.\n[54] Rahimi, A., and Recht, B. (2009). Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In D. Koller, D. Schuurman, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems, Curran Associates, Inc. 21, pp. 1313-1320.\n[55] Rawat, W., and Wang, Z. (2017). Deep convolutional neural networks for image classification: a comprehensive review. Neural Computation, 29, pp. 2352-2449.\n[56] Robbins, H., and Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical Statistics, pp. 400-407.\n[57] Schmidt-Hieber, J. (2020). Nonparametric regression using deep neural networks with ReLU activation function (with discussion). Annals of Statistics 48, pp. 18751897. Preprint, arXiv:1708.06633v2.\n[58] Spall, J.C. (2003). Introduction To Stochastic Search And Optimization: Estimation, Simulation and Control John Wiley \\& Sons.\n[59] Sun, S., Cao, Z., Zhu, H., and Zhao, J. (2019). A survey of optimization methods from a machine learning perspective. IEEE Transactions on Cybernetics, 50, pp. $3668-3681$.\n\n[60] Suzuki, T. (2018). Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality. Preprint, arXiv: 1810.08033.\n[61] Suzuki, T., and Nitanda, A. (2019). Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space. Preprint, arXiv: 1910.12799.\n[62] Toulis, P. and Airoldi, E.M. (2017). Asymptotic And Finite-Sample Properties Of Estimators Based On Stochastic Gradients. Annals of Statistics, 45, pp. 1694-1727.\n[63] Walter, B. (2021). Analysis of convolutional neural network image classifiers in a hierarchical max-pooling model with additional local pooling. arXiv: 2106.05233\n[64] Wang, M., and Ma, C. (2022). Generalization error bounds for deep neural network trained by SGD. arXiv: 2206.03299 v 1.\n[65] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin,I. (2017). Attention is all you need. Arxiv 1706.03762.\n[66] Yarotsky, D. (2018). Optimal approximation of continuous functions by very deep ReLU networks. Preprint, arXiv: 1802.03620\n[67] Yarotsky, D., and Zhevnerchuk, A. (2019). The phase diagram of approximation rates for deep neural networks. Preprint, arXiv: 1906.09477.\n[68] Yehudai, G., and Shamir, O. (2022). On the power and limitations of random features for understanding neural networks. Preprint, arXiv: 1904.00687\n[69] Zhang, T. (2004). Statistical behavior and consistency of classification methods based on convex risk minimization. Annals of Statistics, 32, pp. 56 - 134.\n[70] Zou, D., Cao, Y., Zhou, D., und Gu, Q. (2018). Stochastic gradient descent optimizes over-parameterized deep ReLU networks. Preprint, arXiv: 1811.08888.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "# A. Supplement: Proof of Lemma 5 \n\nIn the following we prove the weight constraints of Lemma 5 by modifying the auxiliary results of the proof of Kohler and Langer (2021).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 22,
      "text": "## A.1. Further notation and definitions\n\nThe following auxiliary notation is required for the statement of these resuts:\nWe introduce our framework for a fully-connected neural network $g: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ with ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$ : Let\n\n$$\ng(x)=\\sum_{i=1}^{k_{L}} v_{1, i}^{(L)} g_{i}^{(L)}(x)+v_{1,0}^{(L)}\n$$\n\nwhere output weights $v_{1,0}^{(L)}, \\ldots, v_{1, k_{L}}^{(L)} \\in \\mathbb{R}$ denote the output weights of the network.\nThe outputs of the neurons $g_{i}^{(L)}$ are recursively defined by\n\n$$\ng_{i}^{(r)}(x)=\\sigma\\left(\\sum_{j=1}^{k_{r-1}} v_{i, j}^{(r-1)} g_{j}^{(r-1)}(x)+v_{i, 0}^{(r-1)}\\right)\n$$\n\nwith inner weights $v_{i, 0}^{(r-1)}, \\ldots, v_{i, k_{r-1}}^{(r-1)} \\in \\mathbb{R}$ for $i \\in\\left\\{1, \\ldots, k_{r}\\right\\}, r \\in\\{1, \\ldots, L\\}, k_{0}=d$ and\n\n$$\ng_{j}^{(0)}(x)=x^{(j)}\n$$\n\nA fully-connected neural network of the form (28) is dependent on the number of layers $L$ and a width vector $\\mathbf{k}=\\left(k_{1}, \\ldots, k_{L}\\right)$, hence we will denote the corresponding function class by $\\mathcal{F}_{L, \\mathbf{k}}^{F N N}$. In case $k_{1}=\\ldots=k_{L}=r$, we write $\\mathcal{F}_{L, r}^{F N N}$ to indicate that all layers consist of a constant number of $r$ neurons. Further, we denote by $\\mathbf{v}_{f}$ the vector that collects all weights required for the computation of $f \\in \\mathcal{F}_{L, \\mathbf{k}}^{F N N}$ :\n\n$$\n\\mathbf{v}_{f}=\\left(\\binom{v_{i, j}^{(l)}}{i \\in\\left\\{1, \\ldots, k_{l+1}\\right\\}, j \\in\\left\\{0, \\ldots, k_{l}\\right\\}, l \\in\\{0, \\ldots, L-1\\}}, \\left\\langle v_{1, i}^{L}\\right\\rangle_{i \\in\\left\\{1, \\ldots, k_{L}\\right\\}}\\right)\n$$\n\nThe proof is based on an approximation by a piecewise Taylor polynomial, which is defined using a partition into equivolume cubes. If $C$ is a cube, then $C_{\\text {left }}$ is used to denote the \"bottom left\" of $C$. We can thus write each half-open cube $C$ with side length $s$ as a polytope defined by\n\n$$\n-x^{(j)}+C_{l e f t}^{(j)} \\leq 0 \\text { and } x^{(j)}-C_{l e f t}^{(j)}-s<0 \\quad(j \\in\\{1, \\ldots, d\\})\n$$\n\nFurthermore, we describe by $C_{\\delta}^{0} \\subset C$ the cube, which contains all $x \\in C$ that lie with a distance of at least $\\delta$ to the boundaries of $C$, i.e. $C_{\\delta}^{0}$ is the polytope defined by\n\n$$\n-x^{(j)}+C_{l e f t}^{(j)} \\leq-\\delta \\text { and } x^{(j)}-C_{l e f t}^{(j)}-s<-\\delta \\quad(j \\in\\{1, \\ldots, d\\})\n$$\n\nIf $\\mathcal{P}$ is a partition of cubes of $[-a, a)^{d}$ and $x \\in[-a, a)^{d}$, then we denote the cube $C \\in \\mathcal{P}$, which satisfies $x \\in C$, by $C_{\\mathcal{P}}(x)$.\n\nLet $\\mathcal{P}_{N}$ be the linear span of all monomials of the form\n\n$$\n\\prod_{k=1}^{d}\\left(x^{(k)}\\right)^{r_{k}}\n$$\n\nfor some $r_{1}, \\ldots, r_{d} \\in \\mathbb{N}_{0}, r_{1}+\\cdots+r_{d} \\leq N$. Then, $\\mathcal{P}_{N}$ is a linear vector space of functions of dimension\n\n$$\n\\operatorname{dim} \\mathcal{P}_{N}=\\left|\\left\\{\\left(r_{0}, \\ldots, r_{d}\\right) \\in \\mathbb{N}_{0}^{d+1}: r_{0}+\\cdots+r_{d}=N\\right\\}\\right|=\\binom{d+N}{d}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 23,
      "text": "# A.2. Auxiliary results \n\nLemma 12 Let $p=q+s$ for some $q \\in \\mathbb{N}_{0}$ and $s \\in(0,1]$, and let $C>0$. Let $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ be a $(p, C)$-smooth function, let $x_{0} \\in \\mathbb{R}^{d}$ and let $T_{f, q, x_{0}}$ be the Taylor polynomial of total degree $q$ around $x_{0}$ defined by\n\n$$\nT_{f, q, x_{0}}(x)=\\sum_{j \\in \\mathbb{N}_{0}^{d} \\backslash[[]]_{1} \\leq q}\\left(\\partial^{\\mathrm{j}} f\\right)\\left(x_{0}\\right) \\cdot \\frac{\\left(x-x_{0}\\right)^{\\mathrm{j}}}{\\mathrm{j}!}\n$$\n\nThen for any $x \\in \\mathbb{R}^{d}$\n\n$$\n\\left|f(x)-T_{f, q, x_{0}}(x)\\right| \\leq c_{35} \\cdot C \\cdot\\left\\|x-x_{0}\\right\\|^{p}\n$$\n\nholds for a constant $c_{35}=c_{35}(q, d)$ depending only on $q$ and $d$.\nProof. See Lemma 1 in Kohler (2014).\nIn the proof of Lemma 5 we use Lemma 12 and approximate our function by a piecewise Taylor polynomial. To define this piecewise Taylor polynomial, we partition $[-a, a)^{d}$ into $M^{d}$ and $M^{2 d}$ half-open equivolume cubes of the form\n\n$$\n[\\boldsymbol{\\alpha}, \\boldsymbol{\\beta})=\\left[\\boldsymbol{\\alpha}^{(1)}, \\boldsymbol{\\beta}^{(1)}\\right) \\times \\cdots \\times\\left[\\boldsymbol{\\alpha}^{(d)}, \\boldsymbol{\\beta}^{(d)}\\right), \\quad \\boldsymbol{\\alpha}, \\boldsymbol{\\beta} \\in \\mathbb{R}^{d}\n$$\n\nrespectively. Let\n\n$$\n\\mathcal{P}_{1}=\\left\\{C_{k, 1}\\right\\}_{k \\in\\left\\{1, \\ldots, M^{d}\\right\\}} \\text { and } \\mathcal{P}_{2}=\\left\\{C_{j, 2}\\right\\}_{j \\in\\left\\{1, \\ldots, M^{2 d}\\right\\}}\n$$\n\nbe the corresponding partitions. We denote for each $i \\in\\left\\{1, \\ldots, M^{d}\\right\\}$ those cubes of $\\mathcal{P}_{2}$ that are contained in $C_{i, 1}$ by $\\tilde{C}_{1, i, \\ldots}, \\tilde{C}_{M^{d}, i}$ and order the cubes in such a way that the bottom left of $\\tilde{C}_{1, i}$ and $C_{i, 1}$ coincide, i.e. such that we have $\\left(\\tilde{C}_{1, i}\\right)_{\\text {left }}=\\left(C_{i, 1}\\right)_{\\text {left }}$ and that\n\n$$\n\\left(\\tilde{C}_{k, i}\\right)_{\\text {left }}=\\left(\\tilde{C}_{k-1, i}\\right)_{\\text {left }}+\\tilde{\\mathbf{v}}_{k}\n$$\n\nholds for all $k \\in\\left\\{2, \\ldots, M^{d}\\right\\}, i \\in\\left\\{1, \\ldots, M^{d}\\right\\}$ and some vector $\\tilde{\\mathbf{v}}_{k}$ with entries in $\\left\\{0,2 a / M^{2}\\right\\}$ where exactly one entry is different to zero. Here the vector $\\tilde{\\mathbf{v}}_{k}$ describes the position of $\\left(C_{k, i}\\right)_{l e f t}$ relative to $\\left(C_{k-1, i}\\right)_{l e f t}$ and we order the cubes in such a way that the position is independent of $i$. Then Taylor expansion in Lemma 12 can be used to define a piecewise Taylor polynomial on $\\mathcal{P}_{2}$ by\n\n$$\nT_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}}(x)=\\sum_{k \\in\\left\\{1, \\ldots, M^{d}\\right\\}, i \\in\\left\\{1, \\ldots, M^{d}\\right\\}} T_{f, q,\\left(\\tilde{C}_{k, i}\\right)_{l e f t}}(x) \\cdot \\mathbb{1}_{\\tilde{C}_{k, i}}(x)\n$$\n\nand this piecewise Taylor polynomial satisfies\n\n$$\n\\sup _{x \\in[-a, a)^{d}}\\left|f(x)-T_{f, q,\\left(\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}(x)\\right|} \\leq c_{35} \\cdot C \\cdot(2 \\cdot a \\cdot d)^{p} \\cdot \\frac{1}{M^{2 p}}\\right.\n$$\n\nTo compute $T_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}}(x)$ the very deep neural network of Lemma 5 b) proceeds in two steps: In a first step it computes $\\left(C_{\\mathcal{P}_{1}}(x)\\right)_{l e f t}$ and the values of\n\n$$\n\\left(\\partial^{\\mathbf{l}} f\\right)\\left(\\left(C_{i, 1}\\right)_{l e f t}\\right)\n$$\n\nfor each $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{l}\\|_{1} \\leq q$ and suitably defined numbers\n\n$$\nb_{k, i}^{(\\mathbf{l})} \\in \\mathbb{Z}, \\quad\\left|b_{k, i}^{(\\mathbf{l})}\\right| \\leq e^{d}+1 \\quad\\left(k \\in\\left\\{1, \\ldots, M^{d}\\right\\}\\right)\n$$\n\nwhich depend on $C_{i, 1}$ for $i \\in\\left\\{1, \\ldots, M^{d}\\right\\}$. Assume that $x \\in C_{i, 1}$ for some $i \\in\\left\\{1, \\ldots, M^{d}\\right\\}$. In the second step the neural network successively computes approximations\n\n$$\n\\left(\\partial^{\\mathbf{l}} \\hat{f}\\right)\\left(\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right), \\quad k \\in\\left\\{1, \\ldots, M^{d}\\right\\}\n$$\n\nof\n\n$$\n\\left(\\partial^{\\mathbf{l}} f\\right)\\left(\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)\n$$\n\nfor each $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{l}\\|_{1} \\leq q$. To do this we start with\n\n$$\n\\left(\\partial^{\\mathbf{l}} \\hat{f}\\right)\\left(\\left(\\tilde{C}_{1, i}\\right)_{l e f t}\\right)=\\left(\\partial^{\\mathbf{l}} f\\right)\\left(\\left(C_{\\mathcal{P}_{1}}(x)\\right)_{l e f t}\\right)\n$$\n\nBy construction of the first step and since $\\left(\\tilde{C}_{1, i}\\right)_{l e f t}=\\left(C_{\\mathcal{P}_{1}}(x)\\right)_{l e f t}$ these estimates have error zero. As soon as we have computed the above estimates for some $k \\in\\left\\{1, \\ldots, M^{d}-1\\right\\}$ we use the Taylor polynomials with these coefficients around $\\left(\\tilde{C}_{k, i}\\right)_{l e f t}$ in order to compute\n\n$$\n\\sum_{\\substack{\\mathbf{j} \\in \\mathbb{N}_{0}^{d} ; \\\\\\|\\mathbf{j}\\|_{1} \\leq q-\\|\\mathbf{l}\\|_{1}}} \\frac{\\left(\\partial^{\\mathbf{l}+\\mathbf{l}} \\hat{f}\\right)\\left(\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)}{\\mathbf{j}!} \\cdot\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}-\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)^{\\mathbf{j}}\n$$\n\nfor $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{l}\\|_{1} \\leq q$ and we define\n\n$$\n\\left(\\partial^{\\mathbf{l}} \\hat{f}\\right)\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}\\right)=\\sum_{\\substack{\\mathbf{j} \\in \\mathbb{N}_{0}^{d} ; \\\\\\|\\mathbf{j}\\|_{1} \\leq q-\\|\\mathbf{l}\\|_{1}}} \\frac{\\left(\\partial^{\\mathbf{l}+\\mathbf{l}} \\hat{f}\\right)\\left(\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)}{\\mathbf{j}!} \\cdot\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}-\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)^{\\mathbf{j}}\n$$\n\n$$\n+b_{k, i}^{(\\mathbf{I})} \\cdot c_{36} \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{p-[\\mathbf{I}]_{1}}\n$$\n\nwhere\n\n$$\nc_{36}=C \\cdot d^{p} \\cdot \\max \\left\\{c_{35}(q, d), c_{35}(q-1, d), \\ldots, c_{35}(0, d)\\right\\}\n$$\n\n(and $c_{35}$ is the constant of Lemma 12). Assume that\n\n$$\n\\left|\\left(\\partial^{\\mathbf{l}} \\hat{f}\\right)\\left(\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)-\\left(\\partial^{\\mathbf{l}} f\\right)\\left(\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)\\right| \\leq c_{36} \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{p-[\\mathbf{I}]_{1}}\n$$\n\nholds for all $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{l}\\|_{1} \\leq q$ (which holds by construction for $k=1$ ). Then\n\n$$\n\\begin{aligned}\n& \\left|\\sum_{\\substack{\\mathbf{j} \\in \\mathbb{N}_{0}^{d}: \\\\\n\\|\\mathbf{j}\\|_{1} \\leq q-\\|\\mathbf{I}\\|_{1}}} \\frac{\\left(\\partial^{\\mathbf{l}+\\mathbf{j}} \\hat{f}\\right)\\left(\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)}{\\mathbf{j}!} \\cdot\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}-\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)^{\\mathbf{j}}\\right. \\\\\n& \\left.-\\left(\\partial^{\\mathbf{l}} f\\right)\\left(\\left(C_{k+1, i}\\right)_{l e f t}\\right)\\right| \\\\\n& \\leq\\left|\\sum_{\\substack{\\mathbf{j} \\in \\mathbb{N}_{0}^{d}: \\\\\n\\|\\mathbf{j}\\|_{1} \\leq q-\\|\\mathbf{I}\\|_{1}}} \\frac{\\left(\\partial^{\\mathbf{l}+\\mathbf{j}} \\hat{f}\\right)\\left(\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)}{\\mathbf{j}!} \\cdot\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}-\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)^{\\mathbf{j}}\\right| \\\\\n& -\\sum_{\\substack{\\mathbf{j} \\in \\mathbb{N}_{0}^{d}: \\\\\n\\|\\mathbf{j}\\|_{1} \\leq q-\\|\\mathbf{I}\\|_{1}}} \\frac{\\left(\\partial^{\\mathbf{l}+\\mathbf{j}} f\\right)\\left(\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)}{\\mathbf{j}!} \\cdot\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}-\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)^{\\mathbf{j}} \\mid \\\\\n& +\\left|\\sum_{\\substack{\\mathbf{j} \\in \\mathbb{N}_{0}^{d}: \\\\\n\\|\\mathbf{j}\\|_{1} \\leq q-\\|\\mathbf{I}\\|_{1}}} \\frac{\\left(\\partial^{\\mathbf{l}+\\mathbf{j}} f\\right)\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)}{\\mathbf{j}!} \\cdot\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}-\\left(\\tilde{C}_{k, i}\\right)_{l e f t}\\right)^{\\mathbf{j}}\\right. \\\\\n& \\left.-\\left(\\partial^{\\mathbf{l}} f\\right)\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}\\right)\\right| \\\\\n& \\leq \\sum_{\\substack{\\mathbf{j} \\in \\mathbb{N}_{0}^{d}: \\\\\n\\|\\mathbf{j}\\|_{1} \\leq q-\\|\\mathbf{I}\\|_{1}}} \\frac{1}{\\mathbf{j}!} \\cdot c_{36} \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{p-\\|\\mathbf{l}+\\mathbf{j}\\|_{1}} \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{\\|\\mathbf{j}\\|_{1}}+c_{36} \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{p-\\|\\mathbf{I}\\|_{1}} \\\\\n& \\leq\\left(c_{36} \\cdot e^{d}+c_{36}\\right) \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{p-\\|\\mathbf{I}\\|_{1}}\n\\end{aligned}\n$$\n\nThis implies that we can choose $b_{k, i}^{(\\mathbf{I})} \\in \\mathbb{Z}$ such that\n\n$$\n\\left|b_{k, i}^{(\\mathbf{I})}\\right| \\leq e^{d}+1\n$$\n\nand\n\n$$\n\\left|\\left(\\partial^{\\mathbf{l}} \\tilde{f}\\right)\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}\\right)-\\left(\\partial^{\\mathbf{l}} f\\right)\\left(\\left(\\tilde{C}_{k+1, i}\\right)_{l e f t}\\right)\\right| \\leq c_{36} \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{p-\\|\\mathbf{l}\\|_{1}}\n$$\n\nObserve that in this way we have defined the coefficients $b_{k, i}^{(\\mathbf{l})}$ for each cube $C_{i, 1}$. We will encode these coefficients for each $i \\in\\left\\{1, \\ldots, M^{d}\\right\\}$ and each $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{l}\\|_{1} \\leq q$ in the single number\n\n$$\nb_{i}^{(\\mathbf{l})}=\\sum_{k=1}^{M^{d}-1}\\left(b_{k, i}^{(\\mathbf{l})}+\\left\\lceil e^{d}\\right\\rceil+2\\right) \\cdot\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)^{-k} \\in[0,1]\n$$\n\nIn the last step the neural network then computes\n\n$$\n\\hat{T}_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}}(x):=\\sum_{\\substack{\\mathbf{l} \\in \\mathbb{N}_{0}^{d} \\\\\\|\\mathbf{l}\\|_{1} \\leq q}} \\frac{\\left(\\partial^{\\mathbf{l}} \\tilde{f}\\right)\\left(\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}\\right)}{\\mathbf{l}!} \\cdot\\left(x-\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}\\right)^{\\mathbf{l}}\n$$\n\nwhere by construction we have $C_{\\mathcal{P}_{2}}(x)=\\tilde{C}_{k, i}$ for some $k \\in\\left\\{1, \\ldots, M^{d}\\right\\}$. Since\n\n$$\n\\begin{aligned}\n& \\left|\\hat{T}_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}}(x)-T_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}}(x)\\right| \\\\\n& \\leq \\sum_{\\substack{\\mathbf{l} \\in \\mathbb{N}_{0}^{d} \\\\\n\\|\\mathbf{l}\\|_{1} \\leq q}} \\frac{\\left|\\left(\\partial^{\\mathbf{l}} \\tilde{f}-\\partial^{\\mathbf{l}} f\\right)\\left(\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}\\right)\\right|}{\\mathbf{l}!} \\cdot\\left|x-\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}\\right|^{\\mathbf{l}} \\\\\n& \\leq e^{d} \\cdot c_{36} \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{p}\n\\end{aligned}\n$$\n\nthe network approximating $\\hat{T}_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}}(x)$ is also a good approximation for $T_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}}(x)$.\nTo approximate $f(x)$ by neural networks the proof of Kohler and Langer follows four key steps:\n\n1. Compute $\\hat{T}_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}}(x)$ by recursively defined functions.\n2. Approximate the recursive functions by neural networks. The resulting network is a good approximation for $f(x)$ in case that\n\n$$\nx \\in \\bigcup_{k \\in\\left\\{1, \\ldots, M^{2 d}\\right\\}}\\left(C_{k, 2}\\right)_{1 / M^{2 p+2}}^{0}\n$$\n\n3. Approximate the function $w_{\\mathcal{P}_{2}}(x) \\cdot f(x)$ by deep neural networks, where\n\n$$\nw_{\\mathcal{P}_{2}}(x)=\\prod_{j=1}^{d}\\left(1-\\frac{M^{2}}{a} \\cdot\\left|\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}^{(j)}+\\frac{a}{M^{2}}-x^{(j)}\\right|\\right)_{+}\n$$\n\nis a linear tensor product B-spline which takes its maximum value at the center of $C_{\\mathcal{P}_{2}}(x)$, which is nonzero in the inner part of $C_{\\mathcal{P}_{2}}(x)$ and which vanishes outside of $C_{\\mathcal{P}_{2}}(x)$.\n4. Apply those networks to $2^{d}$ slightly shifted partitions of $\\mathcal{P}_{2}$ to approximate $f(x)$ in supremum norm.\n\nWe focus on step 2 and 3 and modify the construction of the auxiliary neural networks by deriving constraints for the required weights.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 24,
      "text": "# A.2.1. Key step 1: A recursive definition of $\\hat{T}_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{\\text {left }}(x)}$ \n\nTo derive a recursive definition of $\\hat{T}_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{\\text {left }}(x)}$, we set\n\n$$\n\\begin{aligned}\n& \\phi_{1,0}=\\left(\\phi_{1,0}^{(1)}, \\ldots, \\phi_{1,0}^{(d)}\\right)=x \\\\\n& \\phi_{2,0}=\\left(\\phi_{2,0}^{(1)}, \\ldots, \\phi_{2,0}^{(d)}\\right)=\\mathbf{0}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\phi_{3,0}^{(\\mathbf{I})}=0 \\text { and } \\phi_{4,0}^{(\\mathbf{I})}=0\n$$\n\nfor each $\\mathbf{1} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{1}\\|_{1} \\leq q$. For $j \\in\\left\\{1, \\ldots, M^{d}\\right\\}$ set\n\n$$\n\\begin{gathered}\n\\phi_{1, j}=\\phi_{1, j-1} \\\\\n\\phi_{2, j}=\\left(C_{j, 1}\\right)_{l e f t} \\cdot \\mathbb{1}_{C_{j, 1}}\\left(\\phi_{1, j-1}\\right)+\\phi_{2, j-1} \\\\\n\\phi_{3, j}^{(\\mathbf{I})}=\\left(\\partial^{\\mathbf{l}} f\\right)\\left(\\left(C_{j, 1}\\right)_{\\text {left }}\\right) \\cdot \\mathbb{1}_{C_{j, 1}}\\left(\\phi_{1, j-1}\\right)+\\phi_{3, j-1}^{(\\mathbf{I})}\n\\end{gathered}\n$$\n\nand\n\n$$\n\\phi_{4, j}^{(\\mathbf{I})}=b_{j}^{(\\mathbf{I})} \\cdot \\mathbb{1}_{C_{j, 1}}\\left(\\phi_{1, j-1}\\right)+\\phi_{4, j-1}^{(\\mathbf{I})}\n$$\n\nFurthermore set\n\n$$\n\\begin{gathered}\n\\phi_{1, M^{d}+j}=\\phi_{1, M^{d}+j-1}, \\quad j \\in\\left\\{1, \\ldots, M^{d}\\right\\}, \\\\\n\\phi_{2, M^{d}+j}=\\phi_{2, M^{d}+j-1}+\\tilde{\\mathbf{v}}_{j+1} \\\\\n\\phi_{3, M^{d}+j}^{(\\mathbf{I})}=\\sum_{\\substack{\\mathbf{s} \\in \\mathbb{N}_{0}^{d} \\\\\n\\|\\mathbf{s}\\|_{1} \\leq q-\\|\\mathbf{1}\\|_{1}}} \\frac{\\phi_{3, M^{d}+j-1}^{(\\mathbf{I}+\\mathbf{s})}}{\\mathbf{s}!} \\cdot\\left(\\tilde{\\mathbf{v}}_{j+1}\\right)^{\\mathbf{s}}\n\\end{gathered}\n$$\n\n$$\n\\begin{gathered}\n+\\left(\\left\\lfloor\\left(4+2 \\cdot\\left\\lceil e^{d}\\right\\rceil\\right) \\cdot \\phi_{4, M^{d}+j-1}^{(\\mathbf{I})}\\right\\rfloor-\\left\\lceil e^{d}\\right\\rceil-2\\right) \\cdot c_{36} \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{p-\\|\\mathbf{I}\\|_{1}} \\\\\n\\phi_{4, M^{d}+j}^{(\\mathbf{I})}=\\left(4+2 \\cdot\\left\\lceil e^{d}\\right\\rceil\\right) \\cdot \\phi_{4, M^{d}+j-1}^{(\\mathbf{I})}-\\left\\lfloor\\left(4+2 \\cdot\\left\\lceil e^{d}\\right\\rceil\\right) \\cdot \\phi_{4, M^{d}+j-1}^{(\\mathbf{I})}\\right\\rfloor\n\\end{gathered}\n$$\n\nfor $j \\in\\left\\{1, \\ldots, M^{d}-1\\right\\}$ and each $\\mathbf{I} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{I}\\|_{1} \\leq q$ and\n\n$$\n\\phi_{5, M^{d}+j}=\\mathbb{1}_{\\mathcal{A}^{(j)}}\\left(\\phi_{1, M^{d}+j-1}\\right) \\cdot \\phi_{2, M^{d}+j-1}+\\phi_{5, M^{d}+j-1}\n$$\n\nand\n\n$$\n\\phi_{6, M^{d}+j}^{(\\mathbf{I})}=\\mathbb{1}_{\\mathcal{A}^{(j)}}\\left(\\phi_{1, M^{d}+j-1}\\right) \\cdot \\phi_{3, M^{d}+j-1}^{(\\mathbf{I})}+\\phi_{6, M^{d}+j-1}^{(\\mathbf{I})}\n$$\n\nfor $j \\in\\left\\{1, \\ldots, M^{d}\\right\\}$, where\n\n$$\n\\phi_{5, M^{d}}=\\left(\\phi_{5, M^{d}}^{(1)}, \\ldots, \\phi_{5, M^{d}}^{(d)}\\right)=\\mathbf{0}, \\phi_{6, M^{d}}^{(\\mathbf{I})}=0\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\mathcal{A}^{(j)} & =\\left\\{x \\in \\mathbb{R}^{d}:-x^{(k)}+\\phi_{2, M^{d}+j-1}^{(k)} \\leq 0\\right. \\\\\n& \\left.\\text { und } x^{(k)}-\\phi_{2, M^{d}+j-1}^{(k)}-\\frac{2 a}{M^{2}}<0 \\text { for all } k \\in\\{1, \\ldots, d\\}\\right\\}\n\\end{aligned}\n$$\n\nFinally define\n\n$$\n\\phi_{1,2 M^{d}+1}=\\sum_{\\substack{\\mathbf{I} \\in \\mathbb{N}_{0}^{d} ; \\\\\\|\\mathbf{I}\\|_{1} \\leq q}} \\frac{\\phi_{6,2 M^{d}}^{(\\mathbf{I})}}{\\mathbf{I}!} \\cdot\\left(\\phi_{1,2 M^{d}}-\\phi_{5,2 M^{d}}\\right)^{\\mathbf{I}}\n$$\n\nThe next lemma shows that this recursion computes $\\hat{T}_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{i \\in f t}}(x)$.\nLemma 13 Let $p=q+s$ for some $q \\in \\mathbb{N}_{0}$ and $s \\in(0,1]$, let $C>0$ and $x \\in[-a, a)^{d}$. Let $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ be a $(p, C)$-smooth function and let $\\hat{T}_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{i \\in f t}}$ be defined as in (31). Define $\\phi_{1,2 M^{d}+1}$ recursively as above. Then we have\n\n$$\n\\phi_{1,2 M^{d}+1}=\\hat{T}_{f, q,\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{i \\in f t}}(x)\n$$\n\nProof. See Lemma 11 in Kohler and Langer (2021).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 25,
      "text": "# A.2.2. Key step 2: Approximating $\\phi_{1,2 M^{d}+1}$ by neural networks \n\nIn this step we show that a neural network approximates $\\phi_{1,2 M^{d}+1}$ in case that\n\n$$\nx \\in \\bigcup_{i \\in\\left\\{1, \\ldots, M^{2 d}\\right\\}}\\left(C_{i, 2}\\right)_{1 / M^{2 p+2}}^{0}\n$$\n\nWe define a composition neural network, which approximately computes the recursive functions in the definition of $\\phi_{1,2 M^{d}+1}$.\n\nLemma 14 Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Let $\\mathcal{P}_{2}$ be defined as in (29). Let $p=q+s$ for some $q \\in \\mathbb{N}_{0}$ and $s \\in(0,1]$, and let $C>0$. Let $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ be a $(p, C)$-smooth function. Let $1 \\leq a<\\infty$. Then there exists for $M \\in \\mathbb{N}$ sufficiently large (independent of the size of $a$, but\n\n$$\n\\begin{aligned}\n& M^{2 p} \\geq 2^{4(q+1)+1} \\max \\left\\{c_{37} \\cdot\\left(6+2\\left\\lceil e^{d}\\right\\rceil\\right)^{4(q+1)}, c_{36} \\cdot e^{d}\\right\\} \\\\\n&\\left.\\cdot\\left(\\max \\left\\{a,\\|f\\|_{C^{q}([-a, a]^{d})}\\right\\}\\right)^{4(q+1)}\\right.\n\\end{aligned}\n$$\n\nmust hold), a neural network $\\hat{f}_{\\text {deep, } \\mathcal{P}_{2}} \\in \\mathcal{F}(L, r)$ with\n\n$$\n\\begin{aligned}\n& \\text { (i) } L=4 M^{d}+\\left\\lceil\\log _{4}\\left(M^{2 p+4 \\cdot d \\cdot(q+1)} \\cdot e^{4 \\cdot(q+1) \\cdot\\left(M^{d}-1\\right)}\\right)\\right\\rceil \\\\\n& \\cdot\\left\\lceil\\log _{2}(\\max \\{q+1,2\\})\\right\\rceil \\\\\n& \\text { (ii) } r=\\max \\left\\{10 d+4 d^{2}+2 \\cdot\\binom{d+q}{d} \\cdot\\left(2 \\cdot\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)+5+2 d\\right)\\right. \\\\\n& \\left.18 \\cdot(q+1) \\cdot\\binom{d+q}{d}\\right\\}\n\\end{aligned}\n$$\n\nsuch that\n\n$$\n\\left|\\hat{f}_{\\text {deep }, \\mathcal{P}_{2}}(x)-f(x)\\right| \\leq c_{38} \\cdot\\left(\\max \\left\\{2 a,\\|f\\|_{C^{q}([-a, a]^{d})}\\right\\}\\right)^{4(q+1)} \\cdot \\frac{1}{M^{2 p}}\n$$\n\nholds for all $x \\in \\bigcup_{i \\in\\left\\{1, \\ldots, M^{2 d}\\right\\}}\\left(C_{i, 2}\\right)_{1 / M^{2 p+2}}^{0}$. The network value is bounded by\n\n$$\n\\begin{aligned}\n\\left|\\hat{f}_{\\text {deep }, \\mathcal{P}_{2}}(x)\\right| \\leq & 1+\\left(\\|f\\|_{C^{q}([-a, a]^{d})} \\cdot e^{\\left(M^{d}-1\\right)}\\right. \\\\\n& \\left.+\\left(4+2 \\cdot\\left\\lceil e^{d}\\right\\rceil\\right) \\cdot\\left(M^{d}-1\\right) \\cdot e^{\\left(M^{d}-2\\right)}\\right) \\cdot e^{2 a d}\n\\end{aligned}\n$$\n\nfor all $x \\in[-a, a)^{d}$.\n$\\hat{f}_{\\text {deep, } \\mathcal{P}_{2}}$ satisfies the weight constraint:\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {deep, } \\mathcal{P}_{2}}}\\right\\|_{\\infty} \\leq e^{c_{39} \\cdot\\left(M^{d}+d\\right) \\cdot 2(q+1)}\n$$\n\nwhere $c_{39}=c_{39}(f)$.\nAs in Kohler and Langer (2021), auxiliary networks are required to prove these results. We introduce the auxiliary networks with weight constraints and modify parts of the proofs accordingly.\n\nIn the construction of our network we will compose smaller subnetworks to successively build the final network. Here instead of using an additional layer, we \"merge\" the weights of both networks $f$ and $g$ to define $f \\circ g$. The following lemma clarifies this idea and derives appropriate weight constraints:\n\nLemma 15 Let $f_{0}: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}$ be a neural network of the class $\\mathcal{F}(L, r)$ with weight vector $\\mathbf{v}_{0}$ and let $f_{1}, \\ldots, f_{k}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ be neural networks of class $\\mathcal{F}(\\bar{L}, \\bar{r})$ with weight vectors $\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{k}$. Denote by $\\overline{\\mathbf{v}}$ the vector that contains $\\left(\\mathbf{v}_{j}\\right)_{j \\in\\{1, \\ldots, k\\}}$.\nThen the network $f=f_{0}\\left(f_{1}, \\ldots, f_{k}\\right)$ has $L+\\bar{L}$ layers and at most $\\max \\{k \\cdot \\bar{r}, r\\}$ neurons.\na) In general $\\mathbf{v}$ satisfies the constraints\n\n$$\n\\|\\mathbf{v}\\|_{\\infty} \\leq \\max \\left\\{\\left\\|\\mathbf{v}_{0}\\right\\|_{\\infty},\\|\\overline{\\mathbf{v}}\\|_{\\infty},\\left\\|\\left(\\mathbf{v}_{0}\\right)^{(0)}\\right\\|_{\\infty} \\cdot\\left(k\\left\\|(\\overline{\\mathbf{v}})^{(\\bar{L})}\\right\\|_{\\infty}+1\\right)\\right\\}\n$$\n\nb) If $\\left(\\mathbf{v}_{j}\\right)_{1,0}^{(L)}=0$ for all $j \\in\\{1, \\ldots, k\\}$, $\\mathbf{v}$ satisfies :\n\n$$\n\\|\\mathbf{v}\\|_{\\infty} \\leq \\max \\left\\{\\left\\|\\mathbf{v}_{0}\\right\\|_{\\infty},\\|\\overline{\\mathbf{v}}\\|_{\\infty},\\left\\|\\left(\\mathbf{v}_{0}\\right)_{i, j>0}^{(0)}\\right\\|_{\\infty} \\cdot\\left\\|(\\overline{\\mathbf{v}})_{1, j>0}^{(L)}\\right\\|_{\\infty}\\right\\}\n$$\n\nc) If additionally to $\\left(\\mathbf{v}_{j}\\right)_{1,0}^{(L)}=0$ for all $j \\in\\{1, \\ldots, k\\}$, we have $\\left\\|\\left(\\mathbf{v}_{0}\\right)_{i, j>0}^{(0)}\\right\\|_{\\infty} \\leq 1$ or $\\left\\|(\\overline{\\mathbf{v}})_{i, j>0}^{(L)}\\right\\|_{\\infty} \\leq 1$, then $\\mathbf{v}$ satisfies\n\n$$\n\\|\\mathbf{v}\\|_{\\infty} \\leq \\max \\left\\{\\left\\|\\mathbf{v}_{0}\\right\\|_{\\infty},\\|\\overline{\\mathbf{v}}\\|_{\\infty}\\right\\}\n$$\n\nProof. The network $f=f_{0}\\left(f_{1}, \\ldots, f_{k}\\right)$ is recursively defined as follows\n\n$$\nf(x)=\\sum_{i=1}^{r}\\left(\\mathbf{v}_{0}\\right)_{1, i}^{(L)} f_{i}^{(\\bar{L}+L)}(x)+\\left(\\mathbf{v}_{0}\\right)_{1,0}^{(L)}\n$$\n\nwhere for $l \\in\\{2, \\ldots, L\\}$ the outputs of the neurons $f_{i}^{(\\bar{L}+l)}$ are recursively defined by\n\n$$\nf_{i}^{(\\bar{L}+l)}(x)=\\sigma\\left(\\sum_{j=1}^{r}\\left(\\mathbf{v}_{0}\\right)_{i, j}^{(l-1)} f_{j}^{(\\bar{L}+l-1)}(x)+\\left(\\mathbf{v}_{0}\\right)_{i, 0}^{(l-1)}\\right)\n$$\n\nfor $i \\in\\{1, \\ldots, r\\}$. In layer $\\bar{L}$, the effect of \"merging\" the networks together becomes apparent and we can see that layer $\\bar{L}$ the network $f=f_{0}\\left(f_{1}, \\ldots, f_{k}\\right)$ consists of $k \\cdot \\bar{r}$ neurons:\n\n$$\n\\begin{aligned}\nf_{i}^{(\\bar{L}+1)}(x) & =\\sigma\\left(\\sum_{j=1}^{k}\\left(\\mathbf{v}_{0}\\right)_{i, j}^{(0)} f_{j}(x)+(\\mathbf{v})_{i, 0}^{(0)}\\right) \\\\\n& =\\sigma\\left(\\sum_{j=1}^{k}\\left(\\mathbf{v}_{0}\\right)_{i, j}^{(0)} \\cdot\\left(\\sum_{l=1}^{r}\\left(\\mathbf{v}_{j}\\right)_{1, l}^{(L)} \\cdot f_{j, l}^{(L)}(x)+\\left(\\mathbf{v}_{j}\\right)_{1,0}^{(L)}\\right)+\\left(\\mathbf{v}_{0}\\right)_{i, 0}^{(0)}\\right) \\\\\n& =\\sigma\\left(\\sum_{j=1}^{k} \\sum_{l=1}^{r}\\left(\\mathbf{v}_{0}\\right)_{i, j}^{(0)} \\cdot\\left(\\mathbf{v}_{j}\\right)_{1, l}^{(L)} \\cdot f_{j, l}^{(L)}(x)+\\sum_{j=1}^{k}\\left(\\mathbf{v}_{0}\\right)_{i, j}^{(0)} \\cdot\\left(\\mathbf{v}_{j}\\right)_{1,0}^{(L)}+\\left(\\mathbf{v}_{0}\\right)_{i, 0}^{(0)}\\right)\n\\end{aligned}\n$$\n\nfor $i \\in\\{1, \\ldots, r\\}$, where the $f_{j, i}^{(s)}(x)$ are defined by\n\n$$\nf_{(j-1) \\cdot i+i}^{(s)}(x)=f_{j, i}^{(s)}(x)=\\sigma\\left(\\sum_{l=1}^{\\bar{r}}\\left(\\mathbf{v}_{j}\\right)_{i, l}^{(s-1)} f_{j, l}^{(s-1)}(x)+\\left(\\mathbf{v}_{j}\\right)_{i, 0}^{(s-1)}\\right)\n$$\n\nfor $j \\in\\{1, \\ldots, k\\}, s \\in\\{1, \\ldots, \\bar{L}\\}$ and $i \\in\\{1, \\ldots, \\bar{r}\\}$. Finally we have\n\n$$\nf_{(j-1) \\cdot l+l}^{(s)}(x)=f_{j, l}^{(0)}(x)=x^{(l)}\n$$\n\nfor $j \\in\\{1, \\ldots, k\\}, l \\in\\{1, \\ldots, d\\}$.\nIn layers $l \\in\\{1, \\ldots \\bar{L}-1\\}$, the weights of $f$ satisfy the same constraints as $f_{1}, \\ldots, f_{k}$, in layers $l \\in\\{\\bar{L}+1, \\ldots, L+\\bar{L}\\}$ the weight constraints of $f$ correspond to the constraints of $f_{0}$. However, in layer $\\bar{L}$ we have to consider the product of the output weights of $f_{i}$ for $i \\in\\{1, \\ldots, k\\}$ and the weights of the input layer of $f_{0}$, as shown in (35). The weights there satisfy\n\n$$\n\\left|\\left(\\mathbf{v}_{0}\\right)_{i, j}^{(0)} \\cdot\\left(\\mathbf{v}_{j}\\right)_{1, l}^{(L)}\\right| \\leq \\max _{j \\in\\{1, \\ldots, k\\}, l \\in\\{1, \\ldots, r\\}}\\left|\\left(\\mathbf{v}_{0}\\right)_{i, j}^{(0)}\\right| \\cdot\\left|\\left(\\mathbf{v}_{j}\\right)_{1, l}^{(L)}\\right| \\leq\\left\\|\\left(\\mathbf{v}_{0}\\right)^{(0)}\\right\\|_{\\infty} \\cdot\\left\\|(\\overline{\\mathbf{v}})^{(L)}\\right\\|_{\\infty}\n$$\n\nand\n\n$$\n\\left|\\sum_{j=1}^{k}\\left(\\mathbf{v}_{0}\\right)_{i, j}^{(0)} \\cdot\\left(\\mathbf{v}_{j}\\right)_{1,0}^{(L)}+\\left(\\mathbf{v}_{0}\\right)_{i, 0}^{(0)}\\right| \\leq\\left\\|\\left(\\mathbf{v}_{0}\\right)^{(0)}\\right\\|_{\\infty} \\cdot\\left(\\sum_{j=1}^{k}\\left\\|\\left(\\mathbf{v}_{j}\\right)^{(L)}\\right\\|_{\\infty}+1\\right)\n$$\n\nwhich implies part a) of the assertion.\nIf, additionally, $\\left(\\mathbf{v}_{j}\\right)_{1,0}^{(L)}=0$ for all $j \\in\\{1, \\ldots, k\\}$, the latter bound can be refined to\n\n$$\n\\left|(\\mathbf{v})\\right|_{i, 0}^{(\\bar{L})}=\\left|\\left(\\mathbf{v}_{0}\\right)_{i, 0}^{(0)}\\right| \\leq\\left\\|\\left(\\mathbf{v}_{0}\\right)^{(0)}\\right\\|_{\\infty}\n$$\n\nSince $\\left\\|\\left(\\mathbf{v}_{0}\\right)^{(0)}\\right\\|_{\\infty} \\leq\\left\\|\\left(\\mathbf{v}_{0}\\right)\\right\\|_{\\infty}$, this case no longer influences the upper bound and we can reduce the third argument of the maximum to $\\left\\|\\left(\\mathbf{v}_{0}\\right)_{i, j>0}^{(0)}\\right\\|_{\\infty} \\cdot\\left\\|(\\overline{\\mathbf{v}})_{i, j>0}^{(L)}\\right\\|_{\\infty}$, which implies the upper bound of part b) of the assertion.\nPart c) follows directly from part b).\nThe following lemma presents a neural network, that approximates the square function, which is essential to build neural networks for more complex tasks.\n\nLemma 16 Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Then for any $R \\in \\mathbb{N}$ and any $a \\geq 1$ a neural network\n\n$$\n\\hat{f}_{s q} \\in \\mathcal{F}(R, 9)\n$$\n\nexists with weight constraints\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{s q}}\\right\\|_{\\infty}=\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{s q}}\\right)^{(R)}\\right\\|_{\\infty} \\leq 4 \\cdot a^{2} \\quad \\text { and } \\quad\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{s q}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1\n$$\n\nsuch that\n\n$$\n\\left|\\hat{f}_{s q}(x)-x^{2}\\right| \\leq a^{2} \\cdot 4^{-R}\n$$\n\nholds for $x \\in[-a, a]$.\n\nProof. This proof follows as in Kohler and Langer (2021). We only modify the parts required to show the constraints on the weights. In Kohler and Langer (2021) it was shown that linear combinations of the \"tooth\" function $g:[0,1] \\rightarrow[0,1]$\n\n$$\ng(x)= \\begin{cases}2 x & , x \\leq \\frac{1}{2} \\\\ 2 \\cdot(1-x) & , x>\\frac{1}{2}\\end{cases}\n$$\n\nand the iterated composition function\n\n$$\ng_{s}(x)=\\underbrace{g \\circ g \\circ \\cdots \\circ g}_{s}(x)\n$$\n\ncan be used to approximate $f(x)=x^{2}$ for $x \\in[0,1]$. Let $S_{R}$ denote the piecewise linear interpolation of $f$ with $2^{R}+1$ uniformly distributed breakpoints.\n\nIt can be shown that $S_{R}(x)$ is given by\n\n$$\nS_{R}(x)=x-\\sum_{s=1}^{R} \\frac{g_{s}(x)}{2^{2 s}}\n$$\n\nand that it satisfies\n\n$$\n\\left|S_{R}(x)-x^{2}\\right| \\leq 2^{-2 R-2}\n$$\n\nfor $x \\in[0,1]$.\nIn a third step of their proof Kohler and Langer show, that there exists a feedforward neural network that computes $S_{R}(x)$ for $x \\in[0,1]$. In order to derive the weight constraints we include the construction of this network.\nThe function $g(x)$ can be implemented by the network:\n\n$$\n\\hat{f}_{g}(x)=2 \\cdot \\sigma(x)-4 \\cdot \\sigma\\left(x-\\frac{1}{2}\\right)+2 \\cdot \\sigma(x-1)\n$$\n\nand the function $g_{s}(x)$ can be implemented by a network\n\n$$\n\\hat{f}_{g_{s}} \\in \\mathcal{F}(s, 3)\n$$\n\nwith\n\n$$\n\\hat{f}_{g_{s}}(x)=\\underbrace{\\hat{f}_{g}\\left(\\hat{f}_{g}\\left(\\ldots\\left(\\hat{f}_{g}(x)\\right)\\right)\\right.}_{s}\n$$\n\nThus we have\n\n$$\n\\left\\|\\mathbf{v}_{f_{g}}\\right\\|_{\\infty}=\\left\\|\\mathbf{v}_{f_{g_{s}}}\\right\\|_{\\infty}=4 \\quad \\text { for all } s \\in \\mathbb{N}\n$$\n\nLet\n\n$$\n\\hat{f}_{i d}(z)=\\sigma(z)-\\sigma(-z)\n$$\n\nand define $\\hat{f}_{i d}^{t}$ recursively by\n\n$$\n\\hat{f}_{i d}^{0}(z)=z\n$$\n\n$$\n\\hat{f}_{i d}^{t+1}(z)=\\hat{f}_{i d}\\left(\\hat{f}_{i d}^{t}(z)\\right) \\quad\\left(z \\in \\mathbb{R}, t \\in \\mathbb{N}_{0}\\right)\n$$\n\nwhich implies\n\n$$\n\\hat{f}_{i d}^{t}(z)=z\n$$\n\nIt is easy to see that these networks satisfy\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{i d}}\\right\\|_{\\infty}=\\left\\|\\mathbf{v}_{\\hat{f}_{i d}^{t}}\\right\\|_{\\infty}=1 \\quad \\text { for all } t \\in \\mathbb{N}\n$$\n\nBy combining the networks above we can implement the function $S_{R}(x)$ by a network\n\n$$\n\\hat{f}_{s q_{[0,1]}} \\in \\mathcal{F}(R, 7)\n$$\n\nrecursively defined as follows: We set $\\hat{f}_{1,0}(x)=\\hat{f}_{2,0}(x)=x$ and $\\hat{f}_{3,0}(x)=0$.\nThen we set\n\n$$\n\\begin{aligned}\n& \\hat{f}_{1, i+1}(x)=\\hat{f}_{i d}\\left(\\hat{f}_{1, i}(x)\\right) \\\\\n& \\hat{f}_{2, i+1}(x)=\\hat{f}_{g}\\left(\\hat{f}_{2, i}(x)\\right)\n\\end{aligned}\n$$\n\nand\n\n$$\n\\hat{f}_{3, i+1}(x)=f_{i d}\\left(\\hat{f}_{3, i}(x)\\right)-\\frac{\\hat{f}_{g}\\left(\\hat{f}_{2, i}(x)\\right)}{2^{2(i+1)}}\n$$\n\nfor $i \\in\\{0,1, \\ldots, R-2\\}$ and\n\n$$\n\\hat{f}_{s q_{[0,1]}}(x)=\\hat{f}_{i d}\\left(\\hat{f}_{1, R-1}(x)\\right)-\\frac{\\hat{f}_{2, R}(x)}{2^{2 R}}+\\hat{f}_{i d}\\left(\\hat{f}_{3, R-1}(x)\\right)\n$$\n\nUsing the positive homogeneity of the ReLU function, this implies\n\n$$\n\\begin{aligned}\n\\hat{f}_{s q_{[0,1]}}(x)= & \\hat{f}_{i d}^{R}(x)-\\frac{1}{2^{2 R}} \\hat{f}_{g_{R}}(x)-\\hat{f}_{i d}\\left(\\frac{1}{2^{2(R-1)}} \\hat{f}_{g_{R-1}}(x)\\right. \\\\\n& \\left.-\\hat{f}_{i d}\\left(\\frac{1}{2^{2(R-2)}} \\hat{f}_{g_{R-2}}(x)-\\cdots-\\hat{f}_{i d}\\left(\\frac{1}{2^{2}} \\hat{f}_{g_{1}}(x)\\right)\\right)\\right) \\\\\n= & S_{R}(x)\n\\end{aligned}\n$$\n\nhence $\\hat{f}_{s q_{[0,1]}}(x)$ satisfies\n\n$$\n\\left|\\hat{f}_{s q_{[0,1]}}(x)-x^{2}\\right| \\leq 2^{-2 R-2}\n$$\n\nfor $x \\in[0,1]$. We can construct the network $\\hat{f}_{s q_{[0,1]}}$ such that its weights satisfy the constraint\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{s q_{[0,1]}}}\\right\\|_{\\infty}=\\frac{1}{4} \\cdot\\left\\|\\mathbf{v}_{\\hat{f}_{g}}\\right\\|_{\\infty}=1\n$$\n\nTo show this, we need to show that the absolute value of each weight of the network $\\hat{f}_{2, i}(x):=\\frac{\\hat{f}_{2, i}(x)}{2^{2 i}}$ is smaller or equal to one for every $i$.\n\nWe proceed by induction. For $i=1$ we have\n\n$$\n\\frac{\\tilde{f}_{2,1}(x)}{2^{2}}=\\frac{1}{4} \\hat{f}_{g}(x)=\\frac{1}{2} \\sigma(x)-\\sigma\\left(x-\\frac{1}{2}\\right)+\\frac{1}{2} \\sigma(x-1)\n$$\n\nand thus $\\left\\|\\mathbf{v}_{\\tilde{f}_{2,1}}\\right\\|_{\\infty}=1$.\nAssume that $\\left\\|\\mathbf{v}_{\\tilde{f}_{2, i}}\\right\\|_{\\infty}=1$ holds for arbitrary but fixed $i>0$.\nUsing the positive homogeneity of the ReLU function we can conclude\n\n$$\n\\begin{aligned}\n\\tilde{f}_{2, i+1}(x)=\\frac{\\hat{f}_{2, i+1}(x)}{2^{2(i+1)}} & =\\frac{1}{4} \\cdot \\frac{1}{2^{2 i}} \\hat{f}_{g}\\left(\\hat{f}_{2,1}(x)\\right) \\\\\n& =\\frac{1}{2^{2 i}} \\cdot\\left(\\frac{1}{2} \\sigma\\left(\\hat{f}_{2, i}(x)\\right)-\\sigma\\left(\\hat{f}_{2, i}(x)-\\frac{1}{2}\\right)+\\frac{1}{2} \\sigma\\left(\\hat{f}_{2, i}(x)-1\\right)\\right) \\\\\n& =\\frac{1}{2} \\sigma\\left(\\frac{1}{2^{2 i}} \\cdot \\hat{f}_{2, i}(x)\\right)-\\sigma\\left(\\frac{1}{2^{2 i}} \\cdot \\hat{f}_{2, i}(x)-\\frac{1}{2^{2 i}} \\cdot \\frac{1}{2}\\right)+\\frac{1}{2} \\sigma\\left(\\frac{1}{2^{2 i}} \\cdot \\hat{f}_{2, i}(x)-\\frac{1}{2^{2 i}}\\right) \\\\\n& =\\frac{1}{2} \\sigma\\left(\\tilde{f}_{2, i}(x)\\right)-\\sigma\\left(\\hat{f}_{2, i}(x)-\\frac{1}{2^{2 i}} \\cdot \\frac{1}{2}\\right)+\\frac{1}{2} \\sigma\\left(\\tilde{f}_{2, i}(x)-\\frac{1}{2^{2 i}}\\right)\n\\end{aligned}\n$$\n\nand thus $\\left\\|\\mathbf{v}_{\\tilde{f}_{2, i+1}}\\right\\|_{\\infty}=1$.\nIn a last step $\\tilde{f}_{s q_{[0,1]}}$ is extended to approximate the function $f(x)=x^{2}$ on the domain $[-a, a]$. Therefore $\\tilde{f}_{\\text {tran }}:[-a, a] \\rightarrow[0,1]$ is defined by\n\n$$\nf_{\\text {tran }}(z)=\\frac{z}{2 a}+\\frac{1}{2}\n$$\n\nto transfers the value of $x \\in[-a, a]$ in the interval, where (36) holds. Set\n\n$$\n\\tilde{f}_{s q}(x)=4 a^{2} \\hat{f}_{s q_{[0,1]}}\\left(f_{\\text {tran }}(x)\\right)-\\left(2 a \\cdot \\hat{f}_{i d}^{R}(x)+a^{2}\\right)\n$$\n\nThe extension to the domain $[-a, a]$ only increases the weights of the last layer of the network, which results in the constraints\n\n$$\n\\left\\|\\left(\\mathbf{v}_{\\tilde{f}_{s q}}\\right)\\right\\|_{\\infty}=\\left\\|\\left(\\mathbf{v}_{\\tilde{f}_{s q}}\\right)^{(R)}\\right\\|_{\\infty} \\leq 4 \\cdot a^{2} \\quad \\text { and } \\quad\\left\\|\\left(\\mathbf{v}_{\\tilde{f}_{s q_{[0,1]}}}\\right)^{(0)}\\right\\|_{\\infty}=\\left\\|\\left(\\mathbf{v}_{\\tilde{f}_{s q}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1\n$$\n\nSince\n\n$$\nx^{2}=4 a^{2} \\cdot\\left(\\frac{x}{2 a}+\\frac{1}{2}\\right)^{2}-2 a x-a^{2}\n$$\n\nwe have\n\n$$\n\\begin{aligned}\n& \\left|\\hat{f}_{s q}(x)-x^{2}\\right| \\\\\n= & \\left|4 a^{2} \\hat{f}_{s q_{[0,1]}}\\left(f_{\\text {tran }}(x)\\right)-\\left(2 a \\cdot \\hat{f}_{i d}^{R}(x)+a^{2}\\right)-\\left(4 a^{2} \\cdot\\left(f_{\\text {tran }}(x)\\right)^{2}-2 a x-a^{2}\\right)\\right| \\\\\n\\leq & 4 a^{2} \\cdot\\left|\\hat{f}_{s q_{[0,1]}}\\left(f_{\\text {tran }}(x)\\right)-\\left(f_{\\text {tran }}(x)\\right)^{2}\\right|+2 a\\left|\\hat{f}_{i d}^{R}(x)-x\\right| \\\\\n\\leq & 4 a^{2} \\cdot 2^{-2 R-2}=a^{2} \\cdot 4^{-R}\n\\end{aligned}\n$$\n\nLemma 17 Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Then for any $R \\in \\mathbb{N}$ and any $a \\geq 1$ a neural network\n\n$$\n\\hat{f}_{\\text {mult }} \\in \\mathcal{F}(R, 18)\n$$\n\nexists, whose weights satisfy\n\n$$\n\\begin{gathered}\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right\\|_{\\infty} \\leq 4 \\cdot a^{2}, \\quad\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)_{1,0}^{(R)}=0 \\quad \\text { and } \\\\\n\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1\n\\end{gathered}\n$$\n\nsuch that\n\n$$\n\\left|\\hat{f}_{\\text {mult }}(x, y)-x \\cdot y\\right| \\leq 2 \\cdot a^{2} \\cdot 4^{-R}\n$$\n\nholds for all $x, y \\in[-a, a]$.\nProof. Let\n\n$$\n\\hat{f}_{s q} \\in \\mathcal{F}(R, 9)\n$$\n\nbe the neural network from Lemma 16, i.e.\n\n$$\n\\hat{f}_{s q}(x)=16 a^{2} \\hat{f}_{s q_{[0,1]}}\\left(\\frac{x}{4 a}+\\frac{1}{2}\\right)-\\left(4 a \\cdot \\hat{f}_{i d}^{R}(x)+4 a^{2}\\right)\n$$\n\nwhich satisfies\n\n$$\n\\left|\\hat{f}_{s q}(x)-x^{2}\\right| \\leq 4 \\cdot a^{2} \\cdot 4^{-R}\n$$\n\nfor $x \\in[-2 a, 2 a]$ and with weight constraints $\\left\\|\\mathbf{v}_{\\hat{f}_{s q_{[0,1]}}}\\right\\|_{\\infty}=\\left\\|\\mathbf{v}_{\\hat{f}_{i d}^{R}}\\right\\|_{\\infty}=1$, and set\n\n$$\n\\begin{aligned}\n\\hat{f}_{\\text {mult }}(x, y)= & \\frac{1}{4} \\cdot\\left(\\hat{f}_{s q}(x+y)-\\hat{f}_{s q}(x-y)\\right) \\\\\n= & 4 a^{2} \\hat{f}_{s q_{[0,1]}}\\left(\\frac{x+y}{4 a}+\\frac{1}{2}\\right)-a \\cdot \\hat{f}_{i d}^{R}(x+y) \\\\\n& -\\left(4 a^{2} \\hat{f}_{s q_{[0,1]}}\\left(\\frac{x-y}{4 a}+\\frac{1}{2}\\right)-a \\cdot \\hat{f}_{i d}^{R}(x-y)\\right)\n\\end{aligned}\n$$\n\nNote that since $\\hat{f}_{s q}(x+y)$ and $\\hat{f}_{s q}(x-y)$ have the same offset in the last layer, they cancel out and we have $\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)_{1,0}^{(R)}=0$. The constraints $\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right\\|_{\\infty}=4 \\cdot a^{2}$ and $\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1$ follow directly from Lemma 16. Since\n\n$$\nx \\cdot y=\\frac{1}{4}\\left((x+y)^{2}-(x-y)^{2}\\right)\n$$\n\nwe have\n\n$$\n\\begin{aligned}\n\\left|\\hat{f}_{\\text {mult }}(x, y)-x \\cdot y\\right| & \\leq \\frac{1}{4} \\cdot\\left|\\hat{f}_{s q}(x+y)-(x+y)^{2}\\right|+\\frac{1}{4} \\cdot\\left|(x-y)^{2}-\\hat{f}_{s q}(x-y)\\right| \\\\\n& \\leq \\frac{1}{4} \\cdot 2 \\cdot 4 \\cdot a^{2} \\cdot 4^{-R} \\\\\n& \\leq 2 \\cdot a^{2} \\cdot 4^{-R}\n\\end{aligned}\n$$\n\nfor $x, y \\in[-a, a]$.\nLemma 18 Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Then for $R \\in \\mathbb{N}, R \\geq \\log _{4}\\left(2 \\cdot 4^{2 \\cdot d} \\cdot a^{2 \\cdot d}\\right)$ and any $a \\geq 1$ a neural network\n\n$$\n\\hat{f}_{\\text {mult }, d} \\in \\mathcal{F}\\left(R \\cdot\\left\\lceil\\log _{2}(d)\\right\\rceil, 18 d\\right)\n$$\n\nwhich satisfies\n\n$$\n\\begin{gathered}\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {mult }, d}}\\right\\|_{\\infty} \\leq 4 \\cdot 4^{2 d} \\cdot a^{2 d}, \\quad\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }, d}}\\right)_{1,0}^{(R \\cdot\\left\\lceil\\log _{2}(d)\\right\\rceil)}=0 \\quad \\text { and } \\\\\n\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }, d}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1\n\\end{gathered}\n$$\n\nexists such that\n\n$$\n\\left|\\hat{f}_{\\text {mult }, d}(x)-\\prod_{i=1}^{d} x^{(i)}\\right| \\leq 4^{4 d+1} \\cdot a^{4 d} \\cdot d \\cdot 4^{-R}\n$$\n\nholds for all $x \\in[-a, a]^{d}$.\nProof. We set $q=\\left\\lceil\\log _{2}(d)\\right\\rceil$. The feedforward neural network $\\hat{f}_{\\text {mult }, d}$ with $L=R \\cdot q$ hidden layers and $r=18 d$ neurons in each layer is constructed as follows: Set\n\n$$\n\\left(z_{1}, \\ldots, z_{2^{q}}\\right)=\\left(x^{(1)}, x^{(2)}, \\ldots, x^{(d)},\\underbrace{1, \\ldots, 1}_{2^{q}-d}\\right)\n$$\n\nIn the construction of our network we will use the network $\\hat{f}_{\\text {mult }}$ of Lemma 17, which satisfies\n\n$$\n\\left|\\hat{f}_{\\text {mult }}(x, y)-x \\cdot y\\right| \\leq 2 \\cdot\\left(4^{d} a^{d}\\right)^{2} \\cdot 4^{-R}\n$$\n\nand\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right\\|_{\\infty} \\leq 4 \\cdot 4^{2 d} a^{2 d}, \\quad\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)_{1,0}^{(R)}=0 \\quad \\text { and } \\quad\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1\n$$\n\nfor $x, y \\in\\left[-4^{d} a^{d}, 4^{d} a^{d}\\right]$. In the first $R$ layers we compute\n\n$$\n\\hat{f}_{\\text {mult }}\\left(z_{1}, z_{2}\\right), \\hat{f}_{\\text {mult }}\\left(z_{3}, z_{4}\\right), \\ldots, \\hat{f}_{\\text {mult }}\\left(z_{2^{q}-1}, z_{2^{q}}\\right)\n$$\n\nwhich can be done by $R$ layers of $18 \\cdot 2^{q-1} \\leq 18 \\cdot d$ neurons. E.g., in case in case $z_{l}=x^{(d)}$ and $z_{l+1}=1$ we have\n\n$$\n\\hat{f}_{\\text {mult }}\\left(z_{l}, z_{l+1}\\right)=\\hat{f}_{\\text {mult }}\\left(x^{(d)}, 1\\right)\n$$\n\nAs a result of the first $R$ layers we get a vector of outputs which has length $2^{q-1}$. Next we pair these outputs and apply $\\hat{f}_{\\text {mult }}$ again. This procedure is continued until there is only one output left. Therefore we need $L=R q$ hidden layers and at most $18 d$ neurons in each layer.\n\nBy (38) and $R \\geq \\log _{4}\\left(2 \\cdot 4^{2 \\cdot d} \\cdot a^{2 \\cdot d}\\right)$ we get for any $l \\in\\{1, \\ldots, d\\}$ and any $z_{1}, z_{2} \\in$ $\\left[-\\left(4^{l}-1\\right) \\cdot a^{l},\\left(4^{l}-1\\right) \\cdot a^{l}\\right]$\n\n$$\n\\left|\\hat{f}_{\\text {mult }}\\left(z_{1}, z_{2}\\right)\\right| \\leq\\left|z_{1} \\cdot z_{2}\\right|+\\left|\\hat{f}_{\\text {mult }}\\left(z_{1}, z_{2}\\right)-z_{1} \\cdot z_{2}\\right| \\leq\\left(4^{l}-1\\right)^{2} a^{2 l}+1 \\leq\\left(4^{2 l}-1\\right) \\cdot a^{2 l}\n$$\n\nFrom this we get successively that all outputs of layer $l \\in\\{1, \\ldots, q-1\\}$ are contained in the interval $\\left[-\\left(4^{2 l}-1\\right) \\cdot a^{2 l},\\left(4^{2 l}-1\\right) \\cdot a^{2 l}\\right]$, hence in particular they are contained in the interval $\\left[-4^{d} a^{d}, 4^{d} a^{d}\\right]$ where inequality (38) does hold.\n\nDefine $\\hat{f}_{2^{q}}$ recursively by\n\n$$\n\\hat{f}_{2^{q}}\\left(z_{1}, \\ldots, z_{2^{q}}\\right)=\\hat{f}_{\\text {mult }}\\left(\\hat{f}_{2^{q-1}}\\left(z_{1}, \\ldots, z_{2^{q-1}}\\right), \\hat{f}_{2^{q-1}}\\left(z_{2^{q-1}+1}, \\ldots, z_{2^{q}}\\right)\\right)\n$$\n\nand\n\n$$\n\\hat{f}_{2}\\left(z_{1}, z_{2}\\right)=\\hat{f}_{\\text {mult }}\\left(z_{1}, z_{2}\\right)\n$$\n\nThe constraints $\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }, d}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1$ and $\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)_{1,0}^{(R)}=0$ follow directly from Lemma 17 , since $\\hat{f}_{\\text {mult }, d}$ is a repeated composition of $\\hat{f}_{\\text {mult }}$. Note that our construction of $\\hat{f}_{\\text {mult }}$ satisfies the special case of Lemma 15, i.e. $\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)_{1,0}^{(R)}=0$ and further $\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)^{(0)}\\right\\|_{\\infty} \\leq$ 1 .\n\nApplying Lemma 15 b) we get for the repeated composition of $\\hat{f}_{\\text {mult }}$ :\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {mult }, d}}\\right\\|_{\\infty} \\leq\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)^{(0)}\\right\\|_{\\infty} \\cdot\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)^{(R)}\\right\\|_{\\infty} \\leq 4 \\cdot 4^{2 d} \\cdot a^{2 d}\n$$\n\nThe rest of the proof follows analogously to the proof of Lemma 8 in Kohler and Langer (2021).\n\nLemma 19 Let $m_{1}, \\ldots, m_{\\binom{d+N}{d}}$ denote all monomials in $\\mathcal{P}_{N}$ for some $N \\in \\mathbb{N}$. Let $r_{1}, \\ldots, r_{\\binom{d+N}{d}} \\in \\mathbb{R}$, define\n\n$$\np\\left(x, y_{1}, \\ldots, y_{\\binom{d+N}{d}}\\right)=\\sum_{i=1}^{\\binom{d+N}{d}} r_{i} \\cdot y_{i} \\cdot m_{i}(x), \\quad x \\in[-a, a]^{d}, y_{i} \\in[-a, a]\n$$\n\nand set $\\bar{r}(p)=\\max _{i \\in\\left\\{1, \\ldots,\\binom{d+N}{d}\\right\\}}\\left|r_{i}\\right|$. Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Then for any $a \\geq 1$ and\n\n$$\nR \\geq \\log _{4}\\left(2 \\cdot 4^{2 \\cdot(N+1)} \\cdot a^{2 \\cdot(N+1)}\\right)\n$$\n\na neural network $\\hat{f}_{p} \\in \\mathcal{F}(L, r)$ with $L=R \\cdot\\left\\lceil\\log _{2}(N+1)\\right\\rceil$ and $r=18 \\cdot(N+1) \\cdot\\binom{d+N}{d}$ exists, whose weights satisfy\n\n$$\n\\begin{gathered}\n\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{p}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1, \\quad\\left(\\mathbf{v}_{\\hat{f}_{p}}\\right)_{1,0}^{(R)}=0 \\quad \\text { and } \\\\\n\\left\\|\\mathbf{v}_{\\hat{f}_{p}}\\right\\|_{\\infty} \\leq 4 \\cdot \\bar{r}(p) \\cdot 4^{2(N+1)} \\cdot a^{2(N+1)}\n\\end{gathered}\n$$\n\nsuch that\n\n$$\n\\left|\\hat{f}_{p}\\left(x, y_{1}, \\ldots, y_{\\binom{d+N}{d}}\\right)-p\\left(x, y_{1}, \\ldots, y_{\\binom{d+N}{d}}\\right)\\right| \\leq c_{40} \\cdot \\bar{r}(p) \\cdot a^{4(N+1)} \\cdot 4^{-R}\n$$\n\nfor all $x \\in[-a, a]^{d}, y_{1}, \\ldots, y_{\\binom{d+N}{d}} \\in[-a, a]$, where $c_{40}$ depends on $d$ and $N$.\nProof. A neural network $\\hat{f}_{m}$ is constructed in order to approximate\n\n$$\ny \\cdot m(x)=y \\cdot \\prod_{k=1}^{d}\\left(x^{(k)}\\right)^{r_{k}}, \\quad x \\in[-a, a]^{d}, y \\in[-a, a]\n$$\n\nwhere $m \\in \\mathcal{P}_{N}$ and $r_{1}, \\ldots, r_{d} \\in \\mathbb{N}_{0}$ with $r_{1}+\\cdots+r_{d} \\leq N$. Note that Lemma 18 can easily be extended to monomials. We set $d$ by $N+1$ and thus get a network\n\n$$\n\\hat{f}_{m} \\in \\mathcal{F}\\left(R \\cdot\\left\\lceil\\log _{2}(N+1)\\right\\rceil, 18 \\cdot(N+1)\\right)\n$$\n\nwhose weights satisfy\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{m}}\\right\\|_{\\infty} \\leq 4 \\cdot 4^{2(N+1)} \\cdot a^{2(N+1)}\n$$\n\nWe then set $\\hat{f}_{p}=\\sum_{i=1}^{\\binom{d+N}{d}} r_{i} \\cdot \\hat{f}_{m_{i}}\\left(x, y_{i}\\right)$, which increases the weight constraint by a factor $\\bar{r}(p)$. The weight constraints $\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{p}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1$ and $\\left(\\mathbf{v}_{\\hat{f}_{p}}\\right)_{1,0}^{(R)}=0$ follow directly from Lemma 18. The rest of the proof follows as in the proof of Lemma 5 of Kohler and Langer (2021).\n\nLemma 20 Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Let $R \\in \\mathbb{N}$. Let $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^{d}$ with\n\n$$\nb^{(i)}-a^{(i)} \\geq \\frac{2}{R} \\text { for all } i \\in\\{1, \\ldots, d\\}\n$$\n\nand let\n\n$$\nK_{1 / R}=\\left\\{x \\in \\mathbb{R}^{d}: x^{(i)} \\notin\\left[a^{(i)}, a^{(i)}+1 / R\\right) \\cup\\left(b^{(i)}-1 / R, b^{(i)}\\right)\\right.\n$$\n\n$$\n\\text { for all } i \\in\\{1, \\ldots, d\\}\\}\n$$\n\na) Then the network\n\n$$\n\\begin{aligned}\n\\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}(x)=\\sigma\\left(1-R \\cdot \\sum_{i=1}^{d}\\right. & \\left(\\sigma\\left(a^{(i)}+\\frac{1}{R}-x^{(i)}\\right)\\right. \\\\\n& \\left.+\\sigma\\left(x^{(i)}-b^{(i)}+\\frac{1}{R}\\right)\\right)\n\\end{aligned}\n$$\n\nof the class $\\mathcal{F}(2,2 d)$ satisfies the weight constraint\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}}\\right\\|_{\\infty} \\leq \\max \\left\\{\\|\\mathbf{a}\\|_{\\infty}+\\frac{1}{R},\\|\\mathbf{b}\\|_{\\infty}+\\frac{1}{R}, R\\right\\}\n$$\n\nas well as\n\n$$\n\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}}\\right)_{i, j>0}^{(0)}\\right\\|_{\\infty}=1, \\quad\\left(\\mathbf{v}_{\\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}}\\right)_{1,0}^{(2)}=0 \\quad \\text { and } \\quad\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}}\\right)_{1, i>0}^{(2)}\\right\\|_{\\infty}=1\n$$\n\nFor $x \\in K_{1 / R}$ we have\n\n$$\n\\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}(x)=\\mathbb{1}_{[\\mathbf{a}, \\mathbf{b})}(x)\n$$\n\nand\n\n$$\n\\left|\\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}(x)-\\mathbb{1}_{[\\mathbf{a}, \\mathbf{b})}(\\mathbf{x})\\right| \\leq 1\n$$\n\nfor $x \\in \\mathbb{R}^{d}$.\nb) Let $|s| \\leq R$. Then the network\n\n$$\n\\begin{aligned}\n\\hat{f}_{\\text {test }}(x, \\mathbf{a}, \\mathbf{b}, s)=\\sigma & \\left(\\hat{f}_{i d}(s)-R^{2} \\cdot \\sum_{i=1}^{d}\\left(\\sigma\\left(a^{(i)}+\\frac{1}{R}-x^{(i)}\\right)\\right.\\right. \\\\\n& \\left.\\left.+\\sigma\\left(x^{(i)}-b^{(i)}+\\frac{1}{R}\\right)\\right)\\right) \\\\\n& -\\sigma\\left(-\\hat{f}_{i d}(s)-R^{2} \\cdot \\sum_{i=1}^{d}\\left(\\sigma\\left(a^{(i)}+\\frac{1}{R}-x^{(i)}\\right)\\right.\\right. \\\\\n& \\left.\\left.\\quad+\\sigma\\left(x^{(i)}-b^{(i)}+\\frac{1}{R}\\right)\\right)\\right)\n\\end{aligned}\n$$\n\nof the class $\\mathcal{F}(2,2 \\cdot(2 d+2))$ satisfies the weight constraint\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {test }}}\\right\\|_{\\infty} \\leq R^{2}\n$$\n\nas well as\n\n$$\n\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {test }}}\\right)_{i, j>0}^{(0)}\\right\\|_{\\infty}=1, \\quad\\left(\\mathbf{v}_{\\hat{f}_{\\text {test }}}\\right)_{1,0}^{(2)}=0 \\quad \\text { and } \\quad\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {test }}}\\right)_{1, i>0}^{(2)}\\right\\|_{\\infty}=1\n$$\n\nFor $x \\in K_{1 / R} \\hat{f}_{\\text {test }}(x, \\mathbf{a}, \\mathbf{b}, s)$ satisfies\n\n$$\n\\hat{f}_{\\text {test }}(x, \\mathbf{a}, \\mathbf{b}, s)=s \\cdot \\mathbb{1}_{[\\mathbf{a}, \\mathbf{b})}(x)\n$$\n\nand\n\n$$\n\\left|\\hat{f}_{\\text {test }}(x, \\mathbf{a}, \\mathbf{b}, s)-s \\cdot \\mathbb{1}_{[\\mathbf{a}, \\mathbf{b})}(x)\\right| \\leq|s|\n$$\n\nfor $x \\in \\mathbb{R}^{d}$.\nProof. The weight constraints can easily be seen in the definition of $\\hat{f}_{\\text {ind, }[\\mathbf{a}, \\mathbf{b})}$ and $\\hat{f}_{\\text {test }}$, the proof of the approximation bounds can be found in the proof of Lemma 6 in Kohler and Langer (2021).\n\nLemma 21 Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Let $R>0, B \\in \\mathbb{N}$ and\n\n$$\n\\hat{f}_{i n d,[j, \\infty)}(z)=R \\cdot \\sigma(z-j)-R \\cdot \\sigma\\left(z-j-\\frac{1}{R}\\right) \\in \\mathcal{F}(1,2)\n$$\n\nfor $j \\in\\{1, \\ldots, B\\}$. Then the neural network\n\n$$\n\\hat{f}_{\\text {trunc }}(z)=\\sum_{j=1}^{B} \\hat{f}_{i n d,[j, \\infty)}(z) \\in \\mathcal{F}(1,2 B)\n$$\n\nsatisfies\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {trunc }}}\\right\\|_{\\infty} \\leq \\max \\left\\{R, B+\\frac{1}{R}\\right\\}\n$$\n\nmore specifically the network has no offset in its last layer, i.e. $\\left(\\mathbf{v}_{\\hat{f}_{\\text {trunc }}}\\right)_{1,0}^{(1)}=0$, and satisfies $\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {trunc }}}\\right)_{i, j>0}^{(0)}\\right\\|_{\\infty}=1$ and $\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {trunc }}}\\right)_{i, j>0}^{(1)}\\right\\|_{\\infty}=R$.\nFurther, $\\hat{f}_{\\text {trunc satisfies }}$\n\n$$\n\\hat{f}_{\\text {trunc }}(z)=\\lfloor z\\rfloor\n$$\n\nfor $z \\in[0, B+1)$ and $\\min \\{|z-j|: j \\in \\mathbb{N}\\} \\geq 1 / R$.\nProof. The weight constraints can easily be derived from the definition of $\\hat{f}_{\\text {trunc }}$, the proof of the approximation bounds can be found in the proof of Lemma 13 in Kohler and Langer (2021).\n\nIn the proof of Lemma 14 every function of $\\boldsymbol{\\phi}_{1,2 M^{d}+1}$ is computed by a neural network. In particular, the indicator functions in $\\boldsymbol{\\phi}_{2, j}, \\phi_{3, j}^{(\\mathbf{I})}$ and $\\phi_{4, j}^{(\\mathbf{I})}\\left(j \\in\\left\\{1, \\ldots, M^{d}\\right\\}, \\mathbf{l} \\in\\right.$ $\\left.\\mathbb{N}_{0}^{d},\\|\\mathbf{l}\\|_{1} \\leq q\\right)$ are computed by Lemma 20 a ), while we apply the identity network to shift the computed values from the previous step. The functions $\\phi_{3, M^{d}+j}^{(\\mathbf{I})}$ and $\\phi_{4, M^{d}+j}^{(\\mathbf{I})}$\n\nare then computed according to their definition above Lemma 13, while we again use the identity network to shift values in the next hidden layers. For the functions $\\boldsymbol{\\phi}_{5, M^{d}+j}$ and $\\phi_{6, M^{d}+j}^{(\\mathrm{I})}$ we use the network of Lemma 20 b) to successively compute $\\left(C_{\\mathcal{P}_{2}}(x)\\right)_{l e f t}$ and the derivatives on the cube $C_{\\mathcal{P}_{2}}(x)$. The final Taylor polynomial in $\\phi_{1,2 M^{d}+1}$ is then approximated with the help of Lemma 19.\nProof of Lemma 14. In the first step of the proof we describe how $\\phi_{1,2 M^{d}+1}$ of Lemma 13 can be approximated by neural networks. In the construction we will use the network $\\hat{f}_{\\text {ind, }[\\mathbf{a}, \\mathbf{b})} \\in \\mathcal{F}(2,2 d)$ and the network $\\hat{f}_{\\text {test }} \\in \\mathcal{F}(2,2 \\cdot(2 d+2))$ of Lemma 20. Here we set $R=B_{M}=M^{2 p+2}$ in Lemma 20, such that the weights of the network satisfy the constraints\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{i n d}}\\right\\|_{\\infty} \\leq M^{2 p+2} \\quad \\text { and } \\quad\\left\\|\\mathbf{v}_{\\hat{f}_{t e s t}}\\right\\|_{\\infty} \\leq M^{4 p+4}\n$$\n\nFor some vector $\\mathbf{v} \\in \\mathbb{R}^{d}$ we set\n\n$$\n\\mathbf{v} \\cdot \\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}(x)=\\left(v^{(1)} \\cdot \\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}(x), \\ldots, v^{(d)} \\cdot \\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})}(x)\\right)\n$$\n\nFurthermore we use the networks\n\n$$\n\\hat{f}_{\\text {trunc }, i} \\in \\mathcal{F}\\left(1,2 \\cdot\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)\\right), \\quad\\left(i \\in\\left\\{1, \\ldots, M^{d}-1\\right\\}\\right)\n$$\n\nof Lemma 21. Here we choose\n\n$$\nR=R_{M, i}=\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)^{M^{d}-i-1} \\quad \\text { and } \\quad B=4+2\\left\\lceil e^{d}\\right\\rceil\n$$\n\nin Lemma 21, which implies\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {trunc }}}\\right\\|_{\\infty} \\leq \\max \\left\\{R, B+\\frac{1}{R}\\right\\} \\leq 4+2\\left\\lceil e^{d}\\right\\rceil+\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)^{M^{d}}\n$$\n\nTo compute the final Taylor polynomial we use the network\n\n$$\n\\hat{f}_{p} \\in \\mathcal{F}\\left(B_{M, p} \\cdot\\left\\lceil\\log _{2}(\\max \\{q+1,2\\})\\right\\rceil, 18 \\cdot(q+1) \\cdot\\binom{d+q}{d}\\right)\n$$\n\nfrom Lemma 19 satisfying\n\n$$\n\\begin{aligned}\n& \\left\\|\\mathbf{v}_{\\hat{f}_{p}}\\right\\|_{\\infty} \\leq 4 \\cdot \\bar{r}(p) \\cdot 4^{2(q+1)} \\cdot\\left(2 \\cdot \\max \\left\\{\\|f\\|_{C^{q}\\left([-a, a]^{d}\\right)}, a\\right\\} \\cdot e^{\\left(M^{d}-1\\right)}\\right. \\\\\n& \\left.+\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right) \\cdot\\left(M^{d}-1\\right) \\cdot e^{\\left(M^{d}-2\\right)}\\right)^{2(q+1)}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& \\left|\\hat{f}_{p}\\left(\\mathbf{z}, y_{1}, \\ldots, y_{\\binom{d+q}{q}}\\right)-p\\left(\\mathbf{z}, y_{1}, \\ldots, y_{\\binom{d+q}{q}}\\right)\\right| \\\\\n& \\leq c_{41} \\cdot\\left(6+2\\left\\lceil e^{d}\\right\\rceil\\right)^{4(q+1)} \\cdot \\bar{r}(p) \\cdot\\left(\\max \\left\\{\\|f\\|_{C^{q}\\left([-a, a]^{d}\\right)}, a\\right\\}\\right)^{4(q+1)}\n\\end{aligned}\n$$\n\n$$\n\\cdot M^{d \\cdot 4 \\cdot(q+1)} \\cdot e^{4(q+1) \\cdot\\left(M^{d}-1\\right)} \\cdot 4^{-B_{M, p}}\n$$\n\nfor all $z^{(1)}, \\ldots, z^{(d)}, y_{1}, \\ldots, y_{\\binom{d+a}{d}}$, contained in\n\n$$\n\\begin{gathered}\n{\\left[-2 \\cdot \\max \\left\\{\\|f\\|_{C^{q}\\left([-a, a]^{d}\\right)}, a\\right\\} \\cdot e^{\\left(M^{d}-1\\right)}+\\left(4+2\\left[e^{d}\\right]\\right) \\cdot\\left(M^{d}-1\\right) \\cdot e^{\\left(M^{d}-2\\right)}\\right.} \\\\\n\\left.2 \\cdot \\max \\left\\{\\|f\\|_{C^{q}\\left([-a, a]^{d}\\right)}, a\\right\\} \\cdot e^{\\left(M^{d}-1\\right)}+\\left(4+2\\left[e^{d}\\right]\\right) \\cdot\\left(M^{d}-1\\right) \\cdot e^{\\left(M^{d}-2\\right)}\\right]\n\\end{gathered}\n$$\n\nwhere\n\n$$\nR=B_{M, p}=\\left[\\log _{4}\\left(M^{2 p+4 \\cdot d \\cdot(q+1)} \\cdot e^{4 \\cdot(q+1) \\cdot\\left(M^{d}-1\\right)}\\right)\\right]\n$$\n\nA polynomial of degree zero is treated as a polynomial of degree 1 , where we choose $r_{i}=0$ for all coefficients greater than zero. Thus we substitute $\\log _{2}(q+1)$ by $\\log _{2}(\\max \\{q+$ $1,2\\})$ in the definition of $L$ in Lemma 19. To compute $\\phi_{1, j}, \\phi_{2, j}, \\phi_{3, j}^{(\\mathrm{I})}$ and $\\phi_{4, j}^{(\\mathrm{I})}$ for $j \\in$ $\\left\\{0, \\ldots, M^{d}\\right\\}$ and each $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{l}\\|_{1} \\leq q$ we use the networks\n\n$$\n\\begin{aligned}\n& \\hat{\\phi}_{1,0}=\\left(\\hat{\\phi}_{1,0}^{(1)}, \\ldots, \\hat{\\phi}_{1,0}^{(d)}\\right)=x \\\\\n& \\hat{\\phi}_{2,0}=\\left(\\hat{\\phi}_{2,0}^{(1)}, \\ldots, \\hat{\\phi}_{2,0}^{(d)}\\right)=\\mathbf{0} \\\\\n& \\hat{\\phi}_{3,0}^{(\\mathrm{I})}=0 \\text { and } \\hat{\\phi}_{4,0}^{(\\mathrm{I})}=0\n\\end{aligned}\n$$\n\nfor $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{l}\\|_{1} \\leq q$. For $j \\in\\left\\{1, \\ldots, M^{d}\\right\\}$ we set\n\n$$\n\\begin{aligned}\n& \\tilde{\\phi}_{1, j}=\\tilde{f}_{i d}^{2}\\left(\\tilde{\\phi}_{1, j-1}\\right) \\\\\n& \\tilde{\\phi}_{2, j}=\\left(C_{j, 1}\\right)_{l e f t} \\cdot \\tilde{f}_{i n d, C_{j, 1}}\\left(\\tilde{\\phi}_{1, j-1}\\right)+\\tilde{f}_{i d}^{2}\\left(\\tilde{\\phi}_{2, j-1}\\right) \\\\\n& \\hat{\\phi}_{3, j}^{(\\mathrm{I})}=\\left(\\partial^{\\mathrm{I}} f\\right)\\left(\\left(C_{j, 1}\\right)_{l e f t}\\right) \\cdot \\tilde{f}_{i n d, C_{j, 1}}\\left(\\tilde{\\phi}_{1, j-1}\\right)+\\tilde{f}_{i d}^{2}\\left(\\hat{\\phi}_{3, j-1}^{(\\mathrm{I})}\\right) \\\\\n& \\hat{\\phi}_{4, j}^{(\\mathrm{I})}=b_{j}^{(\\mathrm{I})} \\cdot \\tilde{f}_{i n d, C_{j, 1}}\\left(\\tilde{\\phi}_{1, j-1}\\right)+\\tilde{f}_{i d}^{2}\\left(\\hat{\\phi}_{4, j-1}^{(\\mathrm{I})}\\right)\n\\end{aligned}\n$$\n\nfor $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ with $\\|\\mathbf{l}\\|_{1} \\leq q$.\nIt is easy to see that this parallelized network needs $2 M^{d}$ hidden layers and $2 d+d$. $(2 d+2)+2 \\cdot\\binom{d+a}{d} \\cdot(2 d+2)$ neurons per layer, where we have used the fact that we have $\\binom{d+a}{d}$ different vectors $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ satisfying $\\|\\mathbf{l}\\|_{1} \\leq q$.\nTo compute $\\phi_{1, M^{d}+j}, \\phi_{5, M^{d}+j}$ and $\\phi_{6, M^{d}+j}^{(\\mathrm{I})}$ for $j \\in\\left\\{1, \\ldots, M^{d}\\right\\}$ and $\\phi_{2, M^{d}+j}, \\phi_{3, M^{d}+j}^{(\\mathrm{I})}$ and $\\phi_{4, M^{d}+j}^{(\\mathrm{I})}$ for $j \\in\\left\\{1, \\ldots, M^{d}-1\\right\\}$ we use the networks\n\n$$\n\\begin{aligned}\n& \\tilde{\\phi}_{1, M^{d}+j}=\\tilde{f}_{i d}^{2}\\left(\\tilde{\\phi}_{1, M^{d}+j-1}\\right), \\quad j \\in\\left\\{1, \\ldots, M^{d}\\right\\} \\\\\n& \\tilde{\\phi}_{2, M^{d}+j}=\\tilde{f}_{i d}^{2}\\left(\\tilde{\\phi}_{2, M^{d}+j-1}+\\tilde{\\mathbf{v}}_{j+1}\\right)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& \\hat{\\phi}_{3, M^{d}+j}^{(\\mathbf{I})}=\\hat{f}_{i d}\\left(\\hat{f}_{i d}\\left(\\sum_{\\substack{\\mathbf{s} \\in \\mathbb{N}_{0}^{d} \\\\\n\\mathbf{s} \\in \\mathbb{N}_{0}^{d}} \\frac{\\hat{\\phi}_{3, M^{d}+j-1}^{(\\mathbf{I}+\\mathbf{s})}}{\\mathbf{s}!} \\cdot\\left(\\widehat{\\mathbf{v}}_{j+1}\\right)^{\\mathbf{s}}\\right)\\right. \\\\\n& +\\left(\\hat{f}_{\\text {trunc }, j}\\left(\\left(4+2 \\cdot\\left\\lceil e^{d}\\right]\\right) \\cdot \\hat{\\phi}_{4, M^{d}+j-1}^{(\\mathbf{I})}\\right)\\right. \\\\\n& \\left.-\\left\\lceil e^{d}\\right\\rceil-2\\right) \\cdot c_{36} \\cdot\\left(\\frac{2 a}{M^{2}}\\right)^{p-\\|\\mathbf{I}\\|_{1}} \\text {, } \\\\\n& \\hat{\\phi}_{4, M^{d}+j}^{(\\mathbf{I})}=\\hat{f}_{i d}\\left(\\hat{f}_{i d}\\left(\\left(4+2 \\cdot\\left\\lceil e^{d}\\right]\\right) \\cdot \\hat{\\phi}_{4, M^{d}+j-1}^{(\\mathbf{I})}\\right)\\right. \\\\\n& \\left.-\\hat{f}_{\\text {trunc }, j}\\left(\\left(4+2 \\cdot\\left\\lceil e^{d}\\right]\\right) \\cdot \\hat{\\phi}_{4, M^{d}+j-1}^{(\\mathbf{I})}\\right)\\right)\n\\end{aligned}\n$$\n\nfor $j \\in\\left\\{1, \\ldots, M^{d}-1\\right\\}$.\nFurther we set\n\n$$\n\\begin{aligned}\n\\hat{\\phi}_{5, M^{d}+j}^{(k)}= & \\hat{f}_{t e s t}\\left(\\hat{\\boldsymbol{\\phi}}_{1, M^{d}+j-1}, \\hat{\\boldsymbol{\\phi}}_{2, M^{d}+j-1}\\right. \\\\\n& \\left.\\hat{\\boldsymbol{\\phi}}_{2, M^{d}+j-1}+\\frac{2 a}{M^{2}} \\cdot \\mathbf{1}, \\hat{\\phi}_{2, M^{d}+j-1}^{(k)}\\right) \\\\\n& +\\hat{f}_{i d}^{2}\\left(\\hat{\\phi}_{5, M^{d}+j-1}^{(k)}\\right)\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\hat{\\phi}_{6, M^{d}+j}^{(\\mathbf{I})}= & \\hat{f}_{t e s t}\\left(\\hat{\\boldsymbol{\\phi}}_{1, M^{d}+j-1}, \\hat{\\boldsymbol{\\phi}}_{2, M^{d}+j-1}\\right. \\\\\n& \\left.\\hat{\\boldsymbol{\\phi}}_{2, M^{d}+j-1}+\\frac{2 a}{M^{2}} \\cdot \\mathbf{1}, \\hat{\\phi}_{3, M^{d}+j-1}^{(1)}\\right) \\\\\n& +\\hat{f}_{i d}^{2}\\left(\\hat{\\phi}_{6, M^{d}+j-1}^{(1)}\\right)\n\\end{aligned}\n$$\n\nwhere $\\hat{\\boldsymbol{\\phi}}_{5, M^{d}}=\\left(\\hat{\\phi}_{5, M^{d}}^{(1)}, \\ldots, \\hat{\\phi}_{5, M^{d}}^{(d)}\\right)=\\mathbf{0}$ and $\\hat{\\phi}_{6, M^{d}}^{(\\mathbf{I})}=0$ for each $\\mathbf{l} \\in \\mathbb{N}_{0}^{d}$ with $\\left\\|\\mathbf{l}_{1}\\right\\| \\leq q$.\nAgain it is easy to see, that this parallelized and composed network needs $4 M^{d}$ hidden layers and has width $r$ with\n\n$$\nr=10 d+4 d^{2}+2 \\cdot\\binom{d+q}{d} \\cdot\\left(2 \\cdot\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)+5+2 d\\right)\n$$\n\nChoose $\\mathbf{l}_{1}, \\ldots, \\mathbf{l}_{\\binom{d+q}{d}}$ such that\n\n$$\n\\left\\{\\mathbf{l}_{1}, \\ldots, \\mathbf{l}_{\\binom{d+q}{d}}\\right\\}=\\left\\{\\mathbf{s} \\in \\mathbb{N}_{0}^{d}:\\|\\mathbf{s}\\|_{1} \\leq q\\right\\}\n$$\n\nholds. The value of $\\phi_{1,2 M^{d}+1}$ can then be computed by\n\n$$\n\\hat{\\phi}_{1,2 M^{d}+1}=\\hat{f}_{p}\\left(\\mathbf{z}, y_{1}, \\ldots, y_{\\binom{d+q}{d}}\\right)\n$$\n\nwhere\n\n$$\n\\mathbf{z}=\\hat{\\phi}_{1,2 M^{d}}-\\hat{\\phi}_{5,2 M^{d}}\n$$\n\nand\n\n$$\ny_{v}=\\hat{\\phi}_{6,2 M^{d}}^{(\\mathbf{l}_{v})}\n$$\n\nfor $v \\in\\left\\{1, \\ldots,\\binom{d+q}{d}\\right\\}$. The coefficients $r_{1}, \\ldots, r_{\\binom{d+q}{d}}$ in Lemma 19 are chosen as\n\n$$\nr_{i}=\\frac{1}{\\mathbf{l}_{i}^{1}}, \\quad i \\in\\left\\{1, \\ldots,\\binom{d+q}{d}\\right\\}\n$$\n\ni.e. $\\bar{r}(p) \\leq 1$. The final network $\\hat{\\phi}_{1,2 M^{d}+1}$ is then contained in the class\n\n$$\n\\mathcal{F}\\left(4 M^{d}+B_{M, p} \\cdot\\left\\lceil\\log _{2}(\\max \\{q+1,2\\})\\right], r\\right)\n$$\n\nwith\n\n$$\n\\begin{aligned}\nr=\\max & \\left\\{10 d+4 d^{2}+2 \\cdot\\binom{d+q}{d} \\cdot\\left(2 \\cdot\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)+5+2 d\\right)\\right. \\\\\n& \\left.18 \\cdot(q+1) \\cdot\\binom{d+q}{d}\\right\\}\n\\end{aligned}\n$$\n\nand we set\n\n$$\n\\hat{f}_{\\text {deep }, \\mathcal{P}_{2}}(x)=\\hat{\\phi}_{1,2 M^{d}+1}\n$$\n\nNote that the repeated composition of $\\hat{f}_{i d}$ and $\\hat{f}_{i n d, C_{j, 1}}$ does not affect any weight constraints, since both networks have no offset in their respective output layers and the weights used in the input and output (i.e. last) layers are bounded by 1 respectively, such that Lemma 15 c ) is applicable. This also holds for the repeated composition of $\\hat{f}_{i d}$ and $\\hat{f}_{\\text {test }}$. Since $\\left\\|\\mathbf{v}_{\\hat{f}_{i d}}\\right\\|_{\\infty}=1$, we have $\\left\\|\\mathbf{v}_{\\hat{\\phi}_{1, j}}\\right\\|_{\\infty}=1$ for $j \\in\\left\\{1, \\ldots, 2 M^{d}\\right\\}$.\nFrom Lemma 20, $\\left(\\left(C_{j, 1}\\right)_{\\text {left }}\\right)<a$, the conditions on $M$ and since by definition $\\tilde{\\mathbf{v}}_{j+1}$ has entries in $\\left\\{0, \\frac{2 a}{M^{d}}\\right\\}$, we conclude that $\\hat{\\phi}_{2, j}$ satisfies the weight constraint $\\left\\|\\mathbf{v}_{\\hat{\\phi}_{2, j}}\\right\\|_{\\infty} \\leq$ $\\max \\left\\{a+\\frac{1}{y_{M}^{d}}, B_{M}\\right\\} \\leq M^{2 p+2}$ for $j \\in\\left\\{1, \\ldots, 2 M^{d}\\right\\}$.\nAnalogously, we can derive the weight constraints for $\\hat{\\phi}_{3, j}^{(\\mathbf{l})}$ and $\\hat{\\phi}_{4, j}^{(\\mathbf{l})}$ :\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{\\phi}_{3, j}^{(\\mathbf{l})}}\\right\\|_{\\infty} \\leq \\max \\left\\{\\|f\\|_{C^{q}\\left([-a, a]^{d}\\right)}, a+\\frac{1}{B_{M}}, B_{M}\\right\\} \\leq M^{2 p+2}+c_{42}(f)\n$$\n\nrespectively for $\\hat{\\phi}_{4, j}^{(\\mathbf{l})}$, we have\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{\\phi}_{4, j}^{(\\mathbf{l})}}\\right\\|_{\\infty} \\leq \\max \\left\\{a+\\frac{1}{B_{M}}, B_{M}\\right\\} \\leq M^{2 p+2} \\quad \\text { and }\n$$\n\n$\\left\\|\\left(\\mathbf{v}_{\\hat{\\phi}_{4, j}^{(1)}}\\right)_{1, i>0}^{(0)}\\right\\|_{\\infty}=1,\\left(\\mathbf{v}_{\\hat{\\phi}_{4, j}^{(1)}}\\right)_{1,0}^{(2 j)}=0$ and $\\left\\|\\left(\\mathbf{v}_{\\hat{\\phi}_{4, j}^{(1)}}\\right)_{1, i>0}^{(2 j)}\\right\\|_{\\infty} \\leq 1$, where we have used Lemma 20 and that by definition\n\n$$\nb_{i}^{(\\mathrm{I})} \\in[0,1]\n$$\n\nNote that by construction $\\left(\\mathbf{v}_{\\hat{\\phi}_{2,2 M^{d}}^{(1)}}\\right)_{1,0}^{(4 M^{d})}=0$ and $\\left\\|\\left(\\mathbf{v}_{\\hat{\\phi}_{2,2 M^{d}}}\\right)^{(4 M^{d})}\\right\\|_{\\infty} \\leq 1$. Since we have $\\left\\|\\left(\\mathbf{v}_{\\hat{\\phi}_{4, j}^{(1)}}\\right)_{1, i>0}^{(2 M^{d})}\\right\\|_{\\infty} \\leq 1,\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{f r u n c}}\\right)_{1, i>0}^{(0)}\\right\\|_{\\infty}=1$ and $\\left(\\mathbf{v}_{\\hat{\\phi}_{4, j}^{(1)}}\\right)_{1,0}^{(2 j)}=0$ by Lemma 15 c$)$ the composition of $\\hat{f}_{\\text {trunc }}$ and $\\hat{\\phi}_{4, j}^{(\\mathrm{I})}$ does not affect the weight constraints. Thus $\\hat{\\phi}_{3, M^{d}+j}^{(\\mathrm{I})}$ and $\\hat{\\phi}_{4, M^{d}+j}^{(\\mathrm{I})}$ satisfy the constraint given in (40), i.e.\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{\\phi}_{3, M^{d}+j}^{(1)}}\\right\\|_{\\infty}=\\left\\|\\mathbf{v}_{\\hat{\\phi}_{4, M^{d}+j}^{(1)}}\\right\\|_{\\infty} \\leq 4+2\\left\\lceil e^{d}\\right\\rceil+\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)^{M^{d}}+\\left(c_{36}+1\\right) \\cdot M^{2 p+2}\n$$\n\nBy Lemma 20 a) and the previously stated weight constraints for $\\hat{\\phi}_{1, M^{d}+j}, \\hat{\\phi}_{2, M^{d}+j}$ and $\\hat{\\phi}_{3, M^{d}+j}$, we have\n\n$$\n\\begin{aligned}\n& \\left\\|\\mathbf{v}_{\\hat{\\phi}_{5, M^{d}+j}}\\right\\|_{\\infty} \\leq M^{4 p+4},\\left\\|\\left(\\mathbf{v}_{\\hat{\\phi}_{5, M^{d}+j}}\\right)^{\\left(2 M^{d}+2 j\\right)}\\right\\|_{\\infty} \\leq 1, \\quad\\left(\\mathbf{v}_{\\hat{\\phi}_{5, M^{d}+j}}\\right)_{1,0}^{\\left(2 M^{d}+2 j\\right)}=0 \\quad \\text { and } \\\\\n& \\left\\|\\mathbf{v}_{\\hat{\\phi}_{6, M^{d}+j}}\\right\\|_{\\infty} \\leq \\max \\left\\{M^{4 p+4},\\left\\|\\mathbf{v}_{\\hat{\\phi}_{3, M^{d}+j}^{(1)}}\\right\\|_{\\infty}\\right\\} \\leq 4+2\\left\\lceil e^{d}\\right\\rceil+\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)^{M^{d}}+\\left(c_{36}+1\\right) \\cdot M^{4 p+4} \\\\\n& \\left\\|\\left(\\mathbf{v}_{\\hat{\\phi}_{6, M^{d}+j}}\\right)^{\\left(2 M^{d}+2 j\\right)}\\right\\|_{\\infty} \\leq 1, \\quad\\left(\\mathbf{v}_{\\hat{\\phi}_{6, M^{d}+j}}\\right)_{1,0}^{\\left(2 M^{d}+2 j\\right)}=0\n\\end{aligned}\n$$\n\nTo derive the weight constraints for the final network $\\hat{f}_{\\text {deep }, \\mathcal{P}_{2}}(x)$, note that using the previously stated constraints for the weights of $\\hat{\\phi}_{1,2 M^{d}}, \\hat{\\phi}_{5,2 M^{d}}$ and $\\hat{\\phi}_{6,2 M^{d}}$ as well as Lemma 19, we know that the composition of $\\hat{f}_{p}$ and $\\hat{\\phi}_{1,2 M^{d}}-\\hat{\\phi}_{5,2 M^{d}}$ fulfills the conditions of Lemma 15 c ). This results in\n\n$$\n\\begin{aligned}\n& \\left\\|\\mathbf{v}_{\\hat{f}_{\\text {deep }, \\mathcal{P}_{2}}}\\right\\|_{\\infty} \\leq \\max \\left\\{4 \\cdot 4^{2(q+1)} \\cdot\\left(2 \\cdot \\max \\left\\{\\|f\\|_{C^{q}\\left([-a, a]^{d}}\\right)}, a\\right\\} \\cdot e^{\\left(M^{d}-1\\right)}+\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)\\right.\\right. \\\\\n& \\left.\\cdot\\left(M^{d}-1\\right) \\cdot e^{\\left(M^{d}-2\\right)}\\right)^{2(q+1)},\\left(c_{36}+1\\right) \\cdot M^{4 p+4}\\right\\} \\\\\n& \\leq\\left(c_{43}(f) \\cdot e^{M^{d}-1} \\cdot\\left(6+2\\left\\lceil e^{d}\\right\\rceil\\right) \\cdot M^{d}\\right)^{2(q+1)}+\\left(c_{36}+1\\right) \\cdot M^{4 p+4} \\\\\n& \\leq e^{c_{44}(f) \\cdot(p+1) \\cdot M^{d}}\n\\end{aligned}\n$$\n\nThe rest of the proof follows as in Kohler and Langer (2021).",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 26,
      "text": "# A.2.3. Key step 3: Approximating $w_{\\mathcal{P}_{2}}(x) \\cdot f(x)$ by deep neural networks \n\nIn order to approximate $f(x)$ in supremum norm, a neural network that approximates $w_{\\mathcal{P}_{2}}(x) \\cdot f(x)$, where $w_{\\mathcal{P}_{2}}(x)$ is defined as in (33), is required. The construction of such a network is given in the following three results.\n\nLemma 22 Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Let $1 \\leq a<\\infty$ and $M \\in \\mathbb{N}_{0}$ sufficiently large (independent of the size of $a$, but\n\n$$\n\\begin{aligned}\n& M^{2 p} \\geq 2^{4(q+1)} \\cdot \\max \\left\\{c_{45}\\left(6+2\\left\\lceil e^{d}\\right\\rceil\\right)^{4(q+1)}, c_{36} \\cdot e^{d}\\right\\} \\\\\n&\\left.\\cdot\\left(\\max \\left\\{a,\\|f\\|_{C^{q}\\left([-a, a]^{d}}\\right\\}\\right)^{4(q+1)}\\right.\n\\end{aligned}\n$$\n\nmust hold). Let $p=q+s$ for some $q \\in \\mathbb{N}_{0}, s \\in(0,1]$ and let $C>0$. Let $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ be $a(p, C)$-smooth function and let $w_{\\mathcal{P}_{2}}$ be defined as in (33). Then there exists a network\n\n$$\n\\hat{f} \\in \\mathcal{F}(L, r)\n$$\n\nwith\n\n$$\n\\begin{aligned}\nL= & 5 M^{d}+\\left\\lceil\\log _{4}\\left(M^{2 p+4 \\cdot d \\cdot(q+1)} \\cdot e^{4 \\cdot(q+1) \\cdot\\left(M^{d}-1\\right)}\\right)\\right\\rceil \\cdot\\left\\lceil\\log _{2}(\\max \\{q, d\\}+1)\\right\\rceil \\\\\n& +\\left\\lceil\\log _{4}\\left(M^{2 p}\\right)\\right\\rceil\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{gathered}\nr=\\max \\left\\{10 d+4 d^{2}+2 \\cdot\\binom{d+q}{d} \\cdot\\left(2 \\cdot\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)+5+2 d\\right)\\right. \\\\\n\\left.18 \\cdot(q+1) \\cdot\\binom{d+q}{d}\\right\\}+6 d^{2}+20 d+2\n\\end{gathered}\n$$\n\nwhich satisfies\n\n$$\n\\left\\|\\mathbf{v}_{f}\\right\\|_{\\infty} \\leq e^{c_{36}(f) \\cdot(p+1) \\cdot M^{d}}\n$$\n\nsuch that\n\n$$\n\\left|\\hat{f}(x)-w_{\\mathcal{P}_{2}}(x) \\cdot f(x)\\right| \\leq c_{47} \\cdot\\left(\\max \\left\\{2 a,\\|f\\|_{C^{q}\\left([-a, a]^{d}\\right)}\\right\\}\\right)^{4(q+1)} \\cdot \\frac{1}{M^{2 p}}\n$$\n\nfor $x \\in[-a, a)^{d}$.\nFurther auxiliary lemmata are required to show this result. First, it is shown that each weight $w_{\\mathcal{P}_{2}}(x)$ can also be approximated by a very deep neural network.\n\nLemma 23 Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Let $1 \\leq a<\\infty$ and $M \\geq 4^{4 d+1} \\cdot d$. Let $\\mathcal{P}_{2}$ be the partition defined in (29) and let $w_{\\mathcal{P}_{2}}(x)$ be defined by (33). Then there exists a neural network\n\n$$\n\\hat{f}_{w_{\\mathcal{P}_{2}}, \\text { deep }} \\in \\mathcal{F}(L, r)\n$$\n\nwith\n\n$$\nL=4 M^{d}+1+\\left\\lceil\\log _{4}\\left(M^{2 p}\\right)\\right\\rceil \\cdot\\left\\lceil\\log _{2}(d)\\right\\rceil \\quad \\text { and } \\quad r=\\max \\left\\{18 d, 4 d^{2}+10 d\\right\\}\n$$\n\nwhich satisfies\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{w_{P_{2}, \\text { deep }}}}\\right\\|_{\\infty} \\leq M^{4 p+4}, \\quad\\left(\\mathbf{v}_{\\hat{f}_{w_{P_{2}}, \\text { deep }}}\\right)_{1,0}^{(L)}=0 \\quad \\text { and } \\quad\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{w_{P_{2}}, \\text { deep }}}\\right)_{1, j>0}^{(L)}\\right\\|_{\\infty} \\leq 4^{3 d+1}\n$$\n\nsuch that\n\n$$\n\\left|\\hat{f}_{w_{P_{2}, \\text { deep }}}(x)-w_{P_{2}}(x)\\right| \\leq 4^{4 d+1} \\cdot d \\cdot \\frac{1}{M^{2 p}}\n$$\n\nfor $x \\in \\bigcup_{i \\in\\left\\{1, \\ldots, M^{2 d}\\right\\}}\\left(C_{i, 2}\\right)_{1 / M^{2 p+2}}^{0}$ and\n\n$$\n\\left|\\hat{f}_{w_{P_{2}, \\text { deep }}}(x)\\right| \\leq 1\n$$\n\nfor $x \\in[-a, a)^{d}$.\nProof. The first $4 M^{d}$ hidden layers of $\\hat{f}_{w_{P_{2}}}$ compute the value of\n\n$$\n\\left(C_{P_{2}}(x)\\right)_{\\text {left }}\n$$\n\nusing $\\hat{\\boldsymbol{\\phi}}_{5,2 M^{d}}$ of Lemma 14 (with $d \\cdot(2 \\cdot(2 d+2)+2)+2 d$ neurons per layer) and shift the value of $x$ in the next hidden layer using the network $\\hat{f}_{i d}^{4 M^{d}}$. As stated in the proof of Lemma 14, $\\hat{\\boldsymbol{\\phi}}_{5,2 M^{d}}$ has the following weight constraints $\\left\\|\\mathbf{v}_{\\hat{\\boldsymbol{\\phi}}_{5, M^{d}+j}}\\right\\|_{\\infty} \\leq$ $M^{4 p+4}, \\quad\\left\\|\\left(\\mathbf{v}_{\\hat{\\phi}_{5, M^{d}+j}}\\right)^{\\left(2 M^{d}+2 j\\right)}\\right\\|_{\\infty} \\leq 1$ and $\\left(\\mathbf{v}_{\\hat{\\phi}_{5, M^{d}+j}}\\right)_{\\substack{\\left(2 M^{d}+2 j\\right) \\\\ 1,0}}^{(2 M^{d}+2 j)}=0$. The next hidden layer then computes the functions\n\n$$\n\\begin{aligned}\n& \\left(1-\\frac{M^{2}}{a} \\cdot\\left|\\left(C_{P_{2}}(x)\\right)_{\\text {left }}^{(j)}+\\frac{a}{M^{2}}-x^{(j)}\\right|\\right)_{+} \\\\\n& =\\left(\\frac{M^{2}}{a} \\cdot\\left(x^{(j)}-\\left(C_{P_{2}}(x)\\right)_{\\text {left }}^{(j)}\\right)\\right)_{+} \\\\\n& \\quad-2 \\cdot\\left(\\frac{M^{2}}{a} \\cdot\\left(x^{(j)}-\\left(C_{P_{2}}(x)\\right)_{\\text {left }}^{(j)}-\\frac{a}{M^{2}}\\right)\\right)_{+} \\\\\n& \\quad+\\left(\\frac{M^{2}}{a} \\cdot\\left(x^{(j)}-\\left(C_{P_{2}}(x)\\right)_{\\text {left }}^{(j)}-\\frac{2 \\cdot a}{M^{2}}\\right)\\right)_{+}, \\quad j \\in\\{1, \\ldots, d\\}\n\\end{aligned}\n$$\n\nusing the networks\n\n$$\n\\begin{aligned}\n\\hat{f}_{w_{P_{2}, j}}(x)= & \\sigma\\left(\\frac{M^{2}}{a} \\cdot\\left(\\hat{f}_{i d}^{4 M^{d}}\\left(x^{(j)}\\right)-\\hat{\\phi}_{5,2 M^{d}}^{(j)}\\right)\\right) \\\\\n& -2 \\cdot \\sigma\\left(\\frac{M^{2}}{a} \\cdot\\left(\\hat{f}_{i d}^{4 M^{d}}\\left(x^{(j)}\\right)-\\hat{\\phi}_{5,2 M^{d}}^{(j)}-\\frac{a}{M^{2}}\\right)\\right) \\\\\n& +\\sigma\\left(\\frac{M^{2}}{a} \\cdot\\left(\\hat{f}_{i d}^{4 M^{d}}\\left(x^{(j)}\\right)-\\hat{\\phi}_{5,2 M^{d}}^{(j)}-\\frac{2 \\cdot a}{M^{2}}\\right)\\right)\n\\end{aligned}\n$$\n\nwith $3 d$ neurons in the last layer. Note that $\\left|\\hat{f}_{w_{\\mathcal{P}_{2}, j}}(x)\\right| \\leq 1$ for $j \\in\\{1, \\ldots, d\\}$. The product of $w_{\\mathcal{P}_{2}, j}(x)(j \\in\\{1, \\ldots, d\\})$ can then be computed by the network $\\hat{f}_{\\text {mult }, d}(x)$ of Lemma 18 for values $x \\in[-1,1]^{d}$, where we choose $x^{(j)}=\\hat{f}_{w_{\\mathcal{P}_{2}, j}}(x)$ and $R=\\left\\lceil\\log _{4}\\left(M^{2 p}\\right)\\right\\rceil$. Finally we set\n\n$$\n\\hat{f}_{w_{\\mathcal{P}_{2}, \\text { deep }}}(x)=\\hat{f}_{\\text {mult }, d}\\left(\\hat{f}_{w_{\\mathcal{P}_{2}, 1}}(x), \\ldots, \\hat{f}_{w_{\\mathcal{P}_{2}, d}}(x)\\right)\n$$\n\nSince by Lemma 18, in this case $\\hat{f}_{\\text {mult }, d}$ satisfies\n\n$$\n\\begin{aligned}\n& \\left\\|\\mathbf{v}_{\\hat{f}_{\\text {mult }, d}}\\right\\|_{\\infty} \\leq 4 \\cdot 4^{2 d}, \\quad\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }, d}}\\right)_{1,0}^{(R \\cdot\\left\\lceil\\log _{2}(d)\\right\\rceil)}=0 \\quad \\text { and } \\\\\n& \\quad\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }, d}}\\right)^{(0)}\\right\\|_{\\infty} \\leq 1\n\\end{aligned}\n$$\n\nand by construction\n\n$$\n\\begin{gathered}\n\\left(\\mathbf{v}_{\\hat{f}_{w_{\\mathcal{P}_{2}, j}}}\\right)_{1,0}^{\\left(4 M^{d}+1\\right)}=0, \\quad\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{w_{\\mathcal{P}_{2}, j}}}\\right)_{1, i>0}^{\\left(4 M^{d}+1\\right)}\\right\\|_{\\infty} \\leq 2 \\\\\n\\left\\|\\mathbf{v}_{\\hat{f}_{w_{\\mathcal{P}_{2}, j}}}\\right\\|_{\\infty}=M^{4 p+4}\n\\end{gathered}\n$$\n\nan application Lemma 15 b$)$ gives us\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{w_{\\mathcal{P}_{2}, \\text { deep }}}}\\right\\|_{\\infty} \\leq \\max \\left\\{\\left\\|\\mathbf{v}_{\\hat{f}_{w_{\\mathcal{P}_{2}, j}}}\\right\\|_{\\infty}, 4^{2 d+1}, 2\\right\\}=M^{4 p+4}\n$$\n\nThe rest of the proof follows as in the proof of Lemma 16 in Kohler and Langer (2021).\n\nLemma 24 Let $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the ReLU activation function $\\sigma(x)=\\max \\{x, 0\\}$. Let $1 \\leq a<\\infty$. Let $C_{i, 2}\\left(i \\in\\left\\{1, \\ldots, M^{2 d}\\right\\}\\right)$ be the cubes of partition $\\mathcal{P}_{2}$ as described in (29) and let $M \\in \\mathbb{N}$. Then there exists a neural network\n\n$$\n\\hat{f}_{\\text {check, deep, } \\mathcal{P}_{2}}(x) \\in \\mathcal{F}\\left(5 M^{d}, 2 d^{2}+6 d+2\\right)\n$$\n\nsatisfying\n\n$$\n\\begin{gathered}\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {check, deep, } \\mathcal{P}_{2}}}\\right\\|_{\\infty} \\leq M^{4 p+4}, \\quad\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {check, deep, } \\mathcal{P}_{2}}}^{\\left(5 M^{d}\\right)}\\right\\|_{\\infty} \\leq 1 \\\\\n\\hat{f}_{\\text {check, deep, } \\mathcal{P}_{2}}(x)=\\mathbb{1}_{\\bigcup_{i \\in\\left\\{1, \\ldots, M^{2 d}\\right\\}} C_{i, 2} \\backslash\\left(C_{i, 2}\\right)_{1 / M^{2 p+2}}^{0}(x)}\n\\end{gathered}\n$$\n\nfor $x \\notin \\bigcup_{i \\in\\left\\{1, \\ldots, M^{2 d}\\right\\}}\\left(C_{i, 2}\\right)_{1 / M^{2 p+2}}^{0} \\backslash\\left(C_{i, 2}\\right)_{2 / M^{2 p+2}}^{0}$ and\n\n$$\n\\hat{f}_{\\text {check, deep, } \\mathcal{P}_{2}}(x) \\in[0,1]\n$$\n\nfor $x \\in[-a, a)^{d}$.\n\nProof. The value of $\\left(C_{\\mathcal{P}_{1}}(x)\\right)_{l e f t}$ is computed by the network $\\tilde{\\boldsymbol{\\phi}}_{2, M^{d}}$ of Lemma 14 with $2 M^{d}$ hidden layers and $d \\cdot(2 d+2)$ neurons per layer and $x$ is shifted in consecutive layers by successively applying $\\hat{f}_{i d} \\in \\mathcal{F}(1,2)$. Furthermore we compute\n\n$$\nf_{1}(x)=1-\\sum_{i \\in\\left\\{1, \\ldots, M^{d}\\right\\}} \\mathbb{1}_{\\left(C_{i, 1}\\right)_{1 / M^{2 p+2}}^{0}}(x)\n$$\n\nby a network\n\n$$\n\\hat{f}_{1, j}(x)=\\hat{f}_{i d}^{2}\\left(\\hat{f}_{1, j-1}\\right)-\\hat{f}_{i n d,\\left(C_{j, 1}\\right)_{1 / M^{2 p+2}}^{0}}\\left(\\hat{f}_{i d}^{2(j-1)}(x)\\right), \\quad j \\in\\left\\{1, \\ldots, M^{d}\\right\\}\n$$\n\nwhere $\\hat{f}_{1,0}=1$. Here we use again the network $\\hat{f}_{i n d,[\\mathbf{a}, \\mathbf{b})]}$ from Lemma 20 a with $R=$ $M^{2 p+2}$. Next we define\n\n$$\n\\tilde{\\boldsymbol{\\phi}}_{2, M^{d}+j}=\\tilde{f}_{i d}^{3}\\left(\\tilde{\\boldsymbol{\\phi}}_{2, M^{d}+j-1}+\\tilde{\\mathbf{v}}_{j+1}\\right) \\in \\mathcal{F}\\left(2 M^{d}+3 j, 2 d\\right)\n$$\n\nfor $j \\in\\left\\{1, \\ldots, M^{d}\\right\\}$. It is easy to see that $\\tilde{\\boldsymbol{\\phi}}_{2, M^{d}+j}$ satisfies the same weight constraints as $\\tilde{\\boldsymbol{\\phi}}_{2, M^{d}+j}$, i.e. $\\left\\|\\mathbf{v}_{\\tilde{\\boldsymbol{\\phi}}_{2, M^{d}+j}} \\|_{c}\\right\\|_{\\infty} \\leq M^{2 p+2}$, which we derived in the proof of Lemma 14 and by construction we have $\\left(\\mathbf{v}_{\\tilde{\\boldsymbol{\\phi}}_{2, 2 M^{d}}}\\right)_{1,0}^{\\left(4 M^{d}\\right)}=0$ and $\\left\\|\\left(\\mathbf{v}_{\\tilde{\\boldsymbol{\\phi}}_{2, 2 M^{d}}}\\right)^{\\left(4 M^{d}\\right)}\\right\\|_{\\infty} \\leq 1$. The value of\n\n$$\n\\mathbb{1}_{\\bigcup_{i \\in\\left\\{1, \\ldots, M^{2 d}\\right\\}} C_{i, 3} \\backslash\\left(C_{i, 2}\\right)_{1 / M^{2 p+2}}^{0}}(x)\n$$\n\nis then successively computed by\n\n$$\n\\begin{aligned}\n& \\hat{f}_{1, M^{d}+j}(x) \\\\\n& =1-\\sigma\\left(1-\\hat{f}_{\\text {test }}\\left(\\hat{f}_{i d}^{2 M^{d}+3(j-1)}(x), \\tilde{\\boldsymbol{\\phi}}_{2, M^{d}+j-1}+\\tilde{\\mathbf{v}}_{j}+\\frac{1}{M^{2 p+2}} \\cdot \\mathbf{1}\\right.\\right. \\\\\n& \\left.\\left.\\tilde{\\boldsymbol{\\phi}}_{2, M^{d}+j-1}+\\tilde{\\mathbf{v}}_{j}+\\frac{2 a}{M^{2}} \\cdot \\mathbf{1}-\\frac{1}{M^{2 p+2}} \\cdot \\mathbf{1}, 1\\right)-\\tilde{f}_{i d}^{2}\\left(\\hat{f}_{1, M^{d}+j-1}\\right)\\right)\n\\end{aligned}\n$$\n\nfor $j \\in\\left\\{1, \\ldots, M^{d}\\right\\}$, where we use networks $\\hat{f}_{\\text {test }}$ from Lemma 20 b with $R=M^{2 p+2}$ and which thus satisfies $\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {test }}}\\right\\|_{\\infty} \\leq M^{4 p+4}$ ).\n\nFinally we set\n\n$$\n\\hat{f}_{\\text {check, deep, } \\mathcal{P}_{2}}(x)=\\hat{f}_{1,2 M^{d}}(x)\n$$\n\nThe weight constraints follow again from Lemma 15, arguing in the same way as in the proof of Lemma 14.\n\nIn the proof of Lemma 22 we use Lemma 23 to approximate $w_{\\mathcal{P}_{2}}(x)$ and Lemma 14 to compute $f(x)$. As in Lemma 7 of Kohler and Langer (2021) we apply a network, that checks whether $x$ is close to the boundaries of the cubes of the partition.\n\nProof of Lemma 22. This result follows by a straightforward modification of the proof of Lemma 7 of Kohler and Langer (2021). Here we use the network $\\hat{f}_{\\text {deep, } \\mathcal{P}_{2}}$ of Lemma 14 and $\\hat{f}_{\\text {check,deep, } \\mathcal{P}_{2}}$ of Lemma 22 to define\n\n$$\n\\begin{aligned}\n\\hat{f}_{\\mathcal{P}_{2}, \\text { true }}(x)= & \\sigma\\left(\\hat{f}_{\\text {deep, } \\mathcal{P}_{2}}(x)-B_{\\text {true }} \\cdot \\hat{f}_{\\text {check,deep, } \\mathcal{P}_{2}}(x)\\right) \\\\\n& -\\sigma\\left(-\\hat{f}_{\\text {deep, } \\mathcal{P}_{2}}(x)-B_{\\text {true }} \\cdot \\hat{f}_{\\text {check,deep, } \\mathcal{P}_{2}}(x)\\right)\n\\end{aligned}\n$$\n\nwith\n\n$$\n\\begin{aligned}\nB_{\\text {true }}= & 1+\\left\\lvert\\,\\left(\\|f\\|_{C^{q}\\left([-a, a]^{d}\\right)} \\cdot e^{\\left(M^{d}-1\\right)}\\right.\\right. \\\\\n& \\left.\\left.+\\left(4+2 \\cdot\\left\\lceil e^{d}\\right\\rceil\\right) \\cdot\\left(M^{d}-1\\right) \\cdot e^{\\left(M^{d}-2\\right)}\\right) \\cdot e^{2 a d}\\right\\rvert\\,\n\\end{aligned}\n$$\n\nwhich satisfies\n\n$$\n\\begin{aligned}\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\mathcal{P}_{2}, \\text { true }}}\\right\\|_{\\infty} & \\leq \\max \\left\\{B_{\\text {true }} \\cdot\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {check,deep, } \\mathcal{P}_{2}}}^{(5 M^{d})}\\right\\|_{\\infty},\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {deep, } \\mathcal{P}_{2}}}\\right\\|_{\\infty},\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {check,deep, } \\mathcal{P}_{2}}}\\right\\|_{\\infty}\\right\\} \\\\\n& \\leq e^{c_{48} \\cdot(p+1) \\cdot M^{d}}\n\\end{aligned}\n$$\n\nand $\\left(\\mathbf{v}_{\\hat{f}_{\\mathcal{P}_{2}, \\text { true }}}\\right)_{1,0}^{(L)}=0, \\quad\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\mathcal{P}_{2}, \\text { true }}}\\right)_{\\left.1, j>0\\right]_{1, j>0}}\\right\\|_{\\infty} \\leq 1$.\nRemark that by successively applying $\\hat{f}_{i d}$ to the output of the networks $\\hat{f}_{\\text {deep, } \\mathcal{P}_{2}}$ and $\\hat{f}_{\\text {check,deep, } \\mathcal{P}_{2}}$ we can achieve that both networks have depth\n\n$$\nL=5 M^{d}+\\left\\lceil\\log _{4}\\left(M^{2 p+4 \\cdot d \\cdot(q+1)} \\cdot e^{4 \\cdot(q+1) \\cdot\\left(M^{d}-1\\right)}\\right)\\right\\rceil \\cdot\\left\\lceil\\log _{2}(\\max \\{q+1,2\\})\\right\\rceil\n$$\n\nFurthermore, it is easy to see that this networks needs at most\n\n$$\n\\begin{aligned}\n\\max \\left\\{10 d+4 d^{2}+2\\right. & \\cdot\\binom{d+q}{d} \\cdot\\left(2 \\cdot\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)+5+2 d\\right) \\\\\n& 18 \\cdot(q+1) \\cdot\\binom{d+q}{d}\\right\\}+2 d^{2}+6 d+2\n\\end{aligned}\n$$\n\nneurons per layer. In the definition of the final network we use the network $\\hat{f}_{w_{\\mathcal{P}_{2}, \\text { deep }}}$ of Lemma 23, and the network $\\hat{f}_{\\text {mult }}$\n\n$$\n\\hat{f}_{\\text {mult }} \\in \\mathcal{F}\\left(\\left\\lceil\\log _{4}\\left(M^{2 p}\\right)\\right\\rceil, 18\\right)\n$$\n\nof Lemma 17 for\n\n$$\na=1+\\left(\\|f\\|_{C^{q}\\left([-a, a]^{d}\\right)} \\cdot e^{\\left(M^{d}-1\\right)}+\\left(4+2 \\cdot\\left\\lceil e^{d}\\right\\rceil\\right) \\cdot\\left(M^{d}-1\\right) \\cdot e^{\\left(M^{d}-2\\right)}\\right) \\cdot e^{2 a d}\n$$\n\nwhich satisfies the constraint\n\n$$\n\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right\\|_{\\infty} \\leq 4 \\cdot 4^{2 d} \\cdot a^{2} \\leq e^{c_{49}(f)}\n$$\n\nAgain we synchronize the depth of $\\hat{f}_{w_{P_{2}}, \\text { deep }}$ and $\\hat{f}_{P_{2}, \\text { true }}$ to insure that both networks have\n\n$$\n\\bar{L}=5 M^{d}+\\left\\lceil\\log _{4}\\left(M^{2 p+4 \\cdot d \\cdot(q+1)} \\cdot e^{4 \\cdot(q+1) \\cdot\\left(M^{d}-1\\right)}\\right)\\right\\rceil \\cdot\\left\\lceil\\log _{2}(\\max \\{q, d\\}+1)\\right\\rceil\n$$\n\nmany layers. The final network is given by\n\n$$\n\\hat{f}(x)=\\hat{f}_{\\text {mult }}\\left(\\hat{f}_{P_{2}, \\text { true }}(x), \\hat{f}_{w_{P_{2}}, \\text { deep }}(x)\\right)\n$$\n\nBy Lemma 23 and by definition of $\\hat{f}_{\\mathcal{P}_{2}, \\text { true }}(x)$, it is easy to see that both networks have no offset in their respective output layers, thus Lemma 15 b ) is applicable and we get\n\n$$\n\\begin{aligned}\n& \\left\\|\\mathbf{v}_{\\hat{f}}\\right\\|_{\\infty} \\leq \\max \\left\\{\\left\\|\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right\\|_{\\infty}, \\max \\left\\{\\left\\|\\mathbf{v}_{\\hat{f}_{P_{2}, \\text { true }}}\\right\\|_{\\infty},\\left\\|\\mathbf{v}_{\\hat{f}_{w_{P_{2}}, \\text { deep }}}\\right\\|_{\\infty}\\right\\}\\right. \\\\\n& \\max \\left\\{\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{P_{2}, \\text { true }}}\\right)^{(L)}\\right\\|_{\\infty} \\cdot\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)^{(0)}\\right\\|_{\\infty},\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{w_{P_{2}}, \\text { deep }}}\\right)^{(L)}\\right\\|_{\\infty} \\cdot\\left\\|\\left(\\mathbf{v}_{\\hat{f}_{\\text {mult }}}\\right)^{(0)}\\right\\|_{\\infty}\\right\\}\\right\\} \\\\\n& \\leq \\max \\left\\{e^{c_{49}(f) \\cdot M^{d}}, \\max \\left\\{\\left\\|\\mathbf{v}_{\\hat{f}_{P_{2}, \\text { true }}}\\right\\|_{\\infty},\\left\\|\\mathbf{v}_{\\hat{f}_{w_{P_{2}}, \\text { deep }}}\\right\\|_{\\infty}\\right\\}\\right\\} \\\\\n& \\leq e^{c_{50} \\cdot(p+1) \\cdot M^{d}}\n\\end{aligned}\n$$\n\nThis network is contained in the network class $\\mathcal{F}(L, r)$ with\n\n$$\n\\begin{aligned}\nL= & 5 M^{d}+\\left\\lceil\\log _{4}\\left(M^{2 p+4 \\cdot d \\cdot(q+1)} \\cdot e^{4 \\cdot(q+1) \\cdot\\left(M^{d}-1\\right)}\\right)\\right\\rceil \\cdot\\left\\lceil\\log _{2}(\\max \\{q, d\\}+1)\\right\\rceil \\\\\n& +\\left\\lceil\\log _{4}\\left(M^{2 p}\\right)\\right\\rceil\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& r=\\max \\left\\{10 d+4 d^{2}+2 \\cdot\\binom{d+q}{d} \\cdot\\left(2 \\cdot\\left(4+2\\left\\lceil e^{d}\\right\\rceil\\right)+5+2 d\\right)\\right. \\\\\n& \\left.18 \\cdot(q+1) \\cdot\\binom{d+q}{d}\\right\\}+2 d^{2}+6 d+2+\\max \\left\\{18 d, 4 d^{2}+10 d\\right\\}\n\\end{aligned}\n$$\n\nWith the same argument as in the proof of Lemma 10 of Kohler and Langer (2021) we can show the assertion.\n\nIn a last step of the proof, one has to apply $\\hat{f}$ to slightly shifted partitions. This follows as in section A.1.12 of Kohler and Langer (2021) and has no effect on the weight bounds that were just derived.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2404.07128v3",
  "authors": [
    "Michael Kohler",
    "Adam Krzyzak",
    "Alisha S\u00e4nger"
  ],
  "categories": [
    "math.ST",
    "stat.TH"
  ],
  "abstract": "Image classification from independent and identically distributed random\nvariables is considered. Image classifiers are defined which are based on a\nlinear combination of deep convolutional networks with max-pooling layer. Here\nall the weights are learned by stochastic gradient descent. A general result is\npresented which shows that the image classifiers are able to approximate the\nbest possible deep convolutional network. In case that the a posteriori\nprobability satisfies a suitable hierarchical composition model it is shown\nthat the corresponding deep convolutional neural network image classifier\nachieves a rate of convergence which is independent of the dimension of the\nimages.",
  "updated": "2025-03-05T18:30:14Z",
  "published": "2024-04-10T16:07:29Z"
}