{"title": "On the Existence of One-Sided Representations for the Generalised\n  Dynamic Factor Model", "sections": [{"section_id": 0, "text": "#### Abstract\n\nWe consider the Generalised Dynamic Factor Model (GDFM) under the assumption that the dynamic common component is purely non-deterministic. We show that, under this assumption, the common shocks - and consequently, the dynamic common component - can be represented in terms of current and past observed variables. This generalises existing results on the so-called \"One-Sidedness Problem\" of the GDFM.\n\n\nIndex terms - Generalized Dynamic Factor Model, Representation Theory, MSC: 91B84", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1 Introduction\n\nConsider a high-dimensional time series as a double indexed (zero-mean, stationary) stochastic process $\\left(y_{i t}: i \\in \\mathbb{N}, t \\in \\mathbb{Z}\\right)=\\left(y_{i t}\\right)$, where the index $i$ represents the cross-sectional unit and $t$ represents time. The generalised dynamic factor model (GDFM) as introduced by Forni and Lippi (2001) (see Hallin and Lippi, 2013; Barigozzi and Hallin, 2024, for a time domain approach) relies on the decomposition\n\n$$\ny_{i t}=\\chi_{i t}+\\xi_{i t}=\\underline{b}_{i}(L) u_{t}+\\xi_{i t}=\\sum_{j=-\\infty}^{\\infty} B_{i}(j) u_{t-j}+\\xi_{i t}, \\quad u_{t} \\sim W N\\left(I_{q}\\right)\n$$\n\nwhere the \"dynamic common component\" $\\left(\\chi_{i t}\\right)$ is driven by an orthonormal $q$-dimensional uncorrelated white noise process $\\left(u_{t}\\right)$ of dynamic factors/common shocks, with $q$ usually small, while $\\underline{b}_{i}(L)=\\sum_{j=-\\infty}^{\\infty} B_{i}(j) L^{j}, i \\in \\mathbb{N}$ are pervasive $1 \\times q$ square-summable two-sided linear filters and \" $L$ \" is the lag operator. The \"dynamic idiosyncratic component\", $\\left(\\xi_{i t}\\right)$, is assumed to be weakly correlated over time and cross-section.\n\nWhile the decomposition into common- and idiosyncratic component is unique, there are infinitely many representations of filters $\\underline{b}_{i}(L)$ and the dynamic factor process $\\left(u_{t}\\right)$ to choose from: Clearly, for any orthonormal $q \\times q$ filter, say $\\underline{c}(L)$ gives another \"valid\" representation\n\n$$\n\\chi_{i t}=\\underline{b}_{i}(L) \\underline{c}^{*}(L) \\underline{c}(L) u_{t}=\\underline{\\tilde{b}}_{i}(L) \\tilde{u}_{t}\n$$\n\n[^0]\n[^0]:    *Department of Statistics and Operations Research, University of Vienna, philipp.gersing@univie.ac.at\n\nwith $\\tilde{u}_{t}=\\underline{c}(L) u_{t}$ being orthonormal white noise. The GDFM is theoretically closely tied to dynamic principal components (see Forni and Lippi, 2001) and also naturally $\\left(u_{t}\\right), \\underline{b}_{i}(L)$ and $q$ are estimated via the sample version of dynamic principal components (see Forni and Lippi, 2001; Forni et al., 2004; Hallin and Li\u0161ka, 2007). However, this renders a representation where the estimates of $u_{t}$ are two-sided transformations of the observed data and the estimates of $\\underline{b}_{i}(L)$ are two-sided linear filters. As a consequence, those estimates cannot be used in forecasting applications.\n\nSofar, this issue has been addressed in the literature already by Forni et al. (2005) and Forni and Lippi (2011); Forni et al. (2015, 2017); Barigozzi et al. (2024). While Forni et al. (2005) assume a static factor structure where the contemporaneously pervasive factors are modelled as a VAR process, Forni and Lippi (2011); Forni et al. (2015, 2017); Barigozzi et al. (2024) suppose an ARMA model for $\\left(\\chi_{t}^{n}\\right)$ thus allowing that the contemporaneous space spanned by the $\\chi_{i t}$ 's is infinite dimensional. Both approaches achieve a representation where the dynamic common component is one-sided in the shocks and in the output.\n\nThe contribution of this paper is to further generalise these results. We show that there exists a representation, that is one-sided in the observed process, mainly whenever the common component is purely non-deterministic. Some extremely special cases of transfer functions for the common component discussed in section 4 are not covered by our proof. We argue however, that those do not seem to have practical relevance. The assumption of a time series being \"purely non-deterministic\" or \"regular\" means that we cannot forecast the infinite future and is employed (implicitly) almost throughout in time series analysis. For instance, the commonly assumed models for factors and/or the common component, i.e., $\\operatorname{VAR}(p)$ and $\\operatorname{ARMA}(p, q)$ or state space models all fall into the much broader class of purely non-deterministic processes. Precisely, we prove that there exists a representation of the form\n\n$$\n\\chi_{i t}=\\underline{b}_{i}(L) \\varepsilon_{t}=\\sum_{j=0}^{\\infty} K_{i}(j) \\varepsilon_{t-j}, \\quad \\varepsilon_{t} \\sim W N\\left(I_{q}\\right)\n$$\n\nsuch that $\\varepsilon_{t} \\in \\overline{\\operatorname{sp}}\\left(y_{i s}: i \\in \\mathbb{N}, s \\leq t\\right):=\\mathbb{H}_{t}(y)$ and $\\sum_{j=0}^{\\infty}\\left\\|K_{i}(j)\\right\\|^{2}<\\infty$, where $\\overline{\\operatorname{sp}}(\\cdot)$ denotes the closed linear span. Here $\\left(\\varepsilon_{t}\\right)$ is the innovation process of $\\left(\\chi_{i t}\\right)$ which is unique up to premultiplication by a real orthogonal $q \\times q$ matrix. The representation in (3) is somehow a \"natural way\" to state a GDFM and may also be called the innovation form of the GDFM.\n\nAs we will see, it is a characteristic trait of the GDFM - as opposed to low rank approximations via dynamic principal components in general - that the innovations of the common component, $\\varepsilon_{t}$ are one-sided in the observed process. In other words, there exists a rotation filter in (2) such that (3) is satisfied with $\\varepsilon_{t} \\in \\mathbb{H}_{t}(y)$. The result in this paper thus gives further theoretical support for the GDFM by resolving the One-Sidedness issue in more general terms. We may say that the lack of causal subordination in \"the principal components representation\" (as in Forni et al., 2000) is not a conceptual drawback of the GDFM itself. Rather it is a matter of choosing another representation/corresponding estimation procedure.\n\nAnother aspect relates to the interpretation of the dynamic common component: As shown in Forni and Lippi (2001), the dynamic common component $\\chi_{i t}$ is the Hilbert space projection of $y_{i t}$ on the dynamically common space/ the dynamic aggregation space, say $\\mathbb{D}(y)$. Together\n\nwith equation (3), and given that $\\varepsilon_{t} \\in \\mathbb{H}_{t}(y)$, we may interpret $\\left(\\varepsilon_{t}\\right)$ as the common structural shocks, i.e., the common innovations of $\\left(y_{i t}\\right)$ and $\\left(\\chi_{i t}\\right)$ as the part of $\\left(y_{i t}\\right)$ that is driven by the common innovations.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 2 General Setup\n### 2.1 Notation\n\nLet $\\mathcal{P}=(\\Omega, \\mathcal{A}, \\mathbb{P})$ be a probability space and $L_{2}(\\mathcal{P}, \\mathbb{C})$ be the Hilbert space of square integrable complex-valued, zero-mean, random-variables defined on $\\Omega$ equipped with the inner product $\\langle u, v\\rangle=\\mathbb{E}[u \\bar{v}]$ for $u, v \\in L_{2}(\\mathcal{P}, \\mathbb{C})$. We suppose that $\\left(y_{i t}\\right)$ lives in $L_{2}(\\mathcal{P}, \\mathbb{C})$ using the following abbreviations: $\\mathbb{H}(y):=\\overline{\\operatorname{sp}}\\left(y_{i t}: i \\in \\mathbb{N}, t \\in \\mathbb{Z}\\right)$, the \"time domain\" of $\\left(y_{i t}\\right), \\mathbb{H}_{t}(y):=\\overline{\\operatorname{sp}}\\left(y_{i s}\\right.$ : $i \\in \\mathbb{N}, s \\leq t$ ), the \"infinite past\" of $\\left(y_{i t}\\right)$. We write $y_{t}^{n}=\\left(y_{1 t}, \\ldots, y_{n t}\\right)^{\\prime}$ and by $f_{y}^{n}(\\theta)$ we denote the \"usual spectrum\" of $\\left(y_{t}^{n}\\right)$ times $2 \\pi$, i.e., $\\Gamma_{y}^{u}:=\\mathbb{E}\\left[y_{t}^{n}\\left(y_{t}^{n}\\right)^{*}\\right]=(2 \\pi)^{-1} \\int_{-\\pi}^{\\pi} f_{y}^{n}(\\theta) d \\theta$. For a stochastic vector $u$ with coordinates in $L_{2}(\\mathcal{P}, \\mathbb{C})$, we write $\\mathbb{V} u:=\\mathbb{E}\\left[u u^{*}\\right]$ to denote the variance matrix. Let $u$ be a stochastic vector with coordinates in $L_{2}(\\mathcal{P}, \\mathbb{C})$, let $\\mathbb{M} \\subset L_{2}(\\mathcal{P}, \\mathbb{C})$ be a closed subspace. We denote by $\\operatorname{proj}(u \\mid \\mathbb{M})$ the orthogonal projections of $u$ onto $\\mathbb{M}$ (see e.g. Deistler and Scherrer, 2022, Theorem 1.2) (coordinate-wise). Furthermore we denote by $\\mu_{i}(A)$ the $i$-th largest eigenvalue of a square matrix $A$. If $A$ is a spectral density $\\mu_{i}(A)$ is a measurable function in the frequency $\\theta \\in[-\\pi, \\pi]$.", "tables": {}, "images": {}}, {"section_id": 3, "text": "### 2.2 The Generalised Dynamic Factor Model\n\nThroughout we assume stationarity of $\\left(y_{i t}\\right)$ in the following sense:\nA 0 (Stationary Double Sequence)\nThe process $\\left(y_{t}^{n}: t \\in \\mathbb{Z}\\right)$ is real valued, weakly stationary with zero-mean and such that\n(i) $y_{i t} \\in L_{2}(\\mathcal{P}, \\mathbb{C})$ for all $(i, t) \\in \\mathbb{N} \\times \\mathbb{Z}$;\n(ii) it has existing (nested) spectral density $f_{y}^{n}(\\theta)$ for $\\theta \\in[-\\pi, \\pi]$ defined as the $n \\times n$ matrix:\n\n$$\nf_{y}^{n}(\\theta)=\\frac{1}{2 \\pi} \\sum_{\\ell=-\\infty}^{\\infty} e^{-\\iota \\ell \\theta} \\mathbb{E}\\left[y_{t}^{n} y_{\\ell-\\ell}^{n^{\\prime}}\\right], \\quad \\theta \\in[-\\pi, \\pi]\n$$\n\nIn addition we assume that $\\left(y_{i t}\\right)$ has a $q$-dynamic factor structure (see Forni and Lippi, 2001, Definition 10): Let $f_{\\chi}^{n}(\\theta), f_{\\xi}^{n}(\\theta)$ be the spectrum of $\\left(\\chi_{t}^{n}\\right)$ and $\\left(\\xi_{t}^{n}\\right)$ respectively at frequency $\\theta \\in[-\\pi, \\pi]$. Denote by \"ess sup\" the essential supremum of a measurable function. We assume:\n\nA 1 ( $q$-Dynamic Factor Structure)\nThe process $\\left(y_{i t}\\right)$ can be represented as in (1) with $q<\\infty$, while\n(i) $\\sup _{n \\in \\mathbb{N}} \\mu_{q}\\left(f_{\\chi}^{n}\\right)=\\infty$ a.e. on $[-\\pi, \\pi]$\n(ii) $\\operatorname{ess} \\sup _{\\theta \\in[-\\pi, \\pi]} \\sup _{n \\in \\mathbb{N}} \\mu_{1}\\left(f_{\\xi}^{n}\\right)<\\infty$.\n\nWhile A1(i) describes the sense in which the filter-loadings in (1) are pervasive, A1(ii) defines dynamic idiosyncraticness, which ensures that $\\left(\\xi_{i t}\\right)$ correlates only mildly over cross-section and time. A double sequence $\\left(y_{i t}\\right)$ which satisfies A1 is also called $q$-Dynamic Factor Sequence ( $q$ DFS) (see Forni and Lippi, 2001). For the relation to the static approach see Gersing (2023); Gersing et al. (2024); Gersing (2024).", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 3 Purely Non-Deterministic Processes of Infinite Dimension \n\nWe begin with recalling some basic facts related to purely non-deterministic processes. Suppose $\\left(x_{i t}\\right)$ is a zero-mean weakly stationary stochastic double sequence. We call $\\mathbb{H}^{-}(x):=\\bigcap_{t \\in \\mathbb{Z}} \\mathbb{H}_{t}(x)$ the remote past of $\\left(x_{i t}\\right)$.\n\nDefinition 1 (Purely Non-Deterministic and Purely Deterministic Stationary Process) If $\\mathbb{H}^{-}(x)=\\{0\\}$, then $\\left(x_{i t}\\right)$ is called purely non-deterministic (PND) or regular. If $\\mathbb{H}^{-}(x)=\\mathbb{H}(x)$, then $\\left(x_{t}\\right)$ is called (purely) deterministic (PD) or singular.\n\nNote that \"singular\" in the sense of being purely deterministic must not to be confused with processes that have rank deficient spectrum, like the common component of the GDFM, and are also called \"singular\" in the literature (Anderson and Deistler, 2008; Deistler et al., 2010; Forni and Lippi, 2024). Therefore we shall use PND and PD henceforth to avoid confusion.\n\nOf course there are processes between the two extremes of PD and PND. The future values of a PD process can be predicted perfectly (in terms of mean squared error). On the other hand, a PND process is entirely governed by random innovations. It can only be predicted with positive mean squared error and the further we want to predict ahead, the less variation we can explain: For example, suppose now that $x_{t}=x_{t}^{n}$ for some $n<\\infty, t \\in \\mathbb{Z}$, and set\n\n$$\n\\nu_{h \\mid t}:=x_{t+h}-\\operatorname{proj}\\left(x_{t+h} \\mid \\mathbb{H}_{t}(x)\\right)\n$$\n\nwhich is the $h$-step ahead prediction error. If $\\left(x_{t}\\right)$ is PND, then $\\lim _{h \\rightarrow \\infty} \\forall \\nu_{h \\mid t} \\rightarrow \\forall x_{t}$.\nNext, we recall some basic facts about the finite dimensional case. By Wold's representation Theorem (see Hannan and Deistler, 2012; Deistler and Scherrer, 2022), any weakly stationary process can be written as sum of a PD and PND process, being mutually orthogonal at all leads and lags.\n\nThere are several characterisations for PND processes. Firstly, the Wold decomposition implies (see e.g. Rozanov, 1967; Masani and Wiener, 1957) that $\\left(x_{t}\\right)$ is PND if and only if it can be written as a causal infinite moving average.\n\n$$\nx_{t}=\\nu_{t}+\\sum_{j=1}^{\\infty} C(j) \\nu_{t-j}\n$$\n\nwhere $\\nu_{t}:=\\nu_{1 \\mid t-1}$ is the innovation of $\\left(x_{t}\\right)$, possibly with reduced rank $\\mathrm{rk} \\forall \\nu_{t}:=q \\leq n$. If $q<n$ we may uniquely factorise $\\forall \\nu_{t}=b b^{\\prime}$. Assume without loss of generality that the first $q$ rows of $b$ have full rank (otherwise reorder), a unique factor is obtained choosing $b$ to be upper\n\ntriangular with positive entries on the main diagonal. This results in the representation\n\n$$\nx_{t}=\\sum_{j=0}^{\\infty} K(j) \\varepsilon_{t-j}\n$$\n\nwith $K(j)=C(j) b$ and $\\mathbb{V} \\varepsilon_{t}=I_{q}$.\nSecondly, a stationary process is PND (see Rozanov, 1967; Masani and Wiener, 1957) if and only if the spectral density has constant rank $q \\leq n$ a.e. on $[-\\pi, \\pi]$ and can be factored as\n\n$$\n\\begin{gathered}\nf_{x}(\\theta)=\\underbrace{k(\\theta)}_{n \\times q} k^{*}(\\theta) \\\\\n\\text { while } \\quad k(\\theta)=\\sum_{j=0}^{\\infty} K(j) e^{-i \\theta}, \\quad \\sum_{j=0}^{\\infty}\\|K(j)\\|_{F}^{2}<\\infty\n\\end{gathered}\n$$\n\n$\\|\\cdot\\|_{F}$ denotes the Frobenius norm and\n\n$$\nk(\\theta)=\\underline{k}\\left(e^{-i \\theta}\\right), \\quad \\theta \\in(-\\pi, \\pi], \\quad \\underline{k}(z)=\\sum_{j=0}^{\\infty} K(j) z^{j}, \\quad z \\in D\n$$\n\nhere $z$ denotes a complex number. The entries of the spectral factor $\\underline{k}(z)$ are analytic functions in the open unit disc $D$ and belong to the class $L^{2}(T)$, i.e., are square integrable on the unit circle $T$.\n\nFact 1 (Szabados (2022))\nIf $\\left(x_{t}\\right)$ is PND with $\\operatorname{rk} f_{x}=q<n$ a.e. on $[-\\pi, \\pi]$, then there exists a $q$-dimensional sub-vector of $x_{t}$, say $\\tilde{x}_{t}=\\left(x_{i_{1}, t}, \\ldots, x_{i_{q}, t}\\right)^{\\prime}$ of full dynamic rank, i.e., $\\operatorname{rk} f_{\\tilde{x}}=q$ a.e. on $[-\\pi, \\pi]$.\n\nTo see why, we follow the proof of Theorem 2.1 in Szabados (2022). A principal minor $M(\\theta)=\\operatorname{det}\\left[\\left(f_{x}\\right)_{i_{j}, i_{l}}\\right]_{j, l=1}^{q}$ of $f_{x}$ can be expressed by means of equation (6) in terms of\n\n$$\nM_{f_{x}}(\\theta)=\\operatorname{det}\\left[\\underline{k}_{i_{j}, l}\\left(e^{-i \\theta}\\right)\\right]_{j, l=1}^{q} \\operatorname{det} \\overline{\\left[\\underline{k}_{i_{j}, l}\\left(e^{-i \\theta}\\right)\\right]}_{j, l=1}^{q}=\\left|\\operatorname{det}\\left[\\underline{k}_{i_{j}, l}\\left(e^{-i \\theta}\\right)\\right]_{j, l=1}^{q}\\right|^{2}:=\\left|M_{\\underline{k}}\\left(e^{-i \\theta}\\right)\\right|^{2}\n$$\n\nwith the same row indices in the minor $M_{\\underline{k}}(z)$ of $\\underline{k}(z)$ as in the principal minor $M(\\theta)$ of $f_{x}$. We know that $M_{\\underline{k}}(z)=0$ a.e. or $M_{\\underline{k}}(z) \\neq 0$ a.e. because $M_{\\underline{k}}(z)$ is analytic in $D$. Since $\\operatorname{rk} f_{x}=q$ a.e., the sum of all principal minors of $f_{x}$ of order $q$ is different from zero a.e., so there exists at least one order $q$ principal minor of $f_{x}$ different a.e. from zero.\n\nLet us assume without loss of generality that the first $q$ coordinates of $\\left(x_{t}\\right)$ are like $\\left(\\tilde{x}_{t}\\right)$ in Fact 1. Then with respect to (4), (5) the innovations $\\left(\\varepsilon_{t}\\right)$ of $\\left(\\tilde{x}_{t}\\right)$ are the same as the innovations of $\\left(x_{t}\\right)$ and we have equality of the spaces\n\n$$\n\\mathbb{H}_{t}(x)=\\mathbb{H}_{t}(\\nu)=\\mathbb{H}_{t}(\\tilde{x})=\\mathbb{H}_{t}(\\varepsilon)\n$$\n\nConsequently, for a PND process that is singular in the sense that the spectrum is rank deficient, we always find a sub-process such that the other coordinates can perfectly (even causally) be predicted by $x_{i t}=\\underline{k}_{i}(L) \\underline{k}_{\\tilde{x}}^{-1}(L) \\tilde{x}_{t}$ for $i \\notin\\left(i_{1}, \\ldots, i_{q}\\right)$, while the existence of the causal inverse\n\n$\\underline{k}_{\\bar{x}}^{-1}(L)$ is ensured by the Wold representation. We have obtained the following characterisation of PND for the infinite dimensional case:\n\nTheorem 1 (Purely non-deterministic rank-reduced stochastic double sequence)\nLet $\\left(x_{i t}\\right)$ be a stationary purely non-deterministic stochastic double sequence such that $\\mathrm{rk} f_{x}^{n}=$ $q<\\infty$ a.e. on $[-\\pi, \\pi]$ for all $n \\in \\mathbb{N}$ from a certain $n$ onwards, then $\\left(x_{i t}\\right)$ is PND if and only if\n(i) there exists an index-set $\\left(i_{1}, \\ldots, i_{q}\\right)$ and a $q \\times 1$ orthonormal white noise process $\\left(\\varepsilon_{t}\\right)$ such that\n\n$$\n\\varepsilon_{t} \\in \\operatorname{sp}\\left(\\left(\\begin{array}{c}\nx_{i_{1}, t} \\\\\n\\vdots \\\\\nx_{i_{q}, t}\n\\end{array}\\right)-\\operatorname{proj}\\left(\\begin{array}{c|c}\nx_{i_{1}, t} & \\\\\n\\vdots & \\\\\nx_{i_{q}, t} & \\\\\n\\hline\n\\end{array}\\right) \\mathbb{H}_{t-1}\\left(x_{i_{1}}, \\ldots x_{i_{q}}\\right)\\right)\n$$\n\n(ii) We have $x_{i t} \\in \\mathbb{H}_{t}(\\varepsilon)$ for all $i \\in \\mathbb{N}$ and for all $i \\in \\mathbb{N}$ there exists a causal transfer function $\\underline{k}_{i}(L)$ such that\n\n$$\nx_{i t}=\\underline{k}_{i}(L) \\varepsilon_{t}=\\sum_{j=0}^{\\infty} K_{i}(j) \\varepsilon_{t-j}\n$$\n\nwhere $\\sum_{j=0}^{\\infty}\\left\\|K_{i}(j)\\right\\|^{2}<\\infty$ and $\\underline{k}_{i}(z)$ are analytic in the open unit disc $D$ for all $i \\in \\mathbb{N}$.\nWe call $\\left(\\varepsilon_{t}\\right)$ innovation of $\\left(x_{i t}\\right)$. Uniqueness of $\\left(\\varepsilon_{t}\\right)$ can be achieved e.g. by selecting the first index set in order such that the conditions $(i)$, (ii) of Theorem 1 are satisfied and proceeding as described below equation (4).\n\nEconometric time series analysis (in the realm of stationarity) is almost exclusively concerned with the modelling and prediction of \"regular\" time series. As noted above VAR, VARMA and state space models are all PND (see Deistler and Scherrer, 2022). If we think of $\\left(y_{i t}\\right)$ as a process of (stationarity transformed) economic data, we would not expect that any part of the variation of the process could be explained in the far distant future given information up to now. The same should hold true for the common component which explains a large part of the variation of the observed process. Therefore we assume:\n\nA 2 (Purely Non-Deterministic Dynamic Common Component)\nThe common component $\\left(\\chi_{i t}\\right)$ is PND with orthonormal white noise innovation $\\left(\\varepsilon_{t}\\right)$ (of dimension q) and innovation-form\n\n$$\n\\chi_{i t}=\\underline{k}_{i}(L) \\varepsilon_{t}=\\sum_{j=0}^{\\infty} K_{i}(j) \\varepsilon_{t-j}\n$$\n\nAgain, this assumption resolves only half of the one-sidedness issue as it is not clear whether $\\varepsilon_{t}$ has a representation in terms of current and past $y_{i t}$ 's.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 4 One-Sidedness of the Common Shocks in the Observed Process \n\nConsider the sequence of $1 \\times q$ row transfer functions $\\left(k_{i}: i \\in \\mathbb{N}\\right)$. We construct a sequence of consecutive $q \\times q$ blocks $k^{(j)}:=\\left(k_{(j-1) q+1}^{\\prime}, \\ldots, k_{j q}^{\\prime}\\right)^{\\prime}$. Intuitively, if all $q$ shocks in $\\left(\\varepsilon_{t}\\right)$ are pervasive, $\\mathrm{A} 1(\\mathrm{i})$, we would expect that they are loaded with non-zero filters infinitely often into the common component: In other words with regard to Theorem 3, there exist infinitely many sub-selections of the form $\\tilde{\\chi}_{t}=\\left(\\chi_{i_{1}, t}, \\ldots, \\chi_{i_{q}, t}\\right)^{\\prime}$, such that $\\left(\\tilde{\\chi}_{t}\\right)$ has full rank spectrum a.e. on $[-\\pi, \\pi]$ with innovation $\\left(\\varepsilon_{t}\\right)$.", "tables": {}, "images": {}}, {"section_id": 6, "text": "## Theorem 2\n\nUnder A0-A2, there exists a reordering $\\left(k_{i_{l}}: l \\in \\mathbb{N}\\right)$ of the sequence $\\left(k_{i}: i \\in \\mathbb{N}\\right)$ such that all consecutive $q \\times q$ blocks $\\left(k^{(j)}\\right)$ of $\\left(k_{i_{l}}: l \\in \\mathbb{N}\\right)$ have full rank $q$ a.e. on $[-\\pi, \\pi]$.\n\nProof. By A1(i), we know that\n\n$$\n\\mu_{q}\\left(f_{\\chi}^{n}\\right)=\\mu_{q}\\left(\\left(k^{n}\\right)^{*} k^{n}\\right) \\rightarrow \\infty \\quad \\text { a.e. on }[-\\pi, \\pi]\n$$\n\nwith $k^{n}=\\left(k_{1}^{\\prime}, \\ldots, k_{n}^{\\prime}\\right)^{\\prime}$.\nSince $\\underline{k}_{i}$ is analytic in the open unit disc, it follows that either $k_{i}(\\theta)=0$ a.e. on $[-\\pi, \\pi]$ or $k_{i}(\\theta) \\neq 0$ a.e. on $[-\\pi, \\pi]$. If $k_{i}(\\theta)=0$ a.e, then $\\chi_{i t}=0$ and therefore $\\chi_{i t} \\in \\mathbb{H}_{t}(y)$. By (8) the number of non-zero rows $k_{i}$ must be infinite. Therefore while reordering, we assume henceforth that all $k_{i} \\neq 0$ a.e. on $[-\\pi, \\pi]$ without loss of generality.\n\nWe prove the statement by constructing the reordering using induction. By Theorem 1 and equation (8), we can build the first $q \\times q$ block, having full rank a.e. on $[-\\pi, \\pi]$ by selecting the first linearly independent rows $i_{1}, \\ldots, i_{q}$ of the sequence of row transfer functions $\\left(k_{i}: n \\in \\mathbb{N}\\right)$, i.e., set $k^{(1)}=\\left(k_{i_{1}}^{\\prime}, \\ldots k_{i_{q}}^{\\prime}\\right)^{\\prime}$.\n\nNow look at the block $j+1$ : We use the next $k_{i}$ available in order, as the first row of $k^{(j+1)}$, i.e., $k_{i_{j q+1}}$. Suppose we cannot find $k_{i}$ with $i \\in \\mathbb{N} \\backslash\\left\\{i_{l}: l \\leq j q+1\\right\\}$ linearly independent of $k_{i_{j q+1}}$. Consequently, having built already $j$ blocks of rank $q$, all subsequent blocks that we can build from any reordering are of rank 1 a.e. on $[-\\pi, \\pi]$. In general, for $\\bar{q}<q$, suppose we cannot find rows $k_{i_{j q+\\bar{q}+1}}, \\ldots, k_{i_{j q+q}}$ linearly independent of $k_{i_{j q+1}}, \\ldots, k_{i_{j q+\\bar{q}}}$, then all consecutive blocks that we can obtain from any reordering have at most rank $\\bar{q}$.\n\nFor all $m=j+1, j+2, \\ldots$ by the RQ-decomposition we can factorise $k^{(m)}=R^{(m)}(\\theta) Q^{(m)}$, where $Q^{(m)} \\in \\mathbb{C}^{q \\times q}$ is orthonormal and $R^{(m)}(\\theta)$ is lower triangular $q \\times q$ filter which is analytic in the open unit disc. For $n \\geq i_{j}$ and without loss of generality that $n$ is a multiple of $q$, the\n\nreordered sequence looks like\n\n$$\n\\begin{aligned}\n\\left(k^{n}\\right)^{*} k^{n} & =\\sum_{l=1}^{n} k_{i l}^{*} k_{i l} \\\\\n& =\\left[\\left(k^{(1)}\\right)^{*} \\quad \\cdots \\quad\\left(k^{(j)}\\right)^{*}\\right]\\left[\\begin{array}{c}\nk^{(1)} \\\\\n\\vdots \\\\\nk^{(j)}\n\\end{array}\\right]+\\left[\\left(R^{(j+1)}\\right)^{*} \\quad \\cdots \\quad\\left(R^{(n / q)}\\right)^{*}\\right]\\left[\\begin{array}{c}\nR^{(j+1)} \\\\\n\\vdots \\\\\nR^{(n / q)}\n\\end{array}\\right] \\\\\n& =\\left[\\left(k^{(1)}\\right)^{*} \\quad \\cdots \\quad\\left(k^{(j)}\\right)^{*}\\right]\\left[\\begin{array}{c}\nk^{(1)} \\\\\n\\vdots \\\\\nk^{(j)}\n\\end{array}\\right]+\\left(\\begin{array}{cc}\n\\times & 0 \\\\\n0 & 0\n\\end{array}\\right)=A+B^{n}, \\text { say }\n\\end{aligned}\n$$\n\nwhere $\\times$ is a placeholder. By the structure of the reordering, there are $q-\\bar{q}$ zero end columns/rows in $B^{n}$ for all $n \\geq j q$ where $A$ remains unchanged.\n\nNow by Lancaster and Tismenetsky (1985, theorem 1, p.301), we have\n\n$$\n\\begin{aligned}\n\\mu_{q}\\left(\\left(k^{n}\\right)^{*} k^{n}\\right) & =\\mu_{q}\\left(A+B^{n}\\right) \\\\\n& \\leq \\mu_{1}(A)+\\mu_{q}\\left(B^{n}\\right) \\\\\n& =\\mu_{1}(A)<\\infty \\text { for all } n \\in \\mathbb{N} \\text { a.e. on }[-\\pi, \\pi]\n\\end{aligned}\n$$\n\nThis also implies that for any reordering the $q$-th eigenvalue of the resulting inner product of the transfer function as in equation (8) is bounded by $\\mu_{1}(A)$. This is a contradiction and completes the induction step and the proof.\n\nIn the following, we suppose that $\\left(k_{i}: \\in \\mathbb{N}\\right)$ is ordered as in Theorem 2, such that all consecutive $q \\times q$ blocks have full rank $q$. In the proof of the following theorem, it is convenient to assume that the inverse of a block $\\left(k^{(j)}\\right)^{-1}$ is absolutely summable which is ensured by", "tables": {}, "images": {}}, {"section_id": 7, "text": "# A 3 \n\nThere exists $\\delta>0$ and a sub-sequence $\\left(k^{(j_{l})}: l \\in \\mathbb{N}\\right)$ of $\\left(k^{(j)}: j \\in \\mathbb{N}\\right)$ such that $\\mu_{q}\\left(k^{j_{l}}\\left(k^{j_{l}}\\right)^{*}\\right)>$ $\\delta>0$ a.e. on $[-\\pi, \\pi]$ for all $j_{l}, l \\in \\mathbb{N}$.\n\nThis is similar to the commonly employed assumption in linear systems theory that the transfer function is strictly minimum-phase, i.e., has no zeros on the unit circle as assumed in Deistler et al. (2010), section 2.3 or in Forni et al. (2017), Assumption 7. We will show on a counter-example below how to relax this condition.", "tables": {}, "images": {}}, {"section_id": 8, "text": "## Theorem 3\n\nSuppose A0-A3 hold for $\\left(y_{i t}\\right)$, then the innovations $\\left(\\varepsilon_{t}\\right)$ of $\\left(\\chi_{i t}\\right)$ are causally subordinated to the observed variables $\\left(y_{i t}\\right)$, i.e., $\\varepsilon_{t} \\in \\mathbb{H}_{t}(y)$.\n\nProof. Suppose $\\left(k_{i}: i \\in \\mathbb{N}\\right)$ is re-ordered such that all $q \\times q$ blocks $k^{(j)}$ for $j=1,2, \\ldots$ are of full rank a.e. on $[-\\pi, \\pi]$ (such as in lemma 2). Next, redefine $\\left(k_{i}: i \\in \\mathbb{N}\\right)$ as a suitable sub-sequence of itself such that A3 holds. Suppose $q$ divides $n$ without loss of generality. We follow the idea\n\nof blocking as in Forni et al. (2015) but with $q$ instead of $q+1$ dimensional blocks. Looking at\n\n$$\n\\chi_{t}^{n}=\\left(\\begin{array}{c}\n\\chi_{t}^{(1)} \\\\\n\\chi_{t}^{(2)} \\\\\n\\vdots \\\\\n\\chi_{t}^{(n / q)}\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\underline{k}^{(1)}(L) \\\\\n\\vdots \\\\\n\\underline{k}^{(n / q)}(L)\n\\end{array}\\right) \\varepsilon_{t}=\\left(\\begin{array}{lll}\n\\underline{k}^{(1)}(L) & & \\\\\n& \\ddots & \\\\\n& & \\underline{k}^{(n / q)}(L)\n\\end{array}\\right)\\left(\\begin{array}{l}\nI_{q} \\\\\n\\vdots \\\\\nI_{q}\n\\end{array}\\right) \\varepsilon_{t}\n$$\n\nby Theorem 1, we know that all inverse transfer functions $\\left(k^{(j)}\\right)^{-1}, j=1, \\ldots, n / q$ are one-sided as well. Next we show that $\\left(\\varphi_{i t}\\right)$ in\n\n$$\n\\begin{aligned}\n& \\varphi_{t}^{n}:=\\left(\\begin{array}{llll}\n\\left(\\underline{k}^{(1)}\\right)^{-1}(L) & & & \\\\\n& \\ddots & & \\\\\n& & \\left(\\underline{k}^{(n / q)}\\right)^{-1}(L)\n\\end{array}\\right)\\left(\\begin{array}{c}\ny_{t}^{(1)} \\\\\n\\vdots \\\\\ny_{t}^{(n / q)}\n\\end{array}\\right) \\\\\n& =\\left(\\begin{array}{l}\nI_{q} \\\\\n\\vdots \\\\\nI_{q}\n\\end{array}\\right) \\varepsilon_{t}+\\left(\\begin{array}{lll}\n\\left(\\underline{k}^{(1)}\\right)^{-1}(L) & & \\\\\n& \\ddots & \\\\\n& & \\left(\\underline{k}^{(n / q)}\\right)^{-1}(L)\n\\end{array}\\right)\\left(\\begin{array}{c}\n\\xi_{t}^{(1)} \\\\\n\\vdots \\\\\n\\xi_{t}^{(n / q)}\n\\end{array}\\right)=C_{t}^{\\varphi, n}+e_{t}^{\\varphi, n}, \\text { say, }\n\\end{aligned}\n$$\n\nhas a static factor structure (see definition 5). Then $\\varepsilon_{t}$ can be recovered from static aggregation/ via static principal components applied to $\\left(\\varphi_{i t}\\right)$ by Theorem 5.2. Firstly, $q$ eigenvalues of $\\Gamma_{C^{\\varphi}}^{n}=$ $\\mathbb{E}\\left[C_{t}^{\\varphi, n} C_{t}^{\\varphi, n^{\\prime}}\\right]$ diverge (at rate $n$ ), so A4(i) holds.\n\nWe are left to show that the first eigenvalue of $\\Gamma_{e^{\\varphi}}^{n}=\\mathbb{E}\\left[e_{t}^{\\varphi, n} e_{t}^{\\varphi, n^{\\prime}}\\right]$ is bounded in $n$, i.e., A4(ii) holds. Let $U_{j} \\Sigma_{j} V_{j}^{*}=k^{(j)}(\\theta)$ be the singular value decomposition of $k^{(j)}(\\theta)$, where we suppressed the dependence on $\\theta$ in the notation on the LHS. Let $f_{\\xi}^{n}(\\theta)=P^{*} M P$ be the eigendecomposition of $f_{\\xi}^{n}$ with orthonormal eigenvectors being the rows of $P$ and eigenvalues in the diagonal matrix $M$ (omitting dependence on $n$ ). Then\n\n$$\nf_{e^{\\varphi}}^{n}(\\theta)=\\bigoplus_{j=1}^{J} U_{j} \\underbrace{\\bigoplus_{j=1}^{J} \\Sigma_{j}^{-1} \\bigoplus_{j=1}^{J} V_{j}^{*} P^{*} M P \\bigoplus_{j=1}^{J} V_{j} \\Sigma_{j}^{-1}}_{B^{*}(\\theta)} \\bigoplus_{j=1}^{J} U_{j}^{*}\n$$\n\nwhere we used $\\bigoplus_{j=1}^{J} A_{j}$ to denote the block diagonal matrix with the square matrices $A_{j}$ for $j=1, \\ldots, J$ on the main diagonal block. The largest eigenvalue of $f_{e^{\\varphi}}^{n}(\\theta)$ is equal to the largest eigenvalue of $B^{n}(\\theta)$. Therefore\n\n$$\n\\begin{aligned}\n\\mu_{1}\\left(\\Gamma_{e^{\\varphi}}^{n}\\right)=\\mu_{1}\\left(\\int_{-\\pi}^{\\pi} f_{e^{\\varphi}}^{n}\\right) & \\leq \\int_{-\\pi}^{\\pi} \\mu_{1}\\left(f_{e^{\\varphi}}^{n}\\right) \\\\\n& \\leq 2 \\pi \\operatorname{ess} \\sup _{\\theta \\in[-\\pi, \\pi]} \\sup _{n \\in \\mathbb{N}} \\mu_{1}\\left(f_{e^{\\varphi}}^{n}\\right) \\\\\n& \\leq 2 \\pi \\operatorname{ess} \\sup _{\\theta \\in[-\\pi, \\pi]} \\sup _{n \\in \\mathbb{N}} \\mu_{1}\\left(f_{\\xi}^{n}\\right)\\left(\\inf _{j} \\mu_{q}\\left(\\Sigma_{j}\\right)\\right)^{-2} \\\\\n& <2 \\pi \\operatorname{ess} \\sup _{\\theta \\in[-\\pi, \\pi]} \\sup _{n \\in \\mathbb{N}} \\mu_{1}\\left(f_{\\xi}^{n}\\right)(\\delta)^{-2}<\\infty\n\\end{aligned}\n$$\n\nThis completes the proof.\nNote that we employed A3 in the proof of Theorem 3 to ensure that the filtered idiosyncratic blocks $\\left(k^{(j)}\\right)^{-1}(L) \\xi_{t}^{(j)}=e_{t}^{\\varphi,(j)}$ have finite variance such that $\\left(e_{i t}^{\\varphi}\\right)$ is statically idiosyncratic. One may object, that this excludes processes with transfer functions which have zeros on the unit circle in almost all blocks - even though this is a non-generic case.\n\nFor instance, consider the model\n\n$$\ny_{i t}=\\chi_{i t}+\\xi_{i t}=(1-L) \\varepsilon_{t}+\\xi_{i t}=\\zeta_{t}+\\xi_{i t}\n$$\n\nwhere $\\left(\\xi_{i t}\\right)$ is dynamically idiosyncratic. Also in this case we can recover $\\left(\\varepsilon_{t}\\right)$ one-sided in the $\\left(y_{i t}\\right)$ : The cross-sectional average $\\bar{y}_{t}^{n}=n^{-1} \\sum_{i=1}^{n} y_{i t}$ is a static aggregation of $\\left(y_{i t}\\right)$ and therefore $\\bar{y}_{t}^{n} \\rightarrow \\zeta_{t}$ converges in mean square (see Gersing, 2023, 2024, for details and the appendix for a short summary). Furthermore $\\left(\\zeta_{t}\\right)$ is PND and by the Wold representation Theorem, the innovations are recovered from the infinite past $\\mathbb{H}_{t}(\\zeta) \\subset \\mathbb{H}_{t}(y)$ : Note that it is enough for the inverse of the linear transformation $(1-L)$ to exist only for the input $\\left(\\zeta_{t}\\right)$. Since\n\n$$\n\\begin{aligned}\n& f_{\\zeta}(\\theta)=\\left(1-e^{-\\iota \\theta}\\right) f_{\\varepsilon}(\\theta)\\left(1-e^{\\iota \\theta}\\right)=\\left(1-e^{-\\iota \\theta}\\right) \\frac{1}{2 \\pi}\\left(1-e^{\\iota \\theta}\\right) \\\\\n& \\int_{-\\pi}^{\\pi}\\left(1-e^{-\\iota \\theta}\\right)^{-1} f_{\\zeta}(\\theta)\\left(1-e^{\\iota \\theta}\\right)^{-1}=\\int_{-\\pi}^{\\pi} \\frac{1}{2 \\pi}=1<\\infty\n\\end{aligned}\n$$\n\nwe know that $\\left(1-e^{-\\iota \\theta}\\right)^{-1}$ is an element of the frequency domain of $\\left(\\zeta_{t}\\right)$ and therefore has an inverse also in the time domain (see also Anderson et al., 2016, section 5).\n\nMore generally, we may employ this procedure by factoring out the zeros from the analytic functions $k^{(j)}$. Let $\\operatorname{det} \\underline{k}^{(j)}=\\underline{g}_{j} \\underline{h}_{j}$, where $\\underline{g}_{j}$ is a polynomial defined by the zeros of $\\operatorname{det} \\underline{k}^{(j)}$ which are on the unit circle. Recall that the zeros of an analytic function are isolated, so if $z_{0}$ is a zero, we have $\\underline{h}_{j}(z) \\neq 0$ in a neighbourhood around $z_{0}$. Furthermore the degree of a zero can be only finite or the function is zero everywhere. Thus there can be only finitely many different zeros on the unit circle, since the unit circle is a compact set. Write $\\underline{g}_{j}(z)=\\prod_{k_{j}=1}^{J_{j}}\\left(z-z_{j k_{j}}\\right)^{m_{k_{j}}}$ with $\\left|z_{j k_{j}}\\right|=1$ for $k_{j}=1, \\ldots, J_{j}$ and $j \\in \\mathbb{N}$. It follows that $g_{j}^{-1}(\\theta) \\operatorname{det} k^{(j)}(\\theta) \\neq 0$ a.e. on $[-\\pi, \\pi]$, where $g_{j}(\\theta):=\\underline{g}_{j}\\left(e^{-i \\theta}\\right)$.\n\nConsequently setting $\\underline{k}^{(j)}(L)=\\underline{g}_{j}(L) \\underline{h}^{(j)}(L)$, we have\n\n$$\n\\varphi_{t}^{n}=\\left(\\begin{array}{c}\n\\underline{g}_{1}(L) I_{q} \\\\\n\\vdots \\\\\n\\underline{g}_{n / q}(L) I_{q}\n\\end{array}\\right) \\varepsilon_{t}+\\left(\\begin{array}{c}\n\\left(\\underline{h}^{(1)}\\right)^{-1}(L) \\\\\n\\ddots \\\\\n\\left(\\underline{h}^{(n / q)}\\right)^{-1}(L)\n\\end{array}\\right)\\left(\\begin{array}{c}\n\\xi_{t}^{(1)} \\\\\n\\vdots \\\\\n\\zeta_{t}^{(n / q)}\n\\end{array}\\right)=C_{t}^{\\varphi, n}+e_{t}^{\\varphi, n}\n$$\n\nwith $\\operatorname{det} \\underline{h}_{j}(L) \\neq 0$ a.e. on $[-\\pi, \\pi]$. If A3 holds for $\\left(h^{(j)}\\right)$ instead of $\\left(k^{(j)}\\right)$, with the same arguments as above, we obtain a one-sided representation of $\\left(\\varepsilon_{t}\\right)$, if there exists a static averaging\n\nsequence $\\left(\\tilde{c}_{i}^{(n)}:(i, n) \\in \\mathbb{N} \\times \\mathbb{N}\\right)$ (see definition 2) such that\n\n$$\n\\zeta_{t}^{n}=\\sum_{i=1}^{n} c_{i}^{(n)} \\varphi_{i t}=\\sum_{i=1}^{n} c_{i}^{(n)}\\left(C_{i t}^{\\varphi}+e_{i t}^{\\varphi}\\right)\n$$\n\nconverges in mean square to a PND process, say $\\zeta_{t}$, with innovations $\\left(\\varepsilon_{t}\\right)$, with $\\varphi_{i t}, C_{i t}^{\\varphi}, e_{i t}^{\\varphi}$ from equation (10).\n\nExtending these considerations to extremely special cases of transfer-functions, e.g. where there exists no sub-sequence to a achieve a lower bound $\\delta$ for $\\mu_{q}\\left(h_{j_{l}}\\left(h_{j_{l}}\\right)^{*}\\right)$ or where $\\varepsilon_{t}$ cannot be retrieved from a static aggregate does not seem to have much practical relevance.", "tables": {}, "images": {}}, {"section_id": 9, "text": "# 5 Conclusion \n\nThe dynamic common component of the GDFM is the mean square limit of a $q$-dimensional low-rank approximation obtained via dynamic principal components. While such approximations generally involve two-sided filtrations of the observed process, this is not the case for the common component of the GDFM. Assuming the dynamic common component is purely nondeterministic, we consider its analytic transfer function: First, we factor out any potential zeros on the unit circle. Then, we apply the causal inverse transfer functions of consecutive $q \\times q$ blocks of the common component to the observed process, yielding a static factor structure. Finally, we extract the common shocks by eliminating the idiosyncratic component of this static factor structure through cross-sectional aggregation.", "tables": {}, "images": {}}, {"section_id": 10, "text": "## Acknowledgements\n\nI am grateful foremost to my PhD supervisor Manfred Deistler for his helpful suggestions and support. Furthermore I would like to thank Paul Eisenberg, Sylvia Fr\u00fchwirth-Schnatter for helpful comments that led to the improvement of the paper. Financial support from the Austrian Central Bank under Anniversary Grant No. 18287 and the DOC-Fellowship of the Austrian Academy of Sciences (\u00d6AW) and from the University of Vienna is gratefully acknowledged.", "tables": {}, "images": {}}, {"section_id": 11, "text": "## Data Availability Statement\n\nData sharing is not applicable to this article as no datasets were generated or analysed during the current study.", "tables": {}, "images": {}}, {"section_id": 12, "text": "## References\n\nAnderson, B. D. and Deistler, M. (2008). Generalized linear dynamic factor models-a structure theory. In 2008 47th IEEE Conference on Decision and Control, pages 1980-1985. IEEE.\n\nAnderson, B. D., Deistler, M., Felsenstein, E., Funovits, B., Koelbl, L., and Zamani, M. (2016). Multivariate ar systems and mixed frequency data: G-identifiability and estimation. Econometric Theory, 32(4):793-826.\n\nBarigozzi, M. and Hallin, M. (2024). The dynamic, the static, and the weak factor models and the analysis of high-dimensional time series. arXiv preprint arXiv:2407.10653.\n\nBarigozzi, M., Hallin, M., Luciani, M., and Zaffaroni, P. (2024). Inferential theory for generalized dynamic factor models. Journal of Econometrics, 239(2):105422.\n\nChamberlain, G. and Rothschild, M. (1983). Arbitrage, factor structure, and mean-variance analysis on large asset markets. Econometrica: Journal of the Econometric Society, pages $1281-1304$.\n\nDeistler, M., Anderson, B. D., Filler, A., Zinner, C., and Chen, W. (2010). Generalized linear dynamic factor models: An approach via singular autoregressions. European Journal of Control, 16(3):211-224.\n\nDeistler, M. and Scherrer, W. (2022). Time Series Models, volume Lecture Notes in Statistics 224. Springer.\n\nForni, M., Hallin, M., Lippi, M., and Reichlin, L. (2000). The generalized dynamic-factor model: Identification and estimation. Review of Economics and statistics, 82(4):540-554.\n\nForni, M., Hallin, M., Lippi, M., and Reichlin, L. (2004). The generalized dynamic factor model consistency and rates. Journal of Econometrics, 119(2):231-255.\n\nForni, M., Hallin, M., Lippi, M., and Reichlin, L. (2005). The generalized dynamic factor model: one-sided estimation and forecasting. Journal of the American statistical association, $100(471): 830-840$.\n\nForni, M., Hallin, M., Lippi, M., and Zaffaroni, P. (2015). Dynamic factor models with infinitedimensional factor spaces: One-sided representations. Journal of econometrics, 185(2):359371 .\n\nForni, M., Hallin, M., Lippi, M., and Zaffaroni, P. (2017). Dynamic factor models with infinitedimensional factor space: Asymptotic analysis. Journal of Econometrics, 199(1):74-92.\n\nForni, M. and Lippi, M. (2001). The generalized dynamic factor model: representation theory. Econometric theory, 17(6):1113-1141.\n\nForni, M. and Lippi, M. (2011). The general dynamic factor model: One-sided representation results. Journal of Econometrics, 163(1):23-28.\n\nForni, M. and Lippi, M. (2024). Approximating singular by means of non-singular structural vars. In Recent Advances in Econometrics and Statistics: Festschrift in Honour of Marc Hallin, pages 325-342. Springer.\n\nGersing, P. (2023). Reconciling the Theory of Factor Sequences. PhD thesis, Vienna University of Technology.\n\nGersing, P. (2024). A distributed lag approach to the generalised dynamic factor model. arXiv preprint arXiv:2410.20885.\n\nGersing, P., Rust, C., Deistler, M., and Barigozzi, M. (2024). Weak factors are everywhere. arXiv preprint arXiv:2307.10067.\n\nHallin, M. and Lippi, M. (2013). Factor models in high-dimensional time series-a time-domain approach. Stochastic processes and their applications, 123(7):2678-2695.\n\nHallin, M. and Li\u0161ka, R. (2007). Determining the number of factors in the general dynamic factor model. Journal of the American Statistical Association, 102(478):603-617.\n\nHannan, E. J. and Deistler, M. (2012). The statistical theory of linear systems. SIAM.\nLancaster, P. and Tismenetsky, M. (1985). The theory of matrices: with applications. Elsevier.\nMasani, P. and Wiener, N. (1957). The prediction theory of multivariate stochastic processes. Acta Math, 98:111-150.\n\nRozanov, A. (1967). Stationary random processes. Holden-Day.\nSzabados, T. (2022). Regular multidimensional stationary time series. Journal of Time Series Analysis, 43(2):263-284.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# A Background: Hilbert Space Theory for the Static Case \n\nConsider infinite dimensional constant row-vectors of cross-sectional weights $\\hat{c}=\\left(\\hat{c}_{1}, \\hat{c}_{2}, \\cdots\\right) \\in$ $\\mathbb{R}^{1 \\times \\infty}$ and write $\\hat{c}^{\\{n\\}}:=\\left(\\hat{c}_{1}, \\cdots \\hat{c}_{n}\\right)$ for the truncated vector. Denote by $\\hat{L}_{2}^{\\infty}\\left(\\Gamma_{y}^{n}\\right)$ the set of vectors such that $\\lim _{n \\rightarrow \\infty} \\hat{c}^{\\{n\\}} \\Gamma_{y}^{n}\\left(\\hat{c}^{\\{n\\}}\\right)^{\\prime}<\\infty$, where $\\Gamma_{y}^{n}=\\mathbb{E}\\left[y_{t}^{n} y_{t}^{n^{\\prime}}\\right]$ and by $\\hat{L}_{2}^{\\infty}(I)$ the set of vectors such that $\\lim _{n \\rightarrow \\infty} \\hat{c}^{\\{n\\}}\\left(\\hat{c}^{\\{n\\}}\\right)^{\\prime}<\\infty$. Dynamic averaging sequences have been introduced by Forni and Lippi (2001). In Gersing (2023) those are paralleled with static averaging sequences. For an alternative averaging scheme see Barigozzi and Hallin (2024).\n\nDefinition 2 (Static Averaging Sequence (SAS))\nLet $\\hat{c}^{(k)} \\in \\hat{L}_{2}^{\\infty}(I) \\cap \\hat{L}_{2}^{\\infty}\\left(\\Gamma_{y}\\right) \\cap \\mathbb{R}^{1 \\times \\infty}$ for all $k \\in \\mathbb{N}$. The sequence $\\left(\\hat{c}^{(k)}: k \\in \\mathbb{N}\\right)$ is called Static Averaging Sequence (SAS) if\n\n$$\n\\lim _{k \\rightarrow \\infty} \\hat{c}^{(k)}\\left(\\hat{c}^{(k)}\\right)^{\\prime}=\\lim _{k \\rightarrow \\infty}\\left\\|\\hat{c}^{(k)}\\right\\|_{\\hat{L}_{2}^{\\infty}(I)}=0\n$$\n\nWe denote the set of all static averaging sequences corresponding to $\\left(y_{i t}\\right)$ as\n\n$$\n\\mathcal{S}\\left(\\Gamma_{y}\\right):=\\left\\{\\left(\\hat{c}^{(k)}\\right): \\hat{c}^{(k)} \\in \\hat{L}_{2}^{\\infty}(I) \\cap \\hat{L}_{2}^{\\infty}\\left(\\Gamma_{y}\\right) \\cap \\mathbb{R}^{1 \\times \\infty} \\forall k \\in \\mathbb{N} \\text { and } \\lim _{k \\rightarrow \\infty}\\left\\|\\hat{c}^{(k)}\\right\\|_{\\hat{L}_{2}^{\\infty}(I)}=0\\right\\}\n$$\n\nDefinition 3 (Statically Idiosyncratic)\nA stochastic double sequence $\\left(z_{i t}\\right)$ is called statically idiosyncratic, if $\\lim _{k \\rightarrow \\infty} \\mathbb{E}\\left[\\hat{c}^{\\{k\\}} z_{t}^{k}\\right]^{2}=0$ for all $\\left(\\hat{c}^{(k)}\\right) \\in \\mathcal{S}\\left(\\Gamma_{z}\\right)$ for all $t \\in \\mathbb{Z}$.\n\nThe following Theorem has been stated for the dynamic case in Forni and Lippi (2001), Theorem 1:\n\nTheorem 4 (Characterisation of Statically Idiosyncratic)\nThe following statements are equivalent:\n(i) A stationary stochastic double sequence $\\left(z_{i t}\\right)$ is statically idiosyncratic;\n(ii) the first eigenvalue of the variance matrix is bounded, i.e.,\n\n$$\n\\sup _{n \\in \\mathbb{N}} \\mu_{1}\\left(\\Gamma_{z}^{n}\\right)<\\infty\n$$\n\nThe proof is parallel to the dynamic case treated in Forni and Lippi (2001), for details see Gersing (2023).\n\nThe set of all random variables that can be written as the mean square limit of a static average defines a closed subspace of $\\overline{\\mathfrak{s p}}\\left(y_{i t}: \\in \\mathbb{N}\\right)$ (the proof is analogous to Forni and Lippi, 2001, Lemma 6). Denote by \"ms lim\" the mean square limit.\n\nDefinition 4 (Static Aggregation Space)\nThe space $\\mathbb{S}_{t}(y):=\\left\\{z_{t}: z_{t}=\\operatorname{mslim}_{k \\rightarrow \\infty} \\widetilde{c}^{(k)} y_{t}\\right.$, where $\\left.\\left(\\widetilde{c}^{(k)}\\right) \\in \\mathcal{S}\\left(\\Gamma_{y}\\right)\\right\\} \\subset \\overline{\\mathfrak{s p}}\\left(y_{t}\\right)$ is called Static Aggregation Space at time $t$.\n\nNote that the static aggregation space changes with $t \\in \\mathbb{Z}$ as it emerges from aggregations over the cross-section of $y_{i t}$ - holding $t$ fixed.\n\nWe may suppose that $\\left(y_{i t}\\right)$ has a static factor structure:\nA 4 (r-Static Factor Structure)\nThe process $\\left(y_{i t}\\right)$ can be represented as\n\n$$\ny_{i t}=\\Lambda_{i} F_{t}+e_{i t}=C_{i t}+e_{i t}\n$$\n\nwhere $F_{t}$ is an $r \\times 1$ dimensional process with $r<\\infty, \\mathbb{E}\\left[F_{t}\\right]=0, \\mathbb{E}\\left[F_{t} F_{t}^{\\prime}\\right]=I_{r}$ for all $t \\in \\mathbb{Z}$, $\\mathbb{E}\\left[F_{t} e_{i t}\\right]=0$ for all $i \\in \\mathbb{N}$ and $t \\in \\mathbb{Z}$, and\n(i) $\\sup _{n \\in \\mathbb{N}} \\mu_{r}\\left(\\Gamma_{C}^{n}\\right)=\\infty$;\n(ii) $\\sup _{n \\in \\mathbb{N}} \\mu_{1}\\left(\\Gamma_{e}^{n}\\right)<\\infty$.\n\nWe can compute static low rank approximations (SLRA) of $y_{t}^{n}$ of rank $r$ via \"static\" principal components. For this consider the eigen-decomposition of the variance matrix:\n\n$$\n\\Gamma_{y}^{n}=P_{(n)}^{\\prime} M_{(n)} P_{(n)}\n$$\n\nwhere $P_{(n)}=P_{(n)}\\left(\\Gamma_{y}^{n}\\right)$ is an orthogonal matrix of row eigenvectors and $M_{(n)}=M_{(n)}\\left(\\Gamma_{y}^{n}\\right)$ is a diagonal matrix of the $r$-largest eigenvalues of $\\Gamma_{y}^{n}$ sorted from largest to smallest. Denote by $p_{n j}$ the $j$-th row of $P_{(n)}$ and by $P_{n}:=P_{n r}$ the sub-orthogonal matrix consisting of the first $r$ rows of $P_{(n)}$. Analogously we write $M_{n}$ to denote the $r \\times r$ diagonal matrix of the largest $r$ eigenvalues\n\nof $\\Gamma_{y}^{n}$. Recall that we associate $r$ with the number of divergent eigenvalues of $\\Gamma_{y}^{n}$ (see A4). Set\n\n$$\n\\begin{aligned}\n\\mathcal{K}_{n i} & :=\\mathcal{K}_{n i}\\left(\\Gamma_{y}^{n}\\right):=p_{n i}^{\\prime} P_{n} \\quad \\text { the } i \\text {-th row of } P_{n}^{\\prime} P_{n} \\\\\nC_{t}^{[n]} & :=P_{n r}^{\\prime} P_{n r} y_{t}^{n}=P_{n}^{\\prime} P_{n} y_{t}^{n} \\\\\nC_{i t, n} & :=\\mathcal{K}_{n i} y_{t}^{n} \\quad \\text { the } i \\text {-th row of } C_{t}^{[n]}\n\\end{aligned}\n$$\n\nRecall that $C_{t}^{[n]}$ is the best (with respect to mean squared error) possible approximation of $y_{t}^{n}$ by an $r$ dimensional vector of linear combinations of $y_{1 t}, \\ldots, y_{n t}$. The $r \\times 1$ vector $P_{n} y_{t}^{n}$ are the first $r$ principal components of $y_{t}^{n}$ and provide such a vector of linear combinations, though not uniquely. We call $C_{t}^{[n]}$ the static rank $r$ approximation of $y_{t}^{n}$ which is unique.", "tables": {}, "images": {}}, {"section_id": 14, "text": "# AY 1 \n\nThere exists a natural number $r<\\infty$, such that\n(i) $\\sup _{n \\in \\mathbb{N}} \\mu_{r}\\left(\\Gamma_{y}^{n}\\right)=\\infty$;\n(ii) $\\sup _{n \\in \\mathbb{N}} \\mu_{r+1}\\left(\\Gamma_{y}^{n}\\right)<\\infty$.\n\nTheorem 5 (Chamberlain and Rothschild, 1983)\nConsider a stochastic double sequence $\\left(y_{i t}\\right)$ in $L_{2}(\\mathcal{P}, \\mathbb{C})$, then:\n\n1. AY1 holds if and only if A4 holds; in this case\n2. $C_{i t}=\\operatorname{mslim}_{n \\rightarrow \\infty} C_{i t, n}$\n3. $r, C_{i t}, e_{i t}$ are uniquely identified from the output sequence $\\left(y_{i t}\\right)$;\n4. $C_{i t}=\\operatorname{proj}\\left(y_{i t} \\mid \\mathbb{S}_{t}(y)\\right)$\n\nThe proof is analogous to the proof of the dynamic case provided in Forni and Lippi (2001). For a detailed exposition see Gersing (2023). By Theorem 5.1 the eigenvalue structure of $\\Gamma_{y}^{n}$ in A4 is equivalent to the representation as a factor model. This justifies the name \"static factor sequence\":\n\nDefinition 5 ( $r$-Static Factor Sequence ( $r$-SFS))\nA stochastic double sequence $\\left(y_{i t}\\right)$ in $L_{2}(\\mathcal{P}, \\mathbb{C})$ that satisfies $A 4$ is called $r$-Static Factor Sequence, $r$-SFS.", "tables": {}, "images": {}}], "id": "2410.18159v2", "authors": ["Philipp Gersing"], "categories": ["econ.EM", "math.ST", "stat.TH"], "abstract": "We consider the Generalised Dynamic Factor Model (GDFM) under the assumption\nthat the dynamic common component is purely non-deterministic. We show that,\nunder thisassumption, the common shocks - and consequently, the dynamic common\ncomponent - can be represented in terms of current and past observed variables.\nThis generalises existing results on the so-called \"One-Sidedness Problem\" of\nthe GDFM.", "updated": "2025-03-08T18:52:13Z", "published": "2024-10-23T13:13:55Z"}