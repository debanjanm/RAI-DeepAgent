{"title": "Convergence of Diffusion Models Under the Manifold Hypothesis in\n  High-Dimensions", "sections": [{"section_id": 0, "text": "#### Abstract\n\nDenoising Diffusion Probabilistic Models (DDPM) are powerful state-of-the-art methods used to generate synthetic data from high-dimensional data distributions and are widely used for image, audio, and video generation as well as many more applications in science and beyond. The manifold hypothesis states that high-dimensional data often lie on lowerdimensional manifolds within the ambient space, and is widely believed to hold in provided examples. While recent results have provided invaluable insight into how diffusion models adapt to the manifold hypothesis, they do not capture the great empirical success of these models, making this a very fruitful research direction.\n\nIn this work, we study DDPMs under the manifold hypothesis and prove that they achieve rates independent of the ambient dimension in terms of score learning. In terms of sampling complexity, we obtain rates independent of the ambient dimension w.r.t. the Kullback-Leibler divergence, and $O(\\sqrt{D})$ w.r.t. the Wasserstein distance. We do this by developing a new framework connecting diffusion models to the well-studied theory of extrema of Gaussian Processes.\n\n\nKeywords: Diffusion models, convergence rates, manifold learning", "tables": {}, "images": {}}, {"section_id": 1, "text": "## Contents\n\n1 Introduction ..... 2\n1.1 Related Works ..... 3\n1.2 Our Contribution ..... 4\n2 Preliminaries ..... 5\n2.1 Score-Matching Generative Models ..... 5\n2.2 Manifold Hypothesis ..... 7\n2.3 Notation ..... 9\n3 Approximation of a Score Function in High Dimension ..... 10\n3.1 Construction of the estimator ..... 11\n4 High Probability Bounds on the Score Function ..... 16\n5 Manifold Approximation ..... 18\n\n5.1 Support Estimation ..... 19\n5.2 Manifold Approximation and the Score Function ..... 21\n6 Alternative Approach: Kernel-Based Score Function Approximation ..... 22\n7 Conclusion and Future work ..... 23\nA Geometric Results ..... 28\nB Concentration of the Score Function ..... 29\nB. 1 Correlation with Gaussian Noise ..... 29\nB. 2 Bounds on the Score Function ..... 31\nB. 3 Point Comparision ..... 35\nC Manifold Approximation ..... 37\nC. 1 Relation Between Wasserstein and KL distances. ..... 37\nC. 2 Properties of Support Estimator ..... 39\nC. 3 High-Probability Bounds for the Polynomial Approximation of $M$ ..... 44\nD Score Approximation by Neural Networks ..... 48\nD. 1 Case $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+3}}$ ..... 49\nD.1.1 Approximation of $s^{*}(t, X(t))$ by $s_{t r}^{*}(t, X(t))$ ..... 50\nD.1.2 Polynomial Approximation of $s_{t r}^{*}$ ..... 52\nD.1.3 Neural Networks Approximation ..... 55\nD. 2 Case $\\underline{T} \\leq T_{k} \\leq n^{-\\frac{2}{2 \\alpha+3}}$ ..... 57\nD.2.1 Summary ..... 57\nD.2.2 Score Function Localization ..... 60\nD.2.3 Polynomial Approximation of Measure ..... 61\nD.2.4 Polynomial Approximation of $e_{i}^{*}$ ..... 63\nD.2.5 Neural Network Approximation ..... 66\nD. 3 Generalization error \\& Proof of Theorem 7 ..... 68\nD. 4 Correction of (Oko et al., 2023, Theorem C.4.) ..... 72\nE Proof of Corollary 6 ..... 77\nF Auxilarly Results on Neural Networks ..... 77\nG Bounds on Tangent Spaces ..... 79\nG. 1 Projection on Tangent Space as a Denoiser ..... 79\nG. 2 Correlation Between Tangent Vectors and Gaussian Noise ..... 80", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 1 Introduction \n\nGenerative models Tomczak (2022) represent a cornerstone of modern machine learning, tasked with the synthesis of new samples from an unknown distribution having only access to samples from it. Once trained, they are employed in various applications such as image\n\nand audio generation, natural language processing, and scientific simulations. Prominent examples include Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and, more recently, Diffusion Models.\n\nDiffusion Models Ho et al. (2020); Song et al. (2021), also known as Denoising Diffusion Probabilistic Models (DDPM) or Score-based generative models, have drawn significant attention due to their robust theoretical foundation and exceptional performance in generating high-quality data Dhariwal and Nichol (2021); Watson et al. (2023); Ho et al. (2022); Evans et al. (2024); Yang et al. (2023). They operate by simulating a stochastic process where data is first gradually corrupted with Gaussian noise and then reconstructed back, allowing the model to generate complex data distributions from white noise.\n\nOne of the most intriguing capabilities of diffusion models, supported with a lot of empirical and theoretical evidence Bortoli (2022); Pidstrigach (2022); Stanczuk et al. (2024); Tang and Yang (2024), is their efficiency in learning distributions supported on low-dimensional manifold structures. Even more surprising is that this happens even though diffusion models are solely defined in terms of the ambient space.\n\nThe manifold hypothesis, see e.g. Ma and Fu (2012), postulates that high-dimensional data often lie on lower-dimensional manifolds within the ambient space and offers a framework to explain why complex data can often be represented or approximated using fewer degrees of freedom than their raw dimensionality suggests. Different approaches to incorporate the manifold hypothesis have been fruitfully studied in the literature Aamari and Levrard (2019); Divol (2022); Genovese et al. (2012); Berenfeld et al. (2024); Connor et al. (2021).", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 1.1 Related Works \n\nFormally, we assume that we observe $n$ samples from an unknown $\\alpha$-smooth distribution $\\mu$ that is supported on an unknown $d$-dimensional, $\\beta$-smooth manifold $M$, isometrically embedded into $\\mathbb{R}^{D}$ (see Section 2.2 for definitions). The concept of minimax estimation refers to finding an estimator that minimizes the worst-case error and plays an important role in statistical learning theory. The problem of manifold learning from the minimax perspective was studied by Aamari and Levrard (2019), who proved that the local polynomial estimator achieves the minimax optimal rate $n^{-\\beta / d}$ up to a log-factor. This result was then adapted by Divol (2022) to construct a kernel-based minimax estimator $\\hat{\\mu}_{n}$ of $\\mu$ which converges at the optimal rate $n^{-(\\alpha+1) /(2 \\alpha+d)}$ in the $W_{1}$ metric. However, sampling from $\\hat{\\mu}_{n}$ requires running a costly MCMC algorithm, thereby making it hard to use in practice.\n\nIn contrast, sampling from diffusion models is relatively fast, and the resulting samples are of high quality even in high-dimensional settings Dhariwal and Nichol (2021). On a high level Yang et al. (2023), diffusion models are trained to reverse the forward process that gradually adds noise to the unknown distribution $\\mu$. The resulting reverse process can be described as a Stochastic Differential Equation (SDE) given in terms of the score function $s(t, x)$. More precisely, they first learn an approximation $\\hat{s}(t, x)$ to the true score, and then simulate the discretized SDE to get samples from an approximation $\\hat{\\mu}$ of the true distribution $\\mu$. Thereby two key quantities determining the performance of DDPMs are the accuracy of $\\hat{s}(t, x)$, and the design of the discretization scheme.\n\nIn their seminal paper Oko et al. (2023) obtain (near) minimax convergence rates $n^{-(\\alpha+1) /(2 \\alpha+D)+\\delta}$ of diffusion models for any $\\delta>0$ under the Wasserstein-1 metric, assuming $\\mu$ has $\\alpha$-smooth\n\ncompactly supported density on $\\mathbb{R}^{D}$ and perfect SDE simulation. Recently, Tang and Yang (2024) generalized this result to the case when $\\mu$ is $\\alpha$-smooth and supported on unknown compact $d$-dimensional manifold $M$. They showed that diffusion models achieve the Wasserstein convergence rate of order $D^{\\alpha+d / 2} n^{-(\\alpha+1) /(2 \\alpha+d)}$, which is optimal in $n$ but has a strong dependence on $D$ via the term $D^{\\alpha+d / 2}$. It is worth mentioning that the rate obtained in Divol (2022) is independent of the ambient dimension, raising the question of whether similar results are possible for DDPMs.", "tables": {}, "images": {}}, {"section_id": 4, "text": "# 1.2 Our Contribution \n\nThe strong polynomial dependence on $D$ of bounds $D^{\\alpha+d / 2} n^{-(\\alpha+1) /(2 \\alpha+d)}$ obtained by Tang and Yang (2024) does not allow for a satisfactory explanation of the excellent empirical behavior of diffusion models in typical scenarios, when $D$ is very large, possibly much larger than $n$. For instance if $D \\gtrsim n$ then the error becomes $n^{\\alpha+d / 2} n^{-(\\alpha+1) /(2 \\alpha+d)}$ which goes to infinity with $n$ as soon as $d \\geq 2$. Therefore, in order to understand if the manifold hypothesis provides a reasonable explanation for the success of diffusion models, we need to derive sharper bounds in terms of $D$.\n\nIn this paper, we fill this gap by showing that the normalized score function $\\sigma_{t} s(t, x)$ can be learned by a neural network estimator with the (near) optimal convergence rate $n^{-(\\alpha+1) /(2 \\alpha+d)+\\gamma}$ for any $\\gamma>0$ w.r.t. the score matching loss, implying a (near) optimal rate $D^{1 / 2} n^{-(\\alpha+1)(2 \\alpha+d)}$ in the Wasserstein metric as long as $\\log D=O(\\log n)$. Note that the $D^{1 / 2}$ multiplier appears due to early-stopping, which intuitively convolves the distribution with $D$-dimensional Gaussian noise of variance $\\delta$, thus resulting in $O\\left(D^{1 / 2} \\delta\\right)$ error in $W_{1}$.\n\nIn particular, this bound means that if the smoothness of the density is large compared to the dimension of its support, i.e. $\\alpha \\gg d$, the convergence rate scales as $D^{1 / 2} n^{-1 / 2}$, and thereby to learn a distribution with an error less than $\\varepsilon$ in Wasserstein it is enough to take $n=O\\left(D \\varepsilon^{-2}\\right)$, linear in $D$, number of samples.\n\nThe backbone of both results are novel high-probability bounds on the score function depending only on the intrinsic dimension $d$. To obtain them we leverage classic concentration results on the maximum of Gaussian processes and carefully study how the $D$-dimensional Gaussian noise added during the forward process interacts with vectors on the $d$-dimensional manifold.\n\nWe show that the score function is sensitive only to the points around the denoised preimage, and as a result, the precision with which the score function points in the direction of the denoised point does not depend on the ambient dimension, demonstrating that diffusion models adapt well to the geometry of the manifold.\n\nFinally, we improve the manifold estimator Aamari and Levrard (2019) and construct a dimension-reduction scheme allowing us to use $n$ samples to build an efficient approximation of a $\\beta$-smooth manifold $M$, with error $n^{-\\beta / d}$, by $n$ polynomial surfaces each contained in easy-to-find sub-spaces of dimension $O(\\log n)$.\n\nCombining our high-probability bounds with this dimension reduction scheme, we improve the neural network architecture presented in Tang and Yang (2024), thus obtaining the aforementioned ambient-dimension-free convergence bound $n^{-(\\alpha+1) /(2 \\alpha+d)+\\gamma}$ on the score estimation.\n\nIn Section 2, we introduce the notation and definitions related to diffusion models and manifold learning. In Section 3 we present our main results. In Sections $4-5$ we present key steps in the control of the score matching loss to avoid the curse of dimensionality. In Section 6 we propose an alternative estimator of the score function based on an estimator of $\\mu$ proposed by Divol (2022), and a discussion is provided in Section 7. Finally, the detailed proofs are provided in the appendix.\n\nIn the sequel Potaptchik et al. (2024), applying the bounds obtained in Section 4, we show that the iteration complexity of diffusion models scales linearly in the intrinsic dimension $d$.", "tables": {}, "images": {}}, {"section_id": 5, "text": "# 2 Preliminaries\n### 2.1 Score-Matching Generative Models\n\nThroughout the paper, we consider a data set $\\mathcal{Y}=\\left\\{y_{1}, \\cdots, y_{n}\\right\\}$ consisting of independent, identically distributed samples from $\\mu$. Generative models aim at simulating new data from an approximation of $\\mu$ trained on $\\mathcal{Y}$. We follow the notation used in Oko et al. (2023) to describe the methodology.\n\nLet $\\{B(t)\\}_{t \\in[0, \\bar{T}]}$ denote a $D$-dimensional Brownian motion on $[0, \\bar{T}]$. For a measure $\\mu$ supported on $\\mathcal{X} \\subset \\mathbb{R}^{D}$, we consider the forward process $\\{X(t)\\}_{t \\in[0, \\bar{T}]}$ defined as a standard $D$-dimensional Ornstein-Ulhenbeck process with initial condition $X(0) \\sim \\mu$, which is given as the solution of the SDE\n\n$$\n\\left\\{\\begin{array}{l}\nd X(t)=-X(t) d t+\\sqrt{2} d B(t) \\\\\nX(0) \\sim \\mu\n\\end{array}\\right.\n$$\n\nLet $Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ denote a standard $D$-dimensional random normal vector independent from $X(0)$. It is well known that\n\n$$\nX(t) \\mid X(0) \\stackrel{\\text { dist. }}{=} c_{t} X(0)+\\sigma_{t} Z_{D}\n$$\n\nwhere $c_{t}:=e^{-t}$ and $\\sigma_{t}:=\\sqrt{1-e^{-2 t}}$.\nWe use $\\mu_{t}$ to denote the law of $X(t)$ and $p(t, \\cdot)$ to denote the (unnormalized) marginal density of $X(t)$, i.e.\n\n$$\np(t, x):=\\int_{\\mathcal{X}} e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} \\mu(d y)\n$$\n\nThe backward process $\\{Y(t)\\}_{t \\in[0, \\bar{T}]}$ defined as $Y(t)=X(\\bar{T}-t)$ is given as a solution of the SDE under mild conditions Anderson (1982) on $\\mu$\n\n$$\n\\left\\{\\begin{array}{l}\nd Y(t)=[Y(t)+2 \\nabla \\log p(\\bar{T}-t, Y(t))] d t+\\sqrt{2} d B(t) \\\\\nY(0) \\stackrel{\\text { dist. }}{=} X(\\bar{T})\n\\end{array}\\right.\n$$\n\nBy construction $Y(\\bar{T})=X(0) \\sim \\mu$, in other words, to get a sample from $\\mu$ one can first sample from $Y(0)=X(\\bar{T})$, and then simulate new data from the backward dynamics\n\ngiven in (4). This requires access to samples from $X(\\bar{T})$ and to the score function $s(t, x)=$ $\\nabla \\log p(t, x)$. Using (3) the score function can be expressed in terms of $\\mu$ as\n\n$$\ns(t, x)=\\nabla \\log p(t, x)=\\frac{\\nabla p(t, x)}{p(t, x)}=\\frac{1}{\\sigma_{t}^{2}} \\frac{\\int_{\\mathcal{X}}\\left(c_{t} y-x\\right) e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} \\mu(d y)}{\\int_{\\mathcal{X}} e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} \\mu(d y)}\n$$\n\nThe main idea behind generative score matching models is to simulate the sampling procedure approximately. Firstly, since $X(\\bar{T}) \\xrightarrow{d i d} \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ as $\\bar{T}$ goes to infinity, at a dimensionfress rate, we can approximate $Y(0)=X(\\bar{T})$ by $\\hat{Y}(0) \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$. Secondly, the true score $s(t, x)$ is replaced with an approximation $\\hat{s}(t, x)$ learned using data $\\mathcal{Y}$, and the backward dynamics (4) is replaced by the process\n\n$$\n\\left\\{\\begin{array}{l}\nd \\hat{Y}(t)=\\left[\\hat{Y}(t)+2 \\hat{s}(\\bar{T}-t, \\hat{Y}(t))\\right] d t+\\sqrt{2} B(t) \\\\\n\\hat{Y}(0) \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)\n\\end{array}\\right.\n$$\n\nLet $\\hat{\\mu}_{t}:=\\operatorname{Law}(\\hat{Y}(\\bar{T}-t))$ denotes the law of estimator. A common practice is to use early stopping to avoid the numerical problems with the score function $s(t, x)$ that emerge when $t=0$. When it is applied, one stops the simulation early at time $\\bar{T}-\\underline{T}$ for a small $\\underline{T}>0$ and uses $\\hat{\\mu}_{\\underline{T}}$ as an estimator of $\\mu$.\n\nThe approximation $\\hat{s}(t, x)$ is learned by minimization of the empirical version of the score matching loss which depends on $\\underline{T}<\\bar{T}$ and defined as\n\n$$\n\\int_{\\underline{T}}^{\\bar{T}} \\int_{\\mathbb{R}^{D}}\\|\\hat{s}(t, x)-s(t, x)\\|^{2} p(t, x) d t d x=\\int_{\\underline{T}}^{\\bar{T}} \\mathbb{E}\\|\\hat{s}(t, X(t))-s(t, X(t))\\|^{2} d t\n$$\n\nIn fact, since $s$ is typically not available, in practice one uses a loss equivalent to the above known as the denoising score matching loss, see Song et al. (2021) and the discussion after Eq. (10).\n\nThe score matching loss controls (Oko et al., 2023, Eq. (90)) the Wasserstein loss between the target $\\mu$ and the approximation $\\hat{\\mu}_{\\underline{T}}$. More precisely, let $T_{0}=\\underline{T}<T_{1}<T_{2}<$ $\\ldots T_{K}=\\bar{T}$, then for any $\\delta>0$\n$W_{1}(\\hat{\\mu}, \\mu) \\lesssim \\sqrt{D}\\left(\\sqrt{\\underline{T}}+\\sum_{k=0}^{K-1} \\sqrt{\\log \\delta^{-1} \\cdot \\sigma_{T_{k+1}}^{2} \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|\\hat{s}(t, X(t))-s(t, X(t))\\|^{2} d t}+\\delta+e^{-\\bar{T}}\\right)$.\n\nRemark 1. Eq. (90) in Oko et al. (2023) doesn't address the dependence on the ambient dimension $D$, however, it can be easily deduced from the proof of Lemma D.7.\n\nAnother metric that is often used to assess the performance of Score-matching Generative models is KL distance between $\\hat{Y}(\\bar{T}-\\underline{T})$ and $X(\\underline{T})$. More precisely (Chen et al., 2023, Section 5.2), if $\\mu$ has a finite second moment, then\n\n$$\nD_{\\mathrm{KL}}\\left(\\hat{\\mu}_{\\underline{T}}\\left\\|\\mu_{\\underline{T}}\\right) \\lesssim D e^{-2 \\bar{T}}+\\int_{\\underline{T}}^{\\bar{T}} \\mathbb{E}\\|\\hat{s}(t, X(t))-s(t, X(t))\\|^{2} d t\n$$\n\nwhere we recall that $\\hat{\\mu}_{\\underline{T}}, \\mu_{\\underline{T}}$ are distributions of $\\hat{Y}(\\bar{T}-\\underline{T})$ and $X(\\underline{T})$.\nSince the computation of the loss function (7) requires access to the true score $s(t, x)$, the equivalent denoising score matching loss is used instead during the optimization step. Vincent (2011) showed that there is a constant $C_{\\mu}$ that does not depend on $\\hat{s}$ such that\n\n$$\n\\int_{\\underline{T}}^{\\bar{T}} \\mathbb{E}\\|\\hat{s}(t, X(t))-s(t, X(t))\\|^{2} d t=\\int_{\\underline{T}}^{\\bar{T}} \\mathbb{E}_{X(0) \\sim \\mu} \\mathbb{E}_{Z_{D}}\\left\\|\\hat{s}\\left(t, c_{t} X(0)+\\sigma_{t} Z_{D}\\right)+Z_{D} / \\sigma_{t}\\right\\|^{2} d t+C_{\\mu}\n$$\n\nHence defining for $y \\in M$ the loss function $\\ell_{y}$ as\n\n$$\n\\ell_{y}(\\hat{s}, a, b):=\\int_{a}^{b} \\mathbb{E}_{Z_{D}}\\left\\|\\hat{s}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)+Z_{D} / \\sigma_{t}\\right\\|^{2} d t\n$$\n\nit is enough to control\n\n$$\n\\mathcal{R}(\\hat{s}, a, b)=\\mathbb{E}_{y \\sim \\mu} \\ell_{y}(\\hat{s}, a, b)=\\int_{a}^{b} \\mathbb{E}\\|\\hat{s}(t, X(t))-s(t, X(t))\\|^{2} d t-C_{\\mu}\n$$\n\nfor all $a=T_{k}, b=T_{k+1}$ and $k \\leq K$. Finally, we define the empirical risk $\\mathcal{R}_{\\mathcal{Y}}(\\hat{s}, a, b)$ w.r.t. samples $\\mathcal{Y}=\\left\\{y_{1}, \\ldots, y_{n}\\right\\}$ as\n\n$$\n\\mathcal{R}_{\\mathcal{Y}}(\\hat{s}, a, b)=\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{y_{i}}(\\hat{s}, a, b)\n$$\n\nAnother useful way to interpret the score $s(t, x)$ known as Tweedie's formula Robbins (1956) represents $s(t, x)$ as the expected value of the noise added to get $x$.\n$s(t, x)=\\frac{1}{\\sigma_{t}^{2}} \\frac{\\int_{M}\\left(c_{t} y-x\\right) e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} \\mu(y) d y}{\\int_{M} e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} \\mu(y) d y}=\\int_{M} \\frac{c_{t} y-x}{\\sigma_{t}^{2}} \\mu(d y \\mid t, x) d y=-\\sigma_{t}^{-1} \\mathbb{E}\\left(Z_{D} \\mid X(t)=x\\right)$,\nwhere $\\mu(d y \\mid t, x)$ is defined as the probability measure proportional to $e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} \\mu(d y)$.\nBy Bayes rule $\\mu(d y \\mid t, x)$ is the conditional law of $(X(0) \\mid X(t)=x)$. We will denote the corresponding conditional expectation as\n\n$$\ne(t, x):=\\mathbb{E}(X(0) \\mid X(t)=x)=\\frac{\\int_{M} y \\cdot e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} \\mu(d y)}{\\int_{M} e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} \\mu(d y)}\n$$\n\nsubstituting into (5), we represent the score $s(t, x)$ as\n\n$$\ns(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} e(t, x)-\\frac{x}{\\sigma_{t}^{2}}\n$$", "tables": {}, "images": {}}, {"section_id": 6, "text": "# 2.2 Manifold Hypothesis \n\nIn this paper, we study the behavior of score-matching generative models under the manifold assumption, i.e. we assume that the support $M$ of $\\mu$ is a low dimensional manifold. The manifold hypothesis is particularly relevant as a way to understand the behavior of statistical\n\nor learning algorithms in high dimensions; see for instance Divol (2022) who showed that under the manifold hypothesis, it is possible to construct estimators of $\\mu$ whose Wasserstein distance to $\\mu$ is independent of the ambient dimension $D$. Whether this is feasible in the context of score-matching diffusion models has been an open question so far.\n\nWe mainly follow Divol (2022) to define the class of regular manifolds that we will study. Throughout the paper we denote the distance between a point $x$ and a set $M$ as $\\operatorname{dist}(x, M)=\\inf _{y \\in M}\\|x-y\\|$, the ball in $\\mathbb{R}^{k}$ centered at $x$ with radius $r$, for $k \\geq 1, x \\in \\mathbb{R}^{k}$ and $r>0$, as $B_{k}(x, r)$ and more generally we write $B_{\\mathcal{U}}(x, r)$ for the ball in a space $\\mathcal{U}$. Moreover for $f: \\Omega \\subset \\mathbb{R}^{d_{1}} \\mapsto \\mathbb{R}^{d_{2}}$, we write $d^{i} f(x)$ for the $i$-th differential of $f$ at $x$ and for $k \\in \\mathbb{N}$, the $k$ th H\u00f6lder norm of $f$, when it exists, is denoted by $\\|f\\|_{C^{\\alpha}(\\Omega)}=\\max _{0 \\leq i \\leq k} \\sup _{x \\in \\Omega}\\left\\|d^{i} f(x)\\right\\|_{o p}$, where $\\|\\cdot\\|_{o p}$ denotes the operator norm.\n\nThe key quantity, first introduced in Federer (1959), determining the manifold's regularity is called the reach $\\tau$ and is defined as\n\n$$\n\\begin{aligned}\n\\tau: & =\\sup \\left\\{\\varepsilon \\mid \\forall x \\in M^{\\varepsilon} \\quad \\exists!y \\in M, \\text { s.t. } \\operatorname{dist}(x, M)=\\|x-y\\|\\right\\} \\\\\n\\text { where } M^{\\varepsilon} & =\\left\\{x \\in \\mathbb{R}^{D}: \\operatorname{dist}(x, M)<\\varepsilon\\right\\}\n\\end{aligned}\n$$\n\nThroughout the paper we assume that the reach is bounded from below by $\\tau>\\tau_{\\min }$; we refer to Aamari and Levrard (2019) for a detailed discussion about the necessity of this assumption.\n\nThe smoothness of the manifold is determined in terms of the smoothness of local charts Lee (2013), and in the case when the manifold is embedded into $\\mathbb{R}^{D}$ this means that for all $y \\in M$ the manifold can be locally represented as a graph of a $\\beta$-smooth one-toone function $\\Phi_{y}: U_{y} \\subset \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{D}$, where $0 \\in U_{y}$ is an open set and $\\Phi_{y}(0)=y$. If this holds we say that the manifold $M$ is $\\beta$-smooth.\n\nA natural way to define $\\Phi_{y}$ is in terms of the orthogonal projection $\\pi_{y}:=\\pi_{T_{y} M}$ onto the tangent space $T_{y} M$ at $y$. When $\\tau>0$, the map $\\pi_{y}$ restricted to $M \\cap B_{D}(y, \\tau / 4)$ is one-to-one and $B_{T_{y} M}(0, \\tau / 8) \\subset \\pi_{y}\\left(M \\cap B_{D}(y, \\tau / 4)\\right)$, see for instance Aamari and Levrard (2018). We can then define $\\Phi_{y}$ as the inverse $\\left.\\pi_{y}\\right|_{M \\cap B_{D}(y, \\tau / 4)}$ and $U_{y}=B_{T_{y} M}(0, \\tau / 8)$.\n\nSimilarly we say that a function $f: M \\mapsto \\mathbb{R}$ is $\\alpha$-smooth, if for any $y \\in M$ the function $f \\circ \\Phi_{y}: \\mathbb{R}^{d} \\mapsto \\mathbb{R}$ is $\\alpha$-smooth in the regular sense. The embedding of $M$ into $\\mathbb{R}^{D}$ generates a natural Riemannian metric induced from $\\mathbb{R}^{D}$ together with a volume element $d y$ known as the Hausdorff measure. Finally, it is said that a measure $\\mu$ is $\\alpha$-smooth if the measure $\\mu \\circ \\Phi_{y}^{-1}$ is $\\alpha$-smooth on $\\mathbb{R}^{d}$, i.e. admits an $\\alpha$-smooth density. If a measure $\\mu$ has a density $p$ w.r.t. $d y$, on a chart $U_{y}$, by the change of variables formula, the corresponding density has the form $p(\\Phi(z))\\left|\\nabla \\Phi_{y}(z) \\nabla^{T} \\Phi_{y}(z)\\right|^{-1}$, and since the Jacobian is only guaranteed to be $\\beta-1$ smooth, we will assume that $\\alpha+1 \\leq \\beta$.\n\nTo control smoothness we recall the definition of the $C^{k}$ norm for vector-valued functions. Let $f: \\Omega \\subset \\mathbb{R}^{d_{1}} \\mapsto \\mathbb{R}^{d_{2}}$, then $\\|f\\|_{C^{k}(\\Omega)}=\\max _{0 \\leq i \\leq k} \\sup _{x \\in \\Omega}\\left\\|d^{i} f(x)\\right\\|_{o p}$, where $d^{i} f(x)$ : $\\left(\\mathbb{R}^{d_{1}}\\right)^{\\otimes i} \\mapsto \\mathbb{R}^{d_{2}}$ is the $i$ th differential - a multilinear operator defined as $d^{i} f(x)\\left(v_{1} \\otimes \\ldots \\otimes v_{n}\\right)=$ $d_{v_{1}} \\ldots d_{v_{i}} f(x)$, and $d_{v} f$ is a directional derivative of $f$ along the vector $v \\in \\mathbb{R}^{d_{1}}$. Note that the 0 -th differential is the function itself, so $\\sup _{x \\in \\Omega}\\left\\|d^{0} f(x)\\right\\|_{o p}=\\|f\\|_{\\infty}$ is a regular supnorm.\n\nWith all these definitions in mind, we are finally ready to present our assumptions on a manifold $M$.\n\nAssumption A. The support $M$ of $\\mu$ is a compact, $\\beta$-smooth manifold, of dimension $d \\geq 1$ and reach $\\tau>\\tau_{\\min }$. Additionally, there is a constant $L_{M}>0$ s.t. for all $y \\in M$ the inverse projection function $\\Phi_{y}$ satisfies $\\left\\|\\Phi_{y}\\right\\|_{C^{S}\\left(B_{T_{y} M}(0, \\tau / 8)\\right)} \\leq L_{M}$.\n\nSimilarly, we define a corresponding class of measures.\nAssumption B. The measure $\\mu$ has an $\\alpha$-smooth density $p(y)$ w.r.t. Hausdorff measure on $M$ satisfying $p_{\\min } \\leq p \\leq p_{\\max }$ and there is a constant $L_{\\mu}$ such that for all $y \\in M$ $\\left\\|p \\circ \\Phi_{y}\\right\\|_{C^{s}\\left(B_{T_{y} M}(0, \\tau / 4)\\right)} \\leq L_{\\mu}$.\n\nBesides the dimension $d$, the complexity of the manifold additionally depends on its volume Vol $M$ and its local smoothness which we captured in terms of the reach $\\tau$ and Holder constant $L_{M}$. The complexity of the measure $\\mu$ primarily depends on its similarity to uniform measure, and thus to $p_{\\min }$ and $p_{\\max }$. To take it into account we introduce the constant $C_{\\log }$ defined below.\n\nAssumption C. We assume that there is a constant $C_{\\log }>\\max (\\log 2 d, 4)$ such that the following bounds hold: (i) $e^{-d C_{\\log }}<p(y)<e^{d C_{\\log }}$; (ii) $\\log \\operatorname{Vol} M<e^{d C_{\\log }}$, (iii) $\\min \\left(\\tau, L_{M}^{-1}\\right) \\geq$ $e^{-C_{\\log }}$.\n\nFor convenience we introduce $r_{0}:=\\min \\left(1, \\tau, L_{M}^{-1}\\right) / 8>0$ - the radius of a neighborhood in which the functions $\\Phi_{y}$ for all $y \\in M$ are well behaved, see Proposition 26 for details.\n\nRemark 2. Assumption $C$ imposes different bounds on $r_{0}$ and Vol. Essentially, this follows from a relation between the radius and volume of the d-dimensional sphere, i.e. $\\operatorname{Vol} B_{d}(0, r) \\propto r^{d}$.\n\nThe smoothness of the manifold is defined locally, therefore a common strategy is to find a dense set $\\mathcal{G}$ and then for each point, $G \\in \\mathcal{G}$ consider a manifold in the $r_{0}$-vicinity of this point. The following proposition bounds the size of such a set.\n\nProposition 3. For any $\\varepsilon<r_{0}$ there is an $\\varepsilon$-dense and $\\varepsilon / 2$-sparse set $\\mathcal{G}=\\left\\{G_{1}, \\ldots, G_{N}\\right\\} \\subset$ $M \\subset \\mathbb{R}^{D}$, moreover $N=N(\\varepsilon) \\leq(\\varepsilon / 2)^{-d} \\operatorname{Vol} M$.\n\nProof. Take $\\mathcal{G}$ as a maximal $\\varepsilon$-separated set. By construction, the balls $M \\cap B\\left(G_{i}, \\varepsilon / 2\\right)$ do not intersect, and $\\operatorname{Vol}(M \\cap B(y, \\varepsilon / 2)) \\geq(\\varepsilon / 2)^{d}$ for any $y \\in M$, so $N(\\varepsilon / 2)^{d} \\lesssim \\operatorname{Vol} M$.\n\nLet $\\varepsilon$ and $G_{1}, \\ldots, G_{N}$ be as in Proposition 3. Then we can build a partition $\\left(G_{i}, M_{i}, p_{i}\\right)$ of a measure subordinated to the points $G_{1}, \\ldots, G_{N}$ as follows. We take $M_{i}=\\Phi_{i}\\left(B_{d}(0, \\varepsilon)\\right)$, and define $p_{i}(y)=p(y) \\phi_{i}(y)$, where $\\phi_{1}, \\ldots, \\phi_{N}$-is a smooth partition of unity subordinated to $M_{i}$ and satisfying $\\left.\\phi_{i}\\right|_{B_{D}\\left(G_{i}, \\varepsilon / 2\\right)} \\equiv 1$.", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 2.3 Notation \n\nConvention 1. To simplify notation we assume that diam $M \\leq 1$ and $0 \\in M$. The general case can be obtained by a simple rescaling and shift.\n\nNotation 1. We use $\\lesssim, \\gtrsim, \\simeq$ in cases when the corresponding inequality(equation) holds up to a multiplicative constant that depends only on $\\beta, \\alpha$, the quantities that are responsible for manifold and density regularity. We keep track of quantities $C_{\\log }, D, d$. Similarly, when we say that $n$ is large enough we mean that $n \\geq n_{0}$ where $n_{0}$ may depend on $C_{\\log }, d, \\alpha, \\beta$ but not on $D$.\n\nNotation 2. Throughout the paper we use $t, T$ to denote time, $\\varepsilon, \\delta, \\eta, \\gamma<1$ to denote errors; $D, d$ denote the ambient and manifold dimensions respectively. Unless otherwise stated we assume that $\\log t, \\log T, \\log \\varepsilon, \\log \\delta, \\log \\eta, \\log \\gamma$, and $\\log D$ are all of order $\\log n$. We also use the notation $\\log _{+} x=\\max (\\log x, 0)$.\nNotation 3. Generally, we use the letters $x, y, z$ to denote points in $\\mathbb{R}^{D}, M$, and a tangent space respectively.\n\nNotation 4. We use capital $T_{k}$ to denote the intervals on which we approximate the score function, while we use regular $t_{k}$ to denote the discretization timesteps.\n\nFollowing Oko et al. (2023) we give the formal definition of the class of neural networks we will be working with.\n\nDefinition 4. The class $\\Psi(L, W, S, B)$ of ReLU-neural networks with depth $L$, shape $W=$ $\\left(W_{1}, \\ldots, W_{L}\\right)$, sparsity $S$, and norm constant $B$ is\n\n$$\n\\begin{aligned}\n\\Psi(L, W, S, B)=\\{ \\phi(x) & :=\\left(A^{L} \\operatorname{ReLU}+b^{L}\\right) \\circ \\ldots \\circ\\left(A^{1} \\operatorname{ReLU}+b^{1}\\right)(x) \\mid A^{i} \\in \\mathbb{R}^{W_{i} \\times W_{i+1}} \\\\\n& b^{i} \\in \\mathbb{R}^{W_{i+1}}, \\sum_{i=1}^{L}\\left(\\left\\|A_{i}\\right\\|_{0}+\\left\\|b_{i}\\right\\|_{0}\\right) \\leq S, \\sup _{i}\\left(\\left\\|A_{i}\\right\\|_{\\infty}+\\left\\|b_{i}\\right\\|_{\\infty}\\right) \\leq B,\\}\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 8, "text": "# 3 Approximation of a Score Function in High Dimension \n\nThroughout this section let $\\mu$ be a measure on a $d$-dimensional manifold $M$ satisfying Assumptions $\\mathrm{A}-\\mathrm{C}$, and $\\mathcal{Y}=\\left\\{y_{1}, \\ldots, y_{n}\\right\\}$ be i.i.d. samples from $\\mu$. Let $X(t)$ be the forward process (1) with initial condition $X(0) \\sim \\mu$, denote as $p(t, x)$ the density function (3) of $X(t)$, and as $s(t, x)=\\nabla \\log p(t, x)$ the score function (5).\n\nOur first contribution is the construction of an estimator $\\hat{s}$ that converges to $s(t, x)$ independently of the ambient dimension $D$ with an almost optimal rate. We prove this under the following technical assumption, which we believe can be relaxed with careful handling.\n\nAssumption D. If $d \\geq 3$, the manifold $M$ is $\\beta$-smooth, where $\\beta \\geq \\frac{d}{d-2}(\\alpha+1)$, where $\\alpha$ is smoothness of measure $\\mu$.\n\nTheorem 5. Let $\\mu$ be an $\\alpha$-smooth measure satisfying Assumptions $A-D$ supported on a $d$-dimensional $\\beta$-smooth manifold $M$ embedded into $\\mathbb{R}^{D}$.\n\nLet $X(t)$ be the forward process (1) with initial condition $X(0) \\sim \\mu, p(t, x)$ be the density function (3) of $X(t)$, and $s(t, x)=\\nabla \\log p(t, x)$ be the score function (5).\n\nDenote as $\\mathcal{Y}=\\left\\{y_{1}, \\ldots, y_{n}\\right\\}$ a set of $n$ i.i.d. samples from $\\mu$. Fix any positive $\\gamma<$ $\\frac{\\beta-(\\alpha+1)}{(\\beta-1 / 2)(2 \\alpha+d)}$. Let $\\underline{T} \\geq n^{\\gamma} n^{-\\frac{2(\\alpha+1)}{2 \\alpha+d}}$ and $\\bar{T}=O(\\log n)$. Then, there exists an estimator $\\hat{s}(t, x)=\\hat{s}(t, x, \\mathcal{Y})$ satisfying\n$\\mathbb{E}_{\\mathcal{Y} \\sim \\mu \\cong n} \\int_{\\underline{T}}^{\\bar{T}} \\int_{\\mathbb{R}^{D}} \\sigma_{t}^{2}\\|\\hat{s}(t, x)-s(t, x)\\|^{2} p(t, x) d x d t \\leq \\begin{cases}n^{2 \\gamma \\alpha} \\cdot n^{-\\frac{2(\\alpha+1)}{2 \\alpha+d}}(\\log n)^{\\frac{2 \\alpha+d}{d}}, & \\text { if } d \\geq 3, \\\\ n^{\\gamma} \\cdot n^{-1} & \\text { otherwise }\\end{cases}$\n\nfor all $n$ large enough and independent of $D$.\nThis theorem is a corollary of Theorem 7 presented in Section 3.1, where we also present a detailed description of the estimator $\\hat{s}(t, x)$. Theorem 7 is proved in Appendix D.\n\nSimilar results were obtained in Tang and Yang (2024); Oko et al. (2023), and we generally follow their strategy and build $\\hat{s}$ as a ReLU-neural network(see Definition 4). Our main improvement compared to the literature is that we achieve bounds that do not depend on the ambient dimension $D$, while in Tang and Yang (2024) this bound is multiplied by $D^{d / 2+\\alpha}$.\n\nThe key innovation leading to the result are new regularity bounds on $s(t, x)$ presented in Section 4, and the construction of an efficient manifold approximation algorithm in Section 5 .\nCorollary 6. Under the same assumptions as in Theorem 5, let $\\hat{s}(t, x)$ be an estimator satisfying (16). Assume in addition that $\\log D \\lesssim \\log n$.\n(i) (9) implies that\n\n$$\n\\mathbb{E}_{\\mathcal{Y} \\sim \\mu^{\\otimes n}} D_{\\mathrm{KL}}\\left(\\tilde{Y}(\\bar{T}-\\underline{T}) \\| X(\\underline{T})\\right) \\lesssim \\begin{cases}n^{2 \\gamma \\alpha} n^{-\\frac{2 \\alpha}{2 \\alpha+d}}(\\log n)^{\\frac{(2 \\alpha+d)}{d}} & \\text { if } d \\geq 3 \\\\ n^{\\gamma} \\sigma_{\\underline{T}}^{-2} n^{-1} & \\text { otherwise }\\end{cases}\n$$\n\n(ii) (8) implies that\n\n$$\n\\mathbb{E}_{\\mathcal{Y} \\sim \\mu^{\\otimes n}} W_{1}(\\hat{Y}(\\bar{T}-\\underline{T}), \\mu) \\lesssim \\sqrt{D} \\log n\\left(n^{\\gamma} n^{-1} \\vee n^{2 \\gamma \\alpha} n^{-\\frac{2(\\alpha+1)}{2 \\alpha+d}}(\\log n)^{2 \\alpha / d}\\right)\n$$\n\nProof. See Appendix E.\nSimilarly to Tang and Yang (2024) the bounds involve two terms: $\\sigma_{\\underline{T}}^{-2} n^{-2 \\beta /(2 \\alpha+d)}$ which comes from the estimation of $\\beta$-smooth manifold $M$, and $n^{-2 \\alpha /(2 \\alpha+d)}$ which comes from the estimation of the $\\alpha$-smooth measure $\\mu$ on $M$.", "tables": {}, "images": {}}, {"section_id": 9, "text": "# 3.1 Construction of the estimator \n\nTo provide intuition behind the construction of $\\hat{s}$, let us start with a brief and informal introduction of two auxiliary results that play a key role in that follows. We believe that these results are interesting on their own and devote Section 4 and Section 5 to discuss them.\n\nConcentration of the Score Function (Section 4) Although $s(t, x)$ is defined independently of the low-dimensional structure of $\\mu$, we prove that its regularity properties depend only on the intrinsic dimension $d$ and the logarithm of the measure complexity $C_{\\log }$, rather than on the ambient dimension $D$.\n\nMore precisely, we first recall that by Tweedie's formula (14) the score function $s(t, X(t))$ computed at $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$ a realization of forward process (2) at time $t$ can be represented as\n\n$$\ns(t, X(t))=\\int_{M} \\frac{c_{t} y-X(t)}{\\sigma_{t}^{2}} \\mu(d y \\mid t, X(t)) d y\n$$\n\nThen as we demonstrate in Theorem 12 with high probability measure $\\mu(d y \\mid t, X(t))$ on $M$ is concentrated in $O\\left(\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d C_{\\log }}\\right)$ vicinity of point $X(0)$. This will help us localize the score function and thereby represent it as a regular function in terms of the local charts.\n\nManifold Approximation (Section 5) Next, to efficiently work with the local charts we obtain simple but important improvement of the minimax manifold estimator presented by Aamari and Levrard (2019).\n\nIn Section 5.1, we show that using $N$ i.i.d. samples from $\\mu$ we can approximate manifold $M$ with a piece-wise polynomial surface $M^{*}=\\cup_{i \\leq N} M_{i}^{*}$, where each surface $M_{i}^{*}$ is $O(\\log N)$ dimensional, and construct measure $\\mu^{*}$ on $M^{*}$ such that $W_{2}\\left(\\mu, \\mu^{*}\\right) \\leq N^{-\\frac{2}{2}}$.\n\nFinally, denoting as $s^{*}$ the score function corresponding to the forward process (1) with initial condition $\\mu^{*}$, in Section 5.2, we note that the score matching loss is controlled by Wasserstein distance\n\n$$\n\\int_{T_{\\min }}^{T_{\\max }} \\mathbb{E}\\left\\|s^{*}(t, X(t))-s(t, X(t))\\right\\|^{2} \\leq W_{2}^{2}\\left(\\mu, \\mu^{*}\\right) \\frac{c_{T_{\\min }}^{2}}{4 \\sigma_{T_{\\min }}^{2}}\n$$\n\nCombining these results, and noting that regularity bounds obtained in Section 4 hold for $s^{*}$ too, we reduce the problem to the approximation of the score function $s^{*}$, which has a low-dimensional polynomial nature.\n\nWith these results in mind, we are ready to present the construction of $\\hat{s}$.\nNeural Network Approximation Following Oko et al. (2023) and Tang and Yang (2024) we divide the interval $[\\underline{T}, \\bar{T}]$ into $\\underline{T}=T_{0}<T_{1} \\ldots<T_{K}=\\bar{T}, T_{k+1} / T_{k}=2$, and for each $k$ we build a separate estimator $\\hat{s}_{k}(t, x)$ approximating $s(t, x)$ w.r.t. to $\\mathcal{R}\\left(\\hat{s}_{k}, T_{k}, T_{k+1}\\right)$ - the risk function (12) on the interval $\\left[T_{k}, T_{k+1}\\right]$. We fix an index $k<K$ and focus on constructing the approximation $\\hat{s}_{k}$. Further, we omit the index $k$ when it is clear from the context.\n\nFirst, let us analyze the case $d \\leq 2$. Recall that $\\mathcal{Y}=\\left\\{y_{1}, \\ldots, y_{n}\\right\\}$ is a set of i.i.d. samples from $\\mu$. Consider the empirical measure $\\hat{\\mu}_{n}=\\frac{1}{n} \\sum \\delta_{y_{i}}$. For $d \\leq 2$ we choose $\\hat{s}(t, x)$ as the score function $\\hat{s}_{n}(t, x)$ corresponding to measure $\\hat{\\mu}_{n}$ and equal to\n\n$$\n\\hat{s}_{n}(t, x)=\\frac{1}{\\sigma_{t}^{2}} \\frac{\\sum_{y_{i} \\in \\mathcal{Y}} e^{-\\left\\|x-c_{t} y_{i}\\right\\|^{2} / 2 \\sigma_{t}^{2}}\\left(c_{t} y_{i}-x\\right)}{\\sum_{y_{i} \\in \\mathcal{Y}} e^{-\\left\\|x-c_{t} y_{i}\\right\\|^{2} / 2 \\sigma_{t}^{2}}}=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\sum_{y_{i} \\in \\mathcal{Y}} e^{-\\left\\|x-c_{t} y_{i}\\right\\|^{2} / 2 \\sigma_{t}^{2}} y_{i}}{\\sum_{y_{i} \\in \\mathcal{Y}} e^{-\\left\\|x-c_{t} y_{i}\\right\\|^{2} / 2 \\sigma_{t}^{2}}}-\\frac{x}{\\sigma_{t}^{2}}\n$$\n\nNote that this estimator belongs to $\\mathcal{S}_{k}$ defined in (20) with $\\rho_{i}(x):=e^{-\\left\\|x-c_{t} y_{i}\\right\\|^{2} / 2 \\sigma_{t}^{2}}, \\phi_{w_{i}} \\equiv 1$ and $\\phi_{e_{i}} \\equiv 0$.\n\nProposition 22 provided in Section 5 allow us to control the score matching loss through Theorem 1 Divol (2022) states $\\mathbb{E}_{\\mu \\otimes n} W_{2}\\left(\\mu, \\hat{\\mu}_{n}\\right) \\lesssim(n \\log n)^{-1 / 2}$, so applying\n\n$$\n4 \\sigma_{T_{k}}^{2} \\int_{T_{k}}^{T_{k+1}} \\int_{\\mathbb{R}^{d}}\\left\\|\\hat{s}_{n}(t, x)-s(t, x)\\right\\|^{2} p_{t}(x) d x d t \\leq W_{2}^{2}\\left(\\mu, \\hat{\\mu}_{n}\\right)\n$$\n\nSince $T_{k+1}=2 T_{k}$ we have $\\sigma_{t} \\leq 2 \\sigma_{T_{k}}$ for all $t \\in\\left[T_{k}, T_{k+1}\\right]$. Taking expectations and summing over all $k \\leq K=\\log (n)$ we prove Theorem 5 for $d \\leq 2$\n\n$$\n\\mathbb{E}_{\\mathcal{Y} \\sim \\mu \\otimes n} \\int_{\\underline{T}} \\int_{\\mathbb{R}^{D}} \\sigma_{t}^{2}\\left\\|\\hat{s}_{n}(t, x)-s(t, x)\\right\\|^{2} p(t, x) d x d t \\lesssim n^{-1} \\log ^{2} n \\leq n^{\\gamma} n^{-1}\n$$\n\nNext, we discuss how to construct the estimator $\\hat{s}_{k}$ in case $d \\geq 3$. We recall that by (14) the true score is represented as $s(t, x)=\\left(c_{t} e(t, x)-x\\right) / \\sigma_{t}^{2}$, where $\\|e(t, x)\\| \\leq \\operatorname{diam} M \\leq 1$,\n\nthus we build the estimator $\\hat{s}_{k}$ in form $\\hat{s}_{k}(t, x)=\\left(c_{t} \\hat{e}_{k}(t, x)-x\\right) / \\sigma_{t}^{2}$ with $\\left\\|\\hat{e}_{k}(t, x)\\right\\| \\leq 1$. Note that the error of this estimator is uniformly bounded by $c_{t} / \\sigma_{t}^{2}$. So, it is enough to construct $\\hat{s}_{k}(t, x)$ as a high probability approximation of $s(t, x)$.\n\nOne of the main challenges of score approximation under the manifold hypothesis is the complex structure of the support. To tackle this problem we leverage results from Section 5 and construct a piece-wise polynomial surface $M^{*}=\\bigcup_{i=1}^{N} M_{i}^{*}$ approximating $M$. As a first step, we fix the number of approximating polynomial surfaces $N=N\\left(T_{k}, n\\right)$ to be\n\n$$\nN:=\\left\\{\\begin{array}{ll}\n\\max \\left(n^{2 \\gamma d}\\left(\\sigma_{t_{k}} / c_{t_{k}}\\right)^{-d}, n \\cdot n^{-\\frac{2(n+1)}{2 \\alpha+d}}\\right) & \\text { if } T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}} \\\\\nn^{-\\gamma d} n^{\\frac{d}{2 \\alpha+d}} & \\text { if } T_{k} \\leq n^{-\\frac{2}{2 \\alpha+d}}\n\\end{array}\\right.\n$$\n\nand select $\\mathcal{G}=\\left\\{G_{1}, \\ldots, G_{N}\\right\\}$ with $G_{i}=y_{i} \\in \\mathcal{Y}$. Note that $\\log N \\asymp \\log n$ for all $t_{k}$. Following Section 5.1 we introduce $\\varepsilon_{N}=\\left(C_{d, \\beta} \\frac{p_{\\min }^{2}}{p_{\\min }^{2}} \\frac{\\log N}{N-1}\\right)^{\\frac{1}{d}}$ and define $\\mathcal{V}_{i}=\\left\\{G_{j}-G_{i} \\mid\\left\\|G_{j}-G_{i}\\right\\| \\leq\\right.$ $\\varepsilon_{N}\\}$. We denote as $\\mathcal{H}_{i}=\\operatorname{span} \\mathcal{V}_{i}, d_{i}=\\operatorname{dim} \\mathcal{H}_{i}$ and $\\mathcal{A}_{i}=G_{i}+\\mathcal{H}_{i}$. Finally we define a linear isometric map $P_{\\mathcal{H}_{i}}: \\mathbb{R}^{d_{i}} \\mapsto \\mathcal{H}_{i} \\subset \\mathbb{R}^{D}$ identifying $\\mathcal{H}_{i}$ with $\\mathbb{R}^{d_{i}}$. Note, that according to Proposition $21 \\max _{i \\leq N} d_{i} \\leq C_{\\operatorname{dim}} \\log n$ with probability $1-N^{-\\beta / d}$.\n\nFinally, we construct $M_{i}^{*}=\\Phi_{i}^{*}\\left(B_{d}\\left(0,8 \\varepsilon_{N}\\right)\\right)$, where\n\n$$\n\\Phi_{i}^{*}(z)=G_{i}+P_{i}^{*} z+\\sum a_{i, S}^{*} z^{S}\n$$\n\nand $P_{i}^{*}, a_{i, S}^{*}$ are solutions of (27), satisfying $\\operatorname{Im} P_{i}^{*} \\subset \\mathcal{H}_{i}, a_{i, S}^{*} \\in \\mathcal{H}_{i}$. In particular, this implies that $M_{i}^{*} \\subset \\mathcal{A}_{i}$.\n\nNext, for each $M_{i}^{*}$ we construct a measure $\\mu_{i}^{*}$ supported on it, and define $\\mu^{*}=\\sum \\mu_{i}^{*}$. We write $s^{*}(t, x)$ for the score function corresponding to SDE (1) with initial condition $\\mu^{*}$. We discuss the detailed construction in Appendix D, where we will also show that under Assumption D with probability $1-N^{-\\frac{\\beta}{3}}$ we can choose $\\mu^{*}$ such that\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\sigma_{t}^{2} \\mathbb{E}\\left\\|s^{*}(t, X(t))-s(t, X(t))\\right\\|^{2} d t \\lesssim n^{4 \\gamma \\beta} n^{-\\frac{\\beta+1}{2 \\alpha+d}}\n$$\n\nWe will obtain the estimator $\\hat{s}(t, x)$ as an approximation of $s^{*}(t, x)$. Recalling (15) we first approximate\n\n$$\ne^{*}(t, x):=\\frac{\\int_{M^{*}} y^{*} e^{-\\left\\|x-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu^{*}\\left(d y^{*}\\right)}{\\int_{M^{*}} e^{-\\left\\|x-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu^{*}\\left(d y^{*}\\right)}\n$$\n\nand then reconstruct $s^{*}(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} e^{*}(t, x)-\\frac{x}{\\sigma_{t}^{2}}$.\nSince $e^{*}(t, x)$ is an expectation, recalling that $\\mu^{*}=\\sum \\mu_{i}^{*}$ and applying the law of total probability\n\n$$\ne^{*}(t, x)=\\sum_{i}^{N} \\mu^{*}\\left(M_{i}^{*} \\mid t, x\\right) e_{i}^{*}(t, x)\n$$\n\nwhere $\\mu^{*}\\left(M_{i}^{*} \\mid t, x\\right)$ is the probability that $x$ is obtained as a noisy version of a point from $M_{i}^{*}$, and $e_{i}^{*}(t, x)$ is conditional expectation (15) corresponding to $\\mu_{i}^{*}$. Note that the score matching loss can be rewritten in terms of $e^{*}(t, x)$ as\n\n$$\n\\int_{T_{k}}^{T_{k+1}}\\left(c_{t} / \\sigma_{t}\\right)^{2} \\mathbb{E}\\left\\|e^{*}(t, X(t))-e(t, X(t))\\right\\|^{2} d t\n$$\n\nTo approximate $e_{i}^{*}(t, x)$ we leverage the low dimension of the surfaces $M_{i}^{*} \\subset \\mathcal{A}_{i}$. Denote as $\\widetilde{\\mu}_{i}^{*}$ the pushforward of measure $\\mu_{i}^{*}$ by the map $y \\mapsto P_{\\mathcal{H}_{i}}^{T}\\left(y-G_{i}\\right)$. This measure is supported on the polynomial surface $\\widetilde{M}_{i}^{*}:=P_{\\mathcal{H}_{i}}^{T}\\left(M_{i}^{*}-G_{i}\\right) \\subset \\mathbb{R}^{d_{i}}$ in $\\mathbb{R}^{d_{i}}$. Let $\\widetilde{x}_{i}=$ $P_{\\mathcal{H}_{i}}^{T}\\left(x-c_{t} G_{i}\\right) \\in \\mathbb{R}^{d_{i}}$ be coordinates of the projection of $x_{i}=\\operatorname{pr}_{c_{t} \\mathcal{A}_{t}} x$ onto $c_{t} \\mathcal{A}_{i}=c_{t} G_{i}+\\mathcal{H}_{i}$. Direct computation, see e.g. Oko et al. (2023), shows that the function $e^{*}(t, x)$ can be represented as\n\n$$\ne^{*}(t, x)=\\frac{\\sum_{i}^{N} e^{-\\left\\|x-x_{i}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\widetilde{p}_{i}^{*}\\left(t, \\widetilde{x}_{i}\\right)\\left(G_{i}+P_{\\mathcal{A}_{i}} \\widetilde{e}_{i}^{*}\\left(t, \\widetilde{x}_{i}\\right)\\right)}{\\sum e^{-\\left\\|x-x_{i}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\widetilde{p}_{i}^{*}\\left(t, \\widetilde{x}_{i}\\right)}\n$$\n\nwhere $\\widetilde{p}_{i}^{*}\\left(t, \\widetilde{x}_{i}\\right), \\widetilde{e}_{i}^{*}\\left(t, \\widetilde{x}_{i}\\right)$ are the density (3) and conditional expectation (15) corresponding to the forward process (1) in $\\mathbb{R}^{d_{i}}$ with initial condition $\\widetilde{\\mu}_{i}$.\n\nEssentially, we have reduced the $D$-dimensional problem of score approximation to $N$ simpler $O(\\log n)$-dimensional problems.\n\nSubstituting $\\log n \\geq T_{k} \\geq n^{-1}, \\delta, \\eta=n^{-2}$ into Theorem 12 we get that with probability $1-n^{-2}$\n\n$$\n\\sum_{\\operatorname{dist}\\left(X(0), M_{i}^{*}\\right) \\gtrsim\\left(\\sigma_{t} / c_{t}\\right)} \\frac{\\left(\\sigma_{t} / c_{t}\\right)\\left\\|\\sum \\mu^{*}\\left(M_{i} \\mid t, X(t)\\right) e_{i}^{*}(t, X(t))\\right\\| \\lesssim n^{-1}\n$$\n\nso we may consider in the sum (17) only terms satisfying\n\n$$\n\\operatorname{dist}\\left(X(0), M_{i}^{*}\\right) \\lesssim\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d\\left(\\log n+C_{\\log }\\right)}\n$$\n\nMoreover, denoting $G_{\\min }(x)=\\arg \\min \\left\\|x-G_{i}\\right\\|$, and applying Lemma 13 we can show that with probability $1-n^{-2}$ condition (18) holds if and only if\n\n$$\n\\left\\|X(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} G_{\\min }(X(t))\\right\\|^{2} \\leq C\\left(T_{k}, n\\right)\n$$\n\nfor some constant $C\\left(T_{k}, n\\right)$ that will be specified in Appendix D. Thereby, introducing\n\n$$\n\\rho(x)= \\begin{cases}1 & \\text { if }|x| \\leq 1 / 2 \\\\ 2-2 x & \\text { if }|x| \\in[1 / 2,1] \\\\ 0 & \\text { if }|x| \\geq 1\\end{cases}\n$$\n\nwe define\n\n$$\n\\rho_{i}(t, x)=\\rho\\left(\\frac{\\left\\|x-c_{t} G_{i}\\right\\|^{2}-\\left\\|x-c_{t} G_{\\min }(x)\\right\\|^{2}}{2 C\\left(T_{k}, n\\right)}\\right)\n$$\n\nFinally, as we verify in Appendix D with probability $1-n^{-2}$ for all $i \\leq N$, if $\\rho_{i}(t, X(t))>0$ then $\\mu^{*}\\left(M_{i}^{*} \\mid t, X(t)\\right) \\geq n^{-C_{w}}$ for some absolute constant $C_{w}$.\n\nTaking into account the discussed results, we approximate $s^{*}(t, x)$ with a neural networkbased estimator $\\phi(t, x)$ defined by.\n\n$$\n\\phi(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\sum_{i}^{N} \\rho_{i}\\left(t, x_{i}\\right) \\phi_{w_{i}}\\left(t, P_{\\mathcal{H}_{i}}^{T}\\left(x-c_{t} G_{i}\\right)\\right)\\left(G_{i}+P_{\\mathcal{H}_{i}} \\phi_{e_{i}}\\left(t, P_{\\mathcal{H}_{i}}^{T}\\left(x-c_{t} G_{i}\\right)\\right)\\right)}{\\sum_{i}^{N} \\rho_{i}(t, x) \\phi_{w_{i}}\\left(t, P_{\\mathcal{H}_{i}}^{T}\\left(x-c_{t} G_{i}\\right)\\right)}-\\frac{x}{\\sigma_{t}^{2}}\n$$\n\nwhere $\\phi_{e_{i}^{*}}: \\mathbb{R}_{+} \\times \\mathbb{R}^{d_{i}} \\rightarrow \\mathbb{R}^{d_{i}}, \\phi_{w_{i}}: \\mathbb{R}_{+} \\times \\mathbb{R}^{d_{i}} \\rightarrow \\mathbb{R}_{+}$are neural networks of polylog size. More formally, we consider the class of estimators of the form (19)\n\n$$\n\\begin{gathered}\n\\mathcal{S}_{k}=\\left\\{\\phi: \\phi_{e_{i}}, \\phi_{w_{i}} \\in \\Psi(L, W, B, S),\\left\\|\\phi_{e_{i}^{*}}\\right\\|_{\\infty} \\leq 1,\\left\\|\\frac{\\phi_{e_{i}^{*}}\\left(t, \\tilde{x}_{i}\\right)-\\widetilde{x}_{i}}{\\sigma_{t}}\\right\\|_{\\infty} \\leq C_{e} \\sqrt{C_{\\operatorname{dim}} \\log n}\\right. \\\\\n\\left.\\left\\|\\phi_{w_{i}^{*}}\\right\\|_{\\infty} \\geq n^{-C_{w}}\\right\\}\n\\end{gathered}\n$$\n\nwhere $C_{e}, C_{w}$ are absolute constants specified in Appendix D and\n\n$$\nL=O(\\operatorname{polylog} n),\\|W\\|_{\\infty}=O(\\operatorname{polylog} n), S=O(\\operatorname{polylog} n), B=e^{O(\\operatorname{polylog} n)}\n$$\n\nThe condition $\\left\\|\\frac{\\phi_{e_{i}^{*}}-\\widetilde{x}_{i}}{\\sigma_{t}}\\right\\|_{\\infty} \\leq C_{e} \\sqrt{C_{\\operatorname{dim} \\log n}}$ is essentially invoked by a standard tail bounds on normal distribution in $\\mathbb{R}^{d_{i}}$, for $d_{i} \\leq C_{\\operatorname{dim}} \\log n$.\n\nNote that we do not optimize over $P_{\\mathcal{H}_{i}}, G_{i}$, and find them directly from samples $\\mathcal{Y}$. Alternatively, considering $P_{\\mathcal{H}_{i}}, G_{i}$ as unknown parameters, we can treat $\\phi$ as one large network $\\phi^{\\prime} \\in \\Psi\\left(L^{\\prime}, S^{\\prime}, B^{\\prime}, W^{\\prime}\\right)$, where\n\n$$\n\\begin{gathered}\nL^{\\prime}=O(\\operatorname{polylog} n), S^{\\prime}=O(N \\operatorname{polylog} n), B^{\\prime}=e^{O(\\operatorname{polylog} n)} \\\\\nW_{i}^{\\prime}= \\begin{cases}O(N \\operatorname{polylog} n) & \\text { if } 1<i<L \\\\\nD & \\text { if } i=1, L\\end{cases}\n\\end{gathered}\n$$\n\ncomposing smaller sub-networks $\\phi_{s_{i}}, \\phi_{w_{i}}$. The description of the neural network class in this case can be considerably simplified\n\n$$\n\\mathcal{S}_{k}^{\\prime}=\\left\\{\\phi^{\\prime}: \\phi^{\\prime} \\in \\Psi\\left(L^{\\prime}, W^{\\prime}, B^{\\prime}, S^{\\prime}\\right),\\left\\|\\phi^{\\prime}\\right\\|_{\\infty} \\leq C_{e}^{\\prime} \\frac{\\sqrt{D} \\log n}{\\sigma_{t}}\\right\\}\n$$\n\nThe following theorem is a refined version of Theorem 5\nTheorem 7. Let $d \\geq 3$, and make the same assumptions as in Theorem 5.\n(i) Let $\\hat{s}_{k}(t, x) \\in \\arg \\min _{\\phi \\in \\mathcal{S}_{k}} \\mathcal{R}_{\\mathcal{Y}}\\left(\\phi, T_{k}, T_{k+1}\\right)$ be a solution of the empirical risk (13) minimization problem in class $\\mathcal{S}_{k}$, then\n\n$$\n\\mathbb{E}_{\\mathcal{Y} \\sim \\mu^{\\otimes n}} \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|\\hat{s}(t, X(t))-s(t, X(t))\\|^{2} d t \\lesssim \\begin{cases}n^{-\\frac{2 \\alpha}{2 \\alpha+d}} n^{2 \\gamma \\alpha}(\\log n)^{\\frac{2 \\alpha+d}{d}} & \\text { if } T_{k} \\leq n^{-\\frac{2}{2 \\alpha+d}} \\\\ \\sigma_{T_{k}}^{-2} n^{-\\frac{2 \\beta(d-2)}{d(2 \\alpha+d)}}(\\log n)^{2 \\beta} & \\text { otherwise }\\end{cases}\n$$\n\nfor all $n$ large enough and independent of $D$.\n(ii) Let $\\hat{s}_{k}^{\\prime}(t, x) \\in \\arg \\min _{\\phi^{\\prime} \\in \\mathcal{S}_{k}^{\\prime}} \\mathcal{R}_{\\mathcal{Y}}\\left(\\phi^{\\prime}, t_{k}, t_{k+1}\\right)$ be a solution of the empirical risk (13) minimization problem in class $\\mathcal{S}_{k}^{\\prime}$, then\n\n$$\n\\mathbb{E}_{\\mathcal{Y} \\sim \\mu^{\\otimes n}} \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|\\hat{s}(t, X(t))-s(t, X(t))\\|^{2} d t \\leq \\begin{cases}D n^{-\\frac{2 \\alpha}{2 \\alpha+d}} n^{2 \\gamma \\alpha}(\\log n)^{\\frac{2 \\alpha+d}{d}}, & \\text { if } T_{k} \\leq n^{-\\frac{2}{2 \\alpha+d}} \\\\ D n^{-\\frac{2 \\beta(d-2)}{d(2 \\alpha+d)}}(\\log n)^{2 \\beta} & \\text { otherwise }\\end{cases}\n$$\n\nfor all $n$ large enough and independent of $D$.\n\n(iii) The estimator $\\hat{s}(t, x):=\\hat{s}_{k}(t, x)$ on $\\left[T_{k}, T_{k+1}\\right]$ satisfies Theorem 5.\n\nProof. See Appendix D.\nThe proof generally follows Oko et al. (2023); we decompose the risk of the estimator $\\hat{s}_{k}$ as a sum of bias and variance terms. The variance of the neural network estimators is well-studied Hayakawa and Suzuki (2020); Schmidt-Hieber (2020); Suzuki (2019) and can be expressed through the size of the network. We adapt these techniques in Appendix D.3. The bias term, i.e. the minimal achievable risk $\\min _{\\phi \\in \\mathcal{S}} \\mathcal{R}(\\phi)$ in the class $\\mathcal{S}$, is small due to the careful construction of the aforementioned class $\\mathcal{S}$ as we will show in other subsections of Appendix D.", "tables": {}, "images": {}}, {"section_id": 10, "text": "# 4 High Probability Bounds on the Score Function \n\nIn this section we present our main tool, the high-probability bounds on the score function $s(t, X(t))$ under the manifold hypothesis w.r.t. $X(t)$ - the forward process (1). These results will play a key role in the subsequent discussion on the score approximation in Section 3.1.\n\nWe start with the study of how $D$-dimensional Gaussian noise $Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ interacts with the smooth $d$-dimensional manifold $M$. The following theorem bounds the scalar product between $Z_{D}$ and vectors connecting points on the manifold.\nProposition 8. Let $M$ be a $\\beta \\geq 1$-smooth manifold satisfying Assumption $A$. Then for any $\\delta>0$ and $\\varepsilon<r_{0}$ with probability $1-\\delta$ for all $y, y^{\\prime} \\in M$\n\n$$\n\\left|\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle\\right| \\leq 4 \\varepsilon \\sqrt{d}+\\left(\\left\\|y-y^{\\prime}\\right\\|+6 \\varepsilon\\right) \\sqrt{4 d \\log 2 \\varepsilon^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}}\n$$\n\nProof. See Appendix B. 1 for the complete proof. Adapting classical results on the maximum of Gaussian processes we show that for any $L$-Lipschitz function $f: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{D}$ satisfying $f(0)=0$\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in B_{d}(0, r)}\\left\\langle Z_{D}, f(z)\\right\\rangle \\leq \\operatorname{Lr}\\left(\\sqrt{d}+\\sqrt{2 \\log \\delta^{-1}}\\right)\\right) \\geq 1-\\delta\n$$\n\nThen, for a point $y \\in M$ we locally represent the manifold as $\\Phi_{y}\\left(B_{d}(0, \\varepsilon)\\right)$, see Section 2.2, and applying the inequality above to $f(z)=\\Phi_{y}(z)-\\Phi_{y}(0)$ we get a local version of inequality (22). Finally, taking an $\\varepsilon$-dense set and applying the chaining argument we generalize the local result to the whole manifold $M$.\n\nRemark 9. Proposition 8 is an adaptation to the case of smooth regular manifolds of results related to the connection between metric entropy and Gaussian processes, see e.g. (Wainwright, 2019, Chapter 5) or (Vershynin, 2018, Chapter 9). This connects our approach with Li and Yan (2024) on sampling complexity of diffusion models.\n\nThe above theorem shows that the inner-product between the Gaussian vector $Z_{D}$ and vectors on the manifold does not depend on the ambient dimension. In particular, using standard bounds on the length of Gaussian vectors, this means that the correlation between $Z_{D}$ and $y-y^{\\prime}$ is $\\lesssim \\sqrt{d / D}$ with high probability, implying that the noise added during the forward diffusion process is almost orthogonal to the manifold if $D \\gg d$.\n\nAssuming that $\\beta \\geq 2$ and applying the same technique it is also possible to bound the projection of $Z_{D}$ onto tangent spaces $T_{y} M$, uniformly in $y \\in M$.\n\nTheorem 10. Let $M$ be a $\\beta \\geq 2$-smooth manifold satisfying Assumption $A$. Then for any $\\delta>0$ with probability at least $1-\\delta$ for all $y \\in M$\n\n$$\n\\left\\|\\operatorname{pr}_{T_{y} M} Z_{D}\\right\\| \\leq 8 \\sqrt{d\\left(4 \\log 2 d+2 \\log r_{0}^{-1}+2 \\log _{+} \\operatorname{Vol} M+2 \\log \\delta^{-1}\\right)}\n$$\n\nProof. See Appendix G.2.\nEquipped with the bounds on the scalar product between the noise $Z_{D}$ and the vectors on the manifold $M$ we are ready to analyze the score function. Let $\\mu$ be a measure satisfying Assumptions A-C. We recall Tweedie's formula (14) stating that\n\n$$\ns(t, x)=\\frac{1}{\\sigma_{t}^{2}} \\frac{\\int_{M}\\left(c_{t} y-x\\right) e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} p(y) d y}{\\int_{M} e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} p(y) d y}=\\int_{M} \\frac{c_{t} y-x}{\\sigma_{t}^{2}} p(y \\mid t, x) d y\n$$\n\nwhere $p(y \\mid t, x) \\propto e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} p(y)$ is normalized to be a density function corresponding to the random variable $(X(0) \\mid X(t)=x)$. In other words, the score function is the expectation of the function $\\left(c_{t} y-x\\right) / \\sigma_{t}^{2}$ w.r.t. the measure on $M$ with density $p(y \\mid t, x)$. The density itself is a product of two components: $p(y) d y \\propto d y$ which is proportional to the uniform measure $M$ up to $p_{\\min }, p_{\\max }$ and $e^{-\\|x-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}}$ which penalizes exponentially points far away from $x$.\n\nAs we have shown in Proposition 8 the noise vector $Z_{D}$ is almost orthogonal to vectors on $M$. Therefore, representing the forward process (1) as $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$ we write $\\left\\|X(t)-c_{t} y\\right\\|^{2}=c_{t}^{2}\\left\\|X(0)-y\\right\\|^{2}+\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}+2 c_{t} \\sigma_{t}\\left\\langle X(0)-y, Z_{D}\\right\\rangle \\approx c_{t}^{2}\\|X(0)-y\\|^{2}+\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}$.\n\nSince the vector $Z_{D}$ does not depend on $y$, it disappears during the normalization and therefore, roughly speaking, for small $t<r_{0}^{2}$, the density function $p(y \\mid t, X(t)) \\approx e^{-c_{t}^{2}\\|X(0)-y\\|^{2} / 2 \\sigma_{t}}$ looks like the density of $d$-dimensional Gaussian variable on $T_{X(0)} M$. The next proposition formalizes this argument\n\nProposition 11. Let measure $\\mu$ satisfy Assumptions $A-C$ and be supported on $M$. For any $\\delta>0$ with probability at least $1-\\delta$ on $X(t)$ the following bound holds uniformly in $y \\in M$\n\n$$\n\\begin{aligned}\n-20 d\\left(\\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)-8 \\log & \\delta^{-1}-\\frac{3}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2} \\\\\n& \\leq \\log p(y \\mid t, X(t)) \\\\\n\\leq & 20 d\\left(\\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}-\\frac{1}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2}\n\\end{aligned}\n$$\n\nProof. See Appendix B.2.\nSo, the density $p(y \\mid t, X(t))$ exponentially penalizes for being too far away from $X(0)$, and, therefore, most of the mass of the measure with density $p(y \\mid t, X(t))$ concentrates in a ball centered at $X(0)$.\n\nTheorem 12. Let measure $\\mu$ satisfy Assumptions $A-C$ and be supported on $M$. Fix $t, \\delta, \\eta>$ 0 and define\n\n$$\nr(t, \\delta, \\eta)=2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20 d\\left(\\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}+\\log \\eta^{-1}}\n$$\n\n(i) A.s.in $X(0) \\sim \\mu$ and with probability $1-\\delta$ in $Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ for a random point $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$ the following holds\n\n$$\n\\int_{y \\in M:\\|y-X(0)\\| \\leq r(t, \\delta, \\eta)} p(y \\mid t, X(t)) d y \\geq 1-\\eta\n$$\n\n(ii) As a corollary, a.s. in $X(0) \\sim \\mu$ and with probability $1-\\delta$ in $Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ for a random point $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$ the following holds\n\n$$\n\\left\\|\\sigma_{t} s(t, X(t))+Z_{D}\\right\\| \\leq 4 \\sqrt{20 d\\left(2 \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}}\n$$\n\n(iii) Integrating over $Z_{D}$, for all $y \\in M$\n\n$$\n\\mathbb{E}_{Z_{D}}\\left\\|\\sigma_{t} s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)+Z_{D}\\right\\|^{2} \\leq 8\\left(40 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+80 d C_{\\log }+3\\right)\n$$\n\nProof. See Appendix B.2.\nIn particular, the result shows that on average the score function computed at point $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$ estimates the added noise $\\sigma_{t} Z_{D}$ with an error scaling as $\\sigma_{t} \\sqrt{d}$ up to log-factor, and thereby the estimate $c_{t}^{-1}\\left(X(t)-\\sigma_{t}^{2} s(t, X(t))\\right)$ of point $X(0)$ has an error $\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d}$ up to $\\log$ factor independent on $D$.\n\nFinally, let us present the result, that will play an important role in the construction of weight functions $\\rho_{i}(t, x)$ discussed in Section 3.\n\nLemma 13. Let $\\mathcal{G}=\\left\\{G_{1}, \\ldots, G_{N}\\right\\}$ be an $\\varepsilon$-dense set on $M$. Denote the nearest neighbor of $X(t)$ in $\\mathcal{G}$ as $G_{\\min }(t)=G_{\\min }(X(t)):=\\arg \\min _{i}\\left\\|X(t)-c_{t} G_{i}\\right\\|$. With probability at least $1-\\delta$ for all $G_{i}$\n\n$$\n\\begin{aligned}\n& \\frac{2}{3} c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} G_{\\min }(t)\\right\\|^{2}\\right)-128\\left(\\sigma_{t} / c_{t}\\right)^{2}\\left(d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 d C_{\\log }+\\log \\delta^{-1}\\right) \\\\\n& \\leq\\left\\|X(0)-G_{i}\\right\\|^{2} \\leq \\\\\n& 9 \\varepsilon^{2}+2 c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} G_{\\min }(t)\\right\\|^{2}\\right)+128\\left(\\sigma_{t} / c_{t}\\right)^{2}\\left(d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 d C_{\\log }+\\log \\delta^{-1}\\right)\n\\end{aligned}\n$$\n\nProof. See Appendix B. 3", "tables": {}, "images": {}}, {"section_id": 11, "text": "# 5 Manifold Approximation \n\nIn this section, we discuss how to efficiently approximate the manifold $M:=\\operatorname{supp} \\mu$, using a piecewise polynomial surface $M^{*}$, an important step in the construction of estimator $\\hat{s}(t, x)$ to the true score $s(t, x)$.\n\nDefinition 14. A piecewise polynomial approximation of order $\\beta$ is determined through an integer $N$, a positive scale parameter $\\varepsilon_{N}<r_{0}$, an $\\varepsilon_{N}$ dense set $\\mathcal{G}=\\left\\{G_{1}, \\ldots, G_{N}\\right\\}$ on $M$, and a set of functions $\\left\\{\\Phi_{1}^{*}, \\ldots, \\Phi_{N}^{*}\\right\\}$, where $\\Phi_{i}^{*}: B_{d}\\left(0, \\varepsilon_{N}\\right) \\rightarrow \\mathbb{R}^{D}$ are polynomials of order $\\beta-1$ satisfying $\\Phi_{i}^{*}(0)=G_{i}$.\n\nThe approximating surface $M^{*}$ is then defined as $M^{*}=\\bigcup M_{i}^{*}$, where $M_{i}^{*}=\\Phi_{i}^{*}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)$, and each $\\Phi_{i}^{*}$ plays the role of a polynomial approximation of the local inverse of the projection on the tangent space $T_{G_{i}} M$.\n\nFinally, we call an approximation optimal if it satisfies the following two conditions: (i) $\\log N \\lesssim d \\log \\varepsilon_{N}^{-1}$; (ii) for each $i \\leq N, k \\leq \\beta$ and any $z \\in B_{d}\\left(0, \\varepsilon_{N}\\right)$ it holds that $\\left\\|\\nabla^{k} \\Phi_{i}(z)-\\nabla^{k} \\Phi_{G_{i}}^{*}(z)\\right\\|_{o p} \\leq L^{*} \\varepsilon_{N}^{\\beta-k}$ for some constant $L^{*}>0$.\nExample 15. The natural way to construct an optimal approximation is to fix $\\varepsilon_{N}$, take as $\\mathcal{G}$ a minimal $\\varepsilon_{N}$-dense set $\\mathcal{G}=\\left\\{G_{1}, \\ldots, G_{N}\\right\\}$, and choose $N=|\\mathcal{G}|$. We recall that by Proposition 3, $N \\leq\\left(\\varepsilon_{N} / 2\\right)^{-d} \\operatorname{Vol} M$, so condition (i) is satisfied. Then, let $\\Phi_{i}^{*}$ be the Taylor approximation of $\\Phi_{G_{i}}$ of order $\\beta-1$; by Assumption $A$ we have the uniform bound $\\left\\|\\Phi_{G_{i}}\\right\\|_{C^{\\beta}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)} \\leq L_{M}$ for all $i \\leq N$, so assumption (ii) is also satisfied.\n\nWe use Example 15 to construct the neural network in (ii) of Theorem 7 which leads to a suboptimal dependence in $D$. To improve upon it we use a different approximation based on Aamari and Levrard (2019) which leads to the bound in (i) of Theorem 7.\n\nHaving $M^{*}$, we can approximate $\\mu$ with a measure $\\mu^{*}$ supported on $M^{*}$, by first representing $\\mu=\\sum \\mu_{i}$, where $\\operatorname{supp} \\mu_{i} \\subset M_{i}$ and then taking $\\mu^{*}=\\sum \\mu_{i}^{*}$, where $\\mu_{i}^{*}=\\Phi_{i}^{*} \\circ \\Phi_{i}^{-1} \\mu_{i}$ - is the pushforward of $\\mu_{i}$ from $M_{i}$ to $M_{i}^{*}$. Finally, we define $s^{*}(t, x)$ as the score function corresponding to the forward process (1) with initial distribution $\\mu^{*}$.\n\nWe construct $\\hat{s}(t, x)$ as an approximation of $s^{*}(t, x)$, and in this section, we answer the three following questions: (i) How do we construct optimal approximations efficiently? (ii) How can we bound the score matching loss (7) of $s^{*}(t, x)$ w.r.t. $s^{*}(t, x)$ ? (iii) Do the regularity bounds, developed in Section 4 hold for $s^{*}(t, x)$ ?", "tables": {}, "images": {}}, {"section_id": 12, "text": "# 5.1 Support Estimation \n\nWe start the discussion by answering the first question. The approach described in Example 15 has a major flaw when $M$ is unknown the dimension of the search space consisting of unknown polynomials $\\Phi_{G_{i}}^{*}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{D}$ scales with $D$, blowing up the variance of such an estimator.\n\nHowever, we have access to samples $\\mathcal{Y}=\\left\\{y_{1}, \\ldots, y_{n}\\right\\}$ which already contain some information about $M$, and as has been shown in Aamari and Levrard (2019) one can build an estimator of $M$ using $\\mathcal{Y}$ with convergence rate independent of the extrinsic dimension $D$. We modify this argument to make the size of the search space also independent of $D$.\n\nAs a first step, we fix an arbitrary $N \\leq n$ large enough and take as $\\mathcal{G}$ the subset $\\mathcal{G}=\\left\\{G_{i}:=y_{i}, i \\leq N\\right\\}$. Note that $\\mathcal{G}$ is a set of $N$ i.i.d. samples from the measure $\\mu$. Aamari (2017) has shown that with high probability for an appropriate choice of $\\varepsilon_{N}$ the set $\\mathcal{G}$ is $\\varepsilon_{N}$-dense.\nProposition 16. (Aamari, 2017, Lemma III.23) Let the measure $\\mu$ satisfy Assumption B and $\\mathcal{G}=\\left\\{G_{1}, \\ldots, G_{N}\\right\\}$ be $N$ i.i.d. samples from $\\mu$. For any $\\varepsilon_{N} \\geq\\left(\\frac{\\beta C_{d}}{p_{\\min }} \\frac{\\log N}{N}\\right)^{1 / d}$ where $C_{d}>0$ is large enough and depends only on $d$, for $N$ large enough so that $\\varepsilon_{N}<r_{0}$, with probability $1-N^{-\\frac{\\beta}{d}}$ set $\\mathcal{G}$ is $\\varepsilon_{N} / 2$-dense on $M$.\n\nApplying Proposition 16 we represent the manifold as $M=\\bigcup_{i=1}^{N} \\Phi_{G_{i}}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)$ and aim to approximate $\\Phi_{G_{i}}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)$. Further choosing $\\varepsilon_{N}=\\left(C_{d, \\beta} \\frac{p_{\\min }^{2}}{p_{\\min }^{2}} \\frac{\\log N}{N-1}\\right)^{\\frac{1}{d}}>\\left(\\frac{\\beta C_{d}}{p_{\\min }} \\frac{\\log N}{N}\\right)^{1 / d}$\n\nwhere $C_{d, \\beta}$ large enough, following Aamari and Levrard (2019) we consider a local polynomial estimator of $\\Phi_{G_{i}}$ of the following form. For $G_{i} \\in \\mathcal{G}$ we define $\\mathcal{V}_{i}=\\left\\{G_{j}-G_{i} \\mid\\left\\|G_{j}-G_{i}\\right\\| \\leq\\right.$ $\\varepsilon_{N}\\}$, and consider a solution of the following minimization problem\n\n$$\nP_{i}^{*},\\left\\{a_{i, S}^{*}\\right\\}_{2 \\leq|S|<\\beta} \\in \\underset{P, a_{S} \\leq \\varepsilon_{n}^{-1}}{\\arg \\min } \\sum_{v_{j} \\in \\mathcal{V}_{i}}\\left\\|v_{j}-P P^{T} v_{j}-\\sum_{S} a_{S}\\left(P^{T} v_{j}\\right)^{S}\\right\\|^{2}\n$$\n\nHere $\\arg \\min$ is taken over all linear isometric embeddings $P: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{D}$ and all vectors $a_{S} \\in \\mathbb{R}^{D}$ that satisfy $\\left\\|a_{S}\\right\\| \\leq \\varepsilon_{N}^{-1}$, where $S$ is a multi-index $S=\\left(S_{1}, \\ldots, S_{d}\\right)$ such that $2 \\leq \\sum S_{k} \\leq \\beta$. Then, in the neighborhood of $G_{i}$ the manifold is approximated as $M_{i}^{*}=$ $\\Phi_{i}^{*}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)$ where\n\n$$\n\\Phi_{i}^{*}(z)=G_{i}+P_{i}^{*} z+\\sum a_{i, S}^{*} z^{S}\n$$\n\nwhere for $z \\in \\mathbb{R}^{d}$ and a multi-index $S, z^{S}=\\prod_{j} z_{j}^{S_{j}}$.\nFinally, we construct our support estimator as a union of polynomial surfaces $M^{*}=$ $\\bigcup M_{i}^{*}$.\n\nRemark 17. Aamari and Levrard (2019) optimize a slightly different functional, taking minimum over all projections $\\pi$ on d-dimensional subspaces and symmetric tensors $T_{j}: \\operatorname{Im} \\pi \\mapsto$ $\\mathbb{R}^{D}$ of order less than $\\beta$. This is equivalent to (27) since the operators $P_{i}$ parameterize the choice of the basis in $\\operatorname{Im} \\pi$ and $\\pi=P_{i} P_{i}^{T}$. Finally, the choice of the basis establishes a one-to-one correspondence between tensors of order $k$ and homogeneous polynomials of degree $k$.\n\nThe affine subspace $G_{i}+\\operatorname{Im} P_{i}^{*}$ is a tangent space $T_{G_{i}} M^{*}$. Let $\\mathrm{pr}_{i}$ and $\\mathrm{pr}_{i}^{*}$ denote projections on $T_{G_{i}} M-G_{i}$ and $T_{G_{i}} M^{*}-G_{i}$. (Aamari and Levrard, 2019, Theorem 2) states that $\\angle\\left(T_{G_{i}} M, T_{G_{i}} M^{*}\\right)=\\left\\|\\operatorname{pr}_{i}^{*}-\\operatorname{pr}_{i}\\right\\|_{o p} \\lesssim \\varepsilon_{N}^{\\beta-1}$.\n\nWe restate this result as a bound on $P_{i}^{*}: \\mathbb{R}^{d} \\rightarrow\\left(T_{G_{i}} M^{*}-G_{i}\\right)$ and show that it is close to a linear map $P_{i}:=\\operatorname{pr}_{i} P_{i}^{*}\\left(\\left(P_{i}^{*}\\right)^{T} \\operatorname{pr}_{i} P_{i}^{*}\\right)^{-1 / 2}$ corresponding to embedding $P_{i}: \\mathbb{R}^{d} \\rightarrow$ $\\left(T_{G_{i}} M-G_{i}\\right)$.\nProposition 18. With probability $1-N^{-\\beta / d}$ for all $i \\leq N$, the maps $P_{i}$ are well-defined linear isometric embeddings satisfying $\\left\\|P_{i}^{*}-P_{i}\\right\\|_{o p} \\lesssim \\varepsilon_{N}^{\\beta-1}$.\nProof. See Appendix C. 2 .\nUsing the map $P_{i}$ we identify $\\mathbb{R}^{d}$ with $T_{G_{i}} M$ and set $\\Phi_{i}=\\Phi_{G_{i}}: B_{d}\\left(0, r_{0}\\right) \\subset \\mathbb{R}^{d} \\rightarrow M$ as an inverse of a projection map $P_{i}^{T} \\circ \\mathrm{pr}_{i}: M \\mapsto \\mathbb{R}^{d}$, in particular, this means that $\\Phi_{G_{i}}(z)=G_{i}+P_{i} z+O\\left(\\|z\\|^{2}\\right)$.\n\nThe following statement is a generalization of (Divol, 2022, Lemma A.2).\nLemma 19. There is a large enough constant $L^{*}$, that does not depend on $D$, such that for any $\\varepsilon_{N}<r_{0} / 4$ with probability $1-N^{-\\frac{\\beta}{d}}$ for all $y_{i}$, all $0 \\leq k<\\beta$ and $z \\in B_{d}\\left(0,8 \\varepsilon_{N}\\right)$\n\n$$\n\\left\\|\\nabla^{k} \\Phi_{i}(z)-\\nabla^{k} \\Phi_{i}^{*}(z)\\right\\|_{o p} \\leq L^{*} \\varepsilon_{N}^{\\beta-k}\n$$\n\nIn particular, $\\left\\|y-y^{*}\\right\\| \\lesssim \\varepsilon_{N}^{\\beta}$ and $\\angle\\left(T_{y^{*}} M^{*}, T_{y} M\\right) \\lesssim \\varepsilon_{N}^{\\beta-1}$ for $y=\\Phi_{i}(z) \\in M, y^{*}=\\Phi_{i}^{*}(z) \\in M^{*}$.\n\nProof. See Appendix C.2.\nIn particular combined with Proposition 16, this means that the Hausdorff distance\n\n$$\nd_{H}\\left(M, M^{*}\\right)=\\max \\left(\\sup _{y \\in M} \\operatorname{dist}\\left(y, M^{*}\\right), \\sup _{y^{*} \\in M^{*}} \\operatorname{dist}\\left(y^{*}, M\\right)\\right) \\lesssim \\varepsilon_{N}^{\\beta}\n$$\n\nNote that $\\varepsilon_{N}=\\left(C_{d, \\beta} \\frac{p_{\\max }^{\\beta}}{p_{\\min }^{\\beta}} \\frac{\\log N}{N-1}\\right)^{\\frac{1}{\\beta}}$, implying that $2 d \\log \\varepsilon_{N}^{-1} \\geq \\log N$ if $N$ large enough. Together with Lemma 19 this shows that the constructed approximation satisfies Definition 14 .\n\nLet us look closer at the solution of the optimization problem (27). When $\\operatorname{dim} \\operatorname{span} \\mathcal{V}_{i} \\leq d$ any projection on $\\operatorname{span} \\mathcal{V}_{i}$ perfectly reconstructs vectors in $\\mathcal{V}_{i}$, while when $\\operatorname{dim} \\operatorname{span} \\mathcal{V}_{i}>d$ we claim that we can reduce the problem to a low-dimensional $\\operatorname{span} \\mathcal{V}_{i}$. The following propositions justify this.\n\nProposition 20. If $\\operatorname{dim} \\operatorname{span} \\mathcal{V}_{i}>d$, then there is a solution $P_{i}^{*}, a_{i, S}^{*}$ of (27) such that $\\operatorname{Im} P_{i}^{*} \\subset \\operatorname{span} \\mathcal{V}_{i}$ and $a_{i, S}^{*} \\in \\operatorname{Ker} P_{i}^{*} \\cap \\operatorname{span} \\mathcal{V}_{i}$.\n\nProposition 21. There is a large enough constant $C_{\\text {dim }}$ that does not depend on $D$ such that with probability $1-N^{-\\frac{\\beta}{\\beta}}$ for all $y \\in M$ the size of the set\n\n$$\n\\mathcal{V}_{y}=\\left\\{G_{i}:\\left\\|G_{i}-y\\right\\| \\leq \\varepsilon_{N}\\right\\}\n$$\n\nis bounded by $\\left|\\mathcal{V}_{y}\\right| \\leq C_{\\operatorname{dim}} \\log N$. In particular for all $G_{i} \\in \\mathcal{G}$ holds $\\left|\\mathcal{V}_{i}\\right| \\leq C_{\\operatorname{dim}} \\log N$.\nProof. See Appendix C. 2 for the proof of Propositions 20, 21 .\nTogether these statements show that with probability $1-N^{-\\beta / d}$ there is a piece-wise polynomial surface $M^{*}=\\bigcup_{i=1}^{N} M_{i}^{*}$ approximating $M$ with error $\\varepsilon_{N}^{\\beta} \\simeq\\left(\\frac{\\log N}{N}\\right)^{\\beta / d}$ such that each polynomial piece $M_{i}^{*}$ lies in affine subspace $G_{i}+\\operatorname{span} \\mathcal{V}_{i}$ where $\\mathcal{V}_{i}=\\left\\{G_{j}-\\right.$ $\\left.G_{i} \\mid\\left\\|G_{j}-G_{i}\\right\\| \\leq \\varepsilon_{N}\\right\\}$ which is at most $C_{\\operatorname{dim}} \\log N$ dimensional.\n\nFinally, we recall that by taking a partition $\\mu=\\sum_{i=1}^{N}$ of measure $\\mu$ subordinated to the cover $M=\\bigcup M_{i}$ we construct the measure $\\mu^{*}=\\mu_{i}^{*}$ where $\\mu_{i}^{*}$ is pushforward of $\\mu_{i}$ under $\\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}$. Moreover, functions $\\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}$ gives us a transport map which by Lemma 19 guarantees that $W_{2}\\left(\\mu, \\mu^{*}\\right) \\leq L^{*} \\varepsilon_{N}^{\\beta}$.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# 5.2 Manifold Approximation and the Score Function \n\nIn this section, we answer the second and third questions raised at the beginning of Section 5.\nTo control the score matching error between $s^{*}(t, x)$ and $s(t, x)$, we use a general result bounding the score-matching loss between two compactly supported distributions.\n\nProposition 22. Let $P, Q$ be arbitrary compactly supported measures s.t. $W_{2}(P, Q)<\\infty$, where $W_{2}$ is the quadratic Wasserstein distance.\n\nLetting $X \\sim P, Y \\sim Q$, we write $X(t)=c_{t} X+\\sigma_{t} Z_{D}$, and $Y(t)=c_{t} Y+\\sigma_{t} Z_{D}$, where $Z_{D} \\sim \\mathcal{N}(0, I d)$ be two standard Ornstein-Uhlenbeck processes initialised from $X, Y$\n\nrespectively. These random variables are absolutely continuous w.r.t. the Lebesgue measure with densities $p(t, x), q(t, x)$ respectively.\n\nIf we consider $\\nabla \\log q(t, x)$ as an estimator of $s(t, x)=\\nabla \\log p(t, x)$ on the interval $t \\in\\left[t_{\\min }, t_{\\max }\\right]$, then the corresponding score matching loss is bounded by\n\n$$\n\\int_{t_{\\min }}^{t_{\\max }} \\int_{\\mathbb{R}^{d}}\\|\\nabla \\log p(t, x)-\\nabla \\log q(t, x)\\|^{2} p(t, x) d x d t \\leq W_{2}^{2}(P, Q) \\frac{c_{t_{\\min }}^{2}}{4 \\sigma_{t_{\\min }}^{2}}\n$$\n\nProof. We believe that this statement can be traced back at least to Villani (2008), however, we could not find it in this form. Therefore, we provide proof in Appendix C.1.\n\nRemark 23. The bound is tight, to see this take $P=\\delta_{0}$, and $Q=\\delta_{x}$, for $\\|x\\|=\\varepsilon$.\nIn Section 5.1 we constructed a measure $\\mu^{*}$ satisfying $W_{2}\\left(\\mu, \\mu^{*}\\right) \\leq L^{*} \\varepsilon_{N}^{\\beta}$, so if we denote as $s^{*}$ the score function corresponding to $\\mu^{*}$ by Proposition 22\n\n$$\n\\int_{t_{\\min }}^{t_{\\max }} \\int_{\\mathbb{R}^{d}}\\left\\|s(t, x)-s^{*}(t, x)\\right\\|^{2} p(t, x) d x d t \\leq L^{*} \\varepsilon_{N}^{2 \\beta} \\frac{c_{t_{\\min }}^{2}}{4 \\sigma_{t_{\\min }}^{2}}\n$$\n\nFinally, we answer the last question and discuss how the substitution of the true score by the approximation $s^{*}(t, x)$ affects the regularity bounds on the score function derived in the Section 4. Let us present the bound on the correlation between Gaussian noise $Z_{D}$ and the correction brought by the substitution of $M$ by $M^{*}$\n\nProposition 24. Let $M^{*}$ be a piece-wise polynomial approximation satisfying Definition 14. Then for all positive $\\delta<1$ with probability at least $1-\\delta$ for all $i \\leq N$ and $y \\in M_{i}$\n\n$$\n\\left|\\left\\langle Z_{D}, y-\\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}(y)\\right\\rangle\\right| \\leq L^{*} \\varepsilon_{N}^{\\beta} \\sqrt{2 d \\log \\varepsilon_{N}^{-1}+2 \\log 2 \\delta^{-1}}\n$$\n\nProof. See Appendix C.3.\nAll the bounds in Section 4 follow from Proposition 8, therefore the statements presented in the previous section still hold if $t \\geq \\varepsilon_{N}^{2 \\beta}$ in the case of $M^{*}$ too, albeit with slightly different constants that are still independent of $D$. We present these results in Appendix C.3.", "tables": {}, "images": {}}, {"section_id": 14, "text": "# 6 Alternative Approach: Kernel-Based Score Function Approximation \n\nAn alternative approach to building estimator $\\hat{s}$ develops the idea that we used in case $d \\leq 2$, and based on the kernel-density estimator of $\\mu$ constructed by Divol (2022).\n\nWe assume usual assumptions, $\\mu$ is a measure satisfying Assumptions A-B supported on a $d$-dimensional manifold $M$, and we observed $n$ i.i.d. samples $\\mathcal{Y}=\\left\\{y_{1}, \\ldots, y_{n}\\right\\}$ from $\\mu$.\n\nIn Theorem 3.7 Divol (2022) constructs the kernel-density estimator $\\hat{\\mu}$ in two stages. As a first step, applying results of Aamari and Levrard (2019) that we discussed in Section 5.1 they solve optimization problem (27) and construct an approximating surface $M^{*}=\\cup M_{i}^{*}$. Then, using smooth partitioning functions $\\chi_{i}$, they construct a smooth approximation $\\widehat{\\operatorname{Vol}}\\left(d y^{*}\\right)$ to uniform measure $d y$ as $\\widehat{\\operatorname{Vol}}\\left(d y^{*}\\right)=\\sum \\chi_{i}\\left(y^{*}\\right) d y^{*}$, i.e. for $\\phi \\in C_{0}^{1}\\left(\\mathbb{R}^{D}\\right)$\n\n$$\n\\int_{M^{*}} \\phi\\left(y^{*}\\right) \\widehat{\\operatorname{Vol}}\\left(d y^{*}\\right)=\\sum \\int_{M_{i}^{*}} \\phi\\left(y^{*}\\right) \\chi_{i}\\left(y^{*}\\right) d y^{*}\n$$\n\nAs a second step, they construct a smoothing kernel $K_{n}(x)$ on $\\mathbb{R}^{D}$ as a rescaling of a special kernel $K$, see (Divol, 2022, Section 3) for precise description. Denote as\n\n$$\n\\nu_{n}=\\nu_{n}(\\mathcal{Y}):=\\frac{1}{n} \\sum_{y_{i} \\in \\mathcal{Y}} \\delta_{y_{i}}\n$$\n\nthe empirical density function. The estimator $\\hat{\\mu}_{n}$ is defined as\n\n$$\n\\hat{\\mu}_{n}=K_{n} *\\left(\\frac{\\nu_{n}}{K_{n} * \\operatorname{Vol} M}\\right)\n$$\n\nTheorem 25. (Divol, 2022, Theorem 3.7) For $d \\geq 3$, and any $1 \\leq p \\leq \\infty$\n\n$$\n\\mathbb{E}_{\\mathcal{Y} \\sim \\mu \\otimes \\kappa} W_{p}\\left(\\hat{\\mu}_{n}, \\mu\\right) \\lesssim n^{-\\frac{\\alpha+1}{2 \\alpha+d}}\n$$\n\nWe define $\\hat{s}_{n}$ as the score function of $\\hat{\\mu}_{n}$, then applying Proposition 22\n\n$$\n4 \\sigma_{T_{k}}^{2} \\int_{T_{k}}^{T_{k+1}} \\int_{\\mathbb{R}^{d}}\\left\\|\\hat{s}_{n}(t, x)-s(t, x)\\right\\|^{2} p_{t}(x) d x d t \\leq W_{2}^{2}\\left(\\mu, \\hat{\\mu}_{n}\\right)\n$$\n\nSince $T_{k+1}=2 T_{k}$ we have $\\sigma_{t} \\leq 2 \\sigma_{T_{k}}$ for all $t \\in\\left[T_{k}, T_{k+1}\\right]$. So, taking expectation and summing over all $k \\leq K=\\log (n)$\n\n$$\n\\mathbb{E}_{\\mathcal{Y} \\sim \\mu \\otimes \\kappa} \\int_{\\underline{T}}^{\\bar{T}} \\int_{\\mathbb{R}^{D}} \\sigma_{t}^{2}\\left\\|\\hat{s}_{n}(t, x)-s(t, x)\\right\\|^{2} p(t, x) d x d t \\lesssim n^{-\\frac{\\alpha+1}{2 \\alpha+d}} \\log n\n$$\n\nAt first glance, this gives an alternative and very short proof of Theorem 5. However, in this case, diffusion models are merely used to sample from the already known distribution $\\hat{\\mu}_{n}$, while in the results we presented we indeed learn the distribution, by solving the empirical risk minimization problem, which is much closer to what happens in practice.", "tables": {}, "images": {}}, {"section_id": 15, "text": "# 7 Conclusion and Future work \n\nWe have presented a novel approach for analyzing diffusion models under the manifold hypothesis in a high-dimensional setting. This approach allows us to show that: (i) neuralnetworks can approximate the normalized score function $\\sigma_{t} s(t, x)$ with the ambient-dimensionfree convergence rate $n^{-2(\\alpha+1) /(2 \\alpha+d)+\\gamma}$ w.r.t. the score matching loss (ii) diffusion models achieve the almost optimal convergence rate $\\sqrt{D} n^{-(\\alpha+1) /(2 \\alpha+d)+\\gamma}$ in $W_{1}$ metric.\n\nOur results can be extended in several directions. First, in this work, we have made the seemingly unnecessary Assumption D that can likely be lifted. Secondly, compared to Divol (2022), our convergence rate in $W_{1}$ still scales sub-optimally as $\\sqrt{D}$.\n\nAnother interesting direction is the study of which assumptions might be relaxed. The assumption that density $p(y)>p_{\\min }$ is lower bounded is quite strong, however, the bounds we got are polynomial on $C_{\\log }$, which itself is only logarithmic in $p_{\\min }$, so any smoothing, e.g. by the heat kernel could help. Also, it is interesting to study the case when the observed samples are corrupted by a bounded ambient noise.\n\nFinally, one can focus on the training stage. We built estimator $\\hat{s}$ as empirical risk minimizer over the integral loss $\\ell_{\\hat{s}}(y)$, however, in practice only a Monte Carlo approximation of $\\ell_{\\hat{s}}(y)$ is available, and the number of samples used to form this approximation directly affects the optimization time.", "tables": {}, "images": {}}, {"section_id": 16, "text": "# Acknowledgements \n\nIA was supported by the Engineering and Physical Sciences Research Council [grant number EP/T517811/1]. GD was supported by the Engineering and Physical Sciences Research Council [grant number EP/Y018273/1]. JR received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 834175)\n\nThe authors are also grateful to Leo Zhang for useful discussions.", "tables": {}, "images": {}}, {"section_id": 17, "text": "## References\n\nEddie Aamari. Convergence Rates for Geometric Inference. (Vitesses de convergence en inf\u00e9rence g\u00e9om\u00e9trique). PhD thesis, University of Paris-Saclay, France, 2017.\n\nEddie Aamari and Cl\u00e9ment Levrard. Stability and minimax optimality of tangential delaunay complexes for manifold reconstruction. Discrete and Computational Geometry, 59: $103-114,2018$.\n\nEddie Aamari and Cl\u00e9ment Levrard. Nonasymptotic rates for manifold, tangent space and curvature estimation. The Annals of Statistics, 47(1):177 - 204, 2019.\nR.J. Adler. An Introduction to Continuity, Extrema, and Related Topics for General Gaussian Processes. IMS Lecture Notes. Institute of Mathematical Statistics, 1990.\n\nBrian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313-326, 1982.\n\nCl\u00e9ment Berenfeld, Paul Rosa, and Judith Rousseau. Estimating a density near an unknown manifold: A Bayesian nonparametric approach. The Annals of Statistics, 52(5):2081 2111, 2024.\n\nValentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Transactions on Machine Learning Research, 2022.\n\nSitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.\n\nMarissa Connor, Gregory Canal, and Christopher Rozell. Variational autoencoder with learned latent structure. In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130, pages 2359-2367. PMLR, 2021.\n\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n\nVincent Divol. Measure estimation on manifolds: an optimal transport approach. Probability Theory and Related Fields, 183:581-647, 2022.\n\nHassan Emamirad and Arnaud Rougirel. De bruijn identities in different markovian channels. Electronic Journal of Differential Equations, 2023(01-87):1-11, 2023.\n\nZach Evans, Cj Carr, Josiah Taylor, Scott H. Hawley, and Jordi Pons. Fast timingconditioned latent audio diffusion. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 12652-12665. PMLR, 2024.\n\nHerbert Federer. Curvature measures. Trans. Amer. Math. Soc., 93, 1959.\nChristopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, and Larry Wasserman. Minimax manifold estimation. Journal of Machine Learning Research, 13(43):1263-1291, 2012.\n\nSatoshi Hayakawa and Taiji Suzuki. On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces. Neural Networks, 123:343-361, 2020.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.\n\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022.\nB. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):1302 - 1338, 2000.\nJ.M. Lee. Introduction to Smooth Manifolds. Graduate Texts in Mathematics. Springer New York, 2013.\n\nGen Li and Yuling Yan. Adapting to unknown low-dimensional structures in score-based diffusion models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\n\nYunqian Ma and Yun Fu. Manifold learning theory and applications, volume 434. CRC press Boca Raton, 2012.\n\nBatir Necdet. Inequalities for the gamma function. Archiv der Mathematik, 91, 2008.\nEdward Neuman. Inequalities and bounds for the incomplete gamma function. Results in Mathematics, 63, 2013.\n\nKazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 26517-26582. PMLR, 2023.\n\nJakiw Pidstrigach. Score-based generative models detect manifolds. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 35852-35865. Curran Associates, Inc., 2022.\n\nPeter Potaptchik, Iskander Azangulov, and George Deligiannidis. Linear convergence of diffusion models under the manifold hypothesis, 2024.\n\nHerbert E Robbins. An empirical bayes approach to statistics. Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, 1956.\n\nJohannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation function. The Annals of Statistics, 48(4), August 2020.\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.\n\nJan Pawel Stanczuk, Georgios Batzolis, Teo Deveney, and Carola-Bibiane Sch\u00f6nlieb. Diffusion models encode the intrinsic dimension of data manifolds. In Forty-first International Conference on Machine Learning, 2024.\n\nTaiji Suzuki. Adaptivity of deep reLU network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality. In International Conference on Learning Representations, 2019.\n\nRong Tang and Yun Yang. Adaptivity of diffusion models to manifold structures. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238, pages 1648-1656. PMLR, 2024.\nJ.M. Tomczak. Deep Generative Modeling. Springer International Publishing, 2022. ISBN 9783030931582 .\n\nRoman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.\nC. Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg, 2008.\n\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661-1674, 2011.\n\nMartin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.\n\nJoseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana V\u00e1zquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. De novo design of protein structure and function with rfdiffusion. Nature, 620, 2023.\n\nLing Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1-39, 2023.", "tables": {}, "images": {}}, {"section_id": 18, "text": "# Appendix A. Geometric Results \n\nIn this section we discuss the key properties of local parametrization $\\Phi_{y}, y \\in M$ introduced in Section 2.2.\n\nProposition 26. Let $M$ be a manifold satisfying Assumption $A$, and $y \\in M$. Let $\\Phi_{y}$ : $T_{y} M \\mapsto M$ be a local inverse of projection $\\mathrm{pr}_{y}$ on $T_{y} M$ and $r_{0}=\\min \\left(\\tau, L_{M}^{-1}\\right) / 8$, where $L_{M} \\geq 1$.\n(i) The function $\\Phi_{y}$ is well defined on $B_{T_{y} M}(0, \\tau / 4)$. For any $z \\in B_{T_{y} M}(0, \\tau / 4)$ we have $\\|z\\| \\leq\\left\\|\\Phi_{y}(z)-y\\right\\| \\leq 2\\|z\\|$.\n(ii) $\\left\\|\\nabla \\Phi_{y}(z)-\\nabla \\Phi_{y}(0)\\right\\|_{o p} \\leq L_{M}\\|z\\|$.\n(iii) $\\nabla \\Phi_{y}(0) \\nabla^{T} \\Phi_{y}(0)=\\operatorname{Id}_{d}$, and for $\\|z\\| \\leq r_{0}$ we have $\\left\\|\\nabla \\Phi_{y}(z) \\nabla^{T} \\Phi_{y}(z)-\\operatorname{Id}_{d}\\right\\|_{o p} \\leq 1 / 2$.\n\nProof. (i) See (Divol, 2022, Lemma A.1(iv))\n(ii) By the mean value theorem\n\n$$\n\\left\\|\\nabla \\Phi_{y}(z)-\\nabla \\Phi_{y}(0)\\right\\|_{o p}=\\sup _{\\|u\\|=1}\\left\\|d_{u}\\left(\\Phi_{y}(z)-\\Phi_{y}(0)\\right)\\right\\| \\leq\\|z\\| \\sup _{z^{\\prime} \\in[0, z],\\|u\\|,\\|v\\|=1}\\left\\|d_{u} d_{v} \\Phi_{y}\\left(z^{\\prime}\\right)\\right\\| \\leq L_{M}\\|z\\|\n$$\n\n(iii) The first part holds by the definition of $\\Phi_{y}$ as an inverse projection on $T_{y} M$. Next, we prove the second part.\n\n$$\n\\begin{aligned}\n\\left\\|\\nabla \\Phi_{y} \\nabla^{T} \\Phi_{y}-\\operatorname{Id}\\right\\|_{o p} & =\\sup _{\\|v\\|=1}\\left\\|\\nabla \\Phi_{y}(z) \\nabla^{T} \\Phi_{y}(z) v-v\\right\\|=\\sup _{\\|u\\|,\\|v\\|=1}\\left\\langle\\nabla \\Phi_{y}(z)\\left(d_{v} \\Phi_{y}(z)\\right)-v, u\\right\\rangle \\\\\n& =\\sup _{\\|u\\|,\\|v\\|=1}\\left[d_{v} \\Phi_{y}(z)\\right]^{T} d_{u} \\Phi_{y}(z)-v^{T} u \\\\\n& =\\sup _{\\|u\\|,\\|v\\|=1}\\left[d_{v} \\Phi_{y}(z)\\right]^{T} d_{u} \\Phi_{y}(z)-\\left[d_{v} \\Phi_{y}(0)\\right]^{T} d_{u} \\Phi_{y}(0) \\\\\n& =\\sup _{\\|u\\|,\\|v\\|=1}\\left(d_{v} \\Phi_{y}(z)\\right)^{T} d_{u}\\left(\\Phi_{y}(z)-\\Phi_{y}(0)\\right)+d_{v}\\left(\\Phi_{y}(z)-\\Phi_{y}(0)\\right)^{T} u \\\\\n& \\leq \\sup _{\\|v\\|=1}\\left(\\left\\|d_{v}^{T} \\Phi_{y}(z)\\right\\|+1\\right) \\sup _{\\|u\\|=1}\\left\\|d_{u}\\left(\\Phi_{y}(z)-\\Phi_{y}(0)\\right)\\right\\|\n\\end{aligned}\n$$\n\nApplying part (ii) we get\n\n$$\n\\begin{aligned}\n& \\sup _{\\|v\\|=1}\\left\\|d_{v}^{T} \\Phi_{y}(z)\\right\\| \\leq \\sup _{\\|v\\|=1}\\left(\\left\\|d_{v}^{T} \\Phi_{y}(z)-d_{v}^{T} \\Phi_{y}(0)\\right\\|+\\left\\|d_{v}^{T} \\Phi_{y}(0)\\right\\| \\leq L_{M}\\|z\\|+1\\right. \\\\\n& \\left.\\sup _{\\|u\\|=1}\\left\\|d_{u}\\left(\\Phi_{y}(z)-\\Phi_{y}(0)\\right)\\right\\| \\leq L_{M}\\|z\\|\\right.\n\\end{aligned}\n$$\n\nSo since $\\|z\\|<L_{M}^{-1} / 8$\n\n$$\n\\begin{aligned}\n\\left\\|\\nabla \\Phi \\nabla^{T} \\Phi-\\operatorname{Id}\\right\\|_{o p} \\leq \\sup _{\\|v\\|=1}\\left(\\left\\|d_{v}^{T} \\Phi_{y}(z)\\right\\|+1\\right) & \\sup _{\\|u\\|=1}\\left\\|d_{u}\\left(\\Phi_{y}(z)-\\Phi_{y}(0)\\right)\\right\\| \\\\\n& \\leq L_{M}\\|z\\|\\left(L_{M}\\|z\\|+2\\right) \\leq 3 / 8 \\leq 1 / 2\n\\end{aligned}\n$$\n\nProposition 27. Let $M$ be a manifold satisfying Assumption A. Fix $\\varepsilon<r_{0}$ and denote as $C_{d}$ the volume of the unit d-dimensional ball.\n(i) Let $M_{y}(\\varepsilon)=\\Phi_{y}\\left(B_{d}(0, \\varepsilon)\\right)$ then\n\n$$\nC_{d} \\varepsilon^{d} \\leq \\operatorname{Vol} M_{y}(\\varepsilon) \\leq 2^{d} C_{d} \\varepsilon^{d}\n$$\n\nwhere $C_{d}$ is volume of a d-dimensional unit ball.\n(ii) Recall that for $y \\in M$ we defined $B_{M}(y, \\varepsilon)=M \\cap B(y, \\varepsilon)$ then\n\n$$\n2^{-d} C_{d} \\varepsilon^{d} \\leq \\operatorname{Vol} B_{M}(y, \\varepsilon) \\leq 2^{d} C_{d} \\varepsilon^{d}\n$$\n\nProof. (i) Since projection $\\mathrm{pr}_{T_{y} M}$ is contraction, and $\\Phi_{y}$ is the inverse map\n\n$$\n\\operatorname{Vol} M_{y}(\\varepsilon) \\geq \\operatorname{Vol}\\left(\\operatorname{pr}_{T y M} M_{y}(\\varepsilon)\\right)=\\operatorname{Vol} B_{d}(0, \\varepsilon)=\\operatorname{Vol} B_{d}(0,1) \\varepsilon^{d}=C_{d} \\varepsilon^{d}\n$$\n\nOn the other hand, applying Proposition 26\n\n$$\n\\operatorname{Vol} M_{y}(\\varepsilon)=\\int_{M_{y}(\\varepsilon)} d y=\\int_{B_{d}(0, \\varepsilon)}\\left|\\nabla \\Phi_{y}(z) \\nabla^{T} \\Phi_{y}(z)\\right|^{-1 / 2} d z \\leq 2^{d} \\operatorname{Vol} B_{d}(0, \\varepsilon) \\leq 2^{d} C_{d} \\varepsilon^{d}\n$$\n\n(ii) Since $\\Phi_{y}(z)$ is 2-Lipschitz $\\Phi_{y}\\left(B_{d}(0, \\varepsilon / 2)\\right) \\subset B_{M}(y, \\varepsilon)$\n\n$$\n\\operatorname{Vol} B_{M}(y, \\varepsilon) \\geq \\operatorname{Vol} \\Phi_{y}\\left(B_{d}(0, \\varepsilon / 2)\\right) \\geq 2^{-d} C_{d} \\varepsilon^{d}\n$$\n\nFinally, since $\\mathrm{pr}_{T_{y} M}$ is contraction\n\n$$\n\\operatorname{Vol} B_{M}(y, \\varepsilon) \\leq \\operatorname{Vol} M_{y}(\\varepsilon) \\leq 2^{d} C_{d} \\varepsilon^{d}\n$$", "tables": {}, "images": {}}, {"section_id": 19, "text": "# Appendix B. Concentration of the Score Function\n## B. 1 Correlation with Gaussian Noise\n\nThe main goal of this section is to prove our key technical result Proposition 8. We start the proof by establishing a technical proposition that will be used in the proofs.\n\nProposition 28. Let $f: U \\subset B_{d}(0, r) \\subset \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{D}$ be $L$-Lipschitz such that $f(0)=0$. Let $Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ be a $D$-dimensional standard normal vector and define the centred Gaussian field $F(z):=\\left\\langle Z_{D}, f(z)\\right\\rangle$ on $U$. Then for any $\\delta>0$\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in U} F(z) \\leq \\operatorname{Lr} \\sqrt{d}+\\operatorname{Lr} \\sqrt{2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta\n$$\n\nProof. The main idea of the proof is to find another process $F^{\\prime}$ with a larger variance of pairwise differences and with easy-to-control supremum. Then, the first condition guarantees a higher spread in values, and, therefore, a larger expected maximum, while the second helps us to control this supremum exactly. The difference between values in two points is\n\ncontrolled by the Lipschitz constant allowing us to choose $F^{\\prime}(z)=L\\left\\langle z, Z_{d}\\right\\rangle$ - a process defined intrinsically in $\\mathbb{R}^{d}$.\n\nFormally, by the Lipschitz property for any $z, z^{\\prime} \\in B(y, r)$\n\n$$\n\\mathbb{E}\\left|F(z)-F\\left(z^{\\prime}\\right)\\right|^{2}=\\mathbb{E}\\left|\\left\\langle Z_{D}, f(z)-f\\left(z^{\\prime}\\right)\\right\\rangle\\right|^{2}=\\left\\|f(z)-f\\left(z^{\\prime}\\right)\\right\\|^{2} \\leq L^{2}\\left\\|z-z^{\\prime}\\right\\|^{2}\n$$\n\nLet $Z_{d} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{d}\\right)$, consider $G(z)=L\\left\\langle Z_{d}, z\\right\\rangle$ an auxiliary Gaussian field on $B(y, r)$. Both $F(z)$ and $G(z)$ are continuous zero-mean Gaussian fields, and by construction $\\mathbb{E}\\left|F(z)-F\\left(z^{\\prime}\\right)\\right|^{2} \\leq$ $\\mathbb{E}\\left|G(z)-G\\left(z^{\\prime}\\right)\\right|^{2}$, so by the Sudakov-Fernique inequality (Adler, 1990, Theorem 2.8)\n\n$$\n\\mathbb{E} \\sup _{z \\in U} F(z) \\leq \\mathbb{E} \\sup _{z \\in U} G(z) \\leq \\mathbb{E} \\sup _{z \\in B_{d}(y, r)} L\\left\\langle Z_{d}, z\\right\\rangle=L r \\mathbb{E}\\left\\|Z_{d}\\right\\| \\leq L r \\sqrt{d}\n$$\n\nMoreover,\n\n$$\n\\sup _{z \\in U} \\mathbb{E}|F(z)|^{2}=\\sup _{z \\in U}\\|f(z)\\|^{2} \\leq(L r)^{2}\n$$\n\nFinally, we note that $F(z)$ is continuous, so $\\sup |F(z)|<\\infty$ a.s., and by the Borell-TIS inequality (Adler, 1990, Theorem 2.1),\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in U} F(z) \\leq L r \\sqrt{d}+L r \\sqrt{2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta\n$$\n\nNow we are ready to prove the main result of this section\nProposition 8. Let $M$ be a $\\beta \\geq 1$-smooth manifold satisfying Assumption $A$. Then for any $\\delta>0$ and $\\varepsilon<r_{0}$ with probability $1-\\delta$ for all $y, y^{\\prime} \\in M$\n\n$$\n\\left|\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle\\right| \\leq 4 \\varepsilon \\sqrt{d}+\\left(\\left\\|y-y^{\\prime}\\right\\|+6 \\varepsilon\\right) \\sqrt{4 d \\log 2 \\varepsilon^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}}\n$$\n\nProof. The main idea is to first prove the inequality locally and then generalize it to arbitrary $y, y^{\\prime} \\in M$ using a chaining argument by taking an $\\varepsilon$-dense set on $M$.\n\nFix a small enough constant $\\varepsilon>0$ and an arbitrary point $G \\in M$. By Assumption A the manifold $M$ can be locally represented as a graph of a function $\\Phi_{G}$. By choice of $\\varepsilon$, the function $\\Phi_{G}(z)$ is 2-Lipschitz by Proposition 26 and by the definition $\\Phi_{G}(0)=G$, so applying Proposition 28 we conclude\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in B_{T_{G} M}(G, \\varepsilon)}\\left\\langle Z_{D}, \\Phi_{G}(z)-G\\right\\rangle \\leq 2 \\varepsilon\\left(\\sqrt{d}+\\sqrt{2 \\log \\delta^{-1}}\\right)\\right) \\geq 1-\\delta\n$$\n\nthat implies a local version of the desired result\n\n$$\n\\mathbb{P}\\left(\\sup _{y \\in M \\cap B(G, \\varepsilon)}\\left\\langle Z_{D}, y-G\\right\\rangle \\leq 2 \\varepsilon\\left(\\sqrt{d}+\\sqrt{2 \\log \\delta^{-1}}\\right)\\right) \\geq 1-\\delta\n$$\n\nTo make it global, we take a $\\varepsilon$-dense set $G_{1}, \\ldots, G_{N}$, where by Proposition 3 we can guarantee $N \\leq(\\varepsilon / 2)^{-d} \\operatorname{Vol} M$. For any pair $G_{i}, G_{j}$ we note that $\\left\\langle Z_{D}, G_{i}-G_{j}\\right\\rangle \\sim \\mathcal{N}\\left(0,\\left\\|G_{i}-G_{j}\\right\\|^{2}\\right)$ and therefore\n\n$$\n\\mathbb{P}\\left(\\left\\langle Z_{D}, G_{i}-G_{j}\\right\\rangle \\leq\\left\\|G_{i}-G_{j}\\right\\| \\sqrt{2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta\n$$\n\nWe estimate from above with $\\delta N(N+1) / 2$ the probability that simultaneously for any $G_{i}$ event (29) holds, and for any pair $G_{i} \\neq G_{j}$ event (30) holds too.\n\nRepresenting $\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle=\\left\\langle Z_{D}, y-G\\right\\rangle+\\left\\langle Z_{D}, G-G^{\\prime}\\right\\rangle+\\left\\langle Z_{D}, G^{\\prime}-y^{\\prime}\\right\\rangle$, where $G, G^{\\prime}$ are the nearest points in the $\\varepsilon$-net to $y, y^{\\prime}$ respectively, and applying the bounds above we get that\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left[\\forall y, y^{\\prime} \\in M:\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle \\leq 4 \\varepsilon\\left(\\sqrt{d}+\\sqrt{2 \\log \\delta^{-1}}\\right)+\\left(\\left\\|y-y^{\\prime}\\right\\|+2 \\varepsilon\\right) \\sqrt{2 \\log \\delta^{-1}}\\right] \\\\\n& \\geq 1-\\delta N(N+1) / 2\n\\end{aligned}\n$$\n\nwhere we additionally used the triangle inequality\n\n$$\n\\left\\|G-G^{\\prime}\\right\\| \\leq\\left\\|y-y^{\\prime}\\right\\|+\\|y-G\\|+\\left\\|y^{\\prime}-G^{\\prime}\\right\\| \\leq\\left\\|y-y^{\\prime}\\right\\|+2 \\varepsilon\n$$\n\nNext, we reorder and perform a change of variable\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\forall y, y^{\\prime} \\in M:\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle \\leq 4 \\varepsilon \\sqrt{d}+\\left(\\left\\|y-y^{\\prime}\\right\\|+6 \\varepsilon\\right) \\sqrt{2 \\log N(N+1) / 2+2 \\log \\delta^{-1}}\\right) \\\\\n& \\geq 1-\\delta\n\\end{aligned}\n$$\n\nSince $N(N+1) / 2 \\leq N^{2} \\leq(\\varepsilon / 2)^{-2 d}(\\operatorname{Vol} M)^{2}$ we get\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left[\\forall y, y^{\\prime} \\in M:\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle \\leq 4 \\varepsilon \\sqrt{d}+\\right. \\\\\n&\\left.\\left(\\left\\|y-y^{\\prime}\\right\\|+6 \\varepsilon\\right) \\sqrt{4 d \\log 2 \\varepsilon^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log \\delta^{-1}}\\right] \\geq 1-\\delta\n\\end{aligned}\n$$\n\nFinally, noting that the same bound holds for $-\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle \\stackrel{\\text { dist. }}{=}\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle$ we obtain the bound on $\\left|\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle\\right|$\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left[\\forall y, y^{\\prime} \\in M:\\left|\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle\\right| \\leq 4 \\varepsilon \\sqrt{d}+\\right. \\\\\n&\\left.\\left(\\left\\|y-y^{\\prime}\\right\\|+6 \\varepsilon\\right) \\sqrt{4 d \\log 2 \\varepsilon^{-1}+4 \\log \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}}\\right] \\geq 1-\\delta\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 20, "text": "# B. 2 Bounds on the Score Function \n\nProposition 11. Let measure $\\mu$ satisfy Assumptions $A-C$ and be supported on $M$. For any $\\delta>0$ with probability at least $1-\\delta$ on $X(t)$ the following bound holds uniformly in $y \\in M$\n\n$$\n\\begin{aligned}\n-20 d\\left(\\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)-8 \\log & \\delta^{-1}-\\frac{3}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2} \\\\\n& \\leq \\log p(y \\mid t, X(t)) \\\\\n\\leq & 20 d\\left(\\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}-\\frac{1}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2}\n\\end{aligned}\n$$\n\nProof. Recall that $p(y \\mid t, X(t)) \\propto e^{-\\left\\|X\\left(t\\right)-c_{t} y\\right\\|^{2} / 2 \\sigma_{t}^{2}} p(y)$, so\n\n$$\n\\log p(y \\mid t, X(t))=-\\log \\left(\\int_{M} e^{-\\left\\|X\\left(t\\right)-c_{t} y\\right\\|^{2} / 2 \\sigma_{t}^{2}} p(y) d y\\right)-\\left\\|X\\left(t\\right)-c_{t} y\\right\\|^{2} / 2 \\sigma_{t}^{2}+\\log p(y)\n$$\n\nWe start our proof with an analysis of term $-\\|X(t)-y\\|^{2}$. By definition of $X(t)$ $\\left\\|X(t)-c_{t} y\\right\\|^{2}=\\left\\|c_{t} X(0)+\\sigma_{t} Z_{D}-c_{t} y\\right\\|^{2}=\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}+2 c_{t} \\sigma_{t}\\left\\langle Z_{D}, X(0)-y\\right\\rangle+c_{t}^{2}\\|X(0)-y\\|^{2}$.\nSince $X(0) \\in M$, by Proposition 8 , for $\\varepsilon<r_{0}$ with probability $1-\\delta$ for all $y \\in M$\n\n$$\n\\left|\\left\\langle Z_{D}, X(0)-y\\right\\rangle\\right| \\leq 4 \\varepsilon \\sqrt{d}+(\\|X(0)-y\\|+6 \\varepsilon) \\sqrt{4 d \\log 2 \\varepsilon^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}}\n$$\n\nTaking $\\varepsilon=\\min \\left(r_{0}, \\frac{1}{4}\\left(\\sigma_{t} / c_{t}\\right)\\right)$ we get\n\n$$\n\\begin{aligned}\n& \\left|\\left\\langle Z_{D}, X(0)-y\\right\\rangle\\right| \\leq \\frac{4}{4}\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d} \\\\\n& +\\left(\\|X(0)-y\\|+\\frac{6}{4}\\left(\\sigma_{t} / c_{t}\\right)\\right) \\sqrt{4 d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)^{-1}+4 d \\log 8 r_{0}^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}}\n\\end{aligned}\n$$\n\nSince $2<\\sqrt{4 \\log 8}$ we have $\\frac{4}{4} \\sqrt{d} \\leq \\frac{2}{4} \\sqrt{4 d \\log 8}$ and therefore\n\n$$\n\\begin{aligned}\n& 2 \\sigma_{t} c_{t}\\left|\\left\\langle Z_{D}, X(0)-y\\right\\rangle\\right| \\leq 2 \\sigma_{t}^{2} \\sqrt{4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 d \\log 8 r_{0}^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}} \\\\\n& \\quad+2 \\sigma_{t} c_{t}\\|X(0)-y\\| \\sqrt{4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 d \\log 8 r_{0}^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}}\n\\end{aligned}\n$$\n\nwhile the last term can be bounded by Young's inequality as\n\n$$\n\\begin{aligned}\n& 2 \\sigma_{t} c_{t}\\|X(0)-y\\| \\sqrt{4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 d \\log 8 r_{0}^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}} \\\\\n& \\quad \\leq \\frac{1}{2} c_{t}^{2}\\|X(0)-y\\|^{2}+2 \\sigma_{t}^{2}\\left(4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 d \\log 8 r_{0}^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}\\right)\n\\end{aligned}\n$$\n\nSubstituting, since $\\sqrt{x} \\leq x$ for $x \\geq 1$, we conclude that with probability at least $1-\\delta$ that for all $y \\in M$\n\n$$\n\\begin{aligned}\n2 \\sigma_{t} c_{t}\\left|\\left\\langle Z_{D}, X(0)-y\\right\\rangle\\right| \\leq & \\frac{1}{2} c_{t}^{2}\\|X(0)-y\\|^{2} \\\\\n& +4 \\sigma_{t}^{2}\\left(4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 d \\log 8 r_{0}^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\delta^{-1}\\right)\n\\end{aligned}\n$$\n\nFinally, by definition of $C_{\\log }$ we have $4 d \\log 8 r_{0}^{-1}+4 \\log _{+} \\operatorname{Vol} M+2 \\log 2 \\leq C_{\\log }(12 d+2)$ whence\n\n$$\n\\begin{aligned}\n& \\frac{1}{2} c_{t}^{2}\\|X(0)-y\\|^{2}-4 \\sigma_{t}^{2} C_{0}= \\\\\n& -4 \\sigma_{t}^{2}\\left(4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+(12 d+2) C_{\\log }+2 \\log \\delta^{-1}\\right)+\\frac{1}{2} c_{t}^{2}\\|X(0)-y\\| \\\\\n& \\leq\\left\\|X(t)-c_{t} y\\right\\|^{2}-\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2} \\leq \\\\\n& 3 c_{t}^{2}\\|X(0)-y\\|^{2} / 2+4 \\sigma_{t}^{2}\\left(4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+(12 d+2) C_{\\log }+2 \\log \\delta^{-1}\\right) \\\\\n& =\\frac{3}{2} c_{t}^{2}\\|X(0)-y\\|^{2}+4 \\sigma_{t}^{2} C_{0}\n\\end{aligned}\n$$\n\nwhere, for brevity, we introduced\n\n$$\nC_{0}:=\\left(4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+(12 d+2) C_{\\log }+2 \\log \\delta^{-1}\\right)\n$$\n\nNext, we bound the normalizing constant\n\n$$\n\\log \\left(\\int_{M} e^{-\\|X(t)-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} p(y) d y\\right)\n$$\n\nUsing the inequality (34) and a local representation of the manifold as $\\Phi_{X(0)}\\left(B_{T_{X(0)} M}\\left(X(0), r_{0}\\right)\\right)$, where $\\Phi_{X(0)}$ is 2-Lipschitz, we obtain\n\n$$\n\\begin{aligned}\n\\int_{M} e^{-\\|X(t)-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} p(y) d y & \\geq e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{-2 C_{0}} \\int_{\\Phi_{X(0)}\\left(B_{T_{X(0)} M}\\left(X(0), r_{0}\\right)\\right)} e^{-3 c_{t}^{2}\\|X(0)-y\\|^{2} / 4 \\sigma_{t}^{2}} p(y) d y \\\\\n& \\geq 2^{-d} p_{\\min } e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{-2 C_{0}} \\int_{z \\in \\mathbb{R}^{d},\\|z\\| \\leq r_{0}} e^{-3 c_{t}^{2}\\|z\\|^{2} / \\sigma_{t}^{2}} d z \\\\\n& \\geq \\frac{e^{d}}{(2 d)^{d+1}} e^{-2} p_{\\min } e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{-2 C_{0}}\\left(r_{0} \\wedge\\left(\\sigma_{t} / c_{t}\\right)\\right)^{d}\n\\end{aligned}\n$$\n\nThe last inequality follows from integration in polar coordinates, standard bounds on $\\Gamma-$ function (Necdet, 2008, Theorem 1.5) and inequality on the incomplete gamma function $\\gamma(a, x)$ from (Neuman, 2013, Theorem 4.1). More precisely, we applied that for $a, b>0$ and $c:=\\min (a, b)$ holds\n\n$$\n\\begin{aligned}\n& \\int_{z \\in \\mathbb{R}^{d},\\|z\\| \\leq a} e^{-3\\|z\\|^{2} / b^{2}} d z \\geq \\int_{z \\in \\mathbb{R}^{d},\\|z\\| \\leq c} e^{-3\\|z\\|^{2} / c^{2}} d z=\\int_{0}^{c} e^{-3 r^{2} / c^{2}} r^{d-1} d r \\\\\n& \\quad=\\frac{2 \\pi^{d / 2}}{\\Gamma(d / 2)} \\frac{c^{d}}{6^{d / 2}} \\gamma\\left(\\frac{d}{2}, 3\\right) \\geq \\frac{2 \\cdot \\pi^{d / 2}}{(2 \\pi)^{1 / 2}(d / 2 e)^{d}} \\frac{c^{d}}{6^{d / 2}} e^{-\\frac{3 d}{2 d+1}} \\frac{3^{d / 2}}{(d / 2)} \\geq c^{d} \\frac{e^{d}}{d^{d+1}} \\frac{e^{-2}}{2} .\n\\end{aligned}\n$$\n\nTaking logarithms in (35) we conclude\n\n$$\n\\begin{aligned}\n&-\\log \\left(\\int_{M} e^{-\\|X(t)-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} p(y) d y\\right) \\\\\n& \\quad \\leq\\left\\|Z_{D}\\right\\|^{2} / 2+2 C_{0}+d \\log 2 d+2+\\log _{+} p_{\\min }^{-1}+d \\log r_{0}^{-1}+d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)^{-1} \\\\\n& \\quad \\leq\\left\\|Z_{D}\\right\\|^{2} / 2+2 C_{0}+d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+C_{\\log }(3 d+1)\n\\end{aligned}\n$$\n\nIn a similar way, we can get an upper-bound, since $\\int_{M} p(y) d y=1$\n\n$$\n\\int_{M} e^{-\\|X(t)-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} p(y) d y \\leq e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{2 C_{0}} \\int_{M} e^{-c_{t}^{2}\\|X(0)-y\\|^{2} / 4 \\sigma_{t}^{2}} p(y) d y \\leq e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{2 C_{0}}\n$$\n\nSo,\n\n$$\n-\\log \\left(\\int_{M} e^{-\\|X(t)-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} p(y) d y\\right) \\geq-2 C_{0}+\\left\\|Z_{D}\\right\\|^{2} / 2\n$$\n\nFinally, recalling the representation (32) of $\\log p(y \\mid t, X(t))$ and that $|\\log p(y)| \\leq d C_{\\log }$\n\n$$\n\\begin{aligned}\n& -16 d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)-(49 d+8) C_{\\log }-8 \\log \\delta^{-1}-\\frac{3}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2} \\\\\n& =-4 C_{0}-d C_{\\log }-\\frac{3}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2} \\\\\n& \\leq \\log p(y \\mid t, X(t))=-\\log \\left(\\int_{M} e^{-\\|X(t)-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}} p(y) d y\\right)-\\|X(t)-c_{t} y\\|^{2} / 2 \\sigma_{t}^{2}+\\log p(y) \\\\\n& \\leq 4 C_{0}+d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+C_{\\log }(4 d+1)-\\frac{1}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2} \\\\\n& =17 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+(52 d+9) C_{\\log }+8 \\log \\delta^{-1}-\\frac{1}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2}\n\\end{aligned}\n$$\n\nthat implies\n\n$$\n\\begin{aligned}\n& -19 d\\left(\\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)-8 \\log \\delta^{-1}-\\frac{3}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2} \\\\\n& \\leq \\log p(y \\mid t, X(t)) \\leq \\\\\n& 19 d\\left(\\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}-\\frac{1}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2}\n\\end{aligned}\n$$\n\nTheorem 12. Let measure $\\mu$ satisfy Assumptions $A-C$ and be supported on $M$. Fix $t, \\delta, \\eta>$ 0 and define\n\n$$\nr(t, \\delta, \\eta)=2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20 d\\left(\\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}+\\log \\eta^{-1}}\n$$\n\n(i) A.s.in $X(0) \\sim \\mu$ and with probability $1-\\delta$ in $Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ for a random point $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$ the following holds\n\n$$\n\\int_{y \\in M:|y-X(0)| \\leq r(t, \\delta, \\eta)} p(y \\mid t, X(t)) d y \\geq 1-\\eta\n$$\n\n(ii) As a corollary, a.s. in $X(0) \\sim \\mu$ and with probability $1-\\delta$ in $Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ for a random point $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$ the following holds\n\n$$\n\\left\\|\\sigma_{t} s(t, X(t))+Z_{D}\\right\\| \\leq 4 \\sqrt{20 d\\left(2 \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}}\n$$\n\n(iii) Integrating over $Z_{D}$, for all $y \\in M$\n\n$$\n\\mathbb{E}_{Z_{D}}\\left\\|\\sigma_{t} s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)+Z_{D}\\right\\|^{2} \\leq 8\\left(40 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+80 d C_{\\log }+3\\right)\n$$\n\nProof. (i) Fix $r>0$. Applying (38)\n\n$$\n\\begin{aligned}\n& \\int_{\\|y-X(0)\\| \\geq r} p(y \\mid t, X(t)) d y \\\\\n& \\leq e^{19 d\\left(\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}} \\int_{\\|y-X(0)\\| \\geq r} e^{-\\frac{1}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2}\\|X(0)-y\\|^{2}} d y \\\\\n& \\leq e^{19 d\\left(\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}}\\left[e^{-\\frac{1}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2} r^{2}} \\operatorname{Vol} M\\right] \\\\\n& \\leq e^{20 d\\left(\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}} e^{-\\frac{1}{4}\\left(c_{t} / \\sigma_{t}\\right)^{2} r^{2}} \\leq \\eta\n\\end{aligned}\n$$\n\nwhere we additionally used that $\\operatorname{Vol} M<e^{d C_{\\log }}$ and the last inequality holds if we take\n\n$$\nr=r(t, \\delta, \\eta):=2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20 d\\left(\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}+\\log \\eta^{-1}}\n$$\n\n(ii) First we express the LHS as\n\n$$\n\\begin{aligned}\n\\sigma_{t} s(t, X(t))+Z_{D}=\\int_{M} \\frac{c_{t} y-c_{t} X(0)-\\sigma_{t} Z_{D}}{\\sigma_{t}} p( & y \\mid t, X(t))+Z_{D} \\\\\n& =\\int_{M} \\frac{c_{t}}{\\sigma_{t}}(y-X(0)) p(y \\mid t, X(t))\n\\end{aligned}\n$$\n\nSince $\\operatorname{diam} M \\leq 1,\\|X(0)-y\\| \\leq 1$ a.s., so splitting integral by event $\\|X(0)-y\\| \\leq$ $r(t, \\delta, \\eta)$ and applying (24) we have that with probability $1-\\delta$\n\n$$\n\\left\\|\\sigma_{t} s(t, X(t))+Z_{D}\\right\\| \\leq\\left(c_{t} / \\sigma_{t}\\right)(r(t, \\delta, \\eta)+\\eta)\n$$\n\nTaking $\\eta=\\min \\left(1,\\left(\\sigma_{t} / c_{t}\\right)\\right)$ with probability $1-\\delta$\n\n$$\n\\begin{aligned}\n& \\left\\|\\sigma_{t} s(t, X(t))+Z_{D}\\right\\| \\leq 2 \\sqrt{20 d\\left(\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}+\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)}+1 \\\\\n& \\leq 4 \\sqrt{20 d\\left(2 \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}}\n\\end{aligned}\n$$\n\n(iii) Similarly, by (39), we have that a.s. $\\left\\|\\sigma_{t} s(t, X(t))+Z_{D}\\right\\| \\leq c_{t} / \\sigma_{t}$, so applying the first inequality of (40) and $(a+b)^{2} \\leq 2 a^{2}+2 b^{2}$\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left\\|\\sigma_{t} s(t, X(t))+Z_{D}\\right\\|^{2} \\\\\n& \\quad \\leq 8\\left(20 d\\left(\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 C_{\\log }\\right)+8 \\log \\delta^{-1}+\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)\\right)+2+\\left(c_{t} / \\sigma_{t}\\right)^{2} \\delta\n\\end{aligned}\n$$\n\nTaking $\\delta=\\min \\left(1,\\left(\\sigma_{t} / c_{t}\\right)^{2}\\right)$\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left\\|\\sigma_{t} s(t, X(t))+Z_{D}\\right\\|^{2} \\\\\n& \\quad \\leq 8\\left(20 d\\left(\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 C_{\\log }\\right)+16 \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+2 \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)\\right)+3 \\\\\n& \\quad \\leq 8\\left(20 d\\left(2 \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 C_{\\log }\\right)\\right)+3\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 21, "text": "# B. 3 Point Comparision \n\nThis section proves key statements used to build an efficient approximation of the score function. The following lemma is a full version of Lemma 13\n\nLemma 29. Let $\\mathcal{P}=\\left\\{m_{1}, \\ldots, m_{N}\\right\\} \\subset M$ be a set of points in $M$. Denote the nearest neighbor of $X(t)$ in $\\mathcal{P}$ with $m_{\\min }(t)=m_{\\min }(X(t)):=\\arg \\min _{i}\\left\\|X(t)-c_{t} m_{i}\\right\\|$, then with probability at least $1-\\delta$ :\n(i) $m_{\\min }(t)$ is close to $m_{\\min }(0)$\n\n$$\n\\left\\|X(0)-m_{\\min }(t)\\right\\|^{2} \\leq 3\\left\\|X(0)-m_{\\min }(0)\\right\\|^{2}+64\\left(\\sigma_{t} / c_{t}\\right)^{2}\\left(d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)^{-1}+4 d C_{\\log }+\\log \\delta^{-1}\\right)\n$$\n\n(ii) for any $m_{i} \\in \\mathcal{P}$\n\n$$\n\\begin{aligned}\n& \\left\\|X(t)-c_{t} m_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2} \\geq \\frac{c_{t}^{2}}{2}\\left(\\left\\|X(0)-m_{i}\\right\\|^{2}-9\\left\\|X(0)-m_{\\min }(0)\\right\\|^{2}\\right) \\\\\n& \\quad-128\\left(\\sigma_{t} / c_{t}\\right)^{2}\\left(d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)^{-1}+4 d C_{\\log }+\\log \\delta^{-1}\\right)\n\\end{aligned}\n$$\n\n(iii) if $\\mathcal{P}$ is an $\\varepsilon$-dense set, for all $m_{i}$\n\n$$\n\\begin{aligned}\n& \\frac{2}{3} c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} m_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2}\\right)-128\\left(\\sigma_{t} / c_{t}\\right)^{2}\\left(d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)^{-1}+4 d C_{\\log }+\\log \\delta^{-1}\\right) \\\\\n& \\leq\\left\\|X(0)-m_{i}\\right\\|^{2} \\leq \\\\\n& 9 \\varepsilon^{2}+2 c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} m_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2}\\right)+128\\left(\\sigma_{t} / c_{t}\\right)^{2}\\left(d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)^{-1}+4 d C_{\\log }+\\log \\delta^{-1}\\right)\n\\end{aligned}\n$$\n\nProof. (i) The result follows from inequality (34) that expresses $\\|X(0)-y\\|^{2}$ in terms of $\\|X(t)-y\\|^{2}$; more precisely it states that for $\\delta<1$ with probability $1-\\delta$ for all $y \\in M$\n\n$$\n\\frac{1}{2} c_{t}^{2}\\|X(0)-y\\|^{2}-4 \\sigma_{t}^{2} C_{0} \\leq\\left\\|X(t)-c_{t} y\\right\\|^{2}-\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2} \\leq \\frac{3}{2} c_{t}^{2}\\|X(0)-y\\|^{2}+4 \\sigma_{t}^{2} C_{0}\n$$\n\nwhere $C_{0}=\\left(4 d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+(12 d+2) C_{\\log }+2 \\log \\delta^{-1}\\right) \\leq 4\\left(d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 d C_{\\log }+\\log \\delta^{-1}\\right)$.\nOn the one hand by choice of $m_{\\min }(t)$ and $m_{\\min }(0)$ and the second part of (34)\n\n$$\n\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2} \\leq\\left\\|X(t)-c_{t} m_{\\min }(0)\\right\\|^{2} \\leq \\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}+4 C_{0} \\sigma_{t}^{2}+3 c_{t}^{2}\\left\\|X(0)-m_{\\min }(0)\\right\\|^{2} / 2\n$$\n\nOn the other hand, applying the first part of the same inequality (34)\n\n$$\n\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2} \\geq \\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}-4 C_{0} \\sigma_{t}^{2}+c_{t}^{2}\\left\\|X(0)-m_{\\min }(t)\\right\\|^{2} / 2\n$$\n\nCombining, we prove the first part of the statement\n\n$$\n\\begin{aligned}\n& c_{t}^{2}\\|X(0)-m_{\\min }(t)\\|^{2} \\leq 16 C_{0} \\sigma_{t}^{2}+3 c_{t}^{2}\\left\\|X(0)-m_{\\min }(0)\\right\\|^{2} \\\\\n& \\quad \\leq 3 c_{t}^{2}\\left\\|X(0)-m_{\\min }(0)\\right\\|^{2}+64 \\sigma_{t}^{2}\\left(d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+4 d C_{\\log }+\\log \\delta^{-1}\\right)\n\\end{aligned}\n$$\n\n(ii) To prove the second part we first express both distances to $X(t)$ through distances to $X(0)$ using (34) and then substitute (41) to get the statement\n\n$$\n\\begin{aligned}\n& \\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2}-\\left\\|X(t)-c_{t} m_{i}\\right\\|^{2} \\\\\n& =\\left(\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2}-\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}\\right)-\\left(\\left\\|X(t)-c_{t} m_{i}\\right\\|^{2}-\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}\\right) \\\\\n& \\leq 8 C_{0} \\sigma_{t}^{2}+\\frac{c_{t}^{2}}{2}\\left(3\\left\\|X(0)-m_{\\min }(t)\\right\\|^{2}-\\left\\|X(0)-m_{i}\\right\\|^{2}\\right) \\\\\n& \\leq 32 C_{0} \\sigma_{t}^{2}+\\frac{c_{t}^{2}}{2}\\left(9\\left\\|X(0)-m_{\\min }(0)\\right\\|^{2}-\\left\\|X(0)-m_{i}\\right\\|^{2}\\right)\n\\end{aligned}\n$$\n\n(iii) Since $\\mathcal{P}$ is a $\\varepsilon$-dense set $\\left\\|X(0)-m_{\\min }(0)\\right\\|=\\min _{m_{i} \\in \\mathcal{P}}\\left\\|X(0)-m_{i}\\right\\|^{2} \\leq \\varepsilon^{2}$ rearranging terms in (42) we get RHS\n\n$$\n\\left\\|X(0)-m_{i}\\right\\|^{2} \\leq 9 \\varepsilon^{2}+2 c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} m_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2}\\right)+32\\left(\\sigma_{t} / c_{t}\\right)^{2} C_{0}^{\\prime}\n$$\n\nTo get LHS we need to bound $\\left\\|X(0)-m_{i}\\right\\|^{2}$ from below. Applying (34)\n\n$$\n\\left\\|X(t)-c_{t} m_{i}\\right\\|^{2} \\leq \\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}+4 C_{0} \\sigma_{t}^{2}+3 c_{t}^{2}\\left\\|X(0)-m_{i}\\right\\|^{2} / 2\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2} & \\geq \\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}-4 C_{0} \\sigma_{t}^{2}+c_{t}^{2}\\left\\|X(0)-c_{t} m_{\\min }(t)\\right\\|^{2} / 2 \\\\\n& \\geq \\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}-4 C_{0} \\sigma_{t}^{2}+c_{t}^{2}\\left\\|X(0)-c_{t} m_{\\min }(0)\\right\\|^{2} / 2\n\\end{aligned}\n$$\n\nSubtracting we get\n\n$$\n\\left\\|X(t)-c_{t} m_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2} \\leq 8 C_{0} \\sigma_{t}^{2}+\\frac{c_{t}^{2}}{2}\\left(3\\left\\|X(0)-m_{i}\\right\\|^{2}-\\left\\|X(0)-m_{\\min }(0)\\right\\|^{2}\\right)\n$$\n\nrearranging terms and noting that $\\left\\|X(0)-m_{\\min }(0)\\right\\| \\geq 0$\n\n$$\n\\frac{2}{3} c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} m_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} m_{\\min }(t)\\right\\|^{2}\\right)-16 C_{0}\\left(\\sigma_{t} / c_{t}\\right)^{2} \\leq\\left\\|X(0)-m_{i}\\right\\|^{2}\n$$\n\nwe get the LHS.", "tables": {}, "images": {}}, {"section_id": 22, "text": "# Appendix C. Manifold Approximation\n## C. 1 Relation Between Wasserstein and KL distances.\n\nRecall that the Kullback-Leibler divergence between two distributions $P$ and $Q$ on $\\mathbb{R}^{D}$ with densities $p$ and $q$ respectively is defined as\n\n$$\nD_{K L}(P \\| Q)=\\int_{\\mathbb{R}^{D}} \\log \\frac{p(x)}{q(x)} p(x) d x\n$$\n\nWe start with the following auxiliary statement,\nProposition 30. De Bruijn identity. In the same notation as in Proposition 22, if $P_{t}$ and $Q_{t}$ are the laws of $X(t)$ and $Y(t)$\n\n$$\n\\frac{d}{d t} D_{K L}\\left(P_{t} \\| Q_{t}\\right)=-2 \\mathbb{E}\\|\\nabla \\log p(t, X(t))-\\nabla \\log q(t, X(t))\\|^{2}\n$$\n\nProof. See (Emamirad and Rougirel, 2023, Theorem 7.1).\nProposition 22. Let $P, Q$ be arbitrary compactly supported measures s.t. $W_{2}(P, Q)<\\infty$, where $W_{2}$ is the quadratic Wasserstein distance.\n\nLetting $X \\sim P, Y \\sim Q$, we write $X(t)=c_{t} X+\\sigma_{t} Z_{D}$, and $Y(t)=c_{t} Y+\\sigma_{t} Z_{D}$, where $Z_{D} \\sim \\mathcal{N}(0, I d)$ be two standard Ornstein-Uhlenbeck processes initialised from $X, Y$\n\nrespectively. These random variables are absolutely continuous w.r.t. the Lebesgue measure with densities $p(t, x), q(t, x)$ respectively.\n\nIf we consider $\\nabla \\log q(t, x)$ as an estimator of $s(t, x)=\\nabla \\log p(t, x)$ on the interval $t \\in\\left[t_{\\min }, t_{\\max }\\right]$, then the corresponding score matching loss is bounded by\n\n$$\n\\int_{t_{\\min }}^{t_{\\max }} \\int_{\\mathbb{R}^{d}}\\|\\nabla \\log p(t, x)-\\nabla \\log q(t, x)\\|^{2} p(t, x) d x d t \\leq W_{2}^{2}(P, Q) \\frac{c_{t_{\\min }}^{2}}{4 \\sigma_{t_{\\min }}^{2}}\n$$\n\nProof. Since $W_{2}(P, Q) \\leq \\varepsilon$, there exists a coupling $\\Gamma$ between $P$ and $Q$ such that\n\n$$\n\\int|v-w|^{2} \\Gamma(d v, d w) \\leq \\varepsilon^{2}\n$$\n\nLetting $P_{t}$ and $Q_{t}$ denote the laws of $X(t)$ and $Y(t)$ respectively, by Proposition 30\n\n$$\n\\frac{d}{d t} D_{K L}\\left(P_{t} \\| Q_{t}\\right)=-2 \\mathbb{E}\\|\\nabla \\log p(t, X(t))-\\nabla \\log q(t, X(t))\\|^{2}\n$$\n\nIntegrating over $t$ we rewrite\n\n$$\n\\begin{aligned}\n\\int_{t_{\\min }}^{t_{\\max }} \\int_{\\mathbb{R}^{D}}\\|\\nabla \\log p(s, x)-\\nabla \\log q(s, x)\\|^{2} p_{s}(x) d x d s & =\\frac{1}{2}\\left(D_{K L}\\left(P_{t_{\\min }} \\| Q_{t_{\\min }}\\right)-D_{K L}\\left(P_{t_{\\max }} \\| Q_{t_{\\max }}\\right)\\right) \\\\\n& \\leq \\frac{1}{2} D_{K L}\\left(P_{t_{\\min }} \\| Q_{t_{\\min }}\\right)\n\\end{aligned}\n$$\n\nIt remains to show that\n\n$$\n\\begin{aligned}\nD_{K L}\\left(P_{t} \\| Q_{t}\\right) & :=C_{\\sigma_{t}} \\int \\log \\frac{\\int e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}^{2}} P(d v)}{\\int e^{-|x-c_{t} w|^{2} / 2 \\sigma_{t}^{2}} Q(d w)} \\int e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}^{2}} P(d v) d x \\\\\n& \\leq W_{2}^{2}\\left(P_{t}, Q_{t}\\right) \\frac{c_{t}^{2}}{2 \\sigma_{t}^{2}}\n\\end{aligned}\n$$\n\nUsing the fact that $\\Gamma$ has marginals $P, Q$ we rewrite\n\n$$\nD_{K L}\\left(P_{t} \\| Q_{t}\\right)=C_{\\sigma_{t}} \\int \\log \\frac{\\iint e^{-|x-c_{t} w|^{2} / 2 \\sigma_{t}^{2}} \\Gamma(d v, d w)}{\\iint e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}^{2}} \\Gamma(d v, d w)} \\iint e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}^{2}} \\Gamma(d v, d w) d x\n$$\n\nNext, using the log-sum inequality for a fixed $x$ we get\n\n$$\n\\begin{aligned}\n& \\log \\frac{\\iint e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}^{2}} \\Gamma(d v, d w)}{\\iint e^{-|x-c_{t} w|^{2} / 2 \\sigma_{t}^{2}} \\Gamma(d v, d w)} \\iint e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}^{2}} \\Gamma(d v, d w) \\\\\n& \\leq \\iint e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}^{2}} \\log \\frac{e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}^{2}}}{e^{-|x-c_{t} w|^{2} / 2 \\sigma_{t}^{2}}} \\Gamma(d v, d w) \\\\\n& \\leq \\iint \\frac{|x-c_{t} w|^{2}-|x-c_{t} v|^{2}}{2 \\sigma_{t}^{2}} e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}} \\Gamma(d v, d w) \\\\\n& =\\iint \\frac{2 c_{t}\\langle v-w, x\\rangle+c_{t}^{2}\\left(|v|^{2}-|w|^{2}\\right)}{2 \\sigma_{t}^{2}} e^{-|x-c_{t} v|^{2} / 2 \\sigma_{t}} \\Gamma(d v, d w)\n\\end{aligned}\n$$\n\nIntegrating over $x$ we get\n\n$$\n\\begin{aligned}\nD_{K L}\\left(P_{t} \\| Q_{t}\\right) \\leq & \\frac{1}{2 \\sigma_{t}^{2}} C_{\\sigma_{t}} \\iint \\int\\left(2 c_{t}\\langle v-w, x\\rangle+c_{t}^{2}\\left(|v|^{2}-|w|^{2}\\right)\\right) e^{-\\left|x-c_{t} v\\right|^{2} / 2 \\sigma_{t}^{2}} \\Gamma(d v, d w) d x \\\\\n= & \\frac{1}{2 \\sigma_{t}^{2}} \\iint 2 c_{t}\\left\\langle\\left(C_{\\sigma_{t}} \\int\\left(x e^{-\\left|x-c_{t} v\\right|^{2} / 2 \\sigma_{t}^{2}} d x\\right), v-w\\right\\rangle \\Gamma(d v, d w)\\right. \\\\\n& \\left.\\quad+\\frac{1}{2 \\sigma_{t}^{2}} \\iint\\left(C_{\\sigma_{t}} \\int e^{-\\left|x-c_{t} v\\right|^{2} / 2 \\sigma_{t}^{2}} d x\\right) c_{t}^{2}\\left(|w|^{2}-|v|^{2}\\right) \\Gamma(d v, d w)\\right)\n\\end{aligned}\n$$\n\nSince $C_{\\sigma_{t}} e^{-\\left|x-c_{t} v\\right|^{2} / 2 \\sigma_{t}^{2}}$ is the density of $\\mathcal{N}\\left(c_{t} v, \\sigma_{t}^{2}\\right)$ we get\n\n$$\n\\begin{aligned}\nD_{K L}\\left(P_{t} \\| Q_{t}\\right) & \\leq \\frac{1}{2 \\sigma_{t}^{2}} \\iint 2 c_{t}^{2}\\langle v, v-w\\rangle \\Gamma(d v, d w)+\\frac{1}{2 \\sigma_{t}^{2}} \\iint c_{t}^{2}\\left(|w|^{2}-|v|^{2}\\right) \\Gamma(d v, d w) \\\\\n& =\\frac{c_{t}^{2}}{2 \\sigma_{t}^{2}} \\iint|v-w|^{2} \\Gamma(d v, d w) \\leq W_{2}^{2}(P, Q) \\frac{c_{t}^{2}}{2 \\sigma_{t}^{2}}\n\\end{aligned}\n$$\n\nwhere the last equality follows from the definition of $\\Gamma$.", "tables": {}, "images": {}}, {"section_id": 23, "text": "# C. 2 Properties of Support Estimator \n\nThis section contains the proofs of the results in Section 5. Recall that we consider $\\mathcal{G}=$ $\\left\\{y_{1}, \\cdots, y_{N}\\right\\}$ for $N$ defined in Section 3.1 and for all $y_{i} \\in \\mathcal{G}$ we consider $P_{i}^{*}, \\Phi_{i}^{*}$ defined by (27) and (28) respectively.\n\nProposition 18. With probability $1-N^{-\\beta / d}$ for all $i \\leq N$, the maps $P_{i}$ are well-defined linear isometric embeddings satisfying $\\left\\|P_{i}^{*}-P_{i}\\right\\|_{o p} \\lesssim \\varepsilon_{N}^{\\beta-1}$.\nProof. To simplify notation, we fix index $i$ and drop it for the rest of proof. Recall that $P=\\operatorname{pr} P^{*}\\left(\\left(P^{*}\\right)^{T} \\operatorname{pr} P^{*}\\right)^{-1 / 2}$.\n\nFirst, we show that this operator is well-defined, it is enough to show that $\\left(P^{*}\\right)^{T} \\operatorname{pr} P^{*}$ is symmetric and positive-definite. Since $\\mathrm{pr}^{*}$ is a projection on $\\operatorname{Im} P^{*}$ the image of embedding $P^{*}$ we have $\\mathrm{pr}^{*} P^{*}=P^{*}$ and $\\left(P^{*}\\right)^{T} P^{*}=\\mathrm{Id}_{d}$, so\n\n$$\n\\left(P^{*}\\right)^{T} \\operatorname{pr} P^{*}=\\operatorname{Id}_{d}+\\left(P^{*}\\right)^{T}\\left(\\operatorname{pr}-\\operatorname{pr}^{*}\\right) P^{*}\n$$\n\nnote that $\\left(P^{*}\\right)^{T}\\left(\\operatorname{pr}-\\operatorname{pr}^{*}\\right) P^{*}$ is symmetric and $\\left\\|\\left(P^{*}\\right)^{T}\\left(\\operatorname{pr}-\\operatorname{pr}^{*}\\right) P^{*}\\right\\| \\lesssim \\varepsilon_{N}^{\\beta}<1 / 2$ giving us that $P^{*}$ is well defined.\n\nMoreover, applying $(1+x)^{-1 / 2}=1-x / 2+O\\left(x^{2}\\right)$ we bound\n\n$$\n\\left\\|\\operatorname{Id}_{d}-\\left(\\left(P^{*}\\right)^{T} \\operatorname{pr} P^{*}\\right)^{-1 / 2}\\right\\|=\\left\\|\\operatorname{Id}_{d}-\\left(\\operatorname{Id}_{d}+\\left(P^{*}\\right)^{T}\\left(\\operatorname{pr}-\\operatorname{pr}^{*}\\right) P^{*}\\right)^{-1 / 2}\\right\\| \\leq\\left\\|\\left(P^{*}\\right)^{T}\\left(\\operatorname{pr}-\\operatorname{pr}^{*}\\right) P^{*}\\right\\|_{o p} \\lesssim \\varepsilon_{N}^{\\beta-1}\n$$\n\nWe are now ready to bound $\\left\\|P_{i}^{*}-P_{i}\\right\\|$. Noting that $\\mathrm{pr}^{*} P^{*}=P^{*}, \\mathrm{pr}^{2}=\\mathrm{pr}=\\mathrm{pr}^{T}$, $\\left(\\mathrm{pr}^{*}\\right)^{2}=\\mathrm{pr}^{*}=\\left(\\mathrm{pr}^{*}\\right)^{T}$, and $\\left\\|P^{*}\\right\\|_{o p}=\\left\\|\\mathrm{pr}^{*}\\right\\|_{o p}=\\|\\mathrm{pr}\\|_{o p}=1$\n\n$$\n\\begin{aligned}\n\\left\\|P^{*}-P\\right\\|_{o p} & =\\left\\|\\operatorname{pr}^{*} P^{*}-\\operatorname{pr} P^{*}\\left(\\left(P^{*}\\right)^{T} \\operatorname{pr} P^{*}\\right)^{-1 / 2}\\right\\|_{o p} \\\\\n& \\leq\\left\\|\\left(\\operatorname{pr}-\\operatorname{pr}^{*}\\right) P^{*}\\right\\|_{o p}+\\left\\|\\operatorname{pr} P^{*}\\left(\\operatorname{Id}_{d}-\\left(\\left(P^{*}\\right)^{T} \\operatorname{pr} P^{*}\\right)^{-1 / 2}\\right)\\right\\|_{o p} \\\\\n& \\leq\\left\\|\\operatorname{pr}-\\operatorname{pr}^{*}\\right\\|_{o p}\\left\\|P^{*}\\right\\|_{o p}+\\left\\|\\operatorname{pr} P^{*}\\right\\|_{o p}\\left\\|\\left(\\operatorname{Id}_{d}-\\left(\\left(P^{*}\\right)^{T} \\operatorname{pr} P^{*}\\right)^{-1 / 2}\\right)\\right\\|_{o p} \\lesssim \\varepsilon_{N}^{\\beta-1}\n\\end{aligned}\n$$\n\nFinally, the last statement to prove is that $P_{i}: \\mathbb{R}^{d} \\rightarrow T_{G_{i}} M-G_{i}$ is an isometric embedding. By construction $\\operatorname{Im} P_{i} \\subseteq \\operatorname{Im} \\operatorname{pr}=T_{G_{i}} M-G_{i}$, and for any $v, u \\in \\mathbb{R}^{d}$\n\n$$\n\\begin{aligned}\n& \\left\\langle P_{i} v, P_{i} u\\right\\rangle=\\left(\\operatorname{pr}_{i} P_{i}^{*}\\left(\\left(P_{i}^{*}\\right)^{T} \\operatorname{pr}_{i} P_{i}^{*}\\right)^{-1 / 2} v\\right)^{T}\\left(\\operatorname{pr}_{i} P_{i}^{*}\\left(\\left(P_{i}^{*}\\right)^{T} \\operatorname{pr}_{i} P_{i}^{*}\\right)^{-1 / 2} u\\right)= \\\\\n& \\quad=v^{T}\\left(\\left(P_{i}^{*}\\right)^{T} \\operatorname{pr}_{i} P_{i}^{*}\\right)^{-1 / 2}\\left(P_{i}^{*}\\right)^{T} \\operatorname{pr}_{i} P_{i}^{*}\\left(\\left(P_{i}^{*}\\right)^{T} \\operatorname{pr}_{i} P_{i}^{*}\\right)^{-1 / 2} u=v^{T} u\n\\end{aligned}\n$$\n\nwhere we again used that $\\operatorname{pr}_{i} \\operatorname{pr}_{i}=\\operatorname{pr}_{i}$.\nLemma 19. There is a large enough constant $L^{*}$, that does not depend on $D$, such that for any $\\varepsilon_{N}<r_{0} / 4$ with probability $1-N^{-\\frac{\\beta}{2}}$ for all $y_{i}$, all $0 \\leq k<\\beta$ and $z \\in B_{d}\\left(0,8 \\varepsilon_{N}\\right)$\n\n$$\n\\left\\|\\nabla^{k} \\Phi_{i}(z)-\\nabla^{k} \\Phi_{i}^{*}(z)\\right\\|_{o p} \\leq L^{*} \\varepsilon_{N}^{\\beta-k}\n$$\n\nIn particular, $\\left\\|y-y^{*}\\right\\| \\lesssim \\varepsilon_{N}^{\\beta}$ and $\\angle\\left(T_{y^{*}} M^{*}, T_{y} M\\right) \\lesssim \\varepsilon_{N}^{\\beta-1}$ for $y=\\Phi_{i}(z) \\in M, y^{*}=\\Phi_{i}^{*}(z) \\in M^{*}$.\nProof. We fix the index $i$ and drop it to simplify notation. Let $\\bar{\\Phi}: \\operatorname{Im} P \\mapsto M, \\bar{\\Phi}^{*}: \\operatorname{Im} P^{*} \\mapsto$ $M^{*}$ be defined as $\\bar{\\Phi}(P z)=\\Phi(z), \\bar{\\Phi}^{*}\\left(P^{*} z\\right)=\\Phi^{*}(z)$. Introducing $V_{j}=d^{j} \\bar{\\Phi}(0), V_{j}^{*}=d^{j} \\bar{\\Phi}^{*}(0)$ where $2 \\leq j<\\beta-1$ we represent\n\n$$\n\\begin{array}{ll}\n\\bar{\\Phi}(v)=y_{i}+v+\\sum V_{j}\\left(v^{\\otimes k}\\right)+R(v), & v \\in \\operatorname{Im} P \\\\\n\\bar{\\Phi}^{*}(v)=y_{i}+v+\\sum V_{j}^{*}\\left(v^{\\otimes k}\\right), & v \\in \\operatorname{Im} P^{*}\n\\end{array}\n$$\n\nwhere $R(v)$ is the residual of order $\\beta$. We first control $\\nabla^{k}\\left(\\Phi \\circ P^{T} P^{*}\\right)(z)-\\nabla^{k} \\Phi^{*}(z)$ and then $\\nabla^{k}\\left(\\Phi \\circ P^{T} P^{*}\\right)(z)-\\nabla^{k} \\Phi(z)$.\n\nSubstituting $P^{*} z$ and $P P^{T} P^{*} z=\\operatorname{pr} P^{*} z$\n\n$$\n\\begin{aligned}\n& \\bar{\\Phi}\\left(P P^{T} P^{*} z\\right)=y_{i}+P P^{T} P^{*} z+\\sum V_{j}\\left(\\left(P P^{T} P^{*} z\\right)^{\\otimes k}\\right)+R\\left(P P^{T} P^{*} z\\right) \\\\\n& \\bar{\\Phi}^{*}\\left(P^{*} z\\right)=y_{i}+P^{*} z+\\sum V_{j}^{*}\\left(\\left(P^{*} z\\right)^{\\otimes k}\\right)\n\\end{aligned}\n$$\n\n(Divol, 2022, Lemma A. 2 (iii)) states that $\\left\\|V_{j}^{*} \\circ \\operatorname{pr}^{*}-V_{j} \\circ \\operatorname{pr}\\right\\|_{o p} \\lesssim \\varepsilon_{N}^{\\beta-j}$ for $j \\geq 2$, note that taking $V_{1}=V_{1}^{*}=\\operatorname{Id}$ this also holds for $j=1$. So for all $\\|z\\| \\lesssim \\varepsilon_{N}$,\n\n$$\n\\begin{aligned}\n\\left\\|\\Phi^{*}(z)\\right. & \\left.-\\Phi\\left(P^{T} P^{*} z\\right)\\right\\|=\\left\\|\\bar{\\Phi}^{*}\\left(P^{*} z\\right)-\\bar{\\Phi}\\left(P P^{T} P^{*} z\\right)\\right\\| \\\\\n& \\lesssim\\left\\|\\operatorname{pr}-\\operatorname{pr}^{*}\\right\\|_{o p}\\left\\|P^{*} z\\right\\|+\\sum_{j}\\left\\|V_{j}^{*} \\circ \\operatorname{pr}^{*}-V_{j} \\circ \\operatorname{pr}\\right\\|\\left\\|P^{*} z\\right\\|^{k}+\\left\\|R\\left(P P^{T} P^{*} z\\right)\\right\\| \\lesssim \\varepsilon_{N}^{\\beta}\n\\end{aligned}\n$$\n\nsince $\\operatorname{pr}^{*} P^{*} z=P^{*} z,\\left\\|z^{*}\\right\\|=\\left\\|P^{*} z\\right\\| \\lesssim \\varepsilon_{N}$ and $\\left\\|P P^{T} P^{*}\\right\\|_{o p} \\leq 1$.\nTo prove the result for $k \\geq 1$ we note that if we take $1 \\leq k<\\beta-1$ derivatives along the directions $v_{1}, \\ldots, v_{k} \\in \\mathbb{R}^{d}$ with $\\left\\|v_{j}\\right\\|=1$, since $\\operatorname{pr}^{*} P^{*} z=P^{*} z$\n\n$$\n\\begin{aligned}\n& d_{v_{1}} \\ldots d_{v_{k}}\\left(\\bar{\\Phi}_{i} \\circ \\operatorname{pr} \\circ P^{*}\\right)(z)=\\sum_{j=k}^{\\beta-1} \\frac{j!}{(j-k)!} V_{j}\\left(\\left(\\operatorname{pr} P^{*} z\\right)^{\\otimes(j-k)} \\otimes\\left(\\operatorname{pr} P^{*} v_{1}\\right) \\otimes \\ldots \\otimes\\left(\\operatorname{pr} P^{*} v_{k}\\right)\\right)+R^{k}(z) \\\\\n& d_{v_{1}} \\ldots d_{v_{k}}\\left(\\bar{\\Phi}_{i}^{*} \\circ P^{*}\\right)(z)=\\sum_{j=k}^{\\beta-1} \\frac{j!}{(j-k)!} V_{j}^{*}\\left(\\left(\\operatorname{pr}^{*} P^{*} z\\right)^{\\otimes(j-k)} \\otimes\\left(\\operatorname{pr}^{*} P^{*} v_{1}\\right) \\otimes \\ldots \\otimes\\left(\\operatorname{pr}^{*} P^{*} v_{k}\\right)\\right)\n\\end{aligned}\n$$\n\nwhere $\\left\\|R^{k}(z)\\right\\|=O\\left(\\varepsilon_{N}^{\\beta-k}\\right)$. Thus\n\n$$\n\\begin{aligned}\n& \\left|d_{v_{1}} \\ldots d_{v_{k}}\\left(\\Phi \\circ P^{T} P^{*}\\right)(z)-d_{v_{1}} \\ldots d_{v_{k}} \\Phi^{*}(z)\\right| \\\\\n& =\\left|d_{v_{1}} \\ldots d_{v_{k}}\\left(\\overline{\\Phi_{i}} \\circ \\operatorname{pr} \\circ P^{*}\\right)(z)-d_{v_{1}} \\ldots d_{v_{k}}\\left(\\overline{\\Phi_{i}} \\circ P^{*}\\right)(z)\\right| \\\\\n& \\leq O\\left(\\varepsilon_{N}^{\\beta-k}\\right)+\\sum_{j=k}^{\\beta-1}\\left\\|V_{j}^{*} \\circ \\operatorname{pr}^{*}-V_{j} \\circ \\operatorname{pr}\\right\\|\\left\\|P^{*} z\\right\\|^{j-k} \\prod\\left\\|P^{*} v_{k}\\right\\| \\\\\n& \\lesssim \\varepsilon_{N}^{\\beta-k}+\\sum_{j=k}^{\\beta-1} \\varepsilon_{N}^{\\beta-j} \\varepsilon_{N}^{j-k} \\lesssim \\varepsilon_{N}^{\\beta-k}\n\\end{aligned}\n$$\n\nThis proves that $\\left\\|\\nabla^{k}\\left(\\Phi \\circ P^{T} P^{*}\\right)(z)-\\nabla^{k} \\Phi^{*}(z)\\right\\| \\lesssim \\varepsilon_{N}^{\\beta-k}$. Therefore, to prove the statement it is enough to show that $\\left\\|\\nabla^{k}\\left(\\Phi \\circ P^{T} P^{*}\\right)(z)-\\nabla^{k} \\Phi(z)\\right\\| \\lesssim \\varepsilon_{N}^{\\beta-k}$ and $\\left\\|\\left(\\Phi \\circ P^{T} P^{*}\\right)(z)-\\Phi(z)\\right\\| \\lesssim$ $\\varepsilon_{N}^{\\beta}$.\n\nDenote by $A=P^{T} P^{*}$, then since $P$ is an isometry\n$\\left\\|A-\\operatorname{Id}_{d}\\right\\|_{o p}=\\left\\|P^{T} P^{*}-\\operatorname{Id}_{d}\\right\\|_{o p}=\\left\\|P P^{T} P^{*}-P\\right\\|_{o p}=\\left\\|\\operatorname{pr}\\left(P^{*}-P\\right)\\right\\|_{o p} \\leq\\left\\|P^{*}-P\\right\\|_{o p} \\lesssim \\varepsilon_{N}^{\\beta-1}$.\nand $\\|A\\|_{o p} \\leq\\left\\|P^{T}\\right\\|\\left\\|P^{*}\\right\\|=1$. We start with the case $k=0$\n\n$$\n\\left\\|\\left(\\Phi \\circ P^{T} P^{*}\\right)(z)-\\Phi(z)\\right\\| \\lesssim L_{M}\\left\\|P^{T} P^{*} z-z\\right\\| \\leq L_{M}\\left\\|P^{T} P^{*}-\\operatorname{Id}_{d}\\right\\|\\|z\\| \\lesssim \\varepsilon_{N}^{\\beta}\n$$\n\nLet $k \\geq 1$. Taking derivatives along the directions $v_{1}, \\ldots, v_{k} \\in \\mathbb{R}^{d}$ with $\\left\\|v_{j}\\right\\|=1$ and then applying the chain rule and chaining argument\n\n$$\n\\begin{aligned}\n& \\left\\|d_{v_{1}} \\ldots d_{v_{k}}(\\Phi \\circ A)(z)-d_{v_{1}} \\ldots d_{v_{k}} \\Phi(z)\\right\\| \\\\\n& =\\left\\|d_{v_{1}} \\ldots d_{v_{k}}\\left(\\Phi_{i} \\circ A\\right)(z)-d_{v_{1}} \\ldots d_{v_{k}} \\Phi(z)\\right\\| \\\\\n& =\\left\\|d^{k} \\Phi(A z)\\left(A v_{1} \\otimes \\ldots \\otimes A v_{k}\\right)-d^{k} \\Phi(z)\\left(v_{1} \\otimes \\ldots \\otimes v_{k}\\right)\\right\\| \\\\\n& \\leq\\left\\|d^{k} \\Phi(A z)\\left(v_{1} \\otimes \\ldots \\otimes v_{k}\\right)-d^{k} \\Phi(z)\\left(v_{1} \\otimes \\ldots \\otimes v_{k}\\right)\\right\\| \\\\\n& \\quad+\\sum_{j=1}^{k}\\left\\|d^{k} \\Phi(A z)\\left(v_{1} \\otimes \\ldots v_{j-1} \\otimes\\left(A v_{j}-v_{j}\\right) \\otimes A v_{j+1} \\ldots \\otimes A v_{k}\\right)\\right\\| \\\\\n& \\lesssim \\sup _{z^{\\prime} \\in[z, A z]}\\left\\|d^{k+1} \\Phi\\left(z^{\\prime}\\right)\\right\\|_{o p}\\|A\\|_{o p}\\left\\|A-\\operatorname{Id}_{d}\\right\\|_{o p}\\|z\\|+\\sum_{j=1}^{k}\\left\\|d^{k} \\Phi(A x)\\right\\|_{o p}\\|A\\|_{o p}^{k-j}\\left\\|A-\\operatorname{Id}_{d}\\right\\|_{o p} \\\\\n& \\leq\\left(k+\\varepsilon_{N}\\right) L_{M}\\left(\\left\\|A-\\operatorname{Id}_{d}\\right\\|_{o p}\\right) \\lesssim \\varepsilon_{N}^{\\beta-1}\n\\end{aligned}\n$$\n\nby the mean value theorem. So $\\left\\|\\nabla^{k}\\left(\\Phi \\circ P^{T} P^{*}\\right)(z)-\\nabla^{k} \\Phi(z)\\right\\| \\lesssim \\varepsilon_{N}^{\\beta-1} \\leq \\varepsilon_{N}^{\\beta-k}$.\nProposition 20. If $\\operatorname{dim} \\operatorname{span} \\mathcal{V}_{i}>d$, then there is a solution $P_{i}^{*}, a_{i, S}^{*}$ of (27) such that $\\operatorname{Im} P_{i}^{*} \\subset \\operatorname{span} \\mathcal{V}_{i}$ and $a_{i, S}^{*} \\in \\operatorname{Ker} P_{i}^{*} \\cap \\operatorname{span} \\mathcal{V}_{i}$.\n\nProof. For simplicity, we drop the index $i$ and the star notation and consider the minimization problem\n\n$$\nP,\\left\\{a_{S}\\right\\}_{2 \\leq|S|<\\beta} \\in \\underset{P, a_{S} \\leq \\varepsilon_{N}^{-1}}{\\arg \\min } \\sum_{v_{j} \\in \\mathcal{V}}\\left\\|v_{j}-P P^{T} v_{j}-\\sum_{S} a_{S}\\left(P^{T} v_{j}\\right)^{S}\\right\\|^{2}\n$$\n\nwhere $\\arg \\min$ is taken over all linear isometric embeddings $P: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{D}$ and all vectors $a_{S} \\in \\mathbb{R}^{D}$ that satisfy $\\left\\|a_{S}\\right\\| \\leq \\varepsilon_{N}^{-1}$.\n\nAs a preliminary step, let us show that a solution always satisfies $a_{S} \\perp \\operatorname{Im} P$. Note that $P P^{T}$ is a projection on $\\operatorname{Im} P$, so $v-P^{T} P v \\perp \\operatorname{Im} P$, and therefore\n\n$$\n\\begin{aligned}\n\\left\\|v_{j}-P P^{T} v_{j}\\right. & \\left.-\\sum_{S} a_{S}\\left(P^{T} v_{j}\\right)^{S}\\right\\|^{2} \\\\\n& =\\left\\|\\left(\\operatorname{Id}_{D}-P P^{T}\\right) v_{j}-\\sum_{S}\\left(\\operatorname{Id}_{D}-P P^{T}\\right) a_{S}\\left(P^{T} v_{j}\\right)^{S}\\right\\|^{2}+\\left\\|P P^{T} a_{S}\\left(P^{T} v_{j}\\right)^{S}\\right\\|^{2}\n\\end{aligned}\n$$\n\nso the $\\arg \\min$ should satisfy $P^{T} P a_{S}=0$, or equivalently $a_{S} \\perp \\operatorname{Im} P$.\nNote that $P^{T} P$ is a projection on $\\operatorname{Im} P$. First, we prove that there is $P$ satisfying $\\operatorname{Im} P \\subseteq \\operatorname{span} \\mathcal{V}$. Assume the opposite and take $P$ such that $\\operatorname{dim}(\\operatorname{Im} P \\cap \\operatorname{span} \\mathcal{V})$ is the largest. Then, by the assumption $\\operatorname{dim}(\\operatorname{Im} P \\cap \\operatorname{span} \\mathcal{V})<\\operatorname{dim} \\operatorname{Im} P$, since $\\operatorname{dim} \\operatorname{span} \\mathcal{V} \\geq d=\\operatorname{dim} \\operatorname{Im} P$ there is a vector $u_{P} \\in \\operatorname{Im} P$ such that $u_{P} \\perp \\operatorname{span} \\mathcal{V},\\left\\|u_{P}\\right\\|=1$ and similarly a vector $u_{\\mathcal{V}} \\in \\operatorname{span} \\mathcal{V}$ such that $u_{\\mathcal{V}} \\perp \\operatorname{Im} P,\\left\\|u_{\\mathcal{V}}\\right\\|=1$.\n\nLet $e_{1}, \\ldots, e_{d} \\in \\mathbb{R}^{D}$ be column vectors in $P$, then $\\left\\langle e_{i}, e_{j}\\right\\rangle=\\delta_{i j}$ and\n\n$$\nP^{T} v=\\left(\\left\\langle v, e_{1}\\right\\rangle, \\ldots,\\left\\langle v, e_{d}\\right\\rangle\\right)\n$$\n\nWe introduce vectors $f_{i}=e_{i}+\\left(u_{\\mathcal{V}}-u_{P}\\right)\\left\\langle u_{P}, e_{i}\\right\\rangle$ and note that $u_{\\mathcal{V}} \\perp e_{i}$ for all $i$, so\n\n$$\n\\left\\langle f_{i}, f_{j}\\right\\rangle=\\left\\langle e_{i}+\\left(u_{\\mathcal{V}}-u_{P}\\right)\\left\\langle u_{P}, e_{i}\\right\\rangle, e_{j}+\\left(u_{\\mathcal{V}}-u_{P}\\right)\\left\\langle u_{P}, e_{j}\\right\\rangle\\right\\rangle=\\delta_{i j}\n$$\n\nWe introduce a new operator\n\n$$\nQ=\\left(f_{1}, \\ldots, f_{d}\\right)^{T}\n$$\n\nand note that by construction $u_{P} \\perp \\operatorname{Im} Q$ and $u_{\\mathcal{V}} \\in \\operatorname{Im} Q$, since\n\n$$\n\\sum_{i}\\left\\langle e_{i}, u_{P}\\right\\rangle f_{i}=\\left\\langle e_{i}, u_{P}\\right\\rangle\\left(e_{i}+\\left(u_{\\mathcal{V}}-u_{P}\\right)\\left\\langle u_{P}, e_{i}\\right\\rangle\\right)=\\underbrace{\\left(\\sum_{i}\\left\\langle e_{i}, u_{P}\\right\\rangle^{2}\\right)}_{=\\left\\|u_{P}\\right\\|=1} u_{\\mathcal{V}}\n$$\n\nso\n\n$$\n\\operatorname{dim}(\\operatorname{Im} Q \\cap \\operatorname{span} \\mathcal{V})=\\operatorname{dim}(\\operatorname{Im} P \\cap \\operatorname{span} \\mathcal{V})+1\n$$\n\nThen for any $v_{i} \\in \\mathcal{V}$, since $u_{P} \\perp \\operatorname{span} \\mathcal{V}$\n\n$$\n\\left\\langle e_{j}, v_{i}\\right\\rangle=\\left\\langle f_{j}, v_{i}\\right\\rangle-\\left\\langle u_{P}, e_{j}\\right\\rangle\\left\\langle u_{\\mathcal{V}}, v_{i}\\right\\rangle=\\left\\langle f_{j}, v_{i}\\right\\rangle-\\sum_{k=1}^{d}\\left\\langle f_{k}, v_{i}\\right\\rangle\\left\\langle e_{k}, u_{P}\\right\\rangle\\left\\langle u_{P}, e_{j}\\right\\rangle\n$$\n\nso expanding the brackets we represent\n\n$$\n\\begin{aligned}\n\\sum_{2 \\leq|S|<\\beta} a_{S}\\left(P^{T} v_{i}\\right)^{S} & =\\sum_{|S|<\\beta} a_{S}\\left(\\left\\langle e_{1}, v_{i}\\right\\rangle, \\ldots,\\left\\langle e_{d}, v_{i}\\right\\rangle\\right)^{S} \\\\\n& =\\sum_{|S|<\\beta} b_{S}\\left(\\left\\langle f_{1}, v_{i}\\right\\rangle, \\ldots,\\left\\langle f_{d}, v_{i}\\right\\rangle\\right)^{S}=\\sum_{2 \\leq|S|<\\beta} b_{S}\\left(Q^{T} v_{i}\\right)^{S}\n\\end{aligned}\n$$\n\nfor some vector coefficients $b_{S} \\in \\mathbb{R}^{D}$.\nWe are finally ready to present an estimator that has a smaller or equal error w.r.t. $l_{\\mathcal{V}}$,\n\n$$\n\\Phi^{\\prime}(z)=Q z+\\sum_{2 \\leq|S|<\\beta} \\underbrace{\\left(b_{S}-\\left\\langle b_{S}, u_{P}\\right\\rangle u_{P}-\\left\\langle b_{S}, u_{\\mathcal{V}}\\right\\rangle u_{\\mathcal{V}}\\right)}_{=c_{S}} z^{S}\n$$\n\nand the reconstruction error is equal to\n\n$$\n\\frac{1}{|\\mathcal{V}|} \\sum_{v_{i} \\in \\mathcal{V}}\\left\\|v_{i}-Q Q^{T} v_{i}-\\sum c_{S}\\left(Q^{T} v_{i}\\right)^{S}\\right\\|^{2}\n$$\n\nWe decompose $\\mathbb{R}^{D}$ into two subspaces $L_{1}=\\operatorname{span}\\left\\{u_{\\mathcal{V}}, u_{P}\\right\\}$ and $L_{2}=L_{1}^{\\perp}$ - the orthogonal complement of $L_{1}$, and denote as $\\mathrm{pr}_{1}, \\mathrm{pr}_{2}$ corresponding projections. By construction for $v_{i} \\in \\mathcal{V}$, since $c_{S} \\perp u_{P}, u_{\\mathcal{V}}$ and $v_{i} \\perp u_{P}$, and $Q^{T} Q$ is a projector on $\\operatorname{Im} Q \\ni u_{\\mathcal{V}}$\n\n$$\n\\operatorname{pr}_{1}\\left(v_{i}-Q Q^{T} v_{i}-\\sum c_{S}\\left(Q^{T} v_{i}\\right)^{S}\\right)=\\operatorname{pr}_{1}\\left(v_{i}-Q^{T} v_{i}\\right)=\\left\\langle v_{i}-Q Q^{T} v_{i}, u_{\\mathcal{V}}\\right\\rangle u_{\\mathcal{V}}=0\n$$\n\nAt the same time since by construction $c_{S}=\\operatorname{pr}_{2} b_{S}$, applying (45)\n\n$$\n\\operatorname{pr}_{2}\\left(v_{i}-Q Q^{T} v_{i}-\\sum c_{S}\\left(Q^{T} v_{i}\\right)^{S}\\right)=\\operatorname{pr}_{2}\\left(v_{i}-P P^{T} v_{i}-\\sum a_{S}\\left(P^{T} v_{i}\\right)^{S}\\right)\n$$\n\nwhere we additionally used that $\\operatorname{pr}_{2} Q Q^{T} v_{i}=\\operatorname{pr}_{2} P P^{T} v_{i}$ following from\n\n$$\n\\begin{aligned}\n& Q Q^{T} v_{i}=\\sum\\left\\langle f_{j}, v_{i}\\right\\rangle f_{j}=\\sum\\left\\langle e_{j}+\\left(u_{\\mathcal{V}}-u_{P}\\right)\\left\\langle e_{j}, u_{P}\\right\\rangle, v_{i}\\right\\rangle\\left(e_{j}+\\left(u_{\\mathcal{V}}-u_{P}\\right)\\left\\langle e_{j}, u_{P}\\right\\rangle\\right) \\\\\n= & \\sum_{j}\\left\\langle e_{j}, v_{i}\\right\\rangle e_{j}+\\left\\langle e_{j}+\\left(u_{\\mathcal{V}}-u_{P}\\right)\\left\\langle e_{j}, u_{P}\\right\\rangle, v_{i}\\right\\rangle\\left\\langle e_{j}, u_{P}\\right\\rangle\\left(u_{\\mathcal{V}}-u_{P}\\right)+\\left\\langle u_{\\mathcal{V}}-u_{P}, v_{i}\\right\\rangle\\left\\langle e_{j}, u_{P}\\right\\rangle e_{j} \\\\\n& =P^{T} P v_{i}+\\left[\\sum_{j}\\left\\langle e_{j}+\\left(u_{\\mathcal{V}}-u_{P}\\right)\\left\\langle e_{i}, u_{P}\\right\\rangle, v_{i}\\right\\rangle\\left\\langle e_{i}, u_{P}\\right\\rangle\\right]\\left(u_{\\mathcal{V}}-u_{P}\\right)+\\left\\langle u_{\\mathcal{V}}-u_{P}, v_{i}\\right\\rangle u_{P}\n\\end{aligned}\n$$\n\nAnd we finally conclude that for any $v_{i} \\in \\mathcal{V}$\n\n$$\n\\begin{aligned}\n& \\left\\|v_{i}-P P^{T} v_{i}-\\sum a_{S}\\left(P^{T} v_{i}\\right)^{S}\\right\\|^{2} \\\\\n& \\quad=\\left\\|\\operatorname{pr}_{1}\\left(v_{i}-P P^{T} v_{i}-\\sum a_{S}\\left(P^{T} v_{i}\\right)^{S}\\right)\\right\\|^{2}+\\left\\|\\operatorname{pr}_{2}\\left(v_{i}-P P^{T} v_{i}-\\sum a_{S}\\left(P^{T} v_{i}\\right)^{S}\\right)\\right\\|^{2} \\\\\n& \\quad \\geq\\left\\|v_{i}-Q Q^{T} v_{i}-\\sum b_{S}\\left(Q^{T} v_{i}\\right)^{S}\\right\\|^{2}\n\\end{aligned}\n$$\n\nAveraging over all $v_{i} \\in \\mathcal{V}$ and using (44) we get the contradiction, so there is a $P$ satisfying $\\operatorname{Im} P \\subseteq \\operatorname{span} \\mathcal{V}$.\n\nFinally, choosing $\\operatorname{Im} P \\subset \\mathcal{V}$ for all $v \\in \\mathcal{V}$ by construction $v-P P^{T} v \\in \\mathcal{V} \\cap \\operatorname{Ker} P$, so\n\n$$\n\\left\\|v_{j}-P P^{T} v_{j}-\\sum_{S} a_{S}\\left(P^{T} v_{j}\\right)^{S}\\right\\|^{2} \\geq\\left\\|v_{j}-P P^{T} v_{j}-\\sum_{S} \\operatorname{pr}_{3} a_{S}\\left(P^{T} v_{j}\\right)^{S}\\right\\|^{2}\n$$\n\nwhere $\\operatorname{pr}_{3}$ is the projection on $\\operatorname{span} \\mathcal{V} \\cap \\operatorname{Ker} P$ showing that there is a solution satisfying $a_{S} \\in \\operatorname{span} \\mathcal{V} \\cap \\operatorname{Ker} P$ for all $S$.\n\nProposition 21. There is a large enough constant $C_{\\text {dim }}$ that does not depend on $D$ such that with probability $1-N^{-\\frac{\\beta}{d}}$ for all $y \\in M$ the size of the set\n\n$$\n\\mathcal{V}_{y}=\\left\\{G_{i}:\\left\\|G_{i}-y\\right\\| \\leq \\varepsilon_{N}\\right\\}\n$$\n\nis bounded by $\\left|\\mathcal{V}_{y}\\right| \\leq C_{\\operatorname{dim}} \\log N$. In particular for all $G_{i} \\in \\mathcal{G}$ holds $\\left|\\mathcal{V}_{i}\\right| \\leq C_{\\operatorname{dim}} \\log N$.\nProof. Recall that $\\varepsilon_{N}=\\left(C_{d, \\beta} \\frac{p_{\\max }^{2}}{p_{\\min }^{3}} \\frac{\\log N}{N-1}\\right)^{\\frac{1}{d}} \\simeq\\left(\\frac{\\log N}{N}\\right)^{\\frac{1}{d}}$, where a constant $C_{d, \\beta}$ depends only on $d$ and $\\beta$. Let us take $\\overline{\\mathcal{F}}=\\left\\{F_{1}, \\ldots F_{K}\\right\\}-$ a minimal in size $\\varepsilon_{N}$-dense set on $M$. For a point $y \\in M$ there is a point $F_{j}$, such that $\\left\\|y-F_{j}\\right\\| \\leq \\varepsilon_{N}$, so $B_{M}\\left(y, \\varepsilon_{N}\\right) \\subset$ $B_{M}\\left(F_{j}, 2 \\varepsilon_{N}\\right)$ and therefore $\\left|\\mathcal{V}_{y}\\right| \\leq \\sup _{j}\\left|\\mathcal{G} \\cap B_{M}\\left(F_{j}, 2 \\varepsilon_{N}\\right)\\right|$. So, it is enough to bound quantity $\\sup _{j}\\left|\\mathcal{G} \\cap B_{M}\\left(F_{j}, 2 \\varepsilon_{N}\\right)\\right|$.\n\nSince points $G_{i}$ are i.i.d. samples from measure $\\mu$ with density $p_{\\min } \\leq p(y) \\leq p_{\\max }$ the probability of the event $\\mathbb{P}\\left(G_{i} \\in B_{M}\\left(F_{j}, 2 \\varepsilon_{N}\\right)\\right)=\\mu\\left(B_{M}\\left(F_{j}, 2 \\varepsilon_{N}\\right)\\right) \\simeq \\varepsilon_{N}^{d} \\simeq \\frac{\\log N}{N}$. So by Chernoff's multiplicative bound, for a large enough constant $C$\n\n$$\n\\mathbb{P}\\left(\\left|\\mathcal{Y} \\cap B_{M}\\left(F_{j}, 2 \\varepsilon_{N}\\right)\\right| \\geq C u \\log N\\right) \\leq 2 e^{-u^{2} \\log N}\n$$\n\nand taking $u=\\sqrt{\\frac{\\log 2 \\delta^{-1}}{\\log N}}$\n\n$$\n\\mathbb{P}\\left(\\left|\\mathcal{G} \\cap B_{M}\\left(F_{j}, 2 \\varepsilon_{N}\\right)\\right| \\geq C \\sqrt{\\log 2 \\delta^{-1} \\log N}\\right) \\leq \\delta\n$$\n\nTaking $\\delta=N^{-\\beta / d} / K$ and summing over all $F_{j}$ since $K \\leq\\left(\\varepsilon_{N}\\right)^{-d} \\operatorname{Vol} M \\simeq \\frac{N}{\\log N}$ we conclude that there is a constant $C_{\\text {dim }}$ s.t.\n\n$$\n\\mathbb{P}\\left(\\left|\\mathcal{G} \\cap B\\left(F_{j}, 2 \\varepsilon\\right)\\right| \\geq C_{\\operatorname{dim}} \\log N\\right) \\leq N^{-\\frac{\\beta}{d}}\n$$", "tables": {}, "images": {}}, {"section_id": 24, "text": "# C. 3 High-Probability Bounds for the Polynomial Approximation of $M$ \n\nWe start with a simple proposition that bounds the length of the projection $Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ on subspaces $\\mathcal{H}_{i}=\\operatorname{span} \\mathcal{V}_{i}$ constructed in Section 5.1.\n\nProposition 31. With probability at least $1-\\delta$ for all $i \\leq N$ simultaneously\n\n$$\n\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}} Z_{D}\\right\\| \\leq \\sqrt{\\left(C_{\\operatorname{dim}}+2\\right) \\log N+2 \\log 2 \\delta^{-1}}\n$$\n\nwhere $C_{\\text {dim }}$ is given in Proposition 21\n\nProof. First note that $\\operatorname{pr}_{\\mathcal{H}_{i}} Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{\\operatorname{dim} \\mathcal{H}_{i}}\\right)$, so using standard bounds Laurent and Massart (2000) on the tails of the $\\chi$-distribution\n\n$$\n\\mathbb{P}\\left(\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}} Z_{D}\\right\\| \\leq \\sqrt{\\operatorname{dim} \\mathcal{H}_{i}+2 \\log \\left(2 \\delta^{-1} N\\right)}\\right) \\geq 1-\\delta / N\n$$\n\nSince $\\operatorname{dim} \\mathcal{H}_{i} \\leq C_{\\operatorname{dim}} \\log N$, we get\n\n$$\n\\mathbb{P}\\left(\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}} Z_{D}\\right\\| \\leq \\sqrt{C_{\\operatorname{dim}} \\log N+2 \\log \\left(2 \\delta^{-1} N\\right)}\\right) \\geq 1-\\delta / N\n$$\n\nCombining for all $i \\leq N$ we get the statement.\n\nWe next prove Proposition 24 which we now recall.\nProposition 24. Let $M^{*}$ be a piece-wise polynomial approximation satisfying Definition 14. Then for all positive $\\delta<1$ with probability at least $1-\\delta$ for all $i \\leq N$ and $y \\in M_{i}$\n\n$$\n\\left|\\left\\langle Z_{D}, y-\\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}(y)\\right\\rangle\\right| \\leq L^{*} \\varepsilon_{N}^{\\beta} \\sqrt{2 d \\log \\varepsilon_{N}^{-1}+2 \\log 2 \\delta^{-1}}\n$$\n\nProof. We follow the same strategy as in the proof of Proposition 8. First, we prove that the statement holds on a submanifold $M_{i}=\\Phi_{i}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)$, and then we spread it on the whole manifold.\n\nTake $y \\in M_{i}$ and represent it as $y=\\Phi_{i}(z)$, where $z \\in B_{d}\\left(0, \\varepsilon_{N}\\right)$, and define $y^{*}=\\Phi_{i}^{*}(z)$. Then $y-y^{*}=\\Phi_{i}(z)-\\Phi_{i}^{*}(z)$. By construction $\\left\\|\\nabla \\Phi_{i}(z)-\\nabla \\Phi_{i}^{*}(z)\\right\\| \\leq L^{*}|z|^{\\beta-1}$, so on $B_{d}\\left(0, \\varepsilon_{N}\\right)$ the function $\\Phi_{i}-\\Phi_{i}^{*}$ is $L^{*} \\varepsilon_{N}^{\\beta-1}$ Lipschitz, and $\\Phi_{i}(0)=\\Phi_{i}^{*}(0)=G_{i}$. Therefore by Proposition 28 for all $\\delta<1$\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in B\\left(0, \\varepsilon_{N}\\right)}\\left\\langle Z_{D}, \\Phi_{i}(z)-\\Phi_{i}^{*}(z)\\right\\rangle \\leq L^{*} \\varepsilon_{N}^{\\beta}\\left(\\sqrt{d}+\\sqrt{2 \\log (\\delta / N)^{-1}}\\right)\\right) \\geq 1-\\delta / N\n$$\n\nSo, the probability that such an event holds for all $M_{i}$ is bounded from below as\n\n$$\n\\mathbb{P}\\left(\\forall i: \\sup _{z \\in B\\left(0, \\varepsilon_{N}\\right)}\\left\\langle Z_{D}, \\Phi_{i}(z)-\\Phi_{i}^{*}(z)\\right\\rangle \\leq L^{*} \\varepsilon_{N}^{\\beta}\\left(\\sqrt{d}+\\sqrt{2 \\log N+2 \\log \\delta^{-1}}\\right)\\right) \\geq 1-\\delta\n$$\n\nApplying the same bound to $-\\left\\langle Z_{D}, \\Phi_{i}(z)-\\Phi_{i}^{*}(z)\\right\\rangle \\stackrel{\\text { dist. }}{=}\\left\\langle Z_{D}, \\Phi_{i}(z)-\\Phi_{i}^{*}(z)\\right\\rangle$ and using $\\sqrt{a}+$ $\\sqrt{b} \\leq \\sqrt{2(a+b)}$ we get the statement.\n\nWe generally follow the same strategy as in Appendix B.2. First, we prove an analogue of (34).\n\nProposition 32. Let $t \\geq \\varepsilon_{N}^{2 \\beta}, X(0) \\sim \\mu, Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ and $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$. For any $\\delta>0$ a.s. in $X(0)$ and with probability at least $1-\\delta$ in $Z_{D}$ for all $i \\leq N, y \\in M_{i}$ and $y^{*}=\\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}(y)$\n\n$$\n\\begin{aligned}\n& \\frac{1}{2} c_{t}^{2}\\|X(0)-y\\|^{2}-16 \\sigma_{t}^{2}\\left(4 d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+2 d \\log \\varepsilon_{N}^{-1}+(12 d+2) C_{\\log }+\\left(L^{*}\\right)^{2}+2 \\log \\delta^{-1}\\right) \\\\\n& \\quad \\leq\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2}-\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2} \\leq \\\\\n& \\frac{3}{2} c_{t}^{2}\\|X(0)-y\\|^{2}+8 \\sigma_{t}^{2}\\left(4 d \\log _{+}\\left(\\sigma_{t} / c_{t}\\right)+2 d \\log \\varepsilon_{N}^{-1}+(12 d+2) C_{\\log }+\\left(L^{*}\\right)^{2}+2 \\log \\delta^{-1}\\right)\n\\end{aligned}\n$$\n\nProof. First, we represent\n\n$$\n\\begin{aligned}\n& \\left\\|X(t)-c_{t} y^{*}\\right\\|^{2}=\\left\\|X(t)-c_{t} y\\right\\|^{2}+2 c_{t} \\sigma_{t}\\left\\langle Z_{D}, y-y^{*}\\right\\rangle+2 c_{t}^{2}\\left\\langle X(0)-y, y-y^{*}\\right\\rangle+c_{t}^{2}\\left\\|y-y^{*}\\right\\|^{2} \\\\\n& =\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2}+c_{t}^{2}\\|X(0)-y\\|^{2}+2 c_{t} \\sigma_{t}\\left\\langle Z_{D}, X(0)-y\\right\\rangle+2 c_{t} \\sigma_{t}\\left\\langle Z_{D}, y-y^{*}\\right\\rangle+2 c_{t}^{2}\\left\\langle X(0)-y, y-y^{*}\\right\\rangle+c_{t}^{2}\\left\\|y-y^{*}\\right\\|^{2}\n\\end{aligned}\n$$\n\nWe recall (33) stating that if\n$C_{0}^{2}:=4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 d \\log \\varepsilon_{N}^{-1}+(12 d+2) C_{\\log }+2 \\log 2 \\delta^{-1} \\geq 4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+(12 d+2) C_{\\log }+2 \\log 2 \\delta^{-1}$\nthen with probability at least $1-\\delta$\n\n$$\n2 \\sigma_{t} c_{t}\\left|\\left\\langle Z_{D}, X(0)-y\\right\\rangle\\right| \\leq 2 \\sigma_{t}^{2} C_{0}+2 \\sigma_{t} c_{t}\\|X(0)-y\\| C_{0} \\leq c_{t}^{2}\\|X(0)-y\\|^{2} / 4+6 \\sigma_{t}^{2} C_{0}^{2}\n$$\n\nBy (24) since $\\sigma_{t} \\geq \\varepsilon_{N}^{\\beta}$ with probability at least $1-\\delta$\n\n$$\n2 c_{t} \\sigma_{t}\\left|\\left\\langle Z_{D}, y-y^{*}\\right\\rangle\\right| \\leq 2 c_{t} \\sigma_{t} L^{*} \\varepsilon_{N}^{\\beta}\\left(\\sqrt{d}+\\sqrt{2 d \\log \\varepsilon_{N}^{-1}+2 \\log 2 \\delta^{-1}}\\right) \\leq 2 \\sigma_{t}^{2}\\left(\\left(L^{*}\\right)^{2}+C_{0}^{2}\\right)\n$$\n\nFinally,\n\n$$\n\\begin{aligned}\n\\left|2 c_{t}^{2}\\left\\langle X(0)-y, y-y^{*}\\right\\rangle+c_{t}^{2}\\left\\|y-y^{*}\\right\\|^{2}\\right| & \\leq 2 c_{t}^{2}\\|X(0)-y\\|\\left\\|y-y^{*}\\right\\|+c_{t}^{2}\\left\\|y-y^{*}\\right\\|^{2} \\\\\n& \\leq 2 c_{t}^{2} L^{*} \\varepsilon_{N}^{\\beta}\\|X(0)-y\\|+c_{t}^{2}\\left(L^{*}\\right)^{2} \\varepsilon_{N}^{2 \\beta} \\\\\n& \\leq 2 c_{t}^{2} L^{*} \\sigma_{t}\\|X(0)-y\\|+\\left(L^{*}\\right)^{2} c_{t}^{2} \\sigma_{t}^{2} \\\\\n& \\leq c_{t}^{2}\\|X(0)-y\\|^{2} / 4+5\\left(L^{*}\\right)^{2} \\sigma_{t}^{2}\n\\end{aligned}\n$$\n\nSumming up, we get that with probability at least $1-\\delta$\n\n$$\n\\begin{aligned}\n\\left|2 c_{t} \\sigma_{t}\\left\\langle Z_{D}, X(0)-y\\right\\rangle+2 c_{t} \\sigma_{t}\\left\\langle Z_{D}, y-y^{*}\\right\\rangle+\\right. & \\left.2 c_{t}^{2}\\left\\langle X(0)-y, y-y^{*}\\right\\rangle+c_{t}^{2}\\left\\|y-y^{*}\\right\\|^{2}\\right] \\\\\n& \\leq c_{t}^{2}\\|X(0)-y\\|^{2} / 2+8 \\sigma_{t}^{2} C_{0}^{2}+7\\left(L^{*}\\right)^{2} \\sigma_{t}^{2}\n\\end{aligned}\n$$\n\nand the desired inequality\n\n$$\n\\begin{aligned}\n\\frac{1}{2} c_{t}^{2}\\left\\|X(0)-y^{*}\\right\\|^{2}-8 \\sigma_{t}^{2}\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right) & \\leq\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2}-\\sigma_{t}^{2}\\left\\|Z_{D}\\right\\|^{2} \\\\\n& \\leq \\frac{3}{2} c_{t}^{2}\\left\\|X(0)-y^{*}\\right\\|^{2}+8 \\sigma_{t}^{2}\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right)\n\\end{aligned}\n$$\n\nFinally, we are ready to present the main result of this section. Consider a measure $\\mu$ supported on $M$. Let $\\mu^{*}$ be the measure on $M^{*}$ constructed in Section 5 corresponding to $\\varepsilon_{N}>$ 0 . As in Section 4 we consider a conditional measure $\\mu^{*}(y \\mid t, x) \\propto e^{-\\left\\|x-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu^{*}\\left(d y^{*}\\right)$.\n\nProposition 33. Let a measure $\\mu$ satisfy Assumption B, and measure $\\mu^{*}$ be as above. Fix $t \\geq \\varepsilon_{N}^{2 \\beta}$, non-negative $\\delta, \\eta<1$, and the radius\n\n$$\n\\begin{aligned}\n& r(t, \\delta, \\eta)=20\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d\\left(\\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+\\log \\varepsilon_{N}^{-1}+3 C_{\\log }\\right)+\\left(L^{*}\\right)^{2}+\\log 2 \\delta^{-1}+\\log \\eta^{-1}} \\\\\n& \\geq 3\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{\\log \\eta^{-1}+C_{\\log }(99 d+17)+33 \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+32 d \\log \\varepsilon_{N}^{-1}+16 \\log 2 \\delta^{-1}+8\\left(L^{*}\\right)^{2}}\n\\end{aligned}\n$$\n\nthen with probability at least $1-\\delta$ in $Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$ and a.s. in $X(0) \\sim \\mu$ for a point $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$ the following bound holds\n\n$$\n\\int_{y^{*} \\in M:\\left\\|y^{*}-X(0)\\right\\| \\leq r(t, \\delta, \\eta)} p\\left(y^{*} \\mid t, X(t)\\right) d y^{*} \\geq 1-\\eta\n$$\n\nProof. As in Proposition 32 we introduce $C_{0}^{2}=4 d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+4 d \\log \\varepsilon_{N}^{-1}+(12 d+2) C_{\\log }+$ $2 \\log 2 \\delta^{-1}$.\n\nSince $\\mu^{*}\\left(y^{*} \\mid t, X(t)\\right) \\propto e^{-\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu\\left(d y^{*}\\right)$, first, we control the normalization constant. Applying the change of variable formula, Proposition 32, and then (35) with probability at least $1-\\delta$\n\n$$\n\\begin{aligned}\nB & :=\\int_{M^{*}} e^{-\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu\\left(d y^{*}\\right)=\\sum_{i} \\int_{M_{i}^{*}} e^{-\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right) \\\\\n& =\\sum_{i} \\int_{M_{i}} e^{-\\left\\|X(t)-c_{t} \\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}(y)\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu_{i}(d y) \\geq e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{-4\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right)} \\sum_{i} \\int_{M_{i}} e^{-3\\|X(0)-y\\|^{2} / 2 \\sigma_{t}^{2}} \\mu_{i}(d y) \\\\\n& =e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{-4\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right)} \\int_{M} e^{-3 c_{t}^{2}\\left\\|X(0)-y\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu(d y) \\\\\n& \\geq \\frac{e^{d}}{(2 d)^{d+1}} e^{-2} p_{\\min } e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{-4\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right)}\\left(r_{0} \\wedge\\left(\\sigma_{t} / c_{t}\\right)\\right)^{d} \\geq e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{-C_{\\log }(3 d+1)-4\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right)-d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)}\n\\end{aligned}\n$$\n\nBy the same change of variables formula and Proposition 32 with probability at least $1-\\delta$\n\n$$\n\\begin{aligned}\nA & :=\\int_{y^{*} \\in M^{*}:\\left\\|X(0)-y^{*}\\right\\| \\geq r(t, \\delta, \\eta)} e^{-\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu^{*}\\left(d y^{*}\\right) \\\\\n& =\\sum_{i} \\int_{y^{*} \\in M_{i}^{*}:\\left\\|X(0)-y^{*}\\right\\| \\geq r(t, \\delta, \\eta)} e^{-\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right) \\\\\n& =\\sum_{i} \\int_{y \\in M_{i}:\\left\\|X(0)-\\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}(y)\\right\\| \\geq r(t, \\delta, \\eta)} e^{-\\left\\|X(t)-c_{t} \\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}(y)\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu_{i}(d y) \\\\\n& \\leq \\sum_{i} \\int_{y \\in M_{i}:\\|X(0)-y\\| \\geq r(t, \\delta, \\eta)-L^{*} \\varepsilon_{N}^{\\beta}} e^{-\\left\\|X(t)-c_{t} \\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}(y)\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu_{i}(d y) \\\\\n& \\leq e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{4\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right)} \\int_{y:\\|X(0)-y\\| \\geq r(t, \\delta, \\eta)-L^{*} \\varepsilon_{N}^{\\beta}} e^{-c_{t}^{2}\\|X(0)-y\\|^{2} / 4 \\sigma_{t}^{2}} \\mu(d y) \\\\\n& \\leq e^{-\\left\\|Z_{D}\\right\\|^{2} / 2} e^{4\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right)-c_{t}^{2}\\left(r(t, \\delta, \\eta)-L^{*} \\varepsilon_{N}^{\\beta}\\right)^{2} / 4 \\sigma_{t}^{2}}\n\\end{aligned}\n$$\n\nThus with probability at least $1-\\delta$\n\n$$\n\\begin{aligned}\n\\mu^{*}\\left(B_{M^{*}}(X(0), r(t, \\delta, \\eta) \\mid t,\\right. & X(t))=1-\\frac{A}{B} \\\\\n& \\geq 1-e^{C_{\\log }(3 d+1)+8\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right)+d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)-c_{t}^{2}\\left(r(t, \\delta, \\eta)-L^{*} \\varepsilon_{N}^{\\beta}\\right)^{2} / 4 \\sigma_{t}^{2}}\n\\end{aligned}\n$$\n\nSo, for\n\n$$\nr(t, \\delta, \\eta) \\geq L^{*} \\varepsilon_{N}^{\\beta}+2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{\\log \\eta^{-1}+C_{\\log }(3 d+1)+d \\log _{+}\\left(c_{t} / \\sigma_{t}\\right)+8\\left(C_{0}^{2}+\\left(L^{*}\\right)^{2}\\right)}\n$$\n\nwith probability at least $1-\\delta$\n\n$$\n\\mu^{*}\\left(B_{M^{*}}(X(0), r(t, \\delta, \\eta) \\mid t, X(t)\\right) \\geq 1-\\eta\n$$\n\nThe statement follows if we let $t \\geq \\varepsilon_{N}^{2 \\beta}$, note that $\\left(\\sigma_{t} / c_{t}\\right) \\geq \\varepsilon_{N}^{\\beta}$, and substitute the definition of $C_{0}$.", "tables": {}, "images": {}}, {"section_id": 25, "text": "# Appendix D. Score Approximation by Neural Networks \n\nIn this section, we prove Theorem 7 for $d \\geq 3$ on the score approximation by a neural network.\n\nRecall that $\\underline{T} \\geq n^{4 \\gamma} n^{-\\frac{2(\\alpha+1)}{2 \\alpha+d}}$ for some $\\gamma<\\frac{1}{3(2 \\alpha+d)}$, and that $\\bar{T} \\lesssim \\log n$.\nAs we discussed in Section 3.1 we divide the interval $[\\underline{T}, \\bar{T}]$ into segments $\\underline{T}=T_{1}<$ $T_{2}<\\ldots<T_{K}=\\bar{T}$, where $T_{k+1} / T_{k}=2$ and build estimator $\\hat{s}(t, x)$ as a union of separate estimators $\\hat{s}_{k}(t, x)$, where $k$-th estimator approximates $s(t, x)$ on $\\left[T_{k}, T_{k+1}\\right]$. We also recall that we find each $\\hat{s}_{k}(t, x)$ as a solution of $\\hat{s}_{k}(t, x)=\\arg \\min _{\\phi \\in \\mathcal{S}_{k}} \\mathcal{R}_{\\mathcal{Y}}(\\phi)$, where the class $\\mathcal{S}_{k}$ was defined in (20). The first step is to construct $\\phi \\in \\mathcal{S}_{k}$ approximating the score; and in Appendix D. 1 we prove that for $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}}$\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|\\phi(t, X(t))-s(t, X(t))\\|^{2} d t \\leq \\sigma_{T_{k}}^{-2} n^{4 \\gamma \\beta} n^{-\\frac{2 \\beta}{2 \\alpha+d}}\n$$\n\nwhile in Appendix D. 2 we prove that for $T_{k}<n^{-\\frac{2}{2 \\alpha+d}}$\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|\\phi(t, X(t))-s(t, X(t))\\|^{2} d t \\leq \\sigma_{T_{k}}^{-2} n^{6 \\gamma \\beta} n^{-\\frac{2 \\beta}{2 \\alpha+d}}+n^{-\\frac{2 \\alpha}{2 \\alpha+d}}\n$$\n\nFinally, in Appendix D. 3 we first bound the covering number of classes $\\mathcal{S}_{k}$. Then we bound the variance of the empirical risk estimator via the covering number of $\\mathcal{S}_{k}$, thus obtaining Theorem 7.\n\nWe provide auxiliary results used in Appendix D in Appendices F and G. In Appendix F we recall lemmas on neural-networks approximation capabilities from Oko et al. (2023) and in Appendix G we prove the bounds related to the projection of a normal random variable on a tangent space that will play an important role in case $T_{k} \\leq n^{-\\frac{2}{2 \\alpha+d}}$.\n\nIn both cases ( $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}}$ and $T_{k} \\leq n^{-\\frac{2}{2 \\alpha+d}}$ ) the construction of $\\phi$ is based on first choosing $n^{2 \\gamma d} n^{\\frac{2}{2 \\alpha+d}} \\geq N_{k} \\geq n^{\\frac{d-2}{2 \\alpha+d}}$ and $\\mathcal{G}:=\\left\\{G_{1}, \\ldots, G_{N_{k}}\\right\\} \\subset \\mathcal{Y}=\\left\\{y_{1}, \\cdots, y_{n}\\right\\}$, for instance $\\mathcal{G}=\\left\\{y_{1}, \\cdots, y_{N_{k}}\\right\\}$ so that $\\mathcal{G} \\sim \\mu^{\\otimes N_{k}}$.\nThen define $\\varepsilon_{N_{k}}:=\\left(C_{d, \\beta} \\frac{p_{\\min }^{2} \\log N_{k}}{p_{\\min }^{2}} \\frac{1}{d}\\right.$ and as explained in Section 5.1, for each $G_{i}$ we consider $\\mathcal{V}_{i}:=\\left\\{G_{j}-G_{i} \\mid\\left\\|G_{j}-G_{i}\\right\\| \\leq \\varepsilon_{N}\\right\\}$ and the polynomial functions\n\n$$\n\\Phi_{i}^{*}(z)=y_{i}+P_{i}^{*} z+\\sum a_{i, S}^{*} z^{S}\n$$\n\nand $P_{i}^{*}, a_{i, S}^{*}$ are a solution of (27) satisfying $\\operatorname{Im} P_{i}^{*} \\subset \\operatorname{span} \\mathcal{V}_{i}, a_{i, S}^{*} \\in \\operatorname{span} \\mathcal{V}_{i}$. The functions $\\Phi_{i}^{*}$ are polynomial approximations of the local maps $\\Phi_{i}$ defined as the local inverse to projections on $T_{G_{i}} M$ over the balls $\\left(B_{d}\\left(0, C \\varepsilon_{N}\\right)\\right)$ for any arbitrarily large constants $C$, since $C \\varepsilon_{N} \\leq r_{0}$ (independently of $D$ ) for large $n$. Then summarizing Section 5.1, with probability at least $1-N^{-\\frac{\\beta}{d}} \\geq 1-n^{-\\frac{\\beta}{2 \\alpha+d}}$ :\n\n- Proposition 16 implies that the set $\\mathcal{G}$ is $\\varepsilon_{N} / 2$-dense;\n- Lemma 19 implies that $\\left\\|\\Phi_{i}^{*}(z)-\\Phi_{i}(z)\\right\\| \\lesssim \\varepsilon_{N}^{\\beta} \\lesssim n^{-\\frac{\\alpha+1}{2 \\alpha+d}}(\\log n)^{\\beta / d}$ for all $z \\in B\\left(0,8 \\varepsilon_{N}\\right)$ for all $i \\leq N$;\n- Proposition 21 implies that $\\left|\\mathcal{V}_{i}\\right| \\leq C_{\\text {dim }} \\log N \\leq C_{\\text {dim }} \\log n$.\n\nUsing Proposition 26 we have that for all $z \\in B_{d}\\left(0,8 \\varepsilon_{N}\\right), k<\\beta$ that $\\left\\|\\nabla^{k} \\Phi_{i}^{*}(z)\\right\\| \\leq 2 L_{M}$. In particular, this guarantees that $\\Phi_{i}^{*}(z)$ is 2-Lipschitz on $B_{d}\\left(0,8 \\varepsilon_{N}\\right)$ and $\\left\\|\\nabla \\Phi_{i}^{*}(z) \\nabla^{T} \\Phi_{i}^{*}(z)-\\operatorname{Id}_{d}\\right\\| \\leq$ $\\frac{1}{2}$.\nFurther, we assume that $n$ is large enough and $\\mathcal{G}$ is such that all these events and inequalities hold.\nWe decompose $M=\\cup_{i} M_{i}$, where $M_{i}$ is a neighborhood of $G_{i}$ in $M$ and we then construct and fix a piecewise polynomial approximation $M^{*}=\\bigcup_{i=1}^{N} M_{i}^{*}$ of the manifold $M$, where $M_{i}^{*}=\\Phi_{i}^{*}\\left(\\Phi_{i}^{-1}\\left(M_{i}\\right)\\right)$ is an approximation $M_{i}$. Its precise construction depends on the position of $T_{k}$ with respect to $n^{-\\frac{2}{2 \\alpha+d}}$. Also, recall that\n\n$$\n\\rho(x)= \\begin{cases}1 & \\text { if }|x| \\leq 1 / 2 \\\\ 2-2 x & \\text { if }|x| \\in[1 / 2,1] \\\\ 0 & \\text { if }|x| \\geq 1\\end{cases}\n$$\n\nand the localization functions are defined as\n\n$$\n\\rho_{i}(t, X(t))=\\rho\\left(\\frac{\\left(\\left\\|X(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} G_{\\min }(X(t))\\right\\|^{2}\\right)}{C\\left(T_{k}, n\\right)}\\right)\n$$\n\nwhere $C\\left(T_{k}, n\\right)$ is defined differently whether $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}}$ or not and where $G_{\\min }(x)=$ $\\arg \\min _{i \\leq N_{k}}\\left\\|x-G_{i}\\right\\|$. Thus $\\rho_{i}$ is such that it is equal to 0 when $M_{i}^{*}$ is far from $x$ compared to other $M_{j}^{*}$ 's. Finally recall that $s(t, x)=e(t, x)-x / \\sigma_{t}^{2}$ where\n\n$$\ne(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\int_{M} y e^{-\\frac{\\left\\|x-c_{t} y\\right\\|^{2}}{2 \\sigma_{t}^{2}}} \\mu(d y)}{\\int_{M} e^{-\\frac{\\left\\|x-c_{t} y\\right\\|^{2}}{2 \\sigma_{t}^{2}}} \\mu(d y)}\n$$\n\nWe first consider the (simpler) case $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}}$.\nD. 1 Case $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}}$\n\nChoose\n\n$$\nN=N_{k}:=\\left[\\max \\left(n^{2 \\gamma d}\\left(\\sigma_{T_{k}} / c_{T_{k}}\\right)^{-d}, n \\times n^{-\\frac{2(\\alpha+1)}{2 \\alpha+d}}\\right)\\right]\n$$\n\nBy construction that $N \\leq n^{2 \\gamma d} 4^{d} n^{\\frac{d}{2 \\alpha+d}}$ and $(\\log n)^{1 / d} n^{-\\frac{d-2}{d}} \\frac{1}{2 \\alpha+d} \\gtrsim \\varepsilon_{N} \\geq n^{-2 \\gamma} n^{-\\frac{1}{2 \\alpha+d}}$. Note that $\\varepsilon_{N}^{\\beta} \\lesssim(\\log n)^{\\beta / d} n^{-\\frac{\\alpha+1}{2 \\alpha+d}}$ by Assumption D. Let $M_{i}:=\\Phi_{i}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)$ and define $M_{i}^{*}=\\Phi_{i}^{*}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)$ so that the surface $M_{i}^{*}$ is a polynomial approximation of $M_{i}$ and define $M^{*}=\\cup_{i=1}^{N} M_{i}^{*}$. Since $\\operatorname{diam} M \\leq 1$, and by properties of $\\Phi_{i}^{*}$, $\\operatorname{diam} M^{*} \\leq 2$.\n\nWe build the partition of unity on $M$ corresponding to the set $\\mathcal{G}_{i}$ as\n\n$$\n\\chi_{i}(y)=\\frac{\\rho\\left(\\left\\|y-G_{i}\\right\\| / \\varepsilon_{N}\\right)}{\\sum_{j=1}^{N} \\rho\\left(\\left\\|y-G_{j}\\right\\| / \\varepsilon_{N}\\right)}\n$$\n\nwhere $\\rho$ is defined in (47). Since $\\mathcal{G}$ is $\\varepsilon_{N} / 2$ dense on $M, \\sum \\chi_{i}(y)=1$ for all $y \\in M$. We consider the measure $\\mu_{i}$ with density $\\chi_{i}(y) p(y)$ supported on the sub-manifold $M_{i}$ and denote by $\\mu_{i}^{*}$ its pushforward on $M_{i}^{*}=\\Phi_{i}^{*}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)$ ) by $\\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}$. By construction $\\mu^{*}:=\\sum \\mu_{i}^{*}$ is a probability and satisfies $W_{2}\\left(\\mu, \\mu^{*}\\right) \\lesssim \\varepsilon_{N}^{\\beta}$ Therefore, by Proposition 22 the score function $s^{*}$ corresponding to measure $\\mu^{*}$ verifies\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|s(t, X(t))-s^{*}(t, X(t))\\|^{2} d t \\lesssim \\sigma_{T_{k}}^{-2} \\varepsilon_{N}^{2 \\beta} \\leq \\sigma_{T_{k}}^{-2} n^{-\\frac{2(n+1)}{2 \\alpha+d}}(\\log n)^{\\frac{2 \\beta}{d}}\n$$\n\nAs we discussed in Section 3 introducing $\\mathcal{A}_{i}=G_{i}+\\operatorname{span} \\mathcal{V}_{i}, \\mathcal{H}_{i}=\\operatorname{span} \\mathcal{V}_{i}$ and $x_{i}=\\operatorname{pr}_{\\mathcal{A}_{i}} x$ we can represent the score function $s^{*}(t, x)$ as\n\n$$\ns^{*}(t, x)=\\sum p\\left(M_{i}^{*} \\mid t, x\\right) s_{i}^{*}(t, x)=\\sum p\\left(M_{i}^{*} \\mid t, x\\right)\\left(s_{i}^{*}\\left(t, x_{i}\\right)+\\frac{x_{i}}{\\sigma_{t}^{2}}\\right)-\\frac{x}{\\sigma_{t}^{2}}\n$$\n\nwhere $s_{i}^{*}(t, x)$ is the score function corresponding to the measure $\\mu_{i}^{*}$, and $p^{*}\\left(M_{i}^{*} \\mid t, x\\right):=$ $\\mathbb{P}\\left(X^{*}(0) \\in M_{i}^{*} \\mid X^{*}(t)=x\\right)$, where $X^{*}(t)$ is an OU process with initial condition $\\mu^{*}$ :\n$p^{*}\\left(M_{i}^{*} \\mid t, x\\right)=\\frac{\\int_{M_{i}^{*}} e^{-\\frac{\\left\\|x-c_{t} y^{*}\\right\\|^{2}}{2 \\sigma_{t}^{2}}} \\mu_{i}^{*}\\left(d y^{*}\\right)}{\\sum_{j=1}^{N} \\int_{M_{j}^{*}} e^{-\\frac{\\left\\|x-c_{t} y^{*}\\right\\|^{2}}{2 \\sigma_{t}^{2}}} \\mu_{j}^{*}\\left(d y^{*}\\right)}, \\quad s_{i}^{*}(t, x)=\\frac{1}{\\sigma_{t}^{2}} \\frac{\\int_{M_{i}^{*}}\\left(c_{t} y^{*}-x\\right) e^{-\\left\\|x-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right)}{\\int_{M_{i}^{*}} e^{-\\left\\|x-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right)}$.\nWe approximate $s^{*}$ by the truncated version denoted $s_{t r}^{*}$ defined by:\n\n$$\ns_{t r}^{*}(t, x):=\\frac{\\sum \\rho_{i}(t, x) p^{*}\\left(M_{i}^{*} \\mid t, x\\right) s_{i}^{*}(t, x)}{\\sum \\rho_{i}(t, X(t)) p\\left(M_{i}^{*} \\mid t, x\\right)}\n$$\n\nwhere $\\rho_{i}$ are of form (48). Set $e^{*}(t, x):=s^{*}(t, x)+\\frac{x}{\\sigma_{t}^{2}}$ and $e_{i}^{*}(t, x):=s_{i}^{*}(t, x)+\\frac{x}{\\sigma_{t}^{2}}$. We then have\n\n$$\ne^{*}(t, x)=\\sum p\\left(M_{i}^{*} \\mid t, x\\right) e_{i}^{*}(t, x)=\\sum p\\left(M_{i}^{*} \\mid t, x\\right) e_{i}^{*}\\left(t, x_{i}\\right)\n$$\n\nWe now show that $s^{*}(t, X(t))$ is close to $s_{t r}^{*}(t, X(t))$, where $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$ is sampled from forward the process (1) at time $t$.", "tables": {}, "images": {}}, {"section_id": 26, "text": "# D.1.1 Approximation of $s^{*}(t, X(t))$ by $s_{t r}^{*}(t, X(t))$ \n\nWe introduce\n\n$$\n\\begin{aligned}\nC(n) & :=\\sqrt{\\left(C_{\\mathrm{dim}}+9\\right) \\log n+4 d C_{\\log +\\left(L^{*}\\right)^{2}}} \\\\\n& \\geq \\sqrt{d\\left(\\log _{+}\\left(\\sigma_{t} / c_{t}\\right)^{-1}+\\log \\varepsilon_{N}^{-1}+3 C_{\\log }\\right)+\\left(L^{*}\\right)^{2} \\log n^{2}+\\log n^{2}}\n\\end{aligned}\n$$\n\ncorresponding to the choice of $\\delta, \\eta=n^{-2}$ in Proposition 33, where the additional term $C_{\\text {dim }}$ comes from Proposition 21.\n\nConsider the set\n\n$$\nI_{1}:=\\left\\{i \\leq N: \\operatorname{dist}\\left(X(0), M_{i}^{*}\\right) \\leq 20\\left(\\sigma_{t} / c_{t}\\right) C(n)\\right\\}\n$$\n\nProposition 33 states that with probability at least $1-n^{-2}$\n\n$$\n\\sum_{i \\notin I_{1}} p\\left(M_{i}^{*} \\mid t, X(t)\\right) \\leq n^{-2}\n$$\n\nFor $y^{*} \\in M_{i}^{*}$ there is $z \\in B_{d}\\left(0, \\varepsilon_{N}\\right)$ satisfying $y^{*}=\\Phi_{i}^{*}(z)$, so\n\n$$\n\\left\\|G_{i}-y^{*}\\right\\|=\\left\\|\\Phi_{i}^{*}(0)-\\Phi_{i}^{*}(z)\\right\\| \\leq 2\\|z\\| \\leq 2 \\varepsilon_{N} \\leq n^{-\\gamma}\\left(\\sigma_{t} / c_{t}\\right)\n$$\n\nso for $i \\in I_{1}$ we have\n\n$$\n\\left\\|X(0)-G_{i}\\right\\| \\leq \\operatorname{dist}\\left(X(0), M_{i}^{*}\\right)+n^{-\\gamma}\\left(\\sigma_{t} / c_{t}\\right) \\leq 21\\left(\\sigma_{t} / c_{t}\\right) C(n)\n$$\n\nMoreover, using Lemma 13, when $i \\in I_{1}$ with probability $1-n^{-2}$ we have\n\n$$\n\\begin{aligned}\n\\frac{2}{3} c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X(t)\\right.\\right. & \\left.-c_{t} G_{\\min }(t)\\right\\|^{2} \\\\\n& \\leq\\left\\|X(0)-G_{i}\\right\\|^{2}+128\\left(\\sigma_{t} / c_{t}\\right)^{2} C^{2}(n) \\leq 600\\left(\\sigma_{t} / c_{t}\\right)^{2} C^{2}(n)\n\\end{aligned}\n$$\n\nTherefore we define $\\rho_{i}$ as in (48) with the radius $c\\left(T_{k}, n\\right)=3 \\cdot 600 \\sigma_{t}^{2} C^{2}(n)$, which leads to $\\rho_{i}(t, X(t))=1$ for all $i \\in I_{1}$.\n\nMoreover, since $\\rho(x)=0$ for $|x|>1$, if $\\rho_{i}(t, X(t))>0$ then\n\n$$\n\\frac{2}{3} c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} G_{\\min }(t)\\right\\|^{2}\\right) \\leq 1200\\left(\\sigma_{t} / c_{t}\\right)^{2} C^{2}(n)\n$$\n\nand by Lemma 13, since $\\mathcal{G}$ is an $\\varepsilon_{N} / 2<\\left(\\sigma_{t} / c_{t}\\right)$ dense set of $M$\n\n$$\n\\begin{aligned}\n\\left\\|X(0)-G_{i}\\right\\|^{2} \\leq & 9\\left(\\sigma_{t} / c_{t}\\right)^{2}+2 c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} G_{\\min }(t)\\right\\|^{2}\\right)+128\\left(\\sigma_{t} / c_{t}\\right)^{2} C^{2}(n) \\\\\n& \\leq 3800\\left(\\sigma_{t} / c_{t}\\right)^{2} C^{2}(n)\n\\end{aligned}\n$$\n\nHence, defining\n\n$$\nI_{2}:=\\left\\{i \\leq N:\\left\\|X(0)-G_{i}\\right\\| \\leq \\sqrt{3800}\\left(\\sigma_{t} / c_{t}\\right) C(n)\\right\\}\n$$\n\n$\\rho_{i}(t, X(t))>0$ implies that $i \\in I_{2}$ and\n\n$$\ns_{t r}^{*}(t, x):=\\frac{\\sum_{i \\in I_{2}} \\rho_{i}(t, x) p\\left(M_{i}^{*} \\mid t, x\\right) s_{i}^{*}(t, x)}{\\sum_{i \\in I_{2}} \\rho_{i}(t, X(t)) p\\left(M_{i}^{*} \\mid t, x\\right)}\n$$\n\nWe denote accordingly $e_{t r}^{*}(t, x):=s_{t r}^{*}(t, x)+\\frac{x}{s_{t}^{2}}$, which is equal to\n\n$$\ne_{t r}^{*}(t, x)=\\frac{\\sum \\rho_{i}(t, x) p\\left(M_{i}^{*} \\mid t, x\\right) e_{i}^{*}(t, x)}{\\sum \\rho_{i}(t, X(t)) p\\left(M_{i}^{*} \\mid t, x\\right)}\n$$\n\nSince $\\operatorname{diam} M^{*} \\leq 2,\\left\\|e^{*}(t, X(t))\\right\\| \\leq 2 c_{t} / \\sigma_{t}^{2}$ and since $\\rho_{i}(t, X(t))=1$ for $i \\in I_{1}$ with probability $1-n^{-2}$ we have on this event that\n\n$$\n\\sum_{i=1}^{N} \\rho_{i}(t, X(t)) p\\left(M_{i}^{*} \\mid t, x\\right) \\geq \\sum_{i \\in I_{1}} p\\left(M_{i}^{*} \\mid t, x\\right) \\geq 1-n^{-2}\n$$\n\nTherefore with probability $1-n^{-2}$\n\n$$\n\\begin{aligned}\n\\left\\|s_{t r}^{*}(t, X(t))-s^{*}(t, X(t))\\right\\| & =\\left\\|e_{t r}^{*}(t, X(t))-e^{*}(t, X(t))\\right\\| \\\\\n& \\leq \\frac{\\left\\|e^{*}(t, X(t)) \\sum_{i}\\left(1-\\rho_{i}(t, X(t)) p\\left(M_{i}^{*} \\mid t, x\\right)\\right\\|\\right.}{\\sum_{i} \\rho_{i}(t, X(t)) p\\left(M_{i}^{*} \\mid t, x\\right)} \\\\\n& +\\frac{\\sum_{i}\\left(1-\\rho_{i}(t, X(t)) p\\left(M_{i}^{*} \\mid t, x\\right)\\left\\|e_{i}^{*}(t, X(t))\\right\\|\\right.}{\\sum \\rho_{i}(t, X(t)) p\\left(M_{i}^{*} \\mid t, x\\right)} \\\\\n& \\leq 2 \\frac{c_{t}}{\\left(1-n^{-2}\\right) \\sigma_{t}^{2}}\\left(1-\\sum \\rho_{i}(t, X(t)) p\\left(M_{i}^{*} \\mid t, x\\right)\\right) \\leq \\frac{2 c_{t} n^{-2}}{\\left(1-n^{-2}\\right) \\sigma_{t}^{2}}\n\\end{aligned}\n$$\n\nAt the same time, a.s.\n\n$$\n\\begin{gathered}\n\\left\\|e_{t r}^{*}(t, X(t))-e^{*}(t, X(t))\\right\\| \\leq\\left\\|e^{*}(t, X(t))\\right\\|+\\left\\|e_{t r}^{*}(t, X(t))\\right\\| \\leq 2 \\frac{c_{t}}{\\sigma_{t}^{2}} \\\\\n\\mathbb{E}\\left\\|s_{t r}^{*}(t, X(t))-s^{*}(t, X(t))\\right\\|^{2} \\leq \\frac{c_{t}^{2}}{\\sigma_{t}^{4}}\\left(16 n^{-4}+4 n^{-2}\\right)\n\\end{gathered}\n$$\n\nand since $T_{k} \\leq \\bar{T}=O(\\log n)$ and $d \\geq 3$\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\sigma_{t}^{2} \\mathbb{E}\\left\\|s_{t r}^{*}(t, X(t))-s^{*}(t, X(t))\\right\\|^{2} d t \\lesssim n^{-2} \\int_{T_{k}}^{T_{k+1}} \\frac{1}{t} d t \\lesssim n^{-2}\n$$\n\nWe show in the next section that we can construct a neural network in $\\mathcal{S}_{k}$ which approximates $s_{t r}^{*}$ with an error less than $n^{-B}$ for any $B>0$ and $n$ large enough, which combined with (49) guarantees that we get an approximation of the true score $s(t, x)$ with error less than $\\varepsilon_{N}^{2 \\beta} \\lesssim n^{-\\frac{2(\\alpha+1)}{2 \\alpha+d}}(\\log n)^{2 \\beta / d}$.", "tables": {}, "images": {}}, {"section_id": 27, "text": "# D.1.2 Polynomial Approximation of $s_{t r}^{*}$ \n\nIn this section, we introduce the main regularity bounds that allow us to construct an efficient neural network approximation of $e_{t r}^{*}$. We show that for some functions $h_{i}, f_{i}, g_{i}$ we can represent it as\n\n$$\ne_{t r}^{*}(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\sum_{i} \\rho_{i}(t, x) h_{i}(t, x) f_{i}(t, x)}{\\sum_{i} \\rho_{i}(t, x) h_{i}(t, x) g_{i}(t, x)}\n$$\n\nwhere $\\rho_{i}(t, x) h_{i}(t, x)$ are easy-to-calculate weight functions, $f_{i}(t, x), g_{i}(t, x)$ are functions that can be efficiently approximated by neural networks. This decomposition corresponds to (19) presented in Section 3.1. Recall that $x_{i}=\\operatorname{pr}_{c_{t} \\mathcal{A}_{i}} x$ and since $M_{i}^{*} \\subset \\mathcal{A}_{i}$ we may represent $\\left\\|x-c_{t} y^{*}\\right\\|^{2}=\\left\\|x-x_{i}\\right\\|^{2}+\\left\\|x_{i}-c_{t} y^{*}\\right\\|^{2}$ so that\n\n$$\n\\begin{aligned}\n& p\\left(M_{i}^{*} \\mid t, x\\right)=e^{-\\left\\|x-x_{i}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\int_{M_{i}^{*}} e^{-\\left\\|x_{i}-c_{t} y^{*}\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right) \\\\\n& =e^{-\\left(\\left\\|x-x_{i}\\right\\|^{2}+\\left\\|x-c_{t} G_{\\min }\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} e^{\\left(\\left\\|x-c_{t} G_{\\min }\\right\\|^{2}-\\left\\|x-c_{t} G_{i}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} \\int_{M_{i}^{*}} e^{\\left(\\left\\|x_{i}-c_{t} G_{i}\\right\\|^{2}-\\left\\|x_{i}-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right)\n\\end{aligned}\n$$\n\nUsing a similar argument, and canceling the common factor we write\n\n$$\ne_{i}^{*}(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\int_{M_{i}^{*}} y^{*} e^{\\left(\\left\\|x_{i}-c_{t} G_{i}\\right\\|^{2}-\\left\\|x_{i}-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right)}{\\int_{M_{i}^{*}} e^{\\left(\\left\\|x_{i}-c_{t} G_{i}\\right\\|^{2}-\\left\\|x_{i}-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right)}\n$$\n\nWe can thus write $e_{i}^{*}=\\left(c_{t} / \\sigma_{t}^{2}\\right)\\left(f_{i} / g_{i}\\right)$ with\n\n$$\n\\begin{aligned}\ng_{i}(t, x) & :=\\int_{M_{i}^{*}} e^{\\left(\\left\\|x_{i}-c_{t} G_{i}\\right\\|^{2}-\\left\\|x_{i}-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right) \\\\\nf_{i}(t, x) & :=\\int_{M_{i}^{*}} y^{*} e^{\\left(\\left\\|x_{i}-c_{t} G_{i}\\right\\|^{2}-\\left\\|x_{i}-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right)\n\\end{aligned}\n$$\n\nand take $h_{i}(t, x):=e^{\\left(\\left\\|x-c_{t} G_{\\min }\\right\\|^{2}-\\left\\|x-c_{t} G_{i}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}}$ to get representation (54).\nWe establish regularity bounds to show that $f_{i}$ and $g_{i}$ can be efficiently approximated by neural networks. Recalling that $C(n)=\\sqrt{\\left(d+C_{\\text {dim }}+5\\right) \\log n+4 d C_{\\log }+\\left(L^{*}\\right)^{2}}$ we have:\n\n- with probability $1-n^{-2}$ for all $i \\leq N, \\rho_{i}(t, X(t)) \\neq 0$ implies $i \\in I_{2}$ or equivalently\n\n$$\n\\begin{aligned}\n\\left\\|X(0)-G_{i}\\right\\| & \\leq \\sqrt{3800}\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{(d+4) \\log n+4 d C_{\\log }} \\\\\n& \\leq \\sqrt{3800}\\left(\\sigma_{t} / c_{t}\\right) C(n) \\leq 65\\left(\\sigma_{t} / c_{t}\\right) C(n)\n\\end{aligned}\n$$\n\n- Applying Proposition 24 with $\\delta=n^{-2}$, since $\\varepsilon_{N}>n^{-1}$, we have that with probability $1-n^{-2}$ for all $y_{i} \\in M_{i}, y_{i}^{*}=\\Phi_{i}^{*} \\circ \\Phi_{i}^{-1}(y)$\n\n$$\n\\left|\\left\\langle Z_{D}, y_{i}-y_{i}^{*}\\right\\rangle\\right| \\leq\\left|\\left\\langle Z_{D}, y-y^{*}\\right\\rangle\\right| \\leq L^{*} \\varepsilon_{N}^{\\beta} \\sqrt{(2 d+5) \\log n}\n$$\n\n- Applying Proposition 8 with $\\delta=n^{-2}$ and $\\varepsilon=n^{-2}$ for all $y, y^{\\prime} \\in M$\n\n$$\n\\left|\\left\\langle Z_{D}, y-y^{\\prime}\\right\\rangle\\right| \\leq 4 n^{-2} \\sqrt{d}+\\left(\\left\\|y-y^{\\prime}\\right\\|+6 n^{-2}\\right) \\sqrt{(4 d+2) \\log 2 n^{2}+4 d C_{\\log }}\n$$\n\n- As a corollary of the two previous statements, since $\\beta \\geq 3$, for all $y \\in M, y^{*} \\in M^{*}$ s.t. $\\left\\|y-y^{*}\\right\\| \\lesssim 4 \\varepsilon_{N}$ we have\n\n$$\n\\left|\\left\\langle Z_{D}, y-y^{*}\\right\\rangle\\right| \\leq 5 \\varepsilon_{N} \\sqrt{(8 d+4) \\log n+(4 d+2) \\log 2+4 d C_{\\log }} \\leq 20 \\varepsilon_{N} C(n)\n$$\n\nWe first bound the term in the exponent $\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}$ where $X_{i}(t)=$ $\\operatorname{pr}_{c_{t} \\mathcal{A}_{i}} X(t)$ and $y^{*} \\in M_{i}^{*}$ we write\n\n$$\n\\begin{aligned}\n& \\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}=\\left\\|X(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2} \\\\\n& \\quad=-c_{t}^{2}\\left\\|y^{*}-G_{i}\\right\\|^{2}+2 c_{t}^{2}\\left\\langle y^{*}-G_{i}, X(0)-G_{i}\\right\\rangle+2 c_{t} \\sigma_{t}\\left\\langle y^{*}-G_{i}, Z_{D}\\right\\rangle\n\\end{aligned}\n$$\n\nCombining (55) and (56), since for all $y^{*} \\in M_{i}^{*}\\left\\|y^{*}-G_{i}\\right\\| \\leq 2 \\varepsilon_{N}<\\left(\\sigma_{t} / c_{t}\\right) n^{-\\gamma}$, with probability at least $1-n^{-2}$ for all $i \\in I_{2}$ and $y^{*} \\in M_{i}^{*}$ we have\n\n$$\n\\begin{aligned}\n& \\left|\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}\\right| \\leq 16 c_{t}^{2} \\varepsilon_{N}^{2}+2 \\sigma_{t} c_{t} \\cdot 4 \\varepsilon_{N} \\cdot 46 C(t)+40 c_{t} \\sigma_{t} \\varepsilon_{N} C(n) \\\\\n& \\quad \\leq 16 c_{t}^{2} \\varepsilon_{N}^{2}+2 \\sigma_{t} c_{t} \\cdot 4 \\varepsilon_{N} \\cdot 46 C(n)+40 \\cdot c_{t} \\sigma_{t} \\cdot \\varepsilon_{N} C(n) \\leq 103 \\sigma_{t}^{2} C(n) n^{-\\gamma} \\lesssim \\sigma_{t}^{2} n^{-\\gamma / 2}\n\\end{aligned}\n$$\n\nfor $n$ large enough, since $C(n)=O(\\log n)$. Therefore since $\\left|1-e^{x}\\right| \\leq 2|x|$ for $x \\leq 1$\n\n$$\n\\begin{aligned}\n& \\frac{\\left|\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}\\right|}{2 \\sigma_{t}^{2}} \\leq \\frac{1}{2} n^{-\\gamma / 2} \\\\\n& \\left|e^{-\\left(\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}}-1\\right| \\leq n^{-\\gamma / 2}\n\\end{aligned}\n$$\n\nNow we can bound $g_{i}\\left(t, X_{i}(t)\\right)$ from above and below. Proposition 27 states that $\\operatorname{Vol} M_{i} \\leq$ $C_{d} \\varepsilon_{N}^{d}$ with probability $1-n^{-2}$ for all $i \\in I_{2}$. We bound $g_{i}\\left(t, X_{i}(t)\\right)$ from above by\n\n$$\n\\begin{aligned}\ng_{i}\\left(t, X_{i}(t)\\right) & =\\int_{M_{j}^{*}} e^{\\left(\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} \\mu_{j}^{*}\\left(d y^{*}\\right) \\\\\n& \\leq \\int_{M_{j}^{*}}\\left(1+n^{-\\gamma / 2}\\right) \\mu_{j}^{*}\\left(d y^{*}\\right)=\\left(1+n^{-\\gamma / 2}\\right) \\int_{M_{j}} \\chi_{i}(y) p(d y) \\leq 2^{d+1} p_{\\max } C_{d} \\varepsilon_{N}^{d}\n\\end{aligned}\n$$\n\nAt the same time, with probability $1-n^{-2}$ for all $i \\in I_{2}$ we bound from below $g_{i}\\left(t, X_{i}(t)\\right)$ by\n\n$$\n\\begin{aligned}\ng_{i}\\left(t, X_{i}(t)\\right) & =\\int_{M_{i}^{*}} e^{\\left(\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{i}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right) \\\\\n& \\geq \\int_{M_{i}^{*}}\\left(1-n^{-\\gamma / 2}\\right) \\mu_{i}^{*}\\left(d y^{*}\\right)=\\left(1-n^{-\\gamma / 2}\\right) \\int_{M_{i}} \\chi_{i}(y) p(d y)\n\\end{aligned}\n$$\n\nBy Proposition 21, $\\left|\\mathcal{Y} \\cap B\\left(G_{i}, 4 \\varepsilon_{N}\\right)\\right| \\leq C_{\\operatorname{dim}} \\log N \\leq C_{\\operatorname{dim}} \\log n$, so for $y \\in B_{M}\\left(G_{i}, \\varepsilon_{N} / 2\\right)$\n\n$$\n\\chi_{i}(y)=\\frac{\\rho\\left(\\left\\|y-G_{i}\\right\\| / \\varepsilon_{N}\\right)}{\\sum_{j=1}^{N} \\rho\\left(\\left\\|y-G_{j}\\right\\| / \\varepsilon_{N}\\right)} \\geq \\frac{1}{\\left|\\mathcal{Y} \\cap B\\left(y, \\varepsilon_{N}\\right)\\right|} \\geq \\frac{1}{C_{\\operatorname{dim}} \\log n}\n$$\n\nsubstituting and applying Proposition 27 again\n\n$$\ng_{i}\\left(t, X_{i}(t)\\right) \\geq\\left(1-n^{-\\gamma / 4}\\right) \\int_{M_{i}} \\chi_{i}(y) p(d y) \\geq \\frac{p_{\\min }}{2 C_{\\operatorname{dim}} \\log n} \\int_{B_{M}\\left(G_{i}, \\varepsilon_{N} / 2\\right)} \\geq \\varepsilon_{N}^{d} 4^{-d} \\frac{p_{\\min }}{2 C_{\\operatorname{dim}} \\log n}\n$$\n\nSince $\\operatorname{diam} M^{*} \\leq 2$, we bound $\\left\\|f_{i}\\right\\|$ by\n\n$$\n\\begin{aligned}\n\\left\\|f_{i}\\left(y, X_{i}(t)\\right)\\right\\| & =\\left\\|\\int_{M_{i}^{*}} c_{t} y^{*} e^{\\left(\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} \\mu_{i}^{*}\\left(d y^{*}\\right)\\right\\| \\\\\n& \\leq 2 g_{i}\\left(t, X_{i}(t)\\right) \\leq c_{t} 2^{d+2} p_{\\max } C_{d} \\varepsilon_{N}^{d+1}\n\\end{aligned}\n$$\n\nSince $(\\log n)^{1 / d} n^{-\\frac{d-2}{d} \\frac{1}{2 \\alpha+d}} \\geq \\varepsilon_{N} \\geq n^{-2 \\gamma} n^{-\\frac{1}{2 \\alpha+d}}$, for $n$ large enough, with probability $1-n^{-2}$ for all $i \\in I_{2}$\n\n$$\n\\begin{aligned}\ng_{i}\\left(t, X_{i}(t)\\right) & \\geq \\varepsilon_{N}^{d} 4^{-d} \\frac{p_{\\min }}{2 C_{\\operatorname{dim}} \\log n} \\gtrsim \\frac{\\varepsilon_{N}^{d}}{\\log n} \\geq(\\log n)^{-1} n^{-2 \\gamma d} n^{-\\frac{d}{2 \\alpha+d}} \\\\\n\\left\\|f_{i}\\left(t, X_{i}(t)\\right)\\right\\| & \\leq c_{t} 2^{d+2} p_{\\max } C_{d} \\varepsilon_{N}^{d} \\lesssim \\varepsilon_{N}^{d}<1\n\\end{aligned}\n$$\n\nNote that the constants depend only on $C_{\\text {dim }}, C_{\\text {log }}$ and $d$. We show below that if $\\hat{g}_{i}, \\hat{f}_{i}, \\hat{e}_{i}$ are such that with probability at least $1-n^{-2}$ for all $i \\in I_{2}$\n\n$$\n\\left|\\hat{g}_{i}\\left(t, X_{i}(t)\\right)-g_{i}\\left(t, X_{i}(t)\\right)\\right| \\leq n^{-2} n^{-2 \\gamma d},\\left\\|\\hat{e}_{i}\\left(t, X_{i}(t)\\right)-e_{i}\\left(t, X_{i}(t)\\right)\\right\\| \\leq n^{-1},\\left\\|\\hat{e}_{i}\\right\\| \\leq 1\n$$\n\nwhere $e_{i}=f_{i} / g_{i}$. Note that $\\hat{g}_{i}\\left(t, X_{i}(t)\\right) \\geq g_{i}\\left(t, X_{i}(t)\\right) / 2$ and $\\left\\|e_{i}\\right\\| \\leq 1$. Then the approximation\n\n$$\n\\hat{e}^{*}(t, x):=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\sum_{i} \\rho_{i}(t, x) h_{i}(t, x) \\hat{g}_{i}(t, x) \\hat{e}_{i}(t, x)}{\\sum_{i} \\rho_{i}(t, x) h_{i}(t, x) \\hat{g}_{i}(t, x)}\n$$\n\nverifies with probability greater than $1-n^{-2}$\n\n$$\n\\begin{aligned}\n& \\frac{\\sigma_{t}^{2}}{c_{t}}\\left\\|e_{t r}^{*}(t, X(t))-\\hat{e}^{*}(t, X(t))\\right\\| \\\\\n& =\\left\\|\\frac{\\sum_{i} \\rho_{i}(t, x) h_{i}(t, x) \\hat{g}_{i}(t, x) \\hat{e}_{i}(t, x)}{\\sum_{i} \\rho_{i}(t, x) h_{i}(t, x) \\hat{g}_{i}(t, x)}-\\frac{\\sum_{i} \\rho_{i}(t, x) h_{i}(t, x) g_{i}(t, x) e_{i}(t, x)}{\\sum_{i} \\rho_{i}(t, x) h_{i}(t, x) g_{i}(t, x)}\\right\\| \\\\\n& \\leq\\left\\|\\frac{\\sum \\rho_{i} h_{i}\\left(\\hat{g}_{i} \\hat{e}_{i}-g_{i} e_{i}\\right)}{\\sum \\rho_{i} h_{i} \\hat{g}_{i}}\\right\\|+\\left\\|\\frac{\\sum \\rho_{i} h_{i} g_{i} e_{i}}{\\sum \\rho_{i} h_{i} g_{i}}\\left(\\frac{\\sum \\rho_{i} h_{i} g_{i}}{\\sum \\rho_{i} h_{i} g_{i}}-1\\right)\\right\\| \\\\\n& \\leq \\frac{\\sum \\rho_{i} h_{i} \\hat{g}_{i}\\left\\|\\hat{e}_{i}-e_{i}\\right\\|}{\\sum \\rho_{i} h_{i} \\hat{g}_{i}}+\\frac{\\sum \\rho_{i} h_{i}\\left\\|e_{i}\\right\\|\\left|\\hat{g}_{i}-g_{i}\\right|}{\\sum \\rho_{i} h_{i} \\hat{g}_{i}}+\\left\\|e_{t r}^{*}\\right\\| \\frac{\\sum \\rho_{i} h_{i}\\left|g_{i}-\\hat{g}_{i}\\right|}{\\sum \\rho_{i} h_{i} \\hat{g}_{i}} \\\\\n& \\leq \\sup _{i \\in I_{2}}\\left\\|\\hat{e}_{i}-e_{i}\\right\\|+2 \\frac{\\sup _{i \\in I_{2}}\\left|\\hat{g}_{i}-g_{i}\\right|}{\\inf _{i \\in I_{2}} \\hat{g}_{i}} \\leq n^{-1}+2 \\frac{n^{-2}}{n} \\leq 3 n^{-1}\n\\end{aligned}\n$$\n\nFinally if we define $\\hat{s}^{*}(t, x)=\\hat{e}^{*}(t, x)-\\frac{x}{\\sigma_{t}^{2}}$ and use that $\\left\\|e^{*}\\right\\|,\\left\\|\\hat{e}^{*}\\right\\| \\leq 1$ a.s. we get\n\n$$\n\\mathbb{E}\\left\\|\\hat{s}^{*}(t, X(t))-s^{*}(t, X(t))\\right\\|^{2} \\leq \\frac{4}{\\sigma_{t}^{4}}\\left(n^{-2}+n^{-2}\\right) \\leq \\frac{8}{\\sigma_{t}^{4}} n^{-2}\n$$\n\nAfter integration this results in\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|\\hat{s}_{\\mathrm{clip}}^{*}(t, X(t))-s_{t r}^{*}(t, X(t))\\right\\|^{2} d t \\lesssim \\sigma_{T_{k}}^{-2} n^{-2} \\int_{T_{k}}^{T_{k+1}} \\frac{1}{\\sigma_{t}^{2}} d t \\lesssim \\sigma_{T_{k}}^{-2} n^{-2} \\log n\n$$", "tables": {}, "images": {}}, {"section_id": 28, "text": "# D.1.3 Neural Networks Approximation \n\nThe goal of this section is to construct neural network functions $\\hat{e}_{i}$ and $\\hat{g}_{i}$ that satisfy (60) for all $i \\in I_{2}$ with probability at least $1-n^{-2}$. Since $\\hat{e}_{i}$ is an approximation of $e_{i}=f_{i} / g_{i}$, instead of directly constructing $\\hat{e}_{i}$ we focus on approximation of $f_{i}$ with $\\hat{f}_{i}$, by (58) it is enough to guarantee $\\left\\|\\hat{f}_{i}-f_{i}\\right\\| \\leq n^{-2} n^{-2 \\gamma d}$.\n\nWe first construct polynomial functions in $X_{i}(t)$ and then we use Oko et al. (2023) to approximate the polynomial functions by neural networks. Since $M_{i}^{*}=\\Phi_{i}^{*}\\left(B_{d}\\left(0, \\varepsilon_{N}\\right)\\right)$, using the change of variable $y^{*}=\\Phi_{i}^{*}(z)$, we have\n\n$$\n\\begin{aligned}\n& g_{i}(t, x)=\\int_{B_{d}\\left(0, \\varepsilon_{N}\\right)} e^{\\left(\\left\\|x_{i}-c_{t} G_{i}\\right\\|^{2}-\\left\\|x_{i}-c_{t} \\Phi_{i}^{*}(z)\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} p_{i}(z) d z \\\\\n& f_{i}(t, x)=\\int_{B_{d}\\left(0, \\varepsilon_{N}\\right)} \\Phi_{i}^{*}(z) e^{\\left(\\left\\|x_{i}-c_{t} G_{i}\\right\\|^{2}-\\left\\|x_{i}-c_{t} \\Phi_{i}^{*}(z)\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}} p_{i}(z) d z\n\\end{aligned}\n$$\n\nIn (57) we have shown that with probability $1-n^{-2}$ for all $i \\in I_{2}$ and all $y \\in M_{i}^{*}$\n\n$$\n\\frac{\\left.\\left|\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}\\right|\\right.}{2 \\sigma_{t}^{2}} \\leq \\frac{1}{2} n^{-\\gamma / 2}\n$$\n\nTherefore the Taylor approximation of $e^{n}$ near 0 leads to\n\n$$\ne^{\\left(\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}\\right) / 2 \\sigma_{t}^{2}}=\\sum_{k=0}^{\\lceil 16 \\gamma^{-1}\\rceil} \\frac{\\left(\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} y^{*}\\right\\|^{2}\\right)^{k}}{k!2^{k} \\sigma_{t}^{2 k}}+O\\left(n^{-4}\\right)\n$$\n\nand since $\\operatorname{diam} M^{*} \\leq 2$\n\n$$\n\\begin{aligned}\n\\hat{g}_{i}\\left(t, X_{i}(t)\\right) & :=\\int_{B_{d}\\left(0, \\varepsilon_{N}\\right)}\\left(\\sum_{k=0}^{\\lceil 16 \\gamma^{-1}\\rceil} \\sigma_{t}^{-2 k} \\frac{\\left(\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} \\Phi_{i}^{*}(z)\\right\\|^{2}\\right)^{k}}{k!2^{k}}\\right) p_{i}(z) d z \\\\\n& =g_{i}\\left(t, X_{i}(t)\\right)+O\\left(n^{-4}\\right) \\\\\n\\hat{f}_{i}\\left(t, X_{i}(t)\\right) & :=\\int_{B_{d}\\left(0, \\varepsilon_{N}\\right)} \\Phi_{i}^{*}(z)\\left(\\sum_{k=0}^{\\lceil 16 \\gamma^{-1}\\rceil} \\sigma_{t}^{-2 k} \\frac{\\left(\\left\\|X_{i}(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X_{i}(t)-c_{t} \\Phi_{i}^{*}(z)\\right\\|^{2}\\right)^{k}}{k!2^{k}}\\right) p_{i}(z) d z \\\\\n& =f_{i}\\left(t, X_{i}(t)\\right)+O\\left(n^{-4}\\right)\n\\end{aligned}\n$$\n\nBy construction $\\Phi_{i}^{*}$ is polynomial $\\Phi_{i}^{*}(z)=G_{i}+P_{i}^{*} z+\\sum a_{i, S}^{*} z^{S}$, where $z \\in \\mathbb{R}^{d}, \\operatorname{Im} P_{i}^{*} \\subset$ $\\mathcal{H}_{i}, a_{i, S}^{*} \\in \\mathcal{H}_{i}$, therefore introducing a linear isometry $P_{\\mathcal{H}_{i}}: \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}} \\rightarrow \\mathcal{H}_{i}$ that identifies $\\mathcal{H}_{i}$ with $\\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$ we represent\n\n$$\n\\Phi_{i}^{*}(z)=G_{i}+P_{\\mathcal{H}_{i}} \\widetilde{\\Phi}_{i}^{*}(z)\n$$\n\nwhere $\\widetilde{\\Phi}_{i}^{*}(z): \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$ is a polynomial functions\n\n$$\n\\widetilde{\\Phi}_{i}^{*}(z)=P_{\\mathcal{H}_{i}}^{T} P_{i}^{*} z+\\sum P_{\\mathcal{H}_{i}}^{T} a_{i, S}^{*} z^{S}\n$$\n\nof degree $\\beta-1$ in $d$ variables with coefficients belonging to $\\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$.\nFinally denoting $\\tilde{X}_{i}(t)=P_{\\mathcal{H}_{i}}^{T}\\left(X_{i}(t)-c_{t} G_{i}\\right)$ we introduce\n\n$$\n\\begin{aligned}\n& \\tilde{g}_{i}\\left(t, \\widetilde{X}_{i}(t)\\right):=\\int_{B_{d}(0, \\varepsilon)}\\left(\\sum_{k=0}^{\\lceil 16 \\gamma^{-1}\\rceil} \\sigma_{t}^{-2 k} \\frac{\\left(\\left\\|\\widetilde{X}_{i}(t)\\right\\|^{2}-\\left\\|\\tilde{X}_{i}(t)-c_{t} \\widetilde{\\Phi}_{i}^{*}(z)\\right\\|^{2}\\right)^{k}}{k!2^{k}}\\right) p_{i}(z) d z \\\\\n& \\tilde{f}_{i}\\left(t, \\widetilde{X}_{i}(t)\\right):=\\int_{B_{d}(0, \\varepsilon)} \\widetilde{\\Phi}_{i}^{*}(z)\\left(\\sum_{k=0}^{\\lceil 16 \\gamma^{-1}\\rceil} \\sigma_{t}^{-2 k} \\frac{\\left(\\left\\|\\tilde{X}_{i}(t)\\right\\|^{2}-\\left\\|\\tilde{X}_{i}(t)-c_{t} \\widetilde{\\Phi}_{i}^{*}(z)\\right\\|^{2}\\right)^{k}}{k!2^{k}}\\right) p_{i}(z) d z\n\\end{aligned}\n$$\n\nExpanding brackets we represent it as a polynomial in $\\tilde{X}_{i}(t), \\sigma_{t}^{-2}$ of degree less than $64\\left\\lceil\\gamma^{-1}\\right\\rceil$ with coefficients being integrals depending on unknown coefficients of $\\tilde{\\Phi}_{i}^{*}$ and density\n\n$p_{i}$ that itself depends on samples $\\mathcal{G} \\subset \\mathcal{Y}$ used to construct the measure, and $\\mu$ - the density. Equivalently\n\n$$\n\\begin{aligned}\n& \\tilde{g}_{i}\\left(t, X_{i}(t)\\right)=\\sum_{|S|<40\\left(\\gamma^{-1}+1\\right)}\\left(\\widetilde{X}_{i}(t), \\sigma_{t}^{-1}\\right)^{S} C_{S, g_{i}}(\\mathcal{Y}, \\mu) \\\\\n& \\tilde{f}_{i}\\left(t, X_{i}(t)\\right)=\\sum_{|S|<40\\left(\\gamma^{-1}+1\\right)}\\left(\\widetilde{X}_{i}(t), \\sigma_{t}^{-1}\\right)^{S} C_{S, f_{i}}(\\mathcal{Y}, \\mu)\n\\end{aligned}\n$$\n\nwhere $C_{S, g_{i}}(\\mathcal{Y}, \\mu)$ are scalars and $C_{S, f_{i}}(\\mathcal{Y}, \\mu) \\in \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$ and $S \\in \\mathbb{N}^{\\operatorname{dim} \\mathcal{H}_{i}}$\nWe treat the coefficients $C_{S, g_{i}}(\\mathcal{Y}, \\mu), C_{S, f_{i}}(\\mathcal{Y}, \\mu)$ as unknown weights of the neural network that require optimization. Since $\\operatorname{dim} \\mathcal{H}_{i} \\leq C_{\\text {dim }} \\log n$, the polynomial is no more than in $C_{\\text {dim }} \\log n+1$ variables, so the number of different monomials is bounded by $\\left(2 C_{\\operatorname{dim}} \\log n\\right)^{64\\left\\lceil\\gamma^{-1}\\right\\rceil}$. Therefore the number of weights required to parameterize all the coefficients $C_{S, g_{i}}(\\mathcal{Y}, \\mu), C_{S, f_{i}}(\\mathcal{Y}, \\mu)$ is bounded by $\\left(\\operatorname{dim} \\mathcal{H}_{i}+1\\right)\\left(2 C_{\\operatorname{dim}} \\log n\\right)^{64\\left\\lceil\\gamma^{-1}\\right\\rceil} \\leq\\left(2 C_{\\operatorname{dim}} \\log n\\right)^{64\\left\\lceil\\gamma^{-1}\\right\\rceil+1}=$ $\\operatorname{polylog} n$.\n\nSummarizing $\\tilde{g}_{i}$ and $\\tilde{f}_{i}$ are both represented as polynomials in $O(\\log n)$ variables, of constant degree, and coefficients belonging to polylog $n$ subspace. Therefore, using Lemmas 3739 both $\\tilde{g}_{i}\\left(t, x_{i}\\right), \\tilde{f}_{i}(t, x)$ can be approximated by a neural networks $\\phi_{g_{i}}, \\phi_{f_{i}} \\in \\Psi(L, W, S, B)$, where $L, W, S=\\operatorname{polylog} n$ and $B=e^{\\operatorname{polylog} n}$, with an error less than $n^{-2} n^{-2 \\gamma d}$ (actually less than $n^{-C}$ for any $C>0$ by changing $\\gamma$ ). Finally, applying Lemma 39 we approximate $\\tilde{e}_{i}:=\\tilde{f}_{i}(t, x) / \\tilde{g}_{i}\\left(t, x_{i}\\right)$ with a neural network $\\phi_{e_{i}} \\in \\Psi(L, W, S, B)$ of the same type, with an error less than $2 n^{-2} n^{-2 \\gamma d}$.\n\nApplying argument for all $i \\leq N$ and combining (61), (53) and (49) we obtain that for $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}}$ there is a network $\\phi \\in \\mathcal{S}_{k}$ s.t.\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\sigma_{t}^{2} \\mathbb{E}\\|\\phi(t, X(t))-s(t, X(t))\\|^{2} d t \\lesssim \\sigma_{T_{k}}^{-2} n^{-\\frac{2(\\alpha+1)}{2 \\alpha+d}}(\\log n)^{\\frac{2 \\beta}{d}}\n$$\n\nD. 2 Case $\\underline{T} \\leq T_{k} \\leq n^{-\\frac{2}{2 \\alpha+d}}$", "tables": {}, "images": {}}, {"section_id": 29, "text": "# D.2.1 SUMMARY \n\nRecall that $\\underline{T} \\geq n^{\\gamma} n^{-\\frac{2(\\alpha+1)}{2 \\alpha+d}}$; we choose $N=n^{-\\gamma d} n^{\\frac{d}{2 \\alpha+d}}$. Then $N^{-\\frac{\\beta}{d}}=n^{-\\frac{\\beta}{2 \\alpha+d}} n^{\\beta \\gamma} \\leq r_{0}$, and\n\n$$\n\\forall t \\in\\left[T_{k}, T_{k+1}\\right], \\quad \\sigma_{t} / c_{t} \\leq 2 \\sqrt{t} \\leq n^{-\\gamma} \\varepsilon_{N} \\asymp n^{-1 /(2 \\alpha+d)}(\\log n)^{1 / d}\n$$\n\nSimilarly to the case $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}}$, as a first step, we localize the score function and construct its truncated version $s_{t r}$. As before, we introduce $\\rho_{i}(t, X(t))$ defined in (48), with this time $C\\left(T_{k}, n\\right)=6 \\varepsilon_{N}^{2}$. In this section we define\n\n$$\nC(n)=\\sqrt{d\\left(\\log n+4 C_{\\log }\\right)+4 \\log n} \\asymp \\log n\n$$\n\nWe show that with probability at least $1-n^{-2}$ if $\\rho_{i}(t, X(t)) \\geq 0$ then $i \\in I_{2}$ where\n\n$$\nI_{2}=\\left\\{i \\leq N:\\left\\|X(0)-G_{i}\\right\\| \\leq 4 \\varepsilon_{N}\\right\\}\n$$\n\nIn contrast to the case $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+3}}$, here we have $\\left(\\sigma_{t} / c_{t}\\right)=o\\left(\\varepsilon_{N}\\right)$. On the one hand, Theorem 12 implies that for any $i \\in I_{2}$ the conditional measure $\\mu(y \\mid t, X(t))$ is concentrated on $\\Phi_{i}\\left(G_{i}, 5 \\varepsilon_{N}\\right)$. On the other hand, this means that the $O\\left(\\varepsilon_{N}\\right)$ scale is too coarse, and we need to better localize the support.\n\nWe solve this problem, by considering the projection of $X(t)$ onto $T_{G_{i}} \\Phi_{i}^{*}\\left(0, \\varepsilon_{N}\\right)$, namely\n\n$$\nz_{i}^{*}=z_{i}^{*}(X(t))=\\left(P_{i}^{*}\\right)^{T}\\left(X(t)-c_{t} G_{i}\\right) / c_{t}\n$$\n\nand by defining the submanifolds $M_{i}$ and their counterparts $M_{i}^{*}$ as\n\n$$\nM_{i}:=\\Phi_{i}\\left(B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)\\right), \\quad M_{i}^{*}:=\\Phi_{i}^{*}\\left(B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)\\right)\n$$\n\nAs we show in Appendix D.2.2 with probability at least $1-O\\left(n^{-2}\\right)$ for all $i \\in I_{2}$ the local score $s_{i}(t, X(t))$ verifies\n\n$$\n\\left\\|s_{i}(t, X(t))-s(t, X(t))\\right\\| \\leq 2 \\frac{c_{t}}{\\sigma_{t}^{2}} n^{-2}, \\quad \\text { where } s_{i}(t, X(t)):=\\frac{1}{\\sigma_{t}^{2}} \\frac{\\int_{M_{i}}(y-X(t)) p(y \\mid t, X(t))}{p\\left(M_{i} \\mid t, X(t)\\right)}\n$$\n\nSo, if we define\n\n$$\ns_{t r}(t, X(t))=\\frac{\\sum_{i} \\rho_{i}(t, X(t)) s_{i}(t, X(t))}{\\sum_{i} \\rho_{i}(t, X(t))}\n$$\n\nthen we can prove that\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|s_{t r}(t, X(t))-s(t, X(t))\\right\\|^{2} d t \\lesssim n^{-2} \\sigma_{T_{k}}^{-2}\n$$\n\nThe next step is the replacement of $s_{t r}$ with its polynomial counterpart $s_{t r}^{*}$, which we define by constructing $s_{i}^{*}$. Denoting by $p_{i}(z)=\\left|\\nabla \\Phi_{i} \\nabla^{T} \\Phi_{i}\\right|^{-1} p\\left(\\Phi_{i}(z)\\right)$ the density of pushforward of $\\mu$ restricted on $M_{i}$ on the tangent space $T_{G_{i}} M_{i}$ we represent\n\n$$\ns_{i}(t, X(t))=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} \\Phi_{i}(z) e^{-\\left\\|\\Phi_{i}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}(z) d z}{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} e^{-\\left\\|\\Phi_{i}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}(z) d z}-\\frac{X(t)}{\\sigma_{t}^{2}}\n$$\n\nand approximate it by substituting $\\Phi_{i}$ and $p_{i}$ with their polynomial approximations $\\Phi_{i}^{*}, p_{i}^{*}$ giving us\n\n$$\ns_{i}^{*}(t, X(t)):=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} \\Phi_{i}^{*}(z) e^{-\\left\\|\\Phi_{i}^{*}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z}{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} e^{-\\left\\|\\Phi_{i}^{*}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z}-\\frac{X(t)}{\\sigma_{t}^{2}}\n$$\n\nDefining $s_{t r}^{*}$ as a mixture of $s_{i}^{*}$\n\n$$\ns_{t r}^{*}(t, X(t)):=\\frac{\\sum_{i} \\rho_{i}(t, X(t)) s_{i}^{*}(t, X(t))}{\\sum_{i} \\rho_{i}(t, X(t))}\n$$\n\nthe direct calculations performed in Appendix D.2.3 show\n\n$$\n\\begin{aligned}\n& \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|s_{t r}^{*}(t, X(t))-s_{t r}(t, X(t))\\right\\|^{2} d t=O\\left(\\sigma_{T_{k}}^{-2} \\varepsilon_{n}^{2 \\beta}(\\log n)^{2}+n^{2 \\gamma \\alpha} n^{-\\frac{2 \\alpha}{2 \\alpha+3}}(\\log n)^{2 \\alpha / d+1}\\right) \\\\\n&=O\\left(T_{k}^{-1} n^{2 \\gamma \\beta} n^{-\\frac{2 \\beta}{2 \\alpha+3}} \\log n^{2}+n^{2 \\gamma \\alpha} n^{-\\frac{2 \\alpha}{2 \\alpha+3}}(\\log n)^{\\frac{2 \\alpha+d}{d}}\\right)\n\\end{aligned}\n$$\n\nNext, we introduce $e_{i}^{*}(x)=s_{i}^{*}(x)+\\frac{x}{\\sigma_{t}^{*}}$ and represent it as $e_{i}^{*}=f_{i} / g_{i}$, where $f_{i}, g_{i}$ are regularized such that they can be efficiently approximated using polynomials.\n\nAs in the case $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+3}}$ the main challenge is the term $\\left\\|X(t)-c_{t} \\Phi_{i}^{*}(z)\\right\\|$ in the exponent, and which scales as $O(D)$. To tackle this first note that by definition $M_{i}^{*}$ is centered around $m_{i}^{*}:=\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)$. We approximate $M_{i}^{*}$ by the tangent space $T_{m_{i}^{*}} M_{i}^{*}$ and linearize $\\Phi_{i}^{*}$ as\n\n$$\n\\Phi_{i}^{*}(z)=\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)+\\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)+R_{i}\\left(z, z_{i}^{*}\\right)\n$$\n\nand we finally define the projection of $x$ onto $T_{m_{i}^{*}} M_{i}^{*}$\n\n$$\nx_{i}^{\\mathrm{pr}}:=\\operatorname{pr}_{z_{i}^{*}}\\left(x-c_{t} m_{i}^{*}\\right)\n$$\n\nNote that $x_{i}^{\\text {pr }}$ is not the same as $x_{i}=\\operatorname{pr}_{c_{t} \\mathcal{H}_{i}} x$.\nUsing that $\\left(x-c_{t} m_{i}^{*}\\right)-x_{i}^{\\text {pr }} \\perp T_{m_{i}^{*}}^{*} M_{i}^{*}$ and $x-x_{i} \\perp \\mathcal{H}_{i} \\supseteq T_{m_{i}^{*}}^{*} M_{i}^{*}$ we extract from $\\left\\|X(t)-c_{t} \\Phi_{i}^{*}(z)\\right\\|$ the term $\\|X(t)-X(t)_{i}^{\\text {pr }}\\|^{2}$ which does not depend on $z$ and which gives the order $O(D)$ in $\\left\\|X(t)-c_{t} \\Phi_{i}^{*}(z)\\right\\|$ and we define\n\n$$\n\\begin{aligned}\ng_{i}(t, x):= & \\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} e^{-\\left\\|x_{i}^{\\mathrm{pr}}-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\\\\n& e^{\\left(-c_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}+2 c_{t}\\left(x_{i}-\\Phi_{i}^{*}\\left(z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right)\\right) / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z \\\\\nf_{i}(t, x):= & \\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} \\Phi_{i}^{*}(z) e^{-\\left\\|x_{i}^{\\mathrm{pr}}-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2} / 2 \\sigma_{t}^{2}} \\\\\n& e^{\\left(-c_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}+2 c_{t}\\left(x_{i}-\\Phi_{i}^{*}\\left(z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right)\\right) / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z\n\\end{aligned}\n$$\n\nIn Appendix D.2.4 we verify that $e_{i}^{*}=f_{i} / g_{i}$ and show that with probability at least $1-n^{-2}$ for all $i \\in I_{2}$\n\n$$\n\\begin{aligned}\n& \\left\\|X_{i}^{\\mathrm{pr}}(t)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2} \\leq 676 \\sigma_{t}^{2} C^{2}(n) \\\\\n& c_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}-2 c_{t}\\left\\langle X(t)-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle=O\\left(\\sigma_{t}^{3}(d \\log n)^{2}\\right)\n\\end{aligned}\n$$\n\nimplying $g_{i}(t, X(t)) \\geq n^{-2688 d}$. So if we construct $\\hat{f}_{i}, \\hat{g}_{i}$ satisfying with probability at least $1-n^{-2}$ for all $i \\in I_{2}\\left|g_{i}(t, X(t))-\\hat{g}_{i}(t, X(t))\\right|,\\left\\|f_{i}(t, X(t))-\\hat{f}_{i}(t, X(t))\\right\\| \\leq n^{-2370 d}$, then since $\\left\\|e_{i}\\right\\| \\leq 2$ by construction\n\n$$\n\\hat{e}_{i}(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\hat{f}_{i}(t, x)}{\\hat{g}_{i}(t, x)}, \\quad \\hat{e}_{t r}(t, x)=\\frac{\\sum_{i=1}^{N} \\rho_{i}(t, x) \\mathbb{1}_{\\left\\|\\hat{e}_{i}(t, x)\\right\\| \\leq 2 \\sigma_{t}^{-2}} \\hat{e}_{i}(t, x)}{\\sum_{i=1}^{N} \\rho_{i}(t, x)}, \\quad \\hat{s}(t, x)=\\hat{e}(t, x)-\\frac{x}{\\sigma_{t}^{2}}\n$$\n\nsatisfy\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|\\hat{s}(t, X(t))-s_{t r}^{*}(t, X(t))\\|^{2} d t \\leq n^{-2}\n$$\n\nFinally, in Appendix D.2.5 using (72) we perform the Taylor expansion of the exponents in (71) and show that both $\\hat{f}_{i}$ and $\\hat{g}_{i}$ satisfying (80) can be obtained as a composition of polynomials of polylog size, which can be approximated by neural networks also of polylog size.", "tables": {}, "images": {}}, {"section_id": 30, "text": "# D.2.2 Score Function Localization \n\nIn this section we show that $s(t, X(t))$ is close to $s_{t r}(t, X(t))$ which is defined in (65).\nSince $\\mathcal{G}$ is a $\\varepsilon_{N} / 2$ dense subset of $M$, using Lemma 13, with probability $1-n^{-2}$ for all $i \\leq N$\n\n$$\n\\left\\|X(0)-G_{i}\\right\\|^{2} \\leq \\frac{9}{4} \\varepsilon_{N}^{2}+2 c_{t}^{-2}\\left(\\left\\|X(t)-c_{t} G_{i}\\right\\|^{2}-\\left\\|X(t)-c_{t} G_{\\min }(t)\\right\\|^{2}\\right)+128\\left(\\sigma_{t} / c_{t}\\right)^{2} C^{2}(n)\n$$\n\nwhere $C(n)$ is defined in Appendix D.2.1. Also, since $t \\leq 2 T_{k} \\leq 2 n^{-2 /(2 \\alpha+d)}, \\varepsilon_{N} \\geq$ $n^{\\gamma} n^{-\\frac{1}{2 \\alpha+d}}>>\\left(\\sigma_{t} / c_{t}\\right) C(n)$. Hence if $\\rho_{i}(t, X(t))>0$, necessarily\n\n$$\n\\left\\|X(0)-G_{i}\\right\\|^{2} \\leq \\frac{9}{4} \\varepsilon_{N}^{2}+8 c_{t}^{-2} \\varepsilon_{N}^{2}+o\\left(\\varepsilon_{N}^{2}\\right) \\leq \\frac{45}{4} \\varepsilon_{N}^{2}\n$$\n\nwhich implies that if $\\rho_{i}(t, X(t))>0$ then $i \\in I_{2}$ with $I_{2}=\\left\\{i \\leq N:\\left\\|X(0)-G_{i}\\right\\| \\leq 4 \\varepsilon_{N}\\right\\}$.\n\nApplying Theorem 12 with probability at least $1-n^{-2}$ for all $i \\in I_{2}$\n$\\int_{\\Phi_{i}\\left(B_{d}\\left(0,5 \\varepsilon_{N}\\right)\\right)} p(y \\mid t, X(t)) \\geq \\int_{B_{M}\\left(G_{i}, 5 \\varepsilon_{N}\\right)} p(y \\mid t, X(t)) \\geq \\int_{B_{M}\\left(X(0), 2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} p(y \\mid t, X(t)) \\geq 1-n^{-2}$,\nsince $2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n) \\leq \\varepsilon_{N}$ and $\\Phi_{i}$ is an inverse to a projection map. We also have that with probability $1-n^{-2}$ for all $i \\leq N$ if $i \\in I_{2}$ then $B_{M}\\left(X(0), 2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right) \\subset$ $\\Phi_{i}\\left(B_{d}\\left(0,5 \\varepsilon_{N}\\right)\\right)$.\n\nRecall from (63) that $z_{i}^{*}=z_{i}^{*}(X(t))=\\left(P_{i}^{*}\\right)^{T}\\left(X(t)-c_{t} G_{i}\\right) / c_{t}$ is the normalized projection onto $T_{G_{i}} \\Phi_{i}^{*}\\left(B_{d}\\left(0,5 \\varepsilon_{N}\\right)\\right)$ of $X(t)$ and define $m_{i}=m_{i}(X(t))=\\Phi_{i}\\left(z_{i}^{*}\\right), m_{i}^{*}=m_{i}^{*}(X(t))=$ $\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)$. Note that since $\\operatorname{Im} P^{*} \\subset \\mathcal{H}_{i}$ we have for all $x, z_{i}^{*}(x)=z_{i}^{*}\\left(x_{i}\\right)$, where $x_{i}=\\operatorname{pr}_{\\mathcal{H}_{i}} x$.\n\nAlso, from Proposition 42, with $\\delta=n^{-2}$, for all $i$ such that $X(0) \\in \\Phi_{i}\\left(B_{d}\\left(0,5 \\varepsilon_{N}\\right)\\right)$\n\n$$\n\\mathbb{P}\\left(\\left\\|m_{i}(X(t))-X(0)\\right\\| \\leq O\\left(\\varepsilon_{N}^{\\beta}\\right)+4\\left(\\sigma_{t} / c_{t}\\right) C(n)\\right) \\geq 1-n^{-2}\n$$\n\nso that\n\n$$\nB_{M}\\left(X(0), 2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right) \\subset B_{M}\\left(m_{i}(X(t)), 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)\n$$\n\nWhile, since $z_{i}^{*}=\\Phi_{i}^{-1}\\left(m_{i}(X(t))\\right)-\\Phi_{i}^{-1}\\left(G_{i}\\right)$\n\n$$\n\\left\\|z_{i}^{*}\\right\\| \\leq\\left\\|m_{i}(X(t))-G_{i}\\right\\| \\leq\\left\\|m_{i}(X(t))-X(0)\\right\\|+\\left\\|X(0)-G_{i}\\right\\| \\leq \\varepsilon_{N}+5 \\varepsilon_{N}\n$$\n\nand\n\n$$\nB_{M}\\left(X(0), 2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right) \\subset \\Phi_{i}\\left(B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)\\right) \\subset \\Phi_{i}\\left(B_{d}\\left(0,7 \\varepsilon_{N}\\right)\\right)\n$$\n\nHence since $M_{i}:=\\Phi_{i}\\left(B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)\\right)$, we obtain that with probability greater than $1-n^{-2}$, for all $i \\in I_{2} p\\left(M_{i} \\mid t, X(t)\\right) \\geq 1-n^{-2}$ and using the definition (67) of $s_{i}(t, X(t))$ together with $e_{i}(t, X(t))=s_{i}(t, X(t))+X(t) / \\sigma_{t}^{2}$ we have that with probability at least $1-n^{-2}$, for all $i \\in I_{2}$\n\n$$\n\\left\\|e_{i}(t, X(t)) p\\left(M_{i} \\mid t, X(t)\\right)-e(t, X(t))\\right\\|=\\frac{c_{t}}{\\sigma_{t}^{2}}\\left\\|\\int_{M \\backslash M_{i}} y p(y \\mid t, X(t))\\right\\| \\leq \\frac{c_{t}}{\\sigma_{t}^{2}} n^{-2}\n$$\n\nsince $\\operatorname{diam} M \\leq 1$. Also, since $\\left\\|s_{i}(t, X(t))-s(t, X(t))\\right\\|=\\left\\|e_{i}(t, X(t))-e(t, X(t))\\right\\|$, we bound\n\n$$\n\\begin{aligned}\n& \\left\\|s_{i}(t, X(t))-s(t, X(t))\\right\\| \\\\\n\\leq & \\left\\|e_{i}(t, X(t))\\left(1-p\\left(M_{i} \\mid t, X(t)\\right)\\right)\\right\\|+\\left\\|e_{i}(t, X(t)) p\\left(M_{i} \\mid t, X(t)\\right)-e(t, X(t))\\right\\| \\leq 2 \\frac{c_{t}}{\\sigma_{t}^{2}} n^{-2}\n\\end{aligned}\n$$\n\nwith probability at least $1-n^{-2}$, for all $i \\in I_{2}$.\nFinally, recall that $s_{t r}(t, X(t))=\\sum_{i} \\rho_{i}(t, X(t)) s_{i}(t, X(t)) /\\left(\\sum_{i} \\rho_{i}(t, X(t))\\right)$, is a mixture of $s_{i}$ with weights $\\rho_{i}(t, X(t)) \\neq 0$ only if $i \\in I_{2}$. Therefore with probability at least $1-n^{-2}$\n\n$$\n\\left\\|s_{t r}(t, X(t))-s(t, X(t))\\right\\| \\leq 2 \\frac{c_{t}}{\\sigma_{t}^{2}} n^{-2}\n$$\n\nAlmost surely $\\left\\|e_{i}(t, X(t))-e(t, X(t))\\right\\| \\leq\\left\\|e_{i}(t, X(t))\\right\\|+\\|e(t, X(t))\\| \\leq 2 \\frac{c_{t}}{\\sigma_{t}^{2}}$, so taking the expectation\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left\\|s_{t r}(t, X(t))-s(t, X(t))\\right\\|^{2} & \\leq 8 \\frac{c_{t}^{2}}{\\sigma_{t}^{4}} n^{-4} \\quad \\text { and } \\\\\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|s_{t r}(t, X(t))-s(t, X(t))\\right\\|^{2} d t & \\leq 8 \\frac{n^{-2}}{\\sigma_{T_{k}}^{2}} \\int_{T_{k}}^{T_{k+1}} \\frac{1}{t} d t \\leq 8 \\frac{n^{-2}}{\\sigma_{T_{k}}^{2}} \\log 2\n\\end{aligned}\n$$\n\nwhich proves (66).", "tables": {}, "images": {}}, {"section_id": 31, "text": "# D.2.3 Polynomial Approximation of Measure \n\nThe main goal of this section is to substitute $s_{t r}(t, x)$ with the score corresponding to the piecewise polynomial approximations of the manifold $M$ by $M^{*}=\\cup_{i} M_{i}^{*}$ with $M_{i}^{*}=$ $\\Phi_{i}^{*}\\left(\\Phi_{i}^{-1}\\left(M_{i}\\right)\\right)=\\Phi_{i}^{*}\\left(B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)\\right)$ and for each $i, p_{i}=\\left|\\nabla \\Phi_{i} \\nabla^{T} \\Phi_{i}\\right|^{-1} p \\circ \\Phi_{i}$ by a polynomial function $p_{i}^{*}$.\n\nRecall that from(74), we have with probability at least $1-n^{-2} M_{i} \\subset B_{d}\\left(0,7 \\varepsilon_{N}\\right)$ so that we can express\n\n$$\n\\begin{aligned}\ne_{i}(t, X(t)) & =\\frac{1}{\\sigma_{t}^{2}} \\frac{\\int_{M_{i}} y p(y \\mid t, X(t))}{p\\left(M_{i} \\mid t, X(t)\\right)} \\\\\n& =\\frac{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)}\\left(\\Phi_{i}(z)-\\Phi_{i}\\left(z_{i}^{*}\\right)\\right) e^{-\\left\\|\\Phi_{i}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}(z) d z}{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} e^{-\\left\\|\\Phi_{i}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}(z) d z}+\\frac{\\Phi_{i}\\left(z_{i}^{*}\\right)}{\\sigma_{t}^{2}}\n\\end{aligned}\n$$\n\nA natural way to approximate $e_{i}(t, x)$ is to substitute $\\Phi_{i}$ by $\\Phi_{i}^{*}$, and $p_{i}(z)$ by its Taylor approximation of order $\\alpha-1$ at 0 which we denote $p_{i}^{*}(z)$. This leads to\n$e_{i}^{*}(t, X(t)):=\\frac{1}{\\sigma_{t}^{2}} \\frac{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)}\\left(\\Phi_{i}^{*}(z)-\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\right) e^{-\\left\\|\\Phi_{i}^{*}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z}{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} e^{-\\left\\|\\Phi_{i}^{*}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z}+\\frac{\\Phi^{*}\\left(z_{i}^{*}\\right)}{\\sigma_{t}^{2}}$.\nWe now bound $e_{i}^{*}-e_{i}$. First, since $p_{i}(z)$ is $\\alpha$-smooth then\n\n$$\n\\left|p_{i}(z)-p_{i}^{*}(z)\\right| \\lesssim \\varepsilon_{N}^{\\alpha}\n$$\n\nand by Proposition $26 p_{i}(z)=\\left|\\nabla \\Phi_{i} \\nabla^{T} \\Phi_{i}\\right|^{-1} p\\left(\\Phi_{i}(z)\\right) \\geq 2^{-d} p_{\\min }$, so\n\n$$\np_{i}(z)=p_{i}^{*}(z)\\left(1+O\\left(\\varepsilon_{N}^{\\alpha}\\right)\\right)=p_{i}^{*}(z)\\left(1+O\\left(n^{2 \\gamma \\alpha} n^{-\\frac{\\alpha}{2 \\alpha+d}}\\right)\\right)\n$$\n\nBy Lemma 19 for all $z \\in B_{d}\\left(0,7 \\varepsilon_{N}\\right)$\n\n$$\n\\left\\|\\Phi_{i}^{*}(z)-\\Phi_{i}(z)\\right\\| \\lesssim \\varepsilon_{N}^{\\beta} \\leq n^{2 \\gamma \\beta} n^{-\\frac{\\beta}{2 \\alpha+d}}\n$$\n\nLet $z \\in B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)$, denoting $y:=\\Phi_{i}(z)$ and $y^{*}:=\\Phi_{i}^{*}(z)$ we first bound $\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2}-\\left\\|X(t)-c_{t} y\\right\\|^{2}=c_{t}^{2}\\left\\|y-y^{*}\\right\\|^{2}+2 c_{t}^{2}\\left\\langle X(0)-y, y-y^{*}\\right\\rangle+2 c_{t} \\sigma_{t}\\left\\langle Z_{D}, y-y^{*}\\right\\rangle$.\nBy Lemma 19 the first term is bounded as\n\n$$\nc_{t}^{2}\\left\\|y-y^{*}\\right\\|^{2} \\leq c_{t}^{2}\\left\\|\\Phi_{i}^{*}(z)-\\Phi_{i}(z)\\right\\|^{2} \\lesssim c_{t}^{2} \\cdot \\varepsilon_{N}^{2 \\beta}=o\\left(\\varepsilon_{N}^{\\beta} \\sigma_{t} c_{t}\\right)\n$$\n\nBy Proposition 24 for $n$ large enough with probability $1-n^{-2}$ the third term is bounded by\n\n$$\n2 c_{t} \\sigma_{t} \\cdot\\left|\\left\\langle Z_{D}, y-y^{*}\\right\\rangle\\right| \\lesssim c_{t} \\sigma_{t} \\cdot \\varepsilon_{N}^{\\beta} \\cdot C(n) \\leq c_{t} \\sigma_{t} \\cdot n^{2 \\gamma \\beta} n^{-\\frac{\\beta}{2 \\alpha+d}} / 2\n$$\n\nAt the same time by Proposition 42, $X(0) \\in B_{M}\\left(m_{i}, 2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)$, and since $\\Phi_{i}$ is 2-Lipschitz\n\n$$\n\\|X(0)-y\\| \\leq\\left\\|X(0)-m_{i}\\right\\|+\\left\\|m_{i}-y\\right\\| \\leq(8+2)\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\n$$\n\nthe second term is bounded by, for $n$ large enough,\n\n$$\nc_{t}^{2}\\left|\\left\\langle X(0)-y, y-y^{*}\\right\\rangle\\right| \\lesssim c_{t} \\sigma_{t} C(n) \\varepsilon_{N}^{\\beta}=o\\left(c_{t} \\sigma_{t} \\cdot n^{2 \\gamma \\beta} n^{-\\frac{\\beta}{2 \\alpha+d}}\\right)\n$$\n\nwhere the constants depend only on $d, C_{l o g}, \\beta$. Summing up we conclude that with probability at least $1-n^{-2}$ for all $i \\in I_{2}$ and $y \\in B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)$\n\n$$\n\\frac{\\left|\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2}-\\left\\|X(t)-c_{t} y\\right\\|^{2}\\right|}{2 \\sigma_{t}^{2}} \\lesssim \\frac{\\varepsilon_{N}^{\\beta} c_{t} C(n)}{\\sigma_{t}}=o(1)\n$$\n\nwhere the latter is due to $\\left(\\sigma_{t} / c_{t}\\right) \\geq n^{\\gamma / 2} n^{-\\frac{\\alpha+1}{2 \\alpha+d}} \\geq n^{-\\frac{d \\beta}{(d-2)(2 \\alpha+d)}} n^{\\gamma / 2} \\gg>\\sqrt{\\log n} \\varepsilon_{N}^{\\beta}$ as soon as $\\gamma<\\frac{\\beta-(\\alpha+1)}{(\\beta-1 / 2)(2 \\alpha+d)}$. As a result\n\n$$\ne^{-\\frac{\\left\\|X(t)-c_{t} y^{*}\\right\\|^{2}}{2 \\sigma_{t}^{2}}}=e^{-\\frac{\\left\\|X(t)-c_{t} y\\right\\|^{2}}{2 \\sigma_{t}^{2}}}\\left(1+O\\left(\\frac{\\varepsilon_{N}^{\\beta} c_{t} C(n)}{\\sigma_{t}}\\right)\\right)\n$$\n\nCombining the inequalities above, with probability $1-n^{-2}$ for all $i \\in I_{2}$\n\n$$\n\\begin{aligned}\n& e_{i}^{*}(t, X(t))=\\frac{1}{\\sigma_{t}^{2}} \\frac{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)}\\left(\\Phi_{i}^{*}(z)-\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\right) e^{-\\left\\|\\Phi_{i}^{*}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z}{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} e^{-\\left\\|\\Phi_{i}^{*}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z}+\\frac{\\Phi_{i}\\left(z_{i}^{*}\\right)}{\\sigma_{t}^{2}} \\\\\n& =\\frac{1}{\\sigma_{t}^{2}} \\frac{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)}\\left(\\Phi_{i}(z)-\\Phi_{i}\\left(z_{i}^{*}\\right)\\right) e^{-\\left\\|\\Phi_{i}^{*}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z}{\\int_{B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} e^{-\\left\\|\\Phi_{i}^{*}(z)-X(t)\\right\\|^{2} / 2 \\sigma_{t}^{2}} p_{i}^{*}(z) d z}+\\frac{\\Phi_{i}(z)}{\\sigma_{t}^{2}}+O\\left(\\varepsilon_{N}^{\\beta} / \\sigma_{t}^{2}\\right) \\\\\n& =e_{i}(t, X(t))+O\\left(\\frac{\\varepsilon_{N}^{\\beta}}{\\sigma_{t}^{2}}\\right) \\\\\n& +\\frac{1}{\\sigma_{t}^{2}} \\sup _{z \\in B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)}\\left\\|\\Phi_{i}(z)-\\Phi_{i}\\left(z_{i}^{*}\\right)\\right\\| O\\left(\\varepsilon_{N}^{\\alpha}+\\frac{\\varepsilon_{N}^{\\beta} C(n)}{\\sigma_{t}}\\right)\n\\end{aligned}\n$$\n\nSince $\\Phi_{i}$ is 2-Lipschitz $\\left\\|\\Phi_{i}(z)-\\Phi_{i}\\left(z_{i}^{*}\\right)\\right\\| \\leq 2\\left\\|z-z_{i}^{*}\\right\\| \\leq 8 \\sqrt{d}\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)$, applying that $\\sigma_{t}^{2} \\geq t / 2 \\geq T_{k} / 2, \\sigma_{t} / c_{t} \\leq 4 t<4 T_{k}$ and $C(n) \\lesssim \\log n$ we conclude that with probability at least $1-n^{-2}$\n\n$$\ne_{i}^{*}(t, X(t))=e_{i}(t, X(t))+O\\left(T_{k}^{-1} n^{\\gamma \\beta} n^{-\\frac{\\beta}{2 \\alpha+d}} \\log n^{2}\\right)+O\\left(T_{k}^{-1 / 2} n^{\\gamma \\alpha} n^{-\\frac{\\alpha}{2 \\alpha+d}}(\\log n)^{\\alpha / d+1 / 2}\\right)\n$$\n\nTaking expectations, and noting that $\\left\\|e_{i}^{*}(t, X(t))\\right\\| \\leq 4 T_{k}^{-1},\\left\\|e_{i}(t, X(t))\\right\\| \\leq 2 T_{k}^{-1}$ a.s., we obtain\n\n$$\n\\mathbb{E}\\left\\|s_{i}^{*}(t, X(t))-s_{i}(t, X(t))\\right\\|^{2}=\\mathbb{E}\\left\\|e_{i}^{*}(t, X(t))-e_{i}(t, X(t))\\right\\|^{2}=O\\left(T_{k}^{-2} \\varepsilon_{N}^{2 \\beta} \\log n+T_{k}^{-1} \\varepsilon_{N}^{2 \\alpha} \\log n\\right)\n$$\n\nIntegrating over $\\left[T_{k}, T_{k+1}\\right]$, since $T_{k+1}=2 T_{k} \\leq 2 n^{-2 /(2 \\alpha+d)}$\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|s_{i}^{*}(t, X(t))-s_{i}(t, X(t))\\right\\|^{2} d t=\\frac{1}{\\sigma_{T_{k}}^{2}} O\\left(\\varepsilon_{N}^{2 \\beta} \\log n^{2}+\\sigma_{T_{k}}^{2} \\varepsilon_{N}^{2 \\alpha} \\log n\\right)\n$$\n\nFinally, using that $\\rho_{i}(t, x) \\neq 0$ implies $i \\in I_{2}$ and by definition of $s_{t r}^{*}$ we have\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|s_{t r}^{*}(t, X(t))-s_{t r}(t, X(t))\\right\\|^{2} d t=O\\left(\\sigma_{T_{k}}^{-2} \\varepsilon_{n}^{2 \\beta}(\\log n)^{2}+n^{2 \\gamma \\alpha} n^{-\\frac{2 \\alpha}{2 \\alpha+d}}(\\log n)^{2 \\alpha / d+1}\\right)\n$$\n\nwhich proves (69).", "tables": {}, "images": {}}, {"section_id": 32, "text": "# D.2.4 Polynomial Approximation of $e_{i}^{*}$ \n\nTo construct a neural network approximation of $e_{t r}^{*}$ we first approximate $e_{i}^{*}$ by a low degree polynomial of at most $O(\\operatorname{polylog} n)$ variables. A key point is to treat $e^{-\\left\\|x-c_{t} \\Phi_{i}^{*}(z)\\right\\|^{2} / 2 \\sigma_{t}^{2}}$ in the definition of $e_{i}^{*}$ since $\\left\\|x-c_{t} \\Phi_{i}^{*}(z)\\right\\|^{2} \\asymp D$. As explained in Appendix D.2.1, we introduce the projection operator onto $T_{m_{i}^{*}} M_{i}^{*}$ for $m_{i}^{*}=\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)$,\n\n$$\n\\operatorname{pr}_{z_{i}^{*}}=\\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(\\nabla^{T} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right) \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\right)^{-1} \\nabla^{T} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right), \\quad x_{i}^{\\mathrm{pr}}:=\\operatorname{pr}_{z_{i}^{*}}\\left(x-c_{t} m_{i}^{*}\\right)\n$$\n\nWe then write\n\n$$\n\\Phi_{i}^{*}(z)=\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)+\\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)+R_{i}\\left(z, z_{i}^{*}\\right)\n$$\n\nwhere polynomial $R_{i}\\left(z, z_{i}^{*}\\right)$ is the second order polynomial residual and note that $\\operatorname{Im} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)=$ $T_{m_{i}^{*}} M_{i}^{*}$, so that\n\n$$\n\\begin{aligned}\n& \\left\\|x-c_{t} \\Phi_{i}^{*}(z)\\right\\|^{2}=\\left\\|x-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)-c_{t} R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2} \\\\\n& =\\left\\|x-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2}+c_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}-2 c_{t}\\left\\langle x-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle \\\\\n& =\\left\\|x-x_{i}^{\\mathrm{pr}}\\right\\|^{2}+\\left\\|x_{i}^{\\mathrm{pr}}-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2}-c_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}+2 c_{t}\\left\\langle x_{i}-c_{t} \\Phi_{i}^{*}(z), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle\n\\end{aligned}\n$$\n\nwhere we have used in the last equality that $\\left\\langle x-c_{t} \\Phi_{i}^{*}(z), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle=\\left\\langle x_{i}-c_{t} \\Phi_{i}^{*}(z), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle$ due to $R_{i}\\left(z, z_{i}^{*}\\right) \\in \\mathcal{H}_{i}$. Therefore, since $\\left\\|x-x_{i}^{\\mathrm{pr}}\\right\\|^{2}$ does not depend on $y$ we can write $e_{i}^{*}=f_{i} / g_{i}$ with $f_{i}$ and $g_{i}$ defined in (71).\n\nWe now bound from below $g_{i}$.\nFirst we bound the residual term $e^{\\left(-c_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}+2 c_{t}\\left\\langle x_{i}-c_{t} \\Phi_{i}^{*}(z), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle\\right) / 2 \\sigma_{t}^{2}}$, using Lemma 47 in Appendix G.2. It states that with probability at least $1-n^{-2}, \\forall i \\in I_{2}$ and $\\forall z \\in$ $B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)$\n\n$$\n\\left|c_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}-2 c_{t}\\left\\langle X(t)-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle\\right| \\lesssim \\sigma_{t}^{3}(d \\log n)^{2}\n$$\n\nThen, since $T_{k} \\leq n^{-\\frac{2}{2 \\alpha+d}}$ we get that for $n$ large enough and with probability greater than $1-n^{-2}$\n\n$$\n\\begin{aligned}\n\\left(c_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}-c_{t}\\left\\langle X_{i}(t)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle\\right) / 2 \\sigma_{t}^{2} & \\leq \\frac{1}{2} n^{\\gamma} n^{-\\frac{1}{2 \\alpha+d}} \\\\\n\\left|e^{\\left(c_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}-2 c_{t}\\left\\langle X_{i}(t)-\\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle\\right) / 2 \\sigma_{t}^{2}}-1\\right| & \\leq n^{\\gamma} n^{-\\frac{1}{2 \\alpha+d}}\n\\end{aligned}\n$$\n\nNext, we analyze the main term $\\left\\|x_{i}^{\\mathrm{pr}}-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2}$. Substituting $X(t)$ as $x$ in (76) leads to\n\n$$\n\\begin{aligned}\nX_{i}^{\\mathrm{pr}}(t)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right) & =\\operatorname{pr}_{z_{i}^{*}}\\left((X(t))-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\right)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right) \\\\\n& =\\sigma_{t} \\operatorname{pr}_{z_{i}^{*}} Z_{D}+c_{t} \\operatorname{pr}_{z_{i}^{*}}\\left(X(0)-\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\right)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\n\\end{aligned}\n$$\n\nRecall that $m_{i}^{*}=\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)$ then\n$\\left\\|m_{i}^{*}-X(0)\\right\\| \\leq\\left\\|m_{i}^{*}-m_{i}\\right\\|+\\left\\|m_{i}-X(0)\\right\\| \\leq O\\left(\\varepsilon_{N}^{\\beta}\\right)+2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n) \\leq 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)$,\nwhere the last inequality follows from Proposition 42. We bound the projection $\\mathrm{pr}_{z_{i}^{*}} Z_{D}$ using Lemma 46 in Appendix G. 2 with $\\delta=n^{-2}$, so that with probability greater than $1-n^{-2}$,\n\n$$\n\\left\\|\\operatorname{pr}_{z_{i}^{*}} Z_{D}\\right\\| \\leq 56 d L_{M} \\varepsilon_{N}+8 \\sqrt{d}\\left(1+7 L_{M} \\varepsilon_{N}\\right) \\sqrt{2 \\log 2 d+6 \\log n} \\leq 16 C(n)\n$$\n\nfor $n$ large enough since $\\varepsilon_{N}=o(1)$. Finally, since $\\left\\|\\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\right\\|_{o p} \\leq \\sup _{z \\in B_{d}\\left(0,7 \\varepsilon_{N}\\right)}\\left\\|\\nabla \\Phi_{i}^{*}(z)\\right\\|_{o p} \\leq$ 2 by Lemma 19, the last term of (78) is also bound by $8 \\sqrt{20} \\sigma_{t} C(n)$ leading to\n\n$$\n\\begin{aligned}\n& \\left\\|X_{i}^{\\mathrm{pr}}(t)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2} \\\\\n& \\leq 4 \\sigma_{t}^{2}\\left\\|\\operatorname{pr}_{z_{i}^{*}} Z_{D}\\right\\|^{2}+4 c_{t}^{2}\\left\\|X(0)-\\Phi_{i}\\left(z_{i}^{*}\\right)\\right\\|^{2}+4 c_{t}^{2}\\left\\|\\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2} \\\\\n& \\leq 4 \\cdot \\sigma_{t}^{2} C^{2}(n)[256+16+64]=1344 \\sigma_{t}^{2} C^{2}(n)\n\\end{aligned}\n$$\n\nMoreover, for $n$ large enough $p^{*}(z)>\\frac{1}{2} \\inf _{B_{d}\\left(0,8 \\varepsilon_{N}\\right)} p(z) \\geq 2^{-d-1} p_{\\min }$, so combining with (77) we get\n$g_{i}(t, x) \\geq \\frac{1}{4} p_{\\min } e^{-676 C^{2}(n)} \\int_{B_{d}\\left(z_{i}^{*}, 2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)} d z=\\frac{1}{4} p_{\\min } e^{-676 C^{2}(n)} C_{d}\\left(2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)^{d}$,\nwhere $C_{d}$ is the volume of the $d$-dimensional unit ball. Since $C(n)=\\sqrt{d\\left(\\log n+4 C_{\\log }\\right)+4 \\log n}$, then for $n$ large enough $C^{2}(n)<4 d \\log n$. So, since $\\sigma_{t} / c_{t}>n^{-\\frac{\\alpha+1}{2 \\alpha+d}}>n^{-1}$ for $n$ large enough with probability at least $1-n^{-2}$, for all $i \\in I_{2}$,\n\n$$\ng_{i}(t, X(t)) \\geq n^{-2688 d}\n$$\n\nSuppose $\\hat{f}_{i}$ and $\\hat{g}_{i}$ are approximations of $f_{i}, g_{i}$ such that with probability $1-n^{-2}$ for all $i \\in I_{2}$\n\n$$\n\\left|g_{i}(t, X(t))-\\hat{g}_{i}(t, X(t))\\right| \\leq n^{-2690 d}, \\quad\\left\\|f_{i}(t, X(t))-\\hat{f}_{i}(t, X(t))\\right\\| \\leq n^{-2690 d}\n$$\n\nthen with probability at least $1-n^{-2}$, for all $i \\in I_{2}$\n\n$$\n\\left\\|\\hat{e}_{i}(t, X(t))-e_{i}^{*}(t, X(t))\\right\\| \\leq\\left\\|e_{i}^{*}(t, X(t))\\right\\| n^{-2} \\leq 4 T_{k}^{-1} n^{-2}, \\quad \\text { with } \\hat{e}_{i}(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\hat{f}_{i}(t, x)}{\\hat{g}_{i}(t, x)}\n$$\n\nThis implies that $\\hat{e}_{t r}(t, x)=\\sum_{i=1}^{N} \\rho_{i}(t, x) \\hat{e}_{i}(t, x) /\\left(\\sum_{i=1}^{N} \\rho_{i}(t, x)\\right)$ verifies\n\n$$\n\\left\\|\\hat{e}_{t r}(t, X(t))-e_{t r}^{*}(t, X(t))\\right\\| \\leq 4 T_{k}^{-1} n^{-2}\n$$\n\nSo, defining $\\hat{e}_{\\text {clip }}(t, x)=\\hat{e}(t, x) \\mathbb{1}_{\\|\\hat{e}(t, x)\\| \\leq \\sigma_{t}^{-2}}$ and using that $\\left\\|e^{*}(t, x)\\right\\| \\leq \\sigma_{t}^{-2}$ a.s.\n\n$$\n\\mathbb{E}\\left\\|\\hat{e}_{\\text {clip }}(t, X(t))-e_{t r}^{*}(t, X(t))\\right\\|^{2} \\lesssim T_{k}^{-2}\\left(n^{-2}+n^{-4}\\right) \\lesssim 2 T_{k}^{-2} n^{-2}\n$$\n\nIntegrating we obtain\n\n$$\n\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|\\hat{e}_{\\mathrm{clip}}(t, X(t))-e_{t r}^{*}(t, X(t))\\right\\|^{2} d t \\lesssim T_{k}^{-1} n^{-2} \\leq n^{-1}\n$$\n\nwhere the constant depends only on $d, \\beta, C_{l o g}$. So, defining $\\hat{s}_{\\text {clip }}(t, x):=\\hat{e}_{\\text {clip }}(t, x)-\\frac{x}{\\sigma_{t}^{2}}$ and combining with (69)\n\n$$\n\\int_{t_{k}}^{t_{k+1}} \\mathbb{E}\\left\\|\\hat{s}_{\\text {clip }}(t, X(t))-s_{t r}(t, X(t))\\right\\|^{2} d t=O\\left(\\sigma_{T_{k}}^{-2} n^{4 \\gamma \\beta} n^{-\\frac{2 \\beta}{2 \\alpha+d}}+n^{4 \\gamma \\alpha} n^{-\\frac{2 \\alpha}{2 \\alpha+d}}\\right)\n$$", "tables": {}, "images": {}}, {"section_id": 33, "text": "# D.2.5 Neural Network Approximation \n\nIn this Section we show that we can construct neural networks for $\\hat{f}_{i}, \\hat{g}_{i}$ satisfying (80). To do that we first approximate them by polynomials whose coefficients are themselves polynomials of $\\tilde{x}_{i}, c_{t}, c_{t}^{-1}, \\sigma_{t}, \\sigma_{t}^{-1}$.\n\nFirst, as in the case $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}}$, using the linear isometry $P_{\\mathcal{H}_{i}}: \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}} \\rightarrow \\mathcal{H}_{i}$ that identifies $\\mathcal{H}_{i}$ with $\\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$, we can represent $\\Phi_{i}^{*}(z)=G_{i}+P_{\\mathcal{H}_{i}} \\widetilde{\\Phi}_{i}^{*}(z)$, where $\\widetilde{\\Phi}_{i}^{*}$ is a polynomial of degree $\\beta-1$ with coefficients $\\widetilde{P}_{i}^{*}:=P_{\\mathcal{H}_{i}}^{T} P_{i}^{*}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}, \\widetilde{a}_{i, S}^{*} \\in \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$ belonging to $\\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$. Define $\\widetilde{x}_{i}=P_{\\mathcal{H}_{i}}^{T}\\left(x-c_{t} G_{i}\\right) \\in \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$, and $\\widetilde{x}_{i}^{\\mathrm{pr}}=P_{\\mathcal{H}_{i}} x_{i}^{\\mathrm{pr}}$ so that\n\n$$\n\\widetilde{x}_{i}^{\\mathrm{pr}}=\\widetilde{\\operatorname{pr}}_{z_{i}^{*}}\\left(\\widetilde{x}_{i}-c_{t} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\right) .=\\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(\\nabla^{T} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right) \\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\right)^{-1} \\nabla^{T} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(\\widetilde{x}_{i}-c_{t} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\right)\n$$\n\nAnd since $\\mathbb{P}_{\\mathcal{H}_{i}}$ is an isometry\n\n$$\n\\begin{aligned}\n\\left\\|x_{i}^{\\mathrm{pr}}-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2} & =\\left\\|\\widetilde{x}_{i}^{\\mathrm{pr}}-c_{t} \\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2}, \\quad\\left\\|R\\left(z, z_{i}^{*}\\right)\\right\\|^{2}=\\left\\|\\widetilde{R}\\left(z, z_{i}^{*}\\right)\\right\\|^{2} \\\\\n\\left\\langle X_{i}(t)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right), R\\left(z, z_{i}^{*}\\right)\\right\\rangle & =\\left\\langle\\widetilde{X}_{i}(t)-c_{t} \\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right), \\widetilde{R}\\left(z, z_{i}^{*}\\right)\\right\\rangle\n\\end{aligned}\n$$\n\nTherefore $f_{i}$ and $g_{i}$ can be re-expressed in terms of $\\widetilde{x}_{i}, \\widetilde{\\Phi}_{i}^{*}, \\widetilde{R}$ etc. Then using a Taylor approximation of $e^{n}$ near 0 together with (79) and (77), we have that for $n$ large enough with probability at least $1-n^{-2}$\n\n$$\n\\begin{aligned}\n& \\left|e^{-\\left\\|\\widetilde{x}_{i}^{\\mathrm{pr}}-c_{t} \\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2} / 2 \\sigma_{t}^{2}}-\\sum_{k=1}^{K_{1}} \\frac{1}{k!}\\left(-\\left\\|\\widetilde{x}_{i}^{\\mathrm{pr}}-c_{t} \\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right)\\right\\|^{2} / 2 \\sigma_{t}^{2}\\right)^{k \\mid} \\\\\n& \\quad \\leq\\left(\\frac{e \\cdot 2368 d \\log n}{K_{1}}\\right)^{K_{1}} \\leq e^{-e^{2} \\cdot 2368 d \\log n}<n^{-2370 d}\n\\end{aligned}\n$$\n\nby choosing $K_{1}=e^{2} \\cdot 2368 d \\log n=O(d \\log n)$. And similarly\n\n$$\n\\begin{aligned}\n& \\left|e^{\\left(c_{t}^{2}\\left\\|\\widetilde{R}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}-c_{t}\\left\\langle\\widetilde{X}_{i}(t)-c_{t} \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right), \\widetilde{R}\\left(z, z_{i}^{*}\\right)\\right\\rangle\\right) / 2 \\sigma_{t}^{2}} \\\\\n& -\\sum_{k=1}^{K_{2}} \\frac{1}{k!}\\left(\\left(c_{t}^{2}\\left\\|\\widetilde{R}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}-c_{t}\\left\\langle\\widetilde{X}_{i}(t)-c_{t} \\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z-z_{i}^{*}\\right), \\widetilde{R}\\left(z, z_{i}^{*}\\right)\\right\\rangle\\right) / 2 \\sigma_{t}^{2}\\right)^{k} \\mid \\leq 2\\left(\\frac{c n^{-\\frac{1}{2(2 \\alpha+d)}}}{K_{2}}\\right)^{K_{2}} \\\\\n& \\leq n^{-2370 d}\n\\end{aligned}\n$$\n\nby choosing $K_{2}=2(2 \\alpha+d) 2370 d=O(d \\alpha+d))$. We write $Q_{1}\\left(z-z_{i}^{*}\\right)$ the polynomial approximation of the first term and $Q_{2}\\left(z-z_{i}^{*}\\right)$ the polynomial approximation of the second (as functions of $z$ ). $Q_{1}$ has degree $2 K_{1}$ and $Q_{2} 2 K_{2}$. Moreover $p_{i}^{*}$ is a polynomial function of degree $\\alpha$; therefore\n$\\bar{g}_{i}(t, x):=\\int_{\\left\\|z-z_{i}^{*}\\right\\| \\leq 4 \\sigma_{t} C(n) / c_{t}} Q_{1}\\left(z-z_{i}^{*}\\right) Q_{2}\\left(z-z_{i}^{*}\\right) p_{i}^{*}(z) d z=\\int_{\\|z\\| \\leq 4 \\sigma_{t} C(n) / c_{t}} Q_{1}(z) Q_{2}(z) Q_{3}(z) d z$ where $Q_{3}$ is a polynomial function of $z$ of degree $\\alpha$ and we have $p_{i}^{*} \\leq 2 p_{\\max }$\n\n$$\n\\left|\\bar{g}_{i}(t, x)-g_{i}(t, x)\\right| \\leq 4 n^{-2370 d} \\operatorname{Vol} B_{d}\\left(0,4 \\sigma_{t} C(n) / c_{t}\\right) \\lesssim T_{k}^{d}(\\log n)^{d} n^{-2370 d}=o\\left(n^{-2370 d}\\right)\n$$\n\nSimilarly, since $\\widetilde{\\Phi}_{i}^{*}$ is also polynomial with degree $\\beta$ and $\\left\\|\\widetilde{\\Phi}_{i}^{*}(z)\\right\\| \\leq 2$ over the ball of integration, $\\left|\\bar{f}_{i}(t, x)-f_{i}(t, x)\\right| o\\left(n^{-2370 d}\\right)$ with\n\n$$\n\\bar{f}_{i}(t, x):=\\int_{\\|z\\| \\leq 4 \\sigma_{t} C(n) / c_{t}} Q_{4}(z) Q_{1}(z) Q_{2}(z) Q_{3}(z) d z, \\quad Q_{4}(z)=\\Phi_{i}^{*}\\left(z+z_{i}^{*}\\right)\n$$\n\nWe write $Q_{j}(z)=\\sum_{|S| \\leq 2 K_{j}} a_{S, Q_{j}} z^{S}$ with $\\beta \\geq 2 K_{3} \\geq \\beta-1$ and $\\alpha-1 \\leq 2 K_{4} \\leq \\alpha$ and note that $Q_{j}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ for $j=1,2,3$ and $Q_{4}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$. Then by construction $a_{S, Q_{1}}$ are polynomial functions of degree less than $O\\left(K_{1}\\right)$ of $\\widetilde{x}_{i}^{\\mathrm{pr}}, \\sigma_{t}^{-2}, c_{t}, z_{i}^{*}$ since $\\widetilde{\\Phi}_{i}^{*}=\\widetilde{P}_{i}^{*}+\\sum_{S^{\\prime}} \\widetilde{a}_{i, S}^{*} z^{S^{\\prime}}$ Also $a_{S, Q_{2}}$ are polynomial functions of degree less than $O\\left(K_{2}\\right)$ of the first $\\beta$ derivatives of $\\widetilde{\\Phi}_{i}^{*}$ at $z_{i}^{*}$, of $c_{t}, \\sigma_{t}^{-1}, \\widetilde{x}_{i}$ so that they are polynomial functions of degree less than $O\\left(K_{2}\\right)$ of $c_{t}, \\sigma_{t}^{-1}, \\widetilde{x}_{i}, z_{i}^{*}$. Similarly for $a_{S, Q_{3}}$ which as a degree of order $O(\\alpha d)$. Finally $a_{S, Q_{4}} \\in \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$, whose components are polynomial functions of degree $\\beta$ of $\\widetilde{x}_{i}, z_{i}^{*}$. Since for all vector of indices $S$\n\n$$\n\\int_{\\|z\\| \\leq 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)} z^{S} d z=\\left(\\sigma_{t} / c_{t}\\right)^{S}(4 \\sqrt{20} C(n))\\}^{S} \\int_{\\|z\\| \\leq 1} z^{S}=c_{S, n}\\left(\\sigma_{t} / c_{t}\\right)^{S}\n$$\n\nwe can write\n\n$$\n\\begin{aligned}\n& \\bar{g}_{i}(t, x)=\\sum_{S_{1}, \\cdots, S_{3}} \\prod_{l=1}^{3} a_{S_{l}, Q_{l}}\\left(\\sigma_{t} / c_{t}\\right)^{\\sum_{l} S_{l}} C_{\\sum_{l} S_{l}, n} \\\\\n& \\bar{f}_{i}(t, x)=\\sum_{S_{1}, \\cdots, S_{4}} \\prod_{l=1}^{4} a_{S_{l}, Q_{l}}\\left(\\sigma_{t} / c_{t}\\right)^{\\sum_{l} S_{l}} C_{\\sum_{l} S_{l}, n}\n\\end{aligned}\n$$\n\nWe now show that $\\widetilde{x}_{i}^{\\mathrm{pr}}$ and $z_{i}^{*}$ are polynomial functions of $\\widetilde{x}_{i}$ and $c_{t}^{-1}$. Recall that $x_{i}=\\operatorname{pr}_{\\mathcal{H}_{i}}\\left(x-c t G_{i}\\right)+c_{t} G_{i}$ and since $\\operatorname{Im} P_{i}^{*} \\subset \\mathcal{H}_{i}$ then $z_{i}^{*}:=\\left(P_{i}^{*}\\right)^{T}\\left(x-c_{t} G_{i}\\right) / c_{t}=\\left(P_{i}^{*}\\right)^{T}\\left(x_{i}-\\right.$ $\\left.c_{t} G_{i}\\right) / c_{t}$ and\n\n$$\n\\left(\\widetilde{P}_{i}^{*}\\right)^{T} \\widetilde{x}_{i} / c_{t}=\\left(P_{i}^{*}\\right)^{T} P_{\\mathcal{H}_{i}} P_{\\mathcal{H}_{i}}^{T}\\left(x_{i}-c_{t} G_{i}\\right)=z_{i}^{*}\n$$\n\nTherefore $z_{i}^{*}$ is a polynomial of order 2 of $\\widetilde{x}_{i}, c_{t}^{-1}$. Now recall that\n\n$$\n\\widetilde{x}_{i}^{\\mathrm{pr}}=\\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(\\nabla^{T} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right) \\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\right)^{-1} \\nabla^{T} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(\\widetilde{x}_{i}-c_{t} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\right)\n$$\n\nand that $\\widetilde{\\Phi}_{i}^{*}(z)=\\widetilde{P}_{i}^{*} z+\\sum \\widetilde{a}_{i, S}^{*} z^{S}$; with $\\widetilde{a}_{i, S}^{*} \\in \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}$ and $|S|_{1} \\leq \\beta-1$. Therefore $\\nabla \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)$ is a polynomial of degree smaller than $\\beta-2$ of $z_{i}^{*}$, hence it is a polynomial of degree $2(\\beta-2)$ of $\\widetilde{x}_{i}, c_{t}^{-1}$. As a consequence we obtain that\n\n- the entries of the $d \\times d$ matrix $\\nabla^{T} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right) \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)$ are polynomials in $c_{t}^{-1}, \\widetilde{x}_{i}$ of order at most $4(\\beta-2)$. In other words $\\nabla^{T} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right) \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)=Q_{d, d}\\left(c_{t}^{-1}, \\widetilde{x}_{i}\\right)$, for some polynomial $Q_{1}$ with coefficients in $\\mathbb{R}^{d \\times d}$;\n- the entries of the $d \\times 1$ vector $\\nabla^{T} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\left(\\widetilde{x}_{i}-c_{t} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\right)$ are polynomials in $c_{t}^{-1}, x_{i}$ of order at most $2 \\beta$. In other words $\\nabla^{T} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(\\widetilde{x}_{i}-c_{t} \\widetilde{\\Phi}_{i}^{*}\\left(z_{i}^{*}\\right)\\right)=Q_{d}\\left(c_{t}^{-1}, \\widetilde{x}_{i}\\right)$, for some polynomial $Q_{d}$ with coefficients in $\\mathbb{R}^{d}$;\n\n- Cramer's rule states that if $A \\in \\mathbb{R}^{d \\times d}$ is invertible then\n\n$$\nA^{-1}=\\frac{1}{\\operatorname{det} A}\\left((-1)^{i+j} \\operatorname{det} M_{j i}\\right)_{i, j \\leq d}\n$$\n\nwhere the matrix $M_{i j}$ is the $i j$-minor - matrix. Also the determinant of a $d \\times d$ matrix is a polynomial of order $d$ in its entries Therefore, the inverse matrix can be represented as\n\n$$\n\\left(\\nabla^{T} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right) \\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\right)^{-1}=\\frac{Q_{\\mathrm{adj}}\\left(c_{t}^{-1}, \\widetilde{x}_{i}\\right)}{Q_{\\mathrm{det}}\\left(c_{t}^{-1}, \\widetilde{x}_{i}\\right)}\n$$\n\nwhere $Q_{\\text {adj }}$ is a polynomial in $c_{t}^{-1}, \\widetilde{x}_{i}$ of order $2(d-1)(\\beta-2)$ with coefficients in $\\mathbb{R}^{d \\times d}$, and $Q_{\\text {det }}$ is a polynomial in $c_{t}^{-1}, \\widetilde{x}_{i}$ of order $2 d(\\beta-2)$ with scalar coefficients.\nCombining the above we obtain that $\\widetilde{x}_{i}^{\\mathrm{pr}}$ is a polynomial of degree 4 of $Q_{d}, Q_{d, d}, Q_{\\text {det }}^{-1}, Q_{\\text {adj }}$.\nAlso, applying Lemma 46 we note that with probability $1-n^{-2}$ for all $i \\in I_{2}$\n\n$$\n\\left\\|\\widetilde{X}_{i}(t)\\right\\|=\\left\\|X_{i}(t)\\right\\|=\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}}\\left(c_{t} X(0)+\\sigma_{t} Z_{D}-c_{t} G_{i}\\right)\\right\\| \\leq c_{t}\\left\\|X(0)-G_{i}\\right\\|+\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}} Z_{D}\\right\\| \\lesssim O(\\log n)\n$$\n\nand using that $1 / 2 \\leq\\left\\|\\nabla \\Phi_{i}^{*}(z) \\nabla^{T} \\Phi_{i}^{*}(z)\\right\\| \\leq 2$ we conclude that all variables $\\widetilde{x}_{i}, \\sigma_{t}^{-1}, \\sigma_{t}, c_{t}, c_{t}^{-1}, Q_{d}, Q_{d \\times d}, Q_{\\text {adj }}$ are bounded by $n$.\n\nWe can now construct the neural network approximations of $\\bar{f}_{i}$ and $\\bar{g}_{i}$. Using Lemmas 3739, neural networks can be chosen $\\phi_{\\bar{g}}, \\phi_{\\bar{f}}, \\phi_{Q_{d}}, \\phi_{Q_{d \\times d}}, \\phi_{Q_{\\text {adj }}}, \\phi_{Q_{\\text {det }}^{-1}} \\in \\Psi\\left(L_{1}, W_{1}, S_{1}, B_{1}\\right)$, where $L_{1}, W_{1}, S_{1}=\\operatorname{polylog} n$ and $B_{1}=e^{\\operatorname{polylog} n}$.\n\nFinally, applying Lemma 36 and then Lemma 39 we concatenate these neural networks and get two neural networks $\\phi_{g_{i}}, \\phi_{e_{i}} \\in \\Psi(L, W, S, B)$, where $L, W, S=\\operatorname{polylog} n$ and $B=$ $e^{\\operatorname{polylog} n}$ that approximate $g_{i}$ and $e_{i}=f_{i} / g_{i}$ respectively with an error $n^{-2370 d}$.\n\nConstructing these networks for all $i \\leq N$ we show that for $T_{k} \\leq n^{-\\frac{2}{2 \\alpha+d}}$ there is a network $\\phi \\in \\mathcal{S}_{k}$ s.t.\n\n$$\n\\int_{t_{k}}^{t_{k+1}} \\mathbb{E}\\|\\phi(t, X(t))-s(t, X(t))\\|^{2} d t \\leq O\\left(\\sigma_{T_{k}}^{-2} n^{4 \\gamma \\beta} n^{-\\frac{2 \\beta}{2 \\alpha+d}}+n^{4 \\gamma \\alpha} n^{-\\frac{2 \\alpha}{2 \\alpha+d}}\\right)\n$$", "tables": {}, "images": {}}, {"section_id": 34, "text": "# D. 3 Generalization error \\& Proof of Theorem 7 \n\nIn this Section we complete the proof of Theorem 7. Recall that we are approximating the score $s(t, x)$ by Neural networks of the form\n$\\phi_{s}(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} \\frac{\\sum_{i}^{N} \\rho_{i}\\left(t, x_{i}\\right) \\phi_{w_{i}}\\left(t, \\widetilde{x}_{i}\\right)\\left(G_{i}+P_{\\mathcal{H}_{i}} \\phi_{e_{i}}\\left(t, \\widetilde{x}_{i}\\right)\\right)}{\\sum_{i}^{N} \\rho_{i}(t, x) \\phi_{w_{i}}\\left(t, \\widetilde{x}_{i}\\right)}-\\frac{x}{\\sigma_{t}^{2}}, \\quad t \\in\\left[T_{k}, T_{k+1}\\right], \\quad \\widetilde{x}_{i}=P_{\\mathcal{H}_{i}}\\left(x-c_{t} G_{i}\\right)$,\nwhere $\\phi_{e_{i}^{*}}: \\mathbb{R}_{+} \\times \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}} \\rightarrow \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}}, \\phi_{w_{i}}: \\mathbb{R}_{+} \\times \\mathbb{R}^{\\operatorname{dim} \\mathcal{H}_{i}} \\rightarrow \\mathbb{R}_{+}$are neural networks of polylog size belonging to\n$\\mathcal{S}_{k}=\\left\\{\\phi_{s}: \\phi_{e_{i}}, \\phi_{w_{i}} \\in \\Psi(L, W, B, S),\\left\\|\\phi_{e_{i}}\\right\\|_{\\infty} \\leq 1,\\left\\|\\frac{c_{t} \\phi_{e_{i}}\\left(t, \\widetilde{x}_{i}\\right)-\\widetilde{x}_{i}}{\\sigma_{t}}\\right\\|_{\\infty} \\leq C_{e} \\sqrt{\\log n},\\left\\|\\phi_{w_{i}}\\right\\|_{\\infty} \\geq n^{-C_{w}}\\right\\}$,\nwhere $C_{e}, C_{w}$ are absolute constants specified later and\n\n$$\nL=O(\\operatorname{polylog} n),\\|W\\|_{\\infty}=O(\\operatorname{polylog} n), S=O(\\operatorname{polylog} n), B=e^{O(\\operatorname{polylog} n)}\n$$\n\nThe estimator $\\hat{s}$ of the score $s(t, x)$ on interval $\\left[T_{k}, T_{k+1}\\right]$ is defined as\n\n$$\n\\hat{\\phi}(t, x)=\\underset{\\phi \\in \\mathcal{S}}{\\arg \\min } \\mathcal{R}_{\\mathcal{Y}}\\left(\\phi, T_{k}, T_{k+1}\\right), \\quad \\mathcal{R}_{\\mathcal{Y}}\\left(\\phi, T_{k}, T_{k+1}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{y_{i}}\\left(\\phi, T_{k}, T_{k+1}\\right)\n$$\n\nand the loss function $\\ell_{y}$ for $y \\in M$ is defined in (11) as\n\n$$\n\\ell_{y}\\left(\\hat{s}, T_{k}, T_{k+1}\\right):=\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}_{Z_{D}}\\left\\|\\hat{s}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-Z_{D} / \\sigma_{t}\\right\\|^{2} d t\n$$\n\n(Oko et al., 2023, Theorem C.4.) states (Note that the original proof has an error which we fix in Appendix D.4)\n\n$$\n\\begin{aligned}\n& \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|\\hat{s}_{k}(t, X(t))-s(t, X(t))\\right\\|^{2} d t \\lesssim \\inf _{\\phi \\in \\mathcal{S}_{k}} \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|\\phi(t, X(t))-s(t, X(t))\\|^{2} d t \\\\\n& \\quad+\\frac{\\sup _{y \\in M} \\sup _{\\phi} \\ell_{y}\\left(\\phi, T_{k}, T_{k+1}\\right)}{n}\\left(\\log \\mathcal{N}\\left(\\mathcal{L},\\|\\cdot\\|_{L_{\\infty}(M)}, \\delta\\right)+1\\right)+\\delta\n\\end{aligned}\n$$\n\nWe have shown in Appendix D. 2 that the first term is of order $O\\left(T_{k}^{-1} n^{-2 \\beta /(2 \\beta+d)} n^{4 \\gamma \\beta}+\\right.$ $\\left.n^{-2 \\alpha /(2 \\alpha+d)} n^{4 \\gamma \\alpha}\\right)$. We now study the second term. We follow Oko et al. (2023), but taking into account the dependency on ambient dimension $D$ and the special form (20) of our estimators.\n\nWe first bound $\\sup _{y \\in M} \\ell_{y}\\left(\\phi, T_{k}, T_{k+1}\\right)$ uniformly over $\\phi \\in \\mathcal{S}_{k}$. To do this let us first define\n\n$$\n\\phi_{e}(t, x):=\\frac{\\sigma_{t}^{2}}{c_{t}} \\phi_{s}(t, x)+x=\\frac{\\sum_{i}^{N} \\rho_{i}(t, x) \\phi_{w_{i}}\\left(t, \\widetilde{x}_{i}\\right)\\left(G_{i}+P_{\\mathcal{H}_{i}} \\phi_{e_{i}}\\left(t, \\widetilde{x}_{i}\\right)\\right)}{\\sum_{i}^{N} \\rho_{i}(t, x) \\phi_{w_{i}}\\left(t, \\widetilde{x}_{i}\\right)}\n$$\n\nthe estimator of the conditional expectation $e(t, x)$ defined in (15) and we rewrite\n\n$$\n\\ell_{y}\\left(\\phi, T_{k}, T_{k+1}\\right)=\\int_{T_{k}}^{T_{k+1}} \\frac{c_{t}^{2}}{\\sigma_{t}^{4}} \\mathbb{E}_{X_{y}(t) \\sim \\mathcal{N}\\left(y, \\sigma_{t}^{2} \\operatorname{ld}_{D}\\right)}\\left\\|\\phi_{e}(t, X(t, y))-y\\right\\|^{2} d t\n$$\n\nNote that we have\n\n$$\n\\phi_{e}(t, X(t, y))-y=\\frac{\\sum_{i}^{N} \\rho_{i}(t, X(t, y)) \\phi_{w_{i}}\\left(t, \\widetilde{X}_{i}(t, y)\\right)\\left(G_{i}+P_{\\mathcal{H}_{i}} \\phi_{e_{i}}\\left(t, \\widetilde{X}_{i}(t, y)\\right)-y\\right)}{\\sum_{i}^{N} \\rho_{i}(t, X(t, y)) \\phi_{w_{i}}\\left(t, \\widetilde{X}_{i}(t, y)\\right)}\n$$\n\nSince, $x_{i}=c_{t} G_{i}+P_{\\mathcal{H}_{i}} \\widetilde{x}_{i}$, the condition $\\left\\|\\frac{c_{t} \\phi_{e_{i}^{*}}\\left(t, \\tilde{x}_{i}\\right)-\\widetilde{x}_{i}}{\\sigma_{t}}\\right\\|_{\\infty} \\leq C_{e} \\log n$ implies that\n\n$$\n\\left\\|\\frac{c_{t} G_{i}+c_{t} P_{\\mathcal{H}_{i}} \\phi_{e_{i}^{*}}\\left(t, \\tilde{x}_{i}\\right)-x_{i}}{\\sigma_{t}}\\right\\|=\\left\\|\\frac{c_{t} \\phi_{e_{i}^{*}}\\left(t, \\tilde{x}_{i}\\right)-\\widetilde{x}_{i}}{\\sigma_{t}}\\right\\| \\leq C_{e} \\log n \\sqrt{\\operatorname{dim} \\mathcal{H}_{i}} \\leq C_{e} \\log n^{3 / 2}\n$$\n\nSo\n\n$$\n\\begin{aligned}\n& \\left\\|\\phi_{e}(t, X(t, y))-y\\right\\| \\leq \\sum_{i}^{N} \\frac{\\rho_{i}(t, X(t, y)) \\phi_{w_{i}}\\left(t, \\widetilde{X}_{i}(t, y)\\right)}{\\sum_{i}^{N} \\rho_{i}(t, X(t, y)) \\phi_{w_{i}}\\left(t, \\widetilde{X}_{i}(t, y)\\right)}\\left(\\left\\|X_{i}(t, y)-y\\right\\|+\\sigma_{t} C_{e} \\log n^{3 / 2}\\right) \\\\\n& \\leq \\max _{i: \\rho_{i}(t, X(t, y)) \\neq 0}\\left\\|c_{t}^{-1} X_{i}(t, y)-y\\right\\|+\\left(\\sigma_{t} / c_{t}\\right) C_{e} \\log n^{3 / 2} .\n\\end{aligned}\n$$\n\nMoreover, we bound for all $i \\in I_{2}$\n\n$$\n\\left\\|c_{t}^{-1} X_{i}(t, y)-y\\right\\|=\\left\\|G_{i}+P_{\\mathcal{H}_{i}}^{T} P_{\\mathcal{H}_{i}}\\left(y+\\left(\\sigma_{t} / c_{t}\\right) Z_{D}-c_{t} G_{i}\\right)-y\\right\\| \\leq\\left(\\sigma_{t} / c_{t}\\right)\\left\\|P_{\\mathcal{H}_{i}}^{T} P_{\\mathcal{H}_{i}} Z_{D}\\right\\|+\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}^{\\perp}}\\left(y-c_{t} G_{i}\\right)\\right\\|\n$$\n\nand:\n\n- when $T_{k} \\geq n^{-\\frac{2}{2 \\alpha+d}}$ by (52) with probability $1-n^{-2}$ inequality $\\rho_{i}(t, X(t, y))>0$ implies\n\n$$\n\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}^{\\perp}}\\left(y-c_{t} G_{i}\\right)\\right\\| \\leq 3800\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{\\left(C_{\\operatorname{dim}}+9\\right) \\log n+4 d C_{\\log }+\\left(L^{*}\\right)^{2}}\n$$\n\n- when $T_{k} \\leq n^{-\\frac{2}{2 \\alpha+d}}$, (73) implies that with probability $1-n^{-2}$ the inequality $\\rho_{i}(t, X(t, y))>$ 0 implies $\\left\\|y-G_{i}\\right\\| \\leq 3 \\varepsilon_{N}$. So $\\Phi_{i}^{-1}(y)$ is well defined, and $y^{*}=\\Phi_{i}^{*}\\left(\\Phi_{i}^{-1}(y)\\right)$, satisfies $\\left\\|y-y^{*}\\right\\| \\lesssim \\varepsilon_{N}^{\\beta} \\leq n^{-2 \\gamma \\beta}\\left(\\sigma_{t} / c_{t}\\right)$, so, since $y^{*} \\in \\mathcal{A}_{i}=G_{i}+\\mathcal{H}_{i}$ by construction of $\\Phi_{i}^{*}$ with probability $1-n^{-2}$ for $n$ large enough\n\n$$\n\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}^{\\perp}}\\left(y-c_{t} G_{i}\\right)\\right\\| \\leq\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}^{\\perp}}\\left(y-c_{t} G_{i}\\right)\\right\\| \\leq\\left\\|y-y^{*}\\right\\| \\leq\\left(\\sigma_{t} / c_{t}\\right)\n$$\n\n- by Proposition 31 with probability $1-n^{-2}$\n\n$$\n\\left\\|\\operatorname{pr}_{\\mathcal{H}_{i}} Z_{D}\\right\\| \\leq \\sqrt{\\left(C_{\\operatorname{dim}}+2\\right) \\log n+4+4 \\log n}\n$$\n\nCombining the above inequalities, we obtain with probability $1-n^{-2}$ that for all $i \\in I_{2}$ $\\left\\|c_{t}^{-1} X_{i}(t, y)-y\\right\\| \\leq C_{\\ell}\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{\\log n}$, where $C_{\\ell}$ depends on $\\mu$ but not on $D$. Substituting into (82)\n\n$$\n\\left\\|\\phi_{e}(t, X(t, y))-y\\right\\| \\leq\\left(\\sigma_{t} / c_{t}\\right)\\left(C_{\\ell}+C_{e}\\right) \\log n^{3 / 2}\n$$\n\nSubstituting into (81), and noting that $\\left\\|\\phi_{e}(t, X(t, y))-y\\right\\| \\leq 2$ a.s. we get that for all $y \\in M$ and all $\\phi \\in \\mathcal{S}$\n\n$$\n\\begin{aligned}\n\\ell_{y}\\left(\\phi, T_{k}, T_{k+1}\\right) & =\\int_{T_{k}}^{T_{k+1}} \\frac{c_{t}^{2}}{\\sigma_{t}^{4}} \\mathbb{E}_{X_{y}(t) \\sim \\mathcal{N}\\left(y, \\sigma_{t} \\operatorname{ld}_{d}\\right)}\\left\\|\\phi_{e}(t, X(t, y))-y\\right\\|^{2} d t \\\\\n& \\leq \\int_{T_{k}}^{T_{k+1}} \\frac{c_{t}^{2}}{\\sigma_{t}^{4}}\\left(4 n^{-2}+\\left(\\sigma_{t} / c_{t}\\right)^{2}\\left(C_{\\ell}+C_{e}\\right)^{2} \\log n^{3}\\right) d t \\leq\\left(C_{\\ell}+C_{e}+1\\right)^{2} \\log ^{3} n\n\\end{aligned}\n$$\n\nTo complete the proof we thus only now need to control the covering number of $\\mathcal{L}=$ $\\left\\{y \\mapsto \\ell_{y}\\left(\\phi, T_{k}, T_{k+1}\\right) ; \\phi \\in \\mathcal{S}_{k}\\right\\}$ and show that for $\\delta \\geq 1 / n$\n\n$$\n\\log \\mathcal{N}\\left(\\mathcal{L},\\|\\cdot\\|_{L_{\\infty}(M)}, \\delta\\right) \\lesssim N(\\operatorname{polylog} n)\\left(\\log \\delta^{-1}\\right)\n$$\n\nTo do this we follow the proof of (Oko et al., 2023, Lemma C.2). First note that using Lemma 3 of Suzuki (2019), the covering number of $\\Psi(L, W, S, B)$ is bounded by\n\n$$\n\\log \\mathcal{N}(\\delta, \\Psi(L, W, S, B),\\|\\cdot\\|_{\\infty}) \\leq 2 S L \\log \\left(\\delta^{-1} L(B \\vee 1)(W+1)\\right)\n$$\n\nWe now explain why this allows us to bound the number of $\\mathcal{S}$ by $\\left\\|\\| \\phi \\|_{2}\\right\\|_{L_{\\infty}\\left([-C, C]^{D+1}\\right)^{-}}$balls, where $\\left\\|\\|\\phi\\|_{2}\\right\\|_{L_{\\infty}\\left([-C, C]^{D+1}\\right)}=\\sup _{(x, t) \\in[-C, C]^{D+1}}\\|\\phi(t, x)\\|_{2}$ required to cover $\\mathcal{S}$. Indeed for all $k$ and $\\phi^{1}, \\phi^{2} \\in \\mathcal{S}_{k}$, by definition of $\\mathcal{S}_{k}$ we have for $j=1,2$,\n\n$$\n\\left\\|\\phi_{e_{t}}^{j}\\right\\|_{\\infty} \\leq 1, \\quad\\left\\|\\frac{c_{t} \\phi_{e_{t}}^{j}\\left(t, \\widetilde{x}_{i}\\right)-\\widetilde{x}_{i}}{\\sigma_{t}}\\right\\|_{\\infty} \\leq C_{e} \\sqrt{\\log n}, \\quad \\text { and } \\quad\\left\\|\\phi_{w_{i}}^{j}\\right\\|_{\\infty} \\geq n^{-C_{w}}\n$$\n\nMoreover, $\\sigma_{t}^{2}>n^{-1}$, so that\n\n$$\n\\begin{aligned}\n& \\left\\|\\phi^{1}-\\phi^{2}\\right\\|=\\frac{c_{t}}{\\sigma_{t}^{2}}\\left\\|\\frac{\\sum_{i}^{N} \\rho_{i}\\left(t, x_{i}\\right) \\phi_{w_{i}}^{1}\\left(t, \\widetilde{x}_{i}\\right)\\left(G_{i}+P_{\\mathcal{H}_{i}} \\phi_{e_{i}}^{1}\\left(t, \\widetilde{x}_{i}\\right)\\right)}{\\sum_{i}^{N} \\rho_{i}(t, x) \\phi_{w_{i}}^{1}\\left(t, \\widetilde{x}_{i}\\right)}-\\frac{\\sum_{i}^{N} \\rho_{i}\\left(t, x_{i}\\right) \\phi_{w_{i}}^{2}\\left(t, \\widetilde{x}_{i}\\right)\\left(G_{i}+P_{\\mathcal{H}_{i}} \\phi_{e_{i}}^{2}\\left(t, \\widetilde{x}_{i}\\right)\\right)}{\\sum_{i}^{N} \\rho_{i}(t, x) \\phi_{w_{i}}^{2}\\left(t, \\widetilde{x}_{i}\\right)}\\right\\| \\\\\n& \\leq \\sum_{i \\in I_{2}} \\sigma_{t}^{-2} \\cdot 2 n^{C_{w}}\\left\\|\\phi_{w_{i}}^{1}-\\phi_{w_{i}}^{2}\\right\\|+\\left\\|\\phi_{e_{t}}^{1}(t, x)-\\phi_{e_{t}}^{2}(t, x)\\right\\| \\\\\n& \\leq n^{C_{w}+1}\\left(\\max _{i}\\left\\|\\phi_{e_{i}}^{1}-\\phi_{e_{i}}^{2}\\right\\|+\\max _{i}\\left\\|\\phi_{w_{i}}^{1}-\\phi_{w_{i}}^{2}\\right\\|\\right)\n\\end{aligned}\n$$\n\nwhich implies that for all $\\delta>0$ and all $C>0$\n\n$$\n\\begin{aligned}\n\\log \\mathcal{N}\\left(\\mathcal{S},\\|\\ \\| \\phi \\|_{2}\\right\\|_{L_{\\infty}\\left([-C, C]^{D+1}\\right)}, \\delta) & \\leq \\log \\mathcal{N}\\left(\\delta n^{-\\left(C_{w}+1\\right)} / 2, \\Psi(L, W, S, B),\\|\\cdot\\|_{\\infty}\\right) \\\\\n& \\lesssim 2 N(\\operatorname{polylog} n) \\log \\left(C \\delta^{-1}\\right)\n\\end{aligned}\n$$\n\nTo prove (85), we then bound for $\\left\\|\\left\\|\\phi^{1}-\\phi^{2}\\right\\|_{2}\\right\\|_{L_{\\infty}\\left([-C_{5} \\sqrt{\\log n}, C_{5} \\sqrt{\\log n}]^{D+1}\\right)} \\leq \\delta^{\\prime}$ with $\\phi^{1}, \\phi^{2} \\in$ $\\mathcal{S}_{k}$,\n\n$$\n\\begin{aligned}\n\\left|\\ell_{y}\\left(\\phi^{1}\\right)-\\ell_{y}\\left(\\phi^{2}\\right)\\right| & =\\left|\\int_{T_{k}}^{T_{k+1}}\\left(\\mathbb{E}\\left\\|\\phi^{1}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-Z_{D} / \\sigma_{t}\\right\\|^{2}-\\mathbb{E}\\left\\|\\phi^{2}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-Z_{D} / \\sigma_{t}\\right\\|^{2}\\right) d t\\right| \\\\\n& =\\int_{T_{k}}^{T_{k+1}} \\frac{c_{t}^{2}}{\\sigma_{t}^{4}} \\mathbb{E}_{X_{y}(t) \\sim \\mathcal{N}\\left(y, \\sigma_{t}^{2} I d_{D}\\right)}\\left\\|\\phi_{e}^{1}\\left(t, X_{y}(t)\\right)-y\\right\\|^{2}-\\left\\|\\phi_{e}^{2}\\left(t, X_{y}(t)\\right)-y\\right\\|^{2}\\right] d t\n\\end{aligned}\n$$\n\nwhere the last equality comes from (81) with $\\phi_{e}^{j}(t, x)=\\sigma_{t}^{2} \\phi^{j}(t, x) / c_{t}+x$. We bound, using (83)\n\n$$\n\\begin{aligned}\n\\left\\|\\phi_{e}^{1}\\left(t, X_{y}(t)\\right)-y\\right\\|^{2} & -\\left\\|\\phi_{e}^{2}\\left(t, X_{y}(t)\\right)-y\\right\\|^{2} \\\\\n& =\\left(\\left\\|\\phi_{e}^{1}\\left(t, X_{y}(t)\\right)-y\\right\\|-\\left\\|\\phi_{e}^{2}\\left(t, X_{y}(t)\\right)-y\\right\\|\\right)\\left(\\left\\|\\phi_{e}^{1}\\left(t, X_{y}(t)\\right)-y\\right\\|+\\left\\|\\phi_{e}^{2}\\left(t, X_{y}(t)\\right)-y\\right\\|\\right) \\\\\n& \\lesssim\\left\\|\\phi_{e}^{1}\\left(t, X_{y}(t)\\right)-\\phi_{e}^{2}\\left(t, X_{y}(t)\\right)\\right\\| \\sigma_{t}(\\log n)^{3 / 2} \\lesssim \\sigma_{t}^{2}\\left\\|\\phi^{1}\\left(t, X_{y}(t)\\right)-\\phi^{2}\\left(t, X_{y}(t)\\right)\\right\\|(\\log n)^{3 / 2}\n\\end{aligned}\n$$\n\nWhen $X_{y}(t) \\in\\left[-C_{5} \\sqrt{\\log n}, C_{5} \\sqrt{\\log n}\\right]^{D+1}$ then\n\n$$\n\\left\\|\\phi^{1}\\left(t, X_{y}(t)\\right)-\\phi^{2}\\left(t, X_{y}(t)\\right)\\right\\| \\leq\\left\\|\\left\\|\\phi^{1}-\\phi^{2}\\right\\|_{2}\\right\\|_{L_{\\infty}\\left([-C_{5} \\sqrt{\\log n}, C_{5} \\sqrt{\\log n}]^{D+1}\\right)} \\leq \\delta^{\\prime}\n$$\n\nAlso\n\n$$\n\\begin{aligned}\n\\mathbb{E}_{X_{y}(t) \\sim \\mathcal{N}\\left(y, \\sigma_{t}^{2} I d_{D}\\right)} & {\\left[\\mathbb{1}_{X_{y}(t) \\notin\\left[-C_{5} \\sqrt{\\log n}, C_{5} \\sqrt{\\log n}\\right]^{D}}\\left\\|\\phi_{e}^{j}\\left(t, X_{y}(t)\\right)-y\\right\\|^{2}\\right]} \\\\\n& \\leq \\sigma_{t}^{2}\\left(C_{\\ell}+C_{e}\\right)^{2}(\\log n)^{3} \\mathbb{P}\\left[\\mathcal{N}\\left(y, \\sigma_{t}^{2} I d_{D}\\right) \\notin\\left[-C_{5} \\sqrt{\\log n}, C_{5} \\sqrt{\\log n}\\right]^{D}\\right]\n\\end{aligned}\n$$\n\nSince for all $y \\in M,\\|y\\|_{\\infty} \\leq C_{\\log }$, for all $C>0$\n\n$$\n\\mathbb{P}\\left[\\mathcal{N}\\left(y, \\sigma_{t}^{2} I d_{D}\\right) \\notin\\left[-C_{5} \\sqrt{\\log n}, C_{5} \\sqrt{\\log n}\\right]^{D}\\right] \\leq \\mathbb{P}\\left(\\left\\|Z_{D}\\right\\|_{\\infty} \\geq C_{5} \\sqrt{\\log n} / 2\\right) \\leq n^{-C}\n$$\n\nsince $\\log D \\lesssim \\log n$ and by choosing $C_{5}$ large enough. We finally have that\n\n$$\n\\left|\\ell_{y}\\left(\\phi^{1}\\right)-\\ell_{y}\\left(\\phi^{2}\\right)\\right| \\lesssim(\\log n)^{3} \\int_{T_{k}}^{T_{k+1}} t^{-1} d t\\left[\\delta^{\\prime}+n^{-C} \\log n^{3}\\right] \\lesssim \\delta^{\\prime}(\\log n)^{4}\n$$\n\nand choosing $\\delta^{\\prime} \\asymp \\delta /(\\log n)^{4}$ leads to $\\left|\\ell_{y}\\left(\\phi^{1}\\right)-\\ell_{y}\\left(\\phi^{2}\\right)\\right| \\leq \\delta$, which in turns implies (85).", "tables": {}, "images": {}}, {"section_id": 35, "text": "# D. 4 Correction of (Oko et al., 2023, Theorem C.4.) \n\nTheorem 34. For all $k \\leq K$ let $\\hat{\\phi}_{k}$ be a minimizer of the empirical risk, i.e. $\\hat{\\phi}_{k}:=$ $\\arg \\min _{\\phi \\in S_{k}} \\sum_{y_{i} \\in \\mathcal{Y}} \\ell_{y_{i}}\\left(\\phi, T_{k}, T_{k+1}\\right)$. Then for any $\\delta_{n}>0$ there is a constant $C_{b}>0$ such that\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{\\mathcal{Y}} \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|\\hat{\\phi}_{k}(t, X(t))-s(t, X(t))\\right\\|^{2} d t \\\\\n& \\leq 3 \\inf _{\\phi \\in S_{k}} \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|\\phi(t, X(t))-s(t, X(t))\\|^{2} d t \\\\\n& \\quad+\\frac{C_{b} \\log ^{6} n}{n}\\left(1 \\vee \\log \\mathcal{N}\\left(\\mathcal{L},\\|\\cdot\\|_{L_{\\infty}(M)}, \\delta_{n}\\right)\\right)+12 \\delta_{n}\n\\end{aligned}\n$$\n\nProof. We start by introducing some notation. We fix an index $k$ and suppress it in what follows. We recall that the de-noising score matching loss $\\ell$ at $y \\in M$ is defined as\n\n$$\n\\ell_{y}\\left(\\phi, T_{k}, T_{k+1}\\right):=\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\left\\|\\phi\\left(t, c_{t} y+\\sigma_{t} Z_{D}+\\sigma_{t}^{-1} Z_{D}\\right\\|^{2} d t\\right.\n$$\n\nWe write $\\ell_{\\phi}(y):=\\ell_{y}\\left(\\phi, T_{k}, T_{k+1}\\right)$ to emphasize the dependence on $y \\in M$. The scorematching risk $R(\\phi, s)(7)$ is\n\n$$\nR(\\phi, s):=\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}\\|\\phi(t, X(t))-s(t, X(t))\\|^{2}\n$$\n\nNote that by $(10)$\n\n$$\n0 \\leq R(\\phi, s):=\\mathbb{E}_{\\mu} l_{y}(\\phi)-\\mathbb{E}_{\\mu} l_{y}(s)\n$$\n\nFinally, we introduce the empirical risk and its deviation from the true risk\n\n$$\nR_{n}(\\phi)=\\frac{1}{n} \\sum_{i=1}^{n} \\ell_{\\phi}\\left(y_{i}\\right), \\quad \\Delta_{n}(\\phi)=\\mathbb{E}_{\\mu} \\ell_{\\phi}(y)-R_{n}(\\phi)\n$$\n\nFix $\\delta_{n}>0$ and let $\\left\\{\\phi_{j}\\right\\}_{j \\leq \\mathcal{N}_{n}}$ be a minimal $\\delta_{n}$-cover of $\\mathcal{L}=\\left\\{\\ell_{\\phi}, \\phi \\in \\mathcal{S}_{k}\\right\\}$ with respect to $L_{\\infty}(M)$, in other words $\\mathcal{N}_{n}:=\\mathcal{N}\\left(\\mathcal{L},\\|\\cdot\\|_{L_{\\infty}(M)}, \\delta_{n}\\right)$. The case $\\log \\mathcal{N}_{n}<1$ is trivial, so we assume that $\\log \\mathcal{N}_{n} \\geq 1$. Slightly abusing notation for all $j \\leq \\mathcal{N}_{n}$ we write index $j$ instead of $\\phi_{j}$, namely $f(j)=f_{j}:=f\\left(\\phi_{j}\\right)$, e.g. $\\ell_{j}=\\ell_{\\phi_{j}}$.\n\nSince $\\hat{\\phi}$ is the minimizer of the empirical risk for any $\\phi \\in S_{k}$\n\n$$\n\\begin{aligned}\nR(\\hat{\\phi}, s) & =R_{n}(\\hat{\\phi})+\\Delta_{n}(\\hat{\\phi})-\\mathbb{E}_{\\mu} \\ell_{s}(y) \\\\\n& \\leq R_{n}(\\phi)-\\mathbb{E}_{\\mu} \\ell_{s}(y)+\\Delta_{n}(\\hat{\\phi}) \\\\\n& =R(\\phi, s)-\\left[\\Delta_{n}(\\phi)-\\Delta_{n}(s)\\right]+\\left[\\Delta_{n}(\\hat{\\phi})-\\Delta_{n}(s)\\right]\n\\end{aligned}\n$$\n\nLet $\\phi^{*}:=\\arg \\min _{\\phi \\in \\mathcal{S}_{k}} R(\\phi, s)$, since $\\left\\{\\phi_{j}\\right\\}_{j \\leq \\mathcal{N}_{n}}$ form $\\delta_{n}$ dense net there are $\\hat{j}$ and $j^{*}$ such that\n\n$$\n\\left\\|\\ell_{\\hat{\\phi}}-\\ell_{\\hat{j}}\\right\\|_{\\infty} \\leq \\delta_{n}, \\quad\\left\\|\\ell_{\\phi^{*}}-\\ell_{j^{*}}\\right\\|_{\\infty} \\leq \\delta_{n}\n$$\n\nSubstituting $\\phi^{*}$ into (89) we conclude\n\n$$\nR(\\hat{\\phi}, s) \\leq \\inf _{\\phi \\in \\mathcal{S}_{k}} R(\\phi, s)-\\left[\\Delta_{n}\\left(j^{*}\\right)-\\Delta_{n}(s)\\right]+\\left[\\Delta_{n}(\\hat{j})-\\Delta_{n}(s)\\right]+5 \\delta_{n}\n$$\n\nSo, it is enough to control $\\Delta_{n}(\\hat{j})-\\Delta_{n}(s)$ and $\\Delta_{n}\\left(j^{*}\\right)-\\Delta_{n}(s)$. For $j \\leq N$ we represent\n\n$$\n\\Delta_{n}(j)-\\Delta_{n}(s)=\\frac{1}{n} \\mathbb{E}_{x_{1: n} \\sim \\mu \\otimes n} \\sum_{i=1}^{n}\\left[\\left(\\ell_{j}\\left(x_{i}\\right)-\\ell_{j}\\left(y_{i}\\right)\\right)-\\left(\\ell_{s}\\left(x_{i}\\right)-\\ell_{s}\\left(y_{i}\\right)\\right)\\right]\n$$\n\nand introduce $g_{j}(x, y)=\\left(\\ell_{j}(x)-\\ell_{j}(y)\\right)-\\left(\\ell_{s}(x)-\\ell_{s}(y)\\right)$. Note that $\\mathbb{E}_{x, y \\sim \\mu} g_{j}(x, y)=0$, while the variance can be bounded by Jensen's inequality as\n\n$$\n\\begin{aligned}\n& \\operatorname{Var}_{x, y \\sim \\mu} g_{j}(y, x)=2 \\operatorname{Var}_{y \\sim \\mu}\\left[\\ell_{j}(y)-\\ell_{s}(y)\\right] \\\\\n& \\quad \\leq 2 \\mathbb{E}_{y \\sim \\mu}\\left[\\left(\\int_{T_{k}}^{T_{k+1}} \\mathbb{E}_{Z_{D}}\\left\\|\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\|^{2}-\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\|^{2} d t\\right)^{2}\\right] \\\\\n& \\quad \\leq 2\\left(T_{k+1}-T_{k}\\right) \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}_{y, Z_{D}}\\left[\\left\\|\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\|^{2}-\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\|^{2}\\right]^{2} d t\n\\end{aligned}\n$$\n\nWriting\n\n$$\n\\begin{aligned}\n& \\left\\|\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\|^{2}-\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\|^{2} \\\\\n& =\\left\\|\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)\\right\\|^{2} \\\\\n& \\quad+2\\left\\langle\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right), s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\rangle\n\\end{aligned}\n$$\n\nand recalling that $T_{k+1}=2 T_{k}$ we obtain the following bound\n\n$$\n\\begin{aligned}\n& \\operatorname{Var}_{x, y \\sim \\mu} g_{j}(y, x) \\leq 8 T_{k} \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}_{y, Z_{D}}\\left\\|\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)\\right\\|^{4} d t \\\\\n& \\quad+32 T_{k} \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}_{y, Z_{D}}\\left\\|\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)\\right\\|^{2}\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\|^{2} d t\n\\end{aligned}\n$$\n\n- By Tweedie's formula $s(t, x)=\\frac{c_{t} \\mathbb{E}\\left[X_{0} \\mid X_{t}=x\\right]-X_{t}}{\\sigma_{t}^{2}}$, and since $\\sigma_{t}^{2} \\geq \\sigma_{T_{k}}^{2} \\geq \\min \\left(1, T_{k}\\right) / 2$, a.s.\n\n$$\n\\left\\|s(t, x)+\\sigma_{t}^{-1} Z_{D}\\right\\|_{\\infty}=\\frac{c_{t}}{\\sigma_{t}^{2}}\\left\\|\\mathbb{E}\\left[X_{0} \\mid X_{t}\\right]-X_{0}\\right\\|_{\\infty} \\leq \\frac{c_{t}}{\\sigma_{t}^{2}} \\leq 2 \\max \\left(1, T_{k}^{-1}\\right)\n$$\n\nwhile by Theorem 12, since $T_{k} \\geq n^{-2}$ with probability at least $1-n^{-4}$ for a constant $C_{s}$ that depends only on $C_{\\log }$ and $d$\n\n$$\n\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\| \\leq \\sigma_{t}^{-1} \\cdot 20 \\sqrt{80 d\\left(C_{\\log }+\\log n\\right)} \\leq C_{s} \\max \\left(1, T_{k}^{-1 / 2}\\right) \\sqrt{\\log n}\n$$\n\n- The construction (20) of $\\phi \\in S_{k}$ mimics Tweedie's representation of the score. Any $\\phi \\in$ $S_{k}$ can be represented as $\\phi(t, x)=\\frac{c_{t}}{\\sigma_{t}^{2}} \\phi_{e}(t, x)-\\frac{x}{\\sigma_{t}^{2}}$ where $\\left\\|\\phi_{e}(t, x)\\right\\|_{\\infty} \\leq 1$. Moreover, as we show in (83) with probability at least $1-n^{-4}$ w.r.t. $Z_{D}$, for all $y \\in M$,\n\n$$\n\\begin{gathered}\n\\left\\|\\phi\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\|=\\frac{c_{t}}{\\sigma_{t}^{2}}\\left\\|\\phi_{e}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-y\\right\\| \\leq 2\\left(C_{l}+C_{e}\\right) \\sigma_{t}^{-1} \\log n^{3 / 2} \\\\\n\\leq 4\\left(C_{l}+C_{e}\\right) \\max \\left(1, T_{k}^{-1 / 2}\\right) \\log n^{3 / 2}\n\\end{gathered}\n$$\n\n- Summarizing, with probability at least $1-2 n^{-4}$\n\n$$\n\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\phi\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)\\right\\| \\leq\\left(4\\left(C_{l}+C_{e}\\right)+C_{s}\\right) \\max \\left(1, T_{k}^{-1 / 2}\\right) \\log ^{3 / 2} n\n$$\n\nThis implies that for $\\log n \\gtrsim T_{k} \\gtrsim n^{-1}$\n\n$$\n\\begin{aligned}\n& 8 T_{k} \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}_{y, Z_{D}}\\left\\|\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)\\right\\|^{4} d t \\\\\n& \\lesssim n^{-4} T_{k}^{2} \\max \\left(1, T_{k}^{-4}\\right)+T_{k} \\max \\left(1, T_{k}^{-1}\\right) \\log ^{3} n \\int_{T_{k}}^{T_{k+1}}\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)\\right\\|^{2} d t \\\\\n& \\lesssim n^{-2}+\\log ^{4} n \\int_{T_{k}}^{T_{k+1}}\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)\\right\\|^{2} d t=n^{-2}+\\log ^{4} n R\\left(\\phi_{j}, s\\right)\n\\end{aligned}\n$$\n\nand similarly\n\n$$\n\\begin{aligned}\n& 16\\left(T_{k+1}-T_{k}\\right) \\int_{T_{k}}^{T_{k+1}} \\mathbb{E}_{y, Z_{D}}\\left\\|\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)\\right\\|^{2}\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\|^{2} d t \\\\\n& \\lesssim n^{-2}+\\log ^{4} n \\int_{T_{k}}^{T_{k+1}}\\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\phi_{j}\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)\\right\\|^{2} d t=n^{-2}+\\log ^{4} n R\\left(\\phi_{j}, s\\right)\n\\end{aligned}\n$$\n\nTherefore, there is a constant $C_{g}$ that depends only on $C_{\\log }$ and $d$ such that\n\n$$\n\\operatorname{Var}_{x, y \\sim \\mu} g_{j}(y, x) \\leq C_{g}\\left(n^{-2}+\\log ^{4} n \\cdot R\\left(\\phi_{j}, s\\right)\\right)\n$$\n\nFinally, we compute an a.s. bound on $g_{j}(x, y)$, combining (84) and (25) there is a constant $C_{\\text {sup }}$ that depends only on $C_{\\log }$ and $d$\n\n$$\n\\left|g_{j}(x, y)\\right| \\leq 2 \\sup _{y \\in M}\\left|\\ell_{s}(y)\\right)|+2 \\sup _{\\phi \\in S_{k}} \\sup _{y \\in M}\\left|\\ell_{j}(y)\\right| \\lesssim \\log ^{3} n+\\log ^{2} n \\leq C_{\\sup } \\log ^{3} n\n$$\n\nRemark 35. This argument also works in the original setting (Oko et al., 2023, Theorem C.4), in this case the inequalities have form\n\n$$\n\\begin{aligned}\n& \\left\\|s\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\| \\lesssim 2 \\max \\left(1, T_{k}^{-1 / 2}\\right) \\sqrt{D \\log n} \\\\\n& \\left\\|\\phi\\left(t, c_{t} y+\\sigma_{t} Z_{D}\\right)-\\sigma_{t}^{-1} Z_{D}\\right\\| \\lesssim 2 \\max \\left(1, T_{k}^{-1 / 2}\\right) \\sqrt{D \\log n}\n\\end{aligned}\n$$\n\nThen, by Bernstein's inequality, there is a positive constant $C_{b}<1 / 5$ depending only on $d, C_{\\log }$\n\n$$\n\\mathbb{P}_{\\mathcal{Y}, \\mathcal{X}}\\left(\\left|\\sum_{j} g_{j}\\left(y_{j}, x_{j}\\right)\\right|>t\\right) \\leq 2 \\exp \\left(-C_{b} \\frac{t^{2}}{n^{-1}+n \\log ^{4} n \\cdot R\\left(\\phi_{j}, s\\right)+t \\log ^{3} n}\\right)\n$$\n\nSo, for $r_{j}=\\max \\left(\\sqrt{n^{-1} \\log \\mathcal{N}_{n}}, \\sqrt{R\\left(\\phi_{j}, s\\right)}\\right)$\n\n$$\n\\begin{aligned}\n& \\mathbb{P}_{\\mathcal{Y}, \\mathcal{X}}\\left(\\left|\\frac{\\sum_{j} g_{j}\\left(y_{j}, x_{j}\\right)}{r_{j}}\\right|>t\\right) \\leq 2 \\exp \\left(-C_{b} \\frac{t^{2}}{r_{j}^{-2} n^{-1}+r_{j}^{-2} n \\log ^{4} n \\cdot R\\left(\\phi_{j}, s\\right)+r_{j}^{-1} t \\cdot \\log ^{3} n}\\right) \\\\\n& \\leq 2 \\exp \\left(-C_{b} \\frac{t^{2}}{2 n \\log ^{4} n+\\left(t / r_{j}\\right) \\cdot \\log ^{3} n}\\right) \\leq 2 \\exp \\left(-C_{b} \\frac{t^{2}}{\\log ^{4} n \\cdot\\left(2 n+t / r_{j}\\right)}\\right)\n\\end{aligned}\n$$\n\nfor $t \\geq 2 \\log n \\sqrt{n \\log \\mathcal{N}_{n}}$, since $r_{j} \\geq \\sqrt{n^{-1} \\log \\mathcal{N}_{n}}$ we have\n\n$$\n-t=-\\frac{t}{2}-\\frac{t}{2} \\leq-\\log n \\sqrt{n \\log \\mathcal{N}_{n}}-\\frac{t}{2} \\frac{\\sqrt{n^{-1} \\log \\mathcal{N}_{n}}}{r_{j}}=-\\frac{1}{2} \\log n \\sqrt{\\frac{\\log \\mathcal{N}_{n}}{n}}\\left(2 n+t / r_{j}\\right)\n$$\n\nimplying that\n\n$$\n-\\frac{t^{2}}{\\left(2 n+t / r_{j}\\right) \\log ^{4} n} \\leq-\\frac{t \\sqrt{\\log \\mathcal{N}_{n}}}{2 \\sqrt{n} \\log ^{3} n}\n$$\n\nTherefore, if we introduce\n\n$$\nT_{n}(j)=\\frac{\\sum g_{j}\\left(y_{j}, x_{j}\\right)}{r_{j}}\n$$\n\nsubstituting this bound we get that for $t \\geq 2 \\log n \\sqrt{n \\log \\mathcal{N}_{n}}$\n\n$$\n\\mathbb{P}_{\\mathcal{Y}, \\mathcal{X}}\\left(\\left|\\sup _{j} T_{n}(j)\\right| \\geq t\\right) \\leq 2 \\mathcal{N}_{n} \\sup _{j} \\exp \\left(-C_{b} \\frac{t^{2}}{2 n \\log ^{4} n+\\left(t / r_{j}\\right) \\cdot \\log ^{3} n}\\right) \\leq 2 \\mathcal{N}_{n} \\exp \\left(-\\frac{C_{b}}{2} \\frac{t \\sqrt{\\log \\mathcal{N}_{n}}}{\\sqrt{n} \\cdot \\log ^{3} n}\\right)\n$$\n\nWe are now ready to bound the expectation. Taking $A=2 C_{b}^{-1} \\log ^{3} n \\sqrt{n \\log \\mathcal{N}_{n}} \\geq 2 \\log n \\sqrt{n \\log \\mathcal{N}_{n}}$ for $n$ large enough\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left|\\sup _{j} T_{n}(j)\\right| & =\\int_{0}^{\\infty} \\mathbb{P}\\left(\\left|\\sup _{j} T_{n}(j)\\right| \\geq t\\right) d t \\leq A+\\int_{A}^{\\infty} 2 \\mathcal{N}_{n} \\exp \\left(-\\frac{C}{2} \\frac{t \\sqrt{\\log \\mathcal{N}_{n}}}{\\sqrt{n} \\cdot \\log ^{3} n}\\right) d t \\\\\n& =A+2 \\mathcal{N}_{n} \\exp \\left(-\\frac{C_{b}}{2} \\frac{A \\sqrt{\\log \\mathcal{N}_{n}}}{\\sqrt{n} \\cdot \\log ^{3} n}\\right) \\frac{2}{C_{b}} \\frac{\\sqrt{n} \\cdot \\log ^{3} n}{\\sqrt{\\log \\mathcal{N}_{n}}} \\leq 4 C_{b}^{-1} \\log ^{3} n \\sqrt{n \\log \\mathcal{N}_{n}}\n\\end{aligned}\n$$\n\nNext we bound $\\mathbb{E}\\left|\\sup _{j} T_{n}^{2}(j)\\right|$ using a similar approach. We have\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left|\\sup _{j} T_{n}^{2}(j)\\right|=\\int_{0}^{\\infty} \\mathbb{P}\\left(\\left|\\sup _{j} T_{n}(j)\\right| \\geq \\sqrt{t}\\right) d t \\leq A+\\int_{A}^{\\infty} 2 \\mathcal{N}_{n} \\exp \\left(-\\frac{C_{b}}{2} \\frac{\\sqrt{t} \\sqrt{\\log \\mathcal{N}_{n}}}{\\sqrt{n} \\cdot \\log ^{3} n}\\right) d t \\\\\n& =A+8 \\mathcal{N}_{n} \\exp \\left(-\\frac{C_{b}}{2} \\frac{\\sqrt{A} \\sqrt{\\log \\mathcal{N}_{n}}}{\\sqrt{n} \\log ^{3} n}\\right) \\frac{n\\left(\\log ^{6} n\\right)}{C_{b}^{2} \\log \\mathcal{N}_{n}}\\left(\\frac{C_{b}}{2} \\frac{\\sqrt{A} \\sqrt{\\log \\mathcal{N}_{n}}}{\\sqrt{n} \\log ^{3} n}+1\\right)\n\\end{aligned}\n$$\n\nand choosing $A=4 C_{b}^{-2}\\left(n \\log \\mathcal{N}_{n}\\right) \\log ^{6} n \\geq\\left(2 \\log n \\sqrt{n \\log \\mathcal{N}_{n}}\\right)^{2}$ we get\n\n$$\n\\mathbb{E}\\left|\\sup _{j} T_{n}^{2}(j)\\right| \\lesssim 4 C_{b}^{-2}\\left(n \\log \\mathcal{N}_{n}\\right) \\log ^{6} n+8 \\frac{n\\left(\\log ^{6} n\\right)}{C_{b}^{2} \\log \\mathcal{N}_{n}}\\left(\\log \\mathcal{N}_{n}+1\\right) \\leq 20 C_{b}^{-2} n \\log \\mathcal{N}_{n} \\log ^{6} n\n$$\n\nWe are finally, ready to bound $\\Delta_{n}(j)-\\Delta_{n}(s)$ after noting that $r_{j} \\leq \\sqrt{\\left(\\log \\mathcal{N}_{n}\\right) / n}+$ $\\sqrt{R\\left(\\phi_{j}, s\\right)}$. Let $J=J(\\mathcal{Y})$ be a random index potentially depending on the sample $\\mathcal{Y}$. Then notice that\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\Delta_{n}(J)-\\Delta_{n}(s)\\right] & =\\frac{1}{n} \\mathbb{E}\\left[r_{J} T_{n}(J)\\right] \\leq \\sqrt{\\frac{\\log \\mathcal{N}_{n}}{n}} \\frac{\\mathbb{E}\\left[\\max _{j}\\left|T_{n}(j)\\right|\\right]}{n}+\\frac{\\mathbb{E}\\left(\\sqrt{R\\left(\\phi_{J}, s\\right)} \\max _{j}\\left|T_{n}(j)\\right|\\right)}{n} \\\\\n& \\leq \\frac{\\sqrt{\\log \\mathcal{N}_{n}}}{n^{3 / 2}} \\mathbb{E}\\left[\\max _{j}\\left|T_{n}(j)\\right|\\right]+\\frac{1}{n}\\left[\\mathbb{E} R\\left(\\phi_{J}, s\\right)\\right]^{1 / 2}\\left[\\mathbb{E} \\max _{j}\\left|T_{n}(j)\\right|^{2}\\right]^{1 / 2} \\\\\n& \\leq 5 C_{b}^{-1} \\frac{\\sqrt{\\left(\\log \\mathcal{N}_{n}\\right) / n}}{n} \\sqrt{n \\log \\mathcal{N}_{n}} \\log ^{3} n+\\left[\\mathbb{E} R\\left(\\phi_{J}, s\\right)\\right]^{1 / 2} 5 C_{b}^{-1} \\sqrt{\\frac{\\log \\mathcal{N}_{n}}{n}} \\log ^{3} n \\\\\n& \\leq 5 C_{b}^{-1} \\frac{\\log \\mathcal{N}_{n}}{n} \\log ^{3} n+5 C_{b}^{-1}\\left[\\mathbb{E} R\\left(\\phi_{J}, s\\right)\\right]^{1 / 2} \\sqrt{\\frac{\\log \\mathcal{N}_{n}}{n}} \\log ^{3} n\n\\end{aligned}\n$$\n\nand using Young's inequality\n\n$$\n\\begin{aligned}\n& \\leq 5 C_{b}^{-1} \\frac{\\log \\mathcal{N}_{n}}{n} \\log ^{3} n+\\frac{1}{2} \\mathbb{E} R\\left(\\phi_{J}, s\\right)+\\frac{25 C_{b}^{-2}}{2} \\frac{\\log \\mathcal{N}_{n}}{n} \\log ^{6} n \\\\\n& \\leq \\frac{1}{2} \\mathbb{E} R\\left(\\phi_{J}, s\\right)+50 C_{b}^{-2} \\frac{\\log \\mathcal{N}_{n}}{n} \\log ^{6} n\n\\end{aligned}\n$$\n\nSubstituting $J=\\hat{j}$ and recalling that by construction $\\left\\|\\ell_{\\phi_{\\hat{j}}}-\\ell_{\\hat{\\phi}}\\right\\|_{\\infty} \\leq \\delta_{n}$\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\Delta_{n}(\\hat{j})-\\Delta_{n}(s)\\right] \\leq \\frac{1}{2} \\mathbb{E} R\\left(\\phi_{\\hat{j}}, s\\right)+50 C_{b}^{-2} \\frac{\\log \\mathcal{N}_{n}}{n} \\log ^{6} n \\\\\n& \\leq \\frac{1}{2} \\mathbb{E} R(\\hat{\\phi}, s)+\\frac{1}{2} \\delta_{n}+50 C_{b}^{-2} \\frac{\\log \\mathcal{N}_{n}}{n} \\log ^{6} n\n\\end{aligned}\n$$\n\nWhile substituting $J=j^{*}$ and recalling that $\\left\\|\\ell_{\\phi_{j^{*}}}-\\ell_{\\phi^{*}}\\right\\|_{\\infty} \\leq \\delta_{n}$ and $\\phi^{*}=\\arg \\min _{\\phi \\in S_{k}} R(\\phi, s)$\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\Delta_{n}\\left(j^{*}\\right)-\\Delta_{n}(s)\\right] \\leq \\frac{1}{2} \\mathbb{E} R\\left(\\phi_{j^{*}}, s\\right)+50 C_{b}^{-2} \\frac{\\log \\mathcal{N}_{n}}{n} \\log ^{6} n \\\\\n& \\leq \\frac{1}{2} \\min _{\\phi \\in S_{k}} R(\\phi, s)+\\frac{1}{2} \\delta_{n}+50 C_{b}^{-2} \\frac{\\log \\mathcal{N}_{n}}{n} \\log ^{6} n\n\\end{aligned}\n$$\n\nThat finally, the above combined with (90) gives us the desired bound\n\n$$\n\\begin{aligned}\n\\mathbb{E} R(\\hat{\\phi}, s) & \\leq \\inf _{\\phi \\in \\mathcal{S}_{k}} R(\\phi, s)-\\mathbb{E}\\left[\\Delta_{n}\\left(j^{*}\\right)-\\Delta_{n}(s)\\right]+\\mathbb{E}\\left[\\Delta_{n}(\\hat{j})-\\Delta_{n}(s)\\right]+5 \\delta_{n} \\\\\n& \\leq \\frac{1}{2} \\mathbb{E} R(\\hat{\\phi}, s)+\\frac{3}{2} \\min _{\\phi \\in S_{k}} R(\\phi, s)+6 \\delta_{n}+50 C_{b}^{-2} \\frac{\\log \\mathcal{N}_{n}}{n} \\log ^{6} n\n\\end{aligned}\n$$", "tables": {}, "images": {}}, {"section_id": 36, "text": "# Appendix E. Proof of Corollary 6 \n\nThe corollary is a consequence of Theorem 7 together with the following two inequalities:\n(i) Recall that (9) states that\n\n$$\nD_{\\mathrm{KL}}\\left(\\hat{Y}(\\bar{T}-\\underline{T})\\|X(\\underline{T})\\right) \\lesssim D e^{-2 \\bar{T}}+\\int_{\\underline{T}}^{\\bar{T}} \\mathbb{E}\\|\\hat{s}(t, X(t))-s(t, X(t))\\|^{2} d t\n$$\n\nTaking $\\bar{T}=\\log D+\\log n=O(\\log n)$ bounds the first term by $o(1 / n)$ and using Theorem 5, with $1 \\underline{T}=n^{\\gamma} n^{-2(\\alpha+1) /(2 \\alpha+d)}$ when $d>2$ and $\\underline{T}=n^{\\gamma-1}$ for $d \\leq 2$; we bound the second term of the above inequality by the required rate.\n(ii) Recall that (8) states that\n\n$$\nW_{1}\\left(\\hat{Y}_{\\bar{T}-\\underline{T}}, \\mu\\right) \\lesssim \\sqrt{D}\\left(\\sqrt{\\underline{T}}+\\sum_{k=0}^{K-1} \\sqrt{\\log \\delta^{-1} \\cdot \\sigma_{T_{k+1}}^{2} \\int_{T_{k}}^{T_{k+1}} \\int_{\\mathbb{R}^{D}}\\|\\hat{s}(t, x)-s(t, x)\\|^{2} p(t, x) d x d t}+\\delta+e^{-\\bar{T}}\\right)\n$$\n\nSince $T_{k+1}=2 T_{k}$ for $t \\in\\left[T_{k}, T_{k+1}\\right]$ we have $\\sigma_{T_{k+1}}^{2} \\leq 2 \\sigma_{t}^{2}$. By Theorem 5\n\n$$\n\\begin{aligned}\n& \\mathbb{E}_{\\mu \\otimes n} \\sigma_{T_{k+1}}^{2} \\int_{T_{k}}^{T_{k+1}} \\int_{\\mathbb{R}^{D}}\\|\\hat{s}(t, x)-s(t, x)\\|^{2} p(t, x) d x d t \\\\\n& \\leq 2 \\mathbb{E}_{\\mu \\otimes n} \\int_{T_{k}}^{T_{k+1}} \\int_{\\mathbb{R}^{D}} \\sigma_{t}^{2}\\|\\hat{s}(t, x)-s(t, x)\\|^{2} p(t, x) d x d t \\lesssim n^{2 \\gamma \\alpha} n^{-\\frac{2(\\alpha+1)}{2 \\alpha+d}}\n\\end{aligned}\n$$\n\nBy construction $K=O(\\log n)$, so summing up over all $1 \\leq k \\leq K$, substituting $\\delta=n^{-\\frac{\\alpha+1}{2 \\alpha+d}}, \\bar{T}=\\log D+\\log n=O(\\log n)$, and, finally, using Jensen's inequality we have\n\n$$\n\\mathbb{E}_{\\mu \\otimes n} W_{1}\\left(\\hat{Y}_{\\bar{T}-\\underline{T}}, \\mu\\right) \\lesssim \\sqrt{D} n^{\\gamma \\alpha} n^{-\\frac{\\alpha+1}{2 \\alpha+d}}\n$$", "tables": {}, "images": {}}, {"section_id": 37, "text": "## Appendix F. Auxilarly Results on Neural Networks\n\nBelow we list the results on the neural networks from Oko et al. (2023) that were used to build the approximation in the Euclidean case.\n\nLemma 36. (Oko et al., 2023, Lemma F.1, Concatenation of neural networks.). For any neural networks\n\n$$\n\\phi^{1}: \\mathbb{R}^{d_{1}} \\rightarrow \\mathbb{R}^{d_{2}}, \\phi^{2}: \\mathbb{R}^{d_{2}} \\rightarrow \\mathbb{R}^{d_{3}}, \\ldots, \\phi^{k}: \\mathbb{R}^{d_{k}} \\rightarrow \\mathbb{R}^{d_{k+1}}\n$$\n\nwith $\\phi^{i} \\in \\Psi\\left(L^{i}, W^{i}, S^{i}, B^{i}\\right)(i=1,2, \\ldots, d)$, there exists a neural network $\\phi \\in \\Phi(L, W, S, B)$ satisfying $\\phi(x)=\\phi^{k} \\circ \\phi^{k-1} \\circ \\cdots \\circ \\phi^{1}(x)$ for all $x \\in \\mathbb{R}^{d_{1}}$, with\n\n$$\nL=\\sum_{i=1}^{k} L^{i}, \\quad W \\leq 2 \\sum_{i=1}^{k} W^{i}, \\quad S \\leq \\sum_{i=1}^{k} S^{i}+\\sum_{i=1}^{k-1}\\left(\\left\\|A_{L_{i}}^{i}\\right\\|_{0}+\\left\\|b_{L_{i}}^{i}\\right\\|_{0}+\\left\\|A_{1}^{i+1}\\right\\|_{0}\\right) \\leq 2 \\sum_{i=1}^{k} S^{i}\n$$\n\nand $B \\leq \\max _{1 \\leq i \\leq k} B^{i}$.\nHere $A_{j}^{i}$ is the parameter matrix and $b_{j}^{i}$ is the bias vector at the $j$ th layer of the $i$ th neural network $\\phi^{i}$.\nLemma 37. (Oko et al., 2023, Lemma F.1, Parallelization of neural networks.).\nFor any neural networks $\\phi^{1}, \\phi^{2}, \\ldots, \\phi^{k}$ with $\\phi^{i}: \\mathbb{R}^{d_{i}} \\rightarrow \\mathbb{R}^{d_{i}^{\\prime}}$ and\n\n$$\n\\phi^{i} \\in \\Psi\\left(L^{i}, W^{i}, S^{i}, B^{i}\\right) \\quad(i=1,2, \\ldots, d)\n$$\n\nthere exists a neural network $\\phi \\in \\Psi(L, W, S, B)$ satisfying\n\n$$\n\\phi(x)=\\left[\\phi^{1}\\left(x_{1}\\right)^{\\top} \\phi^{2}\\left(x_{2}\\right)^{\\top} \\cdots \\phi^{k}\\left(x_{k}\\right)^{\\top}\\right]^{\\top}: \\mathbb{R}^{d_{1}+d_{2}+\\cdots+d_{k}} \\rightarrow \\mathbb{R}^{d_{1}^{\\prime}+d_{2}^{\\prime}+\\cdots+d_{k}^{\\prime}}\n$$\n\nfor all $x=\\left(x_{1}^{\\top} x_{2}^{\\top} \\cdots x_{k}^{\\top}\\right)^{\\top} \\in \\mathbb{R}^{d_{1}+d_{2}+\\cdots+d_{k}}$ (here $x_{i}$ can be shared), with\n$L=L_{i}, \\quad\\|W\\|_{\\infty} \\leq \\sum_{i=1}^{k}\\left\\|W^{i}\\right\\|_{\\infty}, \\quad S \\leq \\sum_{i=1}^{k} S^{i}, \\quad$ and $\\quad B \\leq \\max _{1 \\leq i \\leq k} B^{i} \\quad$ (when $L=L_{i}$ holds for all $i$ ),\n$L=\\max _{1 \\leq i \\leq k} L^{i}, \\quad\\|W\\|_{\\infty} \\leq 2 \\sum_{i=1}^{k}\\left\\|W^{i}\\right\\|_{\\infty}, \\quad S \\leq 2 \\sum_{i=1}^{k}\\left(S^{i}+L W_{L}^{i}\\right), \\quad$ and $\\quad B \\leq \\max \\left\\{\\max _{1 \\leq i \\leq k} B^{i}, 1\\right\\} \\quad$ (otherwise).\nMoreover, there exists a network $\\phi_{\\text {sum }}(x) \\in \\Phi(L, W, S, B)$ that realizes $\\sum_{i=1}^{k} \\phi^{i}(x)$, with\n$L=\\max _{1 \\leq i \\leq k} L^{i}+1, \\quad\\|W\\|_{\\infty} \\leq 4 \\sum_{i=1}^{k}\\left\\|W^{i}\\right\\|_{\\infty}, \\quad S \\leq 4 \\sum_{i=1}^{k}\\left(S^{i}+L W_{L}^{i}\\right)+2 W L, \\quad$ and $\\quad B \\leq \\max \\left\\{\\max _{1 \\leq i \\leq k} B^{i}, 1\\right\\}$.\nLemma 38. (Oko et al., 2023, Lemma F.6, Approximation of monomials.). Let $d \\geq 2$, $C \\geq 1,0<\\epsilon_{\\text {error }} \\leq 1$. For any $\\epsilon>0$, there exists a neural network $\\phi_{\\text {mult }}\\left(x_{1}, x_{2}, \\ldots, x_{d}\\right) \\in$ $\\Psi(L, W, S, B)$ with\n\n$$\nL=\\mathcal{O}\\left(\\log d\\left(\\log \\epsilon^{-1}+d \\log C\\right)\\right), \\quad\\|W\\|_{\\infty}=48 d, \\quad S=\\mathcal{O}\\left(d \\log \\epsilon^{-1}+d \\log C\\right), \\quad B=C^{d}\n$$\n\nsuch that\n$\\left|\\phi_{\\text {mult }}\\left(x^{1}, x^{2}, \\ldots, x^{d}\\right)-\\prod_{d^{\\prime}=1}^{d} x_{d^{\\prime}}\\right| \\leq \\epsilon+d C^{d-1} \\epsilon_{\\text {error }}, \\quad$ for all $x \\in[-C, C]^{d}$ and $x^{\\prime} \\in \\mathbb{R}$ with $\\left\\|x-x^{\\prime}\\right\\|_{\\infty} \\leq \\epsilon_{\\text {error }}$.\nLemma 39. (Oko et al., 2023, Lemma F.7, Approximating the reciprocal function). For any $0<\\epsilon<1$, there exists $\\phi_{\\text {rec }} \\in \\Psi(L, W, S, B)$ with $L \\leq O\\left(\\log ^{2} \\epsilon^{-1}\\right),\\|W\\|_{\\infty}=O\\left(\\log ^{3} \\epsilon^{-1}\\right)$, $S=O\\left(\\log ^{4} \\epsilon^{-1}\\right)$, and $B=O\\left(\\epsilon^{-2}\\right)$ such that\n\n$$\n\\left|\\phi_{\\text {rec }}\\left(x^{\\prime}\\right)-\\frac{1}{x}\\right| \\leq \\epsilon+\\frac{\\left|x^{\\prime}-x\\right|}{\\epsilon^{2}}, \\quad \\text { for all } x \\in\\left[\\epsilon, \\epsilon^{-1}\\right] \\text { and } x^{\\prime} \\in \\mathbb{R}\n$$\n\nLemma 40. (Oko et al., 2023, Section B.1) Constants $c_{t}$, and $\\sigma_{t}$ with an error less than $\\varepsilon$ can be computed by neural networks $\\phi_{c}(t), \\phi_{\\sigma}(t)$ of polylog $\\varepsilon^{-1}$ size.\n\nProposition 41. Let $x_{1}, x_{2}, \\ldots, x_{n} \\in R$, then there are neural networks $\\phi_{\\max }, \\phi_{\\min } \\in$ $\\Psi(L, B, S, W)$ where $L=O(\\log n),\\|W\\|_{\\infty}, S=O(n)$ and $B=O(1)$ such that\n\n$$\n\\begin{aligned}\n& \\phi_{\\max }\\left(x_{1}, \\ldots, x_{n}\\right)=\\max \\left(x_{1}, \\ldots, x_{n}\\right) \\\\\n& \\phi_{\\min }\\left(x_{1}, \\ldots, x_{n}\\right)=\\min \\left(x_{1}, \\ldots, x_{n}\\right)\n\\end{aligned}\n$$\n\nProof. Since $\\min \\left(x_{1}, \\ldots, x_{n}\\right)=-\\max \\left(-x_{1}, \\ldots,-x_{n}\\right)$ it is enough to build $\\phi_{\\max }$. Let $a, b \\in$ $\\mathbb{R}$, then\n\n$$\n\\max (a, b)=a+\\max (0, b-a)=\\operatorname{ReLU}(a)+\\operatorname{ReLU}(-a)+\\operatorname{ReLU}(a-b)\n$$\n\nSo, max can be implemented as 1-layer network of constant size. Representing\n\n$$\n\\max \\left(x_{1}, \\ldots, x_{n}\\right)=\\max \\left(\\max \\left(x_{1}, \\ldots, x_{n / 2}\\right), \\max \\left(x_{n / 2+1}, \\ldots, x_{n}\\right)\\right)\n$$\n\nwe get the required statement by induction.", "tables": {}, "images": {}}, {"section_id": 38, "text": "# Appendix G. Bounds on Tangent Spaces\n## G. 1 Projection on Tangent Space as a Denoiser\n\nIn this section, we prove the auxiliary results used to build approximation for $T_{k}<n^{-\\frac{1}{2 a+d}}$.\n\nProposition 42. For all $i$ satisfying $X(0) \\in \\Phi_{i}\\left(B_{d}\\left(0,5 \\varepsilon_{N}\\right)\\right)$\n\n$$\n\\mathbb{P}\\left(\\left\\|m_{i}(X(t))-X(0)\\right\\| \\leq O\\left(\\varepsilon_{N}^{\\beta}\\right)+2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d+\\log n+2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta\n$$\n\nProof. Fix some index $i$, and consider $z \\in B\\left(0,4 \\varepsilon_{N}\\right)$, then since $\\Phi_{i}^{*}(z)-G_{i}-P_{i}^{*} z \\perp \\operatorname{Im} P_{i}^{*}$ and $P_{i}^{*}: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{D}$ is a isometric linear embedding map we have $\\left\\|P_{i}^{*}\\right\\| \\leq 1$ implying\n$\\left(P_{i}^{*}\\right)^{T}\\left(c_{t} \\Phi_{i}(z)+\\sigma_{t} Z_{D}-c_{t} G_{i}\\right)=c_{t}\\left(P_{i}^{*}\\right)^{T}\\left(\\Phi_{i}(z)-\\Phi_{i}^{*}(z)\\right)+c_{t} z+\\sigma_{t}\\left(P_{i}^{*}\\right)^{T} Z_{D}=c_{t} z+\\sigma_{t}\\left(P_{i}^{*}\\right)^{T} Z_{D}+O\\left(\\varepsilon_{N}^{\\beta}\\right)$.\nSince $\\left(P_{i}^{*}\\right)^{T} Z_{D} \\sim \\mathcal{N}\\left(0, \\operatorname{Id}_{d}\\right)$, applying bounds on the tails of Gaussian distribution Laurent and Massart (2000) we get\n$\\mathbb{P}\\left(\\sup _{z \\in B(0, \\varepsilon)}\\left\\|\\left(P_{i}^{*}\\right)^{T}\\left(c_{t} \\Phi_{i}(z)+\\sigma_{t} Z_{D}-c_{t} G_{i}\\right)-c_{t} z\\right\\| / \\sigma_{t} \\leq O\\left(\\varepsilon_{N}^{\\beta} / \\sigma_{t}\\right)+\\sqrt{d+2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta$\nor equivalently\n\n$$\n\\begin{gathered}\n\\mathbb{P}\\left(\\forall y \\in M_{i}:\\left\\|\\left(P_{i}^{*}\\right)^{T}\\left(c_{t} y+\\sigma_{t} Z_{D}-c_{t} G_{i}\\right)-c_{t}\\left(P_{i}^{*}\\right)^{T}\\left(y-G_{i}\\right)\\right\\| \\leq O\\left(\\varepsilon_{N}^{\\beta}\\right)+\\sigma_{t} \\sqrt{d+2 \\log \\delta^{-1}}\\right) \\\\\n\\geq 1-\\delta\n\\end{gathered}\n$$\n\nSince, $|\\mathcal{G}|=N \\leq n$ substituting $\\delta / n$ and combining the events we get that for all $i \\leq N$, $y \\in M_{i}$ with probability at least $1-\\delta$\n\n$$\n\\left\\|\\left(P_{i}^{*}\\right)^{T}\\left(c_{t} y+\\sigma_{t} Z_{D}-c_{t} G_{i}\\right)-c_{t}\\left(P_{i}^{*}\\right)^{T}\\left(y-G_{i}\\right)\\right\\| \\leq O\\left(\\varepsilon_{N}^{\\beta}\\right)+\\sigma_{t} \\sqrt{d+\\log n+2 \\log \\delta^{-1}}\n$$\n\nSubstituting $X(t)=c_{t} X(0)+\\sigma_{t} Z_{D}$, where $X(0) \\sim \\mu$ we conclude that with probability at least $1-\\delta$ for index $i$ satisfying $X(0) \\in M_{i}$\n\n$$\n\\left\\|\\left(P_{i}^{*}\\right)^{T}\\left(X(t)-c_{t} G_{i}\\right)-c_{t}\\left(P_{i}^{*}\\right)^{T}\\left(X(0)-G_{i}\\right)\\right\\| \\leq O\\left(\\varepsilon_{N}^{\\beta}\\right)+\\sigma_{t} \\sqrt{d+\\log n+2 \\log \\delta^{-1}}\n$$\n\nFinally, since $\\Phi_{i}$ is 2-Lipschitz when $\\varepsilon_{N}$ small enough with probability at least $1-\\delta$\n\n$$\n\\left\\|m_{i}(X(t))-\\Phi_{i}\\left(\\left(P_{i}^{*}\\right)^{T}\\left(X(0)-G_{i}\\right)\\right)\\right\\| \\leq O\\left(\\varepsilon_{N}^{\\beta}\\right)+2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d+\\log n+2 \\log \\delta^{-1}}\n$$\n\nWe left to show that for $y \\in M_{i}$ points $\\Phi_{i}\\left(\\left(P_{i}^{*}\\right)^{T}\\left(y-G_{i}\\right)\\right)$ and $y$ are close . By Proposition 18 we have that\n\n$$\n\\left\\|\\left(\\left(P_{i}^{*}\\right)^{T}-P_{i}^{T}\\right)\\left(y-G_{i}\\right)\\right\\| \\leq 4 \\varepsilon_{N}\\left\\|\\left(P_{i}^{*}-P_{i}\\right)^{T}\\right\\|_{o p} \\lesssim \\varepsilon_{N}^{\\beta}\n$$\n\nSince $\\Phi_{i}$ is 2-Lipschitz and $\\Phi_{i}$ is local inverse of $P_{i}^{T}$ we have that\n\n$$\n\\left\\|\\Phi_{i}\\left(\\left(P_{i}^{*}\\right)^{T}\\left(y-G_{i}\\right)\\right)-y\\right\\|=\\left\\|\\Phi_{i}\\left(\\left(P_{i}^{*}\\right)^{T}\\left(y-G_{i}\\right)\\right)-\\Phi_{i}\\left(P_{i}^{T}\\left(y-G_{i}\\right)\\right)\\right\\| \\lesssim \\varepsilon_{N}^{\\beta}\n$$\n\nthat finishes the proof.", "tables": {}, "images": {}}, {"section_id": 39, "text": "# G. 2 Correlation Between Tangent Vectors and Gaussian Noise \n\nThe following lemma is the second order counterpart of Proposition 28.\nLemma 43. Let $f(z): B_{d}(0, \\varepsilon) \\mapsto \\mathbb{R}^{D}$ satisfy $\\|f\\|_{C^{\\beta}\\left(B_{d}(0, \\varepsilon)\\right)} \\leq L$ for $\\beta \\geq 2$ and let $Z_{D} \\simeq$ $\\mathcal{N}\\left(0, \\operatorname{Id}_{D}\\right)$. Let $h\\left(z_{1}, z_{2}\\right)=f\\left(z_{1}\\right)-f\\left(z_{2}\\right)-\\nabla f\\left(z_{2}\\right)\\left(z_{1}-z_{2}\\right)$ - an error at $z_{1}$ of the linear approximation of $f$ at point $z_{2}$, then\n\n$$\n\\mathbb{P}\\left(\\sup _{z_{1}, z_{2} \\in B(0, \\varepsilon)}\\left|\\left\\langle Z_{D}, h\\left(z_{1}, z_{2}\\right)\\right\\rangle\\right| \\leq L \\varepsilon^{2}(4 \\sqrt{d}+\\sqrt{2 \\log \\delta})\\right) \\geq 1-\\delta\n$$\n\nProof. The function $h\\left(z_{1}, z_{2}\\right): B_{d}(0, \\varepsilon) \\times B_{d}(0, \\varepsilon) \\subset B_{2 d}(0,2 \\varepsilon) \\mapsto \\mathbb{R}^{D}$. Since $h(0,0)=0$ to apply Proposition 28 it is enough to estimate the Lipschitz constant of $h\\left(z_{1}, z_{2}\\right)$.\n\nFirst, let us prove an auxiliary bound on $\\|h\\|_{\\infty}$. Since $\\|f\\|_{C^{\\beta}\\left(B_{d}(0, \\varepsilon)\\right)} \\leq L$ integrating along the the segment connecting $z_{1}$ and $z_{2}$\n$h\\left(z_{1}, z_{2}\\right)=f\\left(z_{1}\\right)-f\\left(z_{2}\\right)-\\nabla f\\left(z_{2}\\right)\\left(z_{1}-z_{2}\\right)=\\int_{0}^{1}\\left\\langle\\nabla f\\left(a z_{1}+(1-a) z_{2}\\right)-\\nabla f\\left(z_{2}\\right), z_{1}-z_{2}\\right\\rangle d a$,\nnote $B(0, \\varepsilon)$ is a convex set, so\n\n$$\n\\left\\|h\\left(z_{1}, z_{2}\\right)\\right\\| \\leq \\sup _{a \\in[0,1]}\\left\\|\\nabla f\\left(a z_{1}+(1-a) z_{2}\\right)-\\nabla f\\left(z_{2}\\right)\\right\\|_{o p}\\left\\|z_{1}-z_{2}\\right\\| \\leq L \\varepsilon^{2}\n$$\n\nTo bound the Lipschitz constant we introduce another pair of points $z_{1}^{\\prime}, z_{2}^{\\prime} \\in B(0, \\varepsilon)$ and estimate\n\n$$\n\\begin{aligned}\n& \\left\\|h\\left(z_{1}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}^{\\prime}\\right)\\right\\|=\\left\\|h\\left(z_{1}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}\\right)+h\\left(z_{1}^{\\prime}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}^{\\prime}\\right)\\right\\| \\\\\n& \\leq\\left\\|h\\left(z_{1}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}\\right)\\right\\|+\\left\\|h\\left(z_{1}^{\\prime}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}^{\\prime}\\right)\\right\\|\n\\end{aligned}\n$$\n\nWe deal with each term separately. We represent the first term as\n\n$$\n\\begin{aligned}\nh\\left(z_{1}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}\\right) & =f\\left(z_{1}\\right)-f\\left(z_{1}^{\\prime}\\right)-\\nabla f\\left(z_{2}\\right)\\left(z_{1}-z_{1}^{\\prime}\\right) \\\\\n& =\\int_{0}^{1}\\left\\langle\\nabla f\\left(a z_{1}+(1-a) z_{1}^{\\prime}\\right)-\\nabla f\\left(z_{2}\\right), z_{1}-z_{1}^{\\prime}\\right\\rangle d a\n\\end{aligned}\n$$\n\nso\n\n$$\n\\left\\|h\\left(z_{1}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}\\right)\\right\\| \\leq\\left\\|z_{1}-z_{1}^{\\prime}\\right\\| \\int_{0}^{1}\\left\\|\\nabla f\\left(a z_{1}+(1-a) z_{1}^{\\prime}\\right)-\\nabla f\\left(z_{2}\\right)\\right\\|_{o p} d a \\leq L \\varepsilon\\left\\|z_{1}-z_{1}^{\\prime}\\right\\|\n$$\n\nThe second term $h\\left(z_{1}^{\\prime}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}^{\\prime}\\right)$ is represented in a similar way\n\n$$\nh\\left(z_{1}^{\\prime}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}^{\\prime}\\right)=\\int_{0}^{1}\\left\\langle\\nabla_{z_{a}} h\\left(z_{1}^{\\prime}, z_{a}\\right), z_{2}-z_{2}^{\\prime}\\right\\rangle d a\n$$\n\nwhere $z_{a}=a z_{2}+(1-a) z_{2}^{\\prime}$. The gradient can be computed directly\n\n$$\n\\nabla_{z_{2}} h\\left(z_{1}, z_{2}\\right)=\\nabla_{z_{2}}\\left(f\\left(z_{2}\\right)-\\nabla_{z_{2}} f\\left(z_{2}\\right)\\left(z_{2}-z_{1}\\right)\\right)=-\\left(\\nabla_{z_{2}}^{2} f\\left(z_{2}\\right)\\right)\\left(z_{2}-z_{1}\\right)\n$$\n\nso we get a bound\n\n$$\n\\left\\|h\\left(z_{1}^{\\prime}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}^{\\prime}\\right)\\right\\| \\leq\\left\\|z_{2}-z_{2}^{\\prime}\\right\\| \\int_{0}^{1}\\left\\|\\nabla^{2} f\\left(z_{a}\\right)\\right\\|_{o p}\\left\\|z_{a}-z_{1}^{\\prime}\\right\\| d a \\leq L \\varepsilon\\left\\|z_{2}-z_{2}^{\\prime}\\right\\|\n$$\n\nSumming (92) and (93) up we conclude\n\n$$\n\\left\\|h\\left(z_{1}, z_{2}\\right)-h\\left(z_{1}^{\\prime}, z_{2}^{\\prime}\\right)\\right\\| \\leq L \\varepsilon\\left(\\left\\|z_{2}-z_{2}^{\\prime}\\right\\|+\\left\\|z_{1}-z_{1}^{\\prime}\\right\\|\\right) \\leq L \\sqrt{2}\\left\\|\\left(z_{1}, z_{2}\\right)-\\left(z_{1}^{\\prime}, z_{2}^{\\prime}\\right)\\right\\|\n$$\n\nApplying Proposition 28 we derive the lemma.\nCorollary 44. Let $\\Phi_{1}^{*}, \\ldots, \\Phi_{N}^{*}: B_{d}(0, \\varepsilon) \\mapsto \\mathbb{R}^{D}$ be $L^{*}$-lipschitz functions then for any positive $\\delta<1$ with probability at least $1-\\delta$ for all $i \\leq N$\n\n$$\n\\sup _{z_{1}, z_{2} \\in B(0, \\varepsilon)}\\left|\\left\\langle Z_{D}, \\Phi_{i}^{*}\\left(z_{1}\\right)-\\Phi_{i}^{*}\\left(z_{2}\\right)-\\nabla \\Phi_{i}^{*}\\left(z_{1}\\right)\\left(z_{1}-z_{2}\\right)\\right\\rangle\\right| \\leq 2 L^{*} \\varepsilon^{2}\\left(4 \\sqrt{d}+\\sqrt{2 \\log N+2 \\log \\delta^{-1}}\\right)\n$$\n\nProof. Apply Lemma 43 for each $\\Phi_{i}^{*}$ with $\\delta / N$ and combine the inequalities.\nThe next lemma is a local bound of the projection on the tangent space of $Z_{D} \\sim \\mathcal{N}(0, \\mathrm{Id})$.\n\nLemma 45. For function $f: \\mathbb{R}^{d} \\mapsto \\mathbb{R}^{D}$ s.t. $f(0)=0$ and $\\nabla^{T} f(0) \\nabla f(0)=\\operatorname{Id}_{d}$ and point $z$ we define\n\n$$\n\\operatorname{pr}_{z}=\\nabla f(z)\\left(\\nabla f^{T}(z) \\nabla f(z)\\right)^{-1} \\nabla^{T} f(z)\n$$\n\nthe projection map onto tangent to the image of $f$ at $f(z)$. Let $\\|f\\|_{C^{\\beta}(B(0, \\varepsilon)} \\leq L$, where $\\beta \\geq 3$ then for any $\\varepsilon<L^{-1} / 8$\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in B(0, \\varepsilon)}\\left\\|\\operatorname{pr}_{z} Z_{D}\\right\\| \\leq 4 d L \\varepsilon+4 \\sqrt{d}(1+L \\varepsilon) \\sqrt{2 \\log 4 d+2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta\n$$\n\nProof. We note that (ii) and (iii) in Proposition 26 holds for any $f$ satisfying $\\|f\\|_{C^{\\beta}(B(0, \\varepsilon)} \\leq L$ and $\\nabla^{T} f(0) \\nabla f(0)=\\operatorname{Id}_{d}$, so we have: (i) $\\|\\nabla f(z)\\|_{o p}<2$; (ii) $\\left\\|\\nabla^{T} f(z) \\nabla f(z)-\\operatorname{Id}_{d}\\right\\|_{o p} \\leq 1 / 2$. The latter implies $\\left\\|\\left(\\nabla^{T} f(z) \\nabla f(z)\\right)^{-1}\\right\\|_{o p} \\leq 2$ and we conclude\n\n$$\n\\begin{aligned}\n\\left\\|\\operatorname{pr}_{z} Z_{D}\\right\\| & =\\left\\|\\nabla f(z)\\left(\\nabla f^{T}(z) \\nabla f(z)\\right)^{-1} \\nabla^{T} f(z) Z_{D}\\right\\| \\\\\n& \\leq 2\\|\\nabla f(z)\\|_{o p}\\left\\|\\nabla^{T} f(z) Z_{D}\\right\\| \\leq 4\\left\\|\\nabla^{T} f(z) Z_{D}\\right\\|\n\\end{aligned}\n$$\n\nSo, it is enough to prove that\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in B(0, \\varepsilon)}\\left\\|\\nabla^{T} f(z) Z_{D}\\right\\| \\leq d L \\varepsilon+\\sqrt{d}(1+L \\varepsilon) \\sqrt{2 \\log 2 d+2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta\n$$\n\nThe vector $\\nabla^{T} f(z) Z_{D} \\in \\mathbb{R}^{d}$, and we bound it coordinate-wise. Let $v_{1}, \\ldots, v_{d}$ be an orthonormal basis in $\\mathbb{R}^{d}$ and $f_{v}=\\frac{d}{d v} f(z)$ then $\\nabla^{T} f(z)=\\left(f_{v_{1}}^{T}(z), \\ldots, f_{v_{d}}^{T}(z)\\right)^{T} \\in \\mathbb{R}^{D \\times d}$. For $z, z^{\\prime} \\in B(0, \\varepsilon)$ using that $\\|f\\|_{C^{\\beta}(0, \\varepsilon)} \\leq L$ we bound $\\left\\|f_{v_{i}}(z)-f_{v_{i}}\\left(z^{\\prime}\\right)\\right\\| \\leq L\\left\\|z-z^{\\prime}\\right\\|$. Applying Proposition 28 we get\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in B(0, \\varepsilon)}\\left|\\left\\langle Z_{D}, f_{v_{i}}(z)-f_{v_{i}}(0)\\right\\rangle\\right| \\leq L \\varepsilon\\left(\\sqrt{d}+\\sqrt{2 \\log \\left(4 d \\delta^{-1}\\right)}\\right)\\right) \\geq 1-\\delta / 2 d\n$$\n\nSince $\\nabla^{T} f(0) \\nabla f(0)=\\operatorname{Id}_{d}$, we note that $\\left\\|f_{v_{i}}(0)\\right\\|=1$. So $\\left\\langle Z_{D}, v_{i}\\right\\rangle \\sim \\mathcal{N}(0,1)$ and\n\n$$\n\\mathbb{P}\\left(\\left|\\left\\langle Z_{D}, f_{v_{i}}(0)\\right\\rangle\\right| \\leq \\sqrt{2 \\log \\left(4 d \\delta^{-1}\\right)}\\right) \\geq 1-\\delta / 2 d\n$$\n\nSumming up\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in B(0, \\varepsilon)}\\left|\\left\\langle Z_{D}, f_{v_{i}}(z)\\right\\rangle\\right| \\leq L \\varepsilon \\sqrt{d}+(1+L \\varepsilon) \\sqrt{2 \\log \\left(4 d \\delta^{-1}\\right)}\\right) \\geq 1-\\delta / d\n$$\n\nCombining inequalities for all $v_{i}$\n\n$$\n\\mathbb{P}\\left(\\forall i: \\sup _{z \\in B(0, \\varepsilon)}\\left|\\left\\langle Z_{D}, f_{v_{i}}(z)\\right\\rangle\\right| \\leq L \\varepsilon \\sqrt{d}+(1+L \\varepsilon) \\sqrt{2 \\log 4 d+2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta\n$$\n\nFinally the inequality\n\n$$\n\\left\\|\\nabla^{T} f(z) Z_{D}\\right\\|^{2}=\\sum_{i=1}^{d}\\left|\\left\\langle Z_{D}, f_{v_{i}}(z)\\right\\rangle\\right|^{2} \\leq d \\cdot \\sup _{i}\\left|\\left\\langle Z_{D}, f_{v_{i}}(z)\\right\\rangle\\right|^{2}\n$$\n\ngives\n\n$$\n\\mathbb{P}\\left(\\sup _{z \\in B(0, \\varepsilon)}\\left\\|\\nabla^{T} f(z) Z_{D}\\right\\| \\leq d L \\varepsilon+\\sqrt{d}(1+L \\varepsilon) \\sqrt{2 \\log 4 d+2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta\n$$\n\nAs a corollary we get the lemma announced in Section 4\nTheorem 10. Let $M$ be a $\\beta \\geq 2$-smooth manifold satisfying Assumption $A$. Then for any $\\delta>0$ with probability at least $1-\\delta$ for all $y \\in M$\n\n$$\n\\left\\|\\operatorname{pr}_{T_{y} M} Z_{D}\\right\\| \\leq 8 \\sqrt{d\\left(4 \\log 2 d+2 \\log r_{0}^{-1}+2 \\log _{+} \\operatorname{Vol} M+2 \\log \\delta^{-1}\\right)}\n$$\n\nProof. Let us take $\\varepsilon<r_{0}$-dense set $\\mathcal{G}=\\left\\{G_{1}, \\ldots, G_{N}\\right\\}$ by Proposition 3 we can guarantee $N \\leq(\\varepsilon / 2)^{-d} \\operatorname{Vol} M$. Applying Lemma 45 with $\\delta / N$ and $f(z)=\\Phi_{G_{i}}(z)$\n$\\mathbb{P}\\left(\\sup _{z \\in B(0, \\varepsilon)}\\left\\|\\operatorname{pr}_{z} Z_{D}\\right\\| \\leq 4 d L_{M} \\varepsilon+4 \\sqrt{d}\\left(1+L_{M} \\varepsilon\\right) \\sqrt{2 \\log 2 d+2 \\log N+2 \\log \\delta^{-1}}\\right) \\geq 1-\\delta / N$.\nWe note that since locally $M$ is a graph of $\\Phi_{G_{i}}$ we have $\\operatorname{pr}_{z} \\equiv \\operatorname{pr}_{\\Phi_{G_{i}}(z)} M$, so combining over all $i \\leq N$, since $M=\\bigcup_{i} \\Phi_{G_{i}}\\left(B_{d}(0, \\varepsilon)\\right)$ we conclude\n\n$$\n\\begin{aligned}\n& \\mathbb{P}\\left(\\sup _{y \\in M}\\left\\|\\operatorname{pr}_{T_{y} M} Z_{D}\\right\\| \\leq 4 d L_{M} \\varepsilon+4 \\sqrt{d}\\left(1+L_{M} \\varepsilon\\right) \\sqrt{2 \\log 2 d+2 \\log \\varepsilon^{-1}+2 \\log _{+}} \\operatorname{Vol} M+2 \\log \\delta^{-1}\\right) \\\\\n& \\geq 1-\\delta\n\\end{aligned}\n$$\n\nTaking $\\varepsilon=(2 d)^{-1} r_{0}$\n\n$$\n\\mathbb{P}\\left(\\sup _{y \\in M}\\left\\|\\operatorname{pr}_{T_{y} M} Z_{D}\\right\\| \\leq 8 \\sqrt{d} \\sqrt{4 \\log 2 d+2 \\log r_{0}^{-1}+2 \\log _{+}} \\operatorname{Vol} M+2 \\log \\delta^{-1}\\right) \\geq 1-\\delta\n$$\n\nwe finish the proof.\nThe following result is a counterpart of Lemma above for $M^{*}$.\nLemma 46. For all $\\delta>0$ with probability at least $1-\\delta$ for all $i \\leq I_{2}$\n\n$$\n\\left\\|\\operatorname{pr}_{z_{i}^{*}} Z_{D}\\right\\| \\leq 8 d L_{M} \\varepsilon_{N}+8 \\sqrt{d}\\left(1+7 L_{M} \\varepsilon_{N}\\right) \\sqrt{2 \\log 2 d+2 \\log n+2 \\log (1 / \\delta)}\n$$\n\nProof. Note that by Lemma 19 for $n$ large enough $\\left\\|\\Phi_{i}^{*}\\right\\|_{C^{2}\\left(B_{d}\\left(0,7 \\varepsilon_{N}\\right)\\right)} \\leq L_{M}+L^{*}(7 \\varepsilon)_{N}^{\\beta-2} \\leq$ $2 L_{M}$ for all $i \\leq N$. Then $\\left\\|\\operatorname{pr}_{z_{i}^{*}} Z_{D}\\right\\| \\leq \\sup _{z \\in B_{d}\\left(0,7 \\varepsilon_{N}\\right)}\\left\\|\\operatorname{pr}_{z} Z_{D}\\right\\|$ Apply Lemma 45 for each $\\Phi_{1}^{*}, \\ldots, \\Phi_{N}^{*}$ with $\\delta / N$ and combine the resulting inequalities using that $N \\leq n$.\n\nLemma 47. For all $i \\in I_{2}$ and $z \\in B_{d}\\left(z_{i}^{*}, 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n)\\right)$ with probability at least $1-n^{-2}$\n\n$$\nc_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2}-c_{t}\\left\\langle X(t)-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle \\lesssim \\sigma_{t}^{3}(d \\log n)^{2}\n$$\n\nProof. By Proposition 42 and choice of $z$ with probability at least $1-n^{-2}$\n\n$$\n\\begin{aligned}\n& \\left\\|X(0)-\\Phi\\left(z_{i}^{*}\\right)\\right\\| \\leq 2\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n) \\lesssim\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d \\log n} \\\\\n& \\left\\|z-z_{i}^{*}\\right\\| \\leq 4\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{20} C(n) \\lesssim\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d \\log n}\n\\end{aligned}\n$$\n\nsince $C(n) \\lesssim \\sqrt{d \\log n}$. We estimate the terms separately. Recall that\n\n$$\nR_{i}\\left(z, z_{i}^{*}\\right)=\\Phi_{i}^{*}\\left(z_{i}^{*}\\right)-\\Phi_{i}^{*}(z)-\\nabla \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\left(z_{i}^{*}-z\\right)\n$$\n\nso by (91) for $n$ large enough\n\n$$\nc_{t}^{2}\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\|^{2} \\leq\\left(2 L^{*}\\left\\|z-z_{i}^{*}\\right\\|^{2}\\right)^{2} \\lesssim\\left(\\sigma_{t} / c_{t}\\right)^{4} d^{2} \\log ^{2} n<\\sigma_{t}^{2}\n$$\n\nWe move the second term\n\n$$\n\\left\\langle X(0)-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle=c_{t}\\left\\langle X(0)-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle+\\sigma_{t} c_{t}\\left\\langle Z_{D}, R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle\n$$\n\nThe first term is bounded similarly using (91)\n\n$$\nc_{t}\\left|\\left\\langle X(0)-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right), R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle\\right| \\leq c_{t}\\left\\|X(0)-c_{t} \\Phi_{i}^{*}\\left(z_{i}^{*}\\right)\\right\\|\\left\\|R_{i}\\left(z, z_{i}^{*}\\right)\\right\\| \\lesssim c_{t}\\left(\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d \\log n}\\right)^{3}\n$$\n\nWhile the second term by Corollary 44 with probability at least $1-n^{-2}$ is bounded by\n\n$$\n\\left|\\sigma_{t} c_{t}\\left\\langle Z_{D}, R_{i}\\left(z, z_{i}^{*}\\right)\\right\\rangle\\right| \\lesssim \\sigma_{t} c_{t}\\left(\\left(\\sigma_{t} / c_{t}\\right) \\sqrt{d \\log n}\\right)^{2} \\lesssim c_{t}^{-1} \\sigma_{t}^{3} d \\log n\n$$\n\nSumming the inequalities up and noting that $1>c_{t} \\geq 1 / 2$ we finish the proof.", "tables": {}, "images": {}}], "id": "2409.18804v2", "authors": ["Iskander Azangulov", "George Deligiannidis", "Judith Rousseau"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "abstract": "Denoising Diffusion Probabilistic Models (DDPM) are powerful state-of-the-art\nmethods used to generate synthetic data from high-dimensional data\ndistributions and are widely used for image, audio, and video generation as\nwell as many more applications in science and beyond. The \\textit{manifold\nhypothesis} states that high-dimensional data often lie on lower-dimensional\nmanifolds within the ambient space, and is widely believed to hold in provided\nexamples. While recent results have provided invaluable insight into how\ndiffusion models adapt to the manifold hypothesis, they do not capture the\ngreat empirical success of these models, making this a very fruitful research\ndirection.\n  In this work, we study DDPMs under the manifold hypothesis and prove that\nthey achieve rates independent of the ambient dimension in terms of score\nlearning. In terms of sampling complexity, we obtain rates independent of the\nambient dimension w.r.t. the Kullback-Leibler divergence, and $O(\\sqrt{D})$\nw.r.t. the Wasserstein distance. We do this by developing a new framework\nconnecting diffusion models to the well-studied theory of extrema of Gaussian\nProcesses.", "updated": "2025-04-23T19:38:36Z", "published": "2024-09-27T14:57:18Z"}