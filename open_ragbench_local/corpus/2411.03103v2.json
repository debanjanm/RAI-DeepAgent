{"title": "Benign landscape for Burer-Monteiro factorizations of MaxCut-type\n  semidefinite programs", "sections": [{"section_id": 0, "text": "## CNRS (UMR 7534), Universit\u00e9 Paris Dauphine, Inria Mokaplan\n#### Abstract\n\nWe consider MaxCut-type semidefinite programs (SDP) which admit a low rank solution. To numerically leverage the low rank hypothesis, a standard algorithmic approach is the Burer-Monteiro factorization, which allows to significantly reduce the dimensionality of the problem at the cost of its convexity. We give a sharp condition on the conditioning of the Laplacian matrix associated with the SDP under which any second-order critical point of the non-convex problem is a global minimizer. By applying our theorem, we improve on recent results about the correctness of the Burer-Monteiro approach on $\\mathbb{Z}_{2}$-synchronization problems and the Kuramoto model.", "tables": {}, "images": {}}, {"section_id": 1, "text": "## 1 Introduction\n### 1.1 Presentation of the problem\n\nSemidefinite programs (SDP) are optimization tools that allow the solving and modeling of a variety of problems across applied sciences. A number of problems admits a SDP formulation in combinatorial optimization [Goemans and Williamson, 1995], machine learning\n\n[^0]\n[^0]:    *rakotoendor@ceremade.dauphine.fr\n    ${ }^{\\dagger}$ waldspurger@ceremade.dauphine.fr\n\nand data sciences [Lanckriet et al., 2004], statistics and signal processing [Cand\u00e8s, Strohmer, and Voroninski, 2013]. In this paper, we are interested in so-called MaxCut-type SDPs:\n\n$$\n\\begin{aligned}\n\\min _{X \\in \\mathbb{S}^{n \\times n}} & -\\langle C, X\\rangle \\\\\n\\text { s.t. } & X \\succeq 0 \\\\\n& \\operatorname{diag}(X)=\\mathbf{1}_{n}\n\\end{aligned}\n$$\n\nwhere the operator diag : $\\mathbb{R}^{n \\times n} \\rightarrow \\mathbb{R}^{n}$ extracts the diagonal of a square matrix, $\\mathbf{1}_{n}=(1 \\ldots 1)^{T} \\in \\mathbb{R}^{n}$ and the symmetric matrix $C \\in \\mathbb{R}^{n \\times n}$ is called the cost matrix. SDP of this form are especially known to provide precise convex relaxations of MaxCut problems from graph optimization [Goemans and Williamson, 1995]. They can also model problems such as $\\mathbb{Z}_{2}$-synchronization [Abbe, Bandeira, Bracher, and Singer, 2014], phase retrieval [Waldspurger, d'Aspremont, and Mallat, 2015] and the Kuramoto model [Kuramoto, 1975; Ling, Xu, and Bandeira, 2019], for particular choices of the cost matrix $C$. Several general methods exist to numerically solve problem (SDP), but they scale poorly with $n$. For instance, interiorpoint solvers require $O\\left(n^{3}\\right)$ computations per iteration and $O\\left(n^{2}\\right)$ to store the variable [Benson, Ye, and Zhang, 2000]. Other methods may have a lower per iteration complexity, but this comes at the cost of less accuracy. For instance, in the first-order method [O'donoghue, Chu, Parikh, and Boyd, 2016], each iteration costs at worse $O\\left(n^{2}\\right)$, but $O(1 / \\varepsilon)$ iterations are required to reach $\\varepsilon$-accuracy.\n\nTo reduce the computational complexity of solvers, one must exploit the specific properties of the problem at hand, if any. For instance, it may be known in advance that the solution to (SDP) is lowrank: [Pataki, 1998] guarantees that there exists a solution with rank bounded by $\\sqrt{2 n}+O(1)$ and, when (SDP) is the relaxation of a combinatorial problem, the optimal rank is often much less (see for instance [Cand\u00e8s, Strohmer, and Voroninski, 2013] for a theoretical justification in a particular case, [Journ\u00e9e, Bach, Absil, and Sepulchre, 2010] for a numerical investigation). In this case, it is possible to tackle the problem using its socalled Burer-Monteiro factorization [Burer and Monteiro, 2003]. The principle is to factor the variable as $X=V V^{T}$, for $V \\in \\mathbb{R}^{n \\times p}$, where $p \\in \\mathbb{N}$ is larger or equal to the rank of the sought solution, and much smaller than $n$.\n\nThen, one optimizes over $V$, instead of directly over $X$ :\n\n$$\n\\begin{aligned}\n\\min _{V \\in \\mathbb{R}^{n \\times p}} & -\\left\\langle C, V V^{T}\\right\\rangle \\\\\n\\text { s.t. } & \\operatorname{diag}\\left(V V^{T}\\right)=\\mathbf{1}_{n}\n\\end{aligned}\n$$\n\n(Burer-Monteiro)\nThis factorized problem can be tackled with a number of standard algorithms. For instance, [Burer and Monteiro, 2003] uses an augmented Lagrangian approach, while [Journ\u00e9e, Bach, Absil, and Sepulchre, 2010] proposes a manifold based second-order method.\n\nIn the factorized problem, the number of variables is reduced to $n p$ instead of $O\\left(n^{2}\\right)$ in the initial problem, which is computationally advantageous when $p \\ll n$. However, the convexity is lost, so standard solvers are not guaranteed to reach the solution. Still, in practice, they oftentimes converge to a global solution $V \\in \\mathbb{R}^{n \\times p}$ of the factorized problem, for which $X=V V^{T}$ solves the initial problem.\n\nIn this article, we focus on the simplest case, where Problem (SDP) has a rank 1 solution $X=\\boldsymbol{x} \\boldsymbol{x}^{T}$, for $\\boldsymbol{x} \\in\\{ \\pm 1\\}^{n}$, and try to understand under which conditions one can certify that standard solvers converge to a global solution. The rank 1 hypothesis is satisfied in applications like $\\mathbb{Z}_{2}$-synchronization and the Kuramoto model.", "tables": {}, "images": {}}, {"section_id": 2, "text": "# 1.2 Prior work and our contribution \n\nThe main explanation proposed in the literature for the success of standard algorithms at solving (Burer-Monteiro) has been the benign non-convexity of the optimization landscape: it may be that all second-order critical points of (Burer-Monteiro) are global minimizers. Since standard algorithms typically find a second-order critical point [Lee, Panageas, Piliouras, Simchowitz, Jordan, and Recht, 2019], they consequently find a global minimizer.\n\nLiterature suggests that the greater $p$ is, the more likely it is that the landscape is benign. More precisely, when $p \\geq \\sqrt{2 n}+O(1)$, the landscape of the factorized problem is benign for almost all cost matrices $C$ [Boumal, Voroninski, and Bandeira, 2020]. This property is even true for all cost matrices if $p>\\frac{n}{2}$ [Boumal, Voroninski, and Bandeira, 2020, Cor. 5.11], while it can fail for a zero Lebesgue measure subset of cost matrices if $\\sqrt{2 n}+O(1) \\leq p \\leq \\frac{n}{2}$ [O\u2019Carroll, Srinivas, and Vijayaraghavan, 2022]. However, when $p \\leq \\sqrt{2 n}+O(1)$, there is a subset of cost matrices $C$ of\n\npositive Lebesgue measure for which (Burer-Monteiro) admits non-optimal critical points [Waldspurger and Waters, 2020] (with a gap to the optimal value scaling in $O(1 / p)$ [Mei, Misiakiewicz, Montanari, and Oliveira, 2017], but strictly positive).\n\nNonetheless, in practice, standard algorithms seem to find a solution of (Burer-Monteiro) below the threshold $\\sqrt{2 n}$, suggesting that, maybe, the set of cost matrices with a non-optimal critical point is small, and \"typical\" cost matrices do not belong to it. Therefore, researchers have tried to find properties on $C$ guaranteeing that $C$ is not in this bad set, focusing for the moment on the setting where the minimizer of (SDP) has rank 1. The articles [McRae and Boumal, 2024] and [McRae, Abdalla, Bandeira, and Boumal, 2024] discuss matrices $C$ with a specific structure, motivated by synchronization problems. They prove that the landscape of (Burer-Monteiro) is benign under conditions which involve eigenvalues of the subcomponents of $C$. [Ling, 2025] considers general matrices $C$ and shows that the landscape is benign if the condition number of the associated Laplacian matrix is smaller than $\\frac{p-1}{2}$. For important instances of (SDP) (mainly $\\mathbb{Z}_{2}$-synchronization with additive Gaussian noise and multiplicative Bernoulli noise), these recent results show that standard algorithms, applied to (Burer-Monteiro), retrieve the rank 1 solution under close to optimal conditions.\n\nMain result Our main result is a sufficient condition on the condition number of the Laplacian matrix of (SDP) which ensures that the landscape of (Burer-Monteiro) is benign. This tightens the result of [Ling, 2025]: we show that if the condition number is less than $p$ (instead of $\\frac{p-1}{2}$ in [Ling, 2025]), then the landscape of (Burer-Monteiro) is benign. Importantly, we show that this bound is optimal. Finally, by applying our theorem to $\\mathbb{Z}_{2^{-}}$ synchronization and the Kuramoto oscillator system, we also improve on the applications of [McRae, Abdalla, Bandeira, and Boumal, 2024] and [Ling, 2025].\n\nJust as we were finishing the journal version of this article, the preprint [McRae, 2025] was uploaded on arXiv. It extends our main theorem by accounting for a diagonal preconditioner. This allows to derive improved versions of our applications, matching the correctness bounds for convex semidefinite relaxations (without Burer-Monteiro factorization).", "tables": {}, "images": {}}, {"section_id": 3, "text": "# 1.3 Structure of the paper \n\nIn section 2, we present our main result and, in section 3, its application to $\\mathbb{Z}_{2}$-synchronization with additive Gaussian, then Bernoulli noise, and finally to the synchronization of the Kuramoto model. In section 4 we provide the proof of the main theorem, which consists in a novel reformulation of our main result as finding an appropriate dual certificate for a specific convex minimization problem. We also make a comparison between our strategy of proof and that of [Ling, 2025] by reinterpreting the author's proof as finding a dual certificate. Most technical details will be left in the appendix A.", "tables": {}, "images": {}}, {"section_id": 4, "text": "### 1.4 Notation\n\nThroughout this paper, $\\mathbb{S}^{n \\times n}$ is the set of symmetric $n \\times n$ matrices. We write $X \\succeq 0$ if $X$ is a positive semidefinite matrix. For a matrix $X \\in \\mathbb{R}^{n \\times n}$, when it makes sense, $\\lambda_{1}(X) \\leq \\lambda_{2}(X) \\leq \\cdots \\leq \\lambda_{n}(X)$ are the eigenvalues of $X$ in ascending order. For matrices $X, Y \\in \\mathbb{R}^{n \\times m},\\langle X, Y\\rangle=\\operatorname{Tr}\\left(X^{T} Y\\right)$ is the standard inner product on $\\mathbb{R}^{n \\times m}, X \\odot Y$ is the entry-wise or Hadamard product, $\\|X\\|_{F}=\\sqrt{\\langle X, X\\rangle}$ is the Frobenius norm on $\\mathbb{R}^{n \\times m}, X_{i:} \\in \\mathbb{R}^{m}$ is the $i$-th row of $X$ and $X_{: j} \\in \\mathbb{R}^{n}$ is the $j$-th column of $X$. For $X \\in \\mathbb{R}^{n \\times m},\\|X\\|$ is the spectral or $\\ell_{2}$ operator norm of $X$ and $\\|X\\|_{\\infty}$ is the $\\ell_{\\infty}$-norm of $X$ i.e. the maximum entry in absolute value. The operator $\\operatorname{ddiag}: \\mathbb{R}^{n \\times n} \\rightarrow \\mathbb{S}^{n \\times n}$ zeroes out all the non diagonal entries of a matrix and for any vector $x \\in \\mathbb{R}^{n}$, $\\operatorname{diag}(x) \\in \\mathbb{S}^{n \\times n}$ is the diagonal matrix with the coordinates of $x$ on the diagonal. For any $x, y \\in \\mathbb{R}$ the notation $x \\lesssim y$ means that there exists a constant $C>0$ that does not depend on any parameter, such that $x \\leq C y$. For any vector $x \\in \\mathbb{R}^{n},\\|x\\|$ is the Euclidean norm of $x, \\mathbf{1}_{n}=(1 \\ldots 1)^{T} \\in \\mathbb{R}^{n}$.", "tables": {}, "images": {}}, {"section_id": 5, "text": "## 2 Main result\n\nProblem (Burer-Monteiro) can be seen as minimizing a function over the product of spheres\n\n$$\n\\begin{aligned}\n\\left\\{V \\in \\mathbb{R}^{n \\times p}, \\operatorname{diag}\\left(V V^{T}\\right)=\\mathbf{1}_{n}\\right\\} & =\\left\\{V \\in \\mathbb{R}^{n \\times p},\\left\\|V_{1:}\\right\\|=\\cdots=\\left\\|V_{n:}\\right\\|=1\\right\\} \\\\\n& =\\left(\\mathbb{S}^{p-1}\\right)^{n}\n\\end{aligned}\n$$\n\nThe set $\\left(\\mathbb{S}^{p-1}\\right)^{n}$ can be endowed with the Riemannian structure inherited from that of $\\mathbb{R}^{n \\times p}$. It is then a Riemannian manifold.\n\nDefinition 2.1. Let $\\mathcal{M}$ be a Riemannian manifold and $f: \\mathcal{M} \\rightarrow \\mathbb{R}$ a twicedifferentiable function. For any $x \\in \\mathcal{M}$, we say that\n\n- $x$ is a first-order critical point if $\\nabla f(x)=0$, where $\\nabla f(x)$ is the Riemannian gradient of $f$ at $x$ (which belongs to the tangent space $T_{x} \\mathcal{M}$ );\n- $x$ is a second-order critical point (SOCP) if $\\nabla f(x)=0$ and Hess $f(x) \\succeq$ 0 , where Hess $f(x)$ is the Riemannian Hessian of $f$ at $x$ (which is a bilinear map on $T_{x} \\mathcal{M}$ ).\n\nMore detailed explanations of these concepts can be found in [Absil, 2008] or [Boumal, 2023].\n\nLet us recall that we will study (SDP) under the assumption that it has a rank one solution $X=\\boldsymbol{x} \\boldsymbol{x}^{T}$. To set up the statement of the theorem, let us consider $\\boldsymbol{x} \\in\\{ \\pm 1\\}^{n}$, and define the Laplacian matrix as\n\n$$\n\\boldsymbol{L}=\\operatorname{ddiag}\\left(C \\boldsymbol{x} \\boldsymbol{x}^{T}\\right)-C\n$$\n\nNote that, by construction, $\\boldsymbol{L} \\boldsymbol{x}=0$. Standard duality theory shows that $\\boldsymbol{x} \\boldsymbol{x}^{T}$ is a (rank 1) solution to (SDP) if $\\boldsymbol{L} \\succeq 0$; this solution is unique if, in addition, $\\lambda_{2}(\\boldsymbol{L})>0$.\n\nOur theorem gives a sufficient condition on the condition number $\\frac{\\lambda_{n}(\\boldsymbol{L})}{\\lambda_{2}(\\boldsymbol{L})}$ of the Laplacian matrix under which all SOCP of (Burer-Monteiro) are optimal.\n\nTheorem 2.2. Fix a cost matrix $C \\in \\mathbb{S}^{n \\times n}$ and a binary vector $\\boldsymbol{x} \\in\\{ \\pm 1\\}^{n}$. Assume that $\\boldsymbol{L} \\succeq 0$ and $\\lambda_{2}(\\boldsymbol{L})>0$. If\n\n$$\np>\\frac{\\lambda_{n}(\\boldsymbol{L})}{\\lambda_{2}(\\boldsymbol{L})}\n$$\n\nthen any second-order critical point $V$ of (Burer-Monteiro) is a global minimizer, i.e. $V V^{T}=\\boldsymbol{x} \\boldsymbol{x}^{T}$.\n\nIn particular, if the condition number of the Laplacian matrix is upper bounded by $p$, then standard optimization algorithms converge to a global minimum of the factorized problem. This result is purely deterministic and holds for a variety of cost matrices $C$ without assumption on their structure. It improves on [Ling, 2025, Theorem 2.1], which reads as follows.\n\nTheorem ([Ling, 2025]). Under the same assumptions as in theorem 2.2, assume that\n\n$$\np \\geq \\frac{2 \\lambda_{n}(\\boldsymbol{L})}{\\lambda_{2}(\\boldsymbol{L})}+1\n$$\n\nThen all second-order critical points of (Burer-Monteiro) are optimal.\nOur results are similar in nature but the proofs are quite different. Moreover, our bound is optimal in the sense of the following property, the proof of which can be found in the appendix A.1.\n\nProposition 2.3. Let $p \\geq 2$ and $n \\geq 6 p$. If $n$ or $p$ is even, there exist $C \\in \\mathbb{S}^{n \\times n}, \\boldsymbol{x} \\in\\{ \\pm 1\\}^{n}$ satisfying the assumptions of theorem 2.2 such that $\\frac{\\lambda_{n}(\\boldsymbol{L})}{\\lambda_{2}(\\boldsymbol{L})}=p$ and problem (Burer-Monteiro) admits a non optimal second-order critical point.", "tables": {}, "images": {}}, {"section_id": 6, "text": "# 3 Applications\n## $3.1 \\mathbb{Z}_{2}$-synchronization with additive Gaussian noise\n\nHere, we consider the $\\mathbb{Z}_{2}$-synchronization problem with additive Gaussian noise which consists in reconstructing a binary vector $\\boldsymbol{x}$ with coordinates $x_{1}, \\ldots, x_{n} \\in\\{ \\pm 1\\}$ from noisy measurements $x_{i} x_{j}+\\sigma W_{i j}$ where $W_{i j}=W_{j i} \\sim$ $\\mathcal{N}(0,1), W_{i i}=0$ and $\\sigma>0$. This problem admits a relaxation of the form (SDP) with cost matrix\n\n$$\nC=\\boldsymbol{x} \\boldsymbol{x}^{\\boldsymbol{T}}+\\sigma W\n$$\n\n[Bandeira, 2018] shows that this SDP relaxation retrieves the rank 1 matrix $\\boldsymbol{x} \\boldsymbol{x}^{\\boldsymbol{T}}$ when $\\sigma<\\sqrt{\\frac{n}{(2+\\varepsilon) \\log n}}$ (for any $\\varepsilon>0$ ), and explains that, for larger values of $\\sigma$, no algorithm is expected to succeed. Using our theorem 2.2, we can show that the more tractable Burer-Monteiro factorization reaches the same threshold up to a multiplicative factor which goes to 1 when $p$ becomes large.\n\nCorollary 3.1. We consider the $\\mathbb{Z}_{2}$-synchronization problem with Gaussian noise, where the cost matrix is defined by (2). For any $\\varepsilon>0$ and large enough $n$, if\n\n$$\n\\sigma<\\frac{p-1}{p+1} \\sqrt{\\frac{n}{(2+\\varepsilon) \\log n}}\n$$\n\nthen all second-order critical points of (Burer-Monteiro) are optimal with probability at least $1-n^{-\\varepsilon / 4}-4 e^{-n}$.\n\nThis corollary is proved in A.2. It improves on [Ling, 2025, corollary 2.4], which reads as follows.\n\nCorollary ([Ling, 2025]). Under the same conditions as corollary 3.1, if\n\n$$\n\\sigma<\\frac{p-3}{4(p+1)} \\sqrt{\\frac{n}{\\log n}}\n$$\n\nthen all SOCP of the factorized problem (Burer-Monteiro) are optimal with high probability.\n\nIndeed, our result holds for $p$ as small as 2 whereas theirs needs $p \\geq 4$. In the large $p$ limit, our bound is better by a constant multiplicative factor. We also improve on [McRae, Abdalla, Bandeira, and Boumal, 2024, Corollary 1].\n\nCorollary ([McRae, Abdalla, Bandeira, and Boumal, 2024]). For $n \\geq 2$, $\\varepsilon>0$, if the noise level of cost matrix (2) satisfies\n\n$$\n\\sigma \\leq \\frac{p-3}{p-1} \\sqrt{\\frac{n}{(2+\\varepsilon) \\log n}}\n$$\n\nthen all second-order critical points of (Burer-Monteiro) are optimal with probability $\\rightarrow 1$ as $n \\rightarrow \\infty$.\n\nThe improvement lies in the fact that our result does no prohibit us from taking $p$ as small as 2 . The proof is built upon tools used both in [Ling, 2025] and [McRae, Abdalla, Bandeira, and Boumal, 2024].", "tables": {}, "images": {}}, {"section_id": 7, "text": "# 3.2 $\\mathbb{Z}_{2}$-synchronization with Bernoulli noise \n\nThe problem of $\\mathbb{Z}_{2}$-synchronization with Bernoulli noise consists in recovering a binary vector $\\boldsymbol{x} \\in\\{ \\pm 1\\}^{n}$ from its pairwise observations $x_{i} x_{j}$, where the sign of $x_{i} x_{j}$ is flipped with probability $\\frac{1-\\delta}{2}$, for some $0<\\delta \\leq 1$. In other words, when $\\delta$ is close to 1 , the signs are not flipped and when $\\delta$ is close to 0 , the observations are often corrupted. This leads to a problem of the form (SDP), with\n\n$$\nC_{i j}= \\begin{cases}x_{i} x_{j} & \\text { with probability } \\frac{1+\\delta}{2} \\text { if } i \\neq j \\\\ -x_{i} x_{j} & \\text { with probability } \\frac{1-\\delta}{2} \\text { if } i \\neq j \\\\ 0 & \\text { if } i=j\\end{cases}\n$$\n\nOur result gives a condition on $\\delta$ under which the landscape of the factorized problem (Burer-Monteiro) is benign.\n\nCorollary 3.2. We consider the $\\mathbb{Z}_{2}$-synchronization problem with Bernoulli noise of parameter $0<\\delta \\leq 1$, where the cost matrix is defined by (4). For any $\\varepsilon>0$ and large enough $n$, if\n\n$$\n\\delta>\\frac{p+1}{p-1} \\sqrt{\\frac{(2+\\varepsilon) \\log n}{n}}\n$$\n\nthen all second-order critical points of (Burer-Monteiro) are optimal with probability at least $1-n^{-3}-n^{-\\frac{\\varepsilon}{3}}$.\n\nThe proof of this corollary is in A.2. This corollary is an improvement on [McRae, Abdalla, Bandeira, and Boumal, 2024, Theorem 2] in the case where the observations $x_{i} x_{j}$ are complete. This theorem reads as follows.\n\nTheorem ([McRae, Abdalla, Bandeira, and Boumal, 2024]). Consider the $\\mathbb{Z}_{2}$-synchronization problem with Bernoulli noise for some $0<\\delta \\leq 1$. Assume that $p \\geq 4$ and there exists some $\\varepsilon>0$ such that\n\n$$\n\\delta>\\frac{p-1}{p-3} \\sqrt{\\frac{(2+\\varepsilon) \\log n}{n}}\n$$\n\nThen, with probability $\\rightarrow 1$ as $n \\rightarrow \\infty$, all second-order points of the factorized problem (Burer-Monteiro) with cost matrix as in (4) are optimal.\n\nFirst of all, our result does not prevent us from taking $p$ as small as 2. Furthermore, our bound on $\\delta$ is better in the regime when $p$ stays constant, but $n$ is large. Our proof of this theorem builds on ideas found in [Ling, 2025] and [McRae, Abdalla, Bandeira, and Boumal, 2024].", "tables": {}, "images": {}}, {"section_id": 8, "text": "# 3.3 The Kuramoto model \n\nThe homogeneous Kuramoto model [Kuramoto, 1975] is a system of $n$ oscillators on the sphere $\\mathbb{S}^{1}$ identified with their phase $\\theta_{1}, \\ldots, \\theta_{n}$ evolving according to the following dynamics: for each $i \\leq n$,\n\n$$\n\\left\\{\\begin{array}{l}\n\\dot{\\theta}_{i}(t)=\\sum_{j=1}^{n} C_{i j} \\sin \\left(\\theta_{j}(t)-\\theta_{i}(t)\\right) \\text { for } t \\geq 0 \\\\\n\\dot{\\theta}_{i}(0)=\\theta_{i}^{0} \\in \\mathbb{S}^{1}\n\\end{array}\\right.\n$$\n\nThe matrix $C$ is called a coupling matrix. If $C_{i j}>0$ (resp. $C_{i j}<0$ ), the oscillators $\\theta_{i}$ and $\\theta_{j}$ are attractive (resp. repulsive). The Kuramoto model also exists on higher dimensional spheres $\\mathbb{S}^{p-1}$, in which it describes the behavior of a system of $n$ oscillators $V=\\left(V_{1}, \\ldots, V_{n}\\right)^{T} \\in\\left(\\mathbb{S}^{p-1}\\right)^{n}$, which evolve by maximizing the following energy function\n\n$$\n\\mathcal{E}\\left(V_{1}, \\ldots, V_{n}\\right) \\stackrel{\\text { def }}{=} \\sum_{i, j=1}^{n} C_{i j}\\left\\langle V_{i}, V_{j}\\right\\rangle=\\left\\langle C, V V^{T}\\right\\rangle\n$$\n\nIn other words, the Kuramoto model describes the following (ascending) gradient flow dynamic on the product of spheres $\\left(\\mathbb{S}^{p-1}\\right)^{n}$\n\n$$\n\\left\\{\\begin{array}{l}\n\\dot{V}(t)=\\nabla \\mathcal{E}(V) \\text { for } t \\geq 0 \\\\\nV(0)=V^{0} \\in\\left(\\mathbb{S}^{p-1}\\right)^{n}\n\\end{array}\\right.\n$$\n\nwhere the gradient is taken in the Riemannian sense on $\\left(\\mathbb{S}^{p-1}\\right)^{n}$. Note that model (6), which is the most studied in the literature, corresponds to the case $p=2$ by parameterizing $\\mathbb{S}^{p-1}=\\mathbb{S}^{1}$ as $V_{i}=\\left(\\cos \\theta_{i}, \\sin \\theta_{i}\\right)$.\n\nOne of the most interesting questions arising in the study of the Kuramoto model is whether or not the oscillators synchronize at infinity no matter the initial condition i.e. $\\left\\|V_{i}(t)-V_{j}(t)\\right\\| \\rightarrow 0$ as $t \\rightarrow \\infty$ for any $i, j$ and any generic initial condition $V^{0} \\in\\left(\\mathbb{S}^{p-1}\\right)^{n}$. A very recent work [Jain, Mizgerd, and Sawhney, 2025], which extends results in [Abdalla, Bandeira, Kassabov, Souza, Strogatz, and Townsend, 2022], indicates that if the matrix $C$ is drawn according to an Erd\u0151s-R\u00e9nyi distribution, then the system (6) is globally synchronizing if the underlying graph is connected.\n\nTo tackle the question of synchronization, we adopt the perspective of landscape analysis. We recall that, when the gradient flow of an analytic function converges, its limit is a first-order critical point of that function. Moreover, thanks to the center-stable manifold theorem, for a generic initial condition, the limit is a second-order critical point. Note that there is synchronization if and only if the limit is of the form $V=\\mathbf{1}_{n} \\boldsymbol{z}^{T}$ for some $\\boldsymbol{z} \\in \\mathbb{S}^{p-1}$ (all rows of $V$ are equal to $\\boldsymbol{z}^{T}$ ) or equivalently $V V^{T}=\\mathbf{1}_{n} \\mathbf{1}_{n}^{T}$. Therefore, to show that (7) synchronizes, it is enough to have two properties:\n\n1. any maximizer $V$ of the energy function $\\mathcal{E}$ satisfies $V V^{T}=\\mathbf{1}_{n} \\mathbf{1}_{n}^{T}$,\n2. all second-order critical points of $-\\mathcal{E}$ are optimal.\n\nThis is equivalent to the fact that $\\mathbf{1}_{n} \\mathbf{1}_{n}^{T}$ is an optimal rank 1 solution to (SDP), and any SOCP of (Burer-Monteiro) is globally optimal.\n\nIn the following, will focus on the Kuramoto model with possible repulsion between oscillators as studied in [Ling, 2025] with the following coupling matrix\n\n$$\nC_{i j}= \\begin{cases}1 & \\text { with probability } 1-\\alpha \\text { if } i \\neq j \\\\ -1 & \\text { with probability } \\alpha \\text { if } i \\neq j \\\\ 0 & \\text { if } i=j\\end{cases}\n$$\n\nwhere $\\alpha \\in[0,1 / 2)$ is the intensity of repulsion : two oscillators are attractive with probability $1-\\alpha$ and repulsive with probability $\\alpha$. Note that it is possible to adapt the model and our proof to account for the absence of interaction between particles $i$ and $j$, by imposing $C_{i j}=0$ with some probability as is often the case when one studies the Kuramoto model. Our result on the benign landscape of the Kuramoto model on $\\mathbb{S}^{1}$ is the following.\n\nCorollary 3.3. Consider the Kuramoto model on $\\mathbb{S}^{1}$ (Equation (6)) with initial condition $\\theta_{1}^{0}, \\ldots, \\theta_{n}^{0}$ distributed uniformly at random on the sphere and a coupling matrix $C$ as in (8) for some $\\alpha \\in[0,1 / 2)$. For any $\\varepsilon>0$, and $n$ large enough, if\n\n$$\n\\alpha<\\frac{1}{2}-\\frac{3}{2} \\sqrt{\\frac{(2+\\varepsilon) \\log n}{n}}\n$$\n\nthen the oscillators synchronize with probability at least $1-n^{-3}-n^{-\\frac{r}{3}}$.\nThis is a particular case of synchronization of the Kuramoto model on the sphere $\\mathbb{S}^{p-1}$, the proof of which can be found in A.2. It improves on [Ling, 2025, corollary 2.3], which excludes synchronization on $\\mathbb{S}^{1}$.", "tables": {}, "images": {}}, {"section_id": 9, "text": "# 4 Proof of the main theorem \n\nThroughout the proof, we fix a symmetric cost matrix $C \\in \\mathbb{S}^{n \\times n}, \\boldsymbol{x} \\in\\{ \\pm 1\\}^{n}$ such that the associated Laplacian matrix $\\boldsymbol{L}=\\operatorname{ddiag}\\left(C \\boldsymbol{x} \\boldsymbol{x}^{T}\\right)-C$ is positive semidefinite and $\\lambda_{2}(\\boldsymbol{L})>0$.\n\nMoreover, we assume without loss of generality that $\\boldsymbol{x}=\\mathbf{1}_{n}$ so that the rank one solution of (SDP) is $X_{*}=\\mathbf{1}_{n} \\mathbf{1}_{n}^{T}$ and a solution of the factorized problem is $V_{*}=\\frac{1}{\\sqrt{p}} \\mathbf{1}_{n} \\mathbf{1}_{p}^{T}$. Indeed, if the solution of (SDP) is $\\boldsymbol{x} \\boldsymbol{x}^{T}$, then the solutions of the factorized problem are all vectors of the form $V_{*}=\\boldsymbol{x} \\boldsymbol{z}^{T}$ for $\\boldsymbol{z} \\in \\mathbb{S}^{p-1}$ a unit vector. The change of variable $V \\mapsto \\operatorname{diag}(\\boldsymbol{x}) V$ does\n\nnot affect the landscape of the factorized problem and changes solutions into $V_{*}=\\mathbf{1}_{n} \\boldsymbol{z}^{T}$. We refer to [McRae and Boumal, 2024] for more information on this change of variable.\n\nFurthermore, note that due to the diagonal constraint, changing the diagonal of the cost matrix does not change the landscape of the problems. As such, we can replace the cost matrix $C$ with\n\n$$\nC-\\operatorname{diag}\\left(C \\mathbf{1}_{n}\\right)=-\\boldsymbol{L}\n$$\n\nand study the problem\n\n$$\n\\begin{aligned}\n\\min _{X \\in \\mathbb{S}^{n \\times n}} & \\langle\\boldsymbol{L}, X\\rangle \\\\\n\\text { s.t. } & X \\succeq 0 \\\\\n& \\operatorname{diag}(X)=\\mathbf{1}_{n}\n\\end{aligned}\n$$\n\nand its factorized form\n\n$$\n\\begin{aligned}\n\\min _{V \\in \\mathbb{R}^{n \\times p}} & \\left\\langle\\boldsymbol{L}, V V^{T}\\right\\rangle \\\\\n\\text { s.t. } & \\operatorname{diag}\\left(V V^{T}\\right)=\\mathbf{1}_{n}\n\\end{aligned}\n$$\n\nSimilar changes were made in the proofs of [McRae and Boumal, 2024]. Note that we have by assumption $\\boldsymbol{L} \\succeq 0, \\boldsymbol{L} \\mathbf{1}_{n}=0$ and $\\lambda_{2}(\\boldsymbol{L})>0$.", "tables": {}, "images": {}}, {"section_id": 10, "text": "# 4.1 Formulas for the gradient and Hessian \n\nBefore proving theorem 2.2, we provide explicit formulas for the Riemannian gradient and Hessian of the cost function of (10). First, recall that the tangent space of the manifold $\\left(\\mathbb{S}^{p-1}\\right)^{n}$ at $V$ is\n\n$$\nT_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}=\\left\\{\\dot{V} \\in \\mathbb{R}^{n \\times p}: \\operatorname{diag}\\left(\\dot{V} V^{T}\\right)=0\\right\\}\n$$\n\nLet $P_{T_{V}}: \\mathbb{R}^{n \\times p} \\rightarrow T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}$ be the orthogonal projection onto the tangent space, i.e. $P_{V}(X)=X-\\operatorname{ddiag}\\left(X V^{T}\\right) V$ for $X \\in \\mathbb{R}^{n \\times p}$.\n\nThe gradient of the objective function in (Burer-Monteiro) at a point $V \\in\\left(\\mathbb{S}^{p-1}\\right)^{n}$ is\n\n$$\n2\\left(\\boldsymbol{L}-\\operatorname{ddiag}\\left(\\boldsymbol{L} V V^{T}\\right)\\right) V\n$$\n\nIn particular, $V$ is first-order critical if and only if $\\left(\\boldsymbol{L}-\\operatorname{ddiag}\\left(\\boldsymbol{L} V V^{T}\\right)\\right) V=$ 0 .\n\nThe Hessian at $V$ is\n\n$$\n\\operatorname{Hess}_{V}: \\dot{V} \\in T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n} \\mapsto 2 P_{T_{V}}\\left(\\left(\\boldsymbol{L}-\\operatorname{ddiag}\\left(\\boldsymbol{L} V V^{T}\\right)\\right) \\dot{V}\\right)\n$$\n\nIf $V$ is first-order critical, it is second-order critical if and only if $\\mathrm{Hess}_{V}$ is positive semidefinite. As $P_{T_{V}}$ is self-adjoint, that is equivalent to the fact that for all $\\dot{V} \\in T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}$,\n\n$$\n\\left\\langle\\operatorname{Hess}_{V}(\\dot{V}), \\dot{V}\\right\\rangle=2\\left\\langle\\left(\\boldsymbol{L}-\\operatorname{ddiag}\\left(\\boldsymbol{L} V V^{T}\\right)\\right) \\dot{V}, \\dot{V}\\right\\rangle \\geq 0\n$$", "tables": {}, "images": {}}, {"section_id": 11, "text": "# 4.2 Variational formulation \n\nWe prove the contrapositive of the main theorem : if $V$ is a non optimal SOCP then the condition number of the Laplacian matrix is at least $p$. Let us fix $V \\in\\left(\\mathbb{S}^{p-1}\\right)^{n}$ which is second-order critical, but not optimal for (10), i.e. $V V^{T} \\neq \\mathbf{1}_{n} \\mathbf{1}_{n}^{T}$.\n\nIn this subsection, we rephrase the problem of showing that the condition number of the Laplacian matrix is at least $p$ as looking for a feasible point of a convex problem with large enough objective value. As will be explained in subsection 4.3, our proof admits a more concise formulation which does not necessitate to explicitly introduce the convex problem. However, the rephrasing is the main source of intuition for the proof, so we think it is worth presenting it.\n\nWe denote $v_{1}, \\ldots, v_{p}$ the columns of $V$. If we multiply $V$ by a suitable orthogonal $p \\times p$ matrix (which does not change the fact that $V$ is secondorder critical for (10)), we can assume that $\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle \\geq 0$ and $\\left\\langle v_{k}, \\mathbf{1}_{n}\\right\\rangle=0$ for all $k \\geq 2 .{ }^{1}$ This is summarized by the following assumption.\n\nAssumption 4.1. For $i=2, \\ldots, p,\\left(V^{T} \\mathbf{1}_{n}\\right)_{i}=\\left\\langle v_{i}, \\mathbf{1}_{n}\\right\\rangle=0$. Moreover, $\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle \\geq 0$. In particular,\n\n$$\n\\left\\|V^{T} \\mathbf{1}_{n}\\right\\|=\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle \\leq\\left\\|v_{1}\\right\\|\\left\\|\\mathbf{1}_{n}\\right\\| \\leq n\n$$\n\nShowing that the condition number satisfies $\\frac{\\lambda_{0}\\langle\\boldsymbol{L}\\rangle}{\\lambda_{0}\\langle\\boldsymbol{L}\\rangle} \\geq p$ can be recast as showing that the value of the following optimization problem is at least $p$ :\n\n[^0]\n[^0]:    ${ }^{1}$ If $V$ does not satisfy this condition, consider instead $V G$ where $G$ is an orthogonal matrix such that all but its first columns are orthogonal to $V^{T} \\mathbf{1}_{n}$ and $\\left\\langle G_{, 1}, V^{T} \\mathbf{1}\\right\\rangle>0$.\n\n$$\n\\begin{aligned}\n\\inf _{(L, \\mu) \\in \\mathbb{S}^{n \\times n} \\times \\mathbb{R}^{n}} & \\frac{\\lambda_{n}(L)}{\\lambda_{2}(L)} \\\\\n\\text { s.t. } & L \\succeq 0 \\\\\n& L \\mathbf{1}_{n}=0 \\\\\n& \\lambda_{2}(L)>0 \\\\\n& (L-\\operatorname{diag}(\\mu)) V=0 \\\\\n& \\left\\langle(L-\\operatorname{diag}(\\mu)) \\dot{V}, \\dot{V}\\right\\rangle \\geq 0 \\text { for all } \\dot{V} \\in T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}\n\\end{aligned}\n$$\n\nIndeed, the point $(\\boldsymbol{L}, \\hat{\\mu})$, with $\\hat{\\mu}=\\operatorname{diag}\\left(\\boldsymbol{L} V V^{T}\\right)$ is feasible for the above problem since $V$ is a second-order point of (10). Hence, the condition number of $\\boldsymbol{L}$ is greater than or equal to the optimal value of problem (14).\n\nA similar approach which consists in recasting the problem as finding the worse condition number among possible cost matrices is found in [Zhang, 2024] where the author considers semidefinite programs and their BurerMonteiro factorization without affine constraints.\n\nNote that the first three constraints represent the fact that the semidefinite relaxation admits a rank 1 solution and the last two, the first and second-order optimality conditions on $V$. Problem (14) is not convex, but we will see that it has the same optimal value as a convex problem.\n\nFirst, define the set $K$ as the smallest convex cone containing $\\left\\{\\dot{V} \\dot{V}^{T}, \\dot{V} \\in\\right.$ $\\left.T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}\\right\\}$. The last constraint of problem (14) is equivalent to $\\operatorname{diag}(\\mu)-L \\in$ $K^{\\circ}$, where $K^{\\circ}$ is the polar cone of $K$ :\n\n$$\nK^{\\circ}=\\left\\{M \\in \\mathbb{S}^{n \\times n}:\\langle M, N\\rangle \\leq 0 \\text { for all } N \\in K\\right\\}\n$$\n\nNow, consider the following convex minimization problem:\n\n$$\n\\begin{aligned}\n\\inf _{(L, \\mu) \\in \\mathbb{S}^{n \\times n} \\times \\mathbb{R}^{n}} & \\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right) \\\\\n\\text { s.t. } & P_{\\perp} L P_{\\perp} \\succeq P_{\\perp} \\\\\n& \\left(P_{\\perp} L P_{\\perp}-\\operatorname{diag}(\\mu)\\right) V=0 \\\\\n& \\operatorname{diag}(\\mu)-P_{\\perp} L P_{\\perp} \\in K^{\\circ}\n\\end{aligned}\n$$\n\nwhere $P_{\\perp}=I_{n}-n^{-1} \\mathbf{1}_{n} \\mathbf{1}_{n}^{T}$ is the projection matrix on the space orthogonal to $\\mathbf{1}_{n}$. We have the following two lemmas, whose proofs can be found in A.3.\n\nLemma 4.2. Problem (14) and problem (Primal) have the same optimal value.\n\nIn order to find a lower bound on the convex problem (Primal), a natural idea is to introduce the dual problem and find an appropriate dual certificate.\n\nLemma 4.3. The dual problem of (Primal) is\n\n$$\n\\begin{aligned}\n& \\sup _{(W, Z, H) \\in \\mathbb{R}^{n \\times p} \\times \\mathbb{S}^{n \\times n} \\times \\mathbb{S}^{n \\times n}} \\quad\\left\\langle Z, P_{\\perp}\\right\\rangle \\\\\n& \\text { s.t. } Z \\succeq 0 \\\\\n& H \\in K \\\\\n& \\operatorname{diag}\\left(W V^{T}\\right)=\\operatorname{diag}(H) \\\\\n& M=P_{\\perp}\\left(Z+H-\\frac{1}{2}\\left(W V^{T}+V W^{T}\\right)\\right) P_{\\perp} \\\\\n& M \\succeq 0 \\\\\n& \\operatorname{Tr}(M) \\leq 1\n\\end{aligned}\n$$\n\nBy duality, to show that the optimal value of (Primal) is at least $p$, it suffices to exhibit a dual certificate, i.e. a point feasible for (Dual) for which $\\left\\langle Z, P_{\\perp}\\right\\rangle \\geq p$. A similar duality argument is used in [Zhang, 2024], to lower bound the condition number of the objective function.", "tables": {}, "images": {}}, {"section_id": 12, "text": "# 4.3 Remarks on a proof without using the dual\n### 4.3.1 Possible proof without explicit dual formulation\n\nIt is important to note that it is possible to provide a lower bound on the condition number of $\\boldsymbol{L}$ without passing to the dual problem, provided that we have found an appropriate choice of matrices $(W, Z, H)$ that satisfy the constraints of problem (Dual), except for the trace constraint on $M=P_{\\perp}(Z+$ $\\left.\\left.H-\\frac{1}{2}\\left(W V^{T}+V W^{T}\\right)\\right) P_{\\perp} .\\right]$ Indeed, since $M \\succeq 0$, to make the largest eigenvalue of $\\boldsymbol{L}$ appear, we can upper bound $\\langle\\boldsymbol{L}, M\\rangle$ as follows\n\n$$\n\\langle\\boldsymbol{L}, M\\rangle \\leq \\lambda_{n}(\\boldsymbol{L}) \\operatorname{Tr}(M)\n$$\n\nWe are now going to lower bound $\\langle\\boldsymbol{L}, M\\rangle$ to make $\\lambda_{2}(\\boldsymbol{L})$ appear:\n\n$$\n\\langle\\boldsymbol{L}, M\\rangle=\\left\\langle\\boldsymbol{L}, P_{\\perp}\\left(Z+H-\\frac{1}{2}\\left(W V^{T}+V W^{T}\\right)\\right) P_{\\perp}\\right\\rangle\n$$\n\n$$\n\\begin{aligned}\n& =\\left\\langle\\boldsymbol{L}, P_{\\perp} Z\\right\\rangle+\\langle\\boldsymbol{L}, H\\rangle-\\left\\langle\\boldsymbol{L}, W V^{T}\\right\\rangle \\\\\n& =\\left\\langle\\boldsymbol{L}, P_{\\perp} Z\\right\\rangle+\\langle\\boldsymbol{L}, H\\rangle-\\underbrace{\\left\\langle\\boldsymbol{L}-\\operatorname{ddiag}\\left(\\boldsymbol{L} V V^{T}\\right), W V^{T}\\right\\rangle}_{=0 \\text { since (11) holds }} \\\\\n& -\\left\\langle\\operatorname{ddiag}\\left(\\boldsymbol{L} V V^{T}\\right), W V^{T}\\right\\rangle \\\\\n& =\\left\\langle\\boldsymbol{L}, P_{\\perp} Z\\right\\rangle+\\langle\\boldsymbol{L}, H\\rangle-\\underbrace{\\left\\langle\\operatorname{ddiag}\\left(\\boldsymbol{L} V V^{T}\\right), H\\right\\rangle}_{\\text {since } \\operatorname{diag}(H)=\\operatorname{diag}\\left(W V^{T}\\right)} \\\\\n& =\\left\\langle\\boldsymbol{L}, P_{\\perp} Z\\right\\rangle+\\underbrace{\\left\\langle\\boldsymbol{L}-\\operatorname{ddiag}\\left(\\boldsymbol{L} V V^{T}\\right), H\\right\\rangle}_{\\geq 0 \\text { since (13) holds }} \\\\\n& \\geq \\lambda_{2}(\\boldsymbol{L}) \\operatorname{Tr}\\left(P_{\\perp} Z\\right) \\text {. }\n\\end{aligned}\n$$\n\nThe second equality comes from the fact that $P_{\\perp} \\boldsymbol{L}=\\boldsymbol{L}$ because $\\boldsymbol{L} \\mathbf{1}_{n}=0$, and the last one from the fact that $\\boldsymbol{L} \\succeq \\lambda_{2}(\\boldsymbol{L}) P_{\\perp}$ as all eigenvectors which are not associated with the eigenvalue 0 are orthogonal to $\\mathbf{1}_{n}$. Combining the two bounds on $\\langle\\boldsymbol{L}, M\\rangle$, we get, if $\\operatorname{Tr}(M)>0$,\n\n$$\n\\lambda_{2}(\\boldsymbol{L}) \\operatorname{Tr}\\left(P_{\\perp} Z\\right) \\leq \\lambda_{n}(\\boldsymbol{L}) \\operatorname{Tr}(M) \\Longrightarrow \\frac{\\operatorname{Tr}\\left(P_{\\perp} Z\\right)}{\\operatorname{Tr}(M)} \\leq \\frac{\\lambda_{n}(\\boldsymbol{L})}{\\lambda_{2}(\\boldsymbol{L})}\n$$\n\nThus, finding an appropriate $(W, Z, H)$ such that $\\operatorname{Tr}\\left(P_{\\perp} Z\\right) \\geq p \\operatorname{Tr}(M)$ proves the theorem. The aforementioned remark is, of course, just a rephrasing of weak duality without explicitly computing the dual, but a similar approach was used in [Ling, 2025] to derive the main theorem without passing to the dual formulation.", "tables": {}, "images": {}}, {"section_id": 13, "text": "# 4.3.2 Rephrasing of the proof of [Ling, 2025, Theorem 2.1] in terms of dual certificate \n\nWe can reinterpret the proof of the [Ling, 2025, Theorem 2.1] for $d=1$ as finding an appropriate triplet $(W, Z, H)$ and following the reasoning described in the previous paragraph. Indeed, in the proof, the author makes the implicit choice\n\n$$\n\\begin{aligned}\nW & =(p-1) V \\\\\nZ & =(p-1) V V^{T} \\\\\nH & =(p-2) \\mathbf{1}_{n} \\mathbf{1}_{n}^{T}+\\left(V V^{T}\\right)^{\\oplus 2}\n\\end{aligned}\n$$\n\nWe have $Z \\succeq 0$ and $H_{i j}=\\left(W V^{T}\\right)_{i i}=(p-1)$ for all $i$. Moreover, the author shows that $H=\\mathbb{E}\\left(\\dot{V} V^{T}\\right)$ with a careful choice of random tangent vectors $\\dot{V}$ (see subsection 4.4 for more detail); in other words, $H$ is an average of elements of $K$.\nTo further parallel our proof to that of Ling, with the above choice of dual certificate, we note that Equation (15) corresponds to the Ling's equation (3.6), Equation (18) to (3.7) and line (16) to the equation between (3.6) and (3.7).\n\nWith these choices of dual certificates, one has\n\n$$\n\\begin{aligned}\nM & =P_{\\perp}\\left(Z+H-\\frac{1}{2}\\left(W V^{T}+V W^{T}\\right)\\right) P_{\\perp} \\\\\n& =P_{\\perp}\\left((p-2) \\mathbf{1}_{n} \\mathbf{1}_{n}^{T}+\\left(V V^{T}\\right)^{\\oplus 2}+(p-1) V V^{T}-(p-1) V V^{T}\\right) P_{\\perp} \\\\\n& =P_{\\perp}\\left(V V^{T}\\right)^{\\oplus 2} P_{\\perp} \\\\\n& \\succeq 0\n\\end{aligned}\n$$\n\nThen, thanks to (19), one gets\n\n$$\n\\frac{\\lambda_{n}(\\boldsymbol{L})}{\\lambda_{2}(\\boldsymbol{L})} \\geq \\frac{\\operatorname{Tr}\\left(P_{\\perp} Z\\right)}{\\operatorname{Tr}(M)}=(p-1) \\frac{\\operatorname{Tr}\\left(P_{\\perp} V V^{T}\\right)}{\\operatorname{Tr}\\left(P_{\\perp}\\left(V V^{T}\\right)^{\\oplus 2} P_{\\perp}\\right)}{ }^{2}\n$$\n\nWe have on the one hand\n\n$$\n\\begin{aligned}\n\\operatorname{Tr}\\left(P_{\\perp} V V^{T}\\right) & =\\operatorname{Tr}\\left(V V^{T}\\right)-n^{-1}\\left\\langle V V^{T}, \\mathbf{1}_{n} \\mathbf{1}_{n}\\right\\rangle \\\\\n& \\geq \\frac{n-n^{-1} \\sum_{i j}\\left\\langle v_{i}, v_{j}\\right\\rangle^{2}}{2} \\text { using [Ling, 2025, Lemma 3.2] }\n\\end{aligned}\n$$\n\nOn the other hand\n\n$$\n\\begin{aligned}\n\\operatorname{Tr}\\left(P_{\\perp}\\left(V V^{T}\\right)^{\\oplus 2} P_{\\perp}\\right) & =\\operatorname{Tr}\\left(\\left(V V^{T}\\right)^{\\oplus 2}\\right)-n^{-1}\\left\\langle\\mathbf{1}_{n} \\mathbf{1}_{n},\\left(V V^{T}\\right)^{\\oplus 2}\\right\\rangle \\\\\n& =n-n^{-1} \\sum_{i j}\\left\\langle v_{i}, v_{j}\\right\\rangle^{2}\n\\end{aligned}\n$$\n\nCombining the three previous inequalities, we get the bound proved by the author\n\n$$\n\\frac{\\lambda_{n}(\\boldsymbol{L})}{\\lambda_{2}(\\boldsymbol{L})} \\geq \\frac{p-1}{2}\n$$\n\nThe main difference between our proof and that of [Ling, 2025], other than the primal-dual formulation, is a more careful definition of $W$; our choices of $Z$ and $H$ are the same up to multiplicative constants.\n\n[^0]\n[^0]:    ${ }^{2}$ Note that $\\operatorname{Tr}\\left(P_{\\perp}\\left(V V^{T}\\right)^{\\oplus 2} P_{\\perp}\\right)>0$. If it were not the case then $V$ would be optimal.", "tables": {}, "images": {}}, {"section_id": 14, "text": "# 4.4 Choice of the dual certificate \n\nIn the following, we aim to find an adequate dual certificate $\\left(W_{*}, Z_{*}, H_{*}\\right)$. A natural choice for $Z_{*}$ (which ensures $\\left\\langle Z_{*}, P_{\\perp}\\right\\rangle=p$ ) is\n\n$$\nZ_{*} \\stackrel{\\text { def }}{=} \\frac{p}{\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle} V V^{T}\n$$\n\nNote that since $V V^{T}$ is not colinear to $\\mathbf{1}_{n} \\mathbf{1}_{n}^{T},\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle>0$ so $Z_{*}$ is well defined.\n\nFor $H_{*}$, we choose\n\n$$\nH_{*} \\stackrel{\\text { def }}{=} \\beta\\left((p-2) \\mathbf{1}_{n} \\mathbf{1}_{n}^{T}+\\left(V V^{T}\\right)^{\\oplus 2}\\right) \\text { for some } \\beta \\geq 0\n$$\n\nwhich belongs to $K$. Indeed, [Ling, 2025] and [McRae and Boumal, 2024] showed that $H_{*}=\\beta \\mathbb{E}\\left(\\dot{V} \\dot{V}^{T}\\right)$ with $\\dot{V}=P_{T_{V}}\\left(\\mathbf{1}_{n} \\Phi^{T}\\right)$ and $\\Phi \\sim \\mathcal{N}\\left(0, I_{p}\\right)$. Since $\\dot{V} \\dot{V}^{T} \\in K$, by convexity and closure of $K$, it holds that $\\mathbb{E}\\left(\\dot{V} \\dot{V}^{T}\\right) \\in K$.\n\nThere is no straightforward choice for $W_{*}$. A natural one would be $W_{*}=$ $\\beta(p-1) V$ as it would satisfy the diagonal constraint:\n\n$$\n\\begin{aligned}\n\\operatorname{diag}\\left(W_{*} V^{T}\\right) & =\\beta(p-1) \\mathbf{1}_{n} \\\\\n& =\\beta\\left((p-2) \\mathbf{1}_{n}+\\operatorname{diag}\\left(\\left(V V^{T}\\right)^{\\oplus 2}\\right)\\right) \\\\\n& =\\operatorname{diag}\\left(H_{*}\\right)\n\\end{aligned}\n$$\n\nHowever, numerical experiments suggest that it does not work. Fortunately, this can be corrected by adding to $\\beta(p-1) V$ a matrix proportional to $W_{*}^{\\prime}=$ $\\mathbf{1}_{n} \\mathbf{1}_{n}^{T} V+\\varepsilon$, for some $\\varepsilon \\in \\mathbb{R}^{n \\times p}$ chosen so that $\\operatorname{diag}\\left(W_{*}^{\\prime} V^{T}\\right)=0$. We choose $\\varepsilon=-\\operatorname{diag}\\left(V V^{T} \\mathbf{1}_{n}\\right) V\\left(\\right.$ so that $\\left.W_{*}^{\\prime}=P_{T_{V}}\\left(\\mathbf{1}_{n} \\mathbf{1}_{n}^{T} V\\right)\\right)$. Under assumption 4.1,\n\n$$\nW_{*}^{\\prime}=\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle\\left(\\mathbf{1}_{n} e_{1}-\\operatorname{diag}\\left(v_{1}\\right) V\\right)\n$$\n\nwhich suggests the choice\n\n$$\nW_{*}=\\beta(p-1) V+\\delta\\left(\\mathbf{1}_{n} e_{1}-\\operatorname{diag}\\left(v_{1}\\right) V\\right) \\text { for some } \\delta \\in \\mathbb{R}\n$$", "tables": {}, "images": {}}, {"section_id": 15, "text": "### 4.5 Constraints\n\nThe goal now is to find $\\beta, \\delta$ such that the dual certificate $\\left(W_{*}, Z_{*}, H_{*}\\right)$ defined in the previous subsection satisfies the constraints of problem (Dual) and $\\left\\langle Z_{*}, P_{\\perp}\\right\\rangle \\geq p$.\n\nThe definition of $Z_{*}$ immediately implies\n\n$$\n\\left\\langle Z_{*}, P_{\\perp}\\right\\rangle=p \\text { and } Z_{*} \\succeq 0\n$$\n\nFurthermore, $H_{*}$ defined in (20) is in $K$ if $\\beta \\geq 0$, and the definition of $W_{*}$ ensures that the equality $\\operatorname{diag}\\left(W_{*} V^{T}\\right)=\\operatorname{diag}\\left(H_{*}\\right)$ holds true. Therefore, we only have to find $\\beta, \\delta$ such that\n\n$$\n\\begin{aligned}\n\\beta & \\geq 0 \\\\\nM_{*} & \\succeq 0 \\\\\n\\operatorname{Tr}\\left(M_{*}\\right) & \\leq 1\n\\end{aligned}\n$$\n\nwhere of course $M_{*}=P_{\\perp}\\left(Z_{*}+H_{*}-\\frac{1}{2}\\left(W_{*} V^{T}+V W_{*}^{T}\\right)\\right) P_{\\perp}$.\nFor the positive semidefiniteness of $M_{*}$, we have the following lemma, proved in A.3.\n\nLemma 4.4. Under assumption 4.1, if $\\beta \\geq 0, M_{*}$ is positive semidefinite if\n\n$$\n\\left(\\frac{p}{2(p-1)\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}\\right)^{2} \\geq\\left(\\beta-\\frac{p}{2(p-1)\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}\\right)^{2}+\\left(\\frac{\\delta}{2 \\sqrt{p-1}}\\right)^{2}\n$$\n\nThe idea of the proof is to write $M_{*}=R\\left(I_{p} \\otimes S\\right) R^{T}$ for an appropriate $R \\in \\mathbb{R}^{n \\times 2 p}$ and\n\n$$\nS=\\left(\\begin{array}{cc}\n\\frac{p}{\\left\\langle P_{\\perp} V V^{T}\\right\\rangle}-(p-1) \\beta & \\frac{\\delta}{2} \\\\\n\\frac{\\delta}{2} & \\beta\n\\end{array}\\right)\n$$\n\nIt is straightforward to see that $M_{*} \\succeq 0$ if $S \\succeq 0$ and the latter is verified under condition (23). Now, the trace of $M_{*}$ is given by\n\n$$\n\\begin{aligned}\n\\operatorname{Tr}\\left(M_{*}\\right) & =\\operatorname{Tr}\\left(P_{\\perp} Z_{*}\\right)+\\operatorname{Tr}\\left(P_{\\perp} H_{*}\\right)-\\operatorname{Tr}\\left(P_{\\perp} W_{*} V^{T}\\right) \\\\\n& =p+\\beta\\left\\langle P_{\\perp},\\left(V V^{T}\\right)^{\\oplus 2}\\right\\rangle-\\operatorname{Tr}\\left(P_{\\perp}\\left(\\beta(p-1) V V^{T}-\\delta \\operatorname{diag}\\left(v_{1}\\right) V V^{T}\\right)\\right) \\\\\n& =\\beta\\left\\langle P_{\\perp},\\left(V V^{T}\\right)^{\\oplus 2}-(p-1) V V^{T}\\right\\rangle+\\delta\\left\\langle P_{\\perp}, \\operatorname{diag}\\left(v_{1}\\right) V V^{T}\\right\\rangle+p\n\\end{aligned}\n$$\n\nWe must therefore find $\\beta \\geq 0$ and $\\delta$ satisfying Equation (23) such that\n\n$$\n\\begin{gathered}\nt_{1} \\beta+\\frac{t_{2} \\delta}{2 \\sqrt{p-1}} \\geq p-1 \\\\\n\\text { where } t_{1}=\\left\\langle P_{\\perp},(p-1) V V^{T}-\\left(V V^{T}\\right)^{\\oplus 2}\\right\\rangle\n\\end{gathered}\n$$\n\n$$\n\\text { and } t_{2}=-2 \\sqrt{p-1}\\left\\langle P_{\\perp}, \\operatorname{diag}\\left(v_{1}\\right) V V^{T}\\right\\rangle\n$$\n\nWe set\n\n$$\n\\begin{aligned}\n\\beta & =\\frac{p}{2(p-1)\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}\\left(1+\\frac{t_{1}}{\\sqrt{t_{1}^{2}+t_{2}^{2}}}\\right) \\\\\n\\delta & =\\frac{p}{\\sqrt{p-1}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle} \\frac{t_{2}}{\\sqrt{t_{1}^{2}+t_{2}^{2}}}\n\\end{aligned}\n$$\n\nWith this definition, Equation (23) is true, as a direct computation shows. It remains to show that Equation (24) is also true, which is equivalent to\n\n$$\n\\sqrt{t_{1}^{2}+t_{2}^{2}} \\geq \\frac{2(p-1)^{2}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}{p}-t_{1}\n$$\n\nand therefore implied by $t_{1}^{2}+t_{2}^{2} \\geq\\left(\\frac{2(p-1)^{2}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}{p}-t_{1}\\right)^{2}$. This last inequality is equivalent, from the definitions of $t_{1}, t_{2}$, to\n\n$$\n\\begin{aligned}\n0 \\leq & t_{2}^{2}-\\frac{4(p-1)^{4}}{p^{2}}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle^{2}+\\frac{4(p-1)^{2}}{p}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle t_{1} \\\\\n= & 4(p-1)\\left\\langle P_{\\perp}, \\operatorname{diag}\\left(v_{1}\\right) V V^{T}\\right\\rangle^{2}-\\frac{4(p-1)^{4}}{p^{2}}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle^{2} \\\\\n& +\\frac{4(p-1)^{3}}{p}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle^{2}-\\frac{4(p-1)^{2}}{p}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle\\left\\langle P_{\\perp},\\left(V V^{T}\\right)^{\\oplus 2}\\right\\rangle \\\\\n= & 4(p-1)\\left(\\left\\langle P_{\\perp}, \\operatorname{diag}\\left(v_{1}\\right) V V^{T}\\right\\rangle^{2}+\\frac{(p-1)^{2}}{p^{2}}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle^{2}\\right. \\\\\n& \\left.-\\frac{p-1}{p}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle\\left\\langle P_{\\perp},\\left(V V^{T}\\right)^{\\oplus 2}\\right\\rangle\\right)\n\\end{aligned}\n$$\n\nwhich simplifies to :\n\n$$\n\\begin{aligned}\n\\left\\langle P_{\\perp}, \\operatorname{diag}\\left(v_{1}\\right) V V^{T}\\right\\rangle^{2} & +\\frac{(p-1)^{2}}{p^{2}}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle^{2} \\\\\n& -\\frac{p-1}{p}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle\\left\\langle P_{\\perp},\\left(V V^{T}\\right)^{\\oplus 2}\\right\\rangle \\geq 0\n\\end{aligned}\n$$\n\nWe observe that\n\n$$\n\\left\\langle P_{\\perp},\\left(V V^{T}\\right)^{\\oplus 2}\\right\\rangle=\\left\\langle I_{n}-\\frac{1}{n} \\mathbf{1}_{n} \\mathbf{1}_{n}^{T},\\left(V V^{T}\\right)^{\\oplus 2}\\right\\rangle\n$$\n\n$$\n\\begin{aligned}\n& =n-\\frac{\\left\\|V V^{T}\\right\\|_{F}^{2}}{n} \\\\\n& =n-\\frac{\\left\\|V^{T} V\\right\\|_{F}^{2}}{n} \\\\\n& =n-\\frac{\\sum_{i, j=1}^{p}\\left\\langle v_{i}, v_{j}\\right\\rangle^{2}}{n} \\\\\n& \\leq n-\\frac{\\sum_{i=1}^{p}\\left\\|v_{i}\\right\\|^{4}}{n} \\\\\n& =n-\\frac{\\left\\|v_{1}\\right\\|^{4}}{n}-\\frac{\\sum_{i=2}^{p}\\left\\|v_{i}\\right\\|^{4}}{n} \\\\\n& \\leq n-\\frac{\\left\\|v_{1}\\right\\|^{4}}{n}-\\frac{\\left(\\sum_{i=2}^{p}\\left\\|v_{i}\\right\\|^{2}\\right)^{2}}{n(p-1)} \\\\\n& =n-\\frac{\\left\\|v_{1}\\right\\|^{4}}{n}-\\frac{\\left(n-\\left\\|v_{1}\\right\\|^{2}\\right)^{2}}{n(p-1)}\n\\end{aligned}\n$$\n\nAt line (26), we used $\\operatorname{diag}\\left(V V^{T}\\right)=\\mathbf{1}_{n}$. At line (27), we used Cauchy-Schwarz and, at line (28), we used that $\\sum_{i=1}^{p}\\left\\|v_{i}\\right\\|^{2}=\\|V\\|_{F}^{2}=\\operatorname{Tr}\\left(\\operatorname{diag}\\left(V V^{T}\\right)\\right)=n$.\n\nIn addition, from assumption 4.1,\n\n$$\n\\begin{aligned}\n\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle & =\\left\\langle I_{n}-\\frac{1}{n} \\mathbf{1}_{n} \\mathbf{1}_{n}^{T}, V V^{T}\\right\\rangle \\\\\n& =n-\\frac{\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle^{2}}{n}\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\left\\langle P_{\\perp}, \\operatorname{diag}\\left(v_{1}\\right) V V^{T}\\right\\rangle & =\\left\\langle\\operatorname{diag}\\left(v_{1}\\right), V V^{T}\\right\\rangle-\\frac{1}{n}\\left\\langle\\mathbf{1}_{n} \\mathbf{1}_{n}^{T}, \\operatorname{diag}\\left(v_{1}\\right) V V^{T}\\right\\rangle \\\\\n& =\\left\\langle v_{1}, \\operatorname{diag}\\left(V V^{T}\\right)\\right\\rangle-\\frac{1}{n} \\underbrace{\\left\\langle v_{1}, V V^{T} \\mathbf{1}_{n}\\right\\rangle}_{=\\sum_{k=1}^{p}\\left\\langle v_{1}, v_{k} v_{k}^{T} \\mathbf{1}_{n}\\right\\rangle} \\\\\n& =\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle\\left(1-\\frac{\\left\\|v_{1}\\right\\|^{2}}{n}\\right) \\text { as } v_{k}^{T} \\mathbf{1}_{n}=0 \\text { for } k \\geq 2\n\\end{aligned}\n$$\n\nWe combine the last three equations. They show that the left-hand side of Equation (25) is lower bounded by\n\n$$\n\\left\\langle P_{\\perp}, \\operatorname{diag}\\left(v_{1}\\right) V V^{T}\\right\\rangle^{2}+\\frac{(p-1)^{2}}{p^{2}}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle^{2}\n$$\n\n$$\n\\begin{aligned}\n& -\\frac{p-1}{p}\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle\\left\\langle P_{\\perp},\\left(V V^{T}\\right)^{\\oplus 2}\\right\\rangle \\\\\n\\geq & \\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle^{2}\\left(1-\\frac{\\left\\|v_{1}\\right\\|^{2}}{n}\\right)^{2}+\\frac{(p-1)^{2}}{p^{2}}\\left(n-\\frac{\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle^{2}}{n}\\right)^{2} \\\\\n& -\\frac{p-1}{p}\\left(n-\\frac{\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle^{2}}{n}\\right)\\left(n-\\frac{\\left\\|v_{1}\\right\\|^{4}}{n}-\\frac{\\left(n-\\left\\|v_{1}\\right\\|^{2}\\right)^{2}}{n(p-1)}\\right) \\\\\n= & \\left\\|v_{1}\\right\\|^{4}-\\frac{2}{p}\\left(\\frac{p-1}{n}\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle^{2}+n\\right)\\left\\|v_{1}\\right\\|^{2} \\\\\n& +\\frac{1}{p^{2}}\\left(\\frac{(p-1)^{2}}{n^{2}}\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle^{4}+2(p-1)\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle^{2}+n^{2}\\right) \\\\\n= & \\left(\\left\\|v_{1}\\right\\|^{2}-\\frac{1}{p}\\left(n+\\frac{p-1}{n}\\left\\langle v_{1}, \\mathbf{1}_{n}\\right\\rangle^{2}\\right)\\right)^{2} \\\\\n\\geq & 0\n\\end{aligned}\n$$\n\nEquation (25) is therefore true, which concludes.\nAcknowledgements The authors would like to thank Antonin Chambolle for helpful discussions and feedback for this work, partially funded by ANR-23-PEIA-0004 (PDE-AI), as well as Andrew McRae. Faniriana Rakoto Endor and Ir\u00e8ne Waldspurger have been supported by the French government under management of Agence Nationale de la Recherche as part of the \"Investissements d'avenir\" program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute).", "tables": {}, "images": {}}, {"section_id": 16, "text": "# A Technical lemmas\n## A. 1 Proofs of section 2\n\nProof of proposition 2.3. Let us set $\\boldsymbol{x}=\\mathbf{1}_{n}$ and let $V \\in\\left(\\mathbb{S}^{p-1}\\right)^{n}$ be such that\n\n$$\n\\begin{aligned}\nV^{T} \\mathbf{1}_{n} & =0 \\\\\nV^{T} V & =\\frac{n}{p} I_{p} \\\\\n\\left\\langle v_{i} \\odot v_{j} \\odot v_{k}, \\mathbf{1}_{n}\\right\\rangle & =0, \\text { for all } 1 \\leq i, j, k \\leq p\n\\end{aligned}\n$$\n\nwhere the $v_{l}$ 's are the columns of $V$. Such matrices $V$ exist at least when $p$ is even or $n$ is; an example is provided at the end of the proof. Now, set\n\n$$\nC=-\\left(P_{V}+p P_{V^{\\perp}}-p P_{\\mathbf{1}}\\right)\n$$\n\nwhere $P_{V}=\\frac{p}{n} V V^{T}$ is the orthogonal projector onto $\\operatorname{Range}(V), P_{V \\perp}=I_{n}-$ $\\frac{p}{n} V V^{T}$ the projector onto $\\operatorname{Range}(V)^{\\perp}$ and $P_{\\mathbf{1}}=\\frac{1}{n} \\mathbf{1}_{n} \\mathbf{1}_{n}^{T}$ the projector onto $\\mathbb{R} \\mathbf{1}_{n}$.\n\nSince $C \\mathbf{1}_{n}=0$, the Laplacian matrix is $\\boldsymbol{L}=-C=P_{V}+p P_{V^{\\perp}}-p P_{\\mathbf{1}}$. Its eigenvalues are 0 (with eigenspace $\\mathbb{R} \\mathbf{1}_{n}$ ), 1 (with eigenspace Range $(V)$ ) and $p$ (with eigenspace $\\left(\\operatorname{Range}(V) \\oplus \\mathbb{R} \\mathbf{1}_{n}\\right)^{\\perp}$ ). Therefore, $\\boldsymbol{L} \\succeq 0, \\lambda_{2}(\\boldsymbol{L})>0$ and its condition number is $p$.\n\nUsing (11) and (12), $V$ is second-order optimal if\n\n$$\n\\begin{aligned}\nS V & =0 \\\\\n\\left\\langle\\operatorname{Hess}_{V}(\\dot{V}), \\dot{V}\\right\\rangle=2\\langle S \\dot{V}, \\dot{V}\\rangle & \\geq 0, \\forall \\dot{V} \\in T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}\n\\end{aligned}\n$$\n\nwhere\n\n$$\nS \\stackrel{\\text { def }}{=} \\boldsymbol{L}-\\operatorname{ddiag}\\left(\\boldsymbol{L} V V^{T}\\right)=\\boldsymbol{L}-I_{n}=(p-1) P_{V^{\\perp}}-p P_{\\mathbf{1}_{n}}\n$$\n\nIt is clear that with this choice of $C, \\boldsymbol{L} V=V$, hence $S V=0$. It remains to show that the Hessian is positive semidefinite at $V$. The difficulty stems from the fact that $S$ has a negative eigenvalue: $S \\mathbf{1}_{n}=-\\mathbf{1}_{n}$. We first exhibit a subspace of $T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}$ included in $\\operatorname{Ker} \\operatorname{Hess}_{V}$. Then, we prove Equation (32) by decomposing $\\dot{V}$ onto the kernel and its orthogonal.\n\nNote that any matrix of the form\n\n$$\n\\operatorname{diag}(V a) V-\\mathbf{1}_{n} a^{T}\n$$\n\nwith $a \\in \\mathbb{R}^{p}$, belongs to $T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}$ and to the kernel of $\\operatorname{Hess}_{V}$. Indeed, for any $a \\in \\mathbb{R}^{p}$,\n\n$$\n\\begin{aligned}\n\\frac{1}{2} \\operatorname{Hess}_{V}\\left(\\operatorname{diag}(V a) V-\\mathbf{1}_{n} a^{T}\\right) & =P_{T_{V}}\\left(S\\left(\\operatorname{diag}(V a) V-\\mathbf{1}_{n} a^{T}\\right)\\right) \\\\\n& =P_{T_{V}}\\left(-\\frac{p(p-1)}{n} V V^{T} \\operatorname{diag}(V a) V\\right) \\\\\n& +P_{T_{V}}(\\underbrace{(p-1) \\operatorname{diag}(V a) V)}_{\\in\\left(T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}\\right)^{\\perp}})\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& -P_{T_{V}}\\left((p-1) \\mathbf{1}_{n} a^{T}\\right) \\\\\n& -P_{T_{V}}\\left(\\frac{p}{n} \\mathbf{1}_{n} \\mathbf{1}_{n}^{T} \\operatorname{diag}(V a) V-p \\mathbf{1}_{n} a^{T}\\right) \\\\\n& =P_{T_{V}}\\left(-\\frac{p(p-1)}{n} V V^{T} \\operatorname{diag}(V a) V\\right) \\\\\n& +P_{T_{V}}\\left(\\mathbf{1}_{n} a^{T}-\\frac{p}{n} \\mathbf{1}_{n} a^{T} \\underbrace{V^{T} V}_{=\\frac{n}{p} I_{n}}\\right) \\\\\n& =-\\left(\\frac{p(p-1)}{n}\\right) P_{T_{V}}\\left(V V^{T} \\operatorname{diag}(V a) V\\right)\n\\end{aligned}\n$$\n\nFor $1 \\leq j, k \\leq p$, we have\n\n$$\n\\left(V^{T} \\operatorname{diag}(V a) V\\right)_{j k}=\\sum_{i=1}^{p} a_{i} \\underbrace{\\left\\langle v_{i} \\odot v_{j} \\odot v_{k}, \\mathbf{1}_{n}\\right\\rangle}_{=0}=0\n$$\n\nhence $V^{T} \\operatorname{diag}(V a) V=0$, and $\\operatorname{Hess}_{V}\\left(\\operatorname{diag}(V a) V-\\mathbf{1}_{n} a^{T}\\right)=0$.\nLet us fix $\\dot{V} \\in T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}$. It can be decomposed as $\\dot{V}=X+Y$, for some $X, Y \\in T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}$ such that $X \\in \\operatorname{ker} \\operatorname{Hess}_{V}$ and $Y \\in\\left(\\operatorname{ker} \\operatorname{Hess}_{V}\\right)^{\\perp}$. Since $Y$ is orthogonal to the kernel of $\\operatorname{Hess}_{V}$, it is orthogonal to any matrix of the form (33). Therefore, for any $a \\in \\mathbb{R}^{p}$,\n\n$$\n\\begin{aligned}\n0 & =\\left\\langle\\operatorname{diag}(V a) V-\\mathbf{1}_{n} a^{T}, Y\\right\\rangle \\\\\n& =\\left\\langle V a, \\operatorname{diag}\\left(Y V^{T}\\right)\\right\\rangle-\\left\\langle\\mathbf{1}_{n} a^{T}, Y\\right\\rangle \\\\\n& =-\\left\\langle\\mathbf{1}_{n} a^{T}, Y\\right\\rangle \\quad\\left(\\text { since } Y \\in T_{V}\\left(\\mathbb{S}^{p-1}\\right)^{n}\\right) \\\\\n& =-\\left\\langle a^{T}, \\mathbf{1}_{n}^{T} Y\\right\\rangle\n\\end{aligned}\n$$\n\nwhich implies that $\\mathbf{1}_{n}^{T} Y=0$. Hence, it holds that\n\n$$\nS Y=\\left(P_{V}+p P_{V^{\\perp}}-I_{n}\\right) Y-\\frac{p}{n} \\mathbf{1}_{n} \\mathbf{1}_{n}^{T} Y=\\left(P_{V}+p P_{V^{\\perp}}-I_{n}\\right) Y\n$$\n\nFinally,\n\n$$\n\\begin{aligned}\n\\left\\langle\\operatorname{Hess}_{V}(\\dot{V}), \\dot{V}\\right\\rangle & =\\overbrace{\\left\\langle\\operatorname{Hess}_{V}(X), X\\right\\rangle}^{=0}+\\overbrace{2\\left\\langle\\operatorname{Hess}_{V}(X), Y\\right\\rangle}^{=0}+\\overbrace{\\left\\langle\\operatorname{Hess}_{V}(Y), Y\\right\\rangle}^{2(S Y, Y)} \\\\\n& =2\\left\\langle\\left(P_{V}+p P_{V^{\\perp}}-I_{n}\\right) Y, Y\\right\\rangle \\\\\n& =2(p-1)\\left\\langle P_{V^{\\perp}} Y, Y\\right\\rangle\n\\end{aligned}\n$$\n\nTo conclude, we show the existence of $V$ satisfying equations (29), (30) and (31). For instance, when $p$ is even, for any $j \\in\\left\\{1, \\ldots \\frac{p}{2}\\right\\}$ and $i \\in$ $\\{1, \\ldots, n\\}$, we can set\n\n$$\nV_{i, 2 j-1}=\\sqrt{\\frac{2}{p}} \\cos \\left(\\frac{2 \\pi m_{j}}{n}(i-1)\\right) \\text { and } V_{i, 2 j}=\\sqrt{\\frac{2}{p}} \\sin \\left(\\frac{2 \\pi m_{j}}{n}(i-1)\\right)\n$$\n\nwhere $m_{j}=3 j-2$. All three equations can be proved using similar computations. Let us for instance establish equality (31) in the case where $i, j, k$ are odd. We have\n\n$$\n\\begin{aligned}\n& \\left\\langle v_{i} \\odot v_{j} \\odot v_{k}, \\mathbf{1}_{n}\\right\\rangle=\\left(\\frac{2}{p}\\right)^{\\frac{3}{2}} \\sum_{l=0}^{n-1} \\cos \\left(\\frac{2 \\pi m_{i}}{n} l\\right) \\cos \\left(\\frac{2 \\pi m_{j}}{n} l\\right) \\cos \\left(\\frac{2 \\pi m_{k}}{n} l\\right) \\\\\n& =\\frac{1}{\\sqrt{2 p^{3}}} \\sum_{l=0}^{n-1} \\cos \\left(\\frac{2 \\pi}{n}\\left(m_{i}+m_{j}+m_{k}\\right) l\\right) \\\\\n& +\\cos \\left(\\frac{2 \\pi}{n}\\left(m_{i}+m_{j}-m_{k}\\right) l\\right) \\\\\n& +\\cos \\left(\\frac{2 \\pi}{n}\\left(m_{i}-m_{j}+m_{k}\\right) l\\right) \\\\\n& +\\cos \\left(\\frac{2 \\pi}{n}\\left(m_{i}-m_{j}-m_{k}\\right) l\\right)\n\\end{aligned}\n$$\n\nThis sum is zero because one can check that, for any $\\varepsilon_{j}, \\varepsilon_{k} \\in\\{ \\pm 1\\}, m_{i}+$ $\\varepsilon_{j} m_{j}+\\varepsilon_{k} m_{k} \\not \\equiv 0[n]$\n\nIf $p$ is odd but $n$ is even, we can make the same construction for the first $p-1$ columns of $V$ and add one last column whose entries alternate between $-\\sqrt{\\frac{1}{p}}$ and $\\sqrt{\\frac{1}{p}}$.", "tables": {}, "images": {}}, {"section_id": 17, "text": "# A. 2 Proofs of section 3 \n\nProof of Corollary 3.1. The Laplacian matrix of $C$ defined in (2) is\n\n$$\n\\boldsymbol{L}=n\\left(I_{n}-n^{-1} \\boldsymbol{x} \\boldsymbol{x}^{T}\\right)+\\sigma\\left(\\operatorname{ddiag}\\left(W \\boldsymbol{x} \\boldsymbol{x}^{T}\\right)-W\\right)\n$$\n\nDefine the following matrix:\n\n$$\n\\boldsymbol{L}^{W}=\\operatorname{ddiag}\\left(W \\boldsymbol{x} \\boldsymbol{x}^{T}\\right)-W\n$$\n\nSince $I_{n}-n^{-1} \\boldsymbol{x} \\boldsymbol{x}^{T}$ is the orthogonal projector on the orthogonal space of $\\boldsymbol{x}$, its eigenvalues are 0 (with multiplicity 1 ) and 1 (with multiplicity $n-1$ ).\n\nTherefore, using Weyl's inequality,\n\n$$\n\\begin{aligned}\n& \\lambda_{n}(\\boldsymbol{L}) \\leq n+\\sigma\\left\\|\\boldsymbol{L}^{W}\\right\\| \\\\\n& \\lambda_{2}(\\boldsymbol{L}) \\geq n-\\sigma\\left\\|\\boldsymbol{L}^{W}\\right\\|\n\\end{aligned}\n$$\n\nWe need to upper bound $\\left\\|\\boldsymbol{L}^{W}\\right\\|$. The triangular inequality gives\n\n$$\n\\left\\|\\boldsymbol{L}^{W}\\right\\| \\leq\\|W \\boldsymbol{x}\\|_{\\infty}+\\|W\\|\n$$\n\nMoreover, for all $\\varepsilon^{\\prime}>0$, it holds that\n\n$$\n\\|W \\boldsymbol{x}\\|_{\\infty} \\leq \\sqrt{\\left(2+\\varepsilon^{\\prime}\\right) n \\log n}\n$$\n\nwith probability at least $1-n^{-\\varepsilon^{\\prime} / 2}$. Indeed, note that, for all $i \\leq n,(W \\boldsymbol{x})_{i} \\sim$ $\\mathcal{N}(0, n-1)$. Therefore, from [Vershynin, 2018, Prop 2.1.2], for all $t>0$,\n\n$$\n\\mathbb{P}\\left(\\left|(W \\boldsymbol{x})_{i}\\right|>t\\right) \\leq \\sqrt{\\frac{2(n-1)}{\\pi}} \\frac{e^{-\\frac{t^{2}}{2(n-1)}}}{t}\n$$\n\nApplying a union bound and taking $t=\\sqrt{\\left(2+\\varepsilon^{\\prime}\\right) n \\log n}$ yields (34). Moreover, it is also true that, with probability at least $1-4 e^{-n}$,\n\n$$\n\\|W\\| \\leq c_{0} \\sqrt{n}\n$$\n\nfor some universal constant $c_{0}>0$. This is an immediate consequence of [Vershynin, 2018, Corollary 4.3.6]. Therefore, for any $\\varepsilon>0$, it holds with probability at least $1-n^{-\\varepsilon / 4}-4 e^{-n}$ that\n\n$$\n\\begin{aligned}\n\\left\\|\\boldsymbol{L}^{W}\\right\\| & \\leq \\sqrt{\\left(2+\\frac{\\varepsilon}{2}\\right) n \\log n}+c_{0} \\sqrt{n} \\\\\n& \\leq \\sqrt{(2+\\varepsilon) n \\log n} \\quad \\text { (for } n \\text { large enough). }\n\\end{aligned}\n$$\n\nThen, with the same probability,\n\n$$\n\\sigma<\\frac{p-1}{p+1} \\sqrt{\\frac{n}{(2+\\varepsilon) \\log n}} \\Longleftrightarrow \\frac{n+\\sigma \\sqrt{(2+\\varepsilon) n \\log n}}{n-\\sigma \\sqrt{(2+\\varepsilon) n \\log n}}<p\n$$\n\n$$\n\\Longrightarrow \\frac{\\lambda_{n}(\\boldsymbol{L})}{\\lambda_{2}(\\boldsymbol{L})}<p\n$$\n\nFurthermore, since $\\lambda_{2}(\\boldsymbol{L}) \\geq n-\\sigma \\sqrt{(2+\\varepsilon) n \\log n}$, it is true that $\\lambda_{2}(\\boldsymbol{L})>0$ and $\\boldsymbol{L} \\succeq 0$ for $n$ large enough, with probability at least $1-n^{-\\varepsilon / 4}-4 e^{-n}$. The conclusion follows from theorem 2.2.\n\nProof of corollary 3.2. Let $\\varepsilon>0$ be fixed. We can assume without loss of generality that the vector we want to reconstruct is $\\boldsymbol{x}=\\mathbf{1}_{n}$ (see section 4 for more details). The Laplacian matrix is\n\n$$\n\\boldsymbol{L}=\\operatorname{diag}\\left(C \\mathbf{1}_{n}\\right)-C\n$$\n\nNote that $\\boldsymbol{L}$ can be decomposed as a principal term and a noise term as follows:\n\n$$\n\\begin{aligned}\n\\boldsymbol{L} & =\\mathbb{E}(\\boldsymbol{L})+(\\boldsymbol{L}-\\mathbb{E}(\\boldsymbol{L})) \\\\\n& =\\underbrace{\\delta\\left(n I_{n}-\\mathbf{1}_{n} \\mathbf{1}_{n}^{T}\\right)}_{\\text {principal term }}+\\underbrace{(\\boldsymbol{L}-\\mathbb{E}(\\boldsymbol{L}))}_{\\text {noise term }}\n\\end{aligned}\n$$\n\nTherefore, using Weyl's inequality yields\n\n$$\n\\begin{aligned}\n& \\lambda_{n}(\\boldsymbol{L}) \\leq \\delta n+\\|\\boldsymbol{L}-\\mathbb{E}(\\boldsymbol{L})\\| \\\\\n& \\lambda_{2}(\\boldsymbol{L}) \\geq \\delta n-\\|\\boldsymbol{L}-\\mathbb{E}(\\boldsymbol{L})\\|\n\\end{aligned}\n$$\n\nIn particular, as soon as $\\|\\boldsymbol{L}-\\mathbb{E}(\\boldsymbol{L})\\|<\\delta n, \\lambda_{2}(\\boldsymbol{L})>0$ and\n\n$$\n\\frac{\\lambda_{n}(\\boldsymbol{L})}{\\lambda_{2}(\\boldsymbol{L})} \\leq \\frac{\\delta n+\\|\\boldsymbol{L}-\\mathbb{E}(\\boldsymbol{L})\\|}{\\delta n-\\|\\boldsymbol{L}-\\mathbb{E}(\\boldsymbol{L})\\|}\n$$\n\nso that, from theorem 2.2, all second-order critical points are global minimizers if the right-hand side of the above is below $p$.\n\nNote that\n\n$$\n\\|\\boldsymbol{L}-\\mathbb{E}(\\boldsymbol{L})\\| \\leq\\left\\|\\operatorname{diag}\\left(C \\mathbf{1}_{n}\\right)-\\delta(n-1) I_{n}\\right\\|+\\left\\|C-\\delta\\left(\\mathbf{1}_{n} \\mathbf{1}_{n}^{T}-I_{n}\\right)\\right\\|\n$$\n\nFor $1 \\leq i \\leq n$, we have the following equality:\n\n$$\n\\left(\\operatorname{diag}\\left(C \\mathbf{1}_{n}\\right)-\\delta(n-1)\\right)_{i i}=\\sum_{j \\neq i}\\left(C_{i j}-\\delta\\right)\n$$\n\nLet $h(u)=(1+u) \\log (1+u)-u=\\frac{u^{2}}{2}\\left(1+o_{u \\rightarrow 0}(1)\\right)$. Using Bennett's inequality, we get for $t \\geq 0$\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(\\left|\\sum_{j \\neq i}\\left(C_{i j}-\\delta\\right)\\right|>t\\right) & \\leq 2 \\exp \\left(-\\frac{(n-1)(1-\\delta)}{1+\\delta} h\\left(\\frac{t}{(n-1)(1-\\delta)}\\right)\\right) \\\\\n& \\leq 2 \\exp \\left(-\\left(\\frac{n-1}{1+\\delta}\\right) h\\left(\\frac{t}{n-1}\\right)\\right)\n\\end{aligned}\n$$\n\nThe second inequality is true because $h$ is convex and $h(0)=0$, so $a h(x / a) \\geq$ $h(x)$ for all $\\left.\\left.x \\geq 0, a \\in\\right] 0 ; 1\\right]$.\n\nWe set $t=\\sqrt{\\left(2+\\varepsilon^{\\prime}\\right)(1+\\delta) n \\log n}$ for some $\\varepsilon^{\\prime}<\\frac{\\varepsilon}{2}$. Observe that $\\frac{t}{n-1} \\rightarrow 0$ when $n \\rightarrow+\\infty$, so $h\\left(\\frac{t}{n-1}\\right)=\\frac{t^{2}}{2 n^{2}}(1+o(1))$ and\n\n$$\n\\mathbb{P}\\left(\\left|\\sum_{j \\neq i}\\left(C_{i j}-\\delta\\right)\\right|>\\sqrt{\\left(2+\\epsilon^{\\prime}\\right)(1+\\delta) n \\log n}\\right) \\leq 2 n^{-\\left(1+\\varepsilon^{\\prime} / 2\\right)\\left(1+o_{n \\rightarrow \\infty}(1)\\right)}\n$$\n\nTherefore, using a union bound, we get\n$\\mathbb{P}\\left(\\left\\|\\operatorname{diag}\\left(C \\mathbf{1}_{n}\\right)-\\delta(n-1) I_{n}\\right\\| \\leq \\sqrt{\\left(2+\\varepsilon^{\\prime}\\right)(1+\\delta) n \\log n}\\right) \\geq 1-2 n^{-\\left(\\varepsilon^{\\prime} / 2+o(1)\\right)}$.\nMoreover, from [McRae, Abdalla, Bandeira, and Boumal, 2024, Lemma 2], with probability at least $1-n^{-3}$,\n\n$$\n\\left\\|C-\\delta\\left(\\mathbf{1}_{n} \\mathbf{1}_{n}^{T}-I_{n}\\right)\\right\\| \\lesssim \\sqrt{n}\n$$\n\nThis bound is negligible in front of $\\sqrt{n \\log n}$, for $n$ large enough, so that\n\n$$\n\\|\\boldsymbol{L}-\\mathbb{E}(\\boldsymbol{L})\\|<\\sqrt{\\left(2+2 \\varepsilon^{\\prime}\\right)(1+\\delta) n \\log n}\n$$\n\nwith probability at least $1-n^{-3}-2 n^{-\\frac{\\varepsilon^{\\prime}}{3}}$. Therefore, we get the desired result if\n\n$$\n\\frac{\\delta n+\\sqrt{\\left(2+2 \\varepsilon^{\\prime}\\right)(1+\\delta) n \\log n}}{\\delta n-\\sqrt{\\left(2+2 \\varepsilon^{\\prime}\\right)(1+\\delta) n \\log n}}<p \\Longleftrightarrow \\delta>\\frac{1+\\sqrt{1+4\\left(\\frac{p-1}{p+1}\\right)^{2} \\frac{n}{\\left(2+2 \\varepsilon^{\\prime}\\right) \\log n}}}{2\\left(\\frac{p-1}{p+1}\\right)^{2} \\frac{n}{\\left(2+2 \\varepsilon^{\\prime}\\right) \\log n}}\n$$\n\nIn the regime when $n$ is large, recalling that $\\varepsilon>2 \\varepsilon^{\\prime}$, this is implied by\n\n$$\n\\delta>\\frac{p+1}{p-1} \\sqrt{\\frac{(2+\\varepsilon) \\log n}{n}}\n$$\n\nProof of corollary 3.3. We will prove the following more general result of which Corollary 3.3 is a consequence.\nCorollary. We consider the Kuramoto model on the sphere $\\mathbb{S}^{p-1}$ with coupling matrix as defined in (8), with $\\alpha \\in[0,1 / 2)$. For any $\\varepsilon>0$ and $n$ large enough, if\n\n$$\n\\alpha<\\frac{1}{2}-\\frac{p+1}{2(p-1)} \\sqrt{\\frac{(2+\\varepsilon) \\log n}{n}}\n$$\n\nthen the Kuramoto system (7) synchronizes with probability at least $1-n^{-3}-$ $n^{-\\frac{1}{3}}$.\n\nNote that the coupling matrix of the Kuramoto model (8) is just the cost matrix of the $\\mathbb{Z}_{2}$-synchronization model with Bernoulli noise (4) with the choice $\\boldsymbol{x}=\\mathbf{1}_{n}$ and $1-\\alpha=\\frac{1+\\delta}{2}$, the rest follows.", "tables": {}, "images": {}}, {"section_id": 18, "text": "# A. 3 Proofs of section 4 \n\nProof of lemma 4.2. Let $(L, \\mu)$ be a solution of (Primal). Since $P_{\\perp} L P_{\\perp} \\succeq P_{\\perp}$ it holds that $\\lambda_{2}\\left(P_{\\perp} L P_{\\perp}\\right) \\geq 1$, therefore $\\frac{\\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right)}{\\lambda_{2}\\left(P_{\\perp} L P_{\\perp}\\right)} \\leq \\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right)$. Since $\\left(P_{\\perp} L P_{\\perp}, \\mu\\right)$ is feasible for (14), the optimal value of (14) is less than that of (Primal).\n\nNow, let $(L, \\mu)$ be feasible for (14) and define\n\n$$\n\\left(L^{\\prime}, \\mu^{\\prime}\\right)=\\left(\\frac{L}{\\lambda_{2}(L)}, \\frac{\\mu}{\\lambda_{2}(L)}\\right)\n$$\n\nwhich is feasible for (Primal). The last two constraints of (Primal) are easily verified. For the first constraint, note that $\\operatorname{Ker}(L)=\\mathbf{1}_{n}$; therefore, for all $x \\in \\mathbb{R}^{n}, P_{\\perp} x$ is the projection of $x$ onto the orthogonal of $\\operatorname{Ker}(L)$, and $\\|L x\\|^{2} \\geq \\lambda_{2}(L)\\left\\|P_{\\perp} x\\right\\|^{2}$. This implies that $P_{\\perp} L^{\\prime} P_{\\perp} \\succeq P_{\\perp}$. Thus we have\n\n$$\n\\text { Opt }(\\text { Primal }) \\leq \\lambda_{n}\\left(L^{\\prime}\\right)=\\frac{\\lambda_{n}(L)}{\\lambda_{2}(L)}\n$$\n\nBy minimizing both sides of the inequality for all $L$ feasible for (14), we get\n\n$$\n\\text { Opt }(\\text { Primal }) \\leq \\text { Opt }(14)\n$$\n\nProof of lemma 4.3. First, we first incorporate the constraints into the cost function and problem (Primal) becomes\n\n$$\n\\begin{aligned}\n\\inf _{(L, \\mu) \\in \\mathbb{S}^{n \\times n} \\times \\mathbb{R}^{n}} \\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right) & +\\sup _{Z \\geq 0}-\\left\\langle P_{\\perp} L P_{\\perp}-P_{\\perp}, Z\\right\\rangle \\\\\n& +\\sup _{W \\in \\mathbb{R}^{n \\times p}}\\left\\langle\\left(P_{\\perp} L P_{\\perp}-\\operatorname{diag}(\\mu)\\right) V, W\\right\\rangle \\\\\n& +\\sup _{H \\in K}\\left\\langle\\operatorname{diag}(\\mu)-P_{\\perp} L P_{\\perp}, H\\right\\rangle\n\\end{aligned}\n$$\n\nTo lighten notations, define the constraint set $\\mathcal{C}$ as :\n\n$$\n\\mathcal{C}=\\left\\{(W, Z, H) \\in \\mathbb{R}^{n \\times p} \\times \\mathbb{S}^{n \\times n} \\times \\mathbb{S}^{n \\times n}: Z \\succeq 0 \\text { and } H \\in K\\right\\}\n$$\n\nWe symmetrize and simplify the previous expression of problem (Primal). We get that it is equal to\n\n$$\n\\begin{aligned}\n& \\inf _{(L, \\mu) \\in \\mathbb{S}^{n \\times n} \\times \\mathbb{R}^{n}} \\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right) \\\\\n& \\qquad+\\sup _{(W, Z, H) \\in \\mathcal{C}}-\\left\\langle P_{\\perp}\\left(Z+H-\\frac{W V^{T}+V W^{T}}{2}\\right) P_{\\perp}, L\\right\\rangle \\\\\n& \\quad+\\left\\langle P_{\\perp}, Z\\right\\rangle+\\left\\langle\\operatorname{diag}(\\mu), H-W V^{T}\\right\\rangle\n\\end{aligned}\n$$\n\nBy inverting the inf and the sup we get\nOpt (Primal)\n\n$$\n\\begin{aligned}\n& \\geq \\sup _{(W, Z, H) \\in \\mathcal{C}}\\left\\langle P_{\\perp}, Z\\right\\rangle \\\\\n& +\\inf _{L \\in \\mathbb{S}^{n \\times n}} \\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right)-\\left\\langle P_{\\perp}\\left(Z+H-\\frac{W V^{T}+V W^{T}}{2}\\right) P_{\\perp}, L\\right\\rangle \\\\\n& +\\inf _{\\mu \\in \\mathbb{R}^{n}}\\left\\langle\\operatorname{diag}(\\mu), H-W V^{T}\\right\\rangle\n\\end{aligned}\n$$\n\nThe next step is to rewrite the last two terms of the right hand side of inequality (35) as characteristic functions of convex sets. Note that\n\n$$\n\\inf _{\\mu \\in \\mathbb{R}^{n}}\\left\\langle\\operatorname{diag}(\\mu), H-W V^{T}\\right\\rangle= \\begin{cases}0 & \\text { if } \\operatorname{diag}(H)=\\operatorname{diag}\\left(W V^{T}\\right) \\\\ -\\infty & \\text { otherwise }\\end{cases}\n$$\n\nMoreover, by letting $M=P_{\\perp}\\left(Z+H-\\frac{W V^{T}+V W^{T}}{2}\\right) P_{\\perp}$, we have\n\n$$\n\\inf _{L \\in \\mathbb{S}^{n \\times n}} \\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right)-\\langle M, L\\rangle= \\begin{cases}0 & \\text { if } M \\succeq 0 \\text { and } \\operatorname{Tr}(M) \\leq 1 \\\\ -\\infty & \\text { otherwise }\\end{cases}\n$$\n\nTo see this, assume first that $M$ is not positive semidefinite. Therefore, we can write the eigendecomposition of $M$ as $M=\\sum_{i=1}^{n} \\rho_{i} u_{i} u_{i}^{T}$ with $\\rho_{1}=0$ and $u_{1}=\\frac{\\mathbf{1}_{n}}{\\sqrt{n}}$ (since $\\mathbf{1}_{n}$ belongs to the kernel of $M$ ) and $\\rho_{2}<0$. Take $L_{x}=x u_{2} u_{2}^{T}$, with $x<0$. By noting that $P_{\\perp} L_{x} P_{\\perp}=L_{x}$, we have\n\n$$\n\\lambda_{n}\\left(P_{\\perp} L_{x} P_{\\perp}\\right)-\\left\\langle M, L_{x}\\right\\rangle=-x \\rho_{2}\\left\\|u_{2}\\right\\|^{2} \\underset{x \\rightarrow-\\infty}{\\longrightarrow}-\\infty\n$$\n\nNow, assume that $\\operatorname{Tr}(M)>1$ and take $L_{y}=y I_{n}$ with $y>0$. We have\n\n$$\n\\lambda_{n}\\left(P_{\\perp} L_{y} P_{\\perp}\\right)-\\left\\langle M, L_{y}\\right\\rangle=y(1-\\operatorname{Tr}(M)) \\underset{y \\rightarrow \\infty}{\\longrightarrow}-\\infty\n$$\n\nFinally, assume that $M \\succeq 0$ and $\\operatorname{Tr}(M) \\leq 1$. It is always true that for any symmetric matrix $L, P_{\\perp} L P_{\\perp} \\preceq \\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right) I_{n}$. Therefore, since $M \\succeq 0$, we have\n\n$$\n\\begin{aligned}\n\\left\\langle P_{\\perp} L P_{\\perp}, M\\right\\rangle & \\leq\\left\\langle\\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right) I_{n}, M\\right\\rangle \\\\\n& =\\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right) \\operatorname{Tr}(M)\n\\end{aligned}\n$$\n\nUsing the fact that $\\operatorname{Tr}(M) \\leq 1$ and $\\left\\langle P_{\\perp} L P_{\\perp}, M\\right\\rangle=\\langle L, M\\rangle$, we get that $\\lambda_{n}\\left(P_{\\perp} L P_{\\perp}\\right)-\\langle M, L\\rangle \\geq 0$ and the bound is reached for $L=0$. To conclude, the right hand side of inequality (35) becomes\n\n$$\n\\begin{aligned}\n& \\sup _{(W, Z, H) \\in \\mathcal{C}} \\quad\\left\\langle Z, P_{\\perp}\\right\\rangle \\\\\n& \\text { s.t. } \\quad \\operatorname{diag}\\left(W V^{T}\\right)=\\operatorname{diag}(H) \\\\\n& M=P_{\\perp}\\left(Z+H-\\frac{1}{2}\\left(W V^{T}+V W^{T}\\right)\\right) P_{\\perp} \\\\\n& M \\succeq 0 \\\\\n& \\operatorname{Tr}(M) \\leq 1\n\\end{aligned}\n$$\n\nProof of lemma 4.4. We assume $\\beta \\geq 0$ and Equation (23) holds. By developping and multiplying by $p-1$, this equation can be rewritten as\n\n$$\n\\beta\\left(\\frac{p}{\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}-(p-1) \\beta\\right)-\\frac{\\delta^{2}}{4} \\geq 0\n$$\n\nWe have\n\n$$\nM_{*}=P_{\\perp}\\left(Z_{*}+H_{*}-\\frac{1}{2}\\left(W_{*} V^{T}+V W_{*}^{T}\\right)\\right) P_{\\perp}\n$$\n\n$$\n\\begin{aligned}\n= & P_{\\perp}\\left(\\left(\\frac{p}{\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}-(p-1) \\beta\\right) V V^{T}+\\beta \\underbrace{\\left(V V^{T}\\right) \\odot\\left(V V^{T}\\right)}_{\\succeq\\left(V V^{T}\\right) \\odot\\left(v_{1} v_{1}^{T}\\right)}\\right. \\\\\n& \\left.+\\frac{\\delta}{2}\\left(\\operatorname{diag}\\left(v_{1}\\right) V V^{T}+V V^{T} \\operatorname{diag}\\left(v_{1}\\right)\\right)\\right) P_{\\perp} \\\\\n\\geq & P_{\\perp}\\left(\\left(\\frac{p}{\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}-(p-1) \\beta\\right) V V^{T}+\\beta \\operatorname{diag}\\left(v_{1}\\right) V V^{T} \\operatorname{diag}\\left(v_{1}\\right)^{T}\\right. \\\\\n& \\left.+\\frac{\\delta}{2}\\left(\\operatorname{diag}\\left(v_{1}\\right) V V^{T}+V V^{T} \\operatorname{diag}\\left(v_{1}\\right)\\right)\\right) P_{\\perp} \\\\\n= & \\left(V \\operatorname{diag}\\left(v_{1}\\right) V\\right)\\left(S \\otimes I_{p}\\right)\\left(V \\operatorname{diag}\\left(v_{1}\\right) V\\right)^{T}\n\\end{aligned}\n$$\n\nwhere the notation \" $\\otimes$ \" stands for the Kronecker product and we have defined\n\n$$\nS=\\left(\\begin{array}{cc}\n\\frac{p}{\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle} & -(p-1) \\beta \\\\\n\\frac{\\delta}{2} & \\frac{\\delta}{2}\n\\end{array}\\right)\n$$\n\nAll principal minors of $S$ are nonnegative. Indeed, $\\beta \\geq 0$ by assumption,\n\n$$\n\\begin{aligned}\n\\operatorname{det}(S) & =\\beta\\left(\\frac{p}{\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}-(p-1) \\beta\\right)-\\frac{\\delta^{2}}{4} \\\\\n& \\geq 0 \\text { from Equation (36) }\n\\end{aligned}\n$$\n\nand $\\frac{p}{\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}-(p-1) \\beta \\geq 0$ (otherwise, we must have $\\beta>\\frac{p}{(p-1)\\left\\langle P_{\\perp}, V V^{T}\\right\\rangle}>0$, so $\\operatorname{det}(S)<-\\frac{\\delta^{2}}{4}$, which contradicts the nonnegativity of the determinant). Therefore, $S \\succeq 0$, so $S \\otimes I_{p} \\succeq 0$ and $M_{*} \\succeq 0$.", "tables": {}, "images": {}}, {"section_id": 19, "text": "# References \n\nE. Abbe, A. S. Bandeira, A. Bracher, and A. Singer. Decoding binary node labels from censored edge measurements: Phase transition and efficient recovery. IEEE Transactions on Network Science and Engineering, 1(1): $10-22,2014$.\nP. Abdalla, A. S. Bandeira, M. Kassabov, V. Souza, S. H. Strogatz, and A. Townsend. Expander graphs are globally synchronizing. arXiv preprint arXiv:2210.12788, 2022.\n\nP.-A. Absil. Optimization algorithms on matrix manifolds. Princeton University Press, 2008.\nA. S. Bandeira. Random Laplacian matrices and convex relaxations. Foundations of Computational Mathematics, 18:345-379, 2018.\nS. J. Benson, Y. Ye, and X. Zhang. Solving large-scale sparse semidefinite programs for combinatorial optimization. SIAM Journal on Optimization, 10(2):443-461, 2000.\nN. Boumal. An introduction to optimization on smooth manifolds. Cambridge University Press, 2023.\nN. Boumal, V. Voroninski, and A. S. Bandeira. Deterministic guarantees for Burer-Monteiro factorizations of smooth semidefinite programs. Communications on Pure and Applied Mathematics, 73(3):581-608, 2020.\nS. Burer and R. D. C. Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical programming, 95(2):329-357, 2003.\nE. J. Cand\u00e8s, T. Strohmer, and V. Voroninski. PhaseLift: exact and stable signal recovery from magnitude measurements via convex programming. Communications on Pure and Applied Mathematics, 66(8):12411274, 2013.\nM. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6):1115-1145, 1995.\nV. Jain, C. Mizgerd, and M. Sawhney. The random graph process is globally synchronizing. arXiv preprint arXiv:2501.12205, 2025.\nM. Journ\u00e9e, F. Bach, P.-A. Absil, and R. Sepulchre. Low-rank optimization on the cone of positive semidefinite matrices. SIAM Journal on Optimization, 20(5):2327-2351, 2010.\nY. Kuramoto. Self-entrainment of a population of coupled non-linear oscillators. In Huzihiro Araki, editor, International Symposium on Mathematical Problems in Theoretical Physics, pages 420-422, Berlin, Heidelberg, 1975. Springer Berlin Heidelberg. ISBN 978-3-540-37509-8.\n\nG. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix with semidefinite programming. Journal of Machine learning research, 5(Jan):27-72, 2004.\nJ. D. Lee, I. Panageas, G. Piliouras, M. Simchowitz, M. I. Jordan, and B. Recht. First-order methods almost always avoid strict saddle points. Mathematical programming, 176:311-337, 2019.\nS. Ling. Local geometry determines global landscape in low-rank factorization for synchronization. To appear in Foundations of Computational Mathematics, 2025.\nS. Ling, R. Xu, and A. S. Bandeira. On the landscape of synchronization networks: A perspective from nonconvex optimization. SIAM Journal on Optimization, 29(3):1879-1907, 2019.\nA. D. McRae. Benign landscapes for synchronization on spheres via normalized Laplacian matrices. arXiv preprint arXiv:2503.18801, 2025.\nA. D. McRae and N. Boumal. Benign landscapes of low-dimensional relaxations for orthogonal synchronization on general graphs. SIAM Journal on Optimization, 34(2):1427-1454, 2024.\nA. D. McRae, P. Abdalla, A. S. Bandeira, and N. Boumal. Nonconvex landscapes for $\\mathbb{Z}_{2}$ synchronization and graph clustering are benign near exact recovery thresholds. arXiv preprint arXiv:2407.13407, 2024.\nS. Mei, T. Misiakiewicz, A. Montanari, and R. I. Oliveira. Solving SDPs for synchronization and MaxCut problems via the Grothendieck inequality. In Annual Conference Computational Learning Theory, 2017. URL https://api.semanticscholar.org/CorpusID:6039962.\nL. O'Carroll, V. Srinivas, and A. Vijayaraghavan. The Burer-Monteiro SDP method can fail even above the Barvinok-Pataki bound. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35 - 36th Conference on Neural Information Processing Systems, NeurIPS 2022, Advances in Neural Information Processing Systems. Neural information processing systems foundation, 2022.\n\nB. O'donoghue, E. Chu, N. Parikh, and S. Boyd. Conic optimization via operator splitting and homogeneous self-dual embedding. Journal of Optimization Theory and Applications, 169:1042-1068, 2016.\nG. Pataki. On the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues. Mathematics of Operations Research, 23(2):339-358, 1998. ISSN 0364765X, 15265471. URL http://www.jstor.org/stable/3690515.\nR. Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.\nI. Waldspurger and A. Waters. Rank optimality for the Burer-Monteiro factorization. SIAM J. Optim., 30(3):2577-2602, 2020. doi: 10.1137/ 19M1255318. URL https://doi.org/10.1137/19M1255318.\nI. Waldspurger, A. d'Aspremont, and S. Mallat. Phase recovery, maxcut and complex semidefinite programming. Mathematical Programming, 149:4781, 2015.\nR. Y. Zhang. Improved global guarantees for the nonconvex Burer-Monteiro factorization via rank overparameterization. Mathematical Programming, pages $1-30,2024$.", "tables": {}, "images": {}}], "id": "2411.03103v2", "authors": ["Faniriana Rakoto Endor", "Ir\u00e8ne Waldspurger"], "categories": ["math.OC", "cs.CC", "stat.ML"], "abstract": "We consider MaxCut-type semidefinite programs (SDP) which admit a low rank\nsolution. To numerically leverage the low rank hypothesis, a standard\nalgorithmic approach is the Burer-Monteiro factorization, which allows to\nsignificantly reduce the dimensionality of the problem at the cost of its\nconvexity. We give a sharp condition on the conditioning of the Laplacian\nmatrix associated with the SDP under which any second-order critical point of\nthe non-convex problem is a global minimizer. By applying our theorem, we\nimprove on recent results about the correctness of the Burer-Monteiro approach\non $\\mathbb{Z}_2$-synchronization problems and the Kuramoto model.", "updated": "2025-03-26T10:39:13Z", "published": "2024-11-05T13:47:07Z"}