{
  "title": "Modeling Story Expectations to Understand Engagement: A Generative\n  Framework Using LLMs",
  "sections": [
    {
      "section_id": 0,
      "text": "#### Abstract\n\nUnderstanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by $31 \\%$. The results reveal that different types of engagement - continued reading, commenting, and voting - are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "## 1 Introduction\n\nUnderstanding when and why customers engage with stories (including books, TV shows, movies, etc.) is crucial for various applications in marketing and product design within content-focused industries. Knowing when readers are more likely to continue to the next chapter of a book or when viewers are more likely\n\n[^0]\n[^0]:    *Columbia Business School, Hortense Fong (hf2462@gsb.columbia.edu), George Gui (zg2467@gsb.columbia.edu). We thank researchers whose comments and suggestions have greatly improved the paper: participants at the University of Wisconsin Symposium on Artificial Intelligence in Marketing, American Statistical Association Marketing Section Seminar, and the 2024 Conference on Artificial Intelligence, Machine Learning, and Business Analytics. We also thank Yuting Deng, Raelynn Li, Riccardo Risi, Angela Qianya Wang, and Bo Yang for exceptional research assistance.\n\nto tweet about a TV episode can inform marketing strategies in advertising, pricing, and recommendation systems, among others. Understanding potential drivers of engagement can help content creators decide what to produce.\n\nYet understanding the drivers of engagement is challenging due to the unstructured nature of the data, whether it is text in books or video in shows. Unstructured data is by nature high-dimensional and complex, leading to a vast array of potential features that could relate to content engagement. Given the limited number of books and shows produced annually, the sample size of observations is relatively small compared to the potential feature space, creating significant challenges for traditional analysis methods. Given this challenge, it is valuable to incorporate theory into generating additional valuable features to understand drivers of engagement.\n\nBuilding on economic theory, this paper introduces a framework to extract a set of features that enhance understanding of the factors driving audience engagement, based on the key premise that customer decisions are affected by their beliefs of what is to come (Friedman, 1957; Muth, 1961). We capture these beliefs by using a generative model to imagine potential story continuations and extract from these continuations measures of expectations, uncertainty, and surprise. While expectations and uncertainty have been widely modeled in the economics and marketing literature, they have mostly focused on structured data for important and well-defined variables, such as prices and qualities (Rust, 1987; Erdem and Keane, 1996). In contrast, even though it is natural for audiences to make their content engagement decisions based on what they expect to come next, such aspects of narrative engagement have rarely been modeled or approximated in the empirical literature dealing with unstructured narrative content.\n\nThe lack of modeling such expectations and uncertainty is warranted. Compared to structured data, there is not a clear starting point for how to model expectations and uncertainty in unstructured narrative contexts. With structured data, one starting point, motivated by rational expectations, is to assume that customers have specific expectations that are objectively correct (Manski, 2004). For example, an individual's belief of the distribution of prices, a one-dimensional feature, can be assumed to follow the empirical distribution of the prices observed in the data. Making the same assumption for unstructured content data is difficult. Given the first part of a story, it is unclear what a customer's belief over what is going to happen next is going to look like, not to mention measure. Even if we knew which story features to quantify, it is unclear how to generate the relevant distribution of features.\n\nThis paper proposes a novel framework for modeling expectations and uncertainty in stories. At a high level, our framework is made up of two steps: a story imagination step and a feature extraction step. In the story imagination step, we use a pre-trained large language model (LLM) to generate story continuations. Trained using the text of thousands upon thousands of books, ${ }^{1}$ LLMs can predict many probable story continuations from some initial text. For example, providing the LLM with the text from the first chapter,\n\n[^0]\n[^0]:    ${ }^{1}$ It is speculated that GPT-4 is trained on over 125,000 books. Source: https://accopyright.substack.com/p/has-your-book-been-used-to-train\n\nwe can ask it to predict the plot of the rest of the book. Furthermore, we can ask it to generate not just one continuation but multiple continuations, providing us a distribution of probable story continuations.\n\nThen in the feature extraction step, we convert these imagined story continuations into features useful for explaining engagement. Our proposed method can complement any existing feature engineering method that has developed features related to stories. We demonstrate such complementarity of our method on three sets of features that have shown to be useful for predicting narrative success: 1) emotion features (Berger and Milkman, 2012), 2) psychological themes (Toubia et al., 2019), and 3) semantic path features (Toubia et al., 2021). Using the text from the imagined story continuations, we model expectations, uncertainty, and surprise on these sets of features.\n\nWe apply our method to a dataset of 30,258 book chapters we collected from Wattpad, a large online media platform that allows writers to publish their stories and readers to consume stories. We find that our method complements existing feature engineering techniques in improving model explanatory power in the following sense: if $f$ is a feature engineering technique that converts an unstructured story into a lowdimensional feature such that $f$ (ExistingStory) is useful for explaining engagement, then this same feature engineering technique can be applied to the imagined story continuations to generate a set of complementary features (i.e., expectations, $\\mathbb{E}[f$ (ImaginedStories)], uncertainty, $\\operatorname{Var}[f$ (ImaginedStories)], and surprise, Surprise[f(ImaginedStories)]) that can further help explain engagement. Intuitively, if customers are likely to care about certain dimensions of the story that they have read and experienced, they are also likely to care about the expectations, uncertainty, and surprise along such dimensions. We quantify this complementarity by calculating the relative improvement in model performance from adding features based on imagined stories compared to adding features based on actual stories. We document that our framework amplifies the usefulness of the existing feature engineering techniques by around $31 \\%$.\n\nMoreover, while not causal, examining the regression coefficients of these belief-based features provides a starting point for exploring how customer expectations about stories may affect engagement. For example, we find that expected valence exhibits stronger associations with engagement metrics compared to the valence of the current chapter. Expectations of lower valence align with higher engagement rates. These patterns can help researchers formulate hypotheses about how different dimensions of story expectations influence reader engagement decisions, which in turn help generate actionable insights for content creators aiming to optimize engagement.\n\nThe rest of the paper is organized as follows. Section 2 overviews the relevant literature we build from and contribute to. Section 3 introduces the general framework, detailing our approach to modeling expectations and uncertainty in unstructured narrative data. Section 4 overviews the dataset of book chapters. Section 5 documents that our proposed method can help explain engagement, by applying the method to the books dataset. Section 6 discusses the limitations of this method and the boundary conditions under which the method may or may not work effectively and Section 7 concludes. By addressing the challenges of modeling expectations and uncertainty in unstructured narrative data, this paper contributes to our theoretical\n\nunderstanding of user engagement.We hope that this framework can serve as a starting point for empirical researchers to explore and formulate new hypotheses about how customer beliefs drive their behavior in contexts involving unstructured narrative data.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "# 2 Relevant Literature \n\nUnderstanding and quantifying what drives engagement with stories has been a focus of research across multiple disciplines. Classical narrative theory has established fundamental frameworks for analyzing story structure (Campbell, 1949; Field, 1979; McKee, 1997; Piper et al., 2021). These theoretical frameworks have provided the foundation for more recent empirical work that attempts to quantify narrative elements and their impact on audience engagement. For instance, Eliashberg et al. (2007) use domain knowledge from screenwriting to extract content features from movie spoilers to predict a movie's return on investment. Similarly, Shachar (2022) find that ads that have specific story elements are more successful. Recent advances in computational methods have enabled more sophisticated analysis of story elements at scale (Wilmot and Keller, 2020; Toubia et al., 2021).\n\nBuilding on these foundational approaches, researchers have developed various methods to automatically extract and quantify specific story features. Focusing on emotion, Berger and Milkman (2012) characterize news articles by their valence and arousal to predict sharing behavior. Berger et al. (2023) study what emotions hold readers' attention and cause them to read more of an article. Knight et al. (2024) find that the number and magnitude of narrative reversals (i.e., changes in valence) predict ratings across different narrative formats including movies, TV shows, novels, and fundraising pitches. Focusing on story characters, Bamman et al. (2013) propose methods to learn character types from content text, while Toubia et al. (2019) build on positive psychology literature to extract psychological themes from movie synopses to predict moviechoice behavior.\n\nHowever, partly due to a lack of scalable and efficient frameworks, most methods have focused on analyzing content that customers have already consumed rather than modeling customers' expectations or uncertainty about future content. This gap is noteworthy since economic theory suggests customers' expectations and uncertainty should play a crucial role in shaping their consumption decisions (Friedman, 1957; Muth, 1961; Ely et al., 2015). The role of expectations and uncertainty in consumer behavior has been extensively studied in economics and marketing, particularly for structured product attributes (Rust, 1987; Erdem and Keane, 1996). In marketing specifically, researchers have examined how expectations influence purchase decisions and product satisfaction (Hitsch, 2006; Nair, 2007; Misra and Nair, 2011). While these approaches have proven valuable for structured attributes like price and quality, modeling expectations for narrative content presents unique challenges.\n\nRecent work has begun exploring how suspense and surprise influence audience engagement (Ely et al., 2015; Simonov et al., 2023). Ely et al. (2015) propose mathematical definitions where suspense depends\n\non belief variance and surprise depends on belief changes. Simonov et al. (2023) empirically capture these concepts in online game streaming by using game scores to compute viewers' beliefs about expected outcomes. While effective for structured data like game scores, this approach does not address how to model beliefs about unstructured narrative content.\n\nThe emergence of large language models (LLMs) offers new possibilities for addressing this challenge. LLMs have shown strong capabilities in understanding and generating text, including complex narrative structures (Fan et al., 2018; Brown, 2020). In marketing research, they have found applications in content analysis (Arora et al., 2024; Li et al., 2024) and consumer behavior prediction (Goli and Singh, 2024; Lee, 2024; Gui and Toubia, 2023). Horton (2023) demonstrates how LLMs can simulate human-like decisionmaking in economic scenarios. Others have shown LLMs' ability to predict outcomes ranging from Academy Awards to economic trends (Pham and Cunningham, 2024; Bybee, 2023; Halawi et al., 2024). While these papers have focused on simpler prediction tasks, our work leverages LLMs to model the more complex task of predicting story developments.\n\nOur work integrates these research streams to address key gaps in understanding narrative engagement. Traditional models of consumer expectations struggle with unstructured narrative content, while computational approaches to story analysis often overlook how expectations influence engagement. We propose a framework that leverages LLMs' understanding of narrative structure to model audience expectations, combining theoretical insights about the roles of expectations, uncertainty, and surprise in engagement with practical methods for analyzing unstructured content.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 3 Method \n\nUnlike traditional content-based methods, our approach leverages the power of LLMs to simulate consumer's beliefs about what is yet to come in a story. This novel approach is made possible by two key characteristics of LLMs:\n\n1. Vast Knowledge Base: LLMs are trained on enormous datasets encompassing diverse narratives across various genres, cultures, and time periods (Radford et al., 2019). This expansive training allows them to capture complex patterns in storytelling that would be difficult to model explicitly. In our context, this means the LLM can generate plausible story continuations that reflect the nuanced ways in which narratives typically unfold, mirroring the expectations formed by consumers with broad exposure to stories.\n2. Generative Capabilities: Unlike traditional models that often rely on pre-defined features, LLMs can generate new, contextually relevant content (Fan et al., 2018; Brown, 2020). This generative ability is crucial for our approach, as it allows us to simulate the open-ended, creative process of reader imagination. By generating multiple possible continuations for a given narrative, we can model the\n\nrange of expectations a reader might form, capturing the uncertainty and anticipation inherent in narrative engagement.\n\nWe combine these LLM capabilities with theories from psychology, narratology, and economics to extract mid-level features (e.g., emotion expectations) from the LLM-imagined stories that are potential drivers of consumer engagement. Several of these features are focused on fundamental constructs of human cognition and emotion, which are likely to generalize across diverse narrative settings (Boyd, 2009). Rather than relying on genre-specific plot elements, we capture broader story elements like anticipation of the emotional trajectory or beliefs about character development (Oatley, 1999).\n\nWe overview the proposed framework for modeling expectations and uncertainty in stories. The approach consists of four main steps, as illustrated in Figure 1 and detailed below. While our framework can apply to any type of content with a narrative arc, such as books, TV shows, and movies, we focus our empirical application on books. We detail our method using book chapters as the running example.\n\nFigure 1: Overview of Proposed Method\n![img-0.jpeg](img-0.jpeg)",
      "tables": {},
      "images": {
        "img-0.jpeg": "data:image/jpeg;base64,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"
      }
    },
    {
      "section_id": 4,
      "text": "# 3.1 Story Input \n\nOur starting point is a piece of narrative content, such as the first few chapters of a book. One challenge in using LLMs is there is a limit on the size of the context window for the input into the model. For example, GPT-3.5-turbo can at most accommodate 16,385 tokens, which translates to roughly 12,300 words. ${ }^{2}$ While we can easily feed in the text for one chapter, the context window size becomes binding after just a few chapters. To overcome this issue, we use as input a summary of chapters 1 to $t-1$ and the full text of chapter $t$ to generate the predicted stories.\n\nPast research has shown that GPT models can generate summaries that are preferred to those generated by models fine-tuned for text summarization (Goyal et al., 2022) and even achieve human levels of summarization (Zhang et al., 2024). We summarize the text up until chapter $t-1$ recursively, that is summary( $1, \\ldots, \\mathrm{t}-$ 1) $=$ summary $\\left(\\right.$ summary $\\left(1, \\ldots, \\mathrm{t}-2\\right)$,text(t-1)), and use different prompts depending on whether the chapter is the first chapter or not:\n\n1. Prompt for Chapter 1: \"You are an average book reader. Here is the first chapter of a book. Provide an extensive summary of the chapter. Focus on the characters, their actions and emotions, and events\n[^0]\n[^0]:    ${ }^{2}$ https://platform.openai.com/docs/models/gpt-3-5-turbo, https://platform.openai.com/tokenizer\n\nin a coherent and consistent way without making assumptions. Don't start with sentences mentioning the chapter or the book - such as 'In this chapter'; get right into the summary.\"\n2. Prompt for Chapters 2+: \"You are an average book reader. Here are the summary of the previous chapters of a book you have read, and the entire text of a new chapter. Provide an extensive summary of the entire book based on the previous chapters and the new chapter. Focus on the characters, their actions and emotions, and events in a coherent and consistent way without making assumptions. Don't start with sentences mentioning the chapter or the book - such as 'In this chapter'; get right into the summary.\"\n\nThe summaries along with the focal chapter text become the input to a pre-trained LLM to generate imagined story continuations.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "# 3.2 Imagination Generation \n\nLLMs are trained to predict the next word given a set of preceding words. More specifically, language models estimate the conditional probability of seeing word $w_{i}$, given all the previous words: $p\\left(w_{i} \\mid w_{1}, \\ldots, w_{i-1}\\right)$. Given the large amounts of story data that went into training GPT, we expect the model to excel at generating story continuations. This novel approach aims to model the process of readers anticipating potential story developments, a key factor in engagement that has previously been challenging to model. Readers form beliefs based on their past story consumption and we can think of GPT as a representative reader who has consumed a vast and diverse set of content. Recent research has found that LLMs can generate economic expectations based on historical news that match human expectations (Bybee, 2023) and predict consumer preferences for new products (Lee, 2024).\n\nWe generate multiple imagined continuations of the story based on the content up to a specific chapter. ${ }^{3}$ Importantly, we generate not just one potential story continuation, but several to capture the uncertainty in how the story will develop. We use the following prompts to generate the imagined continuations:\n\n1. Prompt for Chapter 1: \"You have read and understood the first chapter of a book, and now I will provide you with the text of this chapter. Here is the first chapter: \\{chapter text\\}. Based on this, please imagine the plot for the remaining chapters, weaving together the characters, their actions and emotions, and events in a coherent and consistent way. Then, summarize this plot into a set of 20 simple and distinct bullet points.\"\n[^0]\n[^0]:    ${ }^{3}$ We set the temperature to 1.0. The temperature parameter affects the creativity of the generated text by scaling the probabilities of the next word that the model can select from. A temperature of 0 generates more deterministic output, while a temperature of 2.0 generates more unpredictable output. We set the temperature to 1.0 so that the output can vary while remaining reasonable. While tuning the parameter, we found that higher values generated story continuations that did not make as much sense.\n\n2. Prompt for Chapters 2+: \"You have read and understood the previous chapters of a book, and now I will provide you with a summary of those chapters, along with the text from the most recent chapter. Here is the summary of the previous chapters: \\{summary\\}. Here is the current chapter: \\{chapter text\\}. Based on this, please imagine the plot for the remaining chapters, weaving together the characters, their actions and emotions, and events in a coherent and consistent way. Then, summarize this plot into a set of 20 simple and distinct bullet points.\"\n\nIn our empirical application, we generate 10 imagined story continuation per book chapter.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "# 3.3 Feature Extraction \n\nWith multiple imagined story continuations per chapter, the next question is how to quantify the unstructured story text. We approach this question in two steps: 1) we extract from the text predefined features that have been proposed in the literature to be associated with narrative success and 2) we calculate measures of expectations, uncertainty, and surprise based on those features. Let $i$ represent the focal book, $t$ the focal chapter, and $n$ the imagined story number with $N$ capturing the total number of imagined stories per chapter. Let $z_{i t n}$ denote the extracted features from the text (i.e., $z_{i t n}=f\\left(\\right.$ ImaginedStory $\\left._{i t n}\\right)$. The transformation $f$ can be a rule-based algorithm like VADER (Hutto and Gilbert, 2014) or a learned deep learning model like GPT (Radford et al., 2019). In Section 5, we discuss the specific transformations and features we extract as an empirical demonstration of our method. Using the extracted features $z_{i t n}$ we calculate the expectations, uncertainty, and surprise as follows:\n\n1. Expectation Features: We calculate the mean of each feature across all $N$ imagined continuations for a given chapter. This represents the average expected future state of the narrative.\n\n$$\n\\text { Expectations }_{i t}=\\mathbb{E}_{n}\\left[z_{i t n}\\right]=\\frac{1}{N} \\sum_{n=1}^{N} z_{i t n}\n$$\n\n2. Uncertainty Features: We compute the variance of each feature across continuations, quantifying the degree of uncertainty in future narrative developments. This measure is akin to the measure of \"suspense\" proposed by Ely et al. (2015). While Ely et al. (2015) assume that utility is an increasing function of suspense, it is also possible that uncertainty relates to confusion or that readers may prefer certainty on some dimensions and uncertainty on other dimensions. Our framework allows us to treat this as an empirical question to be answered in Section 5.\n\n$$\n\\text { Uncertainty } y_{i t}=\\operatorname{Var}_{n}\\left[z_{i t n}\\right]=\\frac{1}{N} \\sum_{n=1}^{N}\\left(z_{i t n}-\\mathbb{E}_{n}\\left[z_{i t n}\\right]\\right)^{2}\n$$\n\n3. Surprise Features: Following Ely et al. (2015), we define surprise as the squared difference in expectations before and after consuming chapter $t$. It quantifies the degree of unexpectedness in audience expectations.\n\n$$\n\\text { Surprise }_{i t}=\\left(\\text { Expectations }_{i t}-\\text { Expectations }_{i(t-1)}\\right)^{2}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 3.4 Explaining Engagement \n\nFinally, we use the extracted expectations, uncertainty, and surprise based on the imagined story continuations alongside the features extracted from the actual story text $f\\left(\\right.$ ExistingStory $\\left._{i t}\\right)$ to predict user engagement metrics. In our empirical context, the metrics include the continue-to-read rate, the comment-to-read rate, and the vote-to-read rate. We compare the benefit derived from incorporating data from the story imagination versus a standard approach only using the text data from the content.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "## 4 Data\n\nTo demonstrate our proposed methodology, we collect book text with chapter-by-chapter engagement. We collect data from Wattpad, an online media platform that allows users to read and write stories. In October 2024, Wattpad had over 90 million readers and writers. For readers, the vast majority of stories are free to read but some stories require a premium membership or payment to access the later chapters. The major advantage of using Wattpad is that content is posted chapter by chapter and we can observe the read count, vote count, and comment count for each chapter and therefore calculate the continue-to-read, vote-to-read, and comment-to-read rates.\n\nWe collected a corpus of publicly accessible free books from Wattpad by focusing on the genres listed on Wattpad's homepage (e.g., action, adventure, romance). Appendix A provides additional details about the keywords used to collect the books. For each book, we collect its title and description, when the book was created, the language the book is written in, whether the content is for mature audiences, and writerprovided book tags (e.g., \"school\", \"drama\", \"friendship\"). For each chapter, we collect its text, title, date it was written, and its comment count, vote count, and read count.\n\nWe use GPT-4o-mini to summarize the chapters and GPT-3.5-turbo to generate the imagined story continuations. ${ }^{4}$ We use GPT-4o mini for text summarization because of its good summarization capabilities, its lower cost relative to GPT-3.5-turbo, and because we are not concerned about data leakage for this step. However, for the imagination generation, to alleviate the concern about data leakage (i.e., the engagement metrics and actual story continuations being part of the training data), we only include books that were published after the cutoff date of the training data for GPT-3.5-turbo (September 2021) so all book chapters are published January 2022 and onwards. This ensures the content was not used to train the GPT model we use to generate the imagined stories. Since our proposed story imagination method relies critically on the quality of the input text, we take several steps to clean the collected stories, which we detail in Appendix ??. After cleaning, we are left with 30,258 chapters across 1,735 books.\n\n[^0]\n[^0]:    ${ }^{4}$ Specifically, we use GPT-4o-mini-2024-07-18 and GPT-3.5-turbo-0125.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "# 4.1 Summary Statistics \n\nTable 1 provides summary statistics of our dataset of book chapters. Compared to more traditional books, which have on average 3,000 to 4,000 words per chapter, the Wattpad chapters are shorter with an average of 1,827 words per chapter. The engagement with these chapters is fairly high not only in terms of continuation rates but also in terms of comment and vote rates. Readers can comment throughout the text and comments are consolidated at the end of the chapter text. Readers can also vote to show their support for a writer and can only vote once per chapter. Our outcome measures of interest are:\n\n- Continue-to-read rate $=$ read count of next chapter/read count of current chapter\n- Comment-to-read rate $=$ comment count of current chapter/read count of current chapter\n- Vote-to-read rate $=$ vote count of current chapter/read count of current chapter\n\nNotably, the continue-to-read rate can exceed one, suggesting some readers may skip chapters, reread chapters, or share chapters with friends.\n\nTable 1: Summary Statistics\n\n![table_0](table_0)",
      "tables": {
        "table_0": "| Measure | Mean | Std Dev | Min | Max |\n| :-- | --: | --: | --: | --: |\n| Number of words | 1,827 | 1,242 | 400 | 9,970 |\n| Comment count | 33 | 128 | 0 | 6,767 |\n| Vote count | 117 | 280 | 0 | 4,285 |\n| Read count | 3,135 | 8,219 | 1 | 206,440 |\n| Continue-to-read rate | 0.96 | 0.33 | 0 | 31.25 |\n| Comment-to-read rate | 0.05 | 0.22 | 0 | 6.03 |\n| Vote-to-read rate | 0.06 | 0.07 | 0 | 1.00 |"
      },
      "images": {}
    },
    {
      "section_id": 10,
      "text": "## 5 Empirical Application with Books Data\n\nWe apply our proposed framework to the collected Wattpad data.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "### 5.1 Predefined Story Features\n\nAs discussed in Section 2, the literature has identified several sets of features based on existing text to be associated with narrative success. To demonstrate our approach, we use three sets of features: 1) emotion features as measured by valence and arousal, 2) psychological themes, and 3) semantic path measures. We extract these features not only from the chapter text but also from the imagined stories generated by GPT-3.5-turbo. We detail each set of features below.\n\nEmotion Features: Tan (2008) suggests that we consume content because of the emotion of the story. Berger and Milkman (2012) show that emotion drives content sharing behavior of news articles. We extract the valence and arousal (Russell, 1980) from each chapter and the imagined story continuations, capturing the anticipated emotional trajectory of the narrative. Valence captures how positive or negative a reader may feel or expect to feel and arousal captures how excited or calm. To measure valence and arousal, we use the pre-trained RoBERTa models proposed by Mendes and Martins (2023). Each chapter and imagined story is characterized by one measure of valence and one measure of arousal. These features provide insight into the expected emotional impact of the story.\n\nPsychological Themes: We measure a set of psychological themes derived from the positive psychology literature (e.g., personal growth, resilience, social connection) (Seligman and Csikszentmihalyi, 2000). The key idea underlying the use of these psychological themes is that users consume content because of character development (Toubia et al., 2019; Peterson, 2004). Toubia et al. (2019) show that these psychological themes help to predict movie choice behavior. This method allows us to quantify the expected character development of the rest of the imagined story. To measure the psychological themes, we apply the guided LDA approach proposed by Toubia et al. (2019). We characterize each chapter and imagined story by 25 psychological themes.\n\nSemantic Path Features: We measure the speed, volume, and circuitousness of each chapter and imagined story according to the strategy proposed by Toubia et al. (2021). Speed captures pacing, volume captures the ground covered, and circuitousness captures how roundabout the chapter is. These measures are calculated on the word embeddings of the text. Toubia et al. (2021) find that these three measures are associated with narrative success when applied to movie subtitles, TV show dialogues, and academic papers.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# 5.2 Engagement Prediction Results\n### 5.2.1 Relative Marginal Improvement\n\nThis section examines whether our method complements existing feature extraction approaches in explaining engagement. We hypothesize that if a feature extraction method significantly improves model performance when applied to the actual story content, then applying the same method to imagined story continuations should yield additional improvement. The core rationale being that if a reader cares about certain story dimensions in what they have consumed, then they will likely care about the same dimensions in what is to come. We test this hypothesis using emotion features, psychological themes, and semantic path features, since they were documented to be associated with narrative success in prior literature.\n\nFor each outcome variable (vote-to-read rate, comment-to-read rate, or continue-to-read rate), we compare the explanatory power of a baseline model containing only basic controls (log word count and chapter fixed effects) to a model that adds features extracted from the actual story text. This comparison reveals which feature extraction methods provide significant value in explaining engagement.\n\nThen, we evaluate whether applying these same feature engineering techniques to imagined stories provides additional explanatory power. Specifically, we test whether further improvements can be achieved by incorporating three belief-based measures: the expected values of these features across imagined stories, their variance (capturing uncertainty), and surprise (measured as the squared difference in expectations between consecutive chapters). This approach allows us to assess whether modeling reader beliefs complements existing feature extraction methods to understand engagement.\n\nTable 2 shows the change in adjusted $\\mathrm{R}^{2}$ as we add features to a linear regression model. The first column includes as base features chapter number as fixed effects and log current chapter word count as a linear control. The second column includes the base features as well as features extracted from the text of the focal chapter and the average of the feature values for all the preceding chapters. The third column brings in the belief-based features extracted from the imagined story continuations.\n\nOur results show that in cases where the features calculated on the actual story text significantly improve model performance, adding the features based on the imagined story continuations provides additional explanatory power ranging from $6 \\%$ to $50 \\%$.\n\nTable 3 decomposes the total improvement into the contribution from each of the three belief-based components (i.e., expectations, uncertainty, surprise). The major driving component is the expectations over the features from the imagined story continuations.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# 5.2.2 Regression Results \n\nFurther, we can dive into the regression coefficients to gain some insight into how the features relate to engagement. As an example, we show the emotion features in Table 4. We observe that higher arousal past chapters and higher arousal focal chapters correspond to greater engagement. But while readers are more likely to continue consuming more negative content, they are more likely to comment on more positive content. Commenting and sharing may share similar motivations in that they are both outward looking and we find the observed patterns consistent with the conclusions in Berger and Milkman (2012), who find that positive content and high-arousal content are more viral than negative content and low-arousal content, respectively.\n\nSurprise on the valence dimension is associated with increased engagement. This observation is consistent with the finding of Knight et al. (2024) that more narrative reversals (switches between positive and negative valence) correspond to greater content liking. Readers are also more likely to engage when the expectation of the rest of the story is more negative. Interestingly, the expectations on valence appear to play a larger role than the valence of the actual text in predicting engagement.\n\nThese findings suggest actionable implications for content creators and digital platforms aiming to optimize user engagement. Notably, the distinction between types of engagement - continued consumption versus active engagement like commenting and voting - points to an opportunity for differentiated engagement strategies. For example, if comments help to generate a sense of community and this is important for\n\nTable 2: Adjusted $\\mathrm{R}^{2}$ Comparison\n\n![table_1](table_1)\n\nNote: This table reports in-sample adjusted $\\mathrm{R}^{2}$ values (in percentages) for different feature sets and outcomes. The models progressively add features as indicated in the bottom panel. The relative improvement column shows the percentage improvement from adding imagined features relative to the improvement from adding original features.\n\nTable 3: Relative Improvement from Imagined Features by Component\n\n![table_2](table_2)\n\nNote: This table reports the relative improvement in adjusted $\\mathrm{R}^{2}$ from adding imagined features compared to the improvement from adding original features. Components shown are the total improvement from all imagined features (Total) and the individual contributions from the surprise, expectations, and uncertainty components.\nnew customers, then a platform may want to recommend early on more positive and exciting content that generates expectations that the story will become more negative. On Wattpad, authors have the option to directly address their readers in their writing and can strategically place nudges to encourage readers to vote for or comment on their content based on the actual and anticipated emotion.",
      "tables": {
        "table_1": "| Feature Set | Adjusted R ${ }^{2}$ |  |  | Relative Improvement |\n| :--: | :--: | :--: | :--: | :--: |\n|  | Base | Add Feature | Add Belief Features |  |\n| Outcome: Vote |  |  |  |  |\n| Emotion | 1.66 | 2.44 | 2.79 | $44 \\%$ |\n| Psychological Theme | 1.66 | 5.88 | 7.47 | $38 \\%$ |\n| Semantic Path | 1.66 | 2.77 | 2.98 | $19 \\%$ |\n| Outcome: Comment |  |  |  |  |\n| Emotion | 0.15 | 0.33 | 0.42 | $50 \\%$ |\n| Psychological Theme | 0.15 | 2.24 | 2.95 | $33 \\%$ |\n| Semantic Path | 0.15 | 0.51 | 0.66 | $41 \\%$ |\n| Outcome: ContinueRate |  |  |  |  |\n| Emotion | 16.94 | 17.08 | 17.10 | $11 \\%$ |\n| Psychological Theme | 16.94 | 17.34 | 17.48 | $36 \\%$ |\n| Semantic Path | 16.94 | 17.56 | 17.60 | $6 \\%$ |\n| Features Present: |  |  |  |  |\n| Chapter number \\& word count: | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |  |\n| Basic feature: $f$ (ExistingStory) |  | $\\checkmark$ | $\\checkmark$ |  |\n| Expectations: $\\mathbb{E}[f$ (ImaginedStories $)]$ |  |  | $\\checkmark$ |  |\n| Uncertainty: $\\operatorname{Var}[f$ (ImaginedStories $)]$ |  |  | $\\checkmark$ |  |\n| Surprise( $\\mathbb{E}\\left[\\right.$ ImaginedStories $\\left._{t-1}\\right], \\mathbb{E}\\left[\\right.$ ImaginedStories $\\left._{t}\\right]$ ) |  |  | $\\checkmark$ |  |",
        "table_2": "| Relative Improvement in In-Sample R ${ }^{2}$ |  |  |  |  |  |\n| :--: | :--: | :--: | :--: | :--: | :--: |\n| Feature Set | Outcome | Total | Surprise | Expectations | Uncertainty |\n| Emotion | Vote | $44.5 \\%$ | $-0.1 \\%$ | $45.2 \\%$ | $-0.7 \\%$ |\n| Emotion | Comment | $49.8 \\%$ | $8.2 \\%$ | $39.6 \\%$ | $4.8 \\%$ |\n| Emotion | ContinueRate | $11.5 \\%$ | $2.8 \\%$ | $7.6 \\%$ | $-1.5 \\%$ |\n| Psychological Theme | Vote | $37.8 \\%$ | $5.3 \\%$ | $33.1 \\%$ | $20.3 \\%$ |\n| Psychological Theme | Comment | $33.5 \\%$ | $3.9 \\%$ | $30.3 \\%$ | $8.8 \\%$ |\n| Psychological Theme | ContinueRate | $36.5 \\%$ | $3.8 \\%$ | $43.0 \\%$ | $20.7 \\%$ |\n| Semantic Path | Vote | $18.6 \\%$ | $-0.5 \\%$ | $13.1 \\%$ | $5.3 \\%$ |\n| Semantic Path | Comment | $41.4 \\%$ | $0.4 \\%$ | $43.0 \\%$ | $-0.1 \\%$ |\n| Semantic Path | ContinueRate | $5.7 \\%$ | $6.2 \\%$ | $-0.0 \\%$ | $-0.3 \\%$ |"
      },
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# 6 Limitations \n\nOur method has several limitations that warrant discussion and present opportunities for future research. The primary challenge lies in interpreting approximated beliefs. Our method generates a distribution of potential story continuations to model audience expectations, but these may not perfectly align with actual audience perceptions. For instance, in a set of 100 generated continuations for a crime thriller, we might observe an overrepresentation of certain tropes that real audiences would not necessarily anticipate. This misalignment could lead to biased predictions in certain genres or for specific narrative structures.\n\nThis methodological challenge parallels the broader literature in economics on inferred or modeled expectations, where researchers approximate subjective beliefs without direct observation. For example, rational expectations models assume that consumers' forecasts align with objective distributions-an assumption that may not perfectly mirror reality, yet enables tractable empirical and theoretical analysis. In a similar vein, our approach uses a simplified, implementable model of how readers form expectations about stories. While actual reader imagination processes may be more complex or different than what our LLM gener-\n\nTable 4: Regression Results - Emotion Features\n\n![table_3](table_3)\n\nNote:\n\n$$\n{ }^{*} \\mathrm{p}<0.1 ;{ }^{* *} \\mathrm{p}<0.05 ;{ }^{* * *} \\mathrm{p}<0.01 ; \\text { Standard errors in parentheses }\n$$\n\nates, having a concrete, implementable way to model expectations creates a foundation for empirical work. Moreover, the fact that our method enhances the explanatory power of existing narrative features by about $31 \\%$ on average suggests that it meaningfully captures elements of how audiences process and respond to content\u2014even if it does not fully replicate the richness of human imagination.\n\nThe current focus on textual data limits our ability to capture important visual and auditory elements in media like TV shows, films, or short videos. This restriction could lead to underperformance in predicting engagement for visually-driven or musically-rich content. However, our framework is theoretically extensible to multi-modal data. Future research could explore integrating visual and auditory elements using multi-modal LLMs to create more comprehensive distributions of story continuations. This extension could significantly enhance the model's applicability across diverse media formats and potentially improve model performance for visual and auditory-heavy content.\n\nLastly, biases in LLMs could lead to skewed predictions, particularly for underrepresented genres or narrative styles. Addressing this requires developing more diverse training datasets and advancing research in AI ethics and fairness. Future work could explore techniques for debiasing LLMs or developing genre or narrative style-specific models to ensure equitable performance across all types of content.\n\nDespite these challenges, our method offers a valuable starting point for modeling audience expectations and uncertainty in stories. Just as rational expectations served as a useful starting point for studying consumer behavior under uncertainty, our framework offers an initial approach for modeling reader beliefs that can be refined and expanded through future research. By addressing these limitations, future work can potentially lead to more accurate, comprehensive, and robust models of narrative engagement.",
      "tables": {
        "table_3": "|  | Dependent variable: |  |  |\n| :--: | :--: | :--: | :--: |\n|  | Vote-to-Read Rate | Comment-to-Read Rate | Log Continue-to-Read Rate |\n| Log-Word-Count | $0.009^{* * *}(0.001)$ | $0.017^{* * *}(0.002)$ | $-0.009^{* * *}(0.001)$ |\n| Valence-CurrentChapter | $-0.0003(0.001)$ | $0.004^{* *}(0.002)$ | $-0.003^{* * *}(0.001)$ |\n| Arousal-CurrentChapter | $0.002^{* * *}(0.001)$ | $0.008^{* * *}(0.002)$ | $0.001(0.001)$ |\n| Valence-PastChapters | $-0.004^{* * *}(0.001)$ | $0.004(0.003)$ | $0.005^{* * *}(0.001)$ |\n| Arousal-PastChapters | $0.005^{* * *}(0.001)$ | $0.005^{* *}(0.002)$ | $0.002^{* *}(0.001)$ |\n| Valence-Surprise | $0.009^{* * *}(0.002)$ | $0.010^{*}(0.005)$ | $0.009^{* * *}(0.002)$ |\n| Arousal-Surprise | $-0.004(0.003)$ | $0.003(0.011)$ | $0.001(0.005)$ |\n| Valence-Expectation | $-0.018^{* * *}(0.002)$ | $-0.026^{* * *}(0.005)$ | $-0.009^{* * *}(0.002)$ |\n| Arousal-Expectation | $0.005(0.004)$ | $0.003(0.012)$ | $-0.005(0.005)$ |\n| Valence-Uncertainty | $0.001(0.010)$ | $-0.031(0.034)$ | $0.018(0.015)$ |\n| Arousal-Uncertainty | $-0.012(0.017)$ | $-0.090(0.055)$ | $0.006(0.024)$ |\n| Chapter FE | Yes | Yes | Yes |\n| Observations | 30,258 | 30,258 | 30,258 |\n| $\\mathrm{R}^{2}$ | 0.031 | 0.006 | 0.173 |\n| Adjusted R ${ }^{2}$ | 0.029 | 0.004 | 0.171 |\n| Residual Std. Error $(\\mathrm{df}=30198)$ | 0.068 | 0.221 | 0.098 |"
      },
      "images": {}
    },
    {
      "section_id": 15,
      "text": "# 7 Conclusion \n\nThis paper introduces a framework for modeling audience expectations in stories using large language models to simulate story continuations. Our approach quantifies these concepts in unstructured narrative data, offering insights for understanding user engagement.\n\nAt the core of our methodology is a process that transforms narrative content into features representing audience expectations, uncertainty, and surprise. This approach bridges the gap between qualitative narrative analysis and quantitative modeling. To demonstrate its effectiveness, we applied our method to a dataset of over 30,000 book chapters from Wattpad. Our method complements existing feature engineering techniques by providing a framework to extend their application to reader beliefs, amplifying their marginal explanatory value by $31 \\%$. Through careful analysis of the regression results, we can uncover key narrative elements that correspond to audience engagement. These findings contribute to a deeper understanding of the relationship between narrative structure and audience response and generate hypotheses that can be further explored. In conclusion, our framework advances our ability to model audience expectations for narrative content. By quantifying consumers' forward-looking beliefs, we provide a valuable tool for marketers, content creators,\n\nand researchers to better understand audience behavior.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "# References \n\nArora, N., Chakraborty, I., and Nishimura, Y. (2024). Express: Ai-human hybrids for marketing research: Leveraging llms as collaborators. Journal of Marketing, page 00222429241276529.\n\nBamman, D., O\u2019Connor, B., and Smith, N. A. (2013). Learning latent personas of film characters. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $352-361$.\n\nBerger, J. and Milkman, K. L. (2012). What makes online content viral? Journal of Marketing Research, 49(2):192-205.\n\nBerger, J., Moe, W. W., and Schweidel, D. A. (2023). What holds attention? linguistic drivers of engagement. Journal of Marketing, 87(5):793-809.\n\nBoyd, B. (2009). On the origin of stories: Evolution, cognition, and fiction. Harvard University Press.\nBrown, T. B. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\nBybee, J. L. (2023). The ghost in the machine: Generating beliefs with large language models. arXiv preprint arXiv:2305.02823.\n\nCampbell, J. (1949). The Hero with a Thousand Faces. Pantheon Books, New York.\nEliashberg, J., Hui, S. K., and Zhang, Z. J. (2007). From story line to box office: A new approach for green-lighting movie scripts. Management Science, 53(6):881-893.\n\nEly, J., Frankel, A., and Kamenica, E. (2015). Suspense and surprise. Journal of Political Economy, 123(1):215-260.\n\nErdem, T. and Keane, M. P. (1996). Decision-making under uncertainty: Capturing dynamic brand choice processes in turbulent consumer goods markets. Marketing science, 15(1):1-20.\n\nFan, A., Lewis, M., and Dauphin, Y. (2018). Hierarchical neural story generation. arXiv preprint arXiv:1805.04833.\n\nField, S. (1979). Screenplay: The Foundations of Screenwriting. Delta, New York.\nFriedman, M. (1957). The permanent income hypothesis. In A theory of the consumption function, pages 20-37. Princeton University Press.\n\nGoli, A. and Singh, A. (2024). Can llms capture human preferences? Working Paper.\n\nGoyal, T., Li, J. J., and Durrett, G. (2022). News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356.\n\nGui, G. and Toubia, O. (2023). The challenge of using llms to simulate human behavior: A causal inference perspective. arXiv preprint arXiv:2312.15524.\n\nHalawi, D., Zhang, F., Yueh-Han, C., and Steinhardt, J. (2024). Approaching human-level forecasting with language models. arXiv preprint arXiv:2402.18563.\n\nHitsch, G. J. (2006). An empirical model of optimal dynamic product launch and exit under demand uncertainty. Marketing Science, 25(1):25-50.\n\nHorton, J. J. (2023). Large language models as simulated economic agents: What can we learn from homo silicus? Technical report, National Bureau of Economic Research.\n\nHutto, C. and Gilbert, E. (2014). Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Proceedings of the international AAAI conference on web and social media, volume 8, pages $216-225$.\n\nKnight, S., Rocklage, M. D., and Bart, Y. (2024). Narrative reversals and story success. Science Advances, 10(34):eadl2013.\n\nLee, K. (2024). Generative brand choice.\n\nLi, P., Castelo, N., Katona, Z., and Sarvary, M. (2024). Frontiers: Determining the validity of large language models for automated perceptual analysis. Marketing Science.\n\nManski, C. F. (2004). Measuring expectations. Econometrica, 72(5):1329-1376.\n\nMcKee, R. (1997). Story: Substance, Structure, Style, and the Principles of Screenwriting. ReganBooks, New York.\n\nMendes, G. A. and Martins, B. (2023). Quantifying valence and arousal in text with multilingual pre-trained transformers. In European Conference on Information Retrieval, pages 84-100. Springer.\n\nMisra, S. and Nair, H. S. (2011). A structural model of sales-force compensation dynamics: Estimation and field implementation. Quantitative Marketing and Economics, 9:211-257.\n\nMuth, J. F. (1961). Rational expectations and the theory of price movements. Econometrica: journal of the Econometric Society, pages 315-335.\n\nNair, H. (2007). Intertemporal price discrimination with forward-looking consumers: Application to the us market for console video-games. Quantitative Marketing and Economics, 5:239-292.\n\nOatley, K. (1999). Why fiction may be twice as true as fact: Fiction as cognitive and emotional simulation. Review of general psychology, 3(2):101-117.\n\nPeterson, C. (2004). Character strengths and virtues: A handbook and classification. American psychological association, 25 .\n\nPham, V. H. and Cunningham, S. (2024). Can base chatgpt be used for forecasting without additional optimization? Available at SSRN 4907279.\n\nPiper, A., So, R. J., and Bamman, D. (2021). Narrative theory for computational narrative understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 298-311.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\nRussell, J. A. (1980). A circumplex model of affect. Journal of personality and social psychology, 39(6):1161.\nRust, J. (1987). Optimal replacement of gmc bus engines: An empirical model of harold zurcher. Econometrica: Journal of the Econometric Society, pages 999-1033.\n\nSeligman, M. E. and Csikszentmihalyi, M. (2000). Positive psychology: An introduction., volume 55. American Psychological Association.\n\nShachar, R. (2022). Sell me a story: On the role of conflict, and other story elements, in ads' success. Available at SSRN 4199334.\n\nSimonov, A., Ursu, R. M., and Zheng, C. (2023). Suspense and surprise in media product design: Evidence from twitch. Journal of Marketing Research, 60(1):1-24.\n\nTan, E. S.-H. (2008). Entertainment is emotion: The functional architecture of the entertainment experience. Media psychology, 11(1):28-51.\n\nToubia, O., Berger, J., and Eliashberg, J. (2021). How quantifying the shape of stories predicts their success. Proceedings of the National Academy of Sciences, 118(26):e2011695118.\n\nToubia, O., Iyengar, G., Bunnell, R., and Lemaire, A. (2019). Extracting features of entertainment products: A guided latent dirichlet allocation approach informed by the psychology of media consumption. Journal of Marketing Research, 56(1):18-36.\n\nWilmot, D. and Keller, F. (2020). Modelling suspense in short stories as uncertainty reduction over neural representation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages $1763-1788$.\n\nZhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown, K., and Hashimoto, T. B. (2024). Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:39-57.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 17,
      "text": "# A Appendix: Data Cleaning \n\nOur Wattpad dataset is collected based on specific search keywords related to the genres listed on Wattpad's homepage. The genres listed are: Action, Adventure, ChickLit, Classics, Fanfiction, Fantasy, General Fiction, Historical Fiction, Horror, Humor, Mystery, Non-Fiction, Paranormal, Poetry, Random, Romance, Science Fiction, Short Story, Spiritual, Teen Fiction, Thriller, Vampire, Werewolf.\n\nWe preprocess the dataset at three levels - text, chapter, and book - to focus on content that best reflects narrative structure and reader engagement. This multi-stage cleaning ensures high-quality inputs for downstream analysis and generative modeling.\n\nAt the text level, we remove content elements that are clearly outside the core narrative. This includes author commentary, notes directed at readers, and other meta-text that may appear at the beginning or end of chapters. We also remove any formatting tags (e.g., HTML) and other non-linguistic artifacts that may interfere with language model processing.\n\nChapters are then filtered to retain only those likely to contain meaningful narrative content. We begin by identifying and excluding chapters that are unlikely to be part of the story - such as playlists, author updates, and character lists-using a combination of heuristics (e.g., titles, word count thresholds) and a language model classifier that assigns a likelihood score based on the text and metadata. Chapters with very low predicted narrative probability are excluded. To comply with content policies and ensure model compatibility, we further exclude chapters predicted to contain explicit material. These predictions are made using a language model that estimates the likelihood of such content. To avoid discontinuities in behavioral signals, we also exclude the chapter that immediately precedes flagged ones. We additionally remove chapters that appear to have been rewritten during the data collection window, which can reset engagement counters and distort metrics such as continuation rates. Chapters written shortly before the data collection date are excluded as well, due to limited exposure time and low engagement signal. We also omit final chapters (which lack follow-up continuation metrics) and chapters with zero read counts, which can skew rate-based calculations.\n\nAfter chapter-level filtering, we apply further criteria at the book level. First, we restrict the dataset to English-language content, estimating language consistency by measuring the proportion of English words in each chapter and computing a book-level average. Books and chapters that fall below language thresholds are removed. We also exclude books tagged with mature content labels, as well as books with a high proportion of chapters predicted to contain sensitive content. This avoids gaps in narrative flow and ensures compatibility with content generation policies. Books with extremely short average chapter lengths (e.g., meme collections or short-form entries) and excessively long chapters (which may reflect full novels pasted into a single entry) are also removed. To maintain consistency in modeling and avoid overrepresentation, we drop books with more than 50 chapters. Finally, we exclude books whose first chapter shows minimal engagement, as this limits our ability to observe reader behavior.",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2412.15239v2",
  "authors": [
    "Hortense Fong",
    "George Gui"
  ],
  "categories": [
    "cs.CL",
    "cs.AI",
    "econ.GN",
    "q-fin.EC",
    "stat.ME"
  ],
  "abstract": "Understanding when and why consumers engage with stories is crucial for\ncontent creators and platforms. While existing theories suggest that audience\nbeliefs of what is going to happen should play an important role in engagement\ndecisions, empirical work has mostly focused on developing techniques to\ndirectly extract features from actual content, rather than capturing\nforward-looking beliefs, due to the lack of a principled way to model such\nbeliefs in unstructured narrative data. To complement existing feature\nextraction techniques, this paper introduces a novel framework that leverages\nlarge language models to model audience forward-looking beliefs about how\nstories might unfold. Our method generates multiple potential continuations for\neach story and extracts features related to expectations, uncertainty, and\nsurprise using established content analysis techniques. Applying our method to\nover 30,000 book chapters, we demonstrate that our framework complements\nexisting feature engineering techniques by amplifying their marginal\nexplanatory power on average by 31%. The results reveal that different types of\nengagement-continuing to read, commenting, and voting-are driven by distinct\ncombinations of current and anticipated content features. Our framework\nprovides a novel way to study and explore how audience forward-looking beliefs\nshape their engagement with narrative media, with implications for marketing\nstrategy in content-focused industries.",
  "updated": "2025-03-26T18:59:18Z",
  "published": "2024-12-13T04:53:34Z"
}