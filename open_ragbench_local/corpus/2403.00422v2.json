{
  "title": "Inference for Interval-Identified Parameters Selected from an Estimated\n  Set",
  "sections": [
    {
      "section_id": 0,
      "text": "## Parameters Selected from an Estimated Set*\n\nSukjin Han ${ }^{\\dagger}$ Adam McCloskey ${ }^{\\ddagger}$<br>April 9, 2025",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 1,
      "text": "#### Abstract\n\nInterval identification of parameters such as average treatment effects, average partial effects and welfare is particularly common when using observational data and experimental data with imperfect compliance due to the endogeneity of individuals' treatment uptake. In this setting, the researcher is typically interested in a treatment or policy that is either selected from the estimated set of best-performers or arises from a data-dependent selection rule. In this paper, we develop new inference tools for interval-identified parameters chosen via these forms of selection. We develop three types of confidence intervals for data-dependent and interval-identified parameters, discuss how they apply to several examples of interest and prove their uniform asymptotic validity under weak assumptions.\n\n\nKeywords: Partial Identification, Post-Selection Inference, Selective Inference, Conditional Inference, Uniform Validity, Treatment Choice.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 2,
      "text": "## 1 Introduction\n\nThere is now a large and growing literature on partial identification of optimal treatments and policies under practically-relevant assumptions for observational data and experimental data with\n\n[^0]\n[^0]:    *We thank Isaiah Andrews, Irene Botosaru, Patrik Guggenberger, Chris Muris and Jonathan Roth for helpful discussions and comments. McCloskey acknowledges funding by the National Science Foundation (Grant SES-2341730).\n    ${ }^{\\dagger}$ School of Economics, University of Bristol, vincent.han@bristol.ac.uk\n    ${ }^{\\ddagger}$ Department of Economics, University of Colorado, adam.mccloskey@colorado.edu\n\nimperfect compliance (e.g., Stoye, 2012; Kallus and Zhou, 2021; Pu and Zhang, 2021; D'Adamo, 2021; Yata, 2021; Han, 2024 to name just a few). Interval identification of average treatment effects (ATEs), average partial effects and welfare is particularly common in these settings due to the endogeneity of individuals' treatment uptake. In order to use the partial identification results for treatment or policy choice in practice, a researcher must typically estimate a set of best-performing treatments or policies from data. Consequently, the researcher is typically interested in a treatment or policy that is either selected from the estimated set of best-performers or arises from a data-dependent selection rule. It is now well-known that selecting an object of interest from data invalidates standard inference tools (e.g., Andrews et al., 2024). The failure of standard inference tools after data-dependent selection is only compounded by the presence of partially-identified parameters.\n\nIn this paper, we develop new inference tools for interval-identified parameters corresponding to selection from either an estimated set or arising from a data-dependent selection rule. Estimating identified sets for the best-performing treatments/policies or forming data-dependent selection rules in these settings is important for choosing which treatments/policies to implement in practice. Therefore, the ability to infer how well these treatments or policies should be expected to perform when selected, for instance to gauge whether their implementation is worthwhile, is of primary practical importance.\n\nThe current literature has not yet developed valid post-selection inference tools in partiallyidentified contexts, an important deficiency that the methods proposed in this paper aim to correct. The methods we propose here build upon the ideas of conditional and hybrid inference employed in various point-identified contexts by, e.g., Lee et al. (2016), Fithian et al. (2017), Tibshirani et al. (2018), Andrews et al. (2024) and McCloskey (2024) to produce confidence intervals (CIs) for interval-identified parameters such as welfare or ATEs chosen from an estimated set or via a data-dependent selection rule. Although Andrews et al. (2023) also propose conditional and hybrid inference methods in the partial identification context of moment inequality models, they do not allow for data-dependent selection of objects of interest, one of the main focuses of the present paper. Finally, this paper directly relies upon results in the literature on interval identification of welfare, treatment effects and partial outcomes such as Manski (1990), Balke and Pearl (1997,\n\n2011), Manski and Pepper (2000), Mogstad et al. (2018), Han and Yang (2024) and Han (2024). We apply our inference methods to a general class of problems nesting these examples.\n\nAfter sketching the ideas behind our inference methods in a simple example, we introduce the general inference framework to which our methods can be applied. We show that our general inference framework incorporates several problems of interest for data-dependent selection and treatment rules for parameters belonging to an identified set such as Manski bounds for average potential outcomes or ATEs, bounds on parameters derived from linear programming (e.g., Balke and Pearl, 1997, 2011; Mogstad et al., 2018; Han, 2024; Han and Yang, 2024), bounds on welfare for treatment allocation rules that are partially identified by observational data (e.g., Stoye, 2012; Kallus and Zhou, 2021; Pu and Zhang, 2021; D\u2019Adamo, 2021; Yata, 2021) and bounds for dynamic treatment effects (e.g., Han, 2024). We also show how to incorporate inference on parameters chosen via asymptotically optimal treatment choice rules (e.g., Christensen et al., 2023) into our general inference framework. Our framework can also be applied to settings where welfare is partially identified for reasons other than treatment endogeneity (e.g., Ishihara and Kitagawa, 2021; Adjaho and Christensen, 2022; Ben-Michael et al., 2021; Cui and Han, 2024).\n\nWithin the general inference framework, we develop three types of CIs for data-dependent and interval-identified parameters. As the name suggests, the conditional CIs are asymptotically valid conditional on the parameter of interest corresponding to a treatment or policy chosen from an estimated set. The construction of this CI does not require a specific rule for choosing the parameter of interest from the estimated set. In addition, the sampling framework underlying its conditional validity is most appropriate in contexts for which a researcher will only be interested in the parameter because it is chosen from the estimated set. Importantly, we show that these CIs are asymptotically valid uniformly across a large class of data-generating processes (DGPs). Uniform asymptotic validity is especially important for approximately correct finite-sample coverage in post-selection contexts like those in this paper (see, e.g., Andrews and Guggenberger, 2009).\n\nThe second and third types of CIs we develop in this paper are designed for inference on parameters chosen by data-dependent selection rules for which the object of interest is uniquely determined by the selection rule. The projection CIs do not require knowledge of the selection rule to be asymp-\n\ntotically valid, whereas the hybrid CI construction utilizes the particular form of a selection rule to improve upon the length properties of the projection CI. The conditional CIs are short for selections that occur with high probability but can become exceptionally long when this probability is small (see, e.g., Kivaranovic and Leeb, 2021). Conversely, projection CIs are overly conservative when selection probabilities are high. Although conditional CIs can be used in this setting, hybrid CIs interpolate the length properties of the conditional and projection CIs in order to attain good length properties regardless of the value selection probabilities take. In analogy with the conditional CIs, we formally show that both projection and hybrid CIs are asymptotically valid in a uniform sense.\n\nWe analyze the coverage and length properties of our proposed CIs in finite samples. Since, to our knowledge, these are the first uniformly valid CIs for data-dependent selections of partially-identified parameters, there are no existing CIs to which we can directly compare. Nevertheless, since our CIs can also be used for inference on a priori chosen interval-identified parameters, we conduct a power comparison with one of the leading methods for inference on a partially-identified parameter. In particular, we compare the power of the test implied by our hybrid CIs to the power of the hybrid test of Andrews et al. (2023), a test that applies to a general class of moment-inequality models that is also based upon a (different) hybrid between conditional and projection-based inference. Encouragingly, the power of the test implied by our hybrid CI is quite competitive even in this environment for which it was not designed. We also find that the finite-sample coverage of all of our CIs is approximately correct in a simple Manski bound example. Finally, we analyze the length tradeoffs between the three different CIs across different DGPs, finding the hybrid CI to perform best overall.\n\nThe remainder of this paper is structured as follows. Section 2 sketches the ideas behind our general CI constructions in the context of a simple Manski bound example. Section 3 lays down the general high-level inference framework we are interested in, while Section 4 details how the general framework applies in several different examples. Section 5 then details the various CI constructions in the general setting. Sections 6 and 7 are devoted to finite-sample comparisons of the properties of the different CIs in the context of a simple Manski bound example. The final section, Section 8, contains an empirical application where we apply our procedures to dynamic policies of schooling and post-school training. Appendix A contains additional examples that fit\n\nour general framework that are not covered in Section 4, while Appendix B contains additional simulation results corresponding to the dynamic treatment regime example detailed in Section 8. Mathematical proofs are relegated to a Technical Appendix at the end of the paper.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 3,
      "text": "# 2 Basic Ideas: Inference with Manski Bounds Example \n\nWe first provide a simple example to illustrate our proposed methods. Consider a binary outcome of interest $Y$, a binary treatment indicator $D$ and a binary treatment assignment $Z$. Furthermore, let $Y(1)$ and $Y(0)$ denote potential outcomes under treatment $(D=1)$ and no treatment $(D=0)$. Assuming $\\mathbb{E}[Y(d) \\mid Z]=\\mathbb{E}[Y(d)]$, Manski (1990) shows that we can bound the average potential outcomes $W(d)=\\mathbb{E}[Y(d)]$ in the absence and presence of treatment as follows:\n\n$$\nL(d) \\equiv \\max \\left\\{p^{1 d 0}, p^{1 d 1}\\right\\} \\leq W(d) \\leq \\min \\left\\{1-p^{0 d 0}, 1-p^{0 d 1}\\right\\} \\equiv U(d)\n$$\n\nfor $d=0,1$ and $p^{y d z} \\equiv \\operatorname{Pr}(Y=y, D=d \\mid Z=z)$. Given (2.1) it is natural to define the set of best-performing options $\\mathcal{D}^{*}$, as a subset of the two options of treatment and no treatment, to be those that are undominated options. From an observed dataset of outcomes, treatments and treatment assignments $\\left\\{\\left(Y_{i}, D_{i}, Z_{i}\\right)\\right\\}_{i=1}^{n}$, such a set can be estimated as:\n\n$$\n\\widehat{\\mathcal{D}}=\\left\\{d \\in\\{0,1\\}: \\widehat{U}(d) \\geq \\widehat{L}\\left(d^{\\prime}\\right) \\forall d^{\\prime} \\in\\{0,1\\} \\text { s.th. } d^{\\prime} \\neq d\\right\\}\n$$\n\nwhere $\\widehat{L}(d) \\equiv \\max \\left\\{\\hat{p}^{1 d 0}, \\hat{p}^{1 d 1}\\right\\}$ and $\\widehat{U}(d) \\equiv \\min \\left\\{1-\\hat{p}^{0 d 0}, 1-\\hat{p}^{0 d 1}\\right\\}$ with $\\hat{p}^{y d z}$ being an empirical estimate of the fitted probability $p^{y d z} \\equiv \\mathbb{P}(Y=y, D=d \\mid Z=z)$.\n\nWe are interested in inference on the identified interval $[L(d), U(d)]$ for the average potential outcome $W(d)$ of option $d \\in\\{0,1\\}$ after the researcher selects this option from $\\widehat{\\mathcal{D}}$. In other words, we would like to provide statistically precise statements about the true average potential outcome of an option selected from the data to give the researcher an idea of how well this selected option should be expected to perform in the population. We first provide some intuition for why standard inference techniques based upon asymptotic normality fail and then sketch our proposals for valid inference in this context.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 4,
      "text": "# 2.1 Why Does Standard Inference Fail? \n\nTo fix ideas, let us focus for now on inference for the lower bound $L(d)$ of a selected option rather than the entire identified set $[L(d), U(d)]$. More specifically, since $L(d)$ is a lower bound for the average potential outcome $W(d)$, we would like to obtain a probabilistic lower bound for $L(d)$. Under standard conditions, a central limit theorem implies $\\hat{p}=\\left(\\hat{p}^{100}, \\hat{p}^{010}, \\hat{p}^{110}, \\hat{p}^{101}, \\hat{p}^{011}, \\hat{p}^{111}\\right)^{\\prime}$ is normally distributed in large samples. So why not form a CI using $\\widehat{L}(d)$ and quantiles from a normal distribution as the basis for inference? There are two reasons such an approach is (asymptotically) invalid:\n\n1. Even in the absence of selection, $\\widehat{L}(d)=\\max \\left\\{\\hat{p}^{1 d 0}, \\hat{p}^{1 d 1}\\right\\}$ is not normally distributed in large samples.\n2. Data-dependent selection of $d$ further complicates the distribution of $\\widehat{L}(d)$.\n\nReason 1. is easy to see since $\\widehat{L}(d)$ is the maximum of two normally distributed random variables in large samples when $d$ is chosen a priori. To better understand reason 2., note that the distribution of $\\widehat{L}(d)$ given $d \\in \\widehat{\\mathcal{D}}$ is the conditional distribution of the maximum of two normally distributed random variables given that the minimum of two other normally distributed random variables, $\\widehat{U}(d) \\equiv \\min \\left\\{1-\\hat{p}^{0 d 0}, 1-\\hat{p}^{0 d 1}\\right\\}$, exceeds the maximum of yet another set of two normally distributed random variables, $\\widehat{L}\\left(d^{\\prime}\\right)=\\max \\left\\{\\hat{p}^{1 d^{\\prime} 0}, \\hat{p}^{1 d^{\\prime} 1}\\right\\}$ for $d^{\\prime} \\neq d$. Unconditionally, $\\widehat{L}(\\hat{d})$ for any data-dependent choice of $\\hat{d}$ is distributed as a mixture of the distributions of $\\widehat{L}(0)$ and $\\widehat{L}(1)$, neither of which are themselves normally distributed.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 5,
      "text": "### 2.2 Conditional Confidence Intervals\n\nSuppose that a researcher's interest in inference on $L(d)$ only arises when $d$ is estimated to be in the set of best-performing options, viz., $d \\in \\widehat{\\mathcal{D}}$. In such a case, we are interested in a probabilistic lower bound for $L(d)$ that is approximately valid across repeated samples for which $d \\in \\widehat{\\mathcal{D}}$, i.e., we would like to form a conditionally valid lower bound $\\widehat{L}(d)_{\\alpha}^{C}$ such that ${ }^{1}$\n\n$$\n\\mathbb{P}\\left(L(d) \\geq \\widehat{L}(d)_{\\alpha}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right) \\geq 1-\\alpha\n$$\n\n[^0]\n[^0]:    ${ }^{1}$ See Andrews et al. (2024) for an extensive discussion of when conditional vs unconditional validity is desirable for inference after selection.\n\nfor some $\\alpha \\in(0,1)$ in large samples. To do so we characterize the conditional distribution of $\\widehat{L}(d)$. Specifically, let $\\hat{j}_{L}(d) \\equiv \\operatorname{argmax}_{j \\in\\{0,1\\}} \\hat{p}^{1 d j}$ be the value of $Z$ at which the maximum between the two estimated probabilities is achieved. Then $\\widehat{L}(d)=\\hat{p}^{1 d \\hat{j}_{L}(d)}$. Also, since the conditioning event $\\left\\{d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}\\right\\}$ can be written as a polyhedron in $\\hat{p}$ and $\\widehat{L}(d)$ is equal to an element of $\\hat{p}$, Lemma 5.1 of Lee et al. (2016) implies\n\n$$\n\\widehat{L}(d) \\mid\\left\\{d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}\\right\\} \\sim \\hat{p}^{1 d \\hat{j}_{L}^{*}} \\mid\\left\\{\\widehat{\\mathcal{L}}_{L}\\left(\\mathcal{Z}_{L}\\right) \\leq \\hat{p}^{1 d \\hat{j}_{L}^{*}} \\leq \\widehat{\\mathcal{U}}_{L}\\left(\\mathcal{Z}_{L}\\right)\\right\\}\n$$\n\nfor some known functions $\\widehat{\\mathcal{L}}_{L}(\\cdot)$ and $\\widehat{\\mathcal{U}}_{L}(\\cdot)$, where $\\hat{p}^{1 d \\hat{j}_{L}^{*}} \\sim \\mathcal{N}\\left(p^{1 d \\hat{j}_{L}^{*}}, \\operatorname{Var}\\left(\\hat{p}^{1 d \\hat{j}_{L}^{*}}\\right)\\right)$ in large samples and $\\mathcal{Z}_{L}$ is a sufficient statistic for the nuisance parameter $p=\\left(p^{100}, p^{010}, p^{110}, p^{101}, p^{011}, p^{111}\\right)^{\\prime}$ that is asymptotically independent of $\\hat{p}^{1 d \\hat{j}_{L}^{*}}$. Using results in Pfanzagl (1994), the characterization in (2.3) permits the straightforward computation of a conditionally quantile-unbiased estimator for $p^{1 d \\hat{j}_{L}(d)}=$ $p^{1 d \\hat{j}_{L}^{*}}$, since the latter is equal to the mean of the underlying normally distributed random variable $\\hat{p}^{1 d \\hat{j}_{L}^{*}}$ that is subject to truncation. Denoting this quantile-unbiased estimator as $\\widehat{L}(d)_{\\alpha}^{C}$, we have\n\n$$\n\\mathbb{P}\\left(p^{1 d \\hat{j}_{L}(d)} \\geq \\widehat{L}(d)_{\\alpha}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)=1-\\alpha\n$$\n\nin large samples. However, noting that $L(d) \\geq p^{1 d \\hat{j}_{L}(d)}$ with probability one, we can see that (2.2) holds for this choice of $\\widehat{L}(d)_{\\alpha}^{C}$.\n\nAlthough (2.2) does not hold with exact equality, we note that the left-hand side cannot be much larger than the right-hand side. In other words, although $\\widehat{L}(d)_{\\alpha}^{C}$ is a conservative probabilistic lower bound for $L(d)$, it is not very conservative. This can be seen heuristically by working through the two possible values that $L(d)$ can take:\n\n1. If $L(d)=p^{1 d \\hat{j}_{L}(d)}$, then (2.2) holds with equality in large samples by (2.4).\n2. If $L(d) \\neq p^{1 d \\hat{j}_{L}(d)}$, then $L(d) \\approx p^{1 d \\hat{j}_{L}(d)}$ since $\\widehat{L}(d)=\\hat{p}^{1 d \\hat{j}_{L}(d)}$ so that the left-hand side of (2.2) cannot be much larger than the right-hand side.\n\nFinally, a construction analogous to that described above for producing a probabilistic lower bound for $L(d)$ produces a conditionally valid probabilistic upper bound $\\widehat{U}(d)_{1-\\alpha}^{C}$ for $U(d)$ that\n\nsatisfies\n\n$$\n\\mathbb{P}\\left(U(d) \\leq \\widehat{U}(d)_{1-\\alpha}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right) \\geq 1-\\alpha\n$$\n\nfor some $\\alpha \\in(0,1)$ in large samples. The probabilistic lower and upper bounds can then be combined to form a CI, $\\left[\\widehat{L}(d)_{\\alpha / 2}^{C}, \\widehat{U}(d)_{1-\\alpha / 2}^{C}\\right]$, that is conditionally valid for $[L(d), U(d)]$ in large samples since\n\n$$\n\\begin{gathered}\n\\mathbb{P}\\left(L(d) \\geq \\widehat{L}(d)_{\\alpha / 2}^{C}, U(d) \\leq \\widehat{U}(d)_{1-\\alpha / 2}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right) \\\\\n\\geq 1-\\mathbb{P}\\left(L(d)<\\widehat{L}(d)_{\\alpha / 2}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\mathbb{P}\\left(U(d)>\\widehat{U}(d)_{1-\\alpha / 2}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right) \\geq 1-\\alpha\n\\end{gathered}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 6,
      "text": "# 2.3 Unconditional Confidence Intervals \n\nSuppose now that the researcher uses a data-dependent rule to select a unique option of inferential interest. For example, suppose the researcher is interested in choosing the option with the highest potential outcome in the worst case across its identified set so that she chooses $\\hat{d}=\\operatorname{argmax}_{d \\in\\{0,1\\}} \\widehat{L}(d)$. In such a case, it is natural to form a probabilistic lower bound $\\widehat{L}(\\hat{d})_{\\alpha}^{U}$ for $L(\\hat{d})$ that is unconditionally valid across repeated samples such that\n\n$$\n\\mathbb{P}\\left(L(\\hat{d}) \\geq \\widehat{L}(\\hat{d})_{\\alpha}^{U}\\right) \\geq 1-\\alpha\n$$\n\nfor some $\\alpha \\in(0,1)$ in large samples. Given its conditional validity (2.2), the conditional lower bound $\\widehat{L}(d)_{\\alpha}^{C}$ also satisfies (2.7) upon changing the definition of $\\widehat{\\mathcal{D}}$ to $\\widehat{\\mathcal{D}}=\\{\\hat{d}\\}$ in its construction. However, it is well known in the literature on selective inference that conditionally-valid probabilistic bounds can be very uninformative (i.e., far below the true value) when the probability of the conditioning event is small (see e.g., Kivaranovic and Leeb, 2021, Andrews et al., 2024 and McCloskey, 2024). Here, we propose two additional forms of probabilistic bounds that are only unconditionally valid but do not suffer from this drawback.\n\nFirst, we can form a probabilistic lower bound for $L(\\hat{d})$ by projecting a one-sided rectangular simultaneous confidence lower bound for all possible values $L(\\hat{d})$ can take: $\\widehat{L}(\\hat{d})_{\\alpha}^{P} \\equiv$ $\\widehat{L}(\\hat{d})-\\hat{c}_{1-\\alpha, L} \\sqrt{\\widehat{\\Sigma}_{L, 2 \\hat{d}+1+\\hat{j}_{L}(\\hat{d})}}$, where $\\hat{c}_{1-\\alpha, L}$ is the $1-\\alpha$ quantile of $\\max _{i} \\hat{\\zeta}_{i} / \\sqrt{\\widehat{\\Sigma}_{L, i}}$ for $\\hat{\\zeta} \\sim \\mathcal{N}\\left(0, \\widehat{\\Sigma}_{L}\\right)$,\n\n$\\widehat{\\Sigma}_{L}$ is a consistent estimator of $\\Sigma_{L} \\equiv \\operatorname{Var}\\left(\\hat{p}^{100} \\cdot \\hat{p}^{101} \\cdot \\hat{p}^{110} \\cdot \\hat{p}^{111}\\right)$ and $\\Upsilon_{i}$ denotes the $i^{\\text {th }}$ element of the main diagonal of any square matrix $\\Upsilon$. Here, the maximum is taken to guarantee simultaneous coverage of all possible values of $L(\\hat{d})$. Since $p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\in\\left\\{\\hat{p}^{100} \\cdot \\hat{p}^{101} \\cdot \\hat{p}^{110} \\cdot \\hat{p}^{111}\\right\\}$ with probability one,\n\n$$\n\\begin{gathered}\n\\mathbb{P}\\left(p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\alpha}^{P}\\right)=\\mathbb{P}\\left(p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\geq \\hat{p}^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})}-\\hat{c}_{1-\\alpha, L} \\sqrt{\\widehat{\\Sigma}_{L, 2 \\hat{d}+1+\\hat{j}_{L}(\\hat{d})}}\\right) \\\\\n\\geq \\mathbb{P}\\left(\\left(p^{100} \\cdot p^{101} \\cdot p^{110} \\cdot p^{111}\\right) \\geq\\left(\\hat{p}^{100} \\cdot \\hat{p}^{101} \\cdot \\hat{p}^{110} \\cdot \\hat{p}^{111}\\right)-\\hat{c}_{1-\\alpha, L} \\sqrt{\\operatorname{Diag}\\left(\\widehat{\\Sigma}_{L}\\right)}\\right)=1-\\alpha\n\\end{gathered}\n$$\n\nin large samples and (2.7) holds for $\\widehat{L}(\\hat{d})_{\\alpha}^{U}=\\widehat{L}(\\hat{d})_{\\alpha}^{P}$ because $L(\\hat{d}) \\geq p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})}$. However, $\\widehat{L}(\\hat{d})_{\\alpha}^{P}$ suffers from a converse drawback to that of $\\widehat{L}(\\hat{d})_{\\alpha}^{C}$ : it is unnecessarily conservative when $\\hat{d}=d$ is chosen with high probability (see e.g., Andrews et al., 2024 and McCloskey, 2024).\n\nWe propose a second probabilistic lower bound for $L(\\hat{d})$ that combines the complementary strengths of $\\widehat{L}(d)_{\\alpha}^{C}$ and $\\widehat{L}(\\hat{d})_{\\alpha}^{P}$. Construction of this hybrid lower bound $\\widehat{L}(\\hat{d})_{\\alpha}^{H}$ proceeds analogously to the construction of $\\widehat{L}(d)_{\\alpha}^{C}$ after adding the additional condition $\\left\\{p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\beta}^{P}\\right\\}$ for $\\beta<\\alpha$ to the conditioning event and instead computing a conditionally quantile-unbiased estimator for $p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})}$, denoted as $\\widehat{L}(\\hat{d})_{\\alpha}^{H}$, satisfying\n\n$$\n\\mathbb{P}\\left(p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\alpha}^{H} \\mid \\hat{d}=d^{*} \\cdot p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\beta}^{P}\\right)=\\frac{1-\\alpha}{1-\\beta}\n$$\n\nin large samples, where $d^{*}$ is any realized value of the random variable $\\hat{d}$. Imposing this additional condition in the formation of the hybrid bound ensures that $\\widehat{L}(\\hat{d})_{\\alpha}^{H}$ is always greater than $\\widehat{L}(\\hat{d})_{\\beta}^{P}$, limiting its worst-case performance relative to $L(\\hat{d})_{\\beta}^{P}$ when $\\mathbb{P}\\left(\\hat{d}=d^{*}\\right)$ is small. On the other hand, when $\\mathbb{P}\\left(\\hat{d}=d^{*}\\right)$ is large, the additional condition $\\left\\{p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\beta}^{P}\\right\\}$ is far from binding with high probability so that $\\widehat{L}(\\hat{d})_{\\alpha}^{H}$ becomes very close to $\\widehat{L}(d)_{(\\alpha-\\beta) /(1-\\beta)}^{C}$. In this case, $\\widehat{L}(d)_{(\\alpha-\\beta) /(1-\\beta)}^{C}$ is close to the naive lower bound based upon the normal distribution $\\widehat{L}(\\hat{d})-z_{(1-\\alpha) /(1-\\beta)} \\sqrt{\\operatorname{Var}\\left(\\hat{p}^{1 \\hat{d} \\hat{j}_{L}}\\right)}$ because the truncation bounds in (2.3) are very wide (Proposition 3 in Andrews et al., 2024).\n\nTo see how (2.7) holds for $\\widehat{L}(\\hat{d})_{\\alpha}^{U}=\\widehat{L}(\\hat{d})_{\\alpha}^{H}$, note first that\n\n$$\n\\mathbb{P}\\left(L(\\hat{d}) \\geq \\widehat{L}(\\hat{d})_{\\alpha}^{H} \\mid \\hat{d}=d^{*} \\cdot p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\beta}^{P}\\right) \\geq \\mathbb{P}\\left(p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\alpha}^{H} \\mid \\hat{d}=d^{*} \\cdot p^{1 \\hat{d} \\hat{j}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\beta}^{P}\\right)=\\frac{1-\\alpha}{1-\\beta}\n$$\n\nfor all $d^{*} \\in\\{0,1\\}$. Then, note that\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left(L(\\hat{d}) \\geq \\widehat{L}(\\hat{d})_{\\alpha}^{H}\\right) \\geq \\mathbb{P}\\left(L(\\hat{d})\\right. & \\left.\\geq \\widehat{L}(\\hat{d})_{\\alpha}^{H} \\mid p^{1 \\hat{d} \\hat{d}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\beta}^{P}\\right) \\cdot \\mathbb{P}\\left(p^{1 \\hat{d} \\hat{d}_{L}(\\hat{d})} \\geq \\widehat{L}(\\hat{d})_{\\beta}^{P}\\right) \\\\\n& \\geq \\frac{1-\\alpha}{1-\\beta}(1-\\beta)=1-\\alpha\n\\end{aligned}\n$$\n\nby the law of total probability.\nBy similar reasoning to that used for the conditional CIs in Section 2.2 above, $\\widehat{L}(\\hat{d})_{\\alpha}^{H}$ is not very conservative as a probabilistic lower bound for $L(\\hat{d})$. The researcher's choice of $\\beta \\in(0, \\alpha)$ trades off the performance of $\\widehat{L}(\\hat{d})_{\\alpha}^{H}$ across scenarios for which $\\mathbb{P}\\left(\\hat{d}=d^{*}\\right)$ is large and small with a small $\\beta$ corresponding to better performance when $\\mathbb{P}\\left(\\hat{d}=d^{*}\\right)$ is large. See McCloskey (2024) for an in-depth discussion of these tradeoffs. We recommend $\\beta=\\alpha / 10$.\n\nFinally, analogous constructions to those above produce unconditional projection and hybrid probabilistic upper bounds $\\widehat{U}(\\hat{d})_{1-\\alpha}^{P}$ and $\\widehat{U}(\\hat{d})_{1-\\alpha}^{H}$ that can then be combined with the lower bounds to form CIs $\\left[\\widehat{L}(d)_{\\alpha / 2}^{P}, \\widehat{U}(d)_{1-\\alpha / 2}^{P}\\right]$ and $\\left[\\widehat{L}(d)_{\\alpha / 2}^{H}, \\widehat{U}(d)_{1-\\alpha / 2}^{H}\\right]$ for $[L(d), U(d)]$ that are unconditionally valid in large samples by the same arguments as those used in (2.2) above.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 7,
      "text": "# 3 General Inference Framework \n\nWe now introduce the general inference framework that we propose, nesting the Manski bound example of the previous section as a special case. After introducing the general framework, we describe several additional example applications that fall within this framework.\n\nWe are interested in performing inference on a parameter $W(d)$ that is indexed by a finite set $d \\in \\mathcal{D} \\equiv\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$ for some $K>0$. The index $d$ may correspond to a particular treatment, treatment allocation rule or policy, depending upon the application. We assume that $W(d)$ belongs to an identified set taking a particular interval form that is common to many applications of interest.\n\nAssumption 3.1. For all $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$ and an unknown finite-dimensional parameter $p$,\n\n1. $L(d) \\equiv \\max _{j \\in\\left\\{1, \\ldots, J_{L}\\right\\}}\\left\\{\\tilde{\\ell}_{d, j}+\\ell_{d, j} p\\right\\} \\leq W(d)$ for some fixed and known $J_{L}, \\tilde{\\ell}_{d, 1}, \\ldots, \\tilde{\\ell}_{d, J_{L}}$ and nonzero row vectors $\\ell_{d, 1}, \\ldots, \\ell_{d, J_{L}}$ such that $\\ell_{d, j} \\neq \\ell_{d, j^{\\prime}}$ for $j \\neq j^{\\prime}$.\n\n2. $U(d) \\equiv \\min _{j \\in\\left\\{1, \\ldots, J_{U}\\right\\}}\\left\\{\\tilde{u}_{d, j}+u_{d, j} p\\right\\} \\geq W(d)$ for some fixed and known $J_{U}, \\tilde{u}_{d, 1}, \\ldots, \\tilde{u}_{d, J_{U}}$ and nonzero row vectors $u_{d, 1}, \\ldots, u_{d, J_{U}}$ such that $u_{d, j} \\neq u_{d, j^{\\prime}}$ for $j \\neq j^{\\prime}$.\n\nThe lower and upper endpoints of identified sets for the welfare, average potential outcome or ATE typically take the form of $L(d)$ and $U(d)$, especially when (sequences of) outcomes, treatments and instruments are discrete.\n\nIn the setting of this paper, a researcher's interest in $W(d)$ arises when $d$ belongs to a set $\\tilde{\\mathcal{D}} \\subset \\mathcal{D}$ that is estimated from a sample of $n$ observations. It is often the case that $\\tilde{\\mathcal{D}}$ is an estimate of the identified set of best performers $\\mathcal{D}^{*}$. This set could correspond to an estimated set of optimal treatments or policies or other data-dependent index sets of interest. The estimated set is determined by an estimator $\\hat{p}$ of the finite-dimensional parameter $p$ that determines the bounds on $W(d)$ according to Assumption 3.1. Let\n\n$$\n\\hat{j}_{L}(d) \\equiv \\underset{j \\in\\left\\{1, \\ldots, J_{L}\\right\\}}{\\operatorname{argmax}}\\left\\{\\tilde{\\ell}_{d, j}+\\ell_{d, j} \\hat{p}\\right\\}, \\quad \\hat{j}_{U}(d) \\equiv \\operatorname{argmin}_{j \\in\\left\\{1, \\ldots, J_{U}\\right\\}}\\left\\{\\tilde{u}_{d, j}+u_{d, j} \\hat{p}\\right\\}\n$$\n\nwhich are the indices at which the estimated lower and upper bounds are realized. Then, the estimated lower and upper bounds for $W(d)$ are equal to $\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} \\hat{p}$ and $\\tilde{u}_{d, \\hat{j}_{U}(d)}+u_{d, \\hat{j}_{U}(d)} \\hat{p}$. We work under the high-level assumption that the following event can be written as a polyhedron in $\\hat{p}$ : (i) an option index $d$ is in the set of interest $\\tilde{\\mathcal{D}}$, (ii) the estimated bounds on $W(d)$ are realized at a given value and (iii) (optionally) an additional random vector is realized at any given value.\n\nAssumption 3.2. 1. For some fixed and known matrix $A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)$, some fixed and known vector $c^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)$ and some finite-valued random vector $\\hat{\\gamma}_{L}(d)$, the event $\\left\\{d \\in \\tilde{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}\\right.$ and $\\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\}$ is equivalent to $\\left\\{A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right) \\hat{p} \\leq c^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)\\right\\}$, where $j_{L}^{*} \\in\\left\\{1, \\ldots, J_{L}\\right\\}$ and $\\gamma_{L}^{*}$ is in the support of $\\hat{\\gamma}_{L}(d)$.\n2. For some fixed and known matrix $A^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right)$, some fixed and known vector $c^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right)$ and some finite-valued random vector $\\hat{\\gamma}_{U}(d)$, the event $\\left\\{d \\in \\tilde{\\mathcal{D}}, \\hat{j}_{U}(d)=j_{U}^{*}\\right.$ and $\\left.\\hat{\\gamma}_{U}(d)=\\gamma_{U}^{*}\\right\\}$ is equivalent to $\\left\\{A^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right) \\hat{p} \\leq c^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right)\\right\\}$, where $j_{U}^{*} \\in\\left\\{1, \\ldots, J_{U}\\right\\}$ and $\\gamma_{U}^{*}$ is in the support of $\\hat{\\gamma}_{U}(d)$.\n\nDepending upon the application, $\\hat{\\gamma}_{L}(d)$ and $\\hat{\\gamma}_{U}(d)$ (and thus $\\gamma_{L}^{*}$ and $\\gamma_{U}^{*}$ ) in this assumption may\n\nnot be necessary to condition on, in which case they can be vacuously set to constants. Although not immediately obvious, this assumption holds in a variety of settings; see the examples below. In many cases, this assumption can be simplified because, consistent with Assumption 3.1, $d \\in \\widehat{\\mathcal{D}}$ if and only if $A_{\\mathcal{D}} \\hat{p} \\leq c_{\\mathcal{D}}$ for some fixed and known matrix $A_{\\mathcal{D}}$ and vector $c_{\\mathcal{D}}$. For these cases, $\\hat{\\gamma}_{L}(d)$ and $\\hat{\\gamma}_{U}(d)$ are not needed and can be vacuously set to fixed constants and\n\n$$\nA^{L}(d, j, \\gamma)=\\left(\\begin{array}{c}\n\\ell_{d, 1}-\\ell_{d, j} \\\\\n\\vdots \\\\\n\\ell_{d, J_{L}}-\\ell_{d, j} \\\\\nA_{\\mathcal{D}}\n\\end{array}\\right), \\quad A^{U}(d, j, \\gamma)=\\left(\\begin{array}{c}\nu_{d, j}-u_{d, 1} \\\\\n\\vdots \\\\\nu_{d, j}-u_{d, J_{L}} \\\\\nA_{\\mathcal{D}}\n\\end{array}\\right)\n$$\n\nand\n\n$$\nc^{L}(d, j, \\gamma)=\\left(\\begin{array}{c}\n\\tilde{\\ell}_{d, j}-\\tilde{\\ell}_{d, 1} \\\\\n\\vdots \\\\\n\\tilde{\\ell}_{d, j}-\\tilde{\\ell}_{d, J_{L}} \\\\\nc_{\\mathcal{D}}\n\\end{array}\\right), \\quad c^{U}(d, j, \\gamma)=\\left(\\begin{array}{c}\n\\tilde{u}_{d, 1}-\\tilde{u}_{d, j} \\\\\n\\vdots \\\\\n\\tilde{u}_{d, J_{L}}-\\tilde{u}_{d, j} \\\\\nc_{\\mathcal{D}}\n\\end{array}\\right)\n$$\n\nA leading example of this special case is\n\n$$\n\\widehat{\\mathcal{D}}=\\left\\{d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}: \\widehat{U}(d) \\geq \\max _{d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}} \\widehat{L}(d)\\right\\}\n$$\n\nwhere $\\widehat{L}(d) \\equiv \\max _{j \\in\\left\\{1, \\ldots, J_{L}\\right\\}}\\left\\{\\tilde{\\ell}_{d, j}+\\ell_{d, j} \\hat{p}\\right\\}$ and $\\widehat{U}(d) \\equiv \\min _{j \\in\\left\\{1, \\ldots, J_{U}\\right\\}}\\left\\{\\tilde{u}_{d, j}+u_{d, j} \\hat{p}\\right\\}$, since $d \\in \\widehat{\\mathcal{D}}$ if and only if\n\n$$\n\\left(\\ell_{d^{\\prime}, j^{\\prime}}-u_{d, j}\\right) \\hat{p} \\leq \\tilde{u}_{d, j}-\\tilde{\\ell}_{d^{\\prime}, j^{\\prime}}\n$$\n\nfor all $d^{\\prime} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}, j \\in\\left\\{1, \\ldots, J_{U}\\right\\}$ and $j^{\\prime} \\in\\left\\{1, \\ldots, J_{L}\\right\\}$.\nWe also note that Assumption 3.2 is compatible with the absence of data-dependent selection for which the researcher is interested in forming a CI for an identified interval $\\left[L\\left(d^{*}\\right), U\\left(d^{*}\\right)\\right]$ chosen by the researcher a priori. In these cases, $\\widehat{\\mathcal{D}}=\\left\\{d^{*}\\right\\}, \\hat{\\gamma}_{M}\\left(d^{*}\\right)$ can be vacuously set to a fixed constant,\n\n$A^{M}\\left(d^{*}, j, \\gamma\\right)=A_{M}\\left(d^{*}, j\\right)$ and $c^{M}\\left(d^{*}, j, \\gamma\\right)=c_{M}\\left(d^{*}, j\\right)$ for $M=L, U$. Indeed, we examine an example of this special case when conducting a finite-sample power comparison in Section 6 below.\n\nIn general, less conditioning is more desirable in terms of the lengths of the CIs we propose. Although conditioning on the events $d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}$ and $\\hat{j}_{U}(d)=j_{U}^{*}$ is necessary to construct our CIs (see Section 5.1 below), the researcher should therefore minimize the number of elements in $\\hat{\\gamma}_{L}(d)$ and $\\hat{\\gamma}_{U}(d)$ subject to satisfying Assumption 3.2 when constructing our CIs. In some cases it is necessary to condition on these additional random vectors in order to satisfy Assumption 3.2. But in many cases, such as the example given immediately above, additional conditioning random vectors are unnecessary and can be vacuously set to fixed constants.\n\nWe impose the following assumption for our unconditional hybrid CIs in order for the object of inferential interest to be well-defined unconditionally.\n\nAssumption 3.3. $\\widehat{\\mathcal{D}}=\\{\\hat{d}\\}$ almost surely for a random variable $\\hat{d}$ with support $\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$.\n\nIn conjunction, Assumptions 3.2 and 3.3 hold naturally when the object of interest $\\hat{d}$ is selected by uniquely maximizing a linear combination of the estimates of the bounds characterizing the identified intervals and the additional conditioning vectors $\\hat{\\gamma}_{L}(d)$ and $\\hat{\\gamma}_{U}(d)$ are defined appropriately. Leading examples of this form of selection include when $\\hat{d}$ corresponds to the largest estimated lower bound, upper bound or weighted average of lower and upper bounds.\n\nProposition 3.1. Suppose $\\widehat{\\mathcal{D}}=\\{\\hat{d}\\}$, where $\\hat{d}=\\operatorname{argmax}_{d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}}\\left\\{w_{L} \\widehat{L}(d)+w_{U} \\widehat{U}(d)\\right\\}$ is unique almost surely for some fixed known weights $w_{L}, w_{U} \\geq 0$. Then Assumptions 3.2 and 3.3 are satisfied for\n\n1. $\\hat{\\gamma}_{L}(d)$ equal to any fixed constant and $\\hat{\\gamma}_{U}(d)=\\hat{j}_{L}(d)$ when $w_{U}=0$,\n2. $\\hat{\\gamma}_{L}(d)=\\hat{\\gamma}_{U}(d)=\\left(\\hat{j}_{U}(0), \\ldots, \\hat{j}_{U}(T)\\right)^{\\prime}$ when $w_{L}=0$,\n3. $\\hat{\\gamma}_{L}(d)=\\hat{\\gamma}_{U}(d)=\\left(\\hat{j}_{L}(0), \\ldots, \\hat{j}_{L}(T), \\hat{j}_{U}(0), \\ldots, \\hat{j}_{U}(T)\\right)^{\\prime}$ when $w_{L}, w_{U} \\neq 0$.\n\nExpressions for $A^{M}\\left(d, j_{M}^{*}, \\gamma_{M}^{*}\\right)$ and $c^{M}\\left(d, j_{M}^{*}, \\gamma_{M}^{*}\\right)$ for $M=L, U$ in the settings of Proposition 3.1 are available for reference in its proof in Appendix C. As this proposition makes clear, the additional conditioning vectors needed for Assumption 3.3 to hold depend upon the particular form of selection\n\nrule used by the researcher. For example, when $\\hat{d}$ is chosen to maximize the estimated lower bound of the identified set $\\widehat{L}(d)$, one must condition not only on the realized value of $\\hat{j}_{U}(\\hat{d})$ when forming a probabilistic upper bound for $U(\\hat{d})$ but also $\\hat{j}_{L}(\\hat{d})$. On the other hand, the formation of either a probabilistic lower bound for $L(\\hat{d})$ or upper bound for $U(\\hat{d})$ when $\\hat{d}$ is chosen to maximize the estimated upper bound of the identified set $\\widehat{U}(d)$ requires conditioning on the entire vector $\\left(\\hat{j}_{U}(0), \\ldots, \\hat{j}_{U}(T)\\right)^{\\prime}$.\n\nAlthough intuitively appealing, the treatment choice rules of the form described in Proposition 3.1 can be sub-optimal from a statistical decision-theoretic point of view (see, e.g., Manski, 2021, 2023 and Christensen et al., 2023). In Section 4.4, we show how proper definition of $\\hat{\\gamma}_{L}(d)$ and $\\hat{\\gamma}_{U}(d)$ satisfies Assumptions 3.2 and 3.3 in the context of the optimal selection rules of Christensen et al. (2023).\n\nWe suppose that the sample of data is drawn from some unknown distribution $\\mathbb{P} \\in \\mathcal{P}_{n}$. As an estimator for $p$, we assume that $\\hat{p}$ is uniformly asymptotically normal under $\\mathbb{P} \\in \\mathcal{P}_{n}$.\n\nAssumption 3.4. For the class of Lipschitz functions that are bounded in absolute value by one and have Lipschitz constant bounded by one, $B L_{1}$, there exist functions $p(\\mathbb{P})$ and $\\Sigma(\\mathbb{P})$ such that for $\\xi_{\\mathbb{P}} \\sim \\mathcal{N}(0, \\Sigma(\\mathbb{P}))$ with\n\n$$\n\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\sup _{f \\in B L_{1}}\\left|E_{\\mathbb{P}}[f(\\sqrt{n}(\\hat{p}-p(\\mathbb{P})))]-E_{\\mathbb{P}}\\left[f\\left(\\xi_{\\mathbb{P}}\\right)\\right]\\right|=0\n$$\n\nThe notation of this assumption makes explicit that the parameter $p$ and the asymptotic variance $\\Sigma$ depend upon the unknown distribution of the data $\\mathbb{P}$. It holds naturally for standard estimators $\\hat{p}$ under random sampling or weak dependence in the presence of bounds on the moments and dependence of the underlying data.\n\nNext, we assume that the asymptotic variance of $\\hat{p}$ can be uniformly consistently estimated by an estimator $\\widehat{\\Sigma}$.\n\nAssumption 3.5. For all $\\varepsilon>0$, the estimator $\\widehat{\\Sigma}$ satisfies\n\n$$\n\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\mathbb{P}\\left(\\|\\widehat{\\Sigma}-\\Sigma(\\mathbb{P})\\|>\\varepsilon\\right)=0\n$$\n\nThis assumption is again naturally satisfied when using a standard sample analog estimator of\n\n$\\Sigma$ under random sampling or weak dependence in the presence of moment and dependence bounds.\nIn addition, we restrict the asymptotic variance of $\\hat{p}$ to be positive definite.\n\nAssumption 3.6. For some finite $\\bar{\\lambda}>0,1 / \\bar{\\lambda} \\leq \\lambda_{\\min }(\\Sigma(\\mathbb{P})) \\leq \\lambda_{\\max }(\\Sigma(\\mathbb{P})) \\leq \\bar{\\lambda}$ for all $\\mathbb{P} \\in \\mathcal{P}_{n}$.\nThis assumption is naturally satisfied, for example, when $\\hat{p}$ is a standard sample analog estimator of reduced-form probabilities composing $p$ that are non-redundant and bounded away from zero and one.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 8,
      "text": "# 4 Examples \n\nIn this section, we show that the proposed inference method is applicable to various examples for which parameters are interval-identified. In particular, we show that Assumptions 3.1, 3.2 and 3.4 are satisfied in these examples. See Appendix A for additional examples.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 9,
      "text": "### 4.1 Bounds Derived from Linear Programming\n\nIn more complex settings, calculating analytical bounds on $W(d)$ or $W(d)-W(\\tilde{d})$ may be cumbersome. This is especially true when the researcher wants to incorporate additional identifying assumptions. In this situation, the computational approach using linear programming can be useful (Mogstad et al., 2018; Han and Yang, 2024).\n\nTo incorporate many complicated settings, suppose that $W(d)=A_{d} q$ and $p=B q$ for some known row vector $A_{d}$ and matrix $B$, an unknown vector $q$ in a simplex $\\mathcal{Q}$, and a vector $p$ that is estimable from data. Typically $q$ is a vector of probabilities of a latent variable that governs the DGP; see Balke and Pearl (1997, 2011), Han (2024) and Han and Yang (2024). The linearity in this assumption is usually implied by the nature of a particular problem (e.g., discreteness). Then we have\n\n$$\n\\begin{aligned}\n& L(d)=\\min _{q \\in \\mathcal{Q}} A_{d} q, \\\\\n& U(d)=\\max _{q \\in \\mathcal{Q}} A_{d} q,\n\\end{aligned} \\quad \\text { s.t. } \\quad B q=p\n$$\n\nand ATE bounds for a change from treatment $d$ to treatment $\\tilde{d}$\n\n$$\n\\begin{aligned}\n& L(\\tilde{d}, d)=\\min _{q \\in \\mathcal{Q}}\\left(A_{\\tilde{d}}-A_{d}\\right) q, \\\\\n& U(\\tilde{d}, d)=\\max _{q \\in \\mathcal{Q}}\\left(A_{\\tilde{d}}-A_{d}\\right) q,\n\\end{aligned} \\quad \\text { s.t. } \\quad B q=p\n$$\n\nNote that $L(\\tilde{d}, d) \\neq L(\\tilde{d})-U(d)$ in general because the $q$ that solves (4.1) for $L(\\tilde{d})$ and $U(d)$ may be different (and similarly for $U(\\tilde{d}, d)$ ). As before, the identified set of optimal treatments here is characterized as $\\mathcal{D}^{*} \\equiv\\{d: L(\\tilde{d}, d) \\leq 0, \\forall \\tilde{d} \\neq d\\}$.\n\nAn example of this setting can be found in Han and Yang (2024). Let $(Y, D, Z)$ be a vector of a binary outcome, treatment and instrument and let $p$ be a vector with entries $p(y, d \\mid z) \\equiv \\mathbb{P}(Y=$ $y, D=d \\mid Z=z)$ across $(y, d, z) \\in\\{0,1\\}^{3,2}$ Suppose $W(d)=\\mathbb{E}[Y(d)]$ for $d \\in\\{0,1\\}$. Then, we can define the response type $\\varepsilon \\equiv(Y(1), Y(0), D(1), D(0))$ with a realized value $e \\equiv(y(1), y(0), d(1), d(0))$, where $Y(d)$ denotes the potential outcome under treatment $d$ and $D(z)$ denotes the potential treatment under instrument value $z$. Let $q(e) \\equiv \\mathbb{P}(\\varepsilon=e)$ be the latent distribution. Then\n\n$$\nW(d)=\\mathbb{P}[Y(d)=1]=\\sum_{e: y(d)=1} q(e) \\equiv A_{d} q\n$$\n\nwhere $q$ is the vector of $q(e)$ 's and $A_{d}$ is an appropriate selector (a row vector).\nAssume that $(Y(d), D(z))$ is independent of $Z$ for $d, z \\in\\{0,1\\}$. The data distribution $p$ is related to the latent distribution by\n\n$$\n\\mathbb{P}[Y=1, D=d \\mid Z=z]=\\mathbb{P}[Y(d)=1, D(z)=d]=\\sum_{e: y(d)=1, d(z)=d} q(e) \\equiv B_{d, z} q\n$$\n\nwhere the first equality follows by the independence assumption, $q$ is a vector of $q(e)$ 's and $B_{d, z}$\n\n[^0]\n[^0]:    ${ }^{2}$ See Han and Yang (2024) for the use of linear programming with continuous $Y$.\n\nis an appropriate selector (a row vector). Now define\n\n$$\nB \\equiv\\left[\\begin{array}{c}\nB_{1,1} \\\\\nB_{0,1} \\\\\nB_{1,0} \\\\\n\\vdots\n\\end{array}\\right], \\quad p \\equiv\\left[\\begin{array}{c}\np(1,1 \\mid 1) \\\\\np(1,0 \\mid 1) \\\\\np(1,1 \\mid 0) \\\\\np(1,0 \\mid 0) \\\\\n\\vdots\n\\end{array}\\right]\n$$\n\nso that all of the constraints relating the data distribution to the latent distribution can be expressed as $B q=p$.\n\nTo verify Assumption 3.1, it is helpful to invoke strong duality for the primal problems (4.1) (under regularity conditions) and write the following dual problems:\n\n$$\n\\begin{aligned}\n& L(d)=\\max _{\\lambda}-\\tilde{p}^{\\prime} \\lambda, \\quad \\text { s.t. } \\quad \\tilde{B}^{\\prime} \\lambda \\geq-A_{d}^{\\prime} \\\\\n& U(d)=\\min _{\\lambda} \\tilde{p}^{\\prime} \\lambda, \\quad \\text { s.t. } \\quad \\tilde{B}^{\\prime} \\lambda \\geq A_{d}^{\\prime}\n\\end{aligned}\n$$\n\nwhere $\\tilde{B} \\equiv\\left[\\begin{array}{c}B \\\\ \\mathbf{1}^{\\prime}\\end{array}\\right]$ is a $\\left(d_{p}+1\\right) \\times d_{q}$ matrix with $\\mathbf{1}$ being a $d_{q} \\times 1$ vector of ones, and $\\tilde{p} \\equiv\\left[\\begin{array}{c}p \\\\ 1\\end{array}\\right]$ is a $\\left(d_{p}+1\\right) \\times 1$ vector. By using a vertex enumeration algorithm (e.g., Avis and Fukuda (1991)), one can find all (or a relevant subset) of vertices of the polyhedra $\\left\\{\\lambda: \\tilde{B}^{\\prime} \\lambda \\geq-A_{d}^{\\prime}\\right\\}$ and $\\left\\{\\lambda: \\tilde{B}^{\\prime} \\lambda \\geq A_{d}^{\\prime}\\right\\}$. Let $\\Lambda_{L, d} \\equiv\\left\\{\\lambda_{1}, \\ldots, \\lambda_{J_{L, d}}\\right\\}$ and $\\Lambda_{U, d} \\equiv\\left\\{\\lambda_{1}, \\ldots, \\lambda_{J_{U, d}}\\right\\}$ be the sets that collect such vertices, respectively. Then, it is easy to see that $L(d)=\\max _{\\lambda \\in \\Lambda_{L, d}}-\\tilde{p}^{\\prime} \\lambda$ and $U(d)=\\min _{\\lambda \\in \\Lambda_{U, d}} \\tilde{p}^{\\prime} \\lambda$, and thus Assumption 3.1 holds.\n\nTo verify Assumption 3.2, we use the dual problems to (4.2):\n\n$$\n\\begin{aligned}\n& L(\\tilde{d}, d)=\\max _{\\lambda}-\\tilde{p}^{\\prime} \\lambda, \\quad \\text { s.t. } \\quad \\tilde{B}^{\\prime} \\lambda \\geq-\\Delta_{\\tilde{d}, d}^{\\prime} \\\\\n& U(\\tilde{d}, d)=\\min _{\\lambda} \\tilde{p}^{\\prime} \\lambda, \\quad \\text { s.t. } \\quad \\tilde{B}^{\\prime} \\lambda \\geq \\Delta_{\\tilde{d}, d}^{\\prime}\n\\end{aligned}\n$$\n\nwhere $\\Delta_{\\tilde{d}, d} \\equiv A_{\\tilde{d}}-A_{d}$. Analogous to the vertex enumeration argument above, let $\\Lambda_{L, \\tilde{d}, d} \\equiv$ $\\left\\{\\lambda_{1}, \\ldots, \\lambda_{J_{L, \\tilde{d}, d}}\\right\\}$ and $\\Lambda_{U, \\tilde{d}, d} \\equiv\\left\\{\\lambda_{1}, \\ldots, \\lambda_{J_{U, \\tilde{d}, d}}\\right\\}$ be the sets that collect all (or a relevant subset)\n\nof vertices of the polyhedra $\\left\\{\\lambda: \\tilde{B}^{\\prime} \\lambda \\geq-\\Delta_{\\tilde{d}, d}^{\\prime}\\right\\}$ and $\\left\\{\\lambda: \\tilde{B}^{\\prime} \\lambda \\geq \\Delta_{\\tilde{d}, d}^{\\prime}\\right\\}$, respectively. Then, $L(\\tilde{d}, d)=\\max _{\\lambda \\in \\Lambda_{L, \\tilde{d}, d}}-\\tilde{p}^{\\prime} \\lambda$ and $U(\\tilde{d}, d)=\\min _{\\lambda \\in \\Lambda_{U, \\tilde{d}, d}} \\tilde{p}^{\\prime} \\lambda$. Let $\\widehat{\\mathcal{D}}=\\{d: \\widehat{L}(\\tilde{d}, d) \\leq 0, \\forall \\tilde{d} \\neq d\\}$, where $\\widehat{L}(\\tilde{d}, d)$ is the sample counterpart of $L(\\tilde{d}, d)$ with $\\widehat{\\tilde{p}} \\equiv\\left[\\begin{array}{c}\\hat{p} \\\\ 1\\end{array}\\right]$ replacing $\\tilde{p} \\equiv\\left[\\begin{array}{c}p \\\\ 1\\end{array}\\right]$. Partition $\\lambda$ as $\\lambda=\\left(\\lambda^{1 \\prime}, \\lambda^{0}\\right)^{\\prime}$ where $\\lambda^{0}$ is the last element of $\\lambda$. Note that $d \\in \\widehat{\\mathcal{D}}$ if and only if\n\n$$\n\\max _{\\lambda \\in \\hat{\\Lambda}_{L, d}}-\\left(\\tilde{p}^{\\prime} \\lambda^{1}+\\lambda^{0}\\right) \\leq 0\n$$\n\nwhere $\\hat{\\Lambda}_{L, d}=\\bigcup_{\\tilde{d} \\neq d} \\Lambda_{L, \\tilde{d}, d}$. Also let $\\hat{\\lambda}$ be such that $-\\widehat{\\tilde{p}}^{\\prime} \\hat{\\lambda}=\\max _{\\lambda \\in \\hat{\\Lambda}_{L, d}}-\\widehat{\\tilde{p}}^{\\prime} \\lambda$. Then, $\\hat{\\lambda}=\\lambda_{j_{L}^{\\prime}}$ if and only if\n\n$$\n\\tilde{p}^{\\prime} \\lambda_{j_{L}^{\\prime}}^{1}+\\lambda_{j_{L}^{\\prime}}^{0}-\\left(\\tilde{p}^{\\prime} \\lambda^{1}+\\lambda^{0}\\right) \\leq 0 \\quad \\forall \\lambda \\in \\hat{\\Lambda}_{L, d} \\backslash\\left\\{\\lambda_{j_{L}^{\\prime}}\\right\\}\n$$\n\nso that Assumption 3.2 holds.\nFinally, $\\hat{p}$ is again equal to a vector of sample means so that Assumption 3.4 is satisfied if $\\hat{p}$ is calculated using the random sample $\\left\\{Y_{i}, D_{i}, Z_{i}\\right\\}_{i=1}^{n}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 10,
      "text": "# 4.2 Empirical Welfare Maximization with Observational Data \n\nConsider allocating a binary treatment based on observed covariates $X \\in \\mathcal{X}$. A treatment allocation rule can be defined as a function $\\delta: \\mathcal{X} \\rightarrow\\{0,1\\}$ in a class of rules $\\mathcal{D}$. Consider the utilitarian welfare of deploying $\\delta$ relative to treating no one. The optimal allocation $\\delta^{*}$ satisfies\n\n$$\n\\delta^{*} \\in \\underset{\\delta \\in \\mathcal{D}}{\\arg \\max } W(\\delta)\n$$\n\nNote that $\\mathbb{E}[Y(\\delta(X))-Y(0)]=E[\\delta(X) \\Delta(X)]$, where $\\Delta(X) \\equiv \\mathbb{E}[Y(1)-Y(0) \\mid X]$. This problem is considered in Kitagawa and Tetenov (2018) and Athey and Wager (2021), among others. When only observational data for $(Y, D, X)$ are available with $D$ being endogenous, $W(\\delta)$ is only partially identified unless strong treatment effect homogeneity is assumed. This problem has been studied in Kallus and Zhou (2021); Pu and Zhang (2021); D\u2019Adamo (2021); Byambadalai (2022), among others. Using instrumental variables, one can consider bounds on the conditional ATE based on\n\nconditional versions of the bounds considered in Sections A. 1 and 4.1 (i.e., Manski's bounds and bounds produced by linear programming).\n\nIn particular, assume that $(Y(d), D(z))$ is independent of $Z$ given $X$. Let $L(X)$ and $U(X)$ be conditional Manski bounds on $\\Delta(X)$. Then, bounds on $W(\\delta)$ can be characterized as\n\n$$\nL(\\delta) \\equiv \\mathbb{E}[\\delta(X) L(X)], \\quad U(\\delta) \\equiv \\mathbb{E}[\\delta(X) U(X)]\n$$\n\nSimilarly, bounds on $W(\\tilde{\\delta})-W(\\delta)=\\mathbb{E}[(\\tilde{\\delta}(X)-\\delta(X)) \\Delta(X)]$ can be characterized as\n\n$$\nL(\\tilde{\\delta}, \\delta) \\equiv \\mathbb{E}[(\\tilde{\\delta}(X)-\\delta(X)) L(X)], \\quad U(\\tilde{\\delta}, \\delta) \\equiv \\mathbb{E}[(\\tilde{\\delta}(X)-\\delta(X)) U(X)]\n$$\n\nNote that $L(\\tilde{\\delta}, \\delta) \\neq L(\\tilde{\\delta})-U(\\delta)$ in general (and similarly for $U(\\tilde{\\delta}, \\delta)$ ).\nSuppose $\\mathcal{X}$ is finite and $\\mathcal{X}=\\left\\{x_{1}, \\ldots, x_{K}\\right\\}$ where $x_{k}$ can be a vector and $K$ can potentially be large. For simplicity of exposition, suppose $\\mathcal{X}=\\{0,1,2\\}$. Then $\\mathcal{D}=\\left\\{\\delta_{1}, \\ldots, \\delta_{8}\\right\\}$ where each $\\delta_{j}$ corresponds to a mapping type from $\\{0,1,2\\}$ to $\\{0,1\\}$. To verify Assumptions 3.1 and 3.2, we proceed as follows. For given $x \\in \\mathcal{X}$, by arguments analogous to those in Section 4.1 (and Section A.1), bounds $L_{x}$ and $U_{x}$ on $\\Delta(x)$ satisfy, for some scalars $\\tilde{\\ell}_{j}$ and $\\tilde{u}_{j}$ and row vectors $\\ell_{j}$ and $u_{j}$,\n\n$$\nL_{x}=\\max _{j \\in\\left\\{1, \\ldots, J_{L}\\right\\}}\\left\\{\\tilde{\\ell}_{j}+\\ell_{j} p_{x}\\right\\}, \\quad U_{x}=\\min _{j \\in\\left\\{1, \\ldots, J_{U}\\right\\}}\\left\\{\\tilde{u}_{j}+u_{j} p_{x}\\right\\}\n$$\n\nwhere $p(x)$ is the vector of $p(y, d \\mid z, x)$ 's across $(y, d, z)$ fixing $x$. Then, by Jensen's inequality, for each $\\delta \\in \\mathcal{D}$,\n\n$$\n\\begin{aligned}\n& L(\\delta) \\geq \\tilde{L}(\\delta) \\equiv \\max _{j \\in\\left\\{1, \\ldots, J_{L}\\right\\}}\\left\\{\\tilde{\\ell}_{j} \\mathbb{E}[\\delta(X)]+\\ell_{j} \\mathbb{E}[\\delta(X) p(X)]\\right\\} \\\\\n& U(\\delta) \\leq \\tilde{U}(\\delta) \\equiv \\min _{j \\in\\left\\{1, \\ldots, J_{U}\\right\\}}\\left\\{\\tilde{u}_{j} \\mathbb{E}[\\delta(X)]+u_{j} \\mathbb{E}[\\delta(X) p(X)]\\right\\}\n\\end{aligned}\n$$\n\nNote that $\\tilde{L}(\\delta)$ and $\\tilde{U}(\\delta)$ are non-sharp bounds; for calculation of sharp bounds, see Section A.2.\n\nWe can verify Assumption 3.1 with $\\tilde{L}(\\delta)$ and $\\tilde{U}(\\delta)$ by defining\n\n$$\np=\\left(\\begin{array}{c}\n\\mathbb{E}\\left[\\delta_{1}(X)\\right] \\\\\n\\mathbb{E}\\left[\\delta_{1}(X) p(X)\\right] \\\\\n\\vdots \\\\\n\\mathbb{E}\\left[\\delta_{8}(X)\\right] \\\\\n\\mathbb{E}\\left[\\delta_{8}(X) p(X)\\right]\n\\end{array}\\right)\n$$\n\nand, for $\\delta=\\delta_{1}$ as an example, by using $\\ell_{\\delta_{1}, j}=\\left(\\begin{array}{llll}\\tilde{r}_{j} & \\ell_{j} & 0 & \\ldots & 0\\end{array}\\right)$. Similarly, we can verify Assumptions 3.2 and 3.4 by estimating $p(X)$ and $\\mathbb{E}[\\delta(X)]$ with sample means and $E[\\delta(X) p(X)]$ with $\\frac{1}{n} \\sum_{i}^{n} \\delta\\left(X_{i}\\right) \\hat{p}\\left(X_{i}\\right)$. If the data $\\left\\{Y_{i}, D_{i}, Z_{i}, X_{i}\\right\\}_{i=1}^{n}$ form a random sample, and $\\tilde{\\mathcal{D}}=\\{\\delta \\in \\mathcal{D}: \\tilde{L}(\\tilde{\\delta}, \\delta) \\leq$ $0 \\forall \\tilde{\\delta} \\neq \\delta\\}$ for $\\tilde{L}(\\tilde{\\delta}, \\delta)$ defined the same as $L(\\tilde{\\delta}, \\delta)$ in (4.3) after substituting $\\hat{p}$ for $p$, Assumptions 3.2 and 3.4 hold.\n\nThis framework can be generalized to settings where $W(\\delta)$ is partially identified, not necessarily due to treatment endogeneity but because $W(\\delta)$ is a non-utilitarian welfare defined as a functional of the joint distribution of potential outcomes (e.g., Cui and Han, 2024): $W(\\delta)=f\\left(F_{Y(1), Y(0) \\mid X}\\right)$ where $f$ is some functional and $F_{Y(1), Y(0) \\mid X}$ is the joint distribution of $(Y(1), Y(0))$ conditional on $X$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 11,
      "text": "# 4.3 Bounds for Dynamic Treatment Effects \n\nConsider binary $Y_{t}$ and $D_{t}$ for $t=1, \\ldots, T$. Let $Y \\equiv\\left(Y_{1}, \\ldots, Y_{T}\\right)$ and $D \\equiv\\left(D_{1}, \\ldots, D_{T}\\right)$. Suppose that we are equipped with a sequence of binary instruments $Z \\equiv\\left(Z_{t_{1}}, \\ldots, Z_{t_{K}}\\right)$, which is a subvector of $\\left(Z_{1}, \\ldots, Z_{T}\\right)$. For $t=1, \\ldots, T$, let $Y_{t}\\left(d_{1}, \\ldots, d_{t}\\right)$ be the potential outcome at $t$ and $Y(d) \\equiv\\left(Y_{1}\\left(d_{1}\\right), \\ldots, Y_{T}\\left(d_{1}, \\ldots, d_{T}\\right)\\right)$. We assume that the instruments $Z$ are independent of the potential outcomes $Y(d)$.\n\nLet $T=2$. Then $Y \\equiv\\left(Y_{1}, Y_{2}\\right)$ and $D \\equiv\\left(D_{1}, D_{2}\\right)$. For given welfare $W(d)$ with $d \\equiv\\left(d_{1}, d_{2}\\right)$, we are interested in the optimal policy $d^{*}$ that satisfies\n\n$$\nd^{*} \\in \\underset{d \\in D}{\\operatorname{argmax}} W(d)\n$$\n\nwhere $\\mathcal{D} \\equiv\\{(1,1),(1,0),(0,1),(0,0)\\}$. The sign of the welfare difference, $W(d)-W(\\tilde{d})$ for $d, \\tilde{d} \\in \\mathcal{D}$, is useful for establishing the ordering of $W(d)$ with respect to $d$ and thus to identifying $d^{*}$. However, without additional identifying assumptions, we can only establish a partial ordering of $W(d)$ based on the bounds on the welfare difference (Han, 2024). This will produce the identified set $\\mathcal{D}^{*}$ for $d^{*}$.\n\nAn example of the welfare is $W(d) \\equiv \\mathbb{E}\\left[Y_{2}(d)\\right]$, namely, the average potential terminal outcome. The bounds on welfare $W(d)$ are\n\n$$\nL(d) \\equiv \\max _{z} L(d ; z), \\quad U(d) \\equiv \\min _{z} U(d ; z)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& L(d ; z) \\equiv \\mathbb{P}\\left(Y_{2}=1, D=d \\mid Z=z\\right) \\\\\n& U(d ; z) \\equiv \\mathbb{P}\\left(Y_{2}=1, D=d \\mid Z=z\\right)+\\sum_{d^{\\prime} \\neq d} \\mathbb{P}\\left(D=d^{\\prime} \\mid Z=z\\right)\n\\end{aligned}\n$$\n\nwhich have forms analogous to those in the static case. Define the dynamic ATE in the terminal period for a change in treatment from $d$ to $\\tilde{d}$ as\n\n$$\nW(\\tilde{d})-W(d)=\\mathbb{E}\\left[Y_{2}(\\tilde{d})-Y_{2}(d)\\right]=\\mathbb{P}\\left(Y_{2}(\\tilde{d})=1\\right)-\\mathbb{P}\\left(Y_{2}(d)=1\\right)\n$$\n\nThen the bounds on the dynamic ATE are as follows:\n\n$$\nL(\\tilde{d}, d) \\equiv L(\\tilde{d})-U(d), \\quad U(\\tilde{d}, d) \\equiv U(\\tilde{d})-L(d)\n$$\n\nAnother example of welfare is the joint distribution $W(d) \\equiv \\mathbb{P}(Y(d)=(1,1))$ where $Y(d) \\equiv$ $\\left(Y_{1}\\left(d_{1}\\right), Y_{2}(d)\\right)$. The bounds on $W(d)$ in this case are $L(d) \\equiv \\max _{z} L(d ; z)$ and $U(d) \\equiv \\min _{z} U(d ; z)$ where\n\n$$\n\\begin{aligned}\n& L(d ; z) \\equiv \\mathbb{P}\\left(Y_{2}=1, D=d \\mid Z=z\\right) \\\\\n& U(d ; z) \\equiv \\mathbb{P}\\left(Y_{2}=1, D=d \\mid Z=z\\right)+\\mathbb{P}\\left(Y_{1}=1, D_{1}=d_{1}, D_{2}=d_{2}^{\\prime} \\mid Z=z\\right)\n\\end{aligned}\n$$\n\n$$\n+\\mathbb{P}\\left(D_{1}=d_{1}^{\\prime}, D_{2}=d_{2} \\mid Z=z\\right)+\\mathbb{P}\\left(D_{1}=d_{1}^{\\prime}, D_{2}=d_{2}^{\\prime} \\mid Z=z\\right)\n$$\n\nwith $d_{1}^{\\prime} \\neq d_{1}$ and $d_{2}^{\\prime} \\neq d_{2}$. Consider the effect of treatment on the joint distribution, for example,\n\n$$\nW(1,1)-W(1,0)=\\mathbb{P}(Y(1,1)=(1,1))-\\mathbb{P}(Y(1,0)=(1,1))\n$$\n\nThen, with $\\tilde{d}=(1,1)$ and $d=(1,0)$, the bounds on this parameter are\n\n$$\n\\begin{aligned}\nL(\\tilde{d}, d) \\equiv & \\max _{z} \\mathbb{P}\\left(Y_{2}=1, D=(1,1) \\mid Z=z\\right) \\\\\n& -\\min _{z}\\left\\{\\mathbb{P}\\left(Y_{2}=1, D=(1,0) \\mid Z=z\\right)+\\mathbb{P}\\left(Y_{1}=1, D=(1,1) \\mid Z=z\\right)\\right. \\\\\n& \\left.+\\mathbb{P}(D=(0,1) \\mid Z=z)+\\mathbb{P}(D=(0,0) \\mid Z=z)\\right\\} \\\\\nU(\\tilde{d}, d) \\equiv & \\min _{z}\\left\\{\\mathbb{P}\\left(Y_{2}=1, D=(1,1) \\mid Z=z\\right)+\\mathbb{P}\\left(Y_{1}=1, D=(1,0) \\mid Z=z\\right)\\right. \\\\\n& \\left.+\\mathbb{P}(D=(0,1) \\mid Z=z)+\\mathbb{P}(D=(0,0) \\mid Z=z)\\right\\} \\\\\n& -\\max _{z} \\mathbb{P}\\left(Y_{2}=1, D=(1,0) \\mid Z=z\\right)\n\\end{aligned}\n$$\n\nIn these examples, the identified set $\\mathcal{D}^{*}$ can be characterized as a set of maximal elements:\n\n$$\n\\mathcal{D}^{*}=\\{d: \\nexists \\tilde{d} \\neq d \\text { such that } L(\\tilde{d}, d)>0\\}=\\{d: L(\\tilde{d}, d) \\leq 0, \\forall \\tilde{d} \\neq d\\}\n$$\n\nThese examples are special cases of the model in Han (2024). ${ }^{3}$\nIn both cases, it is easy to see that $L(d)$ and $U(d)$ satisfy Assumption 3.1 with $p$ being the vector of probabilities $p(y, d \\mid z) \\equiv \\mathbb{P}(Y=y, D=d \\mid Z=z)$. To verify Assumption 3.2, let $\\widehat{L}(\\tilde{d}, d)$ be the estimator of $L(\\tilde{d}, d)$ where the sample frequency replaces the population probability. Then $\\widehat{\\mathcal{D}}=\\{d: \\widehat{L}(\\tilde{d}, d) \\leq 0, \\forall \\tilde{d} \\neq d\\}$.\n\nContinuing with the first example, let $Z=Z_{1} \\in\\{0,1\\}$, that is, the researcher is only equipped with a binary instrument in the first period and no instrument in the second period. We focus\n\n[^0]\n[^0]:    ${ }^{3}$ See also Han and Lee (2024) for other examples of dynamic causal parameters that can be used to define optimal treatments.\n\non inference for $L(d)$ for $d \\in \\widehat{\\mathcal{D}}$. Let $\\widehat{z}(d) \\in \\operatorname{argmax}_{z \\in\\{0,1\\}} \\widehat{L}(d ; z)$ so that $\\widehat{L}(d)=\\widehat{L}(d ; \\widehat{z}(d))$. Then we can write the data-dependent event that $d$ is an element of $\\widehat{\\mathcal{D}}, \\widehat{L}(d)=\\widehat{L}\\left(d ; z^{*}\\right)$ as a polyhedron\n\n$$\n\\left\\{d \\in \\widehat{\\mathcal{D}}, \\widehat{z}(d)=z^{*}\\right\\}=\\{\\widehat{L}(\\tilde{d}, d) \\leq 0 \\forall \\tilde{d} \\neq d, \\widehat{z}(d)=z^{*}\\}=\\left\\{A^{L} \\hat{p} \\leq 0\\right\\}\n$$\n\nfor some matrix $A^{L}$, where $\\hat{p}$ is the vector of probabilities $\\widehat{p}(y, d \\mid z)$, so that Assumption 3.2 holds. This is due to the forms of $L(d ; z)$ and $U(d ; z)$ above and $\\widehat{L}(\\tilde{d}, d) \\leq 0 \\forall \\tilde{d} \\neq d$ if and only if\n\n$$\n\\widehat{L}(\\tilde{d} ; z) \\leq \\widehat{U}\\left(d ; z^{\\prime}\\right) \\quad \\forall \\tilde{d} \\neq d, \\forall z, z^{\\prime}\n$$\n\nand, for example, $\\widehat{z}(d)=1$ if and only if $\\widehat{L}(d ; 0)-\\widehat{L}(d ; 1) \\leq 0$. A similar formulation follows for the second example. In fact, this approach applies to a general parameter $W(d)$ with bounds that are minimum and maximums of linear combinations of $p(y, d \\mid z)$ 's, such as parameters that have the following form:\n\n$$\nW(d) \\equiv f\\left(q_{d}\\right)\n$$\n\nfor some linear functional $f$, where $q_{d}(y) \\equiv \\mathbb{P}(Y(d)=y)$.\nFinally, Assumption 3.4 is satisfied in both examples when the data $\\left\\{Y_{i}, D_{i}, Z_{i}\\right\\}_{i=1}^{n}$ form a random sample since the entries of $\\hat{p}$ are sample means.\n\nThis framework can be further generalized to incorporate treatment choices adaptive to covariates or past outcomes as in Han (2024), analogous to Section 4.2. This generalization is considered in our empirical application in Section 8. Sometimes this generalization prevents the researcher from deriving analytical bounds, in which case the linear programming approach can be used.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 12,
      "text": "# 4.4 Optimal Treatment Assignment with Interval-Identified ATE \n\nIn recent work, Christensen et al. (2023) note that \"plug-in\" rules for determining treatment choice can be sub-optimal when ATEs are not point-identified since the bounds on the ATE are not smooth functions of the reduced-form parameter $p$. Using an optimality criterion that minimizes\n\nmaximum regret over the identified set for the ATE, conditional on $p$, they advocate bootstrap and quasi-Bayesian methods for optimal treatment choice. More specifically, they consider settings for which the ATE of a treatment is identified via intersection bounds:\n\n$$\nb_{L}(p) \\equiv \\max _{j \\in\\left\\{1, \\ldots, J_{L}\\right\\}}\\left\\{\\tilde{\\ell}_{j}+\\ell_{j} p\\right\\} \\leq A T E \\leq \\min _{j \\in\\left\\{1, \\ldots, J_{U}\\right\\}}\\left\\{\\tilde{u}_{k}+u_{k} p\\right\\} \\equiv b_{U}(p)\n$$\n\nfor some fixed and known $J_{L}, J_{U}, \\tilde{\\ell}_{1}, \\ldots, \\tilde{\\ell}_{J_{L}}, \\tilde{u}_{1}, \\ldots, \\tilde{u}_{J_{U}}, \\ell_{1}, \\ldots, \\ell_{J_{L}}$ and $u_{1}, \\ldots, u_{J_{U}} \\cdot{ }^{4}$ Therefore, Assumption 3.1 trivially holds for the ATE.\n\nChristensen et al. (2023) advocate a quasi-Bayesian implementation of their optimal treatment choice rule taking the form\n\n$$\n\\hat{d}=\\mathbf{1}\\left(\\frac{1}{m} \\sum_{i=1}^{m}\\left[\\max \\left\\{b_{U}\\left(\\hat{p}+\\varepsilon_{i}\\right), 0\\right\\}+\\min \\left\\{b_{L}\\left(\\hat{p}+\\varepsilon_{i}\\right), 0\\right\\}\\right] \\geq 0\\right)\n$$\n\nfor some large $m$, where $\\varepsilon_{1}, \\ldots, \\varepsilon_{m}{ }^{\\text {i.i.d. }} \\mathcal{N}(0, \\widehat{\\Sigma})$ are independent of $\\hat{p}$. As the following proposition shows, this form of $\\hat{d}$ satisfies Assumption 3.2 when $\\hat{\\gamma}_{L}(d)=\\hat{\\gamma}_{U}(d)$ are specified properly.\n\nProposition 4.1. Suppose $\\widehat{\\mathcal{D}}=\\{\\hat{d}\\}$, where $\\hat{d}$ is defined by (4.6). Then Assumptions 3.2 and 3.3 are satisfied for\n\n$$\n\\hat{\\gamma}_{L}(d)=\\hat{\\gamma}_{U}(d)=\\left(\\varepsilon_{1}^{\\prime}, \\ldots, \\varepsilon_{m}^{\\prime}, \\underline{k}_{1}, \\ldots, \\underline{k}_{m}, \\bar{k}_{1}, \\ldots, \\bar{k}_{m}, s_{1}^{\\ell}, \\ldots, s_{m}^{\\ell}, s_{1}^{u}, \\ldots, s_{m}^{u}\\right)^{\\prime}\n$$\n\nwhere $\\underline{k}_{i} \\equiv \\operatorname{argmin}_{k \\in\\left\\{1, \\ldots, J_{U}\\right\\}}\\left\\{\\tilde{u}_{k}+u_{k}\\left(\\hat{p}+\\varepsilon_{i}\\right)\\right\\}, \\bar{k}_{i} \\equiv \\operatorname{argmax}_{k \\in\\left\\{1, \\ldots, J_{L}\\right\\}}\\left\\{\\tilde{\\ell}_{k}+\\ell_{k}\\left(\\hat{p}+\\varepsilon_{i}\\right)\\right\\}, s_{i}^{\\ell} \\equiv$ $\\operatorname{sign}\\left(\\tilde{\\ell}_{\\bar{k}_{i}}+\\ell_{\\bar{k}_{i}}\\left(\\hat{p}+\\varepsilon_{i}\\right)\\right)$ and $s_{i}^{u} \\equiv \\operatorname{sign}\\left(\\tilde{u}_{\\underline{k}_{i}}+u_{\\underline{k}_{i}}\\left(\\hat{p}+\\varepsilon_{i}\\right)\\right)$ for $i=1, \\ldots, m$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 13,
      "text": "# 5 Confidence Interval Construction \n\nWe now generalize the CI construction described in Section 2 to apply in the general framework of Section 3, covering all example applications discussed above and in Appendix A. We start with conditional CIs and then move to unconditional CIs.\n\n[^0]\n[^0]:    ${ }^{4}$ Although Christensen et al. (2023) do not write the form of their bounds as they are written here, the representation here is equivalent to the one in that paper upon proper definition of $p$ since the elements in the intersection bounds are smooth functions of a reduced-form parameter.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 14,
      "text": "# 5.1 Conditional Confidence Intervals \n\nWe first generalize the conditional CI construction described in Section 2.2. As in Section 2.2, we are interested in forming probabilistic lower and upper bounds $\\widehat{L}(\\hat{d})_{\\alpha}^{C}$ and $\\widehat{U}(\\hat{d})_{1-\\alpha}^{C}$ that satisfy (2.2) and (2.5) for all $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$ as endpoints in the formation of a conditionally valid CI. This is because the researcher's interest in inference on option $d$ only arises when it is a member of the estimated set $\\widehat{\\mathcal{D}}$.\n\nTo begin, we characterize the conditional distributions of $\\widehat{L}(d)$ given the event $\\left\\{d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}\\right.$ and $\\left.\\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right\\}$ characterized by Assumption 3.2. These conditional distributions depend upon the nuisance parameter $p$. As a first step, we form sufficient statistics for $p$ that are asymptotically independent of $\\widehat{L}(d)$ given $\\hat{j}_{L}(d)=j_{L}^{*}$ and $\\widehat{U}(d)$ given $\\hat{j}_{U}(d)=j_{U}^{*}$. Since $\\widehat{L}(d)=\\hat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} \\hat{p}$ given $\\hat{j}_{L}(d)=j_{L}^{*}$ by Assumption 3.1 and (3.1) (and similarly for $\\widehat{U}(d)$ ), such sufficient statistics can be constructed as\n\n$$\n\\widehat{\\mathcal{Z}}_{L}\\left(d, j_{L}^{*}\\right) \\equiv \\sqrt{n} \\hat{p}-\\hat{b}_{L}\\left(d, j_{L}^{*}\\right) \\sqrt{n}\\left(\\hat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} \\hat{p}\\right)\n$$\n\nand\n\n$$\n\\widehat{\\mathcal{Z}}_{U}\\left(d, j_{U}^{*}\\right) \\equiv \\sqrt{n} \\hat{p}-\\hat{b}_{U}\\left(d, j_{U}^{*}\\right) \\sqrt{n}\\left(\\tilde{u}_{d, j_{U}^{*}}+u_{d, j_{U}^{*}} \\hat{p}\\right)\n$$\n\nwith\n\n$$\n\\hat{b}_{L}\\left(d, j_{L}^{*}\\right) \\equiv \\widehat{\\Sigma} \\ell_{d, j_{L}^{*}}^{\\prime}\\left(\\ell_{d, j_{L}^{*}} \\widehat{\\Sigma} \\ell_{d, j_{L}^{*}}^{\\prime}\\right)^{-1} \\quad \\text { and } \\quad \\hat{b}_{U}\\left(d, j_{U}^{*}\\right) \\equiv \\widehat{\\Sigma} u_{d, j_{U}^{*}}^{\\prime}\\left(u_{d, j_{U}^{*}} \\widehat{\\Sigma} u_{d, j_{U}^{*}}^{\\prime}\\right)^{-1}\n$$\n\nby the asymptotic normality of $\\hat{p}$ and asymptotic absence of correlation between $\\sqrt{n} \\hat{p}$ and $\\widehat{\\mathcal{Z}}_{L}\\left(d, j_{L}^{*}\\right)$ and $\\widehat{\\mathcal{Z}}_{U}\\left(d, j_{U}^{*}\\right)$. Next, Assumption 3.2 characterizes the conditioning events $\\left\\{d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=\\right.$ $\\left.j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right\\}$ and $\\left\\{d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{U}(d)=j_{U}^{*}, \\hat{\\gamma}_{U}(d)=\\gamma_{U}^{*}\\right\\}$ as polyhedra in $\\hat{p}$, which can in turn be expressed as intervals for $\\hat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} \\hat{p}$ and $\\tilde{u}_{d, j_{U}^{*}}+u_{d, j_{U}^{*}} \\hat{p}$; see, e.g., (3.2) and (3.3). Then, again since $\\widehat{L}(d)=\\hat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} \\hat{p}$ given $\\hat{j}_{L}(d)=j_{L}^{*}$ by Assumption 3.1, Lemma 1 of McCloskey (2024) implies\n\n$$\n\\begin{gathered}\n\\sqrt{n} \\widehat{L}(d) \\mid\\left\\{d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right\\} \\\\\n\\sim \\sqrt{n}\\left(\\hat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} \\hat{p}\\right) \\mid\\left\\{\\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, j_{L}^{*}\\right), d, j_{L}^{*}, \\gamma_{L}^{*}\\right) \\leq \\sqrt{n}\\left(\\hat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} \\hat{p}\\right) \\leq \\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, j_{L}^{*}\\right), d, j_{L}^{*}, \\gamma_{L}^{*}\\right)\\right. \\\\\n\\left.\\widehat{\\mathcal{V}}_{L}^{0}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, j_{L}^{*}\\right), d, j_{L}^{*}, \\gamma_{L}^{*}\\right) \\geq 0\\right\\}\n\\end{gathered}\n$$\n\nand\n\n$$\n\\begin{gathered}\n\\sqrt{n} \\widehat{U}(d) \\mid\\left\\{d \\in \\widehat{\\mathcal{D}} \\hat{j}_{U}(d)=j_{U}^{*}, \\hat{\\gamma}_{U}(d)=\\gamma_{U}^{*}\\right\\} \\\\\n\\sim \\sqrt{n}\\left(\\tilde{u}_{d, j_{U}^{*}}+u_{d, j_{U}^{*}} \\hat{p}\\right) \\mid\\left\\{\\widehat{\\mathcal{V}}_{U}^{-}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d, j_{U}^{*}\\right), d, j_{U}^{*}, \\gamma_{U}^{*}\\right) \\leq \\sqrt{n}\\left(\\tilde{u}_{d, j_{U}^{*}}+u_{d, j_{U}^{*}} \\hat{p}\\right) \\leq \\widehat{\\mathcal{V}}_{U}^{+}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d, j_{U}^{*}\\right), d, j_{U}^{*}, \\gamma_{U}^{*}\\right)\\right. \\\\\n\\left.\\widehat{\\mathcal{V}}_{U}^{0}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d, j_{U}^{*}\\right), d, j_{U}^{*}, \\gamma_{U}^{*}\\right) \\geq 0\\right\\}\n\\end{gathered}\n$$\n\nwith\n\n$$\n\\begin{aligned}\n& \\widehat{\\mathcal{V}}_{M}^{*}(z, d, j, \\gamma) \\equiv \\max _{k:\\left(A^{M}(d, j, \\gamma) \\hat{b}_{M}(d, j)\\right)_{k}<0} \\frac{\\sqrt{n}\\left(c^{M}(d, j, \\gamma)\\right)_{k}-\\left(A^{M}(d, j, \\gamma) z\\right)_{k}}{\\left(A^{M}(d, j, \\gamma) \\hat{b}_{M}(d, j)\\right)_{k}} \\\\\n& \\widehat{\\mathcal{V}}_{M}^{*}^{*}(z, d, j, \\gamma) \\equiv \\min _{k:\\left(A^{M}(d, j, \\gamma) \\hat{b}_{M}(d, j)\\right)_{k}>0} \\frac{\\sqrt{n}\\left(c^{M}(d, j, \\gamma)\\right)_{k}-\\left(A^{M}(d, j, \\gamma) z\\right)_{k}}{\\left(A^{M}(d, j, \\gamma) \\hat{b}_{M}(d, j)\\right)_{k}} \\\\\n& \\widehat{\\mathcal{V}}_{M}^{0}(z, d, j, \\gamma) \\equiv \\min _{k:\\left(A^{M}(d, j, \\gamma) \\hat{b}_{M}(d, j)\\right)_{k}=0} \\sqrt{n}\\left(c^{M}(d, j, \\gamma)\\right)_{k}-\\left(A^{M}(d, j, \\gamma) z\\right)_{k}\n\\end{aligned}\n$$\n\nfor $M=L, U$.\nNow, under Assumptions 3.4 and 3.5, the distribution of $\\sqrt{n}\\left(\\hat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} \\hat{p}\\right)$ can be approximated by a $\\mathcal{N}\\left(\\sqrt{n}\\left(\\hat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} p\\right), \\ell_{d, j_{L}^{*}} \\widehat{\\Sigma} \\ell_{d, j_{L}^{*}}^{\\prime}\\right)$-distributed random variable that is asymptotically independent of $\\widehat{\\mathcal{Z}}_{L}\\left(d, j_{L}^{*}\\right)$. Using the distributional characterization in (5.1), we can therefore use the corresponding truncated normal cumulative distribution function to produce quantile-unbiased estimators of the underlying mean $\\sqrt{n}\\left(\\hat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} p\\right)$. Let $F_{T N}\\left(\\cdot ; \\mu, \\sigma^{2} \\mid \\mathcal{V}^{-}, \\mathcal{V}^{+}\\right)$denote the truncated normal cumulative distribution function for an underlying normally-distributed random variable with mean $\\mu$ and variance $\\sigma^{2}$ that is truncated to lie between $\\mathcal{V}^{-}$and $\\mathcal{V}^{+}$. For $\\alpha \\in(0,1)$, define $\\widehat{L}(d)_{\\alpha}^{C}$ to solve\n\n$$\n\\begin{gathered}\nF_{T N}\\left(\\sqrt{n}\\left(\\hat{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} \\hat{p}\\right) ; \\mu, \\ell_{d, \\hat{j}_{L}(d)} \\widehat{\\Sigma} \\ell_{d, \\hat{j}_{L}(d)}^{\\prime} \\mid \\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right), d, \\hat{j}_{L}(d), \\hat{\\gamma}_{L}(d)\\right) \\\\\n\\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right), d, \\hat{j}_{L}(d), \\hat{\\gamma}_{L}(d)\\right)\\right)=1-\\alpha\n\\end{gathered}\n$$\n\nin $\\mu$. Similarly, define $\\widehat{U}(d)_{\\alpha}^{C}$ to solve\n\n$$\nF_{T N}\\left(\\sqrt{n}\\left(\\tilde{u}_{d, \\hat{j}_{U}(d)}+u_{d, \\hat{j}_{U}(d)} \\hat{p}\\right) ; \\mu, u_{d, \\hat{j}_{U}(d)} \\widehat{\\Sigma} u_{d, \\hat{j}_{U}(d)}^{\\prime} \\mid \\widehat{\\mathcal{V}}_{U}^{-}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d, \\hat{j}_{U}(d)\\right), d, \\hat{j}_{U}(d), \\hat{\\gamma}_{U}(d)\\right)\n$$\n\n$$\n\\widehat{\\mathcal{V}}_{U}^{+}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d, \\hat{j}_{U}(d)\\right), d, \\hat{j}_{U}(d), \\hat{\\gamma}_{U}(d)\\right)\\right)=1-\\alpha\n$$\n\nin $\\mu$. Then, results in Pfanzagl (1994) imply that $\\widehat{L}(d)_{\\alpha}^{C}$ and $\\widehat{U}(d)_{\\alpha}^{C}$ are optimal $\\alpha$ quantile-unbiased estimators of $\\sqrt{n}\\left(\\widehat{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} p\\right)$ and $\\sqrt{n}\\left(\\widehat{u}_{d, j_{L}^{*}}+u_{d, j_{L}^{*}} p\\right)$ asymptotically.\n\nFinally, combine these quantile-unbiased estimators to form a conditional CI for the identified interval $[L(d), U(d)]$ :\n\n$$\n\\left(n^{-1 / 2} \\widehat{L}(d)_{\\alpha_{1}}^{C}, n^{-1 / 2} \\widehat{U}(d)_{1-\\alpha_{2}}^{C}\\right)\n$$\n\nWe establish the conditional uniform asymptotic validity of this CI.\nTheorem 5.1. Suppose Assumptions 3.1, 3.2 and 3.4-3.6 hold. Then, for any $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$ and $0<\\alpha_{1}, \\alpha_{2}<1 / 2$,\n\n$$\n\\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left\\{\\left[\\mathbb{P}\\left([L(d), U(d)] \\subseteq\\left(n^{-1 / 2} \\widehat{L}(d)_{\\alpha_{1}}^{C}, n^{-1 / 2} \\widehat{U}(d)_{1-\\alpha_{2}}^{C}\\right) \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\left(1-\\alpha_{1}-\\alpha_{2}\\right)\\right] \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}})\\right\\} \\geq 0\n$$\n\nfor all $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 15,
      "text": "# 5.2 Unconditional Confidence Intervals \n\nIn parallel with the previous subsection, we now generalize the unconditional CI constructions described in Section 2.3 to the general framework of Section 3. Note that conditional inference on $W(d)$ is well-defined for any given $d \\in \\widehat{\\mathcal{D}}$, when conditioning on $d \\in \\widehat{\\mathcal{D}}$. In contrast, unconditional inference on a data-dependent $W(d)$ requires it to be uniquely defined, as $W(\\hat{d})$ in our notation. This is implied by Assumption 3.3. Here, we would like to construct CIs that unconditionally cover the identified interval corresponding to a unique data-dependent object of inferential interest. As mentioned in Section 2.3, if only unconditional coverage of $[L(\\hat{d}), U(\\hat{d})]$ is desired the conditional CI (5.2) with $d=\\hat{d}$ can be unnecessarily wide. We describe two different methods-projection and hybrid methods-to form the unconditional probabilistic bounds that constitute the endpoints of these unconditional CIs in this general framework.\n\nThe general formation of the probabilistic bounds based upon projecting simultaneous confidence bounds for all possible values of $\\sqrt{n} L(\\hat{d})$ and $\\sqrt{n} U(\\hat{d})$ proceeds by computing $\\hat{c}_{1-\\alpha, M}$, the $1-\\alpha$\n\nquantile of $\\max _{i \\in\\left\\{1, \\ldots,(T+1) J_{M}\\right\\}} \\hat{\\zeta}_{M, i} / \\sqrt{\\widehat{\\Sigma}_{M, i}}$, where $\\hat{\\zeta}_{M} \\sim \\mathcal{N}\\left(0, \\widehat{\\Sigma}_{M}\\right)$ for $M=L, U$ with $\\widehat{\\Sigma}_{L}=\\ell^{\\text {mat }} \\widehat{\\Sigma} \\ell^{\\text {matr }}$, $\\widehat{\\Sigma}_{U}=u^{\\text {mat }} \\widehat{\\Sigma} u^{\\text {matr }}, \\ell^{\\text {mat }}=\\left(\\ell_{0,1}^{\\prime}, \\ldots, \\ell_{0, J_{L}}^{\\prime}, \\ldots, \\ell_{T, 1}^{\\prime}, \\ldots, \\ell_{T, J_{L}}^{\\prime}\\right)^{\\prime}$ and $u^{\\text {mat }}=\\left(u_{0,1}^{\\prime}, \\ldots, u_{0, J_{U}}^{\\prime}, \\ldots, u_{T, 1}^{\\prime}, \\ldots, u_{T, J_{U}}^{\\prime}\\right)^{\\prime}$, recalling that $\\Upsilon_{i}$ denotes the $i^{\\text {th }}$ element of the main diagonal of any square matrix $\\Upsilon$. Here, the maximum is taken to guarantee simultaneous coverage. The lower level $1-\\alpha$ projection confidence bound for $\\sqrt{n} L(\\hat{d})$ is $\\widehat{L}(\\hat{d})_{\\alpha}^{P}=\\sqrt{n} \\widehat{L}(\\hat{d})-\\hat{c}_{1-\\alpha, L} \\sqrt{\\widehat{\\Sigma}_{L, \\hat{d} J_{L}+\\hat{j}_{L}(\\hat{d})}}$ and the upper level $1-\\alpha$ projection confidence bound for $\\sqrt{n} U(\\hat{d})$ is $\\widehat{U}_{1-\\alpha}^{P}(\\hat{d})=\\sqrt{n} \\widehat{U}(\\hat{d})+\\hat{c}_{1-\\alpha, U} \\sqrt{\\widehat{\\Sigma}_{U, \\hat{d} J_{U}+\\hat{j}_{U}(\\hat{d})}}$, because e.g., $\\sqrt{n} L(\\hat{d})$ can take value equal to any entry of the vector $\\sqrt{n}\\left(\\hat{\\ell}_{0,1}, \\ldots, \\hat{\\ell}_{0, J_{L}}, \\ldots, \\hat{\\ell}_{T, 1}, \\ldots, \\hat{\\ell}_{T, J_{L}}\\right)^{\\prime}+\\sqrt{n} \\ell^{\\text {mat }} p$, $\\sqrt{n}\\left(\\ell^{\\text {mat }} \\hat{p}-\\ell^{\\text {mat }} p\\right)$ is asymptotically distributed $\\mathcal{N}\\left(0, \\Sigma_{L}\\right)$ for $\\Sigma_{L}=\\ell^{\\text {mat }} \\Sigma \\ell^{\\text {matr }}$ by Assumption 3.4 and $\\widehat{\\Sigma}_{L}$ is consistent for $\\Sigma_{L}$ by Assumption 3.5.\n\nCombining these two confidence bounds at appropriate levels yields an unconditional CI for the identified interval $[L(\\hat{d}), U(\\hat{d})]$ of the selected $\\hat{d}$,\n\n$$\n\\left(n^{-1 / 2} \\widehat{L}(\\hat{d})_{\\alpha_{1}}^{P}, n^{-1 / 2} \\widehat{U}(\\hat{d})_{\\mathrm{I}-\\alpha_{2}}^{P}\\right)\n$$\n\nwith uniformly correct asymptotic coverage, regardless of how $\\hat{d}$ is selected from the data.\n\nTheorem 5.2. Suppose Assumptions 3.1 and 3.4-3.6 hold. Then, for any (random) $\\hat{d} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$ and $0<\\alpha_{1}, \\alpha_{2}<1 / 2$,\n\n$$\n\\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\mathbb{P}\\left([L(\\hat{d}), U(\\hat{d})] \\subseteq\\left(n^{-1 / 2} \\widehat{L}(\\hat{d})_{\\alpha_{1}}^{P}, n^{-1 / 2} \\widehat{U}(\\hat{d})_{\\mathrm{I}-\\alpha_{2}}^{P}\\right)\\right) \\geq 1-\\alpha_{1}-\\alpha_{2}\n$$\n\nNote that the projection CI (5.3) has the benefit of correct coverage regardless of how $\\hat{d}$ is chosen from the data. In this sense, it is more robust than the other CIs we propose in this paper. On the other hand, by using the common selection structure of Assumption 3.2, we are able to produce a hybrid CI that combines the strengths of the conditional CI (5.2) and the projection CI (5.3) which, as described in Section 2.3, are shorter under complementary scenarios.\n\nIn analogy with the construction of the conditional CIs, to construct the hybrid CIs we begin by characterizing the conditional distributions of $\\widehat{L}(\\hat{d})$ and $\\widehat{U}(\\hat{d})$ but now adding an additional component to the conditioning events. More specifically, under Assumptions 3.2 and 3.3, by\n\nintersecting the events\n\n$$\n\\begin{gathered}\n\\left\\{\\hat{d}=d^{*} \\hat{j}_{L}(\\hat{d})=j_{L}^{*}, \\hat{\\gamma}_{L}(\\hat{d})=\\gamma_{L}^{*}\\right\\}=\\left\\{d^{*} \\in \\widehat{\\mathcal{D}} \\hat{j}_{L}\\left(d^{*}\\right)=j_{L}^{*}, \\hat{\\gamma}_{L}\\left(d^{*}\\right)=\\gamma_{L}^{*}\\right\\} \\\\\n=\\left\\{\\widehat{\\mathcal{V}}_{L}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d^{*} \\cdot j_{L}^{*}\\right), d^{*} \\cdot j_{L}^{*}, \\gamma_{L}^{*}\\right) \\leq \\sqrt{n}\\left(\\hat{\\ell}_{d^{*} \\cdot j_{L}^{*}}+\\ell_{d^{*} \\cdot j_{L}^{*}} \\hat{p}\\right) \\leq \\widehat{\\mathcal{V}}_{L}^{*}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d^{*} \\cdot j_{L}^{*}\\right), d^{*} \\cdot j_{L}^{*}, \\gamma_{L}^{*}\\right) \\\\\n\\widehat{\\mathcal{V}}_{L}^{0}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d^{*} \\cdot j_{L}^{*}\\right), d^{*} \\cdot j_{L}^{*}, \\gamma_{L}^{*}\\right) \\geq 0\\}\n\\end{gathered}\n$$\n\nand\n\n$$\n\\begin{aligned}\n\\left\\{\\sqrt{n}\\left(\\hat{\\ell}_{\\hat{d}, \\hat{j}_{L}(\\hat{d})}+\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} p\\right) \\geq \\widehat{L}_{\\beta}^{P}(\\hat{d})\\right\\} \\\\\n=\\left\\{\\sqrt{n}\\left(\\hat{\\ell}_{\\hat{d}, \\hat{j}_{L}(\\hat{d})}+\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} \\hat{p}\\right) \\leq \\sqrt{n}\\left(\\hat{\\ell}_{\\hat{d}, \\hat{j}_{L}(\\hat{d})}+\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} p\\right)+\\hat{c}_{1-\\beta, L} \\sqrt{\\widehat{\\Sigma}_{L, \\hat{d}, j_{L}+\\hat{j}_{L}(\\hat{d})}}\\right\\}\n\\end{aligned}\n$$\n\nfor some $0<\\beta<\\alpha<1$, we have\n\n$$\n\\begin{gathered}\n\\sqrt{n} \\widehat{L}(\\hat{d})\\left|\\left\\{\\hat{d}=d^{*} \\hat{j}_{L}(\\hat{d})=j_{L}^{*}, \\hat{\\gamma}_{L}(\\hat{d})=\\gamma_{L}^{*}, \\sqrt{n}\\left(\\hat{\\ell}_{\\hat{d}, \\hat{j}_{L}(\\hat{d})}+\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} p\\right) \\geq \\widehat{L}_{\\beta}^{P}(\\hat{d})\\right\\}\\right. \\\\\n\\left.\\sim \\sqrt{n}\\left(\\hat{\\ell}_{d^{*} \\cdot j_{L}^{*}}+\\ell_{d^{*} \\cdot j_{L}^{*}} \\hat{p}\\right)\\left|\\left\\{\\widehat{\\mathcal{V}}_{L}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d^{*} \\cdot j_{L}^{*}\\right), d^{*} \\cdot j_{L}^{*}, \\gamma_{L}^{*}\\right) \\leq \\sqrt{n}\\left(\\hat{\\ell}_{d^{*} \\cdot j_{L}^{*}}+\\ell_{d^{*} \\cdot j_{L}^{*}} \\hat{p}\\right)\\right.\\right. \\\\\n\\leq \\widehat{\\mathcal{V}}_{L}^{+, H}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d^{*} \\cdot j_{L}^{*}\\right), d^{*} \\cdot j_{L}^{*}, \\gamma_{L}^{*}, \\sqrt{n}\\left(\\hat{\\ell}_{d^{*} \\cdot j_{L}^{*}}+\\ell_{d^{*} \\cdot j_{L}^{*}} p\\right)\\right), \\widehat{\\mathcal{V}}_{L}^{0}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d^{*} \\cdot j_{L}^{*}\\right), d^{*} \\cdot j_{L}^{*}, \\gamma_{L}^{*}\\right) \\geq 0\\right\\}\n\\end{gathered}\n$$\n\nwhere\n\n$$\n\\widehat{\\mathcal{V}}_{L}^{+, H}(z, d, j, \\gamma, \\mu) \\equiv \\min \\left\\{\\widehat{\\mathcal{V}}_{L}^{+}(z, d, j, \\gamma), \\mu+\\hat{c}_{1-\\beta, L} \\sqrt{\\widehat{\\Sigma}_{L, d J_{L}+\\hat{j}}}\\right\\}\n$$\n\nSimilarly,\n\n$$\n\\begin{gathered}\n\\sqrt{n} \\widehat{U}(\\hat{d})\\left|\\left\\{\\hat{d}=d^{*} \\hat{j}_{U}(\\hat{d})=j_{U}^{*}, \\hat{\\gamma}_{U}(\\hat{d})=\\gamma_{U}^{*}, \\sqrt{n}\\left(\\hat{u}_{\\hat{d}, \\hat{j}_{U}(\\hat{d})}+u_{\\hat{d}, \\hat{j}_{U}(\\hat{d})} p\\right) \\leq \\widehat{U}_{1-\\beta}^{P}(\\hat{d})\\right\\}\\right. \\\\\n\\left.\\sim \\sqrt{n}\\left(\\hat{u}_{d^{*} \\cdot j_{U}^{*}}+u_{d^{*} \\cdot j_{U}^{*}} \\hat{p}\\right)\\left|\\left\\{\\widehat{\\mathcal{V}}_{U}^{-, H}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d^{*} \\cdot j_{U}^{*}\\right), d^{*} \\cdot j_{U}^{*}, \\gamma_{U}^{*}, \\sqrt{n}\\left(\\hat{u}_{d^{*} \\cdot j_{U}^{*}}+u_{d^{*} \\cdot j_{U}^{*}} p\\right)\\right)\\right.\\right. \\\\\n\\leq \\sqrt{n}\\left(\\hat{u}_{d^{*} \\cdot j_{U}^{*}}+u_{d^{*} \\cdot j_{U}^{*}} \\hat{p}\\right) \\leq \\widehat{\\mathcal{V}}_{U}^{+}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d^{*} \\cdot j_{U}^{*}\\right), d^{*} \\cdot j_{U}^{*}, \\gamma_{U}^{*}\\right), \\widehat{\\mathcal{V}}_{U}^{0}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d^{*} \\cdot j_{U}^{*}\\right), d^{*} \\cdot j_{U}^{*}, \\gamma_{U}^{*}\\right) \\geq 0\\right\\}\n\\end{gathered}\n$$\n\nwhere\n\n$$\n\\widehat{\\mathcal{V}}_{U}^{-, H}(z, d, j, \\gamma, \\mu) \\equiv \\max \\left\\{\\widehat{\\mathcal{V}}_{U}^{-}(z, d, j, \\gamma), \\mu-\\hat{c}_{1-\\beta, U} \\sqrt{\\widehat{\\Sigma}_{U, d J_{U}+\\hat{j}}}\\right\\}\n$$\n\nSince the distribution of $\\sqrt{n}\\left(\\widehat{\\ell}_{d^{*} \\hat{j}_{L}^{*}}+\\ell_{d^{*} \\hat{j}_{L}^{*}} \\hat{p}\\right)$ can be approximated by $\\mathcal{N}\\left(\\sqrt{n}\\left(\\widehat{\\ell}_{d^{*} \\hat{j}_{L}^{*}}+\\ell_{d^{*} \\hat{j}_{L}^{*}} p\\right), \\ell_{d^{*} \\hat{j}_{L}^{*}} \\Sigma \\ell_{d^{*} \\hat{j}_{L}^{*}}^{\\prime}\\right)$ asymptotically and $\\widehat{\\mathcal{Z}}_{L}\\left(d^{*} \\hat{j}_{L}^{*}\\right)$ is asymptotically independent, we again work with the truncated normal distribution to compute a hybrid probabilistic lower bound for $\\sqrt{n} L(\\hat{d})$ : for $0<\\beta<\\alpha<1$, define $\\widehat{L}(d)_{\\alpha}^{H}$ to solve\n\n$$\n\\begin{gathered}\nF_{T N}\\left(\\sqrt{n}\\left(\\widehat{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} \\hat{p}\\right): \\mu, \\ell_{d, \\hat{j}_{L}(d)} \\widehat{\\Sigma} \\ell_{d, \\hat{j}_{L}(d)}^{\\prime} \\mid \\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right), d, \\hat{j}_{L}(d), \\hat{\\gamma}_{L}(d)\\right) \\\\\n\\widehat{\\mathcal{V}}_{L}^{+, H}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right), d, \\hat{j}_{L}(d), \\hat{\\gamma}_{L}(d), \\mu\\right)\\right)=\\frac{1-\\alpha}{1-\\beta}\n\\end{gathered}\n$$\n\nin $\\mu$. Similarly, define $\\widehat{U}(d)_{\\alpha}^{H}$ to solve\n\n$$\n\\begin{gathered}\nF_{T N}\\left(\\sqrt{n}\\left(\\widehat{u}_{d, \\hat{j}_{U}(d)}+u_{d, \\hat{j}_{U}(d)} \\hat{p}\\right): \\mu, u_{d, \\hat{j}_{U}(d)} \\widehat{\\Sigma} u_{d, \\hat{j}_{U}(d)}^{\\prime} \\mid \\widehat{\\mathcal{V}}_{U}^{-, H}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d, \\hat{j}_{U}(d)\\right), d, \\hat{j}_{U}(d), \\hat{\\gamma}_{U}(d), \\mu\\right) \\\\\n\\widehat{\\mathcal{V}}_{U}^{+}\\left(\\widehat{\\mathcal{Z}}_{U}\\left(d, \\hat{j}_{U}(d)\\right), d, \\hat{j}_{U}(d), \\hat{\\gamma}_{U}(d)\\right)\\right)=\\frac{1-\\alpha}{1-\\beta}\n\\end{gathered}\n$$\n\nin $\\mu$.\nHere, $\\widehat{L}(\\hat{d})_{\\alpha}^{H}$ is an unconditionally valid probabilistic lower bound for $L(\\hat{d})$ and $\\widehat{U}(\\hat{d})_{1-\\alpha}^{H}$ is an unconditionally valid probabilistic upper bound for $U(\\hat{d})$. Combining these two confidence bounds at appropriate levels yields an unconditional CI for the identified interval $[L(\\hat{d}), U(\\hat{d})]$ of the selected $\\hat{d}$,\n\n$$\n\\left(n^{-1 / 2} \\widehat{L}(\\hat{d})_{\\alpha_{1}}^{H}, n^{-1 / 2} \\widehat{U}(\\hat{d})_{1-\\alpha_{2}}^{H}\\right)\n$$\n\nwith uniformly correct asymptotic coverage when $\\hat{d}$ is selected from the data according to Assumptions 3.2 and 3.3 .\n\nTheorem 5.3. Suppose Assumptions 3.1-3.6 hold. Then, for any $0<\\alpha_{1}, \\alpha_{2}<1 / 2$,\n\n$$\n\\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\mathbb{P}\\left([L(\\hat{d}), U(\\hat{d})] \\subseteq\\left(n^{-1 / 2} \\widehat{L}(\\hat{d})_{\\alpha_{1}}^{H}, n^{-1 / 2} \\widehat{U}(\\hat{d})_{1-\\alpha_{2}}^{H}\\right)\\right) \\geq 1-\\alpha_{1}-\\alpha_{2}\n$$",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 16,
      "text": "# 6 Reality Check Power Comparison \n\nTo our knowledge, the CIs proposed in this paper are the first with proven asymptotic validity for interval-identified parameters selected from the data. Therefore, we have no existing inference method to compare the performance of our CIs to when the interval-identified parameter is data-dependent. However, as discussed in Section 3 above, our inference framework covers cases for which the interval-identified parameter is chosen a priori. For these special cases, there is a large literature on inference on partially-identified parameters or their identified sets that can be applied to form CIs. Although these special cases are not of primary interest for this paper, in this section we compare the performance of our proposed inference methods to one of the leading inference methods in the partial identification literature as a \"reality check\" on whether our proposed methods are reasonably informative.\n\nIn particular, we compare the power of the test implied by our hybrid CI (i.e., a test that rejects when the value of the parameter under the null hypothesis lies outside of the hybrid CI) to the power of the hybrid test of Andrews et al. (2023), which applies to a general class of moment-inequality models. When $d$ is chosen a priori and the parameter $p$ is equal to a vector of moments of underlying data, Assumption 3.1 implies that $L(d) \\leq W(d) \\leq U(d)$ can be written as a set of (unconditional) moment inequalities, a special case of the general framework of that paper. Of the many papers on inference for moment inequalities, we choose the test of Andrews et al. (2023) for comparison for two reasons: (i) it has been shown to be quite competitive in terms of power and (ii) it is also based upon a (different) inference method that is a hybrid between conditional and projection-based inference.\n\nWe compare the power of tests on the ATE in the same setting of the Manski bounds example of Section 2, strengthening the mean independence assumption $\\mathbb{E}[Y(d) \\mid Z]=\\mathbb{E}[Y(d)]$ to full statistical independence $(Y(1), Y(0)) \\perp Z$ and using the sharp bounds on the ATE\n\n$W(1)-W(0)=\\mathbb{E}[Y(1)]-\\mathbb{E}[Y(0)]$ derived by Balke and Pearl (1997, 2011):\n\n$$\nL=\\max \\left\\{\\begin{array}{c}\np^{111}+p^{000}-1 \\\\\np^{110}+p^{001}-1 \\\\\np^{110}-p^{111}-p^{101}-p^{010}-p^{100} \\\\\np^{111}-p^{110}-p^{100}-p^{011}-p^{101} \\\\\n-p^{011}-p^{101} \\\\\n-p^{010}-p^{100} \\\\\np^{001}-p^{011}-p^{101}-p^{010}-p^{000} \\\\\np^{000}-p^{010}-p^{100}-p^{011}-p^{001}\n\\end{array}\\right\\}\n$$\n\nand\n\n$$\nU=\\min \\left\\{\\begin{array}{c}\n1-p^{011}-p^{100} \\\\\n1-p^{010}-p^{101} \\\\\n-p^{010}+p^{011}+p^{001}+p^{110}+p^{000} \\\\\n-p^{011}+p^{111}+p^{001}+p^{010}+p^{000} \\\\\np^{111}+p^{001} \\\\\np^{110}+p^{000} \\\\\n-p^{101}+p^{111}+p^{001}+p^{110}+p^{100} \\\\\n-p^{100}+p^{110}+p^{000}+p^{111}+p^{101}\n\\end{array}\\right\\}\n$$\n\nFor a sample size of $n=100$, we generated $\\hat{p}$ from a $\\mathcal{N}(p, \\Sigma)$ distribution. ${ }^{5}$ Figure 1 plots the power curves of the hybrid Andrews et al. (2023) test and the test implied by our hybrid CI for three different DGPs within the framework of this example, as well as the true identified interval for the ATE. The DGP corresponding to $p=(.08, .001, .001, .073, .139, .473)^{\\prime}$ is calibrated to the probabilities estimated by Balke and Pearl (2011) in the context of a treatment for high-cholesterol (specifically, by the drug cholestyramine). ${ }^{6}$ The DGP corresponding to $p=(.25, .25, .25, .25, .25, .25)^{\\prime}$ generates completely uninformative bounds for the ATE. And the DGP\n\n[^0]\n[^0]:    ${ }^{5}$ Note that in this problem, the value of $\\Sigma$ is implied by the value of $p$.\n    ${ }^{6}$ Balke and Pearl (2011) estimate $p$ to equal $(.081,0,0, .073, .139, .473)^{\\prime}$. If the true DGP is set exactly equal to this, Assumption 3.6 would be violated. We therefore slightly alter the calibrated probabilities.\n\n![img-0.jpeg](img-0.jpeg)\n\nFigure 1: Power curves for hybrid Andrews et al. (2023) test (red) and test implied by hybrid CI (blue) of ATE using bounds from Balke and Pearl $(1997,2011)$ for $p=(.08, .001, .001, .073, .139, .473)^{\\prime}$ (left), $p=(.25, .25, .25, .25, .25)^{\\prime}$ (middle) and $p=(.01, .44, .01, .01, .01, .54)^{\\prime}$ (right) and $n=100$. The vertical lines illustrate the true identified interval in each of these settings.\ncorresponding to $p=(.01, .44, .01, .01, .01, .54)^{\\prime}$ generates quite informative bounds.\nWe can see that overall, the power of the test implied by our hybrid CI is quite competitive with that of Andrews et al. (2023). Interestingly, it appears that our test tends to be more powerful than that of Andrews et al. (2023) when the true ATE is larger than the hypothesized one, which can be seen from the hypothesized ATE lying to the left of the lower bound of the identified interval, and vice versa. Although the main innovation of our inference procedures is really their validity in the presence of data-dependent selection, the exercise of this section is reassuring for the informativeness of the procedures we propose as they are quite competitive in the absence of selection.",
      "tables": {},
      "images": {
        "img-0.jpeg": "data:image/jpeg;base64,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"
      }
    },
    {
      "section_id": 17,
      "text": "# 7 Finite Sample Performance of Confidence Intervals \n\nMoving now to a context in which the object of interest is selected from the data, we compare the finite sample performance of our conditional, projection and hybrid CIs again in the setting of the Manski bounds example of Section 2. In this case, we are interested in inference on the average potential outcome $W(\\bar{d})$ for $W(d)=\\mathbb{E}[Y(d)]$, where interest arises either in the average potential outcome for treatment $(\\bar{d}=1)$ or control $(\\bar{d}=0)$ depending upon which has the largest estimated lower bound: $\\hat{d}=\\operatorname{argmax}_{d \\in\\{0,1\\}} \\widehat{L}(d)$. This form of $\\hat{d}$ corresponds to case 1. of Proposition 3.1 and we use the corresponding result of the proposition to specify $\\hat{\\gamma}_{L}(\\bar{d})$ and $\\hat{\\gamma}_{U}(\\bar{d})$ in the construction of the conditional and hybrid CIs. We report analogous simulation results for the dynamic treatment regime example in Appendix B with $\\hat{p}$ generated from a multinomial, rather than normal, distribution.\n\nFor the same DGPs as in Section 6, we compute the unconditional coverage frequencies of\n\nthe conditional, projection and hybrid CIs as well as that of the conventional CI based upon the normal distribution. These coverage frequencies are reported in Table 1. Consistent with the asymptotic results of Theorems 5.1, 5.2 and 5.3, the conditional, projection and hybrid CIs all have correct coverage for all DGPs and the modest sample size of $n=100$. Also consistent with Theorem 5.2, we note that the projection CI tends to be conservative with true coverage above the nominal level of $95 \\%$. Finally, we note that the conventional CI can substantially under-cover, consistent with the discussion in Section 2.1.\n\nTable 1: Unconditional Coverage Frequencies\n\n![table_0](table_0)\n\nThis table reports unconditional coverage frequencies for the potential outcome selected by maximizing the estimated lower bound on the potential outcomes of either treatment or control, all evaluated at the nominal coverage level of $95 \\%$. Coverage frequencies are reported for conventional (\"Conv\"), conditional (\"Cond\"), projection (\"Proj\") and hybrid (\"Hyb\") CIs for a sample size of $n=100$.\n\nNext, we compare the length quantiles of the CIs with correct coverage for these same DGPs. Figure 2 plots the ratios of the $5^{\\text {th }}, 25^{\\text {th }}, 50^{\\text {th }}, 75^{\\text {th }}$ and $95^{\\text {th }}$ quantiles of the length of the conditional, projection and hybrid CIs relative to those same length quantiles of the projection CI. As can be seen from the figure, the conditional CI has the tendency to become very long, especially at high quantile levels for certain DGPs, whereas the hybrid CI tends to perform the best overall by limiting the worst-case length performance of the conditional CI relative to the projection CI. Relative to projection, the hybrid CI enjoys length reductions of $20-30 \\%$ for favorable DGPs while only showing length increases of $5-10 \\%$ for unfavorable DGPs.",
      "tables": {
        "table_0": "|  | Confidence Interval |  |  |  |\n| :-- | :--: | :--: | :--: | :--: |\n| Data-Generating Process | Conv | Cond | Proj | Hyb |\n| $p=(.08, .001, .001, .073, .139, .473)^{\\prime}$ | 0.95 | 0.95 | 0.99 | 0.95 |\n| $p=(.25, .25, .25, .25, .25, .25)^{\\prime}$ | 0.85 | 0.95 | 0.96 | 0.95 |\n| $p=(.01, .44, .01, .01, .01, .54)^{\\prime}$ | 0.95 | 0.95 | 0.99 | 0.95 |"
      },
      "images": {}
    },
    {
      "section_id": 18,
      "text": "# 8 Application to Dynamic Treatment Choice \n\nWe revisit Han's (2024) application. Han (2024) considers schooling and post-school training as a sequence of treatments and estimates the partial ordering of treatment regimes and the identified set of the optimal regime. Han (2024) considers the Job Training Partnership Act (JTPA) program\n\n![img-1.jpeg](img-1.jpeg)\n\nFigure 2: Ratios of CI length quantiles relative to those of the projection CI for the conditional CI (red), projection CI (black) and hybrid CI (blue) of the average potential outcome selected by maximizing the estimated lower bound when $p=(.08,.001, .001, .073, .139, .473)^{\\prime}$ (left), $p=(.25, .25, .25, .25, .25, .25)^{\\prime}$ (middle) and $p=(.01, .44, .01, .01, .01, .54)^{\\prime}$ (right) and $n=100$.\nfor post-school training and a high school diploma (HS) (or its equivalents) for schooling. He considers high school diplomas rather than college degrees because the former is more relevant for the disadvantaged population of Title II of the JTPA program. In this paper, we are interested in conducting inference on welfare-earnings-evaluated at the regime chosen in a data-driven manner. The dataset is constructed by combining the JTPA data with the US Census and the National Center for Education Statistics (NCES). The following is the set of variables: $Y_{2}$ is an indicator for whether the individual is above or below the median of 30-month earnings, $D_{2}$ is an indicator for whether the individual participated in the job training program, $Z_{2}$ is an indicator for whether the individual was (randomly) assigned to the program, $Y_{1}$ is an indicator for whether the individual is above or below the 80th percentile of pre-program earnings, $D_{1}$ is an indicator for whether the individual received a HS diploma or GED, and $Z_{1}$ is an indicator for the density of high schools (Neal, 1997). ${ }^{7}$ The number of individuals in the sample is 9,223 . We assume $Z \\perp(Y(d), D(z))$.\n\nFollowing Han (2024), consider the dynamic treatment regime $\\boldsymbol{\\delta}(\\cdot) \\equiv\\left(\\delta_{1}, \\delta_{2}(\\cdot)\\right) \\in \\mathcal{D}^{*}$, where $\\delta_{1}$ indicates receipt of a HS diploma and $\\delta_{2}\\left(y_{1}\\right)$ indicates receipt of the job training program given pre-program earning type $y_{1}$. By having $\\delta_{2}$ as a function of $y_{1}$, the allocation decision adaptively incorporates information about unobserved characteristics of the individuals reflected in the response $Y_{1}$ to allocation $\\delta_{1}$. The counterfactual earning type in the terminal stage given $\\boldsymbol{\\delta}(\\cdot)$ is defined as $Y_{2}\\left(\\boldsymbol{\\delta}(\\cdot)\\right) \\equiv Y_{2}\\left(\\delta_{1}, \\delta_{2}\\left(Y_{1}\\left(\\delta_{1}\\right)\\right)\\right)$, where $Y_{1}\\left(\\delta_{1}\\right)$ is the counterfactual earning type in the first stage given\n\n[^0]\n[^0]:    ${ }^{7}$ Specifically, $Z_{1}=1$ if the number of high schools per square mile in each training site (e.g. a city) is above 35 .\n\n![table_1](table_1)\n\nTable 2: Dynamic Regimes $\\boldsymbol{\\delta}(\\cdot)$ When $T=2$ and $\\delta_{1}(x)=\\delta_{1}$\n$\\delta_{1}$. All possible regimes in $\\mathcal{D}^{*}$ are listed in Table 2. Suppose $\\boldsymbol{\\delta}^{*}$ is the optimal regime that maximizes the average terminal earning $W(\\boldsymbol{\\delta}) \\equiv \\mathbb{E}\\left[Y_{2}(\\boldsymbol{\\delta}(\\cdot))\\right]$ as welfare. We are interested in constructing CIs for $W(\\boldsymbol{\\delta})$ evaluated at $\\boldsymbol{\\delta}=\\overline{\\boldsymbol{\\delta}}$, where $\\overline{\\boldsymbol{\\delta}}$ is calculated from the estimated identified set of $\\boldsymbol{\\delta}^{*}$.\n\nWe first derive analytical bounds on the welfare under no additional assumptions. The distribution of data is expressed as the vector $p$ of the form\n\n$$\np \\equiv\\left\\{\\mathbb{P}\\left[D_{1}=d_{1}, Y_{1}=y_{1}, D_{2}=d_{2}, Y_{2}=y_{2} \\mid Z_{1}=z_{1}, Z_{2}=z_{2}\\right]\\right\\}_{\\left\\{d_{1}, y_{1}, d_{2}, y_{2}, z_{1}, z_{2}\\right\\}}\n$$\n\nThe welfare $W(\\boldsymbol{\\delta}) \\equiv \\mathbb{E}\\left[Y_{2}(\\boldsymbol{\\delta}(\\cdot))\\right]$ can be expressed as\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left[Y_{2}(\\boldsymbol{\\delta}(\\cdot))=1\\right] & =\\sum_{y_{1} \\in\\{0,1\\}} \\mathbb{P}\\left[Y_{2}\\left(\\delta_{1}, \\delta_{2}\\left(Y_{1}\\left(\\delta_{1}\\right), \\delta_{1}\\right)\\right)=1 \\mid Y_{1}\\left(\\delta_{1}\\right)=y_{1}\\right] \\mathbb{P}\\left[Y_{1}\\left(\\delta_{1}\\right)=y_{1}\\right] \\\\\n& =\\sum_{y_{1} \\in\\{0,1\\}} \\mathbb{P}\\left[Y_{1}\\left(\\delta_{1}\\right)=y_{1}, Y_{2}\\left(\\delta_{1}, \\delta_{2}\\left(y_{1}, \\delta_{1}\\right)\\right)=1\\right]\n\\end{aligned}\n$$\n\nby the law of iterated expectation. To derive bounds on $W(\\boldsymbol{\\delta})$, first consider bounds on $W_{y}(d) \\equiv \\mathbb{P}[Y(d)=y]$ for $d \\equiv\\left(d_{1}, d_{2}\\right)$, which are $L_{y}(d) \\equiv \\max _{z} L_{y}(d ; z)$ and $U_{y}(d) \\equiv \\min _{z} U_{y}(d ; z)$ where\n\n$$\n\\begin{aligned}\nL_{y}(d ; z) \\equiv & \\mathbb{P}[Y=y, D=d \\mid Z=z] \\\\\nU_{y}(d ; z) \\equiv & \\mathbb{P}[Y=y, D=d \\mid Z=z]+\\mathbb{P}\\left[Y_{1}=y_{1}, D_{1}=d_{1}, D_{2}=1-d_{2} \\mid Z=z\\right] \\\\\n& +\\mathbb{P}\\left[D_{1}=1-d_{1}, D_{2}=d_{2} \\mid Z=z\\right]+\\mathbb{P}\\left[D_{1}=1-d_{1}, D_{2}=1-d_{2} \\mid Z=z\\right] \\\\\n= & \\mathbb{P}[Y=y, D=d \\mid Z=z]+\\mathbb{P}\\left[Y_{1}=y_{1}, D_{1}=d_{1}, D_{2}=1-d_{2} \\mid Z=z\\right]\n\\end{aligned}\n$$\n\n$$\n+\\mathbb{P}\\left[D_{1}=1-d_{1} \\mid Z=z\\right]\n$$\n\nUsing these bounds, we can calculate bounds on\n\n$$\n\\begin{aligned}\nW(\\boldsymbol{\\delta}) \\equiv \\mathbb{P}\\left[Y_{2}(\\boldsymbol{\\delta}(\\cdot))=1\\right] & =\\sum_{y_{1} \\in\\{0,1\\}} \\mathbb{P}\\left[Y_{1}\\left(\\delta_{1}\\right)=y_{1}, Y_{2}\\left(\\delta_{1}, \\delta_{2}\\left(y_{1}, \\delta_{1}\\right)\\right)=1\\right] \\\\\n& =\\mathbb{P}\\left[Y_{1}\\left(\\delta_{1}\\right)=1, Y_{2}\\left(\\delta_{1}, \\delta_{2}\\left(1, \\delta_{1}\\right)\\right)=1\\right]+\\mathbb{P}\\left[Y_{1}\\left(\\delta_{1}\\right)=0, Y_{2}\\left(\\delta_{1}, \\delta_{2}\\left(0, \\delta_{1}\\right)\\right)=1\\right]\n\\end{aligned}\n$$\n\nwhich are\n\n$$\n\\begin{aligned}\n& L(\\boldsymbol{\\delta}) \\equiv \\max _{z} L_{(1,1)}\\left(\\delta_{1}, \\delta_{2}\\left(1, \\delta_{1}\\right) ; z\\right)+\\max _{z} L_{(0,1)}\\left(\\delta_{1}, \\delta_{2}\\left(0, \\delta_{1}\\right) ; z\\right) \\\\\n& U(\\boldsymbol{\\delta}) \\equiv \\min _{z} U_{(1,1)}\\left(\\delta_{1}, \\delta_{2}\\left(1, \\delta_{1}\\right) ; z\\right)+\\min _{z} U_{(0,1)}\\left(\\delta_{1}, \\delta_{2}\\left(0, \\delta_{1}\\right) ; z\\right)\n\\end{aligned}\n$$\n\nFor example, for the fourth regime in Table 2,\n\n$$\n\\begin{aligned}\n\\mathbb{P}\\left[Y_{2}\\left(\\boldsymbol{\\delta}_{(4)}(\\cdot)\\right)=1\\right] & =\\sum_{y_{1} \\in\\{0,1\\}} \\mathbb{P}\\left[Y_{1}\\left(\\delta_{1}\\right)=y_{1}, Y_{2}\\left(\\delta_{1}, \\delta_{2}\\left(y_{1}, \\delta_{1}\\right)\\right)=1\\right] \\\\\n& =\\mathbb{P}\\left[Y_{1}(1)=1, Y_{2}\\left(1, \\delta_{2}(1,1)\\right)=1\\right]+\\mathbb{P}\\left[Y_{1}(1)=0, Y_{2}\\left(1, \\delta_{2}(0,1)\\right)=1\\right] \\\\\n& =\\mathbb{P}\\left[Y_{1}(1)=1, Y_{2}(1,1)=1\\right]+\\mathbb{P}\\left[Y_{1}(1)=0, Y_{2}(1,0)=1\\right]\n\\end{aligned}\n$$\n\nis bounded by\n\n$$\n\\begin{aligned}\n& L\\left(\\boldsymbol{\\delta}_{(4)}\\right) \\equiv \\max _{z} L_{(1,1)}(1,1 ; z)+\\max _{z} L_{(0,1)}(1,0 ; z) \\\\\n& U\\left(\\boldsymbol{\\delta}_{(4)}\\right) \\equiv \\min _{z} U_{(1,1)}(1,1 ; z)+\\min _{z} U_{(0,1)}(1,0 ; z)\n\\end{aligned}\n$$\n\nSince $\\max _{z} L(z)+\\max _{z} \\tilde{L}(z)=\\max _{z, \\tilde{z}}\\{L(z)+\\tilde{L}(\\tilde{z})\\}$ for any functions $L$ and $\\tilde{L}$, we can express (8.3) as\n\n$$\nL(\\boldsymbol{\\delta})=\\max _{z, \\tilde{z}}\\left\\{L_{(1,1)}\\left(\\delta_{1}, \\delta_{2}\\left(1, \\delta_{1}\\right) ; z\\right)+L_{(0,1)}\\left(\\delta_{1}, \\delta_{2}\\left(0, \\delta_{1}\\right) ; \\tilde{z}\\right)\\right\\}\n$$\n\n$$\n\\equiv \\max _{z, \\tilde{z}} L(\\boldsymbol{\\delta} ; z, \\tilde{z})\n$$\n\nand, analogously, (8.4) as $U(\\boldsymbol{\\delta}) \\equiv \\max _{z, \\tilde{z}} U(\\boldsymbol{\\delta} ; z, \\tilde{z})$. Therefore the bounds satisfy Assumption 3.1. Now, consider choosing a single optimal regime that maximizes $L(\\boldsymbol{\\delta})$. Then, for example, we can show that the following event can be characterized as a polyhedron in the space of $p$\n\n$$\n\\left\\{\\boldsymbol{\\delta}_{(4)}=\\underset{\\boldsymbol{\\delta}}{\\arg \\max } L(\\boldsymbol{\\delta}) \\text { and } L\\left(\\boldsymbol{\\delta}_{(4)} ; z^{*}, \\tilde{z}^{*}\\right) \\geq L\\left(\\boldsymbol{\\delta}_{(4)} ; z, \\tilde{z}\\right) \\text { for all } z, \\tilde{z}\\right\\}\n$$\n\nwhere $\\left(z^{*}, \\tilde{z}^{*}\\right)=\\arg \\max _{z, \\tilde{z}} L\\left(\\boldsymbol{\\delta}_{(4)} ; z, \\tilde{z}\\right)$. Note that this event is equivalent to $\\left\\{L\\left(\\boldsymbol{\\delta}_{(4)} ; z^{*}, \\tilde{z}^{*}\\right) \\geq\\right.$ $L(\\boldsymbol{\\delta} ; z, \\tilde{z})$ for all $\\boldsymbol{\\delta}$ and $z, \\tilde{z}\\}$ or equivalently,\n\n$$\n\\left\\{L(\\boldsymbol{\\delta} ; z, \\tilde{z})-L\\left(\\boldsymbol{\\delta}_{(4)} ; z^{*}, \\tilde{z}^{*}\\right) \\leq 0 \\text { for all } \\boldsymbol{\\delta} \\text { and } z, \\tilde{z}\\right\\}\n$$\n\nNote that each $L(\\boldsymbol{\\delta} ; z, \\tilde{z})$ is a linear combination of the elements in $p$ as shown in (8.1) and (8.2). More formally, we can show that (8.5) is equivalent to $A_{L} p \\leq 0$ for some $A_{L}$. Note that\n\n$$\n\\begin{aligned}\nL(\\boldsymbol{\\delta} ; z, \\tilde{z}) & =L_{(1,1)}\\left(\\delta_{1}, \\delta_{2}\\left(1, \\delta_{1}\\right) ; z\\right)+L_{(0,1)}\\left(\\delta_{1}, \\delta_{2}\\left(0, \\delta_{1}\\right) ; \\tilde{z}\\right) \\\\\n& =A^{1}(\\boldsymbol{\\delta} ; z) p+A^{0}(\\boldsymbol{\\delta} ; \\tilde{z}) p\n\\end{aligned}\n$$\n\nfor some row vectors $A^{1}$ and $A^{0}$ because, for $\\boldsymbol{\\delta}=\\boldsymbol{\\delta}_{(4)}$,\n\n$$\n\\begin{aligned}\nL_{(1,1)}\\left(\\delta_{1}, \\delta_{2}\\left(1, \\delta_{1}\\right) ; z\\right) & =L_{(1,1)}(1,1 ; z)=\\mathbb{P}[Y=(1,1), D=(1,1) \\mid Z=z] \\\\\n& =\\mathbb{P}\\left[D_{1}=1, Y_{1}=1, D_{2}=1, Y_{2}=1 \\mid Z=z\\right] \\\\\nL_{(0,1)}\\left(\\delta_{1}, \\delta_{2}\\left(0, \\delta_{1}\\right) ; \\tilde{z}\\right) & =L_{(0,1)}(1,0 ; \\tilde{z})=\\mathbb{P}[Y=(0,1), D=(1,0) \\mid Z=\\tilde{z}] \\\\\n& =\\mathbb{P}\\left[D_{1}=1, Y_{1}=0, D_{2}=0, Y_{2}=1 \\mid Z=z\\right]\n\\end{aligned}\n$$\n\nAn analogous argument can be applied to $U(\\boldsymbol{\\delta} ; z, \\tilde{z})$. This characterization implies that Assumption 3.2 holds in this setting. Also, this characterization facilitates the CI calculations in Sections 5.1-5.2.\n\nTable 3 reports $95 \\%$ CIs for the welfare selected by maximizing the estimated lower bound on the welfare. Recall that the welfare is the probability that the 30-month earnings is above the median. It is notable that the hybrid CI is shorter than the conventional CI even though the latter does not have (uniformly) correct coverage. We can also see that although the hybrid CI is not quite contained in the projection CI, it is somewhat shorter. Finally, the conditional CI has infinite length in this example, demonstrating an extreme example of how the conditional CIs can be uninformatively long (see Kivaranovic and Leeb, 2021).\n\nTable 3: Confidence Intervals\n\n![table_2](table_2)\n\nThis table reports $95 \\%$ CIs for the welfare selected by maximizing the estimated lower bound on the welfare: conventional (\"Conv\"), conditional (\"Cond\"), projection (\"Proj\") and hybrid (\"Hyb\") CIs.\n\nAppendix B contains Monte Carlo simulated coverage frequencies of the CIs with various DGPs in the setting of this application. Overall, the findings are consistent with the ones in Section 7.",
      "tables": {
        "table_1": "| Regime \\# | $\\delta_{1}$ | $\\delta_{2}\\left(1, \\delta_{1}\\right)$ | $\\delta_{2}\\left(0, \\delta_{1}\\right)$ |\n| :--: | :--: | :--: | :--: |\n| 1 | 0 | 0 | 0 |\n| 2 | 1 | 0 | 0 |\n| 3 | 0 | 1 | 0 |\n| 4 | 1 | 1 | 0 |\n| 5 | 0 | 0 | 1 |\n| 6 | 1 | 0 | 1 |\n| 7 | 0 | 1 | 1 |\n| 8 | 1 | 1 | 1 |",
        "table_2": "| Conv | Cond | Proj | Hyb |\n| :--: | :--: | :--: | :--: |\n| $(0.32,0.78)$ | $(0.28, \\infty)$ | $(0.29,0.79)$ | $(0.28,0.73)$ |"
      },
      "images": {
        "img-1.jpeg": "data:image/jpeg;base64,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"
      }
    },
    {
      "section_id": 19,
      "text": "# A Additional Examples \n\nThis appendix contains examples in addition to Section 4, showing how they fall under the general framework of this paper.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 20,
      "text": "## A. 1 Revisiting Manski Bounds with a Continuous Outcome\n\nWe revisit the example with Manski bounds in Section 2, now allowing for a continuous outcome with bounded support. Let $Y \\in\\left[y^{I}, y^{u}\\right]$ be continuously distributed and assume $\\mathbb{E}[Y(d) \\mid Z]=\\mathbb{E}[Y(d)]$ for $d \\in\\{0,1\\}$. Note that the sharp bounds on $W(d) \\equiv \\mathbb{E}[Y(d)]$ are\n\n$$\n\\begin{aligned}\n& L(d) \\equiv \\max _{z \\in\\{0,1\\}}\\left\\{\\mathbb{E}[Y \\mid D=d, Z=z] \\mathbb{P}(D=d \\mid Z=z)+(1-\\mathbb{P}(D=d \\mid Z=z)) y^{I}\\right\\} \\\\\n& U(d) \\equiv \\min _{z \\in\\{0,1\\}}\\left\\{\\mathbb{E}[Y \\mid D=d, Z=z] \\mathbb{P}(D=d \\mid Z=z)+(1-\\mathbb{P}(D=d \\mid Z=z)) y^{u}\\right\\}\n\\end{aligned}\n$$\n\nand the sharp bounds on the ATE are $L(1,0)$ and $U(1,0)$ for $L(\\tilde{d}, d) \\equiv L(\\tilde{d})-U(d)$ and $U(\\tilde{d}, d) \\equiv U(\\tilde{d})-L(d)$. We can define the identified set $\\mathcal{D}^{*} \\subseteq \\mathcal{D}$ of optimal treatments as\n\n$$\n\\mathcal{D}^{*} \\equiv\\left\\{d \\in\\{0,1\\}: L(\\tilde{d}, d) \\leq 0, \\forall \\tilde{d} \\in\\{0,1\\}\\right\\}=\\left\\{d \\in\\{0,1\\}: \\max _{\\tilde{d} \\in\\{0,1\\}} L(\\tilde{d}) \\leq U(d)\\right\\}\n$$\n\nThen, Assumption 3.1 holds with\n\n$$\np=\\left(\\begin{array}{c}\n\\mathbb{E}[Y \\mid D=0, Z=1] \\mathbb{P}(D=0 \\mid Z=1) \\\\\n\\mathbb{E}[Y \\mid D=0, Z=0] \\mathbb{P}(D=0 \\mid Z=0) \\\\\n\\mathbb{E}[Y \\mid D=1, Z=1] \\mathbb{P}(D=1 \\mid Z=1) \\\\\n\\mathbb{E}[Y \\mid D=1, Z=0] \\mathbb{P}(D=1 \\mid Z=0) \\\\\n\\mathbb{P}(D=0 \\mid Z=1) \\\\\n\\mathbb{P}(D=0 \\mid Z=0)\n\\end{array}\\right)\n$$\n\nand $\\tilde{\\ell}_{0, j}=y^{I}$ and $\\tilde{\\ell}_{1, j}=0$ for $j \\in\\{0,1\\}$ and\n\n$$\n\\ell_{0,0}=\\left(\\begin{array}{llllllllll}\n0 & 1 & 0 & 0 & 0 & -y^{I}\n\\end{array}\\right), \\quad \\ell_{0,1}=\\left(\\begin{array}{llllllllllll}\n1 & 0 & 0 & 0 & -y^{I} & 0\n\\end{array}\\right)\n$$\n\n$$\n\\ell_{1,0}=\\left(\\begin{array}{llllllllll}\n0 & 0 & 0 & 1 & 0 & y^{l}\n\\end{array}\\right), \\quad \\ell_{1,1}=\\left(\\begin{array}{llllllllll}\n0 & 0 & 1 & 0 & y^{l} & 0\n\\end{array}\\right)\n$$\n\nand symmetrically for $\\tilde{u}_{d, j}$ and $u_{d, j}$. For $\\widehat{L}(d)$ and $\\widehat{U}(d)$ being the sample counterparts of $L(d)$ and $U(d)$ upon replacing $p$ with $\\hat{p}$, Assumption 3.2 holds by the argument in the paragraph after Assumption 3.2 since\n\n$$\n\\widehat{\\mathcal{D}}=\\left\\{d \\in\\{0,1\\}: \\max _{\\hat{d} \\in\\{0,1\\}} \\widehat{L}(\\hat{d}) \\leq \\widehat{U}(d)\\right\\}\n$$\n\nFor Assumption 3.4, let $\\left[y^{l}, y^{u}\\right]=[0,1]$ for simplicity. Note that\n\n$$\n\\begin{aligned}\n\\mathbb{E}[Y \\mid D=d, Z=z] \\mathbb{P}(D=d \\mid Z=z) & =\\mathbb{P}(D=d \\mid Z=z)-\\int_{0}^{1} \\mathbb{P}(Y \\leq y, D=d \\mid Z=z) d y \\\\\n& =\\mathbb{P}(D=d \\mid Z=z)-E\\left[\\int_{0}^{1} 1(Y \\leq y, D=d) d y \\mid Z=z\\right]\n\\end{aligned}\n$$\n\nwhere the first equality uses integration by parts. Therefore, we can estimate the elements of $p$ by sample means, forming $\\hat{p}$.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 21,
      "text": "# A. 2 Empirical Welfare Maximization via Linear Programming \n\nContinuing from Section 4.2, we show how sharp bounds on $W(\\delta)$ can be computed using linear programming. This can be done by extending the example in Section 4.1 with binary $Y$. Again, let $\\mathcal{X}=\\left\\{x_{1}, \\ldots, x_{K}\\right\\}$. Let $q(e \\mid x) \\equiv \\mathbb{P}(\\varepsilon=e \\mid X=x)$. In analogy,\n\n$$\n\\mathbb{E}[Y(d) \\mid X=x]=\\mathbb{P}[Y(d)=1 \\mid X=x]=\\sum_{e: y(d)=1} q(e \\mid x) \\equiv A_{d} q(x)\n$$\n\nwhere $q(x)$ is a vector with entries $q(e \\mid x)$ across $e$, and\n\n$$\n\\begin{gathered}\n\\mathbb{P}[Y=1, D=d \\mid Z=z, X=x]=\\mathbb{P}[Y(d)=1, D(z)=d \\mid X=x] \\\\\n=\\sum_{e: y(d)=1, d(z)=d} \\mathbb{P}[\\varepsilon=e \\mid X=x] \\equiv B_{d, z} q(x)\n\\end{gathered}\n$$\n\nwhere the first equality holds by the independence assumption in the previous section. Then the constraint for each $x \\in \\mathcal{X}$ becomes\n\n$$\nB q(x)=p(x)\n$$\n\nwhere $p(x)$ is the vector of $p(y, d \\mid z, x)$ 's across $(y, d, z)$ fixing $x$. Now we can construct a linear program for welfare:\n\n$$\n\\begin{aligned}\nW(\\delta)=\\mathbb{E}[\\delta(X) \\Delta(X)] & =\\sum_{x_{k} \\in \\mathcal{X}} p\\left(x_{k}\\right) \\delta\\left(x_{k}\\right) \\Delta\\left(x_{k}\\right) \\\\\n& =\\sum_{x_{k} \\in \\mathcal{X}} p\\left(x_{k}\\right) \\delta\\left(x_{k}\\right)\\left(A_{1}-A_{0}\\right) q\\left(x_{k}\\right)\n\\end{aligned}\n$$\n\nTherefore $W(\\delta)$ satisfies the structure of Section 4.1 and by analogous arguments, Assumptions 3.1, 3.2 and 3.4 hold.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 22,
      "text": "# B Additional Monte Carlo Simulations \n\nIn analogy with Section 7, we compute the unconditional coverage frequencies of the conditional, projection and hybrid CIs for DGPs in the dynamic treatment regime setting of the empirical application (Section 8). In particular, we consider two DGPs: DGP 1 generates $\\hat{p}$ from a multinomial distribution based on $p_{d_{1}, y_{1}, d_{2}, y_{2} \\mid z_{1}, z_{2}}=0.25$ for $\\left(d_{1}, y_{1}, d_{2}, y_{2}\\right) \\in\\{(1,0,0,1),(1,1,1,1)\\}$ and all $\\left(z_{1}, z_{2}\\right)$ and $p_{d_{1}, y 1, d_{2}, y_{2} \\mid z_{1}, z_{2}}=0.0357$ for all other $\\left(d_{1}, y_{1}, d_{2}, y_{2}\\right)$ and all $\\left(z_{1}, z_{2}\\right)$; DGP 2 generates $\\hat{p}$ based on $p_{d_{1}, y_{1}, d_{2}, y_{2} \\mid z_{1}, z_{2}}=0.375$ for $\\left(d_{1}, y_{1}, d_{2}, y_{2}\\right) \\in\\{(1,0,0,1),(1,1,1,1)\\}$ and all $\\left(z_{1}, z_{2}\\right)$ and $p_{d_{1}, y 1, d_{2}, y_{2} \\mid z_{1}, z_{2}}=0.0179$ for all other $\\left(d_{1}, y_{1}, d_{2}, y_{2}\\right)$ and all $\\left(z_{1}, z_{2}\\right)$. The coverage frequencies of the CIs are reported in Table 4. Again, consistent with the asymptotic results of Theorems 5.1-5.3, the conditional, projection and hybrid CIs all have correct coverage for all DGPs and the sample size of $n=1000$. Also, the projection CI tends to be conservative with true coverage above the nominal level of $95 \\%$, and the conventional CI can substantially under-cover.\n\nFigure 3 plots the ratios of the $5^{\\text {th }}, 25^{\\text {th }}, 50^{\\text {th }}, 75^{\\text {th }}$ and $95^{\\text {th }}$ quantiles of the length of the conditional, projection and hybrid CIs relative to those same length quantiles of the projection\n\nTable 4: Unconditional Coverage Frequencies\n\n![table_3](table_3)\n\nThis table reports unconditional coverage frequencies for the potential outcome selected by maximizing the estimated lower bound on the potential outcomes of either treatment or control, all evaluated at the nominal coverage level of $95 \\%$. Coverage frequencies are reported for conventional (\"Conv\"), conditional (\"Cond\"), projection (\"Proj\") and hybrid (\"Hyb\") CIs for a sample size of $n=1000$.\n![img-2.jpeg](img-2.jpeg)\n\nFigure 3: Ratios of CI length quantiles relative to those of the projection CI for the conditional CI (red), projection CI (black) and hybrid CI (blue) of the average potential outcome selected by maximizing the estimated lower bound for DGP 1 (left) and DGP 2 (right) and $n=100$.\nCI. The figure shows that the conditional CI has the tendency to become very long, especially at high quantile levels for certain DGPs, whereas the hybrid CI tends to perform the best overall by limiting the worst-case length performance of the conditional CI relative to the projection CI. Relative to projection, the hybrid CI enjoys length reductions of $10-20 \\%$ for favorable DGPs while showing length increases of $5-10 \\%$ for unfavorable DGPs.",
      "tables": {
        "table_3": "|  | Confidence Interval |  |  |  |\n| :-- | :--: | :--: | :--: | :--: |\n| Data-Generating Process | Conv | Cond | Proj | Hyb |\n| DGP 1 | 0.66 | 0.94 | 0.99 | 0.94 |\n| DGP 2 | 0.88 | 0.94 | 0.99 | 0.95 |"
      },
      "images": {
        "img-2.jpeg": "data:image/jpeg;base64,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"
      }
    },
    {
      "section_id": 23,
      "text": "# C Technical Appendix \n\nProof of Proposition 3.1: Assumption 3.3 is trivially satisfied by supposition. For Assumption 3.2 , note that $d \\in \\widehat{\\mathcal{D}}$ is equivalent to\n\n$$\nw_{L} \\widehat{L}(d)+w_{U} \\widehat{U}(d) \\geq w_{L} \\widehat{L}\\left(d^{\\prime}\\right)+w_{U} \\widehat{U}\\left(d^{\\prime}\\right)\n$$\n\nfor all $d^{\\prime} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$ or\n\n$$\n\\begin{aligned}\n& w_{L} \\cdot \\max _{j \\in\\left\\{1, \\ldots, J_{L}\\right\\}}\\left\\{\\tilde{\\ell}_{d, j}+\\ell_{d, j} \\hat{p}\\right\\}+w_{u} \\cdot \\min _{j \\in\\left\\{1, \\ldots, J_{U}\\right\\}}\\left\\{\\tilde{u}_{d, j}+u_{d, j} \\hat{p}\\right\\} \\\\\n\\geq & w_{L} \\cdot \\max _{j \\in\\left\\{1, \\ldots, J_{L}\\right\\}}\\left\\{\\tilde{\\ell}_{d^{\\prime}, j}+\\ell_{d^{\\prime}, j} \\hat{p}\\right\\}+w_{u} \\cdot \\min _{j \\in\\left\\{1, \\ldots, J_{U}\\right\\}}\\left\\{\\tilde{u}_{d^{\\prime}, j}+u_{d^{\\prime}, j} \\hat{p}\\right\\}\n\\end{aligned}\n$$\n\nfor all $d^{\\prime} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$. Therefore, we have the following.\n\n1. When $w_{U}=0, d \\in \\widetilde{\\mathcal{D}}$ and $\\hat{j}_{L}(d)=j_{L}^{*}$ if and only if $\\tilde{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} \\hat{p} \\geq \\tilde{\\ell}_{d^{\\prime}, j}+\\ell_{d^{\\prime}, j} \\hat{p}$ for all $d^{\\prime} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$ and $j \\in\\left\\{1, \\ldots, J_{L}\\right\\}$. Similarly, when $w_{U}=0, d \\in \\widetilde{\\mathcal{D}}, \\hat{j}_{U}(d)=j_{U}^{*}$ and $\\hat{j}_{L}(d)=j_{L}^{*}$ if and only if $\\tilde{\\ell}_{d, j_{L}^{*}}+\\ell_{d, j_{L}^{*}} \\hat{p} \\geq \\tilde{\\ell}_{d^{\\prime}, j}+\\ell_{d^{\\prime}, j} \\hat{p}$ for all $d^{\\prime} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$ and $j \\in\\left\\{1, \\ldots, J_{L}\\right\\}$ and $A_{U}\\left(d, j_{U}^{*}\\right) \\hat{p} \\leq c_{U}\\left(d, j_{U}^{*}\\right)$. Thus,\n\n$$\n\\begin{aligned}\n& A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)=\\left(\\begin{array}{c}\n\\ell_{0,1}-\\ell_{d, j_{L}^{*}} \\\\\n\\vdots \\\\\n\\ell_{0, J_{L}}-\\ell_{d, j_{L}^{*}} \\\\\n\\vdots \\\\\n\\ell_{T, 1}-\\ell_{d, j_{L}^{*}} \\\\\n\\vdots \\\\\n\\ell_{T, J_{L}}-\\ell_{d, j_{L}^{*}}\n\\end{array}\\right), \\quad c^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)=\\left(\\begin{array}{c}\n\\tilde{\\ell}_{d, j_{L}^{*}}-\\tilde{\\ell}_{0,1} \\\\\n\\vdots \\\\\n\\tilde{\\ell}_{d, j_{L}^{*}}-\\tilde{\\ell}_{0, J_{L}} \\\\\n\\vdots \\\\\n\\tilde{\\ell}_{d, j_{L}^{*}}-\\tilde{\\ell}_{T, 1} \\\\\n\\vdots \\\\\n\\tilde{\\ell}_{d, j_{L}^{*}}-\\tilde{\\ell}_{T, J_{L}}\n\\end{array}\\right), \\\\\n& A^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right)=\\left(\\begin{array}{c}\nA^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right) \\\\\nA_{U}\\left(d, j_{U}^{*}\\right)\n\\end{array}\\right), \\quad c^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right)=\\left(\\begin{array}{c}\n\\tilde{\\ell}_{d, j_{L}^{*}} \\tilde{\\ell}_{0,1} \\\\\n\\vdots \\\\\n\\tilde{\\ell}_{d, j_{L}^{*}}-\\tilde{\\ell}_{0, J_{L}} \\\\\n\\vdots \\\\\n\\tilde{\\ell}_{d, j_{L}^{*}}-\\tilde{\\ell}_{T, 1} \\\\\n\\vdots \\\\\n\\tilde{\\ell}_{d, j_{L}^{*}}-\\tilde{\\ell}_{T, J_{L}}\n\\end{array}\\right) \\text {, } \\\\\n& \\text { where } A_{U}(d, j)=\\left(u_{d, j}-u_{d, 1}, \\ldots, u_{d, j}-u_{d, J_{L}}\\right)^{\\prime} \\text { and } c_{U}(d, j)=\\left(\\tilde{u}_{d, 1}-\\tilde{u}_{d, j}, \\ldots, \\tilde{u}_{d, J_{L}}-\\tilde{u}_{d, j}\\right)^{\\prime} \\text {. } \\\\\n& \\text { 2. When } w_{L}=0, d \\in \\widetilde{\\mathcal{D}}, \\left(\\hat{j}_{U}(0), \\ldots, \\hat{j}_{U}(T)\\right)^{\\prime}=\\left(j_{U}^{*}(0), \\ldots, j_{U}^{*}(T)\\right)^{\\prime} \\text { and } \\hat{j}_{L}(d)=j_{L}^{*} \\text { if and only if } \\\\\n& \\tilde{u}_{d, j_{U}^{*}(d)}+u_{d, j_{U}^{*}(d)} \\hat{p} \\geq \\tilde{u}_{d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)}+u_{d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)} \\hat{p} \\text { for all } d^{\\prime} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}, A_{U}\\left(d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)\\right) \\hat{p} \\leq c_{U}\\left(d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)\\right) \\\\\n& \\text { for all } d^{\\prime} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\} \\text { and } A_{L}\\left(d, j_{L}^{*}\\right) \\hat{p} \\leq c_{L}\\left(d, j_{L}^{*}\\right) \\text {. Similarly, when } w_{L}=0, d \\in \\widetilde{\\mathcal{D}} \\text { and } \\\\\n& \\left(\\hat{j}_{U}(0), \\ldots, \\hat{j}_{U}(T)\\right)^{\\prime}=\\left(j_{U}^{*}(0), \\ldots, j_{U}^{*}(T)\\right)^{\\prime} \\text { if and only if } \\tilde{u}_{d, j_{U}^{*}(d)}+u_{d, j_{U}^{*}(d)} \\hat{p} \\geq \\tilde{u}_{d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)}+u_{d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)} \\hat{p} \\text { and }\n\\end{aligned}\n$$\n\n$A_{U}\\left(d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)\\right) \\hat{p} \\leq c_{U}\\left(d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)\\right)$ for all $d^{\\prime} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$. Thus,\n\n$$\n\\begin{aligned}\n& A^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right)=\\left(\\begin{array}{c}\nu_{0, j_{U}^{*}(0)}-u_{d, j_{U}^{*}(d)} \\\\\n\\vdots \\\\\nu_{T, j_{U}^{*}(T)}-u_{d, j_{U}^{*}(d)} \\\\\nA_{U}\\left(0, j_{U}^{*}(0)\\right) \\\\\n\\vdots \\\\\nA_{U}\\left(T, j_{U}^{*}(T)\\right)\n\\end{array}\\right), \\quad c^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right)=\\left(\\begin{array}{c}\n\\tilde{u}_{d, j_{U}^{*}(d)}-\\tilde{u}_{0, j_{U}^{*}(0)} \\\\\n\\vdots \\\\\n\\tilde{u}_{d, j_{U}^{*}(d)}-\\tilde{u}_{T, j_{U}^{*}(T)} \\\\\nc_{U}\\left(0, j_{U}^{*}(0)\\right) \\\\\n\\vdots \\\\\nc_{U}\\left(T, j_{U}^{*}(T)\\right)\n\\end{array}\\right), \\\\\n& A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)=\\left(\\begin{array}{c}\nA^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right) \\\\\nA_{L}\\left(d, j_{L}^{*}\\right)\n\\end{array}\\right), \\quad c^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)=\\left(\\begin{array}{c}\nc^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right) \\\\\nc_{L}\\left(d, j_{L}^{*}\\right)\n\\end{array}\\right),\n\\end{aligned}\n$$\n\nwhere $A_{L}(d, j)=\\left(\\ell_{d, 1}-\\ell_{d, j}, \\ldots, \\ell_{d, J_{L}}-\\ell_{d, j}\\right)^{\\prime}$ and $c_{L}(d, j)=\\left(\\tilde{\\ell}_{d, j}-\\tilde{\\ell}_{d, 1}, \\ldots, \\tilde{\\ell}_{d, j}-\\tilde{\\ell}_{d, J_{L}}\\right)^{\\prime}$.\n3. When $w_{L}, w_{U} \\neq 0, d \\in \\widehat{\\mathcal{D}}$ and\n\n$$\n\\left(\\hat{j}_{L}(0), \\ldots, \\hat{j}_{L}(T), \\hat{j}_{U}(0), \\ldots, \\hat{j}_{U}(T)\\right)^{\\prime}=\\left(j_{L}^{*}(0), \\ldots, j_{L}^{*}(T), j_{U}^{*}(0), \\ldots, j_{U}^{*}(T)\\right)^{\\prime}\n$$\n\nif and only if\n\n$$\n\\begin{gathered}\nw_{L}\\left(\\tilde{\\ell}_{d, j_{L}^{*}(d)}+\\ell_{d, j_{L}^{*}(d)} \\hat{p}\\right)+w_{U}\\left(\\tilde{u}_{d, j_{U}^{*}(d)}+u_{d, j_{U}^{*}(d)} \\hat{p}\\right) \\\\\n\\geq w_{L}\\left(\\tilde{\\ell}_{d^{\\prime}, j_{L}^{*}(d^{\\prime})}+\\ell_{d^{\\prime}, j_{L}^{*}(d^{\\prime})} \\hat{p}\\right)+w_{U}\\left(\\tilde{u}_{d^{\\prime}, j_{U}^{*}(d^{\\prime})}+u_{d^{\\prime}, j_{U}^{*}(d^{\\prime})} \\hat{p}\\right),\n\\end{gathered}\n$$\n\n$$\nA_{U}\\left(d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)\\right) \\hat{p} \\leq c_{U}\\left(d^{\\prime}, j_{U}^{*}\\left(d^{\\prime}\\right)\\right) \\text { and } A_{L}\\left(d^{\\prime}, j_{L}^{*}\\left(d^{\\prime}\\right)\\right) \\hat{p} \\leq c_{L}\\left(d^{\\prime}, j_{L}^{*}\\left(d^{\\prime}\\right)\\right) \\text { for all } d^{\\prime} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}\n$$\n\n$$\n\\begin{aligned}\n& A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)=A^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right)=\\left(\\begin{array}{c}\nw_{L}\\left(\\ell_{0, j_{L}^{*}(0)}-\\ell_{d, j_{L}^{*}(d)}\\right)+w_{U}\\left(u_{0, j_{L}^{*}(0)}-u_{d, j_{L}^{*}(d)}\\right) \\\\\n\\vdots \\\\\nw_{L}\\left(\\ell_{T, j_{L}^{*}(T)}-\\ell_{d, j_{L}^{*}(d)}\\right)+w_{U}\\left(u_{T, j_{L}^{*}(T)}-u_{d, j_{L}^{*}(d)}\\right) \\\\\nA_{L}\\left(0, j_{L}^{*}(0)\\right) \\\\\n\\vdots \\\\\nA_{L}\\left(T, j_{L}^{*}(T)\\right) \\\\\nA_{U}\\left(0, j_{U}^{*}(0)\\right) \\\\\n\\vdots \\\\\nA_{U}\\left(T, j_{U}^{*}(T)\\right)\n\\end{array}\\right) \\\\\n& c^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)=c^{U}\\left(d, j_{U}^{*}, \\gamma_{U}^{*}\\right)=\\left(\\begin{array}{c}\nw_{L}\\left(\\tilde{\\ell}_{d, j_{L}^{*}(d)}-\\tilde{\\ell}_{0, j_{L}^{*}(0)}\\right)+w_{U}\\left(\\tilde{u}_{d, j_{L}^{*}(d)}-\\tilde{u}_{0, j_{L}^{*}(0)}\\right) \\\\\n\\vdots \\\\\nw_{L}\\left(\\tilde{\\ell}_{d, j_{L}^{*}(d)}-\\tilde{\\ell}_{T, j_{L}^{*}(T)}\\right)+w_{U}\\left(\\tilde{u}_{d, j_{L}^{*}(d)}-\\tilde{u}_{T, j_{L}^{*}(T)}\\right) \\\\\nc_{L}\\left(0, j_{L}^{*}(0)\\right) \\\\\n\\vdots \\\\\nc_{L}\\left(T, j_{L}^{*}(T)\\right) \\\\\nc_{U}\\left(0, j_{U}^{*}(0)\\right) \\\\\n\\vdots \\\\\nc_{U}\\left(T, j_{U}^{*}(T)\\right)\n\\end{array}\\right)\n\\end{aligned}\n$$\n\nProof of Proposition 4.1: Assumption 3.3 is trivially satisfied by supposition. For Assumption 3.2, note that\n\n1. $\\underline{k}_{i}=\\underline{k}_{i}^{*}$ and $\\tilde{k}_{i}=\\tilde{k}_{i}^{*}$ if and only if $\\left(u_{\\underline{k}_{i}^{*}}-u_{k}\\right) \\hat{p} \\leq \\tilde{u}_{k}-\\tilde{u}_{\\underline{k}_{i}^{*}}+\\left(u_{k}-u_{\\underline{k}_{i}^{*}}\\right) \\varepsilon_{i}$ for all $k=1, \\ldots, J_{u}$ and $\\left(\\ell_{k}-\\ell_{\\tilde{k}_{i}^{*}}\\right) \\hat{p} \\leq \\tilde{\\ell}_{\\tilde{k}_{i}^{*}}-\\tilde{\\ell}_{k}+\\left(\\ell_{\\tilde{k}_{i}^{*}}-\\ell_{k}\\right) \\varepsilon_{i}$ for all $k=1, \\ldots, J_{L} ;$\n2. $s_{i}^{\\ell}=-$ and $s_{i}^{u}=-$ if and only if $\\ell_{\\tilde{k}_{i}} \\hat{p} \\leq-\\tilde{\\ell}_{\\tilde{k}_{i}}-\\ell_{\\tilde{k}_{i}} \\varepsilon_{i}$ and $u_{\\underline{k}_{i}} \\hat{p} \\leq-\\tilde{u}_{\\underline{k}_{i}}-u_{\\underline{k}_{i}} \\varepsilon_{i}$\n3. $\\hat{d}=1$ if and only if $\\sum_{i \\in \\underline{m}}\\left(-u_{\\underline{k}_{i}} \\hat{p}\\right)+\\sum_{i \\in \\bar{m}}\\left(-\\ell_{\\tilde{k}_{i}} \\hat{p}\\right) \\leq \\sum_{i \\in \\underline{m}}\\left(\\tilde{u}_{\\underline{k}_{i}}+u_{\\underline{k}_{i}} \\varepsilon_{i}\\right)+\\sum_{i \\in \\bar{m}}\\left(\\tilde{\\ell}_{\\tilde{k}_{i}}+\\ell_{\\tilde{k}_{i}} \\varepsilon_{i}\\right)$,\n\nwhere $\\underline{m}=\\{i \\in\\{1, \\ldots, m\\}: s_{i}^{u}=+\\}$ and $\\bar{m}=\\{i \\in\\{1, \\ldots, m\\}: s_{i}^{\\ell}=+\\}$.\nLemma C.1. Suppose Assumptions 3.1, 3.2 and 3.4-3.6 hold. Then, for any $0<\\alpha<1$,\n\n$$\n\\begin{gathered}\n\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{\\alpha}^{C} \\mid d \\in \\widehat{\\mathcal{D}} \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right)-\\alpha \\right| \\\\\n\\cdot \\mathbb{P}\\left(d \\in \\widehat{\\mathcal{D}} \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right)=0 \\\\\n\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{u}_{d, \\hat{j}_{U}(d)}+u_{d, \\hat{j}_{U}(d)} p\\right) \\leq \\widehat{U}(d)_{\\alpha}^{C} \\mid d \\in \\widehat{\\mathcal{D}} \\hat{j}_{U}(d)=j_{U}^{*}, \\hat{\\gamma}_{U}(d)=\\gamma_{U}^{*}\\right)-\\alpha \\right| \\\\\n\\cdot \\mathbb{P}\\left(d \\in \\widehat{\\mathcal{D}} \\hat{j}_{U}(d)=j_{U}^{*}, \\hat{\\gamma}_{U}(d)=\\gamma_{U}^{*}\\right)=0\n\\end{gathered}\n$$\n\nfor all $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}, j_{L}^{*} \\in\\left\\{1, \\ldots, J_{L}\\right\\}, j_{U}^{*} \\in\\left\\{1, \\ldots, J_{U}\\right\\}, \\gamma_{L}^{*}$ in the support of $\\hat{\\gamma}_{L}(d)$ and $\\gamma_{U}^{*}$ in the support of $\\hat{\\gamma}_{U}(d)$.\n\nProof: The proof of (C.2) is nearly identical to the proof of (C.1) so that we only show the latter. Lemma A. 1 of Lee et al. (2016) implies that $F_{T N}\\left(t ; \\mu, \\sigma^{2}, v^{-}, v^{+}\\right)$is strictly decreasing in $\\mu$ so that $\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{\\alpha}^{C}$ is equivalent to\n\n$$\n\\begin{gathered}\nF_{T N}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} \\hat{p}\\right) ; \\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right), \\ell_{d, \\hat{j}_{L}(d)} \\widehat{\\mathcal{V}}_{d, \\hat{j}_{L}(d)}^{\\prime} \\mid \\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right)\\right) \\\\\n\\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right)\\right) \\geq 1-\\alpha\n\\end{gathered}\n$$\n\nwhere we use $\\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right)\\right)$ and $\\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right)\\right)$ as shorthand for\n\n$$\n\\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right), d, \\hat{j}_{L}(d), \\hat{\\gamma}_{L}(d)\\right)\n$$\n\nand\n\n$$\n\\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right), d, \\hat{j}_{L}(d), \\hat{\\gamma}_{L}(d)\\right)\n$$\n\nIn addition, Lemma 2 of McCloskey (2024) implies\n\n$$\n\\begin{gathered}\nF_{T N}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} \\hat{p}\\right) ; \\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right), \\ell_{d, \\hat{j}_{L}(d)} \\widehat{\\mathcal{V}}_{d, \\hat{j}_{L}(d)}^{\\prime} \\mid \\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right)\\right) \\\\\n\\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}\\left(d, \\hat{j}_{L}(d)\\right)\\right)\n\\end{gathered}\n$$\n\n$$\n=F_{T N}\\left(\\ell_{d, \\hat{j}_{L}(d)} \\sqrt{n}(\\hat{p}-p) ; 0, \\ell_{d, \\hat{j}_{L}(d)} \\widehat{\\Sigma} \\ell_{d, \\hat{j}_{L}(d)}^{\\prime} \\mid \\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}^{*}\\left(d, \\hat{j}_{L}(d)\\right)\\right), \\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}^{*}\\left(d, \\hat{j}_{L}(d)\\right)\\right)\\right)\n$$\n\nwhere $\\widehat{\\mathcal{Z}}_{L}^{*}(d, j)=\\sqrt{n} \\hat{p}-\\hat{b}_{L}(d, j) \\ell_{d, j} \\sqrt{n}(\\hat{p}-p)$. Therefore, $\\sqrt{n}\\left(\\hat{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{\\alpha}^{C}$ is equivalent to\n\n$$\nF_{T N}\\left(\\ell_{d, \\hat{j}_{L}(d)} \\sqrt{n}(\\hat{p}-p) ; 0, \\ell_{d, \\hat{j}_{L}(d)} \\widehat{\\Sigma} \\ell_{d, \\hat{j}_{L}(d)}^{\\prime} \\mid \\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}^{*}\\left(d, \\hat{j}_{L}(d)\\right)\\right), \\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}^{*}\\left(d, \\hat{j}_{L}(d)\\right)\\right)\\right) \\geq 1-\\alpha\n$$\n\nUnder Assumptions 3.1, 3.4 and 3.6, a slight modification of Lemma 5 of Andrews et al. (2024) implies that to prove (C.1), it suffices to show that for all subsequences $\\left\\{n_{s}\\right\\} \\subset\\{n\\},\\left\\{\\mathbb{P}_{n_{s}}\\right\\} \\in \\times_{n=1}^{\\infty} \\mathcal{P}_{n}$ with\n\n1. $\\Sigma\\left(\\mathbb{P}_{n_{s}}\\right) \\rightarrow \\Sigma^{*} \\in \\mathcal{S}=\\left\\{\\Sigma: 1 / \\bar{\\lambda} \\leq \\lambda_{\\min }(\\Sigma) \\leq \\lambda_{\\max }(\\Sigma) \\leq \\bar{\\lambda}\\right\\}$,\n2. $\\mathbb{P}_{n_{s}}\\left(d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right) \\rightarrow q^{*} \\in(0,1]$, and\n3. $\\sqrt{n_{s}} p_{n_{s}}\\left(\\mathbb{P}_{n_{s}}\\right) \\rightarrow p^{*} \\in[0, \\infty]^{\\operatorname{dim}(\\xi)}$\nfor some finite $\\bar{\\lambda}$, we have\n\n$$\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}_{n_{s}}\\left(\\sqrt{n_{s}}\\left(\\hat{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\left(\\mathbb{P}_{n_{s}}\\right)\\right) \\leq \\widehat{L}(d)_{\\alpha}^{C} \\mid d \\in \\widehat{\\mathcal{D}} \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right)=\\alpha\n$$\n\nfor all $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}, j_{L}^{*} \\in\\left\\{1, \\ldots, J_{L}\\right\\}$ and $\\gamma_{L}^{*}$ in the support of $\\hat{\\gamma}_{L}(d)$. Let $\\left\\{\\mathbb{P}_{n_{s}}\\right\\}$ be a sequence satisfying conditions 1.-3. Under $\\left\\{\\mathbb{P}_{n_{s}}\\right\\},\\left(\\sqrt{n_{s}}\\left(\\hat{p}-p\\left(\\mathbb{P}_{n_{s}}\\right)\\right), \\widehat{\\Sigma}\\right) \\xrightarrow{d}\\left(\\xi^{*}, \\Sigma^{*}\\right)$ by Assumptions 3.4 and 3.5 , where $\\xi^{*} \\sim \\mathcal{N}\\left(0, \\Sigma^{*}\\right)$. Note that condition 2. implies $\\sqrt{n_{s}}\\left(c^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)-A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right) \\hat{p}\\right)$ is asymptotically greater than zero with positive probability for all $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}, j_{L}^{*} \\in$ $\\left\\{1, \\ldots, J_{L}\\right\\}$ and $\\gamma_{L}^{*}$ in the support of $\\hat{\\gamma}_{L}(d)$ under $\\left\\{\\mathbb{P}_{n_{s}}\\right\\}$ since $\\left\\{d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right\\}=$ $\\left\\{c^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)-A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right) \\hat{p} \\geq 0\\right\\}$ by Assumptions 3.1 and 3.2. Consequently, Assumption 3.4 and condition 3. imply $\\sqrt{n_{s}}\\left(c^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)-A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right) p\\left(\\mathbb{P}_{n_{s}}\\right)\\right) \\rightarrow \\omega\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)>-\\infty$ for all $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}, j_{L}^{*} \\in\\left\\{1, \\ldots, J_{L}\\right\\}$ and $\\gamma_{L}^{*}$ in the support of $\\hat{\\gamma}_{L}(d)$. Thus, under Assumptions 3.1, 3.2 and 3.4-3.6, similar arguments to those used in the proof of Lemma 8 in Andrews et al. (2024) show that for any $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}, j_{L}^{*} \\in\\left\\{1, \\ldots, J_{L}\\right\\}$ and $\\gamma_{L}^{*}$ in the support of $\\hat{\\gamma}_{L}(d)$,\n\n$\\left(\\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}^{*}\\left(d, j_{L}^{*}\\right)\\right), \\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}^{*}\\left(d, j_{L}^{*}\\right)\\right)\\right) \\xrightarrow{d}\\left(\\mathcal{V}_{L}^{-, *}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right), \\mathcal{V}_{L}^{+, *}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)\\right)$ under $\\left\\{\\mathbb{P}_{n_{s}}\\right\\}$, where\n\n$$\n\\begin{aligned}\n& \\mathcal{V}_{L}^{-, *}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)=\\max _{k:\\left(A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right) b_{L}\\left(d, j_{L}^{*}\\right)\\right)_{k}<0} \\frac{\\left(\\omega\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)\\right)_{k}-\\left(A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)\\left(I-b_{L}\\left(d, j_{L}^{*}\\right) \\ell_{d, j_{L}^{*}}\\right) \\xi^{*}\\right)_{k}}{\\left(A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right) b_{L}\\left(d, j_{L}^{*}\\right)\\right)_{k}}, \\\\\n& \\mathcal{V}_{L}^{+, *}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)=\\min _{k:\\left(A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right) b_{L}\\left(d, j_{L}^{*}\\right)\\right)_{k}>0} \\frac{\\left(\\omega\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)\\right)_{k}-\\left(A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)\\left(I-b_{L}\\left(d, j_{L}^{*}\\right) \\ell_{d, j_{L}^{*}}\\right) \\xi^{*}\\right)_{k}}{\\left(A^{L}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right) b_{L}\\left(d, j_{L}^{*}\\right)\\right)_{k}},\n\\end{aligned}\n$$\n\nwith $b_{L}(d, j)=\\Sigma^{*} \\ell_{d, j}^{\\prime}\\left(\\ell_{d, j} \\Sigma^{*} \\ell_{d, j}^{\\prime}\\right)^{-1}$. This convergence is joint with that of $\\left(\\sqrt{n_{s}}\\left(\\hat{p}-p\\left(\\mathbb{P}_{n_{s}}\\right)\\right), \\widehat{\\Sigma}\\right)$ so that under $\\left\\{\\mathbb{P}_{n_{s}}\\right\\}$\n\n$$\n\\begin{gathered}\n\\left(\\sqrt{n_{s}} \\ell_{d, j_{L}^{*}}\\left(\\hat{p}-p\\left(\\mathbb{P}_{n_{s}}\\right)\\right), \\widehat{\\Sigma}, \\widehat{\\mathcal{V}}_{L}^{-}\\left(\\widehat{\\mathcal{Z}}_{L}^{*}\\left(d, j_{L}^{*}\\right)\\right), \\widehat{\\mathcal{V}}_{L}^{+}\\left(\\widehat{\\mathcal{Z}}_{L}^{*}\\left(d, j_{L}^{*}\\right)\\right)\\right) \\\\\n\\xrightarrow{d}\\left(\\ell_{d, j_{L}^{*}} \\xi^{*}, \\Sigma^{*}, \\mathcal{V}_{L}^{-, *}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right), \\mathcal{V}_{L}^{+, *}\\left(d, j_{L}^{*}, \\gamma_{L}^{*}\\right)\\right)\n\\end{gathered}\n$$\n\nfor all $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}, j_{L}^{*} \\in\\left\\{1, \\ldots, J_{L}\\right\\}$ and $\\gamma_{L}^{*}$ in the support of $\\hat{\\gamma}_{L}(d)$.\nUsing (C.4) and the equivalence in (C.3), the remaining arguments to prove (C.1) are nearly identical to those used in the proof of Proposition 1 of McCloskey (2024) and therefore omitted for brevity.\n\nLemma C.2. Suppose Assumptions 3.1, 3.2 and 3.4-3.6 hold. Then, for any $0<\\alpha<1$,\n\n$$\n\\begin{aligned}\n& \\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\tilde{j}_{L}(d)}+\\ell_{d, \\tilde{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{a}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\alpha\\right| \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}})=0 \\\\\n& \\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{u}_{d, \\tilde{j}_{U}(d)}+u_{d, \\tilde{j}_{U}(d)} p\\right) \\leq \\widehat{U}(d)_{a}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\alpha\\right| \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}})=0\n\\end{aligned}\n$$\n\nfor all $d \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$.\nProof: The results of this lemma follow from Lemma C. 1 since, e.g.,\n\n$$\n\\begin{gathered}\n\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\tilde{j}_{L}(d)}+\\ell_{d, \\tilde{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{a}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\alpha\\right| \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}}) \\\\\n=\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\tilde{j}_{L}(d)}+\\ell_{d, \\tilde{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{a}^{C}, d \\in \\widehat{\\mathcal{D}}\\right)-\\alpha \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}})\\right| \\\\\n=\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\sum_{J_{L}=1}^{J_{L}} \\sum_{\\gamma_{L}^{*}}\\left[\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\tilde{j}_{L}(d)}+\\ell_{d, \\tilde{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{a}^{C}, d \\in \\widehat{\\mathcal{D}} \\tilde{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right)\\right.\n\\end{gathered}\n$$\n\n$$\n\\begin{gathered}\n-\\alpha \\cdot \\mathbb{P}\\left(d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right) \\mid \\\\\n=\\limsup _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\sum_{j_{L}^{*}=1}^{J_{L}} \\sum_{\\gamma_{L}^{*}}\\left[\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{\\alpha}^{C} \\mid d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right)-\\alpha\\right]\\right. \\\\\n\\left.\\cdot \\mathbb{P}\\left(d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right) \\right\\rvert\\, \\\\\n\\leq \\limsup _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\sum_{j_{L}^{*}=1}^{J_{L}} \\sum_{\\gamma_{L}^{*}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{\\alpha}^{C} \\mid d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right)-\\alpha\\right| \\\\\n\\cdot \\mathbb{P}\\left(d \\in \\widehat{\\mathcal{D}}, \\hat{j}_{L}(d)=j_{L}^{*}, \\hat{\\gamma}_{L}(d)=\\gamma_{L}^{*}\\right)\n\\end{gathered}\n$$\n\nwhere the inner sums $\\sum_{\\gamma_{L}^{*}}$ are over the elements of the support of $\\hat{\\gamma}_{L}(d)$.\nProof of Theorem 5.1: The result of this theorem follows from Lemma C. 2 since\n\n$$\n\\begin{aligned}\n& \\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left[\\mathbb{P}\\left(\\sqrt{n}[L(d), U(d)] \\subseteq\\left(\\widehat{L}(d)_{\\alpha_{1}}^{C}, \\widehat{U}(d)_{1-\\alpha_{2}}^{C}\\right) \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\left(1-\\alpha_{1}-\\alpha_{2}\\right)\\right] \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}}) \\\\\n& \\geq \\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left[1-\\mathbb{P}\\left(\\sqrt{n} L(d) \\leq \\widehat{L}(d)_{\\alpha_{1}}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\mathbb{P}\\left(\\sqrt{n} U(d) \\geq \\widehat{U}(d)_{1-\\alpha_{2}}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)\\right. \\\\\n& \\left.-\\left(1-\\alpha_{1}-\\alpha_{2}\\right)\\right] \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}}) \\\\\n& \\geq \\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left[1-\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{\\alpha_{1}}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)\\right. \\\\\n& \\left.-\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{u}_{d, \\hat{j}_{U}(d)}+u_{d, \\hat{j}_{U}(d)} p\\right) \\geq \\widehat{U}(d)_{1-\\alpha_{2}}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\left(1-\\alpha_{1}-\\alpha_{2}\\right)\\right] \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}}) \\\\\n& =\\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left[\\alpha_{1}-\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{\\alpha_{1}}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)\\right. \\\\\n& \\left.+\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{u}_{d, \\hat{j}_{U}(d)}+u_{d, \\hat{j}_{U}(d)} p\\right) \\leq \\widehat{U}(d)_{1-\\alpha_{2}}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\left(1-\\alpha_{2}\\right)\\right] \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}}) \\\\\n& \\geq \\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left[\\alpha_{1}-\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p\\right) \\leq \\widehat{L}(d)_{\\alpha_{1}}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)\\right] \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}}) \\\\\n& +\\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left[\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{u}_{d, \\hat{j}_{U}(d)}+u_{d, \\hat{j}_{U}(d)} p\\right) \\leq \\widehat{U}(d)_{1-\\alpha_{2}}^{C} \\mid d \\in \\widehat{\\mathcal{D}}\\right)-\\left(1-\\alpha_{2}\\right)\\right] \\cdot \\mathbb{P}(d \\in \\widehat{\\mathcal{D}})=0\n\\end{aligned}\n$$\n\nwhere the second inequality follows from the facts that $L(d) \\geq \\tilde{\\ell}_{d, \\hat{j}_{L}(d)}+\\ell_{d, \\hat{j}_{L}(d)} p$ and $U(d) \\leq$ $\\tilde{u}_{d, \\hat{j}_{U}(d)}+u_{d, \\hat{j}_{U}(d)} p$ almost surely and the final equality follows from Lemma C.2.\n\nProof of Theorem 5.2: We start by showing\n\n$$\n\\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\mathbb{P}\\left(\\sqrt{n}\\left(\\hat{\\ell}_{\\hat{d}, \\hat{j}_{L}(\\hat{d})}+\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} p\\right) \\geq \\widehat{L}(\\hat{d})_{\\alpha}^{P}\\right) \\geq 1-\\alpha\n$$\n\nBy the same argument as in the proof of Lemma C.1, to prove (C.5), it suffices to show\n\n$$\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}_{n_{s}}\\left(\\sqrt{n_{s}}\\left(\\hat{\\ell}_{\\hat{d}, \\hat{j}_{L}(\\hat{d})}+\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} p\\left(\\mathbb{P}_{n_{s}}\\right)\\right) \\geq \\widehat{L}(\\hat{d})_{\\alpha}^{P}\\right) \\geq 1-\\alpha\n$$\n\nunder conditions 1 . and 3 . Since $\\sqrt{n_{s}}\\left(\\hat{\\ell}_{\\hat{d}, \\hat{j}_{L}(\\hat{d})}+\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} p\\right) \\geq \\widehat{L}(\\hat{d})_{\\alpha}^{P}$ is equivalent to $\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} \\sqrt{n_{s}}(\\hat{p}-p) \\leq$ $\\hat{c}_{1-\\alpha, L} \\sqrt{\\widehat{\\Sigma}_{L, \\hat{d} J_{L}+\\hat{j}_{L}(\\hat{d})}}$, the left-hand side of (C.6) is equal to\n\n$$\n\\begin{aligned}\n& \\lim _{n \\rightarrow \\infty} \\mathbb{P}_{n_{s}}\\left(\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} \\sqrt{n_{s}}\\left(\\hat{p}-p\\left(\\mathbb{P}_{n_{s}}\\right)\\right) \\leq \\hat{c}_{1-\\alpha, L} \\sqrt{\\widehat{\\Sigma}_{L, \\hat{d} J_{L}+\\hat{j}_{L}(\\hat{d})}}\\right) \\\\\n& \\geq \\lim _{n \\rightarrow \\infty} \\mathbb{P}_{n_{s}}\\left(\\ell^{\\text {mat }} \\sqrt{n_{s}}\\left(\\hat{p}-p\\left(\\mathbb{P}_{n_{s}}\\right)\\right) \\leq \\hat{c}_{1-\\alpha, L} \\sqrt{\\operatorname{Diag}\\left(\\widehat{\\Sigma}_{L}\\right)}\\right) \\\\\n= & \\mathbb{P}\\left(\\xi_{L} \\leq c_{1-\\alpha, L} \\sqrt{\\operatorname{Diag}\\left(\\Sigma_{L}\\right)}\\right)=\\mathbb{P}\\left(\\max _{i \\in\\{1, \\ldots,(T+1) J_{L}\\}} \\frac{\\xi_{L, i}}{\\sqrt{\\Sigma_{L, i}}} \\leq c_{1-\\alpha, L}\\right)=1-\\alpha\n\\end{aligned}\n$$\n\nunder conditions 1 . and 3 . for $\\xi_{L} \\sim \\mathcal{N}\\left(0, \\Sigma_{L}\\right), c_{\\alpha, L}$ denoting the $\\alpha$-quantile of\n\n$$\n\\max _{i \\in\\{1, \\ldots,(T+1) J_{L}\\}} \\frac{\\xi_{L, i}}{\\sqrt{\\Sigma_{L, i}}}\n$$\n\nand $\\Sigma_{L}=\\ell^{\\text {mat }} \\Sigma \\ell^{\\text {matr }}$, where all inequalities are taken element-wise across vectors, the inequality follows from the fact that $\\ell_{\\hat{d}, \\hat{j}_{L}(\\hat{d})} \\sqrt{n_{s}}\\left(\\hat{p}-p\\left(\\mathbb{P}_{n_{s}}\\right)\\right)$ is a (random) element of $\\ell^{\\text {mat }} \\sqrt{n_{s}}\\left(\\hat{p}-p\\left(\\mathbb{P}_{n_{s}}\\right)\\right)$ and the first equality follows by identical arguments to those used in the proof of Proposition 11 of Andrews et al. (2024). We have thus proved (C.5). In addition,\n\n$$\n\\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{u}_{\\hat{d}, \\hat{j}_{U}(\\hat{d})}+u_{\\hat{d}, \\hat{j}_{U}(\\hat{d})} p\\right) \\leq \\widehat{U}(\\hat{d})_{1-\\alpha}^{P}\\right) \\geq 1-\\alpha\n$$\n\nfollows by nearly identical arguments. The statement of the theorem then follows by nearly identical arguments to those used in the proof of Theorem 5.1.\n\nLemma C.3. Suppose Assumptions 3.1-3.6 hold. Then, for any $0<\\beta<\\alpha<1$,\n\n$$\n\\begin{gathered}\n\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right) \\leq \\widehat{L}(\\tilde{d})_{n}^{H}\\right| \\tilde{d}=d^{*}, \\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right) \\geq \\widehat{L}(\\tilde{d})_{\\beta}^{P}\\right)-\\frac{\\alpha-\\beta}{1-\\beta} \\mid \\\\\n\\cdot \\mathbb{P}\\left(\\tilde{d}=d^{*}, \\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right) \\geq \\widehat{L}(\\tilde{d})_{\\beta}^{P}\\right)=0 \\\\\n\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{u}_{\\tilde{d}, \\tilde{j}_{U}(\\tilde{d})}+u_{\\tilde{d}, \\tilde{j}_{U}(\\tilde{d})} p\\right) \\leq \\widehat{U}(\\tilde{d})_{n}^{H}\\right| \\tilde{d}=d^{*}, \\sqrt{n}\\left(\\tilde{u}_{\\tilde{d}, \\tilde{j}_{U}(\\tilde{d})}+u_{\\tilde{d}, \\tilde{j}_{U}(\\tilde{d})} p\\right) \\leq \\widehat{U}(\\tilde{d})_{1-\\beta}^{P}\\right)-\\frac{\\alpha-\\beta}{1-\\beta} \\mid \\\\\n\\cdot \\mathbb{P}\\left(\\hat{d}=d^{*}, \\sqrt{n}\\left(\\tilde{u}_{\\tilde{d}, \\tilde{j}_{U}(\\tilde{d})}+u_{\\tilde{d}, \\tilde{j}_{U}(\\tilde{d})} p\\right) \\leq \\widehat{U}(\\hat{d})_{1-\\beta}^{P}\\right)=0\n\\end{gathered}\n$$\n\nfor all $d^{*} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}$.\n\nProof: The proof of (C.8) is nearly identical to the proof of (C.7) so that we only show the latter. Upon noting that $F_{T N}\\left(t ; \\mu, \\sigma^{2}, \\tilde{\\mathcal{V}}_{L}^{-}(z, d, j, \\gamma), \\widehat{\\mathcal{V}}_{L}^{+, H}(z, d, j, \\gamma, \\mu)\\right)$ is decreasing in $\\mu$ by the same argument used in the proof of Proposition 5 of Andrews et al. (2024) and replacing condition 2. in the proof of Lemma C. 1 with\n\n$$\n2^{\\prime} . \\mathbb{P}_{n_{*}}\\left(\\tilde{d}=d^{*}, \\hat{j}_{L}(\\tilde{d})=j_{L}^{*}, \\hat{\\gamma}_{L}(\\tilde{d})=\\gamma_{L}^{*}, \\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right) \\geq \\widehat{L}(\\tilde{d})_{\\beta}^{P}\\right) \\rightarrow q^{*} \\in(0,1]\n$$\n\ncompletely analogous arguments to those used to prove (C.1) in Lemma C. 1 imply\n\n$$\n\\begin{gathered}\n\\lim _{n \\rightarrow \\infty} \\sup _{\\mathbb{P} \\in \\mathcal{P}_{n}}\\left|\\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right) \\leq \\widehat{L}(\\tilde{d})_{n}^{H}\\right| \\hat{d}=d^{*}, \\hat{j}_{L}(\\tilde{d})=j_{L}^{*}, \\hat{\\gamma}_{L}(\\tilde{d})=\\gamma_{L}^{*}, \\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right) \\geq \\widehat{L}(\\tilde{d})_{\\beta}^{P}\\right) \\\\\n\\left.-\\frac{\\alpha-\\beta}{1-\\beta}\\right| \\cdot \\mathbb{P}\\left(\\hat{d}=d^{*}, \\hat{j}_{L}(\\tilde{d})=j_{L}^{*}, \\hat{\\gamma}_{L}(\\tilde{d})=\\gamma_{L}^{*}, \\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right) \\geq \\widehat{L}(\\tilde{d})_{\\beta}^{P}\\right)=0\n\\end{gathered}\n$$\n\nfor all $d^{*} \\in\\left\\{d^{0}, \\ldots, d^{K}\\right\\}, j_{L}^{*} \\in\\left\\{1, \\ldots, J_{L}\\right\\}$ and $\\gamma_{L}^{*}$ in the support of $\\hat{\\gamma}_{L}\\left(d^{*}\\right)$. Then, the same argument as in the proof of Lemma C. 2 implies (C.7).\n\nLemma C.4. Suppose Assumptions 3.1-3.6 hold. Then, for any $0<\\alpha<1$,\n\n$$\n\\begin{gathered}\n\\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right)>\\widehat{L}(\\tilde{d})_{n}^{H}\\right) \\geq 1-\\alpha \\\\\n\\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{u}_{\\tilde{d}, \\tilde{j}_{U}(\\tilde{d})}+u_{\\tilde{d}, \\tilde{j}_{U}(\\tilde{d})} p\\right)<\\widehat{U}(\\tilde{d})_{1-\\alpha}^{H}\\right) \\geq 1-\\alpha\n\\end{gathered}\n$$\n\nProof: The proof of (C.10) is nearly identical to the proof of (C.9) so that we only show the\n\nlatter. Lemma 6 of Andrews et al. (2024) and Lemma C. 3 imply\n\n$$\n\\begin{aligned}\n& \\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right)>\\tilde{L}(\\tilde{d})_{\\alpha}^{H}\\right) \\\\\n\\geq & \\frac{1-\\alpha}{1-\\beta} \\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\sum_{d^{*}=0}^{T} \\mathbb{P}\\left(\\tilde{d}=d^{*}, \\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right) \\geq \\tilde{L}(\\tilde{d})_{\\beta}^{P}\\right) \\\\\n= & \\frac{1-\\alpha}{1-\\beta} \\liminf _{n \\rightarrow \\infty} \\inf _{\\mathbb{P} \\in \\mathcal{P}_{n}} \\mathbb{P}\\left(\\sqrt{n}\\left(\\tilde{\\ell}_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})}+\\ell_{\\tilde{d}, \\tilde{j}_{L}(\\tilde{d})} p\\right) \\geq \\tilde{L}(\\tilde{d})_{\\beta}^{P}\\right) \\geq 1-\\alpha\n\\end{aligned}\n$$\n\nwhere the final inequality follows from (C.5) in the proof of Theorem 5.2.\nProof of Theorem 5.3: Using Lemma C. 4 in the place of Lemma C.2, the proof is nearly identical to the proof of Theorem 5.1.",
      "tables": {},
      "images": {}
    },
    {
      "section_id": 24,
      "text": "# References \n\nAdjaho, C. and Christensen, T. (2022). Externally valid treatment choice. arXiv preprint arXiv:2205.05561, 1. 1\n\nAndrews, D. and Guggenberger, P. (2009). Incorrect asymptotic size of subsampling procedures based on post-consistent model selection estimators. Journal of Econometrics, 152:19-27. 1\n\nAndrews, I., Kitagawa, T., and McCloskey, A. (2024). Inference on winners. Quarterly Journal of Economics, 139:305-358. 1, 1, 2.3, C, C, C, C, C\n\nAndrews, I., Roth, J., and Pakes, A. (2023). Inference for linear conditional moment inequalities. Review of Economic Studies, 90:2763-2791. 1, 6, 1, 6\n\nAthey, S. and Wager, S. (2021). Policy learning with observational data. Econometrica, 89(1):133-161. 4.2\n\nAvis, D. and Fukuda, K. (1991). A pivoting algorithm for convex hulls and vertex enumeration of arrangements and polyhedra. In Proceedings of the seventh annual symposium on Computational geometry, pages 98-104. 4.1\n\nBalke, A. and Pearl, J. (1997). Bounds on treatment effects from studies with imperfect compliance. Journal of the American Statistical Association, 92:1171-1176. 1, 4.1, 6, 1\n\nBalke, A. and Pearl, J. (2011). Nonparametric bounds on causal effects from partial compliance data. UCLA Department of Statistics Paper. 1, 4.1, 6, 6, 1\n\nBen-Michael, E., Greiner, D. J., Imai, K., and Jiang, Z. (2021). Safe policy learning through extrapolation: Application to pre-trial risk assessment. arXiv preprint arXiv:2109.11679. 1\n\nByambadalai, U. (2022). Identification and inference for welfare gains without unconfoundedness. arXiv preprint arXiv:2207.04314. 4.2\n\nChristensen, T., Moon, H. R., and Schorfheide, F. (2023). Optimal decision rules when payoffs are partially identified. arXiv:2204.11748v2. 1, 3, 4.4, 4\n\nCui, Y. and Han, S. (2024). Policy learning with distributional welfare. arXiv preprint arXiv:2311.15878. 1, 4.2\n\nD\u2019Adamo, R. (2021). Orthogonal policy learning under ambiguity. arXiv preprint arXiv:2111.10904. $1,4.2$\n\nFithian, W., Sun, D. L., and Taylor, J. (2017). Optimal inference after model selection. arXiv: $1410.2597 v 4.1$\n\nHan, S. (2024). Optimal dynamic treatment regimes and partial welfare ordering. Journal of the American Statistical Association (forthcoming). 1, 4.1, 4.3, 4.3, 8\n\nHan, S. and Lee, S. (2024). Semiparametric models for dynamic treatment effects and mediation analyses with observational data. University of Bristol, Sogang University. 3\n\nHan, S. and Yang, S. (2024). A computational approach to identification of treatment effects for policy evaluation. Journal of Econometrics (forthcoming). 1, 4.1, 4.1, 2\n\nIshihara, T. and Kitagawa, T. (2021). Evidence aggregation for treatment choice. arXiv:2108.06473v1. 1\n\nKallus, N. and Zhou, A. (2021). Minimax-optimal policy learning under unobserved confounding. Management Science, 67(5):2870-2890. 1, 4.2\n\nKitagawa, T. and Tetenov, A. (2018). Who should be treated? empirical welfare maximization methods for treatment choice. Econometrica, 86:591-616. 4.2\n\nKivaranovic, D. and Leeb, H. (2021). On the length of post-model-selection confidence intervals conditional on polyhedral constraints. Journal of the American Statistical Association, $116: 845-857.1,2.3,8$\n\nLee, J. D., Sun, D. L., Sun, Y., and Taylor, J. E. (2016). Exact post-selection inference, with application to the LASSO. Annals of Statistics, 44:907-927. 1, 2.2, C\n\nManski, C. F. (1990). Nonparametric bounds on treatment effects. American Economic Review Papers and Proceedings, 80:319-323. 1, 2\n\nManski, C. F. (2021). Econometrics for decision making: building foundations sketched by haavelmo and wald. Econometrica, 89:2827-2853. 3\n\nManski, C. F. (2023). Probabilistic prediction for binary treatment choice: with focus on personalized medicine. Journal of Econometrics, 234:647-663. 3\n\nManski, C. F. and Pepper, J. V. (2000). Monotone instrumental variables: with an application to the returns to schooling. Econometrica, 68:997-1010. 1\n\nMcCloskey, A. (2024). Hybrid confidence intervals for informative uniform asymptotic inference after model selection. Biometrika, 111:109-127. 1, 2.3, 5.1, C, C\n\nMogstad, M., Santos, A., and Torgovitsky, A. (2018). Using instrumental variables for inference about policy relevant treatment parameters. Econometrica, 80:755-782. 1, 4.1\n\nNeal, D. (1997). The effects of catholic secondary schooling on educational achievement. Journal of Labor Economics, 15(1, Part 1):98-123. 8\n\nPfanzagl, J. (1994). Parametric Statistical Theory. De Gruyter. 2.2, 5.1\n\nPu, H. and Zhang, B. (2021). Estimating optimal treatment rules with an instrumental variable: A partial identification learning approach. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(2):318-345. 1, 4.2\n\nStoye, J. (2012). Minimax regret treatment choice with covariates or with limited validity of experiments. Journal of Econometrics, 166(1):138-156. 1\n\nTibshirani, R., Rinaldo, A., Tibshirani, R., and Wasserman, L. (2018). Uniform asymptotic inference and the bootstrap after model selection. Annals of Statistics, 46:679-710. 1\n\nYata, K. (2021). Optimal decision rules under partial identification. arXiv preprint arXiv:2111.04926. 1",
      "tables": {},
      "images": {}
    }
  ],
  "id": "2403.00422v2",
  "authors": [
    "Sukjin Han",
    "Adam McCloskey"
  ],
  "categories": [
    "econ.EM"
  ],
  "abstract": "Interval identification of parameters such as average treatment effects,\naverage partial effects and welfare is particularly common when using\nobservational data and experimental data with imperfect compliance due to the\nendogeneity of individuals' treatment uptake. In this setting, the researcher\nis typically interested in a treatment or policy that is either selected from\nthe estimated set of best-performers or arises from a data-dependent selection\nrule. In this paper, we develop new inference tools for interval-identified\nparameters chosen via these forms of selection. We develop three types of\nconfidence intervals for data-dependent and interval-identified parameters,\ndiscuss how they apply to several examples of interest and prove their uniform\nasymptotic validity under weak assumptions.",
  "updated": "2025-04-07T20:26:16Z",
  "published": "2024-03-01T10:17:08Z"
}